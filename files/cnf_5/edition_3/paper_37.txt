Fifth International Conference on Computer Graphics, Imaging and Visualization
Visualisation

Overlaid Text Recognition for Matching Soccer-Concept Keywords
Alfian Abdul Halin, Mandava Rajeswari, Dhanesh Ramachandram
School of Computer Sciences, Universiti Sains Malaysia
{alfian,mandava,dhaneshr}@cs.usm.my

OTs are also very meaningful to human viewers
because they provide an added dimension for video
content understanding. If a viewer misses a soccer
goal, the OT that follows can assist him to understand
what is happening. Instead of dealing with just visual
and aural aspects, OTs can greatly enhance the viewing
experience. It is thus beneficial to include OTs video
semantic analysis as they can provide cues for higher
level inferences. Since the Semantic Gap is still
considered an unsolved problem, OTs might be able to
find a place to possibly help narrow or fill this gap.
Hence, with the notion that common broadcasting
practice is to display OTs only when important
contents occur, we present a framework to extract,
recognize and make sense of OTs from selected sports
video. At this stage, we are focusing on soccer videos,
but it seems possible to extend to other sports domains
as well. The work is divided into two phases: (1) OT
Extraction and Recognition and (2) Domain Keyword
Matching.
Section 2 will briefly discuss related work in the
field of semantic video analysis, followed by Sections
3 and 4, which will explain the two phases of our
work. In Section 5, we show our experimental results.
Section 6 concludes with discussion of possible future
work.

Abstract
Overlaid-text appears frequently in broadcast
sports video. They provide a plethora of information
regarding the goings-on of a particular game.
Examples include important events and video segments
of interest such as bookings and half-time analysis,
respectively. Furthermore, it is common that overlaidtext is displayed when a particular concept is
happening or has happened. This paper presents a
concept identification framework, based on matched
keywords from overlaid-text extraction and
recognition. Possible occurrences of overlaid-text in
soccer programs are extracted and recognized, and
then matched against a soccer-term database.
Preliminary experiments show reliable character
extraction, whose recognition has been successfully
matched with keywords within the database.
Keywords--- Overlaid Text, Videotext, Text
Extraction, Optical Character Recognition,
Semantic Video Analysis, Semantic Gap, and Video
Retrieval.

1. Introduction

2. Related Work

Generally, text appearing in video can be divided
into Scene and Graphic/Overlaid-text [1]. Scene Text
naturally occurs in the recording environment whereas
overlaid-text (OT) is superimposed onto the video by
using title generators [2]. Since OTs are video genre
specific, they will contain text related to the domain
concept being conveyed. Thus, processing of OTs
would be meaningful as they can provide information
about the video at hand. This is especially true for
sports video programs, where OTs are extensively used
to convey current happenings of a game. Examples of
sports OTs are: (1) A soccer goal event accompanied
by the updating of the top left/right score-overlay, and
(2) A golf hole-in-one (HOI), which pops-out an
overlay revealing the HOI event as well as the golfer’s
name.

978-0-7695-3359-9/08 $25.00 © 2008 IEEE
DOI 10.1109/CGIV.2008.34

In the literature, many works have attempted to
translate human observations into computer processable formats, which in turn are used to automatically or
semi-automatically infer higher level concepts. These
involve investigation of low-level visual or aural
aspects of video for possible mid and/or high-level
representations such as action [3, 4], events [5-20],
video structure [21], genre [1] and so on and so forth.
Visual observations can be made from color,
texture, shape, lines, motion, etc. Kobla et al. [1]
studied macroblocks in MPEG video frames, texture
patterns as well motion to distinguish sports videos
from other genres. Ekin et al. [23] used color
histograms to analyze particular properties of soccer
video, such as dominant colors within video frames, as

235

events. Basavaraj and Nagaraj [32] used a
morphological and edge based technique to segment
OTs from complex backgrounds. Fu et al. [33] were
able to extract individual Chinese and English
characters with various orientations from images.
Anthimopoulos et al. [34] used an edge-based and
morphological approach, to extract text from news
video footage. In [35], various test images were used
ranging from news to natural images to identify
candidate text regions via corner point detection, and
further OT segmentation refinement via histogram
based color clustering. Work in [36] does individual
character and digit recognition, and digit-object
interpretation to determine the meaning of caption
overlays in basketball and baseball videos.
The aforementioned techniques are relatively
successful in segmenting or extracting videotext.
However, there exist certain limitations.
For techniques relying on embedded or external
text [6, 8, 10, 26, 27], the main issue is availability. In
[6, 26] for instance, CCs are not available in many
countries, so their utilization, even though valuable,
would be limited. And considering the methods in [8,
10, 27], not all soccer matches are properly logged.
Mostly, only popular matches (i.e. Braclay’s Premiere
League, European Cup, Champions League, Spanish
League etc.) are paid attention to, whereas other less
significant competitions and leagues are ignored.
Existing OT-based techniques also have a minor
drawback. Even though OTs are more reliable due their
extensive availability in video, most works only do
extraction, and merely mention character recognition.
Not many have shown how the recognition results are
used for a higher-level purpose. Work in [1] detects
OTs, but no OCR is done to relate recognized
words/phrases with the domain at hand. [12] mention
recognition results, but these were not shown in their
experiments. Other works that have successfully
detected, extracted and recognized OTs are also
presented in [37], but have also not mentioned using
recognition results for higher level processing.
OTs often appear due to some specific reason,
hence, their respective content recognition should be
analyzed to convey the concepts that either have
happened, currently happening, or will happen.
Thus, the focus of our work is twofold. First, we
intend to extract and recognize text from video
segments containing OTs. Then, we will use the
recognition results for concept identification via
domain keyword matching with our soccer termdatabase. Since each video segment containing a
generated OT will convey only one particular concept
at a time, we will be looking for a keyword match
which appears in the OCR results.

well the difference measurements between them. Min
et al. [12] analyzed compressed domain features such
as texture, motion vectors and DC coefficients to
classify tennis video camera shots.
Other works on visual analysis are as in [22], where
sports video is annotated by analyzing a limited set of
visual cues. Here, playfield zone was classified by
Bayesian classifier with edges and their corresponding
orientations as parameters. In [24], texture and edge
intensity were calculated to differentiate between
audience and non-audience regions in soccer. As may
be seen, visual observations can be used for domain
specific concept inferences.
Aural data are also be used. Audio spectrograms
were computed to identify racket hit, crowd cheering
and speech sounds in tennis [25]. Mel-frequency
cepstral coefficients were utilized in [18] to learn seven
classes of golf-sounds (i.e. applause, ball-hit, female
speech, male speech, music, music with speech and
noise). In [20], zero-crossing rate (ZCR) and volume
were input into an SVM for removal of silence. Min et
al. [13] also used ZCR to distinguish between
whistling and non-whistling in soccer. These work
used time-domain features for particular sound
occurrences.
Some of the works mentioned above [12, 18, 20,
24, 25] , and also [7, 15, 26-29] combined features
from different modalities, and reported better concept
inference as compared to using just single modality
analysis.
As can be seen from the above, the visual and/or
aural modalities have been put through extensive
scrutiny. However, another powerful element that has
yet to be fully utilized is text.

2.1. Text-based Analysis
One category of videotext related research is text
extraction and analysis from outside sources.
Babaguchi et al. [6] proposed a method for event
confirmation in American Football videos via
analyzing closed-caption (CC) text. In [8, 10, 27], the
authors extracted text from online game logs, namely
sports logging websites. Text from these resources was
analyzed for the detection of sports events, as well as
other useful concepts.
Another category utilizes text present ‘on’ the
video itself, namely OTs. Since OT is also present in
non-sports video, other domain examples will also be
included for the sake of discussion.
Work in [1, 27] mentioned OTs in sports videos;
the former detected the presence of OT regions
whereas the latter talked about OT character
recognition for video summarization. OT segmentation
were done in [9, 26, 30, 31] for sports score-related

236

white background. Finally, a vertical projection profile
analysis is done to discard unwanted pixel regions that
are less than a calculated threshold. The final binarized
image is as shown in figure 2. We used ABBYY
FineReader 8.0 (http://finereader.abbyy.com) for OCR.
We found that it was sufficient for our purpose.

3. OT Extraction and Recognition
Our applied method is based on the state-of-the-art
as described in [37]. The fundamental steps can be
viewed in figure 1.

Figure 2: Example of a binarized frame.
Figure 1: Fundamental steps.

4. Domain Keyword Matching

We firstly determine the location(s) of OTs for
extraction. This is followed by Optical Character
Recognition before the final step of domain keyword
matching.

The main motivation is to detect a keyword (or
keywords) that match with soccer concept(s) from our
term-database. This database was constructed on an
ad-hoc basis, where it currently contains:

3.1. OT Extraction

1.

OT regions in sports video normally exhibit strong
vertical edges and are horizontally aligned. We apply a
method somewhat similar to [34], only we use a
different edge algorithm, as well as a different
morphological approach. The preprocessing starts with
generating a Sobel edge image. Dilation with a 4x4
square structuring element (SEL) is then applied to
produce clusters of candidate regions. Rough
segmentation is further performed where the cluster
image is binarized and put through morphological
opening (using a 21x5 cross-shaped SEL). This SEL
was chosen due to its ability to generate the tightest
text bounding boxes, while at the same time providing
low numbers of false alarms. Connected-component
analysis is finally done, where bounding boxes
conforming to a minimum and maximum size
constraint are calculated. All of this can be viewed in
figure 3.

2.

Events (e.g. goal, free kick, yellow card, red card,
corner kick, kick-off, foul, substitution etc.),
Game segments (e.g. first half, second-half,
analysis, time added etc.),

The database has not yet been fully populated with
all possible soccer concepts as well as team
information. As this project is ongoing and still in its
primary, this shall be handled in future works.
For optimal OT extraction (and hence better OCR),
some authors (e.g. [31 and 39]) perform multi-frame
averaging or integration in order to remove noise and
to produce a better image for recognition. Our work
however, differs whereby we do not look at averaged
recognition result. Instead, we take a sample of 12frames where each of the samples undergoes the OT
Extraction and Recognition phases (i.e. Section 3)
individually.
This is because; some OT-frames suffer from
stronger background variations and noise as compared
to others, which directly affects the extraction and
recognition results. We have discovered that by
sampling at least 12-frames for each OT-occurring
video segment, and processing them individually, we
are able to get satisfactory keyword(s) recognition for
some of the frames. This somewhat guarantees
keyword(s) recognition.
Each frame’s recognition results are compared with
our soccer-term database for possible keyword
matches. Table 1 shows an example of keyword
matching, which yielded the concept Yellow Card.

3.2. OT Binarization and Optical Character
Recognition
Binarization is achieved with a gray threshold value
derived from Otsu’s [38] method. Additional steps are
done for a better binary image. Firstly, a horizontal
projection profile analysis is done to detect white text
on black background. If the text color is deemed to be
white, the OT-region is inverted. This is necessary as
most OCR software works best with black text on

237

Figure 3: The OT Extraction Process.
S1-S7 (for STAR SPORTS). Each subset represents the
12 frame samples mentioned in section 4. A
description of each subset ground-truth is as follows:

Table 1: Example of OCR results and database
matching.
Frame
1
2

3

…
11

12

Recognition Results
VELLOW.CARD
K*U I WES BROWN
tfE YELLOW CARD
ff*U I WES BROWN
IMANUTO)
YELLOW CARD
X+1E I WES
BROWN IMANUTDI
…
YELLOW CARD
X+l\ I WES BROWN
(MAN UTDI
YELLOW CARD
ff*H I WES BROWN
1MANUTD1

Matches
-

1.
YELLOW CARD

2.

YELLOW CARD

…
YELLOW CARD

ESPN subsets:
a. E1, E2 and E3: Yellow Card,
b. E4 and E5: Substitution,
c. E6: Half-time,
STAR SPORTS subsets:
a. S1 and S2: Yellow Card,
b. S3 and S4: Substitution,
c. S5: Time Added,
d. S6: Full-time
e. S7: Commentator.

5.1. OT Extraction

YELLOW CARD

We firstly evaluated the extraction algorithm on the
number of overlapping pixels between the detected text
bounding boxes and the ground truth text bounding
boxes. We are interested to know whether the
extraction process yields satisfactory bounding box
regions, which at least overlap a significant portion of
the ground truth text bounding boxes. For this, we
define Precision, p (1) and Recall, r (2):

At this stage, we are only doing simple keyword
matching. This means, we are looking for only one
keyword match. When a particular match is found, the
search stops. An extension of this work will look for
more than one keyword, and hope to be able to
establish dependable relationships between them.

‫ ݌‬ൌ σே
௜ୀଵ ቀ

5. Experimental Results
We tested on 2-sets of recorded programs from
ESPN and STAR SPORTS. Both were chosen due to
two different OT properties: (1) Different typefaces,
font sizes and color, and (2) Different intensity levels
for OT-background. Each set is divided into 6 and 7
subsets respectively, denoted by E1-E6 (for ESPN) and

‫ ݎ‬ൌ σே
௜ୀଵ ቀ

஽೔ ‫ீת‬
஽೔

஽೔ ‫ீת‬
ீ

ቁ

(1)

ቁ

(2)

Di is the detected text bounding boxes pixels for
frame-i, whereas G is the ground truth (GT) text
bounding box pixels. N is the total number of frames
processed for each set, which as mentioned is equal to

238

12. Both Di and G are converted into binary-maps to
calculate the overlaps using the above formulas. The
overall results are in tables 2 and 3. From here, p seems
to suffer. This is due to the number of false alarms
detected. However, r is very promising. This is
preferable because although false alarms exist, the
actual GT text bounding boxes are still accounted for.
This is important, as they are compulsory for the
process of binarization and recognition.

Table 5: Results for STARSPORTS.
STARSPORTS

Table 2: p and r for ESPN.
Frame
# of GT
bounding
boxes
p (%)
r (%)

E1

E2

E3

E4

E5

E6

36

36

36

72

48

60

56
91

52
92

59
91

60
92

60
90

60
91

# of GT
bounding
boxes
p (%)
r (%)

S1

S2

S3

S4

S5

S6

S7

48

48

48

48

36

48

36

69
91

66
90

62
91

61
92

69
85

69
91

66
92

In all, we have presented a framework that can
identify occurrences of simple concepts in video
segments containing OTs. This is done via matching
the OCR results from OT with an ad-hoc soccer-term
database. Experimental results have shown that for
each video segment containing an OT occurrence, a
specific concept pertaining to soccer was successfully
matched. This is in line with the observation that only
one particular concept is conveyed with every OT
being generated. We however believe that more can be
done to improve this technique. Furthermore, our work
is only in its primary, and future improvements that
will be deeply considered are:

Although we have high r, this does not necessarily
translate to good recognition. This is because some
detected text bounding boxes are bigger than the GT,
resulting in noisy binarization, which in the end leads
to poor character recognition.

Figure 4: a) Oversized OT text-box, b) Poor
binarization, and c) Poor recognition.

1.

We have found that, by sampling at least 12-frames,
recognition results could be better guaranteed. This is
based on tests and observations, whereby 12-frames
seemed to be the optimal number. Success is
determined by finding at least one match between
OCR-results and a particular keyword (or keywords)
from our term-database. If none is found, this is
deemed as Failure. The results are shown below.

2.

Table 4: Results for ESPN.
E1
E2
E3
E4
E5
E6

# of frames with OCR &
Keyword Match
4
9
4
8
10
10

12

6. Conclusion

5.2. OT Recognition and Keyword Matching

ESPN

S7

Keywords
Matched
Yellow Card
Yellow Card
Substitution
Substitution
Time added
Full-time OR
Full time
Commentator

From the results, it is clear that all sets generated
successful matching. For sets that exhibit low number
of matches (i.e. 4 and below), this was due to relatively
poor OT extraction and recognition, mainly caused by
strong variation in background pixels. For the higher
number of matches, most of the frames had consistent
backgrounds with only minor changes in intensity,
color and motion.

Table 3: p and r for STAR SPORTS.
Frame

S1
S2
S3
S4
S5
S6

# of frames with OCR
& Keyword Match
12
12
5
12
12
12

Keywords
Matched
Yellow Card
Yellow Card
Yellow Card
Substitution
Substitution
Half-time OR
Half time

3.

239

Improved OT segmentation and extraction: This
can provide optimal character recognition results.
Homogenous color properties of text and multiframe integration can be manipulated for this.
Extensive Database Population and Establish
Relationships between concepts: For this paper,
we merely do simple keyword matching. This is
because we are working on the premise that each
OT conveys only one domain concept when it
appears. However, we observed that when more
than one keyword appears in an OT, there exists a
relationship with other keywords. For instance, the
word Substitution will be followed by a Player
name. This relationship can be created to generate
higher level meaning (i.e. who is being substituted
during the substitution event?).
Visual Analysis for Concept Relationship
Inference: We observed videos containing OTs,

4.

5.

[9] Dongqing, Z. and C. Shih-Fu, Event detection in
baseball video using superimposed caption recognition,
in Proceedings of the tenth ACM international
conference on Multimedia. 2002, ACM: Juan-les-Pins,
France.

but with no supporting keywords. For instance,
some OTs display player name and a yellow
rectangular region to convey a Yellow Card. Since
the ‘yellow card’ keyword is not present, a
possible relationship can possibly be made
between the player and the yellow rectangular
region to identify this concept.
Concept boundary determination: Multimodal
analysis can be applied for the detection of the
start and ending points of a concept segment.
Semantic labeling: Once concept boundaries have
been established, a proper index will be built.

[10] Huaxin, X. and C. Tat-Seng, Fusion of AV features and
external information sources for event detection in team
sports video. ACM Trans. Multimedia Comput.
Commun. Appl., 2006. 2(1): p. 44-67.
[11] Liu, H.-Y., T. He, and H. Zhang. Event Detection in
Sports Video Based on Multiple Feature Fusion. in
Fuzzy Systems and Knowledge Discovery, 2007. FSKD
2007. Fourth International Conference on. 2007.

References
[1] Kobla, V., D. Dementhon, and D. Doermann.
Identifying sports videos using replay, text, and camera
motion features. in Proc SPIE. 2000.

[12] Min, X., et al. A fusion scheme of visual and auditory
modalities for event detection in sports video. in
Multimedia and Expo, 2003. ICME '03. Proceedings.
2003 International Conference on. 2003.

[2] Tsai, T.-H., Y.-C. Chen, and C.-L. Fang. A
Comprehensive Motion Videotext Detection
Localization and Extraction Method. in Data
Engineering Workshop, 2007 IEEE 23rd International
Conference on. 2007.

[13] Min, X., et al. Creating audio keywords for event
detection in soccer video. in Multimedia and Expo,
2003. ICME '03. Proceedings. 2003 International
Conference on. 2003.

[3] Efros, A.A., et al. Recognizing action at a distance. in
Computer Vision, 2003. Proceedings. Ninth IEEE
International Conference on. 2003.

[14] Sadlier, D.A. and N.E. O'Connor, Event detection in
field sports video using audio-visual features and a
support vector Machine. Circuits and Systems for Video
Technology, IEEE Transactions on, 2005. 15(10): p.
1225-1233.

[4] Guangyu, Z., et al. Action Recognition in Broadcast
Tennis Video. in Pattern Recognition, 2006. ICPR 2006.
18th International Conference on. 2006.

[15] Snoek, C.G.M. and M. Worring. Time interval
maximum entropy based event indexing in soccer video.
in Multimedia and Expo, 2003. ICME '03. Proceedings.
2003 International Conference on. 2003.

[5] Babaguchi, N. and R. Jain. Event detection from
continuous media. in Pattern Recognition, 1998.
Proceedings. Fourteenth International Conference on.
1998.

[16] Tovinkere, V. and R.J. Qian. Detecting semantic events
in soccer games: towards a complete solution. in
Multimedia and Expo, 2001. ICME 2001. IEEE
International Conference on. 2001.

[6] Babaguchi, N., Y. Kawai, and T. Kitahashi, Event based
indexing of broadcasted sports video by intermodal
collaboration. Multimedia, IEEE Transactions on, 2002.
4(1): p. 68-75.

[17] Wen-Nung, L. and S. Sheng-Hsiung. Combining
Caption and Visual Features for Semantic Event
Classification of Baseball Video. in Multimedia and
Expo, 2005. ICME 2005. IEEE International
Conference on. 2005.

[7] Bai, L., et al. A Semantic Event Detection Approach for
Soccer Video based on Perception Concepts and Finiste
State Machines. in Image Analysis for Multimedia
Interactive Services, 2007. WIAMIS '07. Eighth
International Workshop on. 2007.

[18] Xiong, Z., et al., Audio-Visual Event Recognition with
Application in Sports Video, in Intelligent Multimedia
Processing with Soft Computing. 2005. p. 129-149.

[8] Changsheng, X., et al., Live sports event detection based
on broadcast video and web-casting text, in
Proceedings of the 14th annual ACM international
conference on Multimedia. 2006, ACM: Santa Barbara,
CA, USA.

[19] Zhang, Y., et al. Semantic Event Extraction from
Basketball Games using Multi-Modal Analysis. in
Multimedia and Expo, 2007 IEEE International
Conference on. 2007.

240

[20] Zhenyan, L. and T. Yap-Peng. Event detection using
multimodal feature analysis. in Circuits and Systems,
2005. ISCAS 2005. IEEE International Symposium on.
2005.

Pattern Recognition, 2006. ICPR 2006. 18th
International Conference on. 2006.
[32] Basavaraj, A. and P. Nagaraj. Video Text Extraction
from Images for Character Recognition. in Electrical
and Computer Engineering, 2006. CCECE '06.
Canadian Conference on. 2006.

[21] Kaabneh, K., et al. Video Segmentation Via Dual Shot
Boundary Detection (DSBD). in Information and
Communication Technologies, 2006. ICTTA '06. 2nd.
2006.

[33] Fu, H., X. Liu, and Y. Jia, Maximum-Minimum
Similarity Training for Text Extraction, in Neural
Information Processing. 2006. p. 268-277.

[22] Bertini, M., A. Del Bimbo, and W. Nunziati, Highlights
modeling and detection in sports videos. Pattern
Analysis & Applications, 2004. 7(4): p. 411-421.

[34] Anthimopoulos, M., et al., Detecting Text in Video
Frames, in The 4th International Conference on Signal
Processing, Pattern Recognition and Applications
(SPPRA). 2007, IASTED: Innsbruck, Austria. p. 40-44.

[23] Ekin, A., A.M. Tekalp, and R. Mehrotra, Automatic
soccer video analysis and summarization. Image
Processing, IEEE Transactions on, 2003. 12(7): p. 796807.

[35] Guo, G., et al. Automatic Video Text Localization and
Recognition. in Image and Graphics, 2007. ICIG 2007.
Fourth International Conference on. 2007.

[24] Chung-Lin, H., S. Huang-Chia, and C. Chung-Yuan,
Semantic analysis of soccer video using dynamic
Bayesian network. Multimedia, IEEE Transactions on,
2006. 8(4): p. 749-760.

[36] Huang-Chia, S. and H. Chung-Lin. A Robust
Superimposed Caption Box Content Understanding for
Sports Videos. in Multimedia, 2006. ISM'06. Eighth
IEEE International Symposium on. 2006.

[25] Dahyot, R., et al. Joint audio visual retrieval for tennis
broadcasts. in Acoustics, Speech, and Signal
Processing, 2003. Proceedings. (ICASSP '03). 2003
IEEE International Conference on. 2003.

[37] Jung, K., K. In Kim, and A. K. Jain, Text information
extraction in images and video: a survey. Pattern
Recognition, 2004. 37(5): p. 977-997.

[26] Babaguchi, N. and N. Nitta. Intermodal collaboration: a
strategy for semantic content analysis for broadcasted
sports video. in Image Processing, 2003. ICIP 2003.
Proceedings. 2003 International Conference on. 2003.

[38] Otsu, N., A thresholding selection method from graylevel histogram. IEEE Transactions on Systems, Man,
and Cybernetics, 1979, 1979.
[39] Sato, T., et al., Video OCR: indexing digital news
libraries by recognition of superimposed captions.
Multimedia Systems, 1999. 7(5): p. 385-395.

[27] Changsheng, X., et al. Sports video personalization for
consumer products. in Consumer Electronics, 2006.
ICCE '06. 2006 Digest of Technical Papers.
International Conference on. 2006.
[28] Snoek, C.G.M., The Authoring Metaphor to Machine
Understanding of Multimedia. 2005, PhD Thesis,
University of Amsterdam.
[29] Yu Lin, K., et al. Visual keywords labeling in soccer
video. in Pattern Recognition, 2004. ICPR 2004.
Proceedings of the 17th International Conference on.
2004.
[30] Dongqing, Z., R.K. Rajendran, and C. Shih-Fu. General
and domain-specific techniques for detecting and
recognizing superimposed text in video. in Image
Processing. 2002. Proceedings. 2002 International
Conference on. 2002.
[31] Yih-Ming, S. and H. Chaur-Heh. A Novel Caption
Extraction Scheme for Various Sports Captions. in

241

