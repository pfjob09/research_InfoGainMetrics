Local Moment Invariant Analysis
Yufeng Chen Mandun Zhang Peng Lu Yangsheng Wang∗
Institute of Automation, Chinese Academy of Sciences
Beijing China, 100080
email:{yfchen mdzhang plu wys}@mail.pattek.com.cn

Abstract
A new Local Moment Invariant Analysis (LMIA) method
of images is proposed in this paper. Considering the background contrast, light condition and the camera parameters, the proposed method achieves the invariant under
main imaging effect based on the model of the feature image
transformation. Our method is specially designed not only
for geometric properties, compared with the other kind of
moments, and also more describable than some of the simple feature detectors. Also an efﬁcient local moment computing algorithm is brought forward to deal with the large
amount of computation, that can search the image almost
in real-time. Experimental results show that the proposed
approach is effective and suitable for the local feature detection.

1. Introduction
Image feature detection and analysis are the fundamental
problems in computer vision. Many works have been done
on this area[1, 2], especially on invariance feature. Moments based method is one of the well-known methods to
deal with the geometric invariant of the objects.
The typical invariant moments are Hu Moments[3],
which are derived from the traditional geometric moments
formed using a monomial basis. Although the basis are
sufﬁcient to reconstruction the image completely, they increase rapidly as the order increases, producing highly correlated descriptions, and the noise become sensitive for the
high order moments. An improvement is to make the basis orthogonal with respect to summation. Legendre[4]
and Zernike[5] are the two well established moment among
many related methods.
Many methods have been proposed to improve the performance, Heikkila[6] tried to extend the moment to afﬁne
invariant, Chong[7] researches on the transformation invari∗ This

ant of the Zernike moments, and Shen[8] use wavelet moment invariant to differentiate the similar objects with subtle
differences.
However, all the moments mentioned above must treat
the object as a whole, which means the model should be
rigid and well segmented or featured without occlusion, this
requirement is not always meet. Takamatsu[9] tried to use
local moment recognize the parts of the object as an improvement, but more work related with local properties is
still needed.
Based on the previous works mentioned above, we proposed the novel LMIA method as well as its fast algorithms,
which use a linear imaging model to describe the difference
appearance of the object and there fore retrieved the invariant.
Compared with the previous method, our method is superior in two aspect, ﬁrstly the feature is locally detected
in a efﬁcient way, that endure the distortion of the object in
large scales; secondly, it’s invariance under different background, lighting condition and camera parameters.
The remainder of this paper is organized as follows: In
section 2, The LMIA is introduced. The fast local moment
algorithm is formulates In section 3 . In the section 4, the
comparison of the two moments is carried out based on the
properties of the moments. The experiment is showed in
the next section. To end this, the method is summarized and
some more applications are expected.

2. Invariant Analysis
Before further discuss of the feature invariance, the image model from real objects should be ﬁrst considered.
Given the perspective transformation, the light amount from
a certain view point, which will be integrated by the camera
sensors to form the pixel of the images, is depend on three
main factors [10] : the normal of the surface N , the lighting condition L and the albedo a. So that the image can be
derived

work is supported by National 863 project.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

I(x, y) = C(a(x, y)N (x, y)L(x, y))

(1)

Where C(·) is the integration transform of the camera sensor, which is always considered as linear integration if the
exposure time is suitable.
From the model Fun.1 of images, we can see clearly that
images does not uniquely determined by the feature. They
also depend on the lighting condition and camera system
which is described by the surface normal N and the albedo
a. Fortunately the camera system can be simpliﬁed as a linear integration and the light condition could be viewed as
a combination of many point light source that is also linear both in amount and their distribution, thus the feature
invariant is formed as follows:
Given a standard feature fˆ(x, y) that depending only
on the object properties, a real image f (x, y) supposed to
be affected by a local multiplicative transformation, which
is corresponded with the different contrast, and a linear
spaced addictive transform, which is introduced by the
lighting conditions. The model is then give as follows:
= k × fˆ(x, y) + a + bx + cy

f (x, y)

(2)

To eliminate the effect of the transformation, let
f˜(x, y)

=

mp,q

=

x0

yd

x y f (x, y)dxdy
p q

upq

m0,0

= k×m
ˆ 0,0 + aka

m1,0
m0,1

= k×m
ˆ 1,0 + bkb
= k×m
ˆ 0,1 + ckc

where,

kc

=
=
dx =

xd

yd

x0
xd

y0
yd

x0
xd

y0
yd

x0

y0

xd −x0
2

xd

=

yd

xp y q f˜(x, y)dxdy

y0

mp,q − m0,0
−m1,0

xd
x0

xd
x0

yd
y0

xd
x0

(3)

(4)

The moment can be simpliﬁed largely

kb

=

yd
y0

xp y q dxdy

xd yd
x0 y0
p+1 q

x

dxdy

y dxdy

yd
y0

x2 dxdy

− m0,1

xd
x0

yd
y0

xd
x0

xp y q+1 dxdy
yd
y0

y 2 dxdy

y0

x0 + xd = 0, y0 + yd = 0

=

k{fˆ(x, y) − ka−1 m
ˆ 0,0 − kb−1 m
ˆ 1,0 − kc−1 m
ˆ 0,1 }(5)

x0

Where the f (x, y) is the two dimensional function and
x0 , xd , y0 , yd are the border of the rectangle window, and
p, q stand for the moment orders.
If the equation(4) is meet,

ka

m0,0
m1,0
m0,1
−
x−
y
ka
kb
kc

Here f˜(x, y) is k times of the transformed feature expression of fˆ(x, y) which has been affected by an addictive
plane depending on the function itself. The parameter k can
be any real number, even negative, which means the contrast
is trivial to the shape analysis. Therefore the invariance of
the feature fˆ(x, y) can be derived by normalize its moments
vectors.
Then the invariant feature is derived from the invariant
image f˜(x, y), which is expressed with the original image
moments.

where, k, a, b, c stand for the unknown factor of the transformation.
Here we use the common Cartesian moment(3) for simple explanation of the method.
xd

= f (x, y) −

dxdy = dx dy
x3 − x30
dy
x2 dxdy = d
3
y 2 dxdy = dx

dy =

This moment is also a common Cartesian moment but
performed on the retrieved invariant image, thus can be easily normalized and transformed into the other moments.

3. Fast Algorithm of local invariant moments
Although the moments based method is widely used, the
local feature detection is seldom implemented. The main
obstacle is the calculation complexity of the moments, since
every image should be calculated for each local window.
Fortunately, a large amount of the computation could be
saved to precalculate the integration result from every point
to the original, and then the integration in any window could
be retrieved by several times of additions. The entire algorithm is listed as follows including the centering and normalization.
npq

yd3 − y03
3

yd −y0
2

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

=
=

1
dx dy

xd
x0

yd

(

y0
q
p

x − x¯ p y − y¯ q
) (
) f (x, y)dxdy
dx
dy

1
(Cpi Cqj (−¯
x)(p−i) (−¯
y)(q−j) mij )
p+1 q+1
dx dy i=0 j=0
where, x¯ =

xd +x0
2

y¯ =

yd +y0
2

Let I(x, y) be the image of the object, the precalculated
integration are
i

Fp (i, j) =

I(x, j)xp
x=0
j

Fp,q (i, j) =

Fp (i, y)y q
y=0

There fore the moment of any order in a given rectangle
window is easy to obtain.
Gp,q (x0 , y0 , xd , yd )

= Fp,q (xd , yd ) − Fp,q (x0 , yd )

According to the equation5 the invariant moment from different image of the same object should be proportional to
each other. Then the contrast normalized moment becomes
mp,q
,p + q > 1
m
ˆ p,q =
||M ||2
Where ||M ||2 is the moments l2 norm.
1
Then, we give the weights w(p, q) = p!q!
of each moment based on their factors in the image reconstruction
equation 6.
At last the distance of each moment vectors are statistic
derived by the weighted KL[12] method:

−Fp,q (xd , y0 ) + Fp,q (x0 , y0 )

Dx2 (ma, mb) =

And further more, the mass center and the center normalized moments are derived as follows.
¯ =
X

mp,q

=

G1,0
G0,0

Y¯ =

−q
d−p
x dy

p,q

ˆ pq ]2
[ma
ˆ pq − mb
ˆ pq
ma
ˆ pq + mb

where, 1 < p + q < N

G0,1
G0,0

p

w(p, q)

q

(Cpi Cqj
i=0 j=0

¯ (p−i) (−Y¯ )(q−j) )Gi,j
(−X)

4. Feature detection
(a) Standard sphere surface

(b) Invariant image of (a)

(c) Transformed (a)

(d) Invariant image of (c)

Theoretically any image could be expressed precisely
by the moment[11],since the frequency analysis can be expressed as
∞

∞

M xy (u, v) =
p=0 q=0

up v q
mpq
p! q!

(6)

However it’s no use computing the high order moment
because of their heavy computation load and sensitiveness
to the noise. For most simple shapes, especially the local features, ﬁrst three order moment of each dimension is
enough to differentiate them. But how to deal with these
moments is not trivial.
First of all we will discuss the physical meaning of each
moment, taking the advantage of the invariant de-correlate
of the moments. As the moment m0,0 is the direct integration of the image, it stands for the average value of the
image, which is useless for shape recognition in most cases.
The ﬁrst moments m0,1 , m1,0 are the mass center of each
direction, which are coming from two sources, the ﬁrst is
the linear difference from the object surface, the other is
coming from the linear parts of the lighting conditions. So
that the full use of the ﬁrst order moment is depending on
the lighting environment.
The higher order moment, which is coming from the invariant image, will give a detailed description of the feature.

Figure 1. The invariant images of standard
sphere and its transformed version.

5. Experiments
The synthesize image of the ball (See Fig1.a) and its
transformed image(See Fig1.c) are used to show the moment invariant. Their invariant images are listed at the right
columns of the ﬁg1. The comparison of the common moment and our invariant moment is show in the table1.
It can be seen clearly that the invariant image are successfully retrieved and their moment is proportional to each
others to the large extent. On the other side the Cartesian
moment will have large difference when the lighting condition changed.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

Table 1. Moments of the test images
Capital letter C. stand for the Cartesian moment and I.
stand for the invariant moment by our method. The lower
case stand for the ﬁgure in the Fig1. To save the space the
unit is set to 10−4
M m0,2 m1,1 m2,0 m0,3 m1,2 m2,1 m3,0
C.a

728

0

728

1

0

0

1

C.c

657

0

657

28

8

15

14

I.a

31

0

31

0

0

0

0

I.c

10

0

10

0

0

0

0

Another real experiment has been implemented on the
tracking system, ﬁrstly many ﬁnger tip images are collected
and a model is learned to express the shape LMIA moment
of the ﬁnger tips, the real image in Fig.2a is tested to detect the ﬁnger, and the result is show in the Fig2.b. At last
the method is tested in a virtual drawing system Fig.2c,d to
tracking the ﬁnger in real time. About 10000 features could
be tested within a second on the PC of 1.7GHZ. The experiment shows it an efﬁcient method to detect the simple shape
features.

method is efﬁcient and invariant under geometry, lighting
transforms, thus it is suitable for local feature searching to
detect parts of nonrigid objects. More other related applications, and more researches including shape recognition are
also expected to be done on this method.

References
[1] K. Kira and L.A. Rendell. The feature selection problem:traditional methods and a new algorithm. In 10th
National Conference on Machine Intelligence, 1992.
[2] J. Shi and C. Tomasi. Good features to track. In IEEE
Conference on Computer Vision and Pattern Recognition, Seattle, June 1994.
[3] M-K. Hu. Visual pattern recognition by moment invariants. IRE Transaction on Information Theory, IT8:179–187, 1962.
[4] M. R. Teague. Image analysis via the general theory
of moment. Journal of the Optical Society of America,
70(8):920–930, 1979.
[5] F. Zernike. Diffraction theory of the cut procedure and
its improved form, the phase contrast method. Physica, (1):689–704, 1934.
[6] J. Heikkila. Pattern matching with afﬁne moment descriptors. Pattern Recognition, 37:1825–1834, 2004.

(a)

(b)

[7] C. Chong, P. Raveendranb, and R. Mukundanc. Translation invariants of zernike moments. Pattern Recognition, 36:1765–1773, 2003.
[8] D. Shen and H.S. Horace. Discriminative wavelet
shape descriptors for recognition of 2-d patterns. Pattern Recognition, 32:151–165, 1999.

(c)

(d)

Figure 2. The experiment of ﬁnger tip tracking.(a) Real image with ﬁnger;(b) The distribution of probability;(c)(d) Virtual drawing:
tracking with this method

6. Conclusion
A novel imaging invariant moment is proposed and the
fast local moment algorithm is put forward as well. The

[9] R. Takamatsu, M. Sato, and H. Kawarada. Pointing
device gazing at hand based on local moments. In Proceedings of SPIE, volume 3028 of Real-Time Imaging
II, pages 155–163, April 1997.
[10] Woodham R.J. Photometry method for determining
surface orientation from multiple images. Optical Engineering, 19(1):139–144, 1980.
[11] M. Pawlak. On the reconstruction aspects of moment descriptors. IEEE Trans. on Information Theory,
38(6):1698–1708, 1992.
[12] J.R.Mathiassen, A.Skavhaug, and K.Bo. Testure similarity measure using kullback-leiber divergence between gamma distributions. In European Conference
on Computer Vision, pages 133–147, 2002.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

