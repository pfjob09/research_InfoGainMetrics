Pattern Recognition Based Color Transfer
Jie Ying, Liang Ji (contact author)
Dept. of Automation, Tsinghua University, Beijing (100084), P. R. China
zc-sa@mail.tsinghua.edu.cn

Abstract
Color transfer is a recently proposed method for
colorizing grayscale images. It transmits the
appropriate chromatic “style” of a reference source
color image to a target gray image. Compared with
pseudocolor methods, color transfer provides more
objective results. In this paper, the original color
transfer method is reviewed briefly, and analyzed from
the perspective of pattern recognition. In order to
improve the quality of the colorized images and the
computation efficiency of the colorizing process,
several powerful techniques are combined to the
original color transfer algorithm, including texture
feature, sample reduction, median k-nearest-neighbor
rule, KD tree and median filter. Experiments show that
the new algorithm works well.

1. Introduction
Color images are usually more appealing and more
informative than corresponding gray scale images. So
transferring a grayscale image to a color one by adding
additional chromatic components is indeed very
attractive. Pseudocolor algorithm [1, 2] is just
following this idea. However the color map used in
pseudocolor is irrelevant to the contents of the gray
image except for the luminance. In contrast, color
transfer [3] shows us a way to learn the objective
chromatic distribution of a referential color image, and
then transfers its color components to a gray image.
Based on Erik Reinhard etc.’s work [4], Tomihisa
Welsh etc. proposed the first operable color transfer
method [3]. Welsh’s algorithm can transfer colors from
a source color image to a target gray image by
matching
their
luminance
distribution
and
neighborhood statistics. The resultant colored image
has similar color style to the source color image and
maintains the original luminance information. A sketch
of Welsh’s color transfer method is illustrated by
figure 1.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

Figure 1. Sketch of Welsh’s method
The transfer process contains the following steps.
Firstly, both the source and target images are converted
into the lαβ color space [4, 5] (l is the luminance
channel and αβ correspond to the chromatic
components), which minimizes correlation among the
three channels in color space. Secondly the luminance
remapping [6] is processed to decrease the global
difference in luminance distribution between the two
images. Next, a small subset of pixels (200
approximately) is selected randomly from the source
color image as sample set. Finally, for each pixel in the
target gray image a best matching sample is chosen
from the sample set, then the α and β of the best
matching sample are transferred to the gray pixel but
the original luminance is retained. The best matching is
determined by minimizing a weighted distance
between the two pixels, and the distance is calculated
from the corresponding luminance and neighborhood
statistics. Welsh etc. used the standard deviation of the
luminance of the neighboring pixels as neighborhood
statistics [3].
For further improvement swatches [3] are
introduced. A swatch is a region selected from an
image by user interaction. The swatch selected from
the target gray image and the corresponding swatch
selected from the source image should have similar
contents. Between the corresponding swatches colors

are transferred by the general method described above,
and the remaining pixels of the target gray image are
then colorized referencing the colorized swatches in
the target image.
Welsh’s color transfer algorithm is general, simple,
and effective for a large class of images, whereas it still
has some drawbacks in error control and computational
complexity. First of all, Welsh’s method cannot
suppress the noise points and wrong hues brought in by
the nearest-neighbor matching. The “bad colorized”
pixels spoil the resultant colored image ineluctably.
What is more, the brute force algorithm used in the
process of finding the best matching sample incurs
linear time costs to the size of the sample set, and the
computational complexity is high.
In order to remedy these shortcomings a pattern
recognition based color transfer method is proposed in
this paper. The detail is described in section 2. To
demonstrate the effectiveness of the proposed method
some experiments were conducted, and the results are
presented in section 3, which show that the new
algorithm can not only enhance the quality of the
resultant images but also reduce the computational
complexity. Conclusion and discussion are given in
section 4.

2. Pattern recognition based algorithm
Color transfer is based on a hypothesis that in
similar situation, similar objects usually have similar
luminance, neighborhood statistics and color. Based on
this simplified and idealized hypothesis, we can infer
that two gray pixels should have similar color if they
have similar luminance and neighborhood statistics
such as standard deviation etc.
The idea of color transfer method is very similar to
the nearest-neighbor rule [7] in pattern recognition,
which says that if feature vectors can describe the
specialty of samples, the test sample’s class label
should be assigned to the label of its nearest sample in
the feature space. Correspondingly, in the case of color
transfer the class label of the sample corresponds to α
and β, and the feature vector contains the luminance
and the neighborhood statistics.

2.1 The basic operations
The general algorithm of pattern recognition based
color transfer consists of the following six steps. The
first two steps are for decorrelating color space and
normalization. Step 3 is for sample set selection. The
last three steps are for assigning color and noise
reduction.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

1) Using lαβ space
Both the source color image and the target gray
image are converted into the lαβ color space.
2) Luminance remapping
The luminance remapping is processed on the
luminance channel of the source color image. We
linearly shift and scale the luminance histogram of the
source color image to fit the histogram of the target
image [6]. This process decreases the global difference
of luminance distribution between the color image and
the gray one.
3) Reducing sample set
Most of the errors introduced in the nearestneighbor matching are caused by outliers, which have
similar feature vectors but difference colors to the
nearest sample. In pattern recognition, sample
reduction [8] is an effective way to reduce the errors.
Fortunately, it also works well in color transfer. Firstly
a small subset of pixels (about 600 pixels) is selected
randomly from the source color image as original
sample set. Then sample reduction is processed,
reducing both the outliers and the size of the sample set
(approximately 200~300 pixels after sample reduction)
[see section 2.3].
4) Finding the best matching sample
For each pixel in the target gray image a best
matching sample is chosen from the sample set
selected in step 3. The best matching is determined by
minimizing a weighted distance between the feature
vectors of two pixels. Following Welsh’s method, we
n

also use weighted chessboard distance

¦ω

i

M i − Ni

i =1

to measure the distance, where (ω1, ω 2 , " , ω n ) is the

weight vector, (M 1 , M 2 ," , M n ) and (N1 , N 2 , " , N n )
are feature vectors of the pixels, and n is the dimension
of the feature vector. The feature vector used in this
paper is three dimensional, including luminance l,
standard deviation of the luminance of the neighboring
pixels and texture entropy [see section 2.2].
In order to reduce the computational complexity, we
use KD tree [9, 10] [see section 2.5] in finding the best
matching sample, which reduces the time complexity
from linear time to logarithmic time.
For reducing the influence of the outliers further,
median k-nearest-neighbor rule is used instead of
nearest-neighbor rule used by Welsh to find the best
matching sample. A gray pixel’s k nearest samples are
found first rather than just find the nearest one, and
then the median one is selected as the best matching
sample.
5) Assigning color components
The α and β of the best matching sample are set to
the gray pixel and the original luminance is retained.

6) Reducing spatial noise
Median filter is applied to the color channels of the
resultant colored image and the luminance channel is
reserved. In this way, most of the granular color noises
can be wiped off.
In order to combine user interaction into our
method, we also use the swatches introduced by
Welsh. Firstly, between the corresponding swatches we
use the step 1~ step 5 described above to transfer color.
Then for the remaining pixels of the target gray image
we no longer find the best matching sample within the
source color image but within the colorized swatches
in the target image. Finally step 6 is processed on the
whole colorized image.

2.2 An additional texture feature
In image processing the co-occurrence matrix based
texture feature is proved very effective [11]. The cooccurrence matrix P(i, j , d ,θ ) of an image can be
calculated when a direction θ and a distance d have
been chosen. The variable d is the distance between a
pixel pair whose gray values are i and j respectively,
and θ is the angle between the joint line of the pixel
pair and the horizontal axis. The co-occurrence matrix
is L × L in size, where L is the number of the gray
levels of the image and each element of the matrix is
the probability of the occurrence of (i, j ) given d and

θ . In lαβ color space, the luminance l is a continuous
variable, which distributes from –4 to 10, so we
quantize the luminance l before using it by the
following formula.
if (l < 0)
new luminance level = 0
else
new luminance level = truncation (l ×10)

(1)

Now the luminance of each pixel is changed to the
0th~ 99th level corresponding l from -4 to 10, and then
a symmetric co-occurrence matrix can be defined by
formula 2, which are 100 × 100 in size.

P (i , j , d ) =

1
4

3

¦ P (i , j , d ,
k =0

kπ
)
2

(2)

Based on P (i, j , d ) , we can compute the texture
entropy [1] as a supplement of the features in color
transfer, and the entropy is defined below.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

L

entropy = −

L

¦¦ P (i, j, d ) log(P (i, j, d ))

(3)

i =1 j =1

2.3 Outliers and sample reduction
When the original sample set is selected from the
source image, some outliers will be included
inevitably. For example, a plot of grass may be
smooth, so its neighborhood standard deviation will be
very small. In this instance the samples of grass may
be mixed with the samples of sky in feature space, and
then some gray pixels in the region of sky may be dyed
green by using nearest neighbor matching.
The edit-nearest-neighbor method [8] can be used to
solve this problem. Firstly the original sample set can
be divided into two sets randomly, that is set M and set
N. Then set M can be reduced by set N by the
following operations. For each sample in set M, we
find the best matching color αbest match and βbest match in
set N using median k-nearest-neighbor rule [see section
2.4]. Then we compare αbest match and βbest match with the
sample’s real color components αsample and βsample. If
the difference between them is greater than a threshold
(30% in our method), the sample in question is an
outlier, and it will be removed from set M. When all
the samples in set M have been checked a new set M’
is obtained. Similarly we can get a new set N’ from set
N by referencing set M. Finally, we obtain a modified
sample set by mixing M’ and N’.

2.4 Median k-nearest-neighbor rule
The sample reduction removes most of the outliers,
but some of them still remain. In order to remedy the
outliers’ influence further, a median k-nearest-neighbor
rule is introduced. Similar to the k-nearest-neighbor
rule [7, 12], firstly we find the gray pixel’s k nearest
neighbors in the sample set, which can be assigned as
S1, S2, … , Sk. Their α components are then sorted and
αmedian is the median. Similarly, βmedian is the median
obtained by sorting their β components. Then the
distances between point (αmedian, βmedian) and (α1, β1),
(α2, β2), … , (αk, βk) are computed. If (αi, βi) is the
nearest to point (αmedian, βmedian), we then transfer αi
and βi to the gray pixel.
This proposed rule works well. For example, if
there are one or two outliers in a gray sample’s seven
nearest samples, the median k-nearest-neighbor rule
can still find out an appropriate color. Empirically, k
should be small compared to the size of the sample set.
We find that the limitation of less than 11 is
appropriate for most of the images.

2.5 KD tree

3. Results

The trivial nearest-neighbor algorithm must scan the
entire sample set and has the time complexity of
Ο(N ) , where N is the size of the sample set. So
selecting a more efficient technique to accelerate the
searching process is very important.
KD tree is a multidimensional binary tree widely
used in computational geometry. It subdivides the
feature space with arbitrary axis-aligned splitting
plane, and utilizes “divide and conquer” strategy to
accelerate the nearest-neighbor searching. When the
sample set is arranged by KD tree, only a small subset
of the samples needs to be scanned in nearest
searching, and the time complexity becomes
logarithmic. A comparison is illustrated in figure 2.
The target gray image was 640 by 512 pixels in size,
and the features used were luminance and standard
deviation. Only the time cost in nearest searching was
taken into account. The program was run on a PC with
Pentium IV 1.6Ghz CPU and 256 KB RAM.

Figure 3.1 ~ figure 3.6 illustrate a direct comparison
between Welsh’s method and the proposed method.
Figure 3.1 is a source color image, and figure 3.3 is a
target gray image. Figure 3.3 is the luminance channel
of figure 3.2 so that figure 3.2 can be treated as the
ground truth for testing the performance of color
transfer algorithms. Figure 3.4 is the resultant colored
image by using Welsh’s method without swatches.
Figure 3.5 is the result of Welsh’s method with
swatches. Figure 3.6 is the result of the proposed
pattern recognition based color transfer algorithm with
swatches. The red, blue, green and yellow rectangles in
the figures are swatches, and the corresponding swatch
pairs are marked with the same color.
The parameters used in the comparison are as
follows: d is 2 [see formula 2 and 3]; ω i of luminance

Figure 2. The time costs of nearest searching

2.6 Median filter for color components
The granular noises, i.e. the scattered pixels dyed
with “wrong” hues, can be suppressed by median filter
easily. Firstly we assign the pixels in a colorized
pixel’s small neighborhood as S1, S2, … , Sk, assuming
αoriginal and βoriginal are the color components of the
colorized pixel. Using the method described in section
2.4, the median color αi and βi will be found, and then
αoriginal and βoriginal are replaced by αi and βi retaining
the luminance.
For each pixel in the resultant colored image the
above filtering step should be repeated, and finally we
obtained a filtered color image with little granular
noises.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

in the chessboard distance is 1, and ω i of standard
deviation and texture entropy are all 3 [see section
2.1.4]. The neighborhood size is 9 × 9 pixels, and k
used in the median k-nearest-neighbor rule is 7 [see
section 2.4].
Obviously figure 3.4 looks very dim, and the colors
are not natural in many regions (indicated by the
arrows). Figure 3.5 is much better than fig 3.4 but still
contains some incorrectly dyed areas (also indicated by
the arrows). Compared with figure 3.4 and 3.5, fig 3.6
looks more smooth and realistic, and the incorrectly
dyed areas are very small. Therefore, we consider fig
3.6 the best result.
In this experiment figure 3.3 is just the gray version
of figure 3.2, so we know the gray image’s true color
components. In order to evaluate the performance of
color transfer in this case, a distance between two color
images is defined by formula 4, where ( x, y ) is the
coordinates of a pixel in image I 1 , and R1 ( x, y ) ,
G1 ( x, y ) and B1 ( x, y ) are its RGB values, and
R2 ( x, y ) , G2 ( x, y ) and B2 ( x, y ) are the RGB values
of pixel ( x, y ) in image I 2 . We use C for R, G and B
respectively to shorten the formula 4.
R ,G , B

¦ ¦ (C ( x, y) − C ( x, y))
¦1
1

D ( I1 , I 2 ) =

( x , y )∈I1

C

2

2

(4)

( x , y )∈I1

Now we can compute the distances between the
resultant images and the original color image. The
results are shown in table 1. We can see that the
proposed method is the best.

Table 1. The distances D(I1, I2)

I2

I1

Figure 3.2

Figure 3.4

Figure 3.5

Figure 3.6

59.39

35.51

27.38

Figure 3.4 Welsh’s method without swatches

Figure 3.1 Source color image

Figure 3.5 Welsh’s method with swatches

Figure 3.2 Original color image

Figure 3.6 The proposed method
with swatches

4. Conclusions

Figure 3.3 Target gray image

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

In this paper, an improved color transfer method is
formulated. It makes the visual appearance of the
resultant colored image much better, and in the same
time, reduces the computational complexity. Texture is

an effective feature for distinguishing objects, so we
introduced a co-occurrence matrix based texture
entropy to expand the feature vector. Sample reduction
and median k-nearest-neighbor rule can suppress the
influence of the outliers and median filter reduces the
spatial noise. In addition, KD tree is used to accelerate
the nearest-neighbor searching.
The general problem of color transfer is ill-posed.
Sometimes luminance and neighborhood statistics are
not sufficient to determine the colors. In this case, the
color information cannot be transferred correctly by
just using current algorithms. We believe that image
segmentation will help us to solve the problems in such
a circumstance.

5. Acknowledgements
This work was supported by Ministry of Science
and Technology of PRC under contract 001CB510307.

6. References
[1]
[2]

K.R. Castleman, Digital Image Processing, Englewood
Cliffs, N.J.: Prentice Hall, 1996.
T.M. Lehmann, A. Kaser, and R. Repges, “A simple
parametric equation for pseudocoloring grey scale
images keeping their original brightness progression”,
Image and vision computing, 1997, 15(3), pp. 251-257.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

[3]

T. Welsh, M. Ashikhmin, and K. Mueller,
“Transferring color to greyscale images”, ACM
Transactions on Graphics, 2002, 21(3), pp. 277-280.
[4] E. Reinhard, M. Ashikhmin, B. Gooch, and P. Shirley,
“Color transfer between images”, IEEE Computer
Graphics and Applications, 2001, 21(5), pp. 34-41.
[5] D.L. Ruderman, T.W. Cronin, and C.C. Chiao,
“Statistics of cone responses to natural images:
implications for visual coding”, Journal of the Optical
Society of America A, 1998, 15(8), pp. 2036-2045.
[6] A. Hertzmann, C. Jacobs, N. Oliver, B. Curless, and D.
Salesin, “Image analogies”, SIGGRAPH 2001
Conference Proceedings, 2001, pp. 327-340.
[7] R.O. Duda, P.E. Hart, and D.G. Stork, Pattern
Classification (Second Edition), New York: John
Wiley & Sons, 2001.
[8] F.J. Ferri, J.V. Albert, and E. Vidal, “Considerations
about sample-size sensitivity of a family of edited
nearest-neighbor rules”, IEEE Transactions on systems,
man, and cybernetics part B, 1999, 29(5), pp. 667-672.
[9] J.L. Bentley, and J.H. Friedman, “Data structures for
range searching”, ACM Computing Surveys, 1979,
11(4), pp. 397-409.
[10] J.L. Bentley, “Multidimensional binary search trees
used for associative searching”, Communications of the
ACM, 1975, 18(9), pp. 509-517.
[11] M. Singh, and S. Singh, “Spatial texture analysis: a
comparative study”, 16th International Conference on
Pattern Recognition, 2002, Volume 1, pp. 676-679.
[12] V. Cherkassky, and F. Mulier, Learning From Data:
Concepts, Theory, and Methods, New York: John
Wiley & Sons, 1998.

