2012 Ninth International Conference on Computer Graphics, Imaging and Visualization

Oriented Animal-mask Decoration Pattern Generation
Ming-Te Chi and Wei-Cheng Liou
Department of Computer Science
National Chengchi University
Taipei, Taiwan
mtchi@cs.nccu.edu.tw g9822@cs.nccu.edu.tw

Abstract—Oriented Animal-mask on the bronze vessels in
the Shang and Zhou dynasty is an art of abstraction and
stylization from animal face. The non-photorealistic rendering
technology tires to establish a system to reproduce the style
after analyzing how people create artworks. In this paper,
we develop a pattern generation method for the animalmask decorations. By analyzing the structure and identifying
association between animals and animal-mask decorations, we
generate facial features from employing a structural matching
method to ﬁnd correspondence from an sample database. For
pattern stylization, we propose a modiﬁed reaction-diffusion
method to stylize the input image patterns and enhance
the features in animal-mask decoration. And the proposed
reaction-diffusion method can also simulate the diffusion alone
user designated ﬂow vectors to generate background decoration
patterns. Combining the processes above, we can generate an
animal-mask decoration style image. We demonstrate results
with a number of animal input images.

Figure 1.

of the butt horn (or ear) and fangs. The foreground texture is
composted with block and stripe patterns, such as tiger and
leopard, used to enhance the style of facial features. Finally,
the spiral geometric patterns (commonly known as thunder
pattern) tile repeatedly in the background.
We develop a feature aware decorative pattern generation
system based on pattern matching and reaction-diffusion.
The main contributions of this paper are
• Example matching of each facial components through
analysis the input image.
• A reaction-diffusion method combined with the vector
ﬁeld dynamically convert the input image into a features
decorative stripe style.
• User can adjust the different parameters to control the
generated stripe results, such as line width, angle and
direction in foreground, background pattern characteristics ... and so on.
The remainder of the paper is organized as follows. In
Section II we review existing research most relevant to
this work. We then provide an ﬂowchart and describe the
details of each step in Section III. We show our results
in Section IV and summarize and discuss possible future
research directions in Section V.

Keywords-Non-photorealistic Rendering; Pattern Generation; Reaction-Diffusion

I. I NTRODUCTION
Abstraction, exaggeration, and symmetry are main characteristic in the ancient Chinese art and decorations. Animal
mask pattern (Taotie or gluttonous ogre mask) is one of the
decorations art on the bronze vessels in the Shang and Zhou
dynasty (Figure 1). Ancient Chinese people generated these
animal faces pattern by imagination and by the combination
of features from different animals to generate these mythological animal patterns. In the ﬁeld of Non-Photorealstic
Rendering, this area has not been studied extensively. By
analyzing these repetitive patterns and structures, we can
derive the meaning behind their appearance and placement.
However, it takes more than artistic talent to create these
decorative patterns as one also needs extensive knowledge,
understanding of fundamental elements in traditional Chinese arts. Acquiring these knowledge normally involves
lengthy learning cycles and therefore we propose a system
that can help generating these traditional decoration patterns
to reduce the time and effort in creating artworks in this
style.
The structure of animal mask pattern contains three parts:
facial features, foreground texture pattern, and background
decorative patterns. The structure of facial feature contains a
pair of big eyes and a nose for the symmetrical arrangement
978-0-7695-4778-7/12 $26.00 © 2012 IEEE
DOI 10.1109/CGIV.2012.19

Animal-mask pattern in Shang dynasty.

II. P REVIOUS W ORKS
There are three most related works: First, the study about
the rule based pattern generation; Second, reaction-diffusion
based method for generating organic patterns. Third, the
study of using vector ﬁeld to enhance the stylization in the
ﬁeld of NPR research.
Rule-based Decorative Pattern The research on decorative pattern generation was pioneered by Alexander [1] in
1975, which discussed method of symmetric pattern generation. Later works on the ﬁeld focused on the generation
7

speciﬁc patterns representing facial features differs. Figure
2 presents the proposed ﬂowchart. Our approach is divided
into three steps: ﬁrst step is facial feature extraction (ﬁgure
2(b)(c)), second step is generating patterns corresponding
to each speciﬁc facial feature(ﬁgure 2(d)(e)), and the third
step is background ﬁlling pattern generation. When performing pattern generation, we can enhance the resulting
pattern using speciﬁc animal texture patterns if the animal
is known beforehand. To simulate animal texture pattern we
use reaction-diffusion. The background patterns are usually
simple in style, and often take the form of thunder patterns.
Reaction-diffusion is also used to generate these background
ﬁlling patterns.

of such patterns. In Prusinkiewicz [2], a rule for plant
structure utilizing L-system were proposed which started
the research of rule-based pattern generation and structural
analysis of patterns to derive these rules. Wong et al. [3]
extended the rule based method as a way of generating
more sophisticated, interleaving ﬂoral decorative patterns.
In addition to above, the work by Xu and Kaplan [4] on
spiral and vortex shaped maze generation which relied on
predictive pattern structure has obvious similarity with some
traditional Chinese decorative patterns.
Reaction-Diffusion In 1952, Turing[5] introduction
reaction-diffusion which can generate a lot of organic pattern. Reaction-diffusion function deﬁned interaction between
different types of elements whose reaction can be diffused
onto neighboring elements due to difference in concentration, which in turn produce a natural-looking spot- and linepatterns. The patterns generated in this way were called
Turing Pattern which has three important characteristics:
self-structuring, self-healing, and pattern generated can be
controlled by varying the reaction function. These characteristics are similar to those patterns found in nature. In 1991,
Turk [6] proposed a way to use reaction-diffusion to generate
patterns found on animals and mapped them onto 3D animal
models. Similarly, in Liu et al. [7] the authors employed
reaction-diffusion to simulate the changes in spot patterns
on leopards as they age from young to mature; when a
leopard grows, spots becomes circles and later rosette when
mature. We can therefore assume the animal patterns can be
simulated through reaction-diffusion.
Non-Photorealstic Rendering Traditionally, NPR research focus on non-optical physics approach of image
processing that stylizes or transforms images. Current trend
in NPR research make use of vector ﬁeld in addition to
luminance and chroma information when performing transformations. Kim et al. proposed the feature-guided image
stippling method [8]. Kyprianidis and Kang [9] proposed a
LIC (Line Integral Convolution) based approach in stylized
abstraction to both images and videos; the vector ﬁeld
obtained from the image is used as a feedback to enhance
feature points within the image. These works gave us the
basis of using vector ﬁeld as an additional input to pure
color-based approach and let us obtain output images that
further emphasize speciﬁc feature points.

A. Facial Feature
The study of face recognition is important to stylization
conversion. The research on human facial recognition has
made profound progress in recent years and many works on
stylized human faces are based on feature detection on human faces. For example, [10] and [11] both perform cartoon
styled transformation on human facial features that are found
using automated detection techniques. In order to improve
application of our system, we applied the methodology of
facial feature detection on animal faces. However, due to
the large variance in animal facial features, there exists no
common rule for such detection and thus we faced signiﬁcant
challenges when applying facial feature detection to different
animals.
We ﬁrst apply image segmentation and Artistic Thresholding [12] on the input image to preserve and enhance the
salient feature. Figure 2(b) show a result of this conversion.
Then, due to the ratio and structure of animal face pattern
is largely exaggeration, thus it is hard to directly ﬁnd the
correspondence between the real animal face and animal
mask pattern. We manually select 5 feature points to extract
the facial feature (eye, horn, ear, and nose) as shown in ﬁgure
2(b). On the other hand, we analyze the historical animal
mask artworks, and measure the ratio and position of each
facial unit in ﬁgure 3. Then, a matching process is applied to
ﬁnd best sample from database, and a morphing process is
used for the resize and combination to form the ﬁnal result
(ﬁgure 2(f)). There are 70 samples in current database, we
collect around 20 samples in eyes and horn, and 10 samples
for fang, ear and nose. we can have 20x20x10x10x10
combination from this database. Figure 2(c) show part of
database on eye.

III. M ETHOD
In this paper, we aim to create traditional Chinese animal
mask patterns known as Taotie. Taotie is an imaginary
animal created by abstracting features from various animals.
We observed from historic artworks that these patterns
follows speciﬁc forms that consists mainly of stripes and
blocks. These animal face patterns were applied mostly on
bronze vessels and therefore our data are obtained using
stone rubbing technique and yielded data in pure bi-tonal
masks with sunk relief. Depending on targeted animals,

B. Texture feature
These reaction-diffusion studies [5] [6] [7] show the
capability to generate organic pattern, and provide the result
with controllable stripe pattern. And we also ﬁnd the feature
of animal mask pattern is mainly composited with block
pattern, and surrounded by stripe pattern. The patterns in
animal mask have many in common with reaction-diffusion

8

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Figure 2. Flowchart. (a)input image. (b)feature enhance and extraction. (c)part of example database. (d)(e)feature stylization. (f)combination with matching
and stylization. (g)ﬁnal result with background.

Figure 3.

how to control the interaction with the kernel A and the
response Y .
2) Local control: To control local variance in patterns,
we employed a control map to change the area where the
reaction-diffusion kernel is applied; this control map could
be a greyscale image of input image or designed image by
user, denoted as CI. We modiﬁed Yk,I terms in equation 1 to
obtain equation 4, and adjust the area of reaction-diffusion
according to grayscale value on the control map. Figure 4
shows reaction-diffusion result with control map. Figure 4
(b)(c) show two control map and result in top and down
row. At the same time, we assign two weights, sx and sy ,
as vertical and horizontal scale factor so we can have ﬁner
control over the rectangular shape of the reaction-diffusion
kernel. This change enabled bolder stripes in the speciﬁc
vertical and horizontal orientation. The pairs in ﬁgure 4
(d)(e) show the effect by sx and sy .

Animal-mask structure.

pattern. We adopt the Cellular Neural Network (CNN) based
maze generation method proposed by Wan et al. [13] as
global control, adding the control map and vector ﬁeld to
control the local abstraction and exaggeration from input
image feature.
1) Global control: Wan et al. [13] proposed the following
equation. Assume each pixel as a cell, the cells state denote
in time t as X t , and its response is Y t . The CNN dynamics
is a nonlinear equation of X and Y within the neighborhood
η , as given by
dXi,t j
dt

= −Xi,t j +

∑

k,l∈η (i, j)

t
ak−i,l− jYk,l
+ w1 Ii, j

1
Yi,t j = [|Xi,t j + 1|] − [|Xi,t j − 1|]
2

dXi,t j
dt

= −Xi,t j +

∑

ak−i,l− jCY t (i, j, k, l) + w1 Ii, j

(4)

k,l∈η (i, j)

t
CY t (i, j, k, l) = Yi+s
x ×CIi, j ×(k−i), j+sy ×CIi, j ×(l− j)

(5)

(1)

(2)
(d)

where (i, j) deﬁnes a grid point (or pixel location); I is the
input bias; and A = ak,l is a cloning template which speciﬁes
the interacting dynamics.
⎤
⎡
−0.75 −1 −1.5 −1.0 −0.75
⎢ −1.0 2.0
5.0
2.0
−1.0 ⎥
⎥
⎢
⎢
−1.5 ⎥
(3)
A = ⎢ −1.5 5.0 −10.0 5.0
⎥
⎣ −1.0 2.0
5.0
2.0
−1.0 ⎦
−0.75 −1 −1.5 −1.0 −0.75

(a)

(b)

(c)

(e)

Figure 4.
Control map.(a)input image. (b)control map. (c)reactiondiffusion result. (d)sx > sy . (d)sx < sy .

C. Background Feature

in which ai, j are kernel for reaction-diffusion which
controls local strip pattern generation.
This dynamic process will convert the input image into
stripe stylization and preserve most feature. For more detail,
please refer to Wan et. al [13]. For this paper, we will focus

The thunder-like background patterns ( ) found in Taotie
patterns were usually arranged in rectangular shaped outline
and have spiral shaped lines within. In this step, we ﬁrst
generate a basic thunder-like pattern and tile it so the
entire background area is ﬁlled with generated pattern. With

9

face pattern transformation and the results are shown in
ﬁgure 8. One slight modiﬁcation we do to our algorithm
has to do with the fact that Taotie patterns usually contains
patterns that represents horns, even for animals that do not
have horns. In our test data of tigers, we treat the ears
as reference to generate horn patterns due to their relative
spatial proximity. To cope with this special case, we use
the round pupil to detect tigers and generate a pair of
symmetric horn pattern, as well as skipping the generation
of ear patterns. All the images on the left side of the ﬁgure 8
below are input images, all the images on the right side are
output after applying our stylized techniques. Each animal
result shows differences in addition to the shape of pupils
includes: in (a) and (c) shows the horns are grown inwards
for cows and deers; outwards pointing horns are shown
on goat in (b) while tiger shows a symmetric horn in (d).
Patterns representing the nose are also different structurally
based on different patterns on the input face. Final stylized
and facial feature morphing are done by applying the vector
ﬁeld shown in ﬁgure 7(d) with reaction-diffusion.
Next, we test our system on different individuals of the
same kind of animal to see how texture patterns affect
the resulting image. We use three different tiger images as
input, and difference in texture patterns are reﬂected through
reaction-diffusion and give different results. As shown in
ﬁgure 9, input images are on the left side of the pair while
result images are on the right side. It is obvious that even
the same kind of animal yields different patterns. However,
when we perform survey with human observers, they found
it difﬁcult to tell which pattern was generated from which
input image as the facial features in input images are similar,
and the difference in texture patterns on the animals are also
too subtle for human to easily distinguish. This results leads
us to believe although our system can achieve the goal of
generating different patterns from different input, it has not
yield a easy way to distinguish animals of the same kind
and this will be something to improve in the future.
Our experiments were conducted on a consumer level
PC with an Intel duo core 2.8G processor and 2GB main
memory. The average running time of our system on several
samples is around 20 sec for a 400 by 400 pixel image.
Note that the most time-consuming step is modiﬁed reactiondiffusion method. When the input image contains complex
pattern and color, it takes more time to archive the stable
condition.
We use our result as texture on the 3D vessel model
to see if our result appear similar to that of real artifacts.
Using bump mapping, we can recreate the feel of the
carvings on the vessel. Another place where these patterns
are commonly seen are on clothes of puppet-show puppets.
Although modern puppet-show were developed long after
the era of bronze vessels, these patterns are still important
parts of the heritage and aesthetics of the culture. The result
is also shown below in ﬁgure 10.

reaction-diffusion, we can generate patterns of concentric
circles on a purely black rectangular outline shape. By applying vector ﬁeld of the correct orientation, these concentric
circles are transformed into spiral shaped stripes. To achieve
this, we set one vector in the vector ﬁeld at position S0 , using
fourth-order Runge-Kutta method to compute corresponding
positions S1 , S2 , S−1 , and S−2 . Connecting these positions
we get a feature vector. Repeating the above process ﬁve
times to ﬁnd ﬁve feature vectors. The new 5 × 5 sample
points (ﬁgure 5) can replace the original uniform 5×5 kernel
ak−i,l− j in equation 4, and provides more ﬂexible stylization.

Figure 5.
ﬁeld.

Example of reaction-diffusion kernel considering the vector

Beside of using the rectangular shape as input, user can
design other shape and apply the reaction-diffusion process
to generate various results. Figure 6 show a few example
output by different input shapes.

Figure 6.

Background generation by different input shape.

D. Combination
After these three processes, we can convert the input
image into a stylized animal mask pattern. Sometimes, the
matching examples in facial feature are not identical with
the texture feature in reaction-diffusion. Considering the
weathering effect on the traditional pattern, we can apply
the reaction-diffusion with a speciﬁc vector ﬁeld on the
combination result to enhance the effect. Figure 7 provides
several results on the effect of vector ﬁeld.
IV. R ESULT
When using different types of animal as input, facial
features and texture pattern generation are certainly going
to differ. We used four kinds of animal to perform animal

10

(a)
Figure 7.

(b)

(c)

(d)

(e)

Reaction-diffusion stylization. (a)initial combination. (b)(c)(d)(e)using different vector ﬁeld to enrich the stripe stylization.

Figure 8.

Generation process for four different animals.

Figure 9.

Figure 10.

Results for three different tigers.

of input image and convert the feature into stripe pattern.
This framework also let user design the artistic pattern in
the background.
In future work, it is interesting to discover better principle for facial features abstraction. If we can automatically
derive the rule from single example or smaller dataset, the
framework will be more ﬂexible and complete. And we also
want to extend this system to generate various stylization,
for example: the Sanxingdui is a kind of style has more
exaggerated pattern of human face.

Applications.

V. C ONCLUSION
This paper proposed a framework to generate the oriented
animal mask pattern combine with feature stylization and
sample matching from database. The modiﬁed reactiondiffusion with vector ﬁeld is used to preserve the feature

ACKNOWLEDGMENT
This work is supported by the National Science Council,
Taiwan under NSC-100-2628-E-004-001.

11

R EFERENCES
[1] H. Alexander, “The computer/plotter and the 17 ornamental
design types,” in Proceedings of the 2nd annual conference
on Computer graphics and interactive techniques, ser.
SIGGRAPH ’75, 1975, pp. 160–167. [Online]. Available:
http://doi.acm.org/10.1145/563732.1138362
[2] P. Prusinkiewicz and A. Lindenmayer, The algorithmic beauty
of plants. Springer-Verlag New York, Inc., 1990.
[3] M. T. Wong, D. E. Zongker, and D. H. Salesin, “Computergenerated ﬂoral ornament,” in Proceedings of the 25th annual
conference on Computer graphics and interactive techniques,
ser. SIGGRAPH ’98, 1998, pp. 423–434. [Online]. Available:
http://doi.acm.org/10.1145/280814.280948
[4] J. Xu and C. S. Kaplan, “Vortex maze construction,” in
Journal of Mathematics and the Arts, vol. 1, no. 1, 2007,
pp. 7–20.
[5] A. M. Turing, “The chemical basis of morphogenesis,” Philosophical Transactions of the Royal Society of London. Series
B, Biological Sciences, vol. 237, no. 641, pp. 37–72, August
1952.
[6] G. Turk, “Generating textures on arbitrary surfaces using
reaction-diffusion,” SIGGRAPH Comput. Graph., vol. 25,
no. 4, pp. 289–298, Jul. 1991. [Online]. Available:
http://doi.acm.org/10.1145/127719.122749
[7] R. Liu, S. Liaw, and P. Maini, “Two-stage turing model for
generating pigment patterns on the leopard and the jaguar.”
Phys Rev E Stat Nonlin Soft Matter Phys, vol. 74, no. 1-1, p.
011914, 2006.
[8] D. Kim, M. Son, Y. Lee, H. Kang, and S. Lee, “Featureguided image stippling,” Computer Graphics Forum, vol. 27,
no. 4, pp. 1209–1216, 2008.
[9] J. E. Kyprianidis and H. Kang, “Image and video abstraction by coherence-enhancing ﬁltering,” Computer Graphics
Forum, vol. 30, no. 2, pp. 593–V602, 2011, proceedings
Eurographics 2011.
[10] L. Liang, H. Chen, Y.-Q. Xu, and H.-Y. Shum, “Examplebased caricature generation with exaggeration,” in Computer
Graphics and Applications, 2002. Proceedings. 10th Paciﬁc
Conference on, 2002, pp. 386 – 393.
[11] P. Chiang, W. Liao, and T. Li, “Automatic caricature generation by analyzing facial features,” in 2004 Asian Conference
on Computer Vision, 2004.
[12] J. Xu and C. S. Kaplan, “Artistic thresholding,” in
Proceedings of the 6th international symposium on Nonphotorealistic animation and rendering, ser. NPAR ’08,
2008, pp. 39–47. [Online]. Available: http://doi.acm.org/10.
1145/1377980.1377990
[13] L. Wan, X. Liu, T.-T. Wong, and C.-S. Leung, “Evolving
mazes from images,” IEEE Transactions on Visualization and
Computer Graphics, vol. 99, no. RapidPosts, pp. 287–297,
2009.

12

