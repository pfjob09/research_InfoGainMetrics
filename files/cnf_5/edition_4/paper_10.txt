Estimation of Camera Parameters from a Single Moving Camera using
Quaternion-based Interpolation of 3D Trajectory
Donghun Lee, Jihun Park
Department of Computer Engineering
Hongik University, Seoul, Korea
{spitter@gmail.com, jhpark@hongik.ac.kr}
Abstract
This paper presents a new quaternion-based method
to estimate the camera parameters in a single video
stream to interpolate 3D trajectory of the moving camera
with camera parameters. We assume that the camera
looks at three fixed points while translating and rotating
in 3D space. To estimate the camera parameters, we get
a set of nonlinear equations derived from a geometric
perspective camera model. We have a large number of
image frames to find each camera parameters while the
number of unknown camera model parameters cannot be
determined from a single frame. In order to solve this
problem, we used interpolation to approximate camera
movement in both translation and rotation. This was
done using the concept of control node set at the specific
frame in the video stream. The camera movement is
based on acceleration level control while satisfying
physical constraints. The control node is the set of
variables used to determine acceleration-based
interpolated equation used for data fitting. In this
process, we used quaternion to interpolate orientation of
the camera. The acceleration-based data fitting problem
is solved using nonlinear parameter optimization solver,
GRG2. Our experimental results show that this approach
to estimate camera parameters and 3D trajectory of the
camera moment is robust enough to handle image
sequences of a common scene without sudden change in
camera moment.
Keywords--- Quaternion Interpolation, Camera
Parameter Estimation

1. Introduction
Estimation of the camera parameters from
consecutive images taken from a single moving camera
with common scene is one of the hardest problems in
augmented reality, and camera interpolation to produce
smooth and natural scene with three dimensional
artificial object embedded is very important in computer
animation. In this paper, we propose a new method for
estimation of the changes in camera parameters, both in

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

orientation and displacement, in a movie sequence. We
have a single camera moving, and the moving condition
may differ according to each scene. The target has three
points in the scene. We do not have any information in
terms of camera and three target points. If the camera is
moving at constant velocity, we need three images to
identify relative position/orientation of three target points
in terms of camera. Possibly we can compute
displacement/angular velocities between the target and
the camera. If the camera is moving at constant
acceleration, we need 4 images to compute all values
involved in the scene. If the camera is moving at varying
acceleration, there is no analytic solution, but we have to
rely on approximate solution. This work is an extension
of our early work in object tracking[13].
Coordination of Three Points from
Images

Nonlinear
Equations

Interpolation
Setting of Real
Camera Position
using Quaternion

Solving Nonlinear Equations with
Interpolation Variables

Camera
Parameters

Real Position of
Three Points

Figure 1. Architecture of the Proposed Method

2. Mathematical Model
This section focuses on developing a methodology
for estimation of camera parameters and interpolation of
its 3D trajectory. We assume that the input for solving

our problem is only consecutive images taken from a
single moving camera. Also we use the set of three
points taken from the consecutive images. Three points
in a scene defines a plane.

2.1. Nonlinear Equations
For estimation of the camera parameters, we assume
3 situations of camera’s movement : (1) constant velocity,
(2) constant acceleration, (3) variable acceleration. First,
if the camera is moving at a constant velocity, we need 7
variables for representing a camera (3 position, 3
orientation, 1 focal length), and additional 6 variables for
its velocity (3 position, 3 orientation). Also we do not
know three points. For three points representation while
setting one of the point as a world coordinate, we need
three variables. As a result need total 16 variables. But
from three images, we get total 18 equations, 6 equations
each from a single image. This results in over
constrained nonlinear equations. Also we know each
image is apart in time by 1/30 second. We can compute
resulting camera position and orientation using
interpolation. Second, if the camera is moving at a
constant acceleration, we need 7 variables for
representing a camera, 6 variables for its velocity, and
additional 6 variables for its acceleration. As a result
need total 22 variables. But from four images, we get
total 24 equations, 6 equations each from a single image.
Also we do not know three points. Third (Fig. 2), if the
camera is moving at a variable acceleration, we need
6n+1 variables for representing a camera, 6 variables for
its velocity, and additional 6 variables for its acceleration.
Also we do not know three points. As a result need total
6n+16 variables. Theoretically, if the acceleration is
changed at each frame individually, we do not have the
solution. But to solve our problem, we simplify the
variation of acceleration at specific points called control
node. In other words, if the control node is three, we
need total 34 variables. But from consecutive image
frames, we can get enough equations to solve our
problem.

2.2. Quaternion and Expansion to Translation

2

G

represent axis of rotation, θ rotation angle around n ,
and cos θ = w represents the real part of a quaternion,
2

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

an extension of the quaternion, h = (w, p, q, r, x, y, z).
The rotational part, (w, p, q, r) where w2 + p2 + q2+ r2 = 1,
is the same as that of a quaternion. The translation part is
(x, y, z). Multiplication of two extended-quaternion is
defined as follows.
h1 * h2 = (w1, v 1 , t 1 ) * (w2, v 2 , t 2 )
= (w1 * w2 – v 1 ⋅ v 2 ,
w1* v 2 + w2 * v 1 + v 1 × v 2 ,
(w12 - v 1 ⋅ v 1 ) t 2 + 2 v 1 ( v 1 ⋅ t 2 )
+ 2 w1 ( v 1 × t 2 ) + t 1 )
where vi = (pi, qi, ri), ti = (xi, yi, zi),
|wi|2 + |vi|2 = 1, i = 1, 2

2.3 Interpolation of 3D Position
To interpolate 3D position and angle between
control nodes, we use acceleration and velocity model.
Fig.2., acceleration s′′(t ) denotes the control node. Later,
the control nodes will be used set of variables to solve
nonlinear equations. Fig.3.(a), velocity is equal to the
area of a trapezoid. And Fig.3.(b), displacement is equal
to the area of a trapezoid. The expressions of velocity
and displacement are denoted as follow.
Δt

v(t ) = v0 + ∫ a(t )dt ,
0

⎛ x′′ − x′p′ ⎞
⎟⎟t +x′p′ ,
a (t ) = ⎜⎜ c
⎝ Δt ⎠
⎛ x′′ − x′p′
⎞
xc′ = ⎜⎜ c
+ x′p′ ⎟⎟Δt + x′p
⎝ 2
⎠
′
′
′
′
x
x
+
⎛
p ⎞
⎟⎟Δt + x′p ,
= ⎜⎜ c
⎝ 2 ⎠
Δt

A quaternion needs four variables to represent a
single rotation or orientation, which has 3 DOFs(Degree
of Freedom). Euler angles sometimes create problems
such as the Gimbal lock problem, where we lose one
degree of freedom and may get an infinite number of
solutions, leading to singularity. But we avoid this by
using a quaternion. A quaternion is also computationally
faster since it does not require the computation of
trigonometric equations. Because a quaternion can
represent only rotation, we have extended the quaternion
by adding a translation step[1]. The quaternion is defined
G
G
by q = (cos θ , n sin θ ) = (w, pi + qj + rk), where n
2

G
n sin θ = pi + qj + rk represents the imaginary part
2
G
of a quaternion. We denote v to be a v vector. Let h be
while

p = p0 +∫ v(t )dt
0

Δt

t

0

0

= p0 +∫ (v0 + ∫ a( s)ds)dt
= p0 +v0 Δt + ∫

Δt

0

t

∫ a(s)dsdt ,
0

⎛ x′′ − x′p′ x′p′ ⎞ 2
xc = ⎜⎜ c
+ ⎟⎟Δt +x′p Δt +x p
2⎠
⎝ 6

variables at the control node and time, we can interpolate
the orientation of camera between control nodes.

Figure 2. Example of a Linear Interpolation for a

Acceleration S”(t)

Figure 3. (a) Velocity and (b) Displacement
Model

2.4. Interpolation of 3D Orientation
Quaternion

derivative

is

derived,

q ′(t ) = 1 w(t )q (t ) , where w is a quaternion based
2
on angular velocity [2]. Also second derivative is derived
as follow.

1
1
w′q + wq′
2
2
1
1
= aq + wq′
2
2
1
1
= aq + wwq ,
2
4

q′′(t ) =

where, w is a quaternion based on angular velocity
G
[0, θ ′n ] and a is a quaternion based on angular
G
acceleration [0, θ ′′n ].
Since we have known initial angular displacement
and velocity, acceleration, we get angular acceleration a
at the control node from the section 2.3. And also, using
angular velocity and acceleration, we made a quaternion
w based on angular velocity and a quaternion a based on
angular acceleration. Therefore, using computed

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

3. Experimental Result
Figure 4. Camera Moving without Rotation
In this paper, we made the moving camera model
expressed as a set of nonlinear equations. Many software
packages exist to solve these nonlinear equations, for
example Amoeba, Eureka, GRG2, Vf02ad, and so forth.
In this paper, we used GRG2. GRG2 is setting the
number of variables, functions, upper and lower bound
of variables and so forth, as the initial values, and solve
nonlinear equations using objective function and
constrained functions. We define the constrained
functions as w2+p2+q2+r2 = 1 at the control node and
objective function as f = [(Ix1, Iy1, … , Ixn, Iyn) – (px1, py1,
… , pxn, pyn)]2, where Ix and Iy are position of three
points in images and px, py are projected positions, which
are computed by transformation matrices based on the

computation result. Our experiment is based on two
movies with 60 consecutive images each. First, camera is
moving-in to the target points. Second, the camera is
moving around the target points, translating and rotating
along the atrget. Figure 4 and Figure 5 show our
experimental results.

Acknowledgements
This research was supported by Seoul Research and
Business Development Program (10555).

References
[1]

[2]
[3]
[4]

Figure 5. Camera Moving with Rotation

4. Conclusions

[5]
[6]

[7]

[8]
[9]
[10]

In order to estimate camera parameters, we used
interpolation to approximate camera movement in both
translation and rotation. Because we have so many image
frames in a movie sequence, we used the concept of
control node set at the specific frame in the video stream.
The camera movement is based on acceleration level
control while satisfying constraints of moving according
to the physical kinematics. The control node is the set of
variables used to determine acceleration-based
interpolated equation used for data fitting. In this process,
we used quaternion to interpolate orientation of the
camera. Our experimental results show that this approach
to estimate camera parameters and 3D trajectory of the
camera moment is robust enough to handle image
sequences of a common scene without sudden change in
camera moment.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

[11]

[12]
[13]

[14]

[15]

E. Guillou, D. Meneveaux, E. Maisel, and K. Bouatouch,
"Using Vanishing Points for Camera Calibration and
Coarse 3D Reconstruction from a Single Image", Visual
Computer, vol. 16 pp. 394-410, 2000.
R. Hartley, "A Linear Method for Reconstruction from
Points and Lines", Proceedings of International
Conference on Computer Vision, pp 882-887, 1995.
R. Hartley, "Computation of the Quarifocal Tensor",
Computer Vision-ECCV'98, Lecture Notes in Computer
Science, vol. 1406, Springer-Verlag, pp. 20-35, 1998.
P. Torr and A. Zisserman, "Robust Computation and
Parameterization of Multiple View Relations",
Proceedings of International Conference of Computer
Vision, Narosa Publishing House, pp. 727-732, 1998.
Don Murray, James J. Little, "Using Real-Time Stereo
Vision for Mobile Robot Navigation", Auton. Robots
8(2), pp. 161-171 (2000)
C. C. Pang, A. R. Guesalaga, and V. O. Roda, "Robust
Estimation of 3D Trajectories from a Monocular Image
Sequence", International Journal of Imaging Science &
Technology, vol. 12, pp. 128-137 2002.
Andrew J. Davison, "Real-Time Simultaneous
Localization and Mapping with a Single Camera",
International Conference on Computer Vision 2003,
2003.
S. G. Hoggar. "Mathematics for Computer Graphics", C
Version, 2/E. Prentice Hall, 1996.
Ken Shoemake. "Animating Rotation with Quaternion
Curves", SIGGRAPH 1985, vol. 19 No.3 pp. 245-254,
1985.
Myung-Jun Kim, Myung-Soo Kim, & Sung Yong Shin.
"A General Construction Scheme for Unit Quaternion
Curves with Simple High Order Derivatives",
SIGGRAPH 1995, 1995.
Myung-Jun Kim, Myung-Soo Kim, & Sung Yong Shin.
"A Compact Differential Formula for First Derivative of
a Unit Quaternion Curve", The Journal of Visualization
and Computer Animation, 1996.
Jihun Park, Jeongseog Ryu. "Heptanion : An Extension
of Quaternion" 1999.
Quming Zhou, Jihun Park, J.K. Aggarwal, "Quaternionbased Tracking Multiple Objects in Synchronized
Videos", Lecture Notes in Computer Science(ISCIS
2003) vol #2869 Springer-Verlag, Verlin, Nov 2003.
Tasi, R., "A Versitile Camera Calibration Technique for
High-accuracy 3D Machine Vision Metrology using Offthe-Shelf TV Cameras and Lenses", IEEE Journal of
Robotics and Automation RA-3 (1987) 323-344.
Jianbo Shi and Carlo Tomasi, "Good Features to Track",
IEEE Conference on Computer Vision and Pattern
Recognition, pages 593-600, 1994.

