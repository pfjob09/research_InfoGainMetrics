Urdu Spoken Digits Recognition Using Classified MFCC and Backpropgation
Neural Network
S. M. Azam, Z.A. Mansoor, M. Shahzad Mughal, S. Mohsin
COMSATS Institute of Information Technology Abbottabad, Pakistan
{azam@ciit.net.pk, zubairalimansoor@hotmail.com, msmpu@hotmail.com,
smohsin@ciit.net.pk}
Abstract
Neural networks have found profound success in the
area of pattern recognition. In the recent years there has
been use of Neural Network for speech recognition. In
this paper Backpropgation Neural Network has been
used for isolated spoken Urdu Digits recognition. Mel
Frequency Cepsptral Coefficients (MFCC) has been
used to represent speech signal. Dimensions of speech
features were reduced to a vector of 39 values. Only 39
values from MFCC features speech are fed to the Neural
Network having more than one hidden layers with
varying number of neurons, for training and recognition
An analysis has been made between different number of
hidden layers and different number of neurons on hidden
layers. It has been found that results for these 39 values
are similar to that obtained using complete MFCC
features that range from 804 to 67x39. With the use of 39
values on input layer, computational complexity and time
for training and recognition of neural network is
reduced. In order to evaluate the significance of the
proposed method on data other than Urdu digits, 30
English words have been trained and recognized that
gave 98% results. All the implementation has been done
in MATLAB.
Keywords---Mel Frequency Cepsptral Coefficients,
Urdu spoken digits recognition, Backprapagation.

1. Introduction
All the speech recognition systems now in the market
are based on the statistical techniques like Hidden
Morkov Model [7], Support Vector Machine, and
Gaussian Mixture Model etc. From the last two decades
Neural Network has been extensively used for speech
recognition. Artificial neural networks have been
investigated.
For many years for the desire of achieving human-like
performance in the field of automatic speech recognition.
These models are composed of many nonlinear
computational elements operating parallel in patterns

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

similar to the biological neural networks. The most
beneficial characteristic of Neural Network for speech
recognition is the fault tolerance and nonlinear property
[3]. Neural networks are excellent classifiers, but
performance is dependent on the quality and quantity of
training samples presented to the network. In cases
where training data is sparse or not fully representative
of the range of values possible, incorporation of fuzzy
techniques improves performance.
It has been observed that by using large number of
neurons on input layer it takes much time for training or
learning the user specific voice patterns. In this paper we
have reduced the number of values that are fed to the
neural network for training. By calculating the two deltas
and energy spectrum of speech pattern Mel Frequency
Cepstral Coefficients give 67x39 values which are too
much and require much time for the network to get the
weights convergence for so many values. Reducing
dimensions of the input patterns in any neural network
based pattern recognizer is always desired because
higher the dimensions greater is the nonlinearity which
requires more hidden layers as well as increased training
iterations. There is limited number of research efforts for
spoken Urdu digit recognition using neural network.
Abdul Ahad [1] has developed spoken Urdu digit
recognizer. He has developed speaker dependant digit
recognizer. His system is based on Mel Scale Cepstral
Coefficients (MSCC) parameters for feature extraction.
Numbers of input layer neurons in his system are
120.His system records voice of user of duration 0.8s.
Alotaibi et al [2] has developed Arabic digit recognizer.
He has used Recurrent Neural Network for spoken
Arabic digit recognition. His system is based on MFCC
parameters for feature extraction. There are 143 values in
his systems that are fed to the Neural Network. There are
two hidden layers comprising of 40 and 15 neurons
respectively. In the Coming sections first there is
discussion of proposed system. The problem that is faced
due to massive data for training the neural network is
solved by using only higher values of extracted features
from speech that have given proven results for the
desired system. Then there is description of
implementation details of the system that leads to the
concluding section of the paper.

.2. Proposed system
We have developed system for Urdu isolated digit
recognition. This is speaker dependant system. A
database has been created that contains 1000 total voice
patterns for 10 digits for each digit there are
100 repititions.500 are used for training and 500 are used
for testing same as Abdul Ahad [1] has used. For English
words there are 1200 voice patterns 40 for each
word.900 repetitions are used for training and 300 are
used for testing. All of these recorded sound files are of
one speaker. This is monospeaker database. Features are
extracted using MFCC. There are 67x39 values after
applying MFCC with two Deltas for dynamicity of voice.
These 67x39 values are reduced to only 39 values by
getting only maximum values from each column of
67x39 MFCC feature vector. Numbers of input neurons
to the neural network are 39. Different numbers of
hidden layers are tested. Using one hidden layer there is
best solution with 19 neurons. Output layer have 10
neurons for digits. On the basis of this technique 30
English words are also trained and recognized. By using
full features 67x39 it takes much time for training. But
with the use of 39 values training is complete within one
hour. Here learning algorithm is back propagation. The
model of neural network used is given in figure 1. The
learning strategy of this type of neural network occurred
is called supervised learning since it tells what to learn. It
is up to the network to carry out how to learn process.

analysis [4]. Me1 Frequency Cepstral coefficients
(MFCCs) are used to encode the speech signal. Me1
scale
Frequencies are distributed linearly in the low range
but logarithmically in the high range, which corresponds
to the physiological characteristics of the human ear [5].
Cepstral analysis calculates the inverse Fourier transform
of the logarithm of the power spectrum of the speech
signal [8]. Typical steps of Mel Frequency Cepstral
Coefficients are shown in the figure 2.
Speech Signal
0.8s Duration

Extract MFCC Features
Pre-emphasis

Windowing

FFT

Mel-Scale
Filterbank

’

Log

DCT
67x39 MFCC Features
Reducing
Dimensions
Reduced 39 Features
BPNN

Figure 1

2.1 DSP technique for feature extraction
The digitized sound signal contains a lot of irrelevant
information and requires a lot of storage space. To
uniquely recognize speaker or to recognize the speech
only useful information must be extracted. The power
spectrum of the speech signal is the most often used
method of encoding. The human ear performs something
very similar to a Fourier Transform on incoming sound
signals before passing the information on to the brain for

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Classified Digit

Figure 2

3. Implementation
The system has been developed in Matlab. Neural
Network using backpropogation of error algorithm is
implemented for Urdu spoken isolated digits recognition
[6].Voice of speaker is recorded using microphone for

the duration of 0.8s. Speech is sampled at 11025kHz.
Then this recorded sound file is made noise free by
passing through pre-emphasis module as described in the
figure 2. After removing noise hamming widowing is
applied. The digitized sound signal contains a lot of
irrelevant information and requires a lot of storage space.
To simplify the subsequent processing of the signal,
useful features must be extracted and the data
compressed. The power spectrum of the speech
Signal is the most often used method of encoding.
The human ear performs something very similar to a
Fourier Transform on incoming sound signals before
passing the information on to the brain for analysis [4].
Me1 Frequency Cepstral coefficients (MFCCs) are used
to encode the speech signal. Me1 scale frequencies are
distributed linearly in the low range but logarithmically
in the high range, which corresponds to the physiological
characteristics of the human ear [5]. MFCC features are
67x39 that are obtained by using the algorithm that we
have applied for feature extraction. By using all these
values it takes much time for training the neural network.
We have reduced these values to just 39 values.
Parameters

Value

Sampling Rate

11025

Database

Isolated 10 Urdu Digits

Speakers

One

Repetitions

1000

Preemphasis

1-0.95z

Window

Hamming 256

From each column maximum value is taken and a
feature vector of 39 values is fed to the neural network
for training, recognition purpose. Recognition results
obtained using these values are shown in the figure 4.
We have used different configurations for neural
network. Different numbers of neurons have been used
on hidden layer.
Zero
50
0
0
0
0
0
0
0
0
0

One
0
49
0
0
0
0
1
0
0
0

Two
0
50
0
0
1
0
0
0
1

Three
0
1
0
50
0
0
0
0
0
0

MLP with one hidden layer (config1)
Activation function
F (x) = (1 - exp (-x)) / (1 + exp (-x))
Input layer neurons
= 39
Hidden layer neurons
= 15
Output layer neurons
=10
MLP with one hidden layer (config2)
Activation function
F (x) = 1 / (1 + exp (-x))
Input layer neurons
= 39
Hidden layer neurons
=25
Output layer neurons
= 10
MLP with one hidden layer (config3)
Activation function
F (x) = (1 - exp (-x)) / (1 + exp (-x))
Input layer neurons
= 39
Hidden layer neurons
=19
Output layer neurons
= 10
MLP with two hidden layer (config4)
Activation function
F (x) = 1 / (1 + exp (-x))
Input layer neurons
= 39
Hidden layer1 neurons
=60
Hidden layer2 neurons
=25
Output layer neurons
=10

Figure 3

Zero
One
Two
Three
Four
Five
Six
Seven
Eight
Nine
Total

Here is description of different neural network
configurations

Four
0
0
0
0
50
0
0
0
0
0

MLP with two hidden layer (config5)
Activation function
F (x) = (1 - exp (-x)) / (1 + exp (-x))
Input layer neurons
= 39
Hidden layer1 neurons
= 100
Hidden layer2 neurons
= 50
Output layer neurons
= 10

Five
0
0
0
0
0
48
0
0
0
0

Figure 4

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Six
0
0
0
0
0
0
49
0
0
0

Seven
0
0
0
0
0
0
0
50
0
0

Eight
0
0
0
0
0
1
0
0
50
0

Nine
0
0
0
0
0
0
0
0
0
49

Accuracy
100
98
100
100
100
96
98
100
100
98
99

Words
Am
Are
Be
At
Boy
Brother
Your
Care
Child
Pakistan

Recognized
29
30
30
29
30
30
30
30
29
30

Words Recognized
Coming 30
Dear
30
Fine
30
Friend
29
Going
30
Good
29
Health
30
How
30
I
30
It
30
Figure 5

Digit

Config1
Recognition
Accuracy

Config2
Recognition
Accuracy

0
1
2
3
4
5
6
7
8
9

98
96
88
76
86
67
91
95
98
79

93
97
84
78
32
72
64
55
77
81

Comparison of different configurations is shown in
figure 6. Different activation functions have also been
used to find best performance. For configuration 1, 2 and
3 there is training until we reduce error to 0.01 while in
other configurations there are 1000 epochs for training.
Best results have been achieved at 19 neurons for one
hidden layer. Results also have been tested on 30 English
words. For each word there are 30 repititions.Neural
Network is trained on 900 total sound sound files for 30
words and then tested. Results are shown in the figure 5.
Results for selected English words are also 98%
correct .some other words have also been tested for
evaluating this technique some. Words that are hardly
differentiated do not have Good results or words that
require more duration for speaking than 0.8s that I have
taken to record the words. At run time if software is
trained on five voice samples of voice patterns in a
particular environment, there are 100 percent recognition
results

3.1 Learning rate and activation function
Learning rate is very much effective to converge the
neural network weights. its should neither be to small

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Config3
Recognition
Accuracy
100
98
100
100
100
96
98
100
100
98
Figure 6

Words
Salaam
Send
She
To
You
We
When
Where
Comsats
By

Recognized
30
30
30
30
29
30
30
30
29
21

Config4
Recognition
Accuracy
90
63
50
74
28
95
40
78
52
30

Config5
Recognition
Accuracy
89
96
90
75
89
90
91
92
90
90

that causes to much time for convergence nor it should
be to big value that also causes problems for
convergence. In our solution we tried different learning
rates but best performance we found at 0.01.Activation
functions we used are
F (.x) = (1 - exp (-x)) / (1 + exp (-x)) and

1/(1 + exp(−value))

First one is good for non linear dynamic data and in our
case its performance is best.

4. Conclusion & future work
Although the use of Neural Networks for speech
recognition is not a matured technique compared to
Hidden Markov Model but the power of neural networks
for learning nonlinear problems has inspired the
researchers a lot to use this technique for the speech
recognition problems [1,2,4,5,7]. In this work we have
evaluated the use of multilayer perceptron with various
variations on the number of hidden layer as well as
number of neurons at the each hidden layer for isolated
Urdu digits recognition. Five different configurations as

discussed in section 3 have been analyzed. It is
concluded from the experimental results that with the
classified (reduced to 39) MFCC coefficient, the best
results are achieved with single hidden layer consisting
of 19 neurons only. Conventionally the MSCC features
with 120 input neurons [1] and using MFCC features 143
input neurons [2] are used whereas in the proposed
technique only 39 input neurons are taken to get
classified MFCC values. The future suggestion for
continuous speech recognition is to use the pattern
separating neural network to break continuous speech
signal into words by feeding the energy variation pattern
with the number of zero crossing patterns.

5. References
[1] Ahad, A.; Fayyaz, A.; Mehmood, T. “Speech
recognition using multilayer perceptron” Students
Conference, ISCON '02. Proceedings. IEEE, Vol.1, Iss.,
16-17
Aug.
2002
Pages: 103- 109 vol.1
[2] Alotaibi, Y.A. “Spoken Arabic digits recognizer
using recurrent neural networks” Signal Processing and
Information Technology, 2004. Proceedings of the
Fourth IEEE International Symposium on, Vol., Iss., 1821 Dec. 2004 Pages: 195- 199

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

[3] Simon Haykin, “Neural Networks: A Comprehensive
Foundation,” Second Edition, Prentice Hall 1999.
[4] Kirschning. “Continuous Speech Recognition Using
the Time-Sliced Paradigm”, MEng. Dissertation,
University Of Tokushinia, 1995.
[5] Tebelskis. J. “Speech Recognition Using Neural
Networks”, PhD. Dissertation, School Of Computer
Science, Carnegie Mellon University, 1995.
[6] L. Fausett, “Fundamentals of Neural Networks:
Architectures, Applications and Algorithms”, 1995,
Prentice-Hall International, London
[7] Rabiner, L.R.; Wilpon, J.G.; Soong, F.K. Soong,
F.K. “High performance connected digit recognition,
using hidden Markovmodels”, Acoustics, Speech, and
Signal Processing [see also IEEE Transactions on Signal
Processing], IEEE Transactions on, Vol.37, Iss.8, Aug
1989 Pages:1214-1225
[8] J. Tchorz, B. Kollmeier; "A Psychoacoustical
Model of the Auditory Periphery as Front-end for
ASR"; ASAEAAiDEGA Joint Meeting on
Acoustics; Berlin, March 1999.
[9] R. P. Lippmann, "Review of neural networks for
speech recognition,"Neural Computation, vol. 1, no. 1,
pp. 1-38, 1989.

