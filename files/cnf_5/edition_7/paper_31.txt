A Framework for Polysensometric Multidimensional Spatial Visualization
Javed Khan, Xuebin Xu, and Yongbin Ma
Media Communications and Networking Research Laboratory,
Department of Math & Computer Science, Kent State University,
233 MSB, Kent, OH 44242
{javed, xxu1, yma}@kent.edu

Abstract
Typically any single sensor instrument suffers from
physical/observation constraints. This paper discusses a
generalized framework, called polymorphic visual
information fusion framework (PVIF) that can enable
information from multiple sensors to be fused and
compared to gain broader understanding of a target of
observation in multidimensional space. An automate
software system supporting comparative cognition has
been developed to form 3D models based on the datasets
from different sensors, such as XPS and LSCM. This
fusion framework not only provides an information
engineering based tool to overcome the limitations of
individual sensor’s scope of observation but also provides
a means where theoretical understanding surrounding a
complex target can be mutually validated by comparative
cognition about the object of interest and 3D model
refinement. Some polysensometric data classification
metrics are provided to measure the quality of input
datasets for fusion visualization.

1. Introduction
3D visualization is becoming widespread and is being
used in many fields. However, a relatively new area is
comparative visualization. The methodology of correlating
information from different experimental techniques to find
the 3D structures improves scientists’ understanding about
the objects they work on. This has been recently
demonstrated in a growth in interest in combining CT
(computerized tomography) and MRI (magnetic resonance
imaging) information [3,4]. Comparative cognition is an
important and strong means to obtain new knowledge
about any complex new object or system. Multiple sets of
data are collected and compared in the process. How to
make use of these data sets is a complex issue and depends
on what objects are worked on, which field the research is
in and what purpose of the research is, etc. The most
straightforward usage is to combine the data sets to obtain

a composed model about the interest object when these
multiple sets of data are complementary to one another.
This is a data additive process for increasing the visibility
of object of interest. Current research in the area mainly
has produced very domain specific systems which fused
data from very specific modes. In our project we
investigate and propose how a generalized fusion
framework can be constructed. We propose an algorithmic
model of multi-sensor information fusion that closely
correspond the cognitive process of comparison based
knowledge exploration and hypothesis refinement. The
overlapped parts among polysensometric datasets are
registered and cropped out to form a refined 3D model,
which is closer to the real structure of object of interest
than that from single sensor. This is subtractive process for
3D model refinement. Along with it we have also
developed a new automated system. It accepts sets of
information from multiple techniques and combines them
for 3D visualization. In the system, all data are mapped
into one domain. Also, new data can be injected based on
the knowledge of the field of the system user. The system
can be easily extended to other new instrument. Naturally
the user knows the physical-chemical meaning of data and
has the proper library to process the data and accordingly
the user can insert domain specific processed and reuse
more generic available information processing tools. The
framework works as an exploration shell for its user.
The following sections of the paper explains
polymorphic visual information fusion framework,
describe our algorithms for inferring a 3D models,
introduce some data classification metrics for the quality
of input datasets to be used for fusion visualization and
present the example about 3D structural information of
liquid crystal based on XPS (X-ray Photoelectron
Spectroscopy) data and LSCM (3D Laser Scanning
Confocal Microscopy) data about the same area of a piece
of liquid crystal film.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

2. Polymorphic Visual Information Fusion
(PVIF) Framework
The basic idea is illustrated in figure 1, in which we
take a real physical-chemical experiment as example. Our
polymorphic visual information fusion model generates
estimation of 3D structure of objects, based on multiple
techniques and allows their systematic comparison and
fusion. In this example, the data, collected from XPS and
LSCM, might be modified by domain specific filters, as
well as geometric transformations (such as rotation,
cropping, zooming, tilting and data mapping, etc.). After
being filtered, their corresponding 3D models are
generated. Next, it allows these 3D models to be
visualized as a composite model. It can further generate a
fuller model by volume-filling algorithm set and measure
the differences by using 3D comparison algorithm set.
This process is extensible to additional sensor modalities.
Overall the system allows domain specific as well as
generic geometric algorithms, filters, and transformation
algorithms to be easily infused in the processing pathway
leading to the comparative 3D model.
In order to explain our information framework, we only
use two abstract techniques A and B. At the same time, we
use the following concepts and corresponding notations to
facilitate explaining our visualization strategy: (i) I - the
total information about the experimental object; (ii)
Estimation I - estimation of I from experimental data; (iii)
IA - the information generated by technique A; (iv) IB - the
information generated by technique B; (v) F - framework
for the real structure of experimental object; (vi) FA framework for the structure “seen” through technique A;
(vii) FB - framework for the structure “seen” through
technique B; (viii) FR - framework for the registration
between the intersection of frameworks A and B, i.e.

Fig. 1. polysensometric visual information fusion
model. In this figure, we derive the 3D models
from sets of information, which are generated by
different experimental techniques, and compare
these models to obtain the structure estimation
of our research object to be visualized. The
model is extensible
registration framework; (ix) FV - visualization framework
for FR; (x) Transformations T[i] - any transformation i
applied on information set; (xi) Error E - the difference
between two information sets.

In reality, for a complex target of observation we are
never able to get the real structure of the target, total I
under framework F. But we can approximate it as close as
possible to obtain estimation I’. So our objective is to
obtain estimation I and visualize it under FV, which is
close enough to F. The relations among these concepts and
notations are: the total information, I, is corresponding to
the total framework, F; IA is information about I under its
framework FA; IB is information about I under its
framework FB; FA and FB are registered towards FR based
on the intersection part between them; IAR is part of
information IA after registration under FR; IBR is part of
information IB after registration under FR; we use the
union of transformed IA and IB as the estimation of I, I; I is
visualized under FV. The inferring algorithm is showed in
figure 2 and the specific details will be covered in the next
part of this paper.

3. Algorithms
The data fusion is achieved through a set of processing:
space projection, registration, volumetric filling and
comparison, etc. This section interprets those algorithms
used to obtain reasonable data fusion results.

3.1. Sensometric
Algorithms

to

Geometric

Projection

The original input datasets are collected from some
sensors. These sensors probably have different space
coordinate systems and resolutions. In these cases, we
need algorithms to project the sensor domain sample
datasets to a common geometric space. After the original
datasets processed by these algorithms, they should have
the same spatial orientation and resolution and be ready
for the following operations. The frequently used
transformations are including zoom and tilt etc.
I * T[0] * …* T[n] -> I’
…(1)
In this formula, I is input dataset, I’ is output dataset.
T[i](0 <= i <= n) are n+1 transformations applied on input
dataset I.

3.2. Registration Algorithms
After we project the original sensometric datasets to
geometric space, we need the automated registration
algorithms to register the voxel in one spatial datasets with
the corresponding voxel in the other spatial datasets. In
our research, registration algorithms are divided into two
groups: 2D registration and 3D registration. We use
mutual information [7,8] or FFT [9] to align our datasets
in registration framework, in which the common part of
two datasets is normally 2D. Also, we use 3D registration
V
V
to fuse two 3D estimation models, IA and IB , in

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

visualization framework. Some 3D transformations are
applied on two models, and difference between them is
calculated. Difference metrics are discussed later in
comparison algorithm. Here least error method is used to
decide which transformation will be used to get the final
estimation of the real model:
V
A A
…(2)
IA = fi [I ];
V
B B
…(3)
IB = fi [I ];
V
V
…(4)
E’ = Diff(IA , IB );
In these formula, fi means ith transformation on dataset A,
Diff means difference metric calculation between two
parameters.

3.3. Comparison Algorithms
Comparison algorithms are used for inferring the final
3D model. It checks if the loop ending conditions have
been met. There are many 2D whole-image level
comparison algorithms, such as mean-absolute-error
(MAE), root-mean-square-error (RMSE), peak signal-tonoise ratio (SNR) etc. [1, 2]. In this example system, we
used comparison algorithms that are extension of those
used in 2D. The greater MAE and RMSE, the more
different these two 3D models are; SNR is on the contrary;
the closer p is to 1, the more correlated are the two sample
images.

Fig. 2. “pounding-and-inferring” algorithm for 3D
model based on sets of information from two
techniques A and B

3.4. 3D Model Inferring Algorithm
We use 3D model inferring algorithm to infer the final
3D visualization model for the object of interest (see
figure 2). The algorithm is composed of two loops. We
explain the algorithm by using two techniques, A and B,
as an example. First, the input datasets from sensors
generate 3D models of their own by using hypotheses Ha
and Hb respectively. Then, these two 3D models are
R
projected onto registration framework, F . In registration
framework, the intersection parts of FA and FB are
registered by applying a series of transformations. Least
errors are used to get the optimum registration position.
After registered, IA and IB are compared to verify the
hypotheses at the very beginning. Least error principle is
also used as the optimum means to obtain the best
theoretic 3D model about the object of interest. At last, we
use the union of IA and IB from the hypotheses with least
error as the estimation of total information I.

4. Data Classification
Our PVIF framework takes advantage of the
intersection parts among datasets extracted from multiple
sensors. So it is important to know how much intersection
there are in the provided datasets quantitatively. We

propose several metrics, such as intersection degree,
exposition ratio and inferring confidence, etc., to tell the
quality of our data for fusion visualization. In the
following section, I will use an example to explain how
these metrics are to be used.
For example, if we have a sample datasets which
contains 2 sets of data from every instrument among
LSCM, XPS and AFM. An external reference is used to
locate the shooting area for all datasets in an experiment.
The spatial relationship among these datasets is illustrated
in figure 3.

4.1. Intersection degree
We describe intersection degree as percentages of the
zones on the surface, which are covered by one dataset,
two datasets, …, and all datasets in sample, over the total
surface area shot by all instruments. From spatial
relationship illustrated in figure 3, we know some pixels
on the surface of object of interest are located in one
dataset; some are in two datasets, etc. We call it degree of
pixel. If only one dataset owns a certain pixel, then its
degree is 1; two datasets own it, then its degree is 2. The
biggest degree value for any pixel is the number of
datasets. We can count the degree of all pixels of our
datasets. After this, we can obtain the pixel number with

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

each different degree value in the datasets. Then the
percentages of each degree are calculated by the pixel
number of each degree over the total pixel number. The
sample result are illustrated one the bottom of figure 3.

2*9,000, i.e.1.8e+4. Therefore the union information of
these two datasets is the difference between 1.6e+5 and
1.8e+4, i.e. 1.42e+5.

Percentage of each group of
pixels

5. System Architecture and Working Process

100
80
60
40
20
0
1

2

3

4

5

6

Degree of Pixels

Fig. 3. top: The spatial locations of all datasets in
a liquid crystal experiment. Cyan color: XPS data;
Blue: LSCM data; Green: AFM data. bottom:
intersection degree.

The architecture of our shell system is illustrated in
figure 4. The shell is composed of such components as
task controller, flowchart object store and some other
functional parts, such as flowchart designer, data
combiner, information verifier etc. Task controller works
as a central control manager. It controls the state of our
program and coordinates the work of the functional parts.
Its work includes deletion of a flowchart object from the
flowchart store, let the flowchart designer insert a
flowchart object into the store according to the
requirement of the user, injection of input data through the
data combiner, declaration and definition of filters and
algorithms through filter/algorithm combiner, verification
of information in the flowchart object through verifer and
driving the transformation engine to process the data
following the controls in flowchart object and generating
the final model. The flowchart object store is used to store
all the information about data and operations what will be
applied on the data. Information transformation engine
drives the input data through the control of the flowchart
and generate 3D results. Also, hypothesis refinement can
be done by this component.

4.2. Exposition ratio and inferring confidence
Exposition ratio (ER) is defined as the ratio of the
union of a certain number of datasets to the sum of these
datasets, while inferring confidence (IC) is the ratio of the
intersection a certain number of datasets to the sum of
these datasets.
ER = (I1 u I2 u … u Im) / (I1 + I2 + … + Im);
…(5)
…(6)
IC = (I1 n I2 n … n Im) / (I1 + I2 + … + Im);
Before we can calculate exposition ratio and inferring
confidence using formula 5 and 6, there is one problem. It
is how we are going to calculate the information quantity
in a dataset. Our solution is to use the total shooting area
as the information in a dataset:
…(7)
I=
Area

¦

i

i

For example, the shooting area is 123 x 185 microns and
there are 35 images in our LSCM dataset. So the
information in the dataset should be 123*185*35, i.e.
7.96e+5.
So it is easy to calculate exposition ratio and inferring
confidence using our definitions and our dataset
information in table 2. For example, if we want to
calculate ER and IC between XPS1 and XPS2. First we
obtain the sum of Ixps1 and Ixps2. It is 2 *(2*(200*200)), i.e.
1.6e+5. Then intersection information of Ixps1 and Ixps2 is

Fig. 4. System architecture
There are total 5 phases to run the whole process:
design phase, name-binding phase, parameter-binding
phase, verification phase and run phase. During design
phase, a flowchart object can be designed by using
flowchart designer. There are two kinds of flowchart
object in the project: one is to take 2D experimental data
as input and generate a 3D model; another is to take 3D
models, which are generated by some flowchart object
before, as input and generate the fusion 3D object or

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

comparison between two 3D models. During name
Binding Phase, we declare filters and algorithms. Another
important thing in this phase is to bind experiment method
and image data. During parameter Binding Phase, we set
parameters for each filter and algorithm declared in the
current flowchart object. During verification Phase, all
those input information need to be verified before the data
are to be transformed following the datapath defined by
the flowchart object. During run Phase, a new 3D model
will be created from 2D or 3D data. This model will be
displayed in a separate window for further visualization. In
this phase, user can also make parameters of filters and
algorithms change on the fly in order to get a content
result. These changes will be flush back to the flowchart
aggregation component when the user pushes a button in
the 3D model presentation window.

6. Case Study
We visualize the fused information about liquid crystal
polymeric film, which was collected by using XPS and
LSCM [5,6]. The polymeric film is 2 microns thick and
composed of two non-equally distributed polymers: PVC
and PMMA. In XPS, only top and bottom surface with
0.01 microns thick into film have contribution to top
image and bottom image of 265 by 265 pixels
respectively. The dimension of the shot area is 700 by 700
microns. We also know that the highest pixel intensity in
the image corresponds to 35 percent PVC and the lowest
pixel intensity is to 5 percent PVC. LSCM generates 32
images of 600 by 800 pixels in this example. The shot area
is 300 by 400 microns. The first and last LSCM images
are in the depth 0.2 microns to the surface of the film. The
highest pixel intensity in the fourth image corresponds to
70 percent PVC and the lowest pixel intensity of the same
image is to 20 percent PVC. Also, we are ensured that the
shot area in XPS is partially overlapped with that in
LSCM. Now our task is to visualize the fusion information
based on data from XPS and LSCM. Also, we let user
choose theoretic model to fill space in between XPS top
and bottom images and then compare them with fusion
model. Thus, user can obtain a proposed theoretic model,
which is closest to experimental data.
Our system is used to process the input XPS and LSCM
datasets. First, we design two flowchart objects to process
XPS and LSCM datasets respectively. These two
flowchart objects should adjust the resolution, image
dimension, concentration mapping, etc., in order to let
them match for fusion. After 3D models of each sensor
domain are generated, we fused them to form a combined
3D model. And we also use 3D in-filling algorithms to fill
the space between XPS images to form several theoretic
3D models. Then comparison metrics are calculated and
the best proposed 3D structure of material is decided.

Fig. 5. Result images. Top: the stack of result
images by combining XPS data and LSCM data in
3D presentation. We put the top image of XPS on
the top of 1st image of LSCM and the bottom
image of XPS under the last image of LSCM.
Middle: the difference between theoretic model 1
and fusion model. MAE: 119.3, SNR: 129.8.
Bottom: the difference between theoretic model 2
and fusion model. MAE = 87.3; SNR = 99.0
The result of our example is illustrated in the attached
figures. In figure 5, a fused 3D model of XPS and LSCM
is illustrated. We put XPS top image on the top of the 1st
image of LSCM image stack and XPS bottom image at the
bottom of the last image of LSCM image stack to form a
composite 3D model based on these two techniques. Also,
we use linear insertion algorithm insert 30 images between
top image and bottom image of XPS datasets. Thus both
datasets have the same number of images. After using all
filters, they also have the same resolution, dimension and
been well registered. So we use our comparison metrics,
MAE and SNR, to compare these two 3D models. The
results are: MAE = 119.3; SNR = 129.8. The comparison
results are illustrated in figure 5. We also use another
different formula to fill the space in between the top and

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

bottom image: 2*topPixel*x+botPixel*(1-x). Comparison
between the new 3D XPS model and LSCM model is
calculated: MAE = 87.3; SNR = 99.0. So according to
MAE and SNR, the second in-filling function generates a
result closer to LSCM. The results are illustrated in figure
5.
From the above case study, we see our system can
generate a fused and theoretic 3D model based on a set of
experimental data and user-defined algorithms. The user
might propose different theoretic 3D models and compare
these theoretic models with real experimental data. Thus
the best theoretic model of chemical structure of the
material of interest could be chosen from the proposals
based on the comparison metrics.

7. Conclusion
In this paper, we propose Polymorphic Visual
Information Fusion (PVIF) Model to infer estimation of
total information of the object of interest based on partial
information from multiple instruments, and implement an
integrated system for automating to fuse the partial
information and compare different 3D models to refine the
best operations for a certain dataset and testify the validity
of a theoretic 3D model. Our system can be used as a tool
for chemists and physicists to explore the structure of a
material.
There are still a lot of interesting and complicated
issues which would need more attention. Among these
difficulties, 2D/3D registration algorithm itself is a
challenge in image processing; how to find a good 3D
comparison metric is also interesting; how to describe a
proposed 3D model is not an easy job. Rendering speed
problem is also important in the fields of image processing
and graphics. The work has been supported by National
Science Foundation (NSF) Grant NSF-0113724.

References
[1] .Kwansik Kim, Alex Pang, Ray-based Data Level
Comparisons of Direct Volume Rendering Algorithms. Technical
Report
UCSC-CRL-97-15,
UCSC
Computer
Science
Department, 1997.
[2] Rafael Gonzalez and Richard Woods, “chapter 8, image
compression”, Digital Image Processing, 2nd edition, prentice
Hall, 2001, pp419-420.
[3] Schiers C, Tiede U, Hohne KH 1989, Interactive 3Dregistration of image volumes from different sources. In: Lemke
HU et al. (eds), Computer Assisted Radiology (Proc. CAR ’89),
Springer, Berlin.
[4] Hohne, K.H., Bomans, M, et al, 3D-Visualization of
tomographic volume data using the generalized voxel-model, CH
Volume Visualization workshop.
[5] Laurie A. Broadwater, “The correalation of X-Ray
Photoelectron Spectroscopy And Laser Scanning Confocal
Microscopy”, thesis, Kent State University, May, 2003.
[6] Kateryna Artyushkova, “Application of Multitechnique
Correlation And Multivariate Analysis To Heterogeneous
Polymer Systems”, dissertation, Kent State University, Dec.,
2001.
[7] William M. Wells, Paul Viola, Hideki Atsumi and Shin
Nakajima, Multi-Modal Volume Registration by Maximization
of Mutual Information, Medical Image Analysis, vol. 1, no. 1,
pp. 35--51, March 1996.
[8] Michael E. Leventon and Eric L. Grimson, Multi-Modal
Volume Registration Using Joint Intensity Distributions, in
Medical Image Computing and Computer-Assisted Intervention MICCAI'98, ed. by W.M. Wells, A. Colchester, S. Delp,
Cambridge, MA, Springer LNCS 1496, pp 1057.
[9] P.E. Anuta, Spatial registration of multispectral and
multitemporal digital imagery using fast fourier transform
techniques. IEEE Trans. on Geoscience Electronics, GE8(4):353--368, October 1970.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

