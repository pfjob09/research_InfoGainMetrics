Imaged-Based 3D Face Modeling
Mandun Zhang, Linna Ma, Xiangyong Zeng, Yangsheng Wang
Institute of Automation, Chinese Academy of Sciences, 100080, China
{mdzhang, lnma,wys}@nlpr.ia.ac.cn
Abstract
In this paper, we present an image-based 3D face
modeling algorithm. Different from traditional complex
stereo vision procedure, our new method needs only two
orthogonal images for fast 3D modeling without any
camera calibration. The proposed method has two steps.
Firstly according to MPEG-4 protocol for 3D face
structure, we appoint and deform feature points by Radial
Basis functions (RBF) in the input images corresponding
to the generic model. Then the texture mapping is carried
out with regard to different directional projections. The
experiments demonstrate that our new algorithm can
photo- realistically render 3D face with very limited
computation.

1. Introduction
From the viewpoint of both computer vision and
computer graphics, fast 3D facial modeling and animation
is one of most challenging and interesting research topics.
And facial animation has been applied in
human-computer
interfaces,
interactive
games,
multimedia titles, VR, and, as always, in a broad variety
of animations production. Moreover it also becomes a
hotspot especially in recent years because of online game
burgeoning. One of the most noticeable trends is the
boom toward “realistic” looking and feeling games. If the
player can become a role and see a realistic copy of the
self within a game system, the game will gain more
attention from its player.
By now various 3D face modeling approaches, such as
a Laser scanner [1,2,3], a stereoscopic camera [5], or an
active light stripper [4], have been proposed. However
this kind of facial 3D data only include shape information
without texture information, and always easily
contaminated by measuring noise. It’s difficult for them
to be used in facial animation. The procedure of the facial
modeling and animation can be summarized as follows: 1)
a structural generic model selection, 2) a specific face
information based on computer vision or based on image

acquirement, 3) the generic model deformation, and 4)
texture mapping.
It is computational expensive to apply these
approaches based on computer vision. Therefore many
research efforts have been made to generate realistic
facial modeling from photos taken with an ordinary video
camera. Reconstructing the true 3D shape and texture of a
face from a single image is an ill-posed problem [14]. It’s
a generic method to reconstruct an individual facial
modeling from two orthogonal photos [6,7,8,9,10,12], but
they are too complex and inconvenient for serious
applications. When generating model from two arbitrary
pictures [16], if skins are so smooth and free of blemishes,
the corner point matching may fail. Creating realistic
textured 3D facial models from multiple photographs of a
human subject [13,15] is theoretically possible, but
computationally too intensive.
In this paper, we advance a practical approach for
reconstructing an individual facial modeling from
orthogonal photos. Firstly the 3D positions of key feature
points from photos are devoted to deform a generic model
with corresponding feature points using RBF. Then
texture mapping is performed for realistic modeling. The
rest of this paper is organized as follows. Section 2
describes how to choose a generic model. Section 3
describes our technique for editing image-based facial
feature points. Section 4 describes deformation algorithm
and texture mapping. The results are shown in Section 5
and are concluded in Section 6.

2. Generic model choice
There is the same basic structure such as eyes, nose
and mouth etc for different people. It’s easy for us to
identify a generic model with them. Everyone has
different features that make one unlike others. A generic
model should be a structural one for facial animation. The
wireframe model is gettable from much software such as
3DMAX, Poser or Maya. It’s usually chosen as a facial
model.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

(lx) is the horizontal distance between point 3 and point 9,
and the x coordinate of point 6 is in the middle of point 3
and point 9.

3.3. Feature points edit
The more facial feature points are, the more precise the
face shape is. But more feature points will make feature
points editing time-consuming. Therefore there is a
tradeoff between the number of feature point and editing
time. In general, we choose more feature points according
to MPEG-4.
(a) Feature points identifying

(b) Feature points

Figure 1. A generic model

We choose a general facial model derived from Poser
5.0 that is made of 8172 3D vertexes and 15980 triangles.
It could reflect tiny facial features.
According to MPEG-4, there are 84 feature points
defined on a neutral face that provides referenced space
for defining facial animation parameter (FAP). They
suffice for identifying proper shape of facial model.
Feature points are divided into several groups such as lip,
eyes and mouth etc. There are 35 feature points selected
according to MPEG-4 and facial shape. Figure 1 shows
their position.

3. Image-based facial feature points editing
According to projection theory, if the front and side
projections of a point could be identified, the 3D position
of the point is confirmed. This principle can help us to
acquire 3D position of facial feature points from two
orthogonal photos. Firstly a datum mark is chosen. Its
position is the same as that on the generic model. We
choose the chin as the datum mark (point 6 in Figure 1
(b)), and the 3D position of other points relative to it may
be identified.

3.1. Normalization
To make the head size of side and front views the same,
we measure the lengths of a face in two views. Then we
choose two datum marks (point 9 and point 6 in Figure
1(b)) from each view to match them with the
corresponding points. Then we use scale transformation
or translation to make two photos normalization. Figure 2
shows the result from normalization process.

3.2. Feature points data of the template input
The template is formed according to feature points’
position identified on the generic model. It promotes
feature points edit. When inputting feature points data of
the template, it’s necessary to adjust their position
according to two datum marks on every photo. The width

Figure 2. Feature points editing

In this paper, the functions of editing feature points can
be summarized as follows (Figure 2):
1. Feature points are symmetrical on both side of the
front photo. When adjusting feature points on one side,
corresponding points on the other side will be moved at
the same time.
2. The method of feature points editing is that clicking
right key of the mouse for the first time picks up one
point (when the distance error is less than the threshold),
and clicking it for the second time is where to be moved.
3. When feature points on the side picture are moved,
those points on the front picture will be moved
correspondingly (only the y coordinate changes).
4. When feature points on the front picture are moved,
those points on the side picture will be moved
correspondingly (only the y coordinate changes).

3.4. Feature points data saving
After editing feature points, we estimate the whole size
of the individual head. The width (lx) and the height (ly)
are decided by the front view, and the depth (lz) is
determined by the side view:
lx is equal the horizontal distance between point 3 and
point 9(Figure 1(b)).
ly is equal the vertical distance between point 1 and
point 6.
lz is equal two times of the horizontal distance between
point 1 and point 6.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

According to the whole size of the individual head, we
use scaling transformation to adjust data of the generic
model which will be used for deformation in next section.

4. Deformation and texture mapping
4.1. Scattered data interpolation
The facial model adjustment is a problem about 3D
space deformation in fact. We identify limited control
points on it and compute their displacements, then choose
an interpolation function which accommodates
displacements of control points. Locations of the rest
nodes are transformed upon the function. At last the final
deformation result of the model is reached.
There are several choices for how to construct the
interpolating function. We use a method based on radial
basis functions (RBF) as approximation functions for
their power to deal with irregular sets of data in
multi-dimensional space in approximating high
dimensional smooth surface [15,19]. We choose 3D
coordinates of origin mesh nodes as embedding space to
construct the interpolation function f(p) that suffices
displacements of control points:
ui =f (pi ) ˄0İ i İ N -1˅ˈ
˄1˅
Where ui is displacements of all control points and N is
the number of feature points. We exploit RBF volume
morphing to directly drive 3D geometry deformation of
face models.
f (pi) =ěci¶(ýpj - piý) ˄0İjǃiİ N - 1˅ ˄2˅
where ¶˄ýpj - piý˅are RBF and the coefficients ci
are the vector coefficients of control points.
We compute equation (2) and get the vector coefficient
of every control point. Displacements of other non-feature
points may be computed in the form
u = ěci¶(ýp - piý) (0İ i İ N - 1)
(3)
In this paper, we have chosen to use ¶˄ýp - piý˅=
e -ýp - piý/ 64.

4.2. Multiresolution image mosaic
The three resulting images are merged with the
pyramid decomposition method using the Gaussian
operator [11,17,18]. We utilize REDUCE and EXPAND
operators to obtain the Gk (Gaussian image) and Lk
(Laplacian image). Then we combine three Lk images on
each level. Then the combined image Pk is augmented
with Sk+1 to get Sk, which is the result for each level. The
final image is S0. This Multiresolution technique is very
useful to remove boundaries between the three images.
Figure 3 shows the merging image.

Figure 3. The merging result

4.3. Texture fitting
The width of the front view is w2 and the width of the
side view is w1. The total width of the image is
w= 2 u w1 + w2 . The x coordinate of point 9 (Figure 1 (b))
is xmax and the x coordinate of point 3 is xmin. The y
coordinate of point 1 is ymax and the y coordinate of
point 6 is ymin. The z coordinate of point 20 is zmax. The
z coordinate of afterbrain should be zmin, but in this
paper we don’t mark this point. We choose two times of
the z coordinate of point 1 as zmin for simplification. The
3D coordinates of every point are x, y and z. We mark
texture coordinates of every point with xtex and ytex.
According to linear interpolation, if one point is projected
into the front, the left or the right view, the texture
coordinates of every point are:
­­xtex w1 /w + ( x - xmin)/(xmax - xmin) u w2/w
(4)
°°
(5)
°®xtex w1/wu ( z - zmin)/(zmax - zmin)
®°
xtex
(w1
+
w2)/w
+
(
z
zmax
)/(
zmin
zmax
)
u
w1/w
(6)
°¯
°¯ ytex ( y - ymin) / ( ymax- ymin)
(7)
For example, if it’s projected into the left view, its texture
coordinates are derived from (5) and (7).

5. Experiment results
The implementation of the above described individual
face generation system is written in VC++ and OpenGL.
A detailed individualized head is shown in Figure 4
where input images are shown in Figure 2. It has proper
shape and texture.

Figure 4. Snapshots of reconstructed heads

in several views

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

Other examples from orthogonal picture data are
shown in Figure 5. The texture image is stored in BMP
format and has sized about 200~3000K depending on the
quality of pictures.

Figure 5. Examples of reconstructed heads

from pictures

6. Conclusion and future work
In this paper, we introduce an image-based approach to
reconstruct facial modeling from two orthogonal photos.
The efficient and robust face modeling method shows the
process of modifying a generic head for shape modifying
and producing texture images by combining orthogonal
image pair smoothly. It’s a simple, quick and economic
method on which facial animation is based.
The emphasis in this paper has been on the quality of
the results. Possible ways to improve it include:
z choosing more generic models to reduce
deformation error
z identifying more feature points to make
deformation precise according to MPEG-4
z improving merging image to make it better.

7. Acknowledgment
We are grateful to people taken photos. This project is
supported by Chinese National Research Foundation.

8. References
[1] Y.C. Lee, D. Terzopoulos, and K. Waters, “Realistic Face
Modeling for Animation”, Siggraph proceedings, 1995,
pp.55-62.
[2] Y.C. Lee, D. Terzopoulos, and K. Waters, “Constructing
Physics-based Facial Models of Individuals”, In Proceedings of
Graphics Interface, 1993, pp. 1-8.
[3] Yu Zhang, Edmond C. Prakashb, and Eric Sunga,
“Constructing a Realistic Face Model of an Individual for
Expression Animation”, Animation International Journal of
Information Technology Vol. 8, No. 2, September 2002,
pp.10-25.

[4] C. Beumier, and M. Acheroy, “3D Facial Surface
Acquisition by Structured Light”, In International Workshop on
Synthetic-Natural Hybrid Coding and Three Dimensional
Imaging, Santorini, Greece, 15-17 Sep, 1999, pp.103-106.
[5] O.Faugeras, L.Quan, and P.Sturn, “Self-calibration of a 1D
Projective Camera and its Application to the Self-calibration of
a 2D Projective Camera”, IEEE Transactions on Pattern
Analysis and Machine Intelligence (PAMI), Vol.22, No.10, 2000,
pp. 1179 – 1185.
[6] Wen Gao, Jie Yan, Shigong Shan, and Xilin Chen,
“Individual 3D Face Synthesis Based on Orthogonal Photos and
Speech-Driven Facial Animation”, 7th IEEE International
Conference on Image Processing (ICIP 2000), Vancouver,
British Columbia, Canada, 10-13 September, 2000.
[7] Jie Yan, and Hongjiang Zhang. “Realistic Virtual Face and
Body Synthesis”, MVA2000 (International Workshop on
Machine Vision Applications), Tokyo, Japan, November 28-30,
2000.
[8] W. Lee, Kalra P., and N. Magenat Thalmann , “Model Based
Face Reconstruction for Animation”, Proc. Multimedia
Modeling (MMM) ’97, Singapore, 1997, pp. 323-338.
[9] W. Lee, and N. Magnenat-Thalmann, “Fast Head Modeling
for Animation”, Journal Image and Vision Computing, Volume
18, Number 4, Elsevier, March, 2000, pp.355-364.
[10] W. Lee, J. Gu, and N. Magnenat-Thalmann, “Generating
Animatable 3D Virtual Humans from Photographs”,
Eurographics, Paris, 2000.
[11] W. Lee, and Nadia Magnenat Thalmann. “Head Modeling
from Pictures and Morphing in 3D with Image Metamorphosis
Based on Triangulation”, In: CAPTECH’98, LNAI 1537,
Springer_Verlag Berlin Heidelberg, 1998, pp.254-267.
[12] Horace H.S. Ip, and Lijin Yin, “Constructing a 3D
Individual Head Model from two Orthogonal Views”, the Visual
Computer, Springer-Verlag, 1996, pp.254-266.
[13] Chia-Ming Cheng, and Shang-Hong Lai. “An Integrated
Approach to 3D Face Model Reconstruction from Video”,
Proceedings of the IEEE ICCV Workshop on Recognition,
Analysis, Tracking of Faces and Gestures in Real-Time Systems
(RATFG-RTS'01), July 2001.
[14] V. Blanz, and T. Vetter, “A Morphable Model for the
Synthesis of 3D Faces”, In Proceedings of ACM SIGGRAPH 99,
AddisonWesley, New York, A. Rockwood, Ed., Annual
Conference Series, pp.187-194.
[15] F. Pighin, J. Hecker, D. Lischinski, R. Szeliski, and D. H.
Salesin, “Synthesizing Realistic Facial Expressions from
Photographs”, Siggraph proceedings, 1998, pp. 75-84.
[16] Zicheng Liu, Zhengyou Zhang, Chuck Jacobs, and Michael
Cohen. “Rapid Modeling of Animated Faces from Video”,
Technical Report, MSR-TR_2000-11.
[17] P. J. Burt, and E.H. Adelson. “A Multiresolution Spline
with Application to Image Mosaics”, ACM Transactions on
Graphics, Vol. 2, No. 4, October 1983, pp. 217-236.
[18] P. J. Burt, and E.H. Adelson. “The Laplacian Pyramid as a
Compact Image Code”, IEEE Trans.Commun. COM-31, (1983),
pp. 532-540.
[19] D. Ruprecht, R. Nagel, and H. Müller: “Spatial Free-Form
Deformation with Scattered Data Interpolation Methods”,
Computers & Graphics 19(1), 1995, pp.63-71.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

