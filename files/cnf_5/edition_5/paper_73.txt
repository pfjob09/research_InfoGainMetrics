Usability Evaluation of a Mixed Reality Collaborative Tool for Design Review
Xiangyu Wang1 and Phillip S. Dunston2
1
Lecturer, Key Center of Design Computing and Cognition, School of Architecture, Design
Science and Planning, University of Sydney, Sydney NSW 2006, Australia. Phone 61-2-90367128, Fax 61-2-9351-3031
2
Associate Professor, Purdue University, School of Civil Engrg., 550 Stadium Mall Dr.,
West Lafayette, IN 47907-2051, USA. Phone 765-494-0640, FAX 765-494-0644
{xiangyu@arch.usyd.edu.au, dunston@purdue.edu}
possess non-intuitive aspects for end users or require
significant training to use efficiently.
The exciting new technology of MR has not yet
been rigorously developed with regard to the
important characteristic of usability, a necessary
coupling if MR systems are to reach their full
potential. MR system developers are typically
occupied with technical hardware and software issues
and as such, give nominal attention to designing usercentered applications, formally validated by usability
evaluations. The migration from technically appealing
MR prototypes to truly usable and performanceenhancing commercial MR systems will require
system usability evaluation as well as technology
development. A focus of our work is creating,
evaluating, and enhancing the human-computer
interface in MR systems.

Abstract
Formal usability evaluation of Mixed Reality
(MR) environments to date has received very little
attention in MR literature and research. While
awareness of the need for usability evaluation of MR
appears to be on the rise, the techniques needed to
perform efficient, meaningful usability evaluation of
these environments are not yet available. This paper
presents how the usability of MR system interfaces
could be evaluated by using a combination of heuristic
evaluation and formative user testing. This paper
demonstrates by applying in assessing an MR-based
collaborative virtual environment prototype for
mechanical design review — Mixed Reality-based
collaborative virtual environment (MRCVE). The
evaluation was implemented through a devised
experiment where human subjects were invited to use
the MRCVE system for identifying design errors
collaboratively from virtual mechanical models.
Usability suggestions were also identified for the
interface of future versions of MRCVE.

2.
MR-based
prototype

collaborative

design

The usability evaluation method presented in this
paper has been applied to an MR prototype—a Mixed
Reality-based collaborative virtual environment
(MRCVE)—where 3D designs can be realized to
appear in space among meeting members, modifiable
by some individuals equipped with 3D CAD editing
capability. The MRCVE system is configured as a
large table with several laptop or desktop computers.
Several experts convene around a table to discuss a
design product. 3D objects are projected into the
common collaboration environment of the users by
means of wearable ARvision-stereoscopic HMDs with
color video cameras (real environment sensors)
attached (see left panel of Figure 1). Tangible
interfaces enable the integration of convenient
arbitrary objects into the real collaboration
environment for effective object manipulation. The

1. Introduction
While the need for usability assessment of
computer interfaces of virtual reality environments has
been gaining well-deserved attention, usability
assessment of more advanced Mixed Reality (MR)
technology has lagged behind. Mixed Reality is a
special class of Virtual Reality (VR)-related
technologies for creating environments wherein real
world and virtual world objects are presented together
in a single display.
Military, government,
commercial, and industrial organizations are investing
enormous amounts of effort and resources to produce
MR technology.
More often than not, these
technologies are very expensive to develop and yet

1

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

seamless blend of real and virtual objects facilitates
collaboration between the partners, who can each see
the same spatially aligned 3D virtual model but can
control their viewpoints independently through their
HMDs. By developing multiple-marker technology
(see left panel of Figure 1), the user can easily obtain a
stable bird’s-eye-view of a large-scale virtual design
without losing the line of sight with the small markers.
Interaction with the models can be implemented
through tangible interfaces. Tangible interfaces
(tracking markers) enable the integration of
convenient arbitrary objects into the real collaboration
environment for effective object manipulation. The
tracking marker is actually a small-size real cube
overlaid with a virtual ball. By using the tracker ball,
the user can track his/her hand’s position spatially
relative to the viewed virtual model via knowledge of
the spatial relationship between the virtual model and
the tracker ball.

3. Methodology: usability evaluation
User-based evaluation is an essential component
of developing any interactive application, and is
especially important for applications as complex and
innovative as MR environments. Usability evaluation
is a process that aims to identify usability problems in
user interface design (Mack and Nielsen 1994). Noted
problems are then used to derive recommendations for
improving the interface design. The method presented
herein uses a combination of heuristic evaluation and
formative usability evaluation. While the concept of
combining heuristic evaluation and formative user
testing is not necessarily new (Nielsen 1994), applying
these methods to an MR-based user interface is novel.
Expert heuristic evaluation (Gabbard 1999) is a
type of analytical evaluation in which an expert in user
interaction design assesses a particular user interface
by determining what usability design guidelines it
violates and supports. Then, based on these findings,
especially the violations, the expert makes
recommendations for changes to improve the design.
MR-based user interfaces are considered to be an
emerging, immature technology. As such, a formal
and standard set of usability guidelines for MR-based
interfaces does not exist. Thus we developed design
guidelines and specifications for MR systems, which
could be used to choose appropriate technology
characteristics including information representations,
interaction methods, and tracking technology for a
more directive or specific task category. Gabbard
(1997) also developed an informal set of MR usability
guidelines based on his experience in human-computer
interfaces in VR systems. The combination of
Gabbard’s approach with ours provides a reasonable
starting point for usability evaluation of the MRCVE
system. In heuristic evaluation, components of the
MRCVE system and interaction techniques under
assessment are compared against usability guidelines
contained in the framework of usability characteristics
(our guideline and Gabbards’ guideline). In our
heuristic evaluation results, we first follow the
usability design guideline, and then we identify and
explain the non-compliant interface components or
interaction techniques. Possible remedies for were
also determined. Results from the heuristic evaluation
are subsequently used to remedy obvious and critical
usability problems as well as to shape the design of
formative evaluations. The guideline and results are
not included in this proposal due to the length limits.
Formative evaluation (Del Galdo et al. 1986) is a
type of empirical, observational assessment with users
that begins in the earliest phases of user interaction
design and continues throughout the entire life cycle,

Figure 1. MRCVE face-to-face collaboration
scenario

Figure 2. Virtual tracking ball

2

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

they should expect. In the real experiment, the test
model was presented to the group in the format of the
MR scene. The goal of subject groups was to
collaboratively identify the design errors (using the
tracking ball shown as virtual white ball in Figure 2)
based on their knowledge of the given error patterns.
The subjects were to complete the task in a
collaborative manner by locating three errors specified
and the total time spent was then measured and
documented for each group.

with the purpose of iteratively and quantifiably
assessing and improving the user interaction design.
An experiment was devised and implemented for the
formative evaluation, which yielded important
usability suggestions for future MRCVE systems.

4. Experiment
Formative evaluation of MRCVE was designed to
follow the typical formative path:
• Develop user task scenario(s)
• Invite users to perform scenario(s)
• Collect qualitative usability data
• Suggest improvements for future user
interaction design

5. Discussion
5.1. Implications of task load index ratings
MRCVE was evaluated in terms of average
ratings for the six TLX categories. The lower than
average mental demand rating (8.7/20) implies that
perceptual activities such as deciding, remembering,
looking, and searching were not very intensive.
Without the need for reasoning the spatial relationship
of objects in the virtual design, users perhaps felt less
frustrated or discouraged than with the 2D design (3D
image on paper media), which in turn perhaps induced
less temporal stress (sense of time pressure). These
considerations can explain why the average ratings of
both frustration level and temporal demand are
relatively low (frustration score: 7.5/20; temporal
demand score: 8.7/20). Due to the lower frustration
level, the subjects were satisfied with their
performance in accomplishing the task goal, which is
supported by the much lower average performance
rating (performance score: 6.8/20; the higher the
score, the poorer the subjects think they performed).
A physical demand score in using the MRCVE system
(13.9/20) stems from the users having to wear
burdensome and bulky head-mounted displays. Also,
the number and limited lengths of cables created
difficulties for moving around.

4.1. Measuring methods
Two types of measurement methods were used:
NASA Task Load Index (NASA TLX) and a usability
questionnaire. Mental workload perceived by subjects
was measured using the NASA Task Load Index
(NASA TLX). Using a 20 point scale, subjects rated
each of the 6 categories (mental demand, physical
demand, temporal demand, effort, performance,
frustration level) based on their experience in the
experiment. They were then asked to perform pair
wise comparisons, indicating which category is more
important corresponding to the task among the 15
possible pairings. A mean weighted workload score
was calculated by adding up the rating multiplied by
an appropriate weighting for each category. Related
usability questions were designed and included in the
questionnaire. With regard to the questionnaire rating
scale, a scale of 1 to 6 (from Very Little to Very
Much) was used.

4.2. Task, materials, and subjects
The user task scenario was to review and inspect a
mechanical design model and then figure out the
design errors. A sample model obtained from an
industry partner was used to simulate the real-world
scenario. We chose a relatively complex and large
model. Sixteen (16) graduate student participants (8
groups with 2 assigned in each group) were recruited
to participate in the study.

5.2. Implications of data from questionnaires
Sample results of questions and usability
suggestions are illustrated in Table 1. The first
column lists the usability issues considered, the second
gives the mean rating value for each usability issue
from the opinions of users, the third explains what
interface component or interaction techniques involve
the usability issue and the reason, the fourth column
lists a possible remedy.
From the questionnaire, about 81.3% (13) of
responders believed that they would not be resistant to
using MRCVE or similar MR systems in the future.
About 69% (11) believed that they would embrace the

4.3. Procedure
Each team consisted of two collaborators. Before
the experiment, in order to finish the design error
identification tasks in the experiment, the novice
subjects were informed regarding the kinds of errors

3

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

opportunity to use the MRCVE system again in the
future. Almost none of the users experienced “blurred
vision”; “dizziness”; “nausea”; “difficulty focusing”;

“loss of vertical orientation” after exposure to the
MRCVE system.

Table 1. Results and interpretation of usability analysis
Issues

Did you often feel disoriented?

Mean

4.0

Summarized Results

Usability Suggestions

Users felt a little disoriented with nothing in
the mixed scene to use as navigational cues or
landmarks. This was expected from the
heuristic evaluation phase.

The presence of directional cues, such as onscreen compass or, a navigational grid and/or a
navigational map may have a positive effect on
users' ability to perform navigational tasks.

•
Is the AR system comfortable for
long-term use?

1.69
•

Is tracking marker lightweight,
portable, non-encumbering, and
comfortable thereby avoiding
issues of limiting your mobility
and fatigue?

•
4.63

•

Very low score demonstrates the most
important weakness of the HMD. Bulky
HMD and limited cable length
compromise the most significant usability
issue of MRCVE system.
Easy user fatigue: Tethered by video
cabling, limiting user mobility to cable
length.

High-bandwidth wireless HMD or large screen
projector.

Tracking ball appears to have high
mobility, being light and portable.
The current size of the ball may occlude
virtual objects or the entire virtual display
as users are prone to holding the props.

Reduce the size of the virtual tracking ball

[3] Gabbard, J. L. et al., (1999). “Usability Evaluation
Techniques: A Novel Method for Assessing the
Usability of an Immersive Medical VE” Proceedings of
Virtual Worlds and Simulation Conference (VWSIM
99), Society for Computer Simulation Int’l, San Diego,
Calif., 165-170.
[4] Mack, R. L. and Nielsen, J. (1994). “Executive
summary.” In Usability Inspection Methods, John
Wiley & Sons, chapter 1, 1-23.
[5] Nielsen, J. (1994). “Heuristic evaluation.” In Usability
Inspection Methods, chapter 2, John Wiley & Sons, 2562.

Conclusions
This paper presented how the usability of MR
system interfaces could be evaluated by a method
using a combination of heuristic evaluation and
formative user testing. As an example, this paper also
presented the application of this method to refine an
MR prototype user interface, a Mixed Reality-based
collaborative virtual environment (MRCVE). The
formative evaluation was implemented through a
devised experiment where human subjects were
invited to use the MRCVE system for identifying
design errors collaboratively from virtual mechanical
models. Usability suggestions were identified for the
interface of future versions of MRCVE.

References
[1] Del Galdo, E. M. et al. (1986). “An Evaluation of
Critical Incidents for Software Documentation Design”
Proceedings of 30th Annual Human Factors and
Ergonomics Society Conference, Human Factors and
Ergonomics Society, Santa Monica, Calif., 19-23.
[2] Gabbard, J. (1997). “A Taxonomy of Usability
Characteristics in Virtual Environments.” MS Thesis,
Virginia Polytechnic Institute and State University.

4

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

