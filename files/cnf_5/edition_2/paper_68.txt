2009 Sixth International Conference on Computer Graphics, Imaging and Visualization

A Dynamic Clustering Algorithm Based on Small Data Set1
Tao Peng, Minghua Jiang, Ming Hu
pt0403@163.com, jmh@wuse.edu.cn
College of Computer Science of Wuhan University of Science and Engineering
Abstract:
The traditional clustering algorithms are designed
for large dataset or vary large dataset. It is not
easy to cluster the small dataset because of the
loss of the statistical character and probability
character. In this paper, the class ration is
introduced, based on the class ratio, the dynamic
clustering algorithm is proposed. The dataset are
divided into all possible classes, and the class
ratios are computed, the min class ratio is chosen
and the clustering about the min class ratio is the
best clustering. With the experiments, the schema
is an effective way for the clustering of small data
sets.
1 introduce
Recent computer, internet and hardware
advances produce massive data which are
accumulated rapidly. Applications include sky
surveys, genomics, remote sensing, pharmacy,
network security and web analysis. Undoubtedly,
knowledge acquisition and discovery from such data
become an important issue. One common technique
to analyze data is clustering which aims at grouping
entities with similar characteristics together so that
main trends or unusual patterns may be discovered.
Clustering is an example of unsupervised learning.
There is no provision of any training examples to
guide the grouping of the data. In the other words,
cluster analysis can be applied without a priori
knowledge of the class distribution. The traditional
clustering algorithms are focus on large datasets or
massive datasets, the clustering of the small datasets
also plays important roles in pattern recognition.

Usually, a measure of within-cluster similarity

2 related works
(i) Optimization-based methods. They seek for
partition of the dataset so as to optimize an objective.

Methods of this type include Valley Seeking [6],

maximized and/or a measure of between-cluster
is similarity is maximized. Constraints such as
umber of clusters or minimum separation
between clusters may be incorporated [1].
Perhaps the most ell-known algorithm of this
type is the Lloyd‚Äôs algorithm for optimizing
the k-means objective [2]. A generalization of
the k-means objective and Lloyd‚Äôs Algorithm to
Bregman Divergence can be found in [3].
Algorithms

for

optimizing

the

k-medoids

objective, a variant of k-means which is ore
robust to outliers, include PAM, CLARA and
CLARANS.
(ii) Hierarchical methods. They aim at
producing a hierarchical tree (dendrogram)
which

depicts

the

successive

merging

(agglomerative methods) or splitting (divisive
methods) of clusters. Examples of hierarchical
methods include DIANA, Single Linkage, and
Average Linkage, Complete Linkage, Centroid
and Ward‚Äôs methods . Different agglomerative
methods differ by the way that the similarity
between two clusters is updated. If the linkage
satisfies a cluster aggregate inequality, then the
algorithm can be implemented efficiently at a
time complexity of O(N2) only [4]. A more
recent method CHAMELEON [5] uses a
sophisticated merging criterion which takes the
clusters‚Äô internal structure into account as well.
(iii) Density-based methods. They are
based on the idea that clusters are high density
regions separated by low density regions.
DBSCAN [7], GDBSCAN [8], CLIQUE [9],
Our approach is distance-based. We will further

1

This paper is supported by Foundation of Wuhan University of Science and Engineering (No:
20073205) and Hubei Provincial Natural Science Foundation of China (No: D200717005 )

978-0-7695-3789-4/09 $25.00 ¬© 2009 IEEE
DOI 10.1109/CGIV.2009.78

410

elaborate the pros and cons of this paradigm in the

Ai is the small dataset.

next section.
(iv) Grid-based methods. The feature space is

3.2 Clustering of the small dataset

projected onto a regular grid. Presumably, most

If we divide the set Ai into two parts, there

nonempty grid cells are highly populated. Thus, by
using a few representatives or summary statistics for

is a simple way:

each grid cell, a form of data compression is obtained.
STING and Wave Cluster fall into this category.

then

3. Distance-based dynamic clustering
The clustering of small datasets is different from
the traditional clustering of large datasets. Such as:
the statistical character and probability character are
lost. In this paper, we proposed a schema to cluster
the small data set.
Distances are normally used to measure the
similarity or dissimilarity between two data objects
Some popular ones include: Minkowski distance:

d(i, j) q (|x x |q ...| x x |q)
i1 j1
ip jp

aij > O (0<j<5, O is the threshold),

If

Such an approach is usually used for large databases.

aij ¬è C1 ,else aij ¬è C 2 ;
There are two special conditions: if

O > max(a ) or O  min(aij ) , the set Ai
ij

just one class.
This schema is very simple and has deadly
shortcomings: how to decide the threshold O .
Especially, when we apply the schema on the set

A { Ai }(1 d i d n) ,(n is the size of the set

O

A), the threshold
in A. for example:

(1)

Ai

where i = (xi1, xi2, ‚Ä¶, xip) and j = (xj1, xj2, ‚Ä¶, xjp)
are two p-dimensional data objects, and q is a
If q = 1, d is Manhattan distance

A j , O = 32 suit to the A j but not
Ai . So, this schema is not the good

suit to the

If q = 2, d is Euclidean distance:

{35,33,21,20} ,

Ai , O =40 will work well, but it not suit

the set

(2)

can not suit to all the set

{50,35,45,48}, A j

for

positive integer

d (i, j) | x  x | ... | x  x |
i1 j1
ip jp

is

way.
3.3 Distance-based dynamic clustering

d (i, j)

(| x  x | ...| x  x | )
i1 j1
ip jp
2

2

(3)

The

principle

of

distance-based

dynamic

clustering is: the distance of the inner class more

Properties:

small and the distance among classes larger.

1 d(i,j) t 0

Definition 1: Center: the average of elements in

2 d(i,i) = 0

the Set

3 d(i,j) = d(j,i)

Ai is the center, marked as Ci .

4 d(i,j) d d(i,k) + d(k,j)

1 m
¬¶ aik
mk1

Ci

3.1 small dataset
The small dataset is the datasets which contains

(4)

less than 10 elements. In this paper, the size of the

Definition 2: distance of the inner class is

small dataset is 4.

defined as :

For example, the set Ai

{ai1 , ai 2 , ai 3 , ai 4 }

Di

m

¬¶a

ik

k 1

411

 Ci

(5)

Definition 3: distance between two classes is
defined as:

Ci  C j

Dij

A2 ={64Àà63Àà62Àà59}

A3 ={65Àà64Àà27Àà23}

(6)

With the schema we proposed, we get the
result:

Definition 4: class Ratio R is defined as:

Di  D j

R

(7)

Dij

R
1.8

The algorithm is described as followed:

1.6

aik ¬è Ai , 3  k  10

1.4

Rmin

0.8

1.2
1

1000 ;

0.6

q=0;

0.4
0.2

Bi =Sort( Ai ) and Bi is descending;

0
abcd

abc d

ab cd

a bcd

For p=1 to 10
Class1 = {bi1,..bip}, Class2={bip+1,..bin}(if
p=10, Class2= ) );
With (4),(5),(6),

Figure 2 class Ratio of set A1

D p1 , D p 2 and D p1 p 2

R
1
0.8

are computed.

Rp

D p1  D p 2

0.6

D p1 p 2

0.4

if R p < Rmin , Rmin = R p ,q=p;

0.2
0

end for:

a bcd

Class1 = { bi1,..biqp },Class2={ bip+1,..bin }

ab cd

abc d

abcd

3.4 Experiments

Figure 3 class ratio of set A2

We get the mark information from the documents
imagesÀñ

2
1.5
R1
0.5
0

Figure 1 mark information
Our task is to decide which parts are marked. We
get the mark information from the document and
get ;

a bcd

ab cd

abc d

-0.5

Figure 4 class ratio of set A3

A1 ={55Àà52Àà51Àà33}
412

abcd

From figure 1,2,3,4, the min class Ratio relates to
the cluster is the best clustering.
We compare our schema with traditional algorithm,
the result is table 1:
Table 1 the compare of schema

algorithm

total

error

error/total

Threshold

1978

94

95.2%

Our way

1978

4

99.8%

Porto,Portugal, 2005, pp. 71‚Äì83.
[5]

G.

Karypis,

E.

Han,

and

V.

Kumar,

CHAMELEON: A hierarchicalclustering algorithm
using dynamic modeling,‚Äù Computer,vol. 32, pp.
68‚Äì75, 1999.
[6] K. Fukunaga, Introduction to Statistical Pattern
Recognition,2nd ed. Boston Academic Press, 1990.
[7] M. Ester, H. Kriegel, J. Sander, and X. Xu, ‚ÄúA
density-based algorithm for discovering clusters in
large spatial databases with noise,‚Äù in Int. Conf.
Knowledge Discovery and Data Mining. Portland, OR:

ReferenceÀñ

AAAI Press, 1996, pp. 226‚Äì231.

[1] P. Hansen and B. Jaumard, ‚ÄúCluster analysis and

[8] J. Sander, M. Ester, H. Kriegel, and X. Xu,

mathematicalprogramming,‚Äù Mathematical Programming,

‚ÄúDensity-based clusteringin spatial databases: The

vol. 79, pp. 191‚Äì215, 1997.

algorithm GDBSCAN and itsapplications,‚Äù Data

[2] J. MacQueen, ‚ÄúSome methods for classification and

Mining and Knowledge Discovery, vol. 2,no. 2, pp.

analysis of multivariate observations,‚Äù in Proceedings of

169‚Äì194, 1998.

5th BerkeleySymposium on Mathematical Statistics and

[9] R. Agrawal, J. Gehrke, D. Gunopulos, and P.

Probability, vol. I:Statistics, 1967, pp. 281‚Äì297.

Raghavan,‚ÄúAutomatic subspace clustering of high

[3] A. Banerjee, S. Merugu, I. Dhillon, and J. Ghosh,

dimensional data fordata mining applications,‚Äù in

‚ÄúClusteringwith bregman divergences,‚Äù in Proc. 4th SIAM

Proc. ACM-SIGMOD Int. Conf.Management of Data.

Int. Conf. onData Mining, 2004, pp. 234‚Äì245.

Seattle, WA: ACM Press, 1998, pp.94‚Äì105.

[4] C. Ding and X. He, ‚ÄúCluster aggregate inequality and
multilevel hierarchical clustering,‚Äù in Proc. 9th European
Conf.Principles of Data Mining and Knowledge Discovery,

413

