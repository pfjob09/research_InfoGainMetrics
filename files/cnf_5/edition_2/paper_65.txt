2009 Sixth International Conference on Computer Graphics, Imaging and Visualization

Using the selected candidate vectors to determine kernel parameters

Li Xiaoyan

Zhang Hongbin

College of computer science
Beijing University of Technology
Beijing, China
lixiaoyan@emails.bjut.edu.cn

College of computer science
Beijing University of Technology
Beijing, China
zhb@public.bta.net.cn
can save the computation time for choosing kernel
parameters. But when the data size is large, training time
increase largely and the many non-SVs may affect the
selection of kernel parameters.
In this paper, we propose an improved scheme of using
the inter-cluster distance in the feature space to choose the
kernel parameters. First we use the algorithm proposed in
[8] to select a certain proportion of the foremost ranked
samples as the candidate vectors. Than the candidate vectors
instead of all the data participate in the process of
calculating inter-cluster distance which is used to select the
proper kernel parameters. At last, the selected kernel
parameters are used to train Support Vector Machine (SVM)
models. The experiment results show that using candidate
vectors to calculate the distance in the feature space between
two classes can choose proper kernel parameters; The
testing accuracy of Support Vector Machine (SVM) that
trained with the selected kernel parameters overcome the
one without the process of selecting candidate vectors, and
the calculating time for choosing proper kernel parameters
is also shortened.
The remainder of this paper is organized as follows: In
section 2, the basic Support Vector Machine (SVM) idea is
introduced. Then related works we are about to improve and
use are reviewed in section 3. In section 4 the proposed
scheme and the corresponding experiments are described and
the results are also presented to show the effectiveness of our
scheme. Conclusions are finally drawn in the last section.

Abstract—This paper proposes an improved scheme of using
the inter-cluster distance in the feature space to choose the
kernel parameters. First, the candidate vectors of the training
set are selected. Then calculate the inter-cluster distance
between classes to choose the proper kernel parameters.
Finally the selected kernel parameters are used to train the
Support Vector Machine (SVM) models. The basic principle is
that the Support Vector (SV) set contains all information
necessary to solve a given classification task. Experiment
results show that our scheme costs much less computation time.
Moreover, suitable kernel parameters can also be selected at
the same time.
Keywords- SVM; kernel parameters; inter-cluster distance;
candidate vectors

I.

INTRODUCTION

Supervised learning is one of the most important tasks in
knowledge discovery in databases (KDD) in the real world.
Since the Support Vector Machines (SVM) methodology
was introduced by Vapnik [1], it has been successfully
applied in many real-world applications, including pattern
recognition [2], communication [3] and image/video
analysis [4]. In the Support Vector Machine (SVM) model
training process, all the training data are mapped into a
feature space by a kernel function. Then the training
algorithms of Support Vector Machine (SVM) try to find
the optimal separating hyperplane in the feature space. As
the data distribution may change in different feature spaces,
the performance of a Support Vector Machine (SVM)
depends on the kernel parameters largely.
Many algorithms on how to determine kernel parameters
by some kind of indexes appeared for the sake of reducing
training time. Debnath and Takahashi create an index for
identifying appropriate kernel parameters by analyzing the
principle components of data in the feature space [5]. This
method requires a non-automatic criterion to choose a good
set of kernel parameters. Bi et al. propose using the
relationship of boundary points in the feature space to select
a suitable kernel parameter for Gaussian kernel [6]. In [7],
an algorithm of using the inter-cluster distances in the
feature spaces as the separation index to predict possible
good kernel parameters with all the training data was
proposed. Such indexes, especially the one proposed in [7],
in comparison to training a Support Vector Machine (SVM)
978-0-7695-3789-4/09 $25.00 © 2009 IEEE
DOI 10.1109/CGIV.2009.35

II.

SUPPORT VECTOR MACHINES

Given a training set of instance-label pairs (

xi , yi ),

n

i =1,…, l , xi ∈ R and yi ∈ {-1,1}, where the R n is the
input space,
space

R and yi is the class label of xi . The training

algorithms
hyperplane
hyperplane
hyperplane
problem:

min
w ,b ,ε

398

xi is the sample vector in n-dimensional real

n

of SVM try to
by maximizing
and the data.
need to solve

l
1
w, w + C ∑ ε i
2
i =1

find the optimal separating
the margin between the
Calculating the separating
the following optimization
(2.1)

subject to
where

yi ( w, xi + b ) ≥ 1 − ε i ,

List 1: Using the distance to choose a desired classifier
(2.2)

i = 1,...l , ε i ≥ 0

for each kernel parameter combination
calculate the preferred distance index with the training
data
with the kernel parameter combination which leads to best
separation index
for each C
train a classifier with the training data

< .,. > indicates the inner product operation, ε i is the

slack variable and C is the penalty parameter of error. The
decision function of the classifier is:
⎛
⎞
(2.3)
sign ⎜ ∑ yiα i φ ( xi ) , φ ( x ) + b ⎟
∈
x
SV
⎝ i
⎠
Here training vectors

The index used in list 1 called inter-cluster distance
between classes, and can be written as follows
( σ 1 ( X + , X − ) is the index in the input space while

xi are mapped into a higher (maybe

infinite) dimensional space by φ , but it is not given
explicitly in most cases. Instead, a kernel function

σ 1F ( X + , X − ) is the index in the feature space):
σ1 ( X + , X − )

K ( x, x ') = φ ( x ) , φ ( x ' ) gives the inner product value
of x and x ' in the feature space. So the decision function
becomes:
⎛
⎞
(2.4)
sign ⎜ ∑ yiα i K ( xi , x ) + b ⎟
∈
x
SV
⎝ i
⎠
The generally used kernel functions are:

⎛
⎞
⎜ ∑ x+ ∑ x− ⎟
x ∈X
x ∈X
= d ( x + , x− ) = d ⎜ + + − − − ⎟
⎜ l+
l− ⎟
⎜
⎟
⎝
⎠

σ 1F ( X + , X − )

linear: K ( x, x ') =< x, x ' >
polynomial:

K ( x, x ') = ( γ x, x ' + r )

∑ φ (x ) ∑ φ(x )

d

radial basis function(RBF): K ( x, x ') = e

−γ x − x '

^
+

^
−

^
+

^ 2
− 2

= d (x , x ) = | x − x | =
d

,r > 0

+

x+ ∈ X +

−

l+

2

−

x− ∈ X −

(3.2)

l−
2

∑

sigmoid: K ( x, x ') = tanh(γ ( x, x ') + r )
Here, γ , r and d are kernel parameters. For certain
parameters, the linear kernel is a special case of RBF kernels.
Also, the sigmoid kernel behaves like the RBF kernel. When
the data are linearly inseparable, a non-linearly kernel can
handle the data better than the linear kernels. As the
polynomial kernel requires more parameters to be chosen
than the RBF kernel, so the RBF kernel is a reasonable first
choice of kernel function [9].
III.

(3.1)

=

∑

K ( x+ i , x+ j )

x+ i ∈ X +
x+ j ∈ X +

l+2

+

K ( x− p , x− q )

x− p ∈ X −
x− q ∈ X −

2
−

l+2

∑

K ( x+ m , x− n )

x+ m ∈ X +
x− n ∈ X −

l+ l−

X + and X − represent positive and negative classes
in the input space, l+ and l− are sample sizes of X + and
X − , x+ and x− are the data in the input space, φ ( x+ ) and

Here

φ ( x+ ) are the data in the mapped feature space, x + and x −
^

are the class means of the data in the input space, x+ and

RELATED WORKS WE ARE ABOUT TO IMPROVE AND

x−^ are the class means of the data in the feature space.

USE

A. Calculate inter-cluster distance to choose the kernel
parameter
As described in [10], the main idea of the algorithm we
are about to improve is to treat the kernel parameters and the
penalty parameter C in different ways, the algorithm based
on the conception that changing the kernel parameters is
equivalent to selecting the feature spaces, and tuning C is
corresponding to weighting the slack variables, the error
terms. The algorithm was an effective one in reducing the
calculating time while the testing accuracy still competitive
to the gird search algorithm. Its corresponding training
process was listed in List 1.
(a)

399

d rel ( x+ i ) =

d (< x+ i , x−* m >) + 1
d (< x−* m , x+* n >) + 1

(3.5)

d rel ( x− i ) =

d (< x− i , x+* m >) + 1
d (< x+* m , x−* n >) + 1

(3.6)

After this process the boundary samples will rank in front
of others, so they have a higher probability to be candidate
support vectors.
At last we select a certain proportion of the foremost
ranked samples as the candidate vectors to calculate the
inter-cluster distance in the feature space.
(b)

IV.

Fig.1. Artificial data representing the relationship
between the class location and the class mean location. The
data of +1 class are the same in both figures. However, the
data of -1 class in (a) are shifted in (b) by adding the vector
(1,-1) to each sample.
As we can see the distances between class means both in
the input and feature space are less affected by noises than
other distances. Referring to Fig.1, as the classes more
separate away, the distance between class means increases.
So, the distance can represent the separation degree of the
classes and thus can estimate the classifier generalization
ability.

In our experiments, we use the data sets a1a-a5a which
are actually random subsets of Adult databases [11] and the
Splice used in [12] as the classification problems. The RBF
kernel and its parameter d mentioned before is employed to
calculate the inter-cluster distance. The process of our
scheme is listed in list 2.
List 2: Using the distance calculate by candidate vectors
to choose a desired classifier
select the candidate vectors
for each kernel parameter combination
calculate the preferred distance index with candidate
vectors
with the kernel parameter combination which leads to
best separation index
for each C
train a classifier with the data
pick the classifier with highest validation accuracy for
testing process usage

B. Select the candidate vectors
In our scheme, the process of how to calculate the
candidate vectors is as follows [8]:
First, the Euclidean distance of all samples to the
boundary of the other classes is calculated, and the
Euclidean distance of an example to the boundary of another
class is defined as:
2
⎧
⎫
d (< x+ i , x−* m >) = min ⎨ x+ i − x− j j = 1, 2,..., l ⎬
⎩
⎭
2
⎧
⎫
d (< x−i , x+* n >) = min ⎨ x− i − x+ j j = 1, 2,..., l ⎬
⎩
⎭

Where,

EXPERIMENT

(3.3)

In the experiment we use LIBSVM [9] to train the SVM
models. As our aim is to compare the result of our improved
method with the one proposed in [7], so we just train and
test simply, and have not done scaling or cross validation
mentioned in [7]. Though the results in our experiment may
be worse than the simply grid search method, but our
emphases is to compare the two different algorithms, so
relatively better is enough to prove which one is more
effective .
The experiment result are shown at table 1.We do not
compare the training time at table 1, but we can calculate
the speed up of our improved algorithm by the following
formula:
S speed up= (number of s1)/ (number of s2)
s1 is the selected candidate vectors while s2 represent the
training data.

(3.4)

x+ i ∈ X + , x− i ∈ X − , x+* n ∈ X + and it’s the

x− i in X + , x−* m ∈ X − and it’s the nearest
one to x+ i in X −

nearest one to

When the margin between the boundaries of two classes
is not uniform, the distance calculate above is not
comparable, and the selection based on this distance will
lead to the result that most selected candidate vectors are
located along the boundaries with narrower margins. In
order to handle the non-uniform margin of a dataset, we
transform the Euclidean distance into relative distance
which will calculate next.
Then compute the relative distance to reorder training
samples ascendingly according to the distance, the relative
distance of an example is defined as:

V.

CONCLUSION

We can see from the experiment results that our proposed
scheme can select better kernel parameters with only 10% to
15% of the total training examples. Using the selected
kernel parameters we can obtain better testing accuracy with

400

In our research we only use candidate vectors to select
kernel parameters. In future, we attempt to incorporate the
candidate vectors into the calculation of penalty parameter C.

less computation time. It has shown that for support vector
machines, the candidate vectors selected in our experiments
contain all information necessary to calculate the intercluster distance between classes.

TABLE I.
TESTING ACCURACY (%) AND NUMBER OF TRAINING SET WITH AND WITHOUT SELECTING CANDIDATE VECTORS
PARENTHESIS ON THE LAST COLUMN ARE THE PROPORTION OF SAMPLES SELECTED)
Data set

Without selecting candidates

Proposed method

Choose
r

Testing accuracy
(%)

Number of training set(s1)

Choose
r

Testing accuracy
(%)

Number of training set(s2)

a1a

-4

82.3239%

1605

-6

82.6566%

200(0.1,0.2)

a2a

-4

82.4339%

2260

-5

82.7343%

283(0.1,0.2)

a3a

-4

82.0982%

2560

-5

82.374%

317(0.1,0.2)

a4a

-4

81.7357%

1700

-5

82.2505%

210(0.1,0.2)

a5a

-4

83.1415%

6414

-5

83.2486%

797(0.1,0.1)

Splice

-7

87.4433%

1000

-7

87.4433%

200(0.2,0.2)

[7]

Kuo-Ping Wu, Sheng-De Wang, Choosing the kernel parameters for
support vector machines by the inter-cluster distance in the feature
space, Pattern Recognition, Volume 42, Issue 5, May 2009, Pages
710-717.
[8] Minqiang Li; Fuzan Chen; Jisong Kou, Candidate Vectors Selection
for Training Support Vector Machines Natural Computation, 2007.
ICNC 2007. Third International Conference onVolume 1, 24-27 Aug.
2007
Page(s):538
542
Digital
Object
Identifier
10.1109/ICNC.2007.292 .
[9] C.-C. Chang and C.-J Lin, LIBSVM: a library for support vector
machines,
available
from
World
Wide
Web:
http://www.csie.ntu.edu.tw/~cjlin/libsvm.
[10] H. – J. Liu, Y. – N. Wang and X. – F. Lu, A Method to Choose
Kernel Function and its Parameters for Support Vector Machines,
Proceeding of 2005 International Conference on Machine Learning
and Cybernetics, 18-21 Aug. 2005, vol. 7, pp. 4277-4280.
[11] J. C. Platt, Fast training of support vector machines using sequential
minimal optimization, Advances in Kernel Methods – Support Vector
Learning. B. Schölkopf, C. J. C. Burges and A. J. Smola Eds.
Cambridge, MA: MIT Press, 1999.
[12] Yang-Guang Liu, Qi Chen, Rui-Zhao Yu, Extract candidates of
support vector from training Machine Learning and Cybernetics,
2003 International Conference on Volume 5, 2-5 Nov. 2003
Page(s):3199 - 3202 Vol.5 .

ACKNOWLEDGMENT
This work is supported in part by the National Natural
Science Foundation of China (NSFC) under grant 60775011.
REFERENCES
[1]
[2]

[3]

[4]

[5]

[6]

(.THE NUMBERS IN

V. Vapnik, Statistical Learning Theory, New York: Wiley, 1998.
A. Tefas. C. Kotropoulos, and I. Pitas, Using Support Vector
Machines to Enhance the Performance of Elastic Graph Matching for
Frontal Face Authentication, IEEE Trans, on Pattern Analysis and
Machine Intelligence, vol. 23, no. 7, pp. 735-746, July 2001.
S. Chen, A.K. Samingan, and L. Hanzo, Support Vector Machine
Multiuser Receiver for DS-CDMA Signals in Multipath Channels,
IEEE Tran. on Neural Networks, vol. 12, no. 3, pp. 604-611, May
2001.
G.D. Guo, A.K. Jain, W.Y. Ma, and H.J. Zhang, Learning Similarity
Measure for Natural Image Retrieval with Relevance Feedback, IEEE
Tran. on Neural Networks, vol. 13, no. 4, pp. 811-820, July 2002.
R. Debnath and H. Takahashi, An efficient method for tuning kernel
parameter of the support vector machine, IEEE International
Symposium on Communication and Information Technology, 26-29
Oct .2004, vol .2, pp 1023-1028.
L. – P. Bi, H. Huang, Z. –Y. Zheng and H. – T. Song, New Heuristic
for Determination Gaussian Kernel's Parameter, Proceedings of 2005
International Conference on Machine Learning and Cybernetics, 1821 Aug.2005, vol. 7, pp. 4299-4304.

401

