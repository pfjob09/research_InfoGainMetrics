2009 13th International Conference Information Visualisation

Probabilistic NeuroScale for Uncertainty Visualisation
Mingmanas Sivaraksa, David Lowe
Neural Computing Research Group, Aston University, Birmingham, United Kingdom
sivarakm@aston.ac.uk, d.lowe@aston.ac.uk
2

Abstract
This paper is a study of low dimensional visualisation
methods for data visualisation under uncertainty of the input data. It focuses on NeuroScale, the feed-forward neural
networks algorithm by trying to make the algorithm able
to accommodate the uncertainty. The standard model is
shown not to work well under high levels of noise within
the data and need to be modified. The modifications of the
model are verified by using synthetic data to show their
ability to accommodate the noise.

1

The traditional techniques for dimensional reduction are
techniques are Principal Component Analysis(PCA) and
Multidimensional Scaling(MDS) [1]. These techniques are
popular for simple dimensionality reduction but their limit
is their linear property. However, data in the real world
applications do not often have linear structure for data visualisation.
The more recent developments in visualisation focus on
retaining the structure of the high-dimensional data. These
are called the topographic mapping. The topographic mappings are mechanisms that map the data in a high dimensional space into a low dimensional space in such a way
that preserves the structure of the data.
This structure of the data usually means the similarity
between two data samples. In other words, the data samples that are similar in a high dimensional space should
stay close together in a lower dimensional space and data
that are dissimilar in a high dimensional space should remain apart in the lower dimensional space. Some famous
examples of the topographic nonlinear topographic visualisation are as follow:

Introduction

Data visualisation is an essential tool for aiding the users
to understand and making further data analysis and evaluation. For human visual perception, the projection from the
high dimensional space which holds all the data entities
to the reduced dimensional form facilitates the understanding of the data and assists the interpretation by using the
brain’s ability to assimilate patterns in data. Clearly we
can not perceive visualised images in more than three dimensions and can understand better two dimensional representations [5]. From the complicated structure of the
data, trustworthy data transformation techniques are necessary to reduce data dimensionality to be used easily with
any graphic representations for the end users. Additionally,
data visualisation can provide unsupervised techniques which
will reveal a real data investigation without class information involved.
Many existing techniques ignore the fact that real world
data is usually noisy and imprecise. Ignoring this information can alter the final projective representation and lead to
incorrect understanding and assumptions of the data. This
paper uses examples from systems biology in which uncertainty is quite common but very sensitive. Projecting
the data down to low dimensions without caution can affect the results and lead to incorrect interpretation, which
is very dangerous especially if it is part of a life-critical application [6, 11, 9, 2].

978-0-7695-3733-7/09 $25.00 © 2009 IEEE
DOI 10.1109/IV.2009.106

Established Topographic Visualisation

2.1

Isometric Mapping

Isometric Mapping (Isomap) [10] builds on classical MDS
but improves by aiming to preserve the geodesic distance
of the data. The geodesic distance is defined by the approximation of adding up a sequence of “short hops” between neighbouring points.This can be done by finding
the shortest paths in a graph with edges connecting neighbouring data points [10]. The distance between two points
which is measured by Euclidean distance in high dimensional space may not be a good measure of structural similarity. Isomap estimates this geodesic manifold distance
between data points and then applies MDS [10] to obtain a
visualisation projection.

2.2

Locally Linear Embedding

Locally Linear Embedding (LLE) [7] is another local
method, similar to the Isomap. It focuses on preserving the
topographic distance in small neighborhoods by using an
eigenvector method [8]. The LLE uses the linearity in the
74

2.4

local area and overcomes many limitations that occur in a
fully global linear method.
Provided there is sufficient data, we expect each data
point and its neighbours to lie on or close to a locally linear patch of the manifold. In the simplest formulation of
LLE, we need to identify K nearest neighbours per data
point, as measured by Euclidean distance from a point of
interest. The idea is to preserve the weight vector which is
proportional to the distance to the neighbour of each data
points between the high dimensional space and the low dimensional space.
Given x is a vector of N data points in D dimensional
space, sampled from some smooth underlying manifold.
This cost function is defined by:
N

K

Wij xj |2 .

|xi −

ε(W ) =
i

The dissimilarities, dij can be based on standard Euclidean distances of the data x in the high dimensional
space and scaled by a smoothing factor σi which is emx −x
pirically determined, dij = iσ2 j .
i
The low dimensional images yi of the points are used
to define a probabilistic density in the mapping space, qij ,
as:
exp(− yi − yj 2 )
.
(5)
qij =
2
k=i exp(− yi − yk )

(1)

j=1

Wij is a weight between a point i and its neighbours j.
Therefore, the optimal weights are given by:
i

k

Wij =

Cjk−1
i

lm

−1
Clm

,

(2)

i
where Cjk
are elements of a covariance matrix within the
neighbourhood of xi .

2.3 Neuroscale
In a Neuroscale [4] topographic map the distribution
and relative positions of the points in the data space are
determined to reflect the relative dissimilarity between data
measurements (gene expression values) in the high-dimensional
space, and hence generalises the established Sammon map.
N measurement vectors xi in Rp are transformed using a
Radial Basis Function (RBF) network to a corresponding
set of feature (visualisation) vectors yi in Rq . Generally,
q < p as dimension reduction is desired, and typically
q = 2 for visualisation. The quality of the projection is
measured by the Sammon stress metric (n.b. we are using
a reduced form here, neglecting a denominator often employed):
N

N

(d∗ij − dij )2 ,

E=
i

Stochastic Neighbor Embedding

Stochastic Neighbour Embedding (SNE) [3] borrows a
concept from NeuroScale as it uses a pairwise similarity
between points but measures similarity using a probabilistic distance approach to preserve the neighbourhood identities. A Gaussian distribution is centred on each data point
in the high dimensional space and a probability density is
defined over all the potential neighbours of that point.
The high dimensional related probability for each point,
i, and each potential neighbour, j, is computed using the
asymmetric probability, pij , that i would pick j as its neighbour,
exp(−d2ij )
(4)
pij =
2 .
k=i exp(−dik )

(3)

j

where dij = yi − yj and d∗ij = xi − xj represent
the inter-point distances in projection space and data space
respectively. The aim of training process is to set the parameters of the RBF, the feed forward neural network, to
minimise the stress metric.

75

The aim of this SNE method is to match the above two distributions as close as possible. The Kullback-Leibler divergence, which is a measure of dissimilarity between two
probability distribution is used here as a cost function.This
can be achieved by manipulating the coordinates yi to minimise the cost:
N

N

pij log

C=
i

j

pij
.
qij

(6)

However the main drawback is the proper way of determining σ to properly fit the data without knowing the underlying projection in advance.

2.5

Comparison

The SNE and the NeuroScale models are similar that
they both use the distances between low and high dimensional spaces to construct a cost function. The previous
model uses the probabilistic similarity and the latter uses
the point-wise similarity. However, SNE is more sensitive to the parameter compared to NeuroScale. Therefore,
it can be difficult in determining the suitable parameter to
the data set with unknown structures.
NeuroScale is related to the ISOMAP if the input dissimilarity is changed to reveal the intrinsic distance of the
data rather than using just a normal Euclidean distance.
Additionally, the NeuroScale results will be similar to the

3.2

Isomap but with an extra advantage of interpolation ability
which can create a transformation mapping.
However, the main drawback of the existing techniques
is that they do not support uncertainty information if it
is available. Uncertainty is dealt with by alternative approaches such as regularisation. These techniques assume
all data points have the same uncertainty information which
is not always true. Techniques using probabilistic approaches
are useful in modelling the data with uncertainty. In addition, the main advantage of using explicit mapping functions such as neural networks for dimensionality reduction
is that they provide an explicit implementation of applying
the trained network to new unseen data points.

3

In a more complete probabilistic approach, rather than
viewing each given information as a single precise location of a data point, each information is viewed as a sample from a probabilistic distribution. Therefore, the distance measure needs to be modified to consider distances
between probabilistic density functions instead of two data
points as in the standard NeuroScale.
The dissimilarity between two distributions, the KullbackLiebler(KL) divergences, f (x) and g(x), defined as:
DKL{f (x)

Probabilistic Approaches to NeuroScale

D{f (x)

With given confidence values, they can be used to modify the original STRESS function:
∗
(Dij
− Dij )2 ,
i

=

DKL{f (x)

N

(9)

g(x)}

+ DKL{g(x)
2

f (x)}

. (10)

N

(11)

j

(7)
3.2.1

The Gradient

The optimisation of the locations of y can be achieved by a
gradient descent method for which we need the derivative
of (11) which will give

N

Kij (d∗ij − dij )2 ,

f (x)
dx.
g(x)

∗
(Dij
− Dij )2 ,
i

j

E=

f (x) log

−∞

E=

by including a confidence term, Ci which can be derived
from the inverse of the variance Ci = σ12 .
The modified cost function is

i

g(x)}

N

E=

∞

=

The KL divergence measure can be applied to both data
and projection spaces. However, this requires a modification of the optimisation process since the gradient will
be different. To achieve a topographic representation, the
standard NeuroScale cost function is modified to

3.1 The Heuristic Approach

N

g(x)}

To symmetrise this divergence in order to make it an
appropriate distance measure an average of the two is performed:

We previously discussed the inability of existing visualisation techniques of incorporating uncertainty. In this section, a probabilistic approach is proposed by using one of
the most effective projective visualisation methods - NeuroScale. However, the NeuroScale projection is a originally deterministic approach and is not designed for probabilities. In this section we introduce new methods to enhance NeuroScale with probabilistic techniques.

N

The Full Probabilistic Approach

∂E
=2
∂yi

(8)

j

∗
(Dij − Dij
)(yi − yj )(
j

1
1
+ 2 ).
σi2
σj

(12)

The locations of yi can be simply achieved by using optimisation algorithms, such as scaled conjugate gradient or
to imitate the mechanism of the shadow target algorithm in
the NeuroScale mechanism.

where Kij =min (Ci , Cj ); Ci is a confidence value on data
point i. More generally, Kij is a function between points i
and j that reduces the influence of these points on disturbing the map depending on our confidence in these data.
Equation (8) can be interpreted that points with low interpoint confidence are less important to determine the mapping parameters.
The inter-point confidence, Kij is the minimum confidence values between the two points. Hence, the new network gained by using this new cost function in training will
be less likely to be influenced by those data which have low
confidence.

3.3

Modification of the Shadow Target Algorithm

In NeuroScale the shadow target of a data point is required for the network training. However, for the probabilistic NeuroScale the shadow target of the mean of the
distribution is required. In addition to the expression of the
error function in (11), the error for the distribution can be
achieved by minimising the error function of the negative
log likelihood of the distribution, φ(x), which we restrict

76

3.4

the distribution to the Gaussian kernel function with spherical covariances,
1
ti − y(xi )
−
φ(ti |xi ) =
p exp
2
2σi2
(2πσi ) 2

We will use a synthetic example to compare the performance of different NeuroScale approaches with nonuniformly distributed noise.
We now consider synthetic data defined in a 2 dimensional plane with 11 × 11 points and different levels of
added noise used to induce distortion into a third dimension. The noise is measured by σi2 = C1i , where Ci is the
confidence of each data point measured by

2

,

(13)

where y is the mapping function from high dimensional
space to the projection space and ti represents an explicit
target value.
Therefore the likelihood function is given by
N

− ln{φ(tn |xn )}.

E=

Ci = dfrom

(14)

n=1

(15)

The idea is the same as standard NeuroScale in that the
target value will need to be estimated using the shadow
target algorithm in which the hypothetical target, tˆ, can be
estimated using (15) and (12) as:
∂E
,
tˆi = yi − σi2
∂yi
= y − 2σi2 (

(16)
∗
(Dij − Dij
)(yi − yj )(

j

1
1
+ 2 )).
σi2
σj
(17)

For the RBF network as a mapping function, the output
of the network is obtained by
Y = ΦW,

(18)

where W represents the matrix of the weights in the network and Φ is the matrix of hidden unit activations including bias terms and Y is the output of the network . The
weight updating process is the same as in standard NeuroScale, which uses the obtained standard target to update
the weight. For a fixed set of targets, the problem can be
solved directly by
ˆ
W = Φ† T.
(19)
The algorithm is the same as updating the target of the
NeuroScale algorithm in which an additional parameter η
will be introduced by
∂E
tˆi = yi − ησi2
,
∂yi

plane / max(dfrom plane ),

where dfrom plane denotes the vertical distance a data point
is removed from the plane in which the main data is constrained. In this example, disturbed noise is not uniformly
distributed to all data points. Some points are more disturbed by noise and the noise is attached as extra information into the NeuroScale approaches suggested previously.
In the example shown in Figure 1, 20% of the data points
(24 points) are selected to be outliers. Those points have
2
= 5. The twovery high variance added with σoutlier
dimensional projection results are shown as follows. Both
projections show similar projections in which high variance data points (outliers) are scattered away from the majority together with the distortion of data points with low
added noise (high certainty data). The results do not preserve the original data structure well in which points are
aligned evenly on the plane. On the other hand the modified model result for the heuristic approach is shown in
1(b) and Figure 1(c) for for the probabilistic approach. By
visual perception, Figure 1(b) shows improved alignment
of non-outliers compared to the standard models. Figure
1(c) clearly shows the best alignment of the data points.
Non-outlier data are only marginally disturbed by the outlier points. The small misalignment are only due to the
outlier data. This full probabilistic model shows the best
alignment. The results show that using the probabilistic
approach preserve the original structure in a more appropriate topographic manner.
To evaluate the the ability of the modified NeuroScale,
the original STRESS measure, using the Euclidean distance, is used to show the ability of structure preservation.
However, only non-outlier data will be used to calculate
this Stress measure since the idea is not to preserve the
outliers but to preserve the structure of the rest. Therefore
the new STRESS is computed by ignoring those outliers.
All NeuroScale approaches are applied to train on the data
samples and use the original stress function (3) to measure
the error. Table 1 shows the Stress value obtained by using
(3). The number of outliers is chosen between 10 and 20
percent which are 12 and 24 points respectively. The outlier’s variance is tested between 0.5 to 5 averaged over 50

Differentiating E over yi gives
∂E
y i − ti
=
.
∂yi
σi2

Synthetic Example on a 2 Dimensional plane
with non-uniform added noise

(20)

η is a step length to be restricted to (0, 1) and is just a
parameter governing convergence of the gradient descent.

77

training networks. The average is created from the mean
over 50 training examples using different NeuroScale approaches, all of which are initialised by PCA.

Standard NeuroScale without noise

Outliers
Percent Variance
0.5
1.0
1.5
2.0
10
2.5
3.0
3.5
4.0
4.5
5.0
0.5
1.0
1.5
2.0
20
2.5
3.0
3.5
4.0
4.5
5.0

(a)
Modified Heuristic NeuroScale

Average Stress value(mean)
Standard Prob.
Heuristic
25.23
22.20
20.70
35.75
25.83
24.86
52.49
32.30
33.01
56.16
32.47
36.37
63.67
41.40
44.53
80.99
51.67
58.80
101.35
61.07
72.45
102.73
61.20
73.45
121.74
71.23
90.26
107.00
78.77
81.36
21.59
16.03
17.08
48.78
23.73
33.40
69.76
33.81
50.00
63.73
35.55
40.46
108.68
50.03
83.71
123.85
51.98
98.02
156.86
75.13
134.52
174.93
97.06
140.16
211.28
265.14 157.13
257.58
322.24 196.15

Table 1: A table showing the average evaluated STRESS
from different fractions and variances of outliers using
different NeuroScale approaches, including the standard
NeuroScale, the standard NeuroScale with noise, the fully
probabilistic NeuroScale without noise, with noise and the
heuristic NeuroScale. The best STRESS value of each row
are highlighted in Bold. The results show that overall the
modified NeuroScale both fully probabilistic and heuristic
way which incorporate uncertainty information gave better
performance. (The standard models and modified models
columns are separated by the double lines)

(b)
Modified Probabilistic NeuroScale

From Table 1, the modified NeuroScale approaches show
an improvement over the standard NeuroScale using the
STRESS measure. The heuristic approach is sometimes
better than the probabilistic approaches, however, most of
the time the fully probabilistic approach gave better performances. The error gradually increases as the σ 2 of outliers increase in all models except with 20 % outliers with
σ 2 = 4.5 and σ 2 = 5 which shows a dramatic drop in the
performance of the modified NeuroScale. Both of them
having poor average performances. This is due to some of
the networks having very poor convergence which is due to
local minima problem However, if the outlier networks are
ignored and the error is averaged by using median, instead
of arithmetic mean, the result of the modified NeuroScale

(c)

Figure 1: A synthetic example using data generated in
a two-dimensional plane with 20% noisy outliers (a) the
Standard NeuroScale, (b) using the heuristic modified cost
function (8) (c) using the modified shadow target approach
with a probabilistic model. The size of the symbols denotes
uncertainty levels.
78

of the 20% outlier improves, the results are shown in Table
2.
Outliers
Percent Variance
0.5
1.0
1.5
2.0
20
2.5
3.0
3.5
4.0
4.5
5.0

Acknowledgements
This work was partially supported by the EUBIOPATTERN Network of Excellence,under contract 508803.

Average Stress value(mode)
Standard Prob.
Heuristic
19.77
13.19
15.05
42.29
21.89
28.03
59.57
26.91
47.27
57.58
30.83
34.39
97.75
46.33
76.30
118.60
47.84
84.72
134.21
57.43
119.72
162.31
71.84
128.35
183.82
104.08 138.58
210.31
115.92 155.66

References
[1] T. Cox and M. Cox. Multidimensional Scaling. Academic
Press, 2 edition, 2001.
[2] D. D’Alimonte, D. Lowe, I. T. Nabney, and M. Sivaraksa.
Visualising uncertain data. In 2nd European Conference
on Emergent Aspects in Clinical Data Analysis, EACDA05,
Pisa, Italy, 2005.
[3] G. E. Hinton and S. Roweis. Stochastic Neighbor Embedding. Neural Information Proceeding Systems:Natural and
Synthetic, 15:833–840, December 2002.
[4] D. Lowe and M. E. Tipping. Neuroscale: novel topographic
feature extraction using RBF networks. In M. C. Mozer,
M. I. Jordan, and T. Petsche, editors, Advances in Neural
Information Processing Systems, volume 9, pages 543–549,
London, UK, 1997.

Table 2: A table showing the average of evaluated STRESS
by using the median instead of the mean average using different NeuroScale approaches. The best STRESS value of
each row are highlighted in Bold. The results show that
overall the modified NeuroScale gave better performance.
(The standard models and modified models columns are
separated by the double lines)

[5] A. Myllari, T. Salakoski, and A. Pasechnik. On the visualization of the DNA sequence and its nucleotide content.
Sigsam Bulletin, 39(4):131–135, 2005.
[6] S. Richard, M. D. Radmacher, K. Dobbin, , and L. M. McShane. Pitfalls in the use of DNA microarray data for diagnostic and prognostic classification. Journal of the National
Cancer Institute, 95:14–18, 2003.

Conclusion
This paper has introduced a modified NeuroScale model
and optimisation algorithm where data uncertainty is explicitly allowed to influence the visualisation. We have
introduced a new probabilistic projection model based on
preserving topographic properties between distributions rather
than just data points.
We have also introduced a modified shadow target algorithm to optimise the new model more efficiently and have
demonstrated its effectiveness on synthetic data problems,
showing its superiority to the standard models.
However, the modified model still has some limitation
of this probabilistic method. First, the latent space is assumed to have the same distribution as in the high-dimensional
space, which are Gaussian with the same variance in the
examples considered so far. However, this is quite optimistic since the projection of the distributions need not
necessarily have the same distribution as the data space.
In addition, it is prone to local minima problems in the
optimisation. Therefore good initialisation is important to
obtain meaningful projections.
The application can be used with any high dimensional
data, for example image processing, biological data, financial data. However, the better utilisation is to deal with data
with very uncertainty, such as microarray data which is a
biological data with a very high throughput and incorrect
interpretation can be related to human lives.

79

[7] S. Roweis and L. Saul. Nonlinear dimensionality reduction
by Locally linear embedding. Science, 290(5500):2323–
2326, December 2000.
[8] L. K. Saul and S. T. Roweis. Think globally, fit locally: Unsupervised learning of low dimensional manifolds. Journal
of Machine Learning Research, 4:119–155, 2003.
[9] M. Sivaraksa and D. Lowe. Unclassifiability in medical
prognosis: example using biopattern gene markers. In Third
International Conference on Computational Intelligence in
Medicine and Healthcare, Plymouth, UK, 2007.
[10] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global
geometric framework for nonlinear dimensionality reduction. Science, 290:2319–2323, December 2000.
[11] M. West, C. Blanchette, H. Dressman, E. Huang, S. Ishida,
R. Spang, H. Zuzan, J. A. Olson, J. R. Marks, and J. R.
Nevins. Predicting the clinical status of human breast cancer by using gene expression profiles. Proceedings of the
National Academy of Sciences of the United States of America, 98(20):11462–11467, 2001.

