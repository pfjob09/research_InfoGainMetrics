2009 13th International Conference Information Visualisation

Mining Textual Data to boost Information Access in OSINT
Federico Neri

Paolo Geraci
Synthema
Semantic Intelligence
Via Malasoma 24
56121 Pisa - Italy

federico.neri@synthema.it

paolo.geraci@synthema.it

has meant, most of the time, a no usable knowledge. In
fact, all the electronic texts are - and will be - written
in various native languages, but these documents are
relevant even to non-native speakers. The most
valuable information is often hidden and encoded in
pages which for their nature are neither structured, nor
classified. Nowadays everyone experiences a mounting
frustration in the attempt of finding the information of
interest, wading through thousands of pieces of data.
The process of accessing all these raw data,
heterogeneous both for type (web pages, crime
reports), source (Internet/Intranet, database, etc),
protocol (HTTP/HTTPS, FTP, GOPHER, IRC, NNTP,
etc) and language used, transforming them into
information, is therefore inextricably linked to the
concepts of automatic textual analysis and synthesis,
hinging greatly on the ability to master the problems of
multilinguality. SYNTHEMA has a relevant
experience in the Intelligence area, having provided
software products and solutions in support of the
Intelligence process and production since 2000.
SYNTHEMA has been supporting Intelligence
operative structures in Italy both on technological and
on substantive content matter issues, in order to
provide hands-on expertise on Open Source
Intelligence operations at both strategic and tactical
levels

Abstract
The revolution in information technology is making
open sources more accessible, ubiquitous, and
valuable. The international Intelligence Communities
have seen open sources grow increasingly easier and
cheaper to acquire in recent years. Up to 80% of
electronic data is textual and most valuable
information is often encoded in pages which are
neither structured, nor classified. The process of
accessing all these raw data, heterogeneous for
language used, and transforming them into
information is therefore inextricably linked to the
concepts of textual analysis and synthesis, hinging
greatly on the ability to master the problems of
multilinguality. This paper describes SYNTHEMA
SPYWatch, a content enabling system for OSINT,
which has been adopted by some Intelligence operative
structures in Italy to support the Collection,
Processing, Exploitation, Production, Dissemination
and Evaluation cycle. By this system, operative officers
can get an overview of great volumes of textual data,
which helps them discover meaningful similarities
among documents and find all related information.

1. Introduction
With Internet, the bulk of predictive intelligence
can be obtained from public and unclassified sources.
The revolution in information technology is making
open sources more accessible, ubiquitous, and
valuable, making Open Source Intelligence at less cost
than ever before. The world today is really in the midst
of an information explosion. Anyway, the availability
of a huge amount of data in Internet and in all the open
sources information channels has lead to the wellidentified modern paradox: an overload of information
978-0-7695-3733-7/09 $25.00 © 2009 IEEE
DOI 10.1109/IV.2009.99

1.1. The State of Art of Information Systems
Current-generation information retrieval (IR)
systems excel with respect to scale and robustness.
However, if it comes to deep analysis and precision,
they lack power. Users are limited by keywords search,
which is not sufficient if answers to complex problems
are sought. This becomes more acute when knowledge
and information are needed from diverse linguistic and
433
427

Table 1. Comparison of information systems

cultural backgrounds, so that both problems and
answers are necessarily more complex. Developments
in the IR have mostly been restricted to improvements
in link and click analysis or smart query expansion or
profiling, rather than focused on a deeper analysis of
text and the building of smarter indexes.
Traditionally, Text and Data mining systems can be
seen as specialized systems that convert more complex
information into a structured database, allowing people
to find knowledge rather than information. For some
domains, Text Mining applications are well-advanced,
for example in the domains of medicine, military and
intelligence, and aeronautics [1].
In addition to domain-specific miners, general
technology has been developed to detect Named
Entities [2], co-reference relations, geographical data
[3], and time points [4].
The field of knowledge acquisition is growing
rapidly with many enabling technologies being
developed that eventually will approach Natural
Language Understanding (NLU). Despite much
progress in Natural Language Processing (NLP), the
field is still a long way from language understanding.
The reason is that full semantic interpretation requires
the identification of every individual conceptual
component and the semantic roles it play. In addition,
understanding requires processing and knowledge that
goes beyond parsing and lexical lookup and that is not
explicitly conveyed by linguistic elements. First,
contextual understanding is needed to deal with the
omissions. Ambiguities are a common aspect of human
communication. Speakers are cooperative in filling
gaps and correcting errors, but automatic systems are
not. Second, lexical knowledge does not provide
background or world knowledge, which is often
required for non-trivial inferences.
Any automatic system trying to understand a simple
sentence will require - among others - accurate
capabilities for Named Entity Recognition and
Classification (NERC), full Syntactic Parsing, Word
Sense Disambiguation (WSD) and Semantic Role
Labeling (SRL) [5].
Current baseline information systems are either largescale, robust but shallow (standard IR systems), or they
are small-scale, deep but ad hoc (Semantic-Web
ontology-based systems).
The next table gives a comparison across different
state-of-the-art information systems, where we
compare ad-hoc Semantic web solutions, wordnetbased information systems and tradition information
retrieval with SYNTHEMA SPYWatch [6]. This
system bridges the gap between expert technology and
end-users that need to be able to use the complex
technology.

Semantic
web

Wordnet
based
“Parole”

IR

SPYWatch

Large scale and NO
multiple domains

YES

YES

Deep semantics

YES

NO

NO

YES

Automatic
acquisition and
Indexing

NO

YES/NO

YES

YES

Multi-lingual

NO

YES

YES

YES

Cross-lingual

NO

YES

NO

YES

Data and fact
mining

YES

NO

NO

YES

“Google”
YES

2. The Logical components
1.
2.
3.
4.
5.

The system is built on the following components:
a Crawler, an adaptive and selective component
that gathers documents from Internet/Intranet or
Database sources.
a Lexical System, which identifies relevant
knowledge in the texts, by detecting semantic
relations and facts.
a Search Engine that enables Functional, Natural
Language queries.
a Machine Translation System, which enables
automatic translation of search results.
a Classification System which classifies search
results into clusters and sub-clusters recursively,
highlighting meaningful relationships among them,
or assigns documents to predefined thematic
groups.

2.1. The Crawler
The crawler is a multimedia content gathering and
indexing system, whose main goal is managing huge
collections of data coming from different and
geographically distributed information sources. It
provides a very flexible and high performance
dynamic indexing for contents retrieval. Its gathering
activities are not limited to the standard Web, but
operate also with other sources like remote databases
by ODBC, Web sources by FTP-Gopher, Usenet news
by NNTP, WebDav and SMB shares, mailboxes by
POP3-POP3/S-IMAP-IMAP/S, file systems and other
proprietary sources. The crawler provides default plugins to extract text from most common types of
documents, like HTML, XML, TXT, PDF, PS and
DOC. Even more complex sources like videos might

428
434

unfilled slots from elliptical coordinated phrases. The
parser - a bottom-up chart parser - employs a parse
evaluation scheme used for pruning away unlikely
analyses during parsing as well as for ranking final
analyses.
The anaphora resolution is based on the detection of
pronominal references, on the interpretation of specific
named entities in the text, as well as assertions about
them. Antecedent words for which coreference is
morphologically or syntactically excluded are
automatically filtered out. A salience weight is
associated with each anaphora candidate, so that
empirically and linguistically motivated heuristics can
select the most appropriate referent in the list of
possible interpretations [7].
Beside Named Entities, locations, time-points, etc, it
detects relevant information like noun phrases which
comply with a set of pre-defined morphological
patterns and whose information exceeds a threshold of
significance [8]. The detected terms are then extracted,
reduced to their Part Of Speech (Noun, Verb,
Adjective, Adverb, etc) and Functional (Agent, Object,
Where, Cause, etc) tagged base form [9][10] (see Fig.
1). The 96% of the words in a sentence is normally
classified without any ambiguity, while the complete
syntactic tree for the sentence is extracted in the 77%
of the cases. The lemmatization speed is about 70
words per second.
By including semantic information directly in the
dependency grammar structures, the system relies on
the lexical semantic information combined with
functional relations. The Word Sense Disambiguation
algorithm considers also possible super-subordinate
related concepts in order to find common senses in
lemmas being analysed. Once referred to their synset –
namely a group of (near) synonyms - they are used as
documents metadata [9][10]. Synsets are interlinked by
means of semantic relations, such as the supersubordinate relation (hypernymy/hyponymy), the partwhole relation (holonomy/meronymy), antonymy, and
several lexical entailment relations. The system
supports English, Italian, German, French, Spanish,
Brazilian-Portuguese and Arabic.

be suitably processed to extract a textual-based
contents.

2.2. The Lexical System
This component identifies relevant knowledge from
the whole raw text, by detecting semantic relations and
facts in texts. Concept extraction is applied through a
pipeline of linguistic and semantic processors that
share a common ground and knowledge. The shared
knowledge base guarantees a uniform interpretation
layer for the diverse information from different sources
and languages.

Figure 1 - Knowledge Extraction

The automatic linguistic analysis of the textual
documents is based on Morphological, Syntactic,
Semantic, Functional and Statistical criteria. At the
heart of the lexical system is the McCord's theory of
Slot Grammar [7]. A slot is a placeholder for the
different parts of a sentence associated with a word. A
word may have several slots associated with it,
forming a slot frame for the word. In order to identify
the most relevant terms in a sentence, the system
analyzes it and, for each word, the Slot Grammar
parser draws on the word's slot frames to cycle through
the possible sentence constructions. Using a series of
word relationship tests to establish the context, the
system tries to assign the context-appropriate meaning
(sense) to each word, determining the meaning of the
sentence. Each slot structure can be partially or fully
instantiated to incrementally build the meaning of a
statement. This includes most of the treatment of
coordination, which uses a method of ‘factoring out’

2.2.1. Lexical Resources. During the analysis, the
system
accesses
some
OSINT
multilingual
dictionaries, which contain all the meaningful terms or
phrases that express the same meaning in this specific
domain, in all the languages supported by the system,
and constitute the basic lexical units for the Knowledge
Base. This is automatically updated with all the
informations available for 5,325 politicians (i.e.
photograph and brief profile), 140 political parties, 582
criminals and terrorists (i.e. photograph and brief

429
435

the reason why the system analyzes the query,
identifying the most relevant terms contained and their
semantic and functional interpretation. By mapping a
query to concepts and relations very precise matches
can be generated, without the loss of scalability and
robustness found in regular search engines that rely on
string matching and context windows. The search
engine returns as result all the documents which
contain the query concepts/lemmas in the same
functional role as in the query, trying to retrieve all the
texts which constitute a real answer to the query (See
Fig. 3). Results are then displayed and ranked by
relevance, reliability and credibility, as specified by the
OSINT doctrine. Source reliability ratings range from
A to F:. An F rating does not necessarily mean the
source is unreliable, but that the processing personnel
have no previous experience upon which to base a
determination. Information credibility ratings instead
range from 1 (Confirmed) to 6 (Cannot Be Judged).
Conceptual and lexical descriptors can be exported to
I2 Analyst’s Notebook®, or to Microsoft® Excel.

profile), 873 criminal and terroristic organizations (i.e.
mothertongue name, aliases, bases of operation,
classifications, financial sources, founding philosophy,
current goals, key leaders, members, related groups,
first and last terroristic attack, etc), 140 political
parties, 197 countries (i.e. interactive geographical
map, background, key people, population, birth rate,
death rate, infant mortality rate, life expectancy,
government, suffrage, economy overview, telephones,
mobile phones, Internet users, etc). All these
informations, which are automatically and constantly
updated by crawling the Web, provide the supersubordinate, part-whole, synonymy and aliasing
relations needed in the Semantic Analysis.

2.3. The Search Engine
2.3.1. Functional Search. Users can search and
navigate by roles, exploring sentences and documents
by the functional role played by each concept. Users
can navigate on the relations chart by simply clicking
on nodes or arches, expanding them and having access
to set of sentences/documents characterized by the
selected criterion. This can be considered a visual
investigative analysis component specifically designed
to bring clarity to complex investigations. It
automatically enables investigative information to be
represented as visual elements that can be easily
analyzed and interpreted (See Fig. 2).

2.3. The Machine Translation System
Terminologies
and
Translation
Memories,
combined with the Machine Translation approach,
enable the automatic translation of all the pages of
interest.
So,
being multilinguality an important part of this
globalised society, the automatization of multilingual
corpora information access is the major step forward in
keeping pace with this challenging world, giving rapid
access to texts relevant even to non-native speakers .

2.4. The Classification System
The automatic classification of results is made,
fulfilling both the Supervised and Unsupervised
Classification schemas. The application assigns texts to
predefined categories and dynamically discovers the
groups of documents which share some common traits.
Figure 2 - Functional Search and Navigation

2.4.1 Supervised clustering. The categorization model
is created during the learning phase, on representative
sets of training documents focused on news about
Middle East - North Africa, Balkans, East Europe,
International Organizations and Rest Of the World.
The bayes rules are used in the learning method: the
probabilistic classification model is normally built on
around 1.000 documents. The overall performance
measures used are Recall and Precision: in our tests,
they are normally 75% and 80% respectively.

2.3.2. Natural Language Search. Users can search
documents by query in Natural Language, expressed
using normal conversational syntax, or by keywords
combined by Boolean operators. Reasoning over facts
and ontological structures makes it possible to handle
diverse and more complex types of questions.
Traditional Boolean queries in fact, while precise,
require strict interpretation that can often exclude
information that is relevant to user interests. So this is

430
436

Figure 3 - Search results. User can navigate on informations, accessing country, organization or humans’ factsheets

Figure 4 - Automatic Translation of search results

nuclear and defense relations with Iran in wake of the
election of a new president accused of the killing of
dissidents....” where only the “new president” - of
Iran, that’s Ahmadinejad! – is “accused of the killing
of dissidents…”. In this case, the system understands
that the president of Iran is Mahmoud Ahmadinejad
from the sentence “... in the wake of the election of
Mahmoud Ahmadinejad in June...” and uses this
anaphora resolution to hilite the selected functional
relation in the text.

2.4.2 Unsupervised clustering. Result documents are
represented by a sparse matrix, where lines and
columns are normalized in order to give more weight
to rare terms. Each document is turned to a vector
comparable to others. Similarity is measured by a
simple cosines calculation between document vectors,
whilst clustering is based on the K-Means algorithm.
The application provides a visual summary of the
clustering analysis. A map shows the different groups
of documents as differently sized bubbles and the
meaningful correlation among them as lines drawn
with different thickness (see Fig. 5). Users can search
inside topics, project clusters on lemmas and their
functional links. In example, when searching for
Ahmadinejad, the president of Iran, the system
suggests all the functional relations for which
Mahmoud Ahmadinejad is Subject, Object, etc: he is
subject for the verbs “criticize” and “present”; then he
is the object for the verb “accuse”. Let’s select the
functional relation “to accuse Ahmadinejad” and
search for the documents which contain this relation,
even in its anaphora form, as displayed in Fig. 6. The
system hilites the sentence “RUSSIA REAFFIRMS
COOPERATION WITH IRAN Russia plans to expand

3. Conclusions
This paper describes a system used by some
security sector-related government institutions and
agencies in Italy to limit information overload in
OSINT. This linguistic approach enables the research,
the analysis, the classification of great volumes of
heterogeneous documents, helping operative officers to
cut through the information labyrinth, mastering the
problems of multilinguality, rapidly accessing all the
potential texts of interest.

431
437

Figure 5 - Thematic map, functional search and projection inside topics

Figure 6 - Sentences hiliting
[7] McCord, M. C., Slot Grammar: A System for Simpler
Construction of Practical Natural Language Grammars
Natural Language and Logic 1989: 118-145. .
[8] Neri, F., Raffaelli, R., Text Mining applied to
Multilingual Corpora, Knowledge Mining : Proceedings
of the NEMIS 2004 Final Conference, Springer Verlag
Pub., Spiros Sirmakessis Ed., ISBN-13: 9783540250708.
[9] Baldini, N., Neri, F., Pettoni, M., A Multilanguage
platform for Open Source Intelligence, Data Mining and
Information Engineering 2007, 8th International
Conference on Data, Text and Web Mining and their
Business Applications, The New Forest (UK),
Proceedings,
ISBN:
978-184564-081-1,
WIT
Transactions on Information and Communication
Technologies, Vol 38, June 2007, 18-20.
[10] Neri, F., Pettoni, M., Stalker, A Multilanguage platform
for Open Source Intelligence, Open Source Intelligence
and Web Mining Symposium, 12th International
Conference on Information Visualization, Proceedings,
ISBN:978-0-7695-3268-4, pp. 314-320, IEEE Computer
Society, LSBU, London (UK), July 2008, 8-

4. References
[1] Grishman, R., Sundheim, B., Message Understanding
Conference - 6: A Brief History. In: Proceedings of the
16th International Conference on Computational
Linguistics (COLING), I, Kopenhagen, 1996, 466–471
[2] Hearst, M., HYPERLINK "Untangling Text Data
Mining”, ACL'99. University of Maryland, June 20-26,
1999
[3] Miller, H.J., Han, J., Geographic Data Mining and
Knowledge Discovery, CRC Press, 2001.
[4] Li Wei, Eamonn Keogh, Semi-Supervised Time Series
Classification, SIGKDD, 2006.
[5] Carreras, X., Màrquez, L., Introduction to the CoNLL2005 Shared Task: Semantic Role Labeling. In CoNLL2005, Ann Arbor, MI USA, 2005
[6] Vossen, P., Neri, F. et al., KYOTO: A System for
Mining, Structuring, and Distributing Knowledge
Across Languages and Cultures, Proceedings of GWC
2008, The 4th Global Wordnet Conference, Szeged,
Hungary, January 2008, 22-25.

432
438

