2009 13th International Conference Information Visualisation

Identifying Social Communities by Frequent Pattern Mining
Muhaimenul Adnan⊗

Reda Alhajj⊗,♣

Jon Rokne⊗

⊗

♣
Dept of Computer Science
Dept of Computer Science
University of Calgary
Global University
Calgary, Alberta, Canada
Beirut, Lebanon
{madnan,alhajj,rokne}@ucalgary.ca

Abstract

how users of these data are connected to each other.
Connecting people to useful information and connecting people to information to which other people are
connected are important information that may provide
useful decision making power to the organizations, yet
received little attention.
Social networks represent relationships or ties
among individual or actors. Formally, a social network
can be described as a graph G = (V, E), where V =
{v1 , v2 , v3 , , vn } is the set of vertices representing individuals and E is the set of edges representing relationships between vertices or individuals. In early days, social networking concept was limited to only friendship
networks like facebook (www.facebook.com), orkut
(www.orkut.com), myspace (www.myspace.com) etc.
But in recent years, it has been discovered that many
complex and real world system from diverse fields can
be modeled as social networks [19]. Identifying community structures based on common actor-interest from
social networks is an important aspect of data mining
and there has been a significant amount of research literature [15, 10, 17, 7, 20, 16, 4] that aim to find community structures from social networks. Knowledge of
communities from these social networks can help us in
various applications such as product marketing, identifying web communities to make better web structure
design, identifying organizational dynamics, etc.
In this paper, the author argue that modeling social
networks from data sources is another important aspect
of social networking research beside the community
structure mining. Different social networks can be formulated from the same data sources based on different
modeling scheme of the data. But care must be taken
so that when the social networks are extracted from
the data sources, they represent the underlying concepts accurately. How a user of the data is connected
to the information represented by the data should be
the main focus. This manuscript addresses the above

This paper presents a social network modeling technique that models the data to be analyzed to create
a social network as frequent closed patterns. Frequent closed patterns have the advantage that they successfully grab the inherent information content of the
dataset and is applicable to a broader application domain. Entropies of the frequent closed patterns are used
to keep the dimensionality of the feature vectors to a
reasonable size. Experimental results presented in the
paper shows that social network produced from these set
of features successfully carries the community structure
information.

1

Introduction

With the advancement of information technology
such as the development of storage media, the improvement of the computing power of the commodity CPUs,
and the availability of internet and broadband telecommunication; business organizations are now storing
more and more data in an attempt to add value to
the services provided to the customers. But data remained in the raw format provide no or little decision
making power to the managing body of the organizations. Hence, several data mining techniques (such as
classification, clustering, association rule mining, time
series prediction) have been emerged or evolved to facilitate the decision making of the organization. These
data mining techniques help the organizations to extract the information or knowledge buried in the data
which is the key to the success of today’s organizations. Most of the data mining techniques concentrate
on finding information from data, or predicting future
outcomes, but they provide little or no knowledge on
978-0-7695-3733-7/09 $25.00 © 2009 IEEE
DOI 10.1109/IV.2009.49

413

mentioned issue. The author argues that the data to be
analyzed to form the social networks can be modeled as
frequent closed patterns. Moreover, we can select only
a few frequent closed patterns (thereby decreasing the
dimensionality) based on an entropy criterion for the
modeling purpose and this allows us to discover useful
communities from the dataset accurately.
The rest of the paper is organized as follows. The
related works are described in section 2. Section 3
presents the problem specification. The proposed
frame work is described in section 4. Section 5 provides the initial experimental results and the paper is
concluded in section 6.

2

single node communities to larger ones such that the
mean squared distances between nodes of a community
are minimized. One of the prominent graph theoretic
methods, K-means algorithm [12] starts with randomly
chosen initial set of k partitions. It then calculates the
centroids of all the partitions. A new set of k partitions are constructed based on these new centroids.
These processes are iterated repeatedly until the centroids converge. On the other hand, hierarchical approaches [17, 7, 20, 16, 4] construct a hierarchy of clusters either using agglomerative or divisive techniques.
Agglomerative methods start with an empty network
and assign edges to networks based on some similarity
measure. On the other hand, divisive methods repeatedly remove edges from networks. Girvan and Newman [7], presented a divisive hierarchical approach that
works on the idea of edge betweenness. Edge betweenness is a measure, which presents the number of shortest paths( between vertices) that edge takes part in. If
betweenness for a edge is high it is assumed that edge is
the connector of two loosely connected groups. Girvan
and Newman [7] repeatedly removes edges with highest betweenness for a network until no edges remain.
It has been observed, by the researchers that hierarchical approaches produce better quality grouping that is
most suitable for social network analysis.
The idea of using frequent patterns for cluster analysis is not new. Beil, et al. [3] proposed two text clustering methods based on frequent itemsets that appear
in text documents. Algorithm FTC is used to find the
flat clustering and algorithm HFTC is used to find the
hierarchical clustering of the text documents. The authors argue that by using frequent patterns to describe
the clusters, one can reduce the dimensionality of the
document vector space by a significant amount which
is otherwise large. Their idea is to find a clustering
description as a subset of frequent itemsets such that
the description covers the whole database. Members of
the clustering description are chosen sequentially using
a greedy method based on some entropy overlap score.
In the proposed social network extraction framework
presented in this literature, similar entropy measure
was chosen to rank the significant features presented
in the social network nodes. In [22]. the authors used
closed termsets to cluster text documents rather than
using frequent itemsets. By using closed interesting
patterns, they are able to reduce the dimensionality
even more and improve clustering quality.

Related Works

We know that, social networks are usually represented as graphs or networks. Each edge in the graph
or network presents a relationship; and strength of
a relation ship is depicted by the weight associated
with each edge. Moreover, each edge can be directed
or undirected presenting an asymmetric or symmetric relationship. How the social networks of relations
are constructed, depend on application to application.
However, data mining and machine learning techniques
can provide significant insight on the modeling of social networks based on data properties. Relationship
between actors can be established by Google webpage
frequency [14, 18], or by using text-based segmentation
of chat room conservations [11]
or by using the influence diffusion model [13], where
relationship is based on frequency of terms propagated
between two individuals. But most of the modeling
methods are application specific and lacks a generalized guideline to model the social networks. In this
paper, the author presents a generalized framework to
model social networks using a well known data mining
technique called frequent pattern mining [1], which is
suitable for many application domains.
Once the social network models are created, the next
task is to identify the interesting groups of communities from the networks. There has been a significant
amount of research literature that focuses on finding
groups or communities from social networks. Based on
the basic concepts of how the grouping is performed
they can be categorized into tow main branches: partitioning based or graph theoretic methods, and hierarchical methods. Some of the notable graph theoretic methods to identify communities from social networks includes [15, 10, 12]. Some of them operate by
repetitively bisecting the graphs to find the required
number of communities or clusters; others use the concept of random walk to grow the communities from

3

Problem Statement

Let us assume that, we have a set E of n entities
{e1 , e2 , , en }. Each entity ej is associated with a dataset
414

Dj that represents the information regarding the entity
ej . Here, the dataset D = j Dj , ∀j ∈ {1..n} is application specific and must be analyzed to identify the
social network of the entities. So, a social network extraction model tries to create a social network G(V, A),
where V = {v1 = e1 , v2 = e2 , v3 = e3 , ..., vn = en } is
the set of entities represented as the vertices of the
graph G and A is the set of arc or edges representing
the relationships among the entities. Moreover, the social network extraction should be done in such a way
that it best represents the information usage by the
entities and is generalized with regard to the type of
concepts the dataset D represents. Here, our focus is
on the relationships that the edges represent so that
the weight of an edge between two entities ei and ej is
higher if their respective data usage is similar.

4

The Proposed Framework

To create a social network for the entities
{e1 , e2 , , en }, one can extract the important features
from the dataset Dj associated with each entity. Then
these features can be used to create a feature vector that will describe each entity. But care must be
taken so that we only extract the useful information
from the datasets and the dimensionality of the feature vectors representing the entities are reasonable.
Let us represent the feature vector related to entity
ej as F j = (w(f1 ), w(f2 ), , w(fm )), where w(fk ) is the
weight of the k-th feature, fk in entity ej . So, the similarity between entity ei and ej can be calculated as the
normalized dot product (F i • F j /||F i ||.||F j ||) of their
feature vectors. Once we have the measure of distance
and similarity we can use many standard community
extraction techniques to identify the communities.
In this paper, the author argue that frequent closed
patterns can be used to generate the features that represent most of the useful information about an entity
and can be used to create the social networks. Using
frequent closed patterns also helps to keep the dimensionality of the feature vectors to a reasonable size.
Figure 1, presents a framework for social network
analysis that is proposed in this manuscript. It has a
feature extraction model that uses frequent closed patterns to create a feature set of reasonable size. Social
networks can be created from the extracted features
in social network creation model. Statistical analysis
and social network visualization tools can be used to
provide feedbacks to the feature selection and network
creation steps. Next, each of the components of the
framework is described in brief.

Figure 1. The Proposed Framework

4.1 Feature Extraction Model
In order to, best represent the user connectivity
with the data this model tries to create a feature
vector to represent a user or entity. Here an entity
may represent an individual, a set of individuals,
a branch of a store, etc depending on the goal of
the organization. It is expected that each entity ej
is associated with a dataset Dj . The dataset D is
first decomposed into user tasks where each task
represents a set of transactions or accesses to data.
Then frequent closed patterns can be identified using
one of the frequent closed pattern mining techniques
like CHARM [23]. Let the set of all frequent closed
patterns be P = {Pi |Pi is a closed pattern and
sup(Pi ) ≥ minsup ∈ D}. Let us also define nj as the
number of all frequent closed patterns supported by
dataset Dj . Following a methodology similar to that
of Beil et al. [3], the entropy for the frequent closed
pattern Pi can be defined as:
1
Entropy(Pi ) = Dj s.t. Pi is f requent in Dj [ −1
nj .ln( nj )]
The main motivation behind such a measure is that
it allows us to identify frequent closed patterns for
which the information content is higher. The set of frequent closed patterns are then sorted in ascending order and ranked according to their entropy values. Only
top few closed patterns are selected as features (based
on a user specified threshold) to keep the number of
features to a reasonable size. But ranking them using
entropy ensures that only best features are selected. As
a result we get a set of frequent closed patterns that
make the features of the feature vectors (the feature
vector for entity ej is, F j = (w(f1 ), w(f2 ), , w(fm )),
where w(fk ) is the weight of the k-th feature, fk in entity ej .). The weights of each feature is calculated using
the following formula, w Dj (fk ) = supDj (fk )/supD (fk ),

415

where wDj (fk ) is the weight of the feature k for entity
ej , supD (fk ) is frequency of feature fk across dataset
Dj of entity ej , and supD (fk ) is frequency of fk across
dataset D of all entities E.

transactions each, where the average transaction size
is 40 and the number of items in the database is 1000.
The 3 sets of data were generated in 3 separate runs so
that each dataset contain transactions of similar properties. Next, each of the dataset is further partitioned
into 3 more sets of size 10K transactions, where each
smaller dataset set is assigned to an entity. Thus, 9
entities are created and each entity is associated with
a 10k transactional dataset. Here, the datasets of entities e3n−2 , e3n−1 , e3n (for n = 1..3) come from a single
dataset of 30K transactions, and hence posses a similar
property. The rationale behind such a creation method
is that the set of entities e3n−2 , e3n−1 , e3n are likely to
fall into a single group forming a community or cluster.

4.2 Social Network Creation Model
The feature vectors created in the feature vector creation model serve in calculating the weights of each
link or edge of the social network to be created. Here
the weight of each link is inversely proportional to the
Euclidian distance of two feature vectors of the two
entities that the edge connects. Once the social network is created this model also helps in identifying the
community structures of the particular social network
created.

4.3 Statistical Analysis Model
This model will provide all the statistical analysis
of the social network created in the previous model.
Tools like UCINET (UCINET: Social Network Analysis Software) or JUNG (JUNG: Java Universal Network/Graph Framework) can help the data miner with
centrality measures, page rank measures, clustering,
decomposition and other elementary graph theory, and
permutation-based statistical analysis methods to analyze the identified social networks. Moreover, based on
the feedbacks from these analyzes the data miner can
adjust the feature vectors to achieve a refined social
network structure from the given dataset which represents the dataset in a better way.

Figure 2. The entities and their corresponding
Euclidean distances
All the datasets of individual entities are combined
to create a 90k transactional dataset D. Frequent
closed patterns are mined from this dataset using the
closed pattern mining implementation by [6] with an
absolute support threshold of 200. About 900 frequent
closed patterns were identified using this way. Then the
entropies of all the frequent closed patterns ware calculated. Based on the entropy ranking 11 features were
selected that possessed most information contents. The
features are {318, 170}, {585, 779}, {589, 886, 779,
813}, {577, 759, 188}, {339, 135, 507, 59}, {336, 63,
258, 130}, {238, 451, 575}, {40, 179, 693}, {253, 225,
533}, {524, 331, 701}, {623, 264, 640}.
Figure 2 represent a matrix for each of the 9 entities
where each cell in the matrix represent the Euclidean
distances between entity ei and ej with respect to their
feature vectors, where i is the row number and j is
the column number. We can observe from this matrix
that the entities that belong to the same group/cluster
have low Euclidean distances among themselves; where
as the entities belong to different groups/clusters have
high Euclidean distances among themselves. Low Euclidean distance values in the matrix are colored with
red, green and blue colors to represent each of the three
groups/communities. So, the useful grouping information is successfully portrayed by the proposed social
network extraction method, where features are created

4.4 Visualization Model
Tools like UCINET and JUNG can also be used to
have a visualization of the social network created in
the social network creation model. Based on the visualization of the network, and the statistical measures
collected at the previous phase, data miner guides a
feedback to feature creation model to adjust the features appropriately.

5

Experimental Results

5.1 Synthetic dataset
In this section, the proposed social network extraction model is tested for synthetically generated set
of entities and their corresponding data repositories.
First a synthetic data generator IBM Quest [2] is used
to generate 3 sets of transactional database of size 30K

416

using frequent closed patterns.

Figure 3. The Enron E-mail users and their
corresponding Euclidean distances
Figure 4. The Enron E-mail users and their
social network based on e-mail usage

5.2 Enron E-mail dataset
For this experimental setup we have used the Enron
e-mail data (http://www-2.cs.cmu.edu/~enron/)
which was publicly made available for researchers. This
dataset contains 500,000 e-mail messages over 150 Enron employees. For our analysis, we only considered
that e-mails that are found in the inbox and the people for whom there are more than 1000 e-mails in the
inbox. From these people or user set we have chosen a
limited number of 15 users randomly in order to present
clearly in this manuscript. From each user’s inbox we
have chose 1000 e-mails randomly that makes the email dataset for the corresponding user. Each e-mail
is parsed to identify the stem words which make the
item set. Moreover, each e-mail addresses inside the
e-mails are identified as items as well. The stem words
appearing in the subject line of the e-mails are also
considered as items. These items appearing in a single
e-mail are considered as a transaction. This way for
each user we make a transactional database of items
that appear in user e-mails. Here each transactional
database consists of 1000 e-mail transactions. From
these transactional databases we like the previous subsection identify the global frequent closed itemsets (corresponding to a support of 10%) and based on entropy
ranking we chose top 100 closed itemsets as our feature
set. Figure 3, presents the 15 Enron e-mail users and
their corresponding Euclidean distances based on the
E-mails in inbox. Here low distance means the users
are related and high distance means the users are relative unrelated.
Figure 4, presents the social network representation
of Figure 3. Here, we only considered the edges if the

Euclidean distance between two entities is less than
a user specified threshold of .30. Here, it is clearly
depicted that some users are related to many other
users; whereas some users are isolated. For example,
kamiski is related to 8 other users and on the other
hand, may is only related to keavy.

6

Conclusion

This paper presents a social network modeling technique that models the data to be analyzed to create a
social network as frequent closed patterns. Using frequent closed patterns, we are able to create a useful
set of features to represent an entity that describes the
connection of the entity to data in a reasonable way.
Moreover, entropies of the collected features are used
to find a reasonable set of features while keeping the
information content high. The feature vectors created
in this way facilitate a social network extraction model
that maintains all the community structure information. The initial result presented in this paper also
verifies the above conjecture for the synthetic datasets.

References
[1] Agrawal R., Imieliski T. and Swami A., “Mining
association rules between sets of items in large
Databases.” Proceedings of ACM SIGMOD international conference on Management of data,
pp.207-216, 1993.

417

[2] Agrawal R., Mehta M., Shafer J., Srikant R., Arning A. and Bollinger T., “The quest data mining
system.” Proceedings of the 2nd Int’l Conference
on Knowledge Discovery in Databases and Data
Mining, pp.244-249, 1996.

[13] Matsumura N., Goldberg D. and Llora X., “Mining directed social network from message board.”
WWW ’05: Special interest tracks and posters of
the 14th international conference on World Wide
Web, pp.1092-1093, 2005.

[3] Beil F., Ester M. and Xu X., “Frequent term-based
text clustering,” Proceedings of ACM Int. Conf.
on Knowledge Discovery and Data Mining, 2002.

[14] Mika P., “Bootstrapping the FOAF-web: An experiment in social network mining.” Proceedings
Of the 1st Workshop on Friend of a Friend, Social Networking and the Semantic Web, Galway,
Ireland, pp.1-2, 2004.

[4] Flake G. W., Lawrence S., Giles C. L. and Coetzee, F. M., “Self Organization and Identification
of Web communities,” Computer, 35(3), pp.66-71,
2002.

[15] Palla G., Derenyi I., Farkas I. and Vicsek T., “Uncovering the Overlapping Community Structure of
Complex Networks in Nature and Society,” Nature, 435, 7043, pp.814-818, 2005.

[5] Garofalakis M. N., Rastogi R. and Shim K.,
“SPIRIT: Sequential Pattern Mining with Regular
Expression Constraints,” Proceedings of the International Conference on Very Large Data Bases,
pp.223-234, 1999.

[16] Radicchi F., Castellano C., Cecconi F., Loreto V.
and Parisi D., “Defining and Identifying Communities in Networks,” Proceedings of Nat’l Academy
of Science (PNAS), 101, pp.2658-2663, 2004.

[6] Grahne G. and Zhu J., “Efficiently using prefixtrees in mining frequent itemsets,” Proceedings of
ICDM Workshop onFrequent Itemset Mining Implementations, 2003.

[17] Scott J. Social Network Analysis: A Handbook
(2nd ed.) Sage Publications, 2000.
[18] Staab S., Domingos M. P., Golbeck J., Ding L.,
Finin T., Joshi A., Nowak A., Vallacher R., “Social networks applied,” IEEE Intelligent Systems,
20, pp.80-83, 2005.

[7] Girvan M. and Newman M.E., “Community
Structure in Social and Biological Networks.” Proceedings of Nat’l Academy of Science (PNAS), 99,
pp.7824-7826, 2002.

[19] Strogatz S. H., “Exploring Complex Networks,”
Nature, 410, 268-276, 2001.

[8] Han J., Pei J., Mortazavi-Asl B., Chen Q., Daval
U. and Hsu M.-C., “FreeSpan: frequent patternprojected sequential pattern mining,” Proceedings of ACM SIGKDD international conference
on Knowledge discovery and data mining, pp.355359, 2000.

[20] Tyler J. R., Wilkinson D. M. and Huberman B. A.,
“Email as Spectroscopy: Automated Discovery
of Community structure within Organizations,”
Proceedings of First Int’l Conf. Communities and
Technologies, pp.81-96, 2003.

[9] JUNG: Java Universal Network/Graph Framework. (n.d.). Retrieved Oct 31, 2008, from http:
//jung.sourceforge.net/

[21] UCINET: Social Network Analysis Software.
(n.d.). Retrieved Oct 31, 2008, from http://www.
analytictech.com/ucinet/ucinet.htm.

[10] Kernighan B. W. and Lin S., “An Efficient Heuristic Procedure for Partitioning Graphs.” Bell System Technical, 49, pp.291-307, 1970.

[22] Yu H., Searsmith D., Li X. and Han J., “Scalable Construction of Topic Directory with Nonparametric Closed Termset Mining.” Proceedings
of IEEE International Conference on Data Mining
(ICDM’04), pp.563-566, 2004.

[11] Khan F., Fisher T., Shuler L., Wu T. and Pottenger W., “Mining chat room conversations for
social and semantic interactions,” Tech. Rep. LUCSE-02-011, Lehigh University, Bethlehem, PA,
2002.

[23] Zaki M. J. and Hsiao C.-J., “CHARM: An efficient algorithm for closed itemset mining.” Proceedings of the 2nd SIAM International Conference on Data Mining, 2002.

[12] Macqueen J. B., “Some methods of classification
and analysis of multivariate observations,” Proceedings of the 5th Berkeley Symp. On Mathematical Statistics and Probability, pp.281-297, 1967.

418

