2012 16th International Conference on Information Visualisation

Evaluation in Information Visualization: Heuristic Evaluation

Camilla Forsell
C-research
Linköping University, Norrköping
Sweden
camilla.forsell@liu.se

aim of conducting an evaluation study and the current state
of development.
This paper presents a review of a method called heuristic
evaluation (HE) [3]. This method has been highly popular
and influential within the area of human-computer
interaction (HCI) since it was first presented in 1990 [4]. The
infovis community has also recognized its usefulness [5].
The method is relatively easy to learn, quick and costeffective to perform and it can generate valuable results at
any stage of development. However, in its original form it
has limitations, which need to be considered in order for it to
be optimal for information visualization [5, 6]. The aim with
this paper is to provide a review of the method and discuss
issues that call for supplemental or modified actions,
materials and other resources in order for the method to
generate valid and useful results. The key contribution is an
assessment of the value of one important method for
evaluation that is available for researchers and developers in
information visualization together with guidance and
recommendations on how it should be applied to be
successful for the community.
The remainder of this paper is organized as follows. The
next section presents an overview of usability and evaluation
methods. Section 3 focuses on heuristic evaluation. It
describes the method and how it is conducted in its original
form. Section 4 reviews the heuristic sets proposed and used
for infovis in the literature. Section 5 outlines issues to
consider and recommendations for use of the method in
infovis. The final section, 6, concludes the paper and outlines
some important issues for future research.

Abstract—This paper presents a review of heuristic evaluation
and recommendations for how to apply the method for
information visualization evaluation. Heuristic evaluation is a
widely known and popular method within the area of humancomputer interaction and the information visualization
community now also recognizes its usefulness. However, in this
area it is not applied to the same extent. In its original form the
method has limitations that need to be considered in order for
it to be optimal for information visualization. The aim with this
paper is to provide the reader with knowledge about the
method and awareness of what issues that call for refined or
supplemental actions and resources in order for it to generate
as valid and useful results as possible. The paper also discusses
the research challenges for future work in how to further
improve the method.
Keywords-evaluation;
visualization.

I.

heuristic

evaluation;

information

INTRODUCTION

In information visualization (infovis) we generate
impressive images and structures of data, and complex
interactive tools to support exploration, analysis and
decision-making. The purpose of any information
visualization process is to promote insight and understanding
[1]. This implies that the goal when developing is not only to
create a visualization of data but also to create a perceivable,
comprehensible and useful visualization of data. Therefore
usability needs to be an integral part of development.
Usability is according to the International Organization for
Standardization (ISO): “The extent to which a product can be
used by specified users to achieve specified goals with
effectiveness, efficiency and satisfaction in a specified
context of use” [2]. Then how do we know if our
visualizations are really useful for real users doing real tasks
in the context in which they are supposed to be using them?
The answer is that it is necessary to evaluate.
Evaluation can be performed in order to assess any
dimension of usability, it can be performed at any stage of
development and there is a wide range of methods proposed
for use. Each method has its own requirements and generally
different methods uncover different usability issues. Further,
some of them can only be applied after the information
visualization technique has been implemented whereas
others can also be applied in the early stages of design and
development. Thus what technique to apply depends on the

1550-6037/12 $26.00 © 2012 IEEE
DOI 10.1109/IV.2012.33

II.

AN OVERVIEW OF EVALUATION

In the past visualization techniques were (in most cases)
presented without reporting any evaluation results. Work
focused primarily on technical issues. The prerequisites of
the users, regarding their abilities for dealing adequately with
the representations and making sense of them, on the other
hand received limited focus. In 2000 a special issue of the
International Journal of Human-Computer Studies [7]
emphasized the need for a shift and since then usability (as
defined in the introduction [2]) and evaluation is considered
to be highly important [8]. Today, user considerations and
evaluation is also part of most calls for publication in the
area of information visualization. Evaluation with potential
users (or other representative participants) offers a sound
scientific approach to assess visualization. Results from
136

evaluations provide a basis on which the next generation of
development can be built with greater confidence and they
present evidence that may encourage adoption by industry or
the public [8, 9].
A suitable definition of evaluation is, “… the systematic
acquisition and assessment of information to provide useful
feedback about some object” [10]. Thus the general purpose
of evaluation is to get "useful feedback". More specifically
the purpose of performing a usability evaluation can be to:
(a) obtain feedback and guidance for improving design and
development, (b) measure in some way whether user and/or
organizational objectives, e.g., requirements and usability
goals, have been achieved, (c) investigate long term use
(often) in a realistic usage environment [11]. In [12] the
authors discusses reasons for evaluation in visualization and
provides useful examples of how and when to conduct
studies and how to make use of the results.
How to proceed to accomplish these purposes depends on
several important issues: the phase of development and the
state of the visualization, the level of control the researcher
can apply to the study, his or her expertise, resources
available etc. Most important the choice of method and
procedure for any evaluation must stem from and be
appropriate for the actual problem or research question that
is related to the visualization.
When it comes to classifications of evaluation methods
there exist a great variety [see e.g., 9, 13]. Bowman, Gabbard
and Hix [14] presents a classification of usability evaluation
methods for virtual reality, which is also relevant to
information visualization. In this structure the authors
compare methods according to three key characteristics (1)
whether representative users participate or not in the
evaluation, (2) the context of the evaluation and (3) the types
of data/results that are obtained. This classification is used to
structure the following text.
The first characteristic concerns whether a method
require participation of users or not. Methods that involve
representative users i.e. user studies, are also known as
empirical studies. Methods that do not involve users are
called analytical and here experts (in usability or the actual
domain) are engaged instead i.e. they are often called expert
reviews [15, 16]. Analytical evaluation method collects data
either by analysis (inspection or walk-trough) of a system
and heuristic evaluation is a well-known example. It can also
be performed by prediction by theoretically based models
e.g., Fitts’ law that predicts performance when interacting
using a pointing device [see chapter 15 in 17]. Analytical
evaluations are effective: they are quick to plan and perform,
requires less participants than user studies and can find
usability problems early in a design that users do not need to
experience, i.e. “have to waste time on”. However, they
should not replace validation by evaluations with users but
rather be used as a complement [15].
The next characteristic is about in what context the study
is conducted i.e. whether the context is generic or application
specific. The nature of the context is important for how
results can be applied and generalized and the more generic
the context the more flexibility (generally speaking).
Regarding the extent to which results can be applied and

generalized to other contexts the level of constraints applied
in the study is also a determinant factor. It is about to what
extent the researcher imposes precision, structure and control
[18] on any part of the study. This level ranges from lowlevel constraint evaluation (e.g., naturalistic observation,
field studies) to high-constraint studies conducted in a
strictly controlled laboratory setting (also known as
experimental studies) and where the nature of processing the
results also is characterized by high demands [18]. The more
that is controlled in a study the more confident one can be
that any results are due to what is actually investigated and
not to any irrelevant (so called confounding) factors.
However, a controlled setting can be unrealistic and not
representative for real use by real users. Therefore there is a
trade-off between control and the usefulness of results that
needs to be taken into careful consideration.
Finally, the third characteristic distinguishes between
evaluation methods in terms of them generating quantitative
or qualitative data. A quantitative evaluation usually consists
of testing hypothesis trough some direct measurement, for
example users performance in terms of accuracy and
response times on specific tasks. Data are thereafter analysed
by means of statistical tests. A qualitative evaluation on the
other hand involves analysing qualitative data that is
obtained by means of interviews, questionnaires,
observations and think-aloud methods [13, 19]. Such study is
very useful in itself. Quantitative studies often focus on
perceptual or simple cognitive tasks. Issues on a higher level
such as exploration, insight and decision-making can be hard
to assess by objective and quantitative approaches. However
it is possible as evident by the work on how to measure
insight by Saraiya, North and Duca [20]. A qualitative study
can also complement a quantitative approach in order to
obtain subjective views from participants and to further
explain quantified data.
There is also another classification that is relevant and
that answers the question when to use what method. A
formative evaluation provides feedback that can be used to
improve design during development. It answers questions
such as what problems happen and why, and what can be
done to improve the visualization. A summative evaluation
on the other hand assesses to what extent user and
organizational objectives have been achieved at the end. It
answers questions such as how effective and efficient is the
visualization to users i.e. is this working as it should? [19]. A
formative and summative evaluation can be conducted by
using either a quantitative or a qualitative method and at all
levels of constraints.
To conclude this section, it should be emphasized that all
approaches to evaluation are scientific and useful (not only
high constraint ones) when used properly.
III.

HEURISTIC EVALUATION

Heuristic evaluation [3, 16, 21] is an analytical method
that belongs to the family of usability inspection methods.
Usability inspection is the generic name for methods where
evaluators inspect the subject for evaluation e.g., an interface
[16]. The purpose is typically to find usability problems
however heuristic evaluation may also address matters such

137

as the severity of the problems found, how they can be
overcome by redesign or the overall usability of an entire
system. Heuristic evaluation can be used at all stages of
development, from early design ideas and prototypes where
the purpose is formative and the results are used for
improvements in an iterative process, to evaluation of more
mature systems where the intent of the evaluation can be
summative. The remaining part of this section describes the
method further and also explains how an evaluation is
conducted as originally proposed.
Heuristic evaluation is essentially informal and
subjective. The evaluators, which are experts in usability (not
potential users), inspect the subject for evaluation and judge
how it complies with a set of recognized usability principles
known as heuristics [3, 21]. They inspect it by using it and it
is suggested that this is done in several rounds; e.g., a first
round to explore the interface and obtain a general feel for it
and then one or more rounds where each interface item is
inspected and judged by using all heuristics. The evaluators
identify general usability problems that users can be
expected to encounter in the opinion of them selves. Each
problem is given a severity rating (described below) and
some times the evaluators also provide suggestions on redesign to overcome the problems and improve the usability.
The results are recorded thoroughly including a description
of the problem, it’s place of occurrence, which heuristic/s
was violated by the design (can be more than one for each
problem) and the severity rating [22].

Specific user considerations can also be taken into
consideration, e.g., any difference between novice and
experienced users, differences in age, gender etc.
In heuristic evaluation each evaluator do the inspection
alone and independently of the others. This is to assure
unbiased results. At the end of the evaluation they are
allowed to discuss with each other and then the results are
compiled into one report. Here duplicates are removed and
each problem is given one final severity rating that is
negotiated by all evaluators.
During evaluation the findings are traditionally recorded
in a report written by the evaluator him/her self. Another
approach is to let the evaluator verbalize the results to an
observer that takes notes instead. This may reduce the load
for the evaluator when doing the actual inspection and thus
the assessment of the interface and if appropriate the
observer (the evaluation manager) can also be responsible for
the aggregation of all findings.
Heuristic evaluation is known to be cost-effective, quick,
intuitive, easy to learn and simple to administer. In a survey
amongst usability practitioners it was rated as one of the top
methods [23]. Conducting a heuristic evaluation does not
require predefined measures of performance or a flawless
system. Basically, what is required is something that explains
the system that shall be evaluated, and that can range from a
set of sketches on paper giving an overview of it (and
perhaps its functionality) all the way to a fully functioning
system that is in use in the field [21].
Regarding the participant pool it is, according to Nielsen
[3, 21], enough with around 5 evaluators. This number of
experts is based on a statistical formula that claims they will
discover around 75% of the overall usability problems. In
accordance with this recommendation one does not gain that
much additional information to motivate using larger
numbers considering the associated resources needed.
Heuristic evaluation has been, and still is, a very well
used and valuable method. For further details of the method
as described and used in HCI see [3, 16]. The disadvantage
with the method is that the standard approach it is not really
tailored to be successful for evaluation in infovis [5]. It
primarily assesses issues related to a graphical user interface
and the control of a system. It does not attend to the
information that is represented or the complex interaction
between these two important aspects. Thus it fails to capture
many important aspects e.g., deeper issues such as how well
users are supported in exploration and analysis of the data
that is represented. Further, experts in usability perform the
evaluation, rather than experts in the actual data domain
visualized by the system, or potential/real users in a specific
context. These other groups may not experience some
problems found by usability experts as real difficulties. And
also experts in usability only will fail to capture many
aspects of use since these are not really covered by the
method. The following two sections describe efforts that has
been undertaken to modify the method to be more suitable
for infovis and thus improve the validity and reliability of its
results. In the section below a number of heuristic sets
proposed and/or used in infovis are described and in section

The rating can be applied in a variety of ways but the
original scale used by Nielsen and Molich [4, 22] looks as
follows:
1. Cosmetic problem only. Not necessary to fix
unless time is available.
2. Minor usability problem. Fixing this problem
should be given low priority.
3. Major usability problem. Important to fix, this
problem should be given high priority.
4. Usability catastrophe. It is imperative to fix this
problem.
There can also be a fifth item on the scale:
0. I do not agree to this being a usability problem.
This option can be used when the results from several
evaluators are discussed and aggregated, se below.
The severity rating allocated to each problem is
dependent by three factors [22]:
• Frequency: is the problem common and affects such
interactions or functions that are used often or does it
happens on rare occasions only?
• Impact: is it easy or hard for a user to overcome the
problem when it occurs?
• Persistence: does the problem happens once only and
disappears when it is known or will it keep causing
trouble?

138

5 issues to consider, and recommendations for how to
proceed, when applying the method are discussed.
IV.

knowledge tasks that should be supported by an infovis
system and which could be used as heuristics. This set
includes heuristics such as “confirm hypothesis” and
“formulate cause and effect”.
Schneiderman [35] proposed a set of guidelines to aid
successful information seeking also known as the “Visual
Information Seeking Mantra”. This set consist of seven tasks
that should be supported such as “filter” and “details on
demand”. This set have been highly influential within the
infovis community both to inform design and evaluation.
The interested reader can also see [36] for a discussion of
how it is considered useful among practitioners.
Freitas et al. [37] stressed the importance of assessing
both the visual representation and the interaction
mechanisms provided by an infovis technique. Based on
previous work by Scapin and Bastien’s who introduced a
large set of criteria for the evaluation of the ergonomic
quality of interactive systems [38], they proposed a set of
ergonomic criteria that encompass both these aspects. In
subsequent work [39] the criteria were used in an evaluation
that also compared them with Nielsen’s heuristics and
Scapin and Bastien’s ergonomic criteria. When proposed the
set was primarily tailored to evaluate hierarchical data
representations but the authors suggest their applicability for
other techniques as well.
It is evident that there is an ongoing effort in developing
heuristics for infovis and all the sets proposed so far cover
important aspects of use. It is up to each researcher to decide
what set that is appropriate to use. Still there exist no general
set to stand-alone that can be applied for any visualization
technique, equivalent to Nielsen’s set of Ten Usability
Heuristics for the HCI community. In [6] the authors
presented a first effort in deriving such set. Based on 63
heuristics from 6 published sets (including most of the sets
described above) they synthesized 10 heuristics that as a set
had the greatest explanatory power (using the same approach
as Nielsen as described in the beginning of this section). The
set has later been applied for evaluation in e.g., [40] and
[41].

HEURISTIC SETS

In heuristic evaluation the evaluator uses a set of
heuristics, i.e. usability principles or guidelines for good
design, and check whether the system follows these
heuristics or not. Nine usability heuristics were initially
proposed for this method by its originators [4] and one was
added later [24]. These ten heuristics were refined in a study
where a factor analysis was done on 249 found usability
problems from 11 systems in order to find the set having the
greatest explanatory power [24]. This study resulted in the
following set called Ten Usability Heuristics [3, 25] which is
the set one come to think of when the method is discussed:
(1) visibility of system status, (2) match between the system
and the real world, (3) user control and freedom, (4)
consistency and standards, (5) error prevention, (6)
recognition rather than recall, (7) flexibility and efficiency of
use, (8) aesthetic and minimalist design, (9) help users
recognize, diagnose and recover from errors, (10) help and
documentation. Each heuristic is also followed with an
explanation to aid the evaluator and generate ideas during
evaluation, e.g., heuristic (1) visibility of system status is
explained by: “The system should always keep users
informed about what is going on, through appropriate
feedback within reasonable time” [3, 25].
This original set has been widely used and successful
over the years. There is also an ongoing work in refining the
set and its applicability for different areas, platforms etc. For
example, the authors of the textbook Interaction Design [17]
provides an online interactive tool which suggests which
heuristics to choose for what artifact e.g., mobile phone,
tablet PC, interactive kiosk etc. [26].
The set of Ten Usability Heuristics do apply to infovis
however it is limited in many aspects and efforts are made to
overcome this limitation.
Existing research in the area of infovis includes heuristics
specific to certain visualization techniques. Baldano et al.
presented eight guidelines for the design and use of multiple
view representations [27]. Heer and Robertson [28]
presented a set of heuristics specific for creating effective
animated
transitions
between
common
statistical
representations of data. Mankoff et al. [29] proposed
heuristics to be used when evaluating ambient displays. They
also compared the outcome when using their heuristics with
Nielsen’s original ten by evaluating two displays and the
results where in favor of their heuristics.
Based on known theories in perception and cognition
from Bertin [30] Tufte [31] and Ware [32], Zuk and
Carpendale [33] aggregated a set of heuristics out of a vast
list generated from the three authors and then used this new
set in an analysis of a selection of eight uncertainty
visualizations. They suggest that the heuristic set should be
relevant to other types of visualizations too, however they
also emphasize its limited use due to the low-level nature of
the heuristics e.g., “Local contrast affects color and gray
perception”. Amar and Stasko on the other [34] hand
presented, although did not apply, a set of higher-level

V.

HEURISTIC EVALUATION MODIFIED

When it comes to how to conduct a heuristic evaluation
this process has developed just as the heuristics that are
proposed and used to include new areas of concern. The
following subsections present recommendations for how to
proceed and issues to consider for how to adjust the method
to the needs in infovis.
A. Applying the heuristics
Heuristic evaluation is a subjective procedure. The
evaluators are provided with a set of heuristics but they may
interpret and apply them different from each other. Each
heuristic should be described in detail and be presented
together with clear and rich statements or questions that aid
the evaluators in discovering usability problems. Hence, it is
imperative that the person in charge of the evaluation makes
sure that each heuristic is clear and detailed enough to be a
good cue and generate ideas during evaluation [21, 33]. This
also allows for a greater consistency in interpretation and

139

hopefully in how they are used to explain the usability
problems found. This is extra important to consider when the
heuristics used are not described in enough detail in the
original literature or when one develops new ones. Also,
overlap and redundancy can be an issue thus appropriate
descriptions and explanations are important to avoid that.
To promote evaluators’ skill in applying the given set of
heuristics instruction and practice (e.g., lecturing, tutorials,
self studies) is essential. This can also prevent that any
misunderstanding may result in false positives, or that
problems are missed or poorly understood [42].

less trivial problems and more domain specific) than they
should have had if working individually. Collaboration can
also overcome the risk of evaluators getting bored and
feeling unmotivated which may be one cause for false alarms
and missed problems [46]. Using collaborative software,
meaning that evaluators are allowed to view each other’s
results while working and later discuss them, has also been
shown to have several benefits. This process generates fewer
duplicates, which saves time both for the individual
evaluator and the evaluation manager that do not need to
remove them in the end. Also, evaluators can in a more
straightforward way come to an agreement on how to
classify their results which again saves time here in
aggregating the final list of problems [47].

B. Participants
In traditional heuristic evaluation usability experts are
engaged as participants. This means that they are not
knowledgeable in the data that is visualized, or the type of
problems and tasks that are solved using the visualization. To
overcome this flaw one can include experts in the data
domain, or actual users. Nielsen [16] also suggested
engaging double experts, i.e. experts both in usability and in
the domain.
When it comes the how many evaluators to engage the
general recommendation, according to Nielsen [3, 21] (as
also discussed in section 3) is five. This is also an
assumption that is widely embraced and used in practice
when applying the method. It is understandable since
researchers often face limited resources in terms of time,
ability to recruit participants etc. However, Nielsen himself
makes clear that the basis for using this number is that all
five are skilled usability experts, i.e. well trained, and that
the group is homogenous. Whit this figure one can expect to
find approximately 80% of the usability problems [3, 21].
Perhaps that is not enough. There are also studies that
presents the benefits of increasing the number [43] and
describes why five users are not enough to find the majority
of problems [44, 45] and that the number may vary with
what is evaluated and what type of problems (minor or
severe) that are found [45]. More important there is yet no
empirical evidence whether the recommendations of five
evaluators or any of the new findings will transfer and thus
be applicable for infovis. Therefore it is up the each
researcher to make an educated guess on how many
participants to engage. One recommendation is to look at
sample sizes from empirical research and another is to
carefully monitor the evaluation process and stop it when
adding more evaluators do not seem to add any new
information.

D. Tasks and scenarios
In principle, the evaluators in a heuristic evaluation
decide themselves how to proceed when doing the
inspection. They are not supposed to use the system as such
(to perform real tasks), but rather “just inspect it freely”. The
effectiveness of this procedure has been criticized [48] and
the results are obviously highly dependent on the skills,
experiences, motivation and curiosity etc. of the evaluator.
Another approach, which is highly recommended for
several reasons, is to provide the evaluators with typical
tasks or usage scenarios to direct the inspection. What
procedure to choose is obviously dependent on the aim with
the evaluation. Is the aim to get feedback on the overall
usability of the visualization, and the evaluators are double
experts, domain experts or real users with their own goals, it
may be possible to let them use the system without any
further instructions than to explore it thoroughly. If the
evaluators are fairly naive with respect to the domain of the
visualization, on the other hand, it will be necessary to assist
them to enable them to use it. For example, listing the steps a
real user would do when performing typical tasks.
Obviously, when the aim is to answer a specific question or
investigate whether some specific requirement or usability
goal has been achieved tasks are needed to make sure that
certain issues are covered by the inspection.
E. When and how to use
Heuristic evaluation can be performed at any stage of
development and it is fairly easy to learn and conduct. When
used with experts, rather than actual or potential users, as
evaluators it is recommended in
[15] that heuristic
evaluation is used as a complement to user studies and that
experts are engaged in a first, formative evaluation followed
by later summative empirical work. When users are engaged
in the evaluation it is highly useful for both intents.
Heuristics can also be used in other ways than as
described by the standard heuristic evaluation procedure. In
[49] Tory and Möller created a set of heuristics that they
used as statements in a questionnaire. After having solved
tasks using visualizations the participants rated on a 7-point
scale how they agreed to the statements. In [50] the authors
also applied heuristics in a post-test questionnaire where
participants rated their agreement with heuristics assessing
their experience with the visualization system they had used.

C. Collaboration
Another approach that can be used to improve heuristic
evaluation is to have evaluators collaborate in pairs.
Traditionally they are not allowed to discuss with each other
until they are finished with the evaluation in order not to
influence each other. However, collaboration can be useful
for several reasons. If it is not possible to recruit double
experts then one expert in usability and one expert in the
domain, or a user, can work together. In this vein such pair,
with two competencies and two set of experiences may
discover more problems or problems of another type (e.g.,

140

This questionnaire was also used as an interview guide when
discussing the outcome of the evaluation with each
participant.
In [40] the authors used the heuristics set proposed by
Forsell and Johansson [6] when analyzing and structuring the
results from a qualitative user study. The ten heuristics were
used to create categories into which the results were sorted.

load while working? Such questions cannot be fully
answered by objective quantitative methods measuring
performance. They need to be assessed and discussed in
terms of qualitative dimensions as expressed by heuristics
and guidelines.
Heuristics can also be useful in other ways than in the
standard approach to obtain feedback, e.g., used as
statements in questionnaires, or as categories when analysing
and reporting the outcome from using other methods such as
interviews and talk-aloud evaluations.
To improve the method further for use in infovis there
are some clear directions for future work that would be
interesting to see in the future. The heuristic sets described in
section 4 have with a few exceptions not been validated.
Lack of repeated use of the same heuristic set brings it
difficult to judge the usefulness of the heuristics, and to
compare results between different infovis techniques or
different evaluations of the same technique. Being able to do
so would allow finding general patterns of usability issues
and improving the ability to judge the validity of results.
Still, there remains no consensus as to what kind of
heuristics could be really useful for assessing infovis
techniques. The initial work on trying to find a small set that
is general enough to be used for revaluating most techniques
could be continued and validated. Likewise, there are yet no
studies that investigate how many evaluators that are really
needed for successful heuristic evaluation in infovis.

F. Some more advice
It is important to note that heuristics used in an
evaluation are meant to guide the evaluators to find usability
problem, but they should not restrict them to find only the
problem that can be explained by the heuristics. All problems
should be reported even if they cannot be fully or even partly
explained by any heuristic.
As evaluation manager it is important to remain neutral.
When interacting with the evaluators, if applicable, one
should prompt them in an unbiased way and ask unbiased
questions, this will motivate them to give honest responses.
It is also essential to assure the evaluators that they shall not
feel any pressure to respond positively if it is you as the
evaluation manager that is the developer of the visualization!
On the contrary, they should be encouraged to point out
problems and give constructive critique since obtaining such
feedback is the main of the evaluation and will improve the
visualization in the end. Also do not defend the visualization
technique or the procedure if there is any ”failure” during the
evaluation. If things go wrong blame the equipment, or
yourself or say that the task was hard to understand/perform
(even if it was not).
Finally, the method can seem overly critical since it is
used for the identification of usability problems and not for
finding good aspects. Thus, you will normally only receive
feedback regarding the negative aspects of the visualization.
When reporting the results from a heuristic evaluation it is
also recommended to include positive feedback as well,
when possible
VI.

REFERENCES
R. Spence. Information Visualization. Design for Interaction. 2nd
edition. Pearson Education, Prentice Hall. 2007.
[2] ISO 9241-11. Ergonomic requirements for office work with visual
display terminals, part 11. Guidance on usability. Geneva,
Switzerland: International Organization for Standardization. 1998.
[3] J. Nielsen. Heuristic evaluation. In J. Nielsen, and R.L. Mack, (Eds.).
Usability Inspection Methods. John Wiley. 1994-& Sons, NY, USA,
25-61.
[4] J. Nielsen and R. Molich, Heuristic evaluation of user interfaces,
Proc. ACM SIGCHI Conference on Human Factors in Computing
Systems, CHI 1990, ACM Press, 152- 158. April 1990
[5] T. Zuk, L. Schleiser, P. Neuman, M.S. Hancook, and S. Carpendale,
Heuristics for information visualization evaluation. Proc. 2006 AVI
Workshop on BEyond time and errors novel evaluation methods for
information visualization, BELIV’06. ACM Press, 1-6. May 2006.
[6] C. Forsell, and J. Johansson, A heuristic set for evaluation in
information visualization. Proc. International Working Conference on
Advanced Visual Interfaces, AVI’10. ACM Press, 199-206. May,
2010.
[7] C. Chen, and M.P. Czerwinski, Empirical evaluation of information
Vvsualization: An introduction. International Journal of HumanComputer Studies, 53(5), 631-635, 2000.
[8] J. Stasko, Evaluating information visualizations: Issues and
opportunities. Proc. 2006 AVI Workshop on BEyond time and errors
novel evaluation methods for information visualization, BELIV’06.
ACM Press, May 2006.
[9] C. Plaisant. The challenge of information visualization evaluation. In
Proc. International Working Conference on Advanced Visual
Interfaces, AVI’04, ACM Press, 109- 116. May, 2004.
[10] http://www.socialresearchmethods.net/kb/intreval.htm.
Retrieved
2012-03-10.
[11] ISO 9241-210, Ergonomics of human-system interaction. Geneva,
Switzerland: International Organization for Standardization. 2010.
[1]

DISCUSSION AND CONCLUSION

This paper aims to add to the understanding of heuristic
evaluation in general and to outline what needs to be
considered in order for it to be as useful and valuable as
possible in infovis. There are issues such as what heuristics
to apply, who and how many evaluators to engage in the
evaluation, and what procedure to deploy during execution
that are crucial for the outcome. The fact that it is a discount
method (quick, cost-effective, and fairly easy to conduct etc.)
speaks in its favor for several reasons. However, one has also
to be aware of its drawbacks e.g., the method is subjective
and depends on the evaluators, one cannot expect to find all
important problems and it cannot be repeated in terms of
results. Nevertheless, when applied properly the method can
be highly useful and I encourage members of the infovis
community to consider using it more. The method can
generate valuable feedback at all stages of development and
is suitable to assess qualitative aspects of use and high-level
cognitive tasks, which are called for in infovis research
today. For example, what is important to support exploration,
for gaining ideas and insight and what is affecting cognitive

141

[12] R. Kosara, C.G. Healey, V. Interrante, D.H. Laidlaw and C. Ware.
Thoughts on user studies: Why, how, and when? Computer Graphics
and Applications, (23)4. 20-25, 2003.
[13] S. Carpendale. Evaluating information visualizations. In Information
Visualization: Human-Centered Issues and Perspectives. A. Kerren,
J. T. Stasko, JD. Fekete, C. North (Eds.), LNCS 4950, Springer, 1945, 2008.
[14] D. Bowman, J. Gabbard, and D. A Hix, Survey of usability evaluation
in virtual environments: Classification and comparison of methods.
Presence: Teleoperators and Virtual Environments. 11(4), 404-424,
2002.
[15] M. Tory, and T. Möller, Evaluating visualizations: Do expert reviews
work? IEEE Computer Graphics and Applications, 25(5), 8-11. 2005.
[16] J. Nielsen, and R.L. Mack, (Eds.). Usability Inspection Methods. John
Wiley & Sons, NY, USA. 1994.
[17] H. Sharp, Y. Rogers, J. and Preece, Interaction Design, 2nd edition,
John Wiley & Sons, UK, 2004.
[18] A.M. Graziano and M.L. Raulin. Research Methods: A Process of
Inquiry, 7th ed. Allyn & Bacon, Boston, MA. 2010.
[19] K. Andrews, Evaluating information visualizations. Proc. 2006 AVI
Workshop on BEyond time and errors novel evaluation methods for
information visualization, BELIV’06. ACM Press, 1-5. May 2006
[20] P. Saraiya, C. North, and K. Duca, “An insight-based methodology
for evaluating bioinformatics visualizations,” IEEE Transactions. on
Visualizations and Computer Graphics, 11( 4), 443-456. 2005.
[21] How
to
Ccnduct
a
heuristic
evaluation
http://www.useit.com/papers/heuristic/heuristic_evaluation.html.
Retrieved 2012-03-10.
[22] Severity
ratings
for
usability
problems.
http://www.useit.com/papers/heuristic/severityrating.html. Retrieved
2012-03-10.
[23] K. Vredenburg, J. Mao, P.W. Smith, and T. Carey, A survey of usercentered design practice. Proc. CHI 2002, ACM SIGCHI Conference
on Human Factors in Computing Systems, 471–478. ACM Press,
May, 2002.
[24] J. Nielsen, 1994. Enhancing the explanatory power of usability
heuristics. Proc. CHI 1994, ACM SIGCHI Conference on Human
Factors in Computing Systems ACM Press, 152- 158.
[25] Ten
usability
heuristics.
http://www.useit.com/papers/heuristic/heuristic_list.html. Retrieved
2012-03-11.
[26] Interactive
heuristic
evaluation
toolkit.
http://www.idbook.com/firstedition/catherb/index.htm. Retrieved 2012-03-03.
[27] M.Q.W. Baldonado, A. Woodruff, and A. Kuchinsky, Guidelines for
using multiple views in information visualization. Proc. International
Working Conference on Advanced Visual Interfaces, AVI’00, ACM
Press, 110- 119. May, 2000.
[28] J. Heer, and G.G. Robertson, Animated transitions in statistical data
graphics. Transactions on Visualization and Computer Graphics, 13,
1240-1247. 2007.
[29] J. Mankoff, A.K. Dey, G. Hsieh, J. Kientz, S. Lederer, and M. Ames,
Heuristic evaluation of ambient displays. Proc. ACM SIGCHI
Conference on Human Factors in Computing Systems CHI 2003,
ACM Press, 169-176. April, 2003.
[30] J. Bertin, Semiology of Graphics, The University of Wisconsin Press,
WI, 1983.
[31] E.R. Tufte, The Visual Display of Quantitative Information. Graphics
Press, Cheshire, CT, 2nd edition. 2001.
[32] C. Ware, Information Visualization: Perception for Design, Morgan
Kaufmann Publishers, 2nd edition. 2004.
[33] Zuk, T. and Carpendale S. Theoretical analysis of uncertainty
visualizations. Proc. SPIE-IS&T Electronic Imaging, SPIE, vol. 6060,
606007. 2006. Jan, 2006.

[34] R. Amar, and J. Stasko, A knowledge task-bssed framework for
design and evaluation of information visualizations. Proc. IEEE
Information Visualization Conference, Infovis 2004, IEEE Computer
Society Press, 143-149. Oct, 2004.
[35] B. Shneiderman, The eyes eave It: A task by data type taxonomy for
information visualizations. Proc. IEEE Symposium on Visual
Languages, IEEE Computer Society, 336-343. Sept, 1996.
[36] B. Craft, and P. Cairns, Beyond guidelines: What can we learn from
the information seeking mantra? Proc. 9th International Conference
on Information Visualization IV’05, IEEE Computer Society Press,
110-118. July, 2005.
[37] C.M.D.S. Freitas, R.G. Luzzardi, R.A. Cava, M.A. Winckler, S. M.
Pimenta, and L.P. Nedel, 2002. Evaluating usability of information
visualization techniques. Proc. 5th Symposium on Human Factors in
Computer Systems (IHC) 2002, Brazilian Computer Society, 40-51.
Oct, 2002.
[38] D-L. Scapin, and J.M.C. Bastien, Ergonomic criteria for evaluating
the ergonomic quality of interactive system. Behavior and
Information Technology 16(4-5) 220-231,1997.
[39] R.G. Luzzardi, C.M.D.S. Freitas, R.A. Cava, G.D. Duarte, and
M.H.S. Vasconcelos, An extended set of ergonomic criteria for
information visualization. Proc. IASTED. International Conference of
Computer Graphics and Imaging, 236-241, 2004.
[40] T. Schneider, and W. Aigner, A-Plan: Integrating interactive
visualization with automated planning for cooperative resource
scheduling. Proc. 11th International Conference on Knowledge
Management and Knowledge Technologies, article 44, 8 pages, ACM
Press, Sept, 2001.
[41] T. Gschwandtner, W. Aigner, K. Kaiser, S. Miksch, and A. Seyfang,
Care Cruiser: Exploring and visualizing plans, events, and effects
interactively. Proc. 4th IEEE Pacific Visualization Symposium
(PacificVis), 2011. IEEE Computer Society, 43-50, March, 2001
[42] E. L-C. Law, and E.T. Hvannberg, Analysis strategies for improving
and estimating the effectiveness of heuristic evaluation. Proc. Third
Nordic Conference on Human-Computer Interaction NordiCHI 04.
ACM press, 241-250, Oct, 2004.
[43] L. Faulkner, Beyond the five-user assumption: Benefits of increased
sample sizes in usability testing. Behavior Research methods,
Instruments and Computers, 35(3), 379-383, 2003.
[44] A. Woolrych, G. and Cockton, Why and when five test users aren’t
enough. Proc. IHM-HCI 2001 Conference, Springer-Verlag, 105-108,
2001, Sept 2001.
[45] J. Spool, J. and W. Schroeder, Testing web sites: Five users is
nowhere near enough. Proc. CHI '01 Extended Abstracts on Human
Factors in Computing Systems (CHI EA '01), ACM Press, 285-286,
March-April, 2001.
[46] L. Buykx, L. and H. Petrie, Collaborative heuristic evaluation:
Improving
the
effectiveness
of
heuristic
evaluation.
https://www.google.se/search?q=heuristic+evaluation+improve&ie=u
tf-8&oe=utf-8&aq=t&rls=org.mozilla:en-US:official&client=firefoxa. Retrieved 2012-02-22.
[47] P.B. Lowry, and T. Roberts, Improving the usability evaluation
technique heuristic evaluation trough the use of collaborative
software. Proc. 9th Annual Americas Conference on Information
Systems (AMCIS). 2203-2211, Aug, 2003.
[48] G. Cockton, G. and A. Woolrych, Sale must end: Should discount
methods be cleared off HCI’s Shelfs? Interactions. 13-18. Sept/Oct,
2002.
[49] M. Tory, and T. Möller, A parallel coordinates style interface for
exploratory volume visualization. IEEE Transactions on
Visualization and Computer Graphics. 11(1), 71-80, 2005.
[50] P. Lundblad, M. Jern, and C. Forsell, Voyage analysis applied to
geovisual analytics. Proc.12th International Conference on
Information Visualization. IV 08, IEEE Society Press, 381-388. July,
2008.

142

