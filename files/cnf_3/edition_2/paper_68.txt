2010 14th International
Information
Conference
Visualisation
Information Visualisation

Hand motion recognition and visualisation for direct sign writing

Gan Lu, Lik-Kwan Shark, Geoff Hall

Ulrike Zeshan

Applied Digital Signal & Image Processing Research
Centre (ADSIP)
University of Central Lancashire, PR1 2HE
Glu@uclan.ac.uk

International Centre for Sign Languages and Deaf
Studies
University of Central Lancashire, PR1 2HE

Abstract—Although SignWriting provides an intuitive notation
system based on pictorial symbols to enable any sign based
language in the world to be transcribed into a written form, it
is a time consuming process for keyboard based input. To
address the challenge of direct sign writing, the paper presents
a human-computer-interaction system developed for
recognition and visualisation of hand movements. The system
is shown to be able to display the corresponding SignWriting
symbols for various hand movements performed by two hands
based on motion characteristics such as movement planes,
movement directions, straight/curve movement paths,
clockwise/anti-clockwise movements, and single/repeated
movements.

I.

problems can be overcome by wearing a sensor at the
expense of introducing a small inconvenience to the signer, it
is adopted as the approach presented in this paper, whereby
an IS-900 wireless tracking system from InterSense with two
tracking devices attached to the signer wrists [4] is used to
provide the required hand position data in a more robust and
rapid manner.
With the paper focusing on recognition of the dynamic
hand movements in 3D between a starting gesture and an
ending gesture, the proposed method assumes that the
starting and ending hand gestures can be recognised using a
data glove. ShapeHand from Measurand which uses fibreoptic sensors to provide all finger joint movement
information has been used by the authors for this purpose [5]
[6].
The paper is organised as follows. While Section 2
describes hand motions and their corresponding notations in
SignWriting, Section 3 presents algorithms developed for
hand motion recognition. These are followed by Section 4
showing a 3D stereoscopic user interface developed to
generate corresponding SignWriting notations and to
produce a stereoscopic view of hand movements made.
Finally, some concluding remarks are given.

INTRODUCTION

There are several notation systems to enable a sign
language communicated in a visual-gestural form to be
transcribed into a written form, and SignWriting (SW)
developed by Valerie Sutton in 1974 is a popular one among
the deaf communities [1]. Each sign in SW is represented by
a sign-box containing a composition of basic pictorial
symbols to depict postures and movements of body, hand
and head as well as facial expressions. On one hand, it is
simple to use SW to write any sign language in the world; on
the other hand, it is a time consuming process for keyboard
based input due to a large number of pictorial symbols for
selection and a number of spatial manipulations (rotation
and translation) of selected symbols required to compose a
sign [2]. A challenge is therefore presented to develop a
Human-Computer-Interaction (HCI) system to enable
automatic transcription of articulated signs into
corresponding sign-boxes in an electronic form without
using keyboards. With hand movements forming a core part
of any sign language, the paper focuses on the development
of automatic recognition and visualisation of hand motion in
3D.
One possible approach for hand motion recognition is
based on the use of single or multiple video cameras [3],
whereby hand movements in 3D are tracked across the video
sequence based on hand positions detected from each frame
using a computer vision algorithm. Possible problems
associated with this approach include occlusion resulting in
loss of hand positions due to the restricted camera view angle,
and high computation cost due to the requirement to track
rapid hand motion and to process a large data set. Since these
1550-6037/10 $26.00 © 2010 IEEE
DOI 10.1109/IV.2010.71

II. SIGNWRITING HAND MOTIONS AND NOTATIONS
According to International SignWriting Alphabet (ISWA)
2008 [7], there are 30 pictorial symbol groups containing
639 basic pictorial symbols. Since each basic symbol can
have up to a maximum of 96 variations (up to 6 different fills
and 16 rotations), it results in a large dictionary that currently
contains a total of 35,023 valid symbols. A much longer time
is therefore needed to select and combine symbols to input a
single sign than to type a word.
By using the ISWA basic symbols, some hand movement
examples in 3D can be transcribed as shown in Fig.1, where
unfilled/filled arrow heads are used to indicate left/right
hands, double-stem/single-stem arrows are used to indicate
movements parallel to the wall/floor planes, and arrow
orientations are used to indicate movement directions in
either plane (with hand movements in each plane divided
into eight principal directions). Furthermore, arrows can be
straight/curved to indicate movement paths, and
duplicated/triplicated for repeated movements.

457
467

(a)

(b)

not the case, subsequent hand position data are processed
based on their distances to pi to detect the second starting
position for the repeated movement. If dist(pi, pn) is less than
both dist(pi, pn-1) and dist(pi, pn+1) by the specified threshold
value after encountering the first farthest hand position, then
pn is identified as the second starting position for the
repeated movement. The search of the farthest position is
then repeated to detect the second farthest position reached
by the hand, and the whole process could be repeated for the
third time in order to reach the ending hand gesture with
n = L  1.

(c)

Figure 1. SW symbols showing (a) vertical movement made by right hand in
the wall plane; (b) diagonal forward movement made by left hand in the
floor plane; (c) two repeated curved movements made by left hand along the
horizontal direction in the wall plane.

III.

HAND MOTION RECOGNITION

B. Identification of Movement Planes
For two-plane movement classification, hand movements
can be interpreted as parallel to either the floor plane or the
wall plane. This is achieved based on the angle made by the
movement direction vector of each hand position along the
hand motion trajectory with respect to the floor plane. Using
the Cartesian co-ordinate system with the x-y plane forming
the wall plane and the x-z plane forming the floor plane
centred at the starting hand position denoted by pi, if pn
denotes a hand position along the hand motion trajectory,
then the absolute angle made by it with respect to the floor
plane is given by:

For simplicity of implementation, the input of each sign
is based on a fixed three-phase signing sequence that consists
of making a hand gesture at the starting position, performing
a follow-up hand movement, and finishing with a hand
gesture at the ending position. While the starting and ending
hand gestures are recognised by processing finger joint
positions acquired from a pair of ShapeHand data glove from
Measurand, hand movements are recognised by processing
hand positions acquired from the InterSense tracking system.
The hand motion data is acquired at a rate of 50 Hz, the
starting and ending hand gestures are used to start and end
the storage of the acquired hand motion data in a memory
buffer for processing. The sequence of processing starts with
identification of repeated movements, and it is followed by
identification of movement planes, movement directions,
path linearity, and clockwise/anti-clockwise movements,
respectively.

Dn

( x i  x n ) 2  ( y i  y n ) 2  ( zi  z n ) 2

( y n  yi )
( x n  xi ) 2  ( z n  z i ) 2

(2)

Since the angle of the hand movement direction vector
with respect to the floor plane should be less than 45o for the
floor plane movement, the hand movement is identified as
parallel to the floor plane if all angle values computed for
each hand position along the hand motion trajectory are less
than 45o with respect to the floor plane, otherwise it is
considered as parallel to the wall plane.

A. Identification of Repeated Movements
Identification of repeated hand movements is based on
the number of times the hand has moved from the space
around the starting position to the space around the farthest
position between the starting and ending gestures. Let the
starting hand position, defined by the starting hand gesture,
be denoted by pi = [xi, yi, zi]; and let the length of the
acquired hand motion data for one sign, defined by the
ending hand gesture, be denoted by L. If pn = [xn, yn, zn]
denotes a hand position along the hand motion trajectory
made by the signer with n < L, then the distance between pi
and pn is given by:
dist( pi , pn )

tan 1

C. Identification of Movement Directions
For eight-direction movement classification, hand
movements can be interpreted as left/right horizontal
movements, up/down vertical movements, or left/right
up/down diagonal movements parallel to either the wall or
floor plane as shown in Fig. 2. This is achieved based on the
angle made by the farthest hand position with respect to the x
axis in the movement plane identified. As shown in Fig. 2, if
pv denotes the farthest hand position detected, then the sine
and cosine of its angle denoted by E in the wall plane are
given by:

(1)

Let pn-1 and pn+1 denote two neighbouring hand positions
along the hand motion trajectory before and after pn. By
computing dist(pi, pn-1), dist(pi, pn) and dist(pi, pn+1) starting
from pi for each hand motion position, if dist(pi, pn) is greater
than both dist(pi, pn-1) and dist(pi, pn+1) by a specified
threshold value, then pn is identified as the first farthest
position reached by the hand. The use of a threshold is to
avoid the small movement jitter problem in the acquired
position data.
Upon detecting the first farthest position, a check is
made to see if n = L  1. If it is, then it indicates that the
ending gesture position has been reached and the hand
movement is identified as a non-repeating movement. If it is

sin E

( y v  y i ) / ( x v  xi ) 2  ( y v  y i ) 2

(3)

cos E

( x v  xi ) / ( x v  x i ) 2  ( y v  y i ) 2

(4)

To determine the angular value of E the signs of sinE
and cosE are used to find the quadrant in which E lies, since,
for the wall plane, sinE is positive for E in the left hand side
and cosE is positive for E in the top half, and for the floor

468
458

plane sinE is positive for E in the bottom half and cosE is
positive for E in the right hand side (see Fig. 2). With each
movement plane partitioned using 45º angular sectors
centred at eight principal directions as shown in Fig. 2, the
final movement direction is determined by finding the
angular sector in which E lies based on the corresponding
limits of sinE values precomputed for each angular sector.
Wall/Floor

-Sin/-Sin
+Cos/-Cos

Up
vertical

Pv

Pi

E
x

Left
horizontal

Right
horizontal

down
vertical

-Sin/+Sin
-Cos/-Cos

+Sin/+Sin
-Cos/+Cos

Figure 2. Movement directions in Wall/Floor plane

D. Identification of Path Linearity
Hand motion paths in SW can be straight (linear) or
curved (non-linear). The identification of path linearity is
based on the maximum deflection angle with respect to the
straight line movement from the starting hand position to the
farthest hand position.
To explain the algorithm implemented, the hand motion
path illustrated in Fig. 3 is used as an example, where the
movement can be interpreted as a curved clockwise one in
parallel to the wall plane. Let (xi, yi) and (xv, yv) be the
coordinates of the starting and farthest hand position denoted
by pi and pv in the wall plane. The straight movement from
(xi, yi) to (xv, yv) is described by a line equation with its slope
and the y-intercept point given by:
k
c

y v  yi
x v  xi
x v y i  xi y v
x v  xi

(7)

k 2 1

Using (7) to compute the distance from each point on
the curved path to the straight line between (xi, yi) and
(xv, yv), the maximum deviation point can be identified and
used to compute the maximum deflection angle with respect
to the horizontal axis denoted by Td in Fig. 3. If the
difference between Td and E is less than the deflection angle
threshold that is set to 11.25º, then the motion path is
identified as a straight one (linear). Otherwise, it is identified
as a curved one (non-linear).
In the implementation, the difference between Td and E is
determined based on the values of sinTd, and cos Td as well
as sinE and cosE obtained from the previous stage of
identification of the movement direction (see Section 3.3).
The differences between these values are also used to
determine clockwise/anti-clockwise movements, since a
clockwise movement in the left half or an anti-clockwise
movement in the right half of the movement plane will result
in sinTd greater than sinE whereas a clockwise movement in
the top half or an anti-clockwise movement in the bottom
half of the movement plane will result in cosTd greater than
sinE Furthermore, in order to avoid the ambiguity problem
caused by those curved movements near the horizontal axis
to result in the same cosine value being produced and near
the vertical axis to result in the same sine values being
produced, the difference of sine values are used for E lying in
the angular sector from 292.5º to 67.5º and from 112.5º to
247.5º and the difference of cosine values are used for E
lying in the angular sectors from 67.5º to 112.5º and from
247.5º to 292.5º to determine clockwise/anti-clockwise
movements, as shown in Fig. 3.

+Sin/-Sin
+Cos/+Cos

y/-z

kxn  yn  c

dn

Wall
Maximum
deviation point

112.5o

+

+

Cos
67.5o

+
Pv
157.5

Pi

Sin

E

202.5o

337.5o

(6)
247.5o

-

For a hand position denoted by (xn, yn) on the curved
path from (xi, yi) to (xv, yv), the shortest distance with respect
to the straight line is given by:

-

292.5o

Cos

-

+

Figure 3. Path linearity identification example

469
459

angle ˥d

22.5o

o

Sin

(5)

Maximum
deflection

IV.

DIRECT SIGN WRITING INTERFACE

circular polarisation mode is used to provide a stereoscopic
display. By wearing a pair of light-weight polarised glasses,
the user is able to see the graphic models of his/her two
hands floating in space, their movements in real-time and in
stereoscopic mode with depth impression from his/her
viewpoint, as well as SW symbols displayed in 5-by-5 lattice
blocks according to the hand movements made.
Various hand movements with different combinations of
movement planes and directions as well as repeated
movements, trajectory curved and clockwise/anti-clockwise
movements were performed to evaluate the recognition
performance. Some representative examples to demonstrate
the capability of hand movement recognition are shown in
Figs. 6-12, where the first and second columns show the 3D
movement trajectories made by the left and right hands,
respectively, and the third column shows the corresponding
SW symbols generated. These examples include separate left
and right diagonal forward movements by the left and right
hands in the floor plane (Fig. 6); the left hand performing a
diagonal backward movement in the left side of the floor
plane with the right hand performing a diagonal up
movement in the right side of the wall plane (Fig. 7); the left
and right hands performing separate clockwise parabolic
movements in the left and right sides of the floor plane along
the horizontal direction (Fig. 8); the left hand performing a
diagonal backward clockwise parabolic movement in the left
side of the floor plane with the right hand performing a
horizontal clockwise movement in the right side of the wall
plane (Fig. 9); duplicated horizontal outward movements
performed by both the left and right hands in the left and
right sides of the floor plane (Fig. 10); the left and right
hands performing the same movement which is an anticlockwise parabolic movement repeated twice along the
horizontal direction in the left side of the wall plane (Fig. 11);
and triplicated movements with the left hand performing
repeated horizontal outward movements in the left side of the
wall plane and the right hand performing repeated diagonal
forward movements in the right side of the floor plane (Fig.
12).

To provide an effective visual feedback of hand motion
and its recognition, a stereoscopic direct sign writing
interface has been developed to produce a stereoscopic view
of the hand movements made in 3D and to generate
corresponding SignWriting notations. Since each sign is
articulated in a sequence with three gestural parts, a
hierarchical sign-box consisting of a 5-by-5 lattice is
constructed to enable three sets of symbols, corresponding to
the hand and motion gestures made for each sign, to be
displayed in a sequential manner from the most inner square
to the outer squares. This is illustrated in Fig. 4. Recognition
of the starting hand gesture results in the corresponding
symbols being displayed in the most inner yellow square;
recognition of the subsequent movements by two hands
results in their symbols being displayed in the sandwiched
red squares along the movement directions; and recognition
of the ending hand gesture results in the corresponding
symbols being displayed in the outer blue squares along the
movement directions.

Figure 4. Sign-box showing diagonal up movements by two hands in wall
plane

V.

CONCLUSIONS

This paper presents the work done to develop a unique
human-computer-interaction system for direct sign writing
with a particular focus on recognition and visualisation of
hand movements. Based on the SW hand motions and
notations, the paper describes the algorithms developed for
recognition of hand motions based on various motion
characteristics, and the direct sign writing interface
implemented for 3D hand motion visualisation and SW
notation display. The system is shown to involving various
combinations of motion characteristics provide a good visual
feedback and an intuitive sign-box display format. A good
mixture of hand movements can be recognised which include
different movement planes and directions as well as repeated
movements, trajectory curved, and clockwise/anti-clockwise
movements. Although the development could be viewed as
at its early stage with a significant amount of further work in
order to cover the full spectrum of hand motions, an

Figure 5. Direct sign writing interface

Figure 5 shows the interface developed for direct sign
writing with a user making signs. From the user perspective,
it is required to wear a pair of wireless ShapeHand data
glove to provide hand gesture information, a wireless
InterSense tracking device on each wrist to provide hand
motion information, and a wireless InterSense head tracker
to provide the position and orientation of the user head. A
large screen with two back projectors operating in passive

470
460

excellent basis is offered by the system to achieve the goal of
rapid and direct sign writing without using keyboards.

[3]

ACKNOWLEDGMENT

[4]

The authors would like to thank Dr. Chen Xin for his
technical help.

[5]
[6]

REFERENCES
[1]
[2]

SignWriting® Site url: http://www.signwriting.org/.
K. Clark and D. Gunsauls. “Written in signed and spoken
languages”. The SignWriter Newsletter. Spring Issue, 1997.

[7]

J. Rett, S. Luis and J. Dias. “Laban Movement Analysis for MultiOcular Systems”. 2008 IEEE/RSJ International Conference on
Intelligent Robots and Systems, Sept, 22-26, 2008, France
IS-900 precision inertial-ultrasonic motion tracking system.
InterSense Inc. url:www.isense.com.
ShapeHand. Measurand Inc. url: www.measurand.com.
G. Lu, L. Shark, G. Hall and U. Zeshan, “Dynamic Hand Gesture
Tracking and Recognition for Real-Time Immersive Virtual Object
Manipulation”, 2009 International Conference on CyberWorlds,
pp.29-35.
SignWriting International SignWriting Alphabet (ISWA 2008). url:
http://www.signwriting.org/lessons/iswa/

Figure 6. Left and right diagonal forward movements in floor plane

Figure 7. Left-back and right- up diagonal movements in floor and wall planes

Figure 8. Left and right horizontal clockwise parabolic movements in floor plane

471
461

Figure 9. Left-back diagonal and right horizontal clockwise parabolic movements in floor and wall planes

Figure 10. Left and right duplicated movements in floor plane

Figure 11. Synchronous parabolic anti-clockwise movements in floor plane

Figure 12. Triplicated left and right-up movements in wall and floor planes

472
462

