2010 14th International
Information
Conference
Visualisation
Information Visualisation

Wearing emotions
physical representation and visualization of human emotions using wearable technologies

Salvatore Iaconesi (Author)
Department of Industrial Design
Università di Roma “La Sapienza
Rome, Italy
salvatore.iaconesi@artisopensource.net
Abstract—This paper presents the iterative design process used
in generating several prototypes and proofs of concept for
wearable, technological devices which are capable of
implementing emotional skins. Reactive layers have the
potential to act as visualization surfaces to represent
individuals' or groups' emotional configurations depending on
the specific strategies which can be applied to a wide range of
human activities. These emotionally-aroused, interactive skins
can be used in generating cognitive processes which are
relevant for social communication, knowledge dissemination
and education. The research project aimed at exploring the
potential use of these devices in emotion classification and
methodologies for representing and enacting dynamics that are
significant from cognitive, ethnographic and psychologic
perspectives. (Abstract)

For instance, the ancient Greek terms designating
emotions correspond to our own concepts only to a certain
extent [3], but enormous discrepancies arise that go beyond
the similarities, as Greek populations used terms in their own
language whose cognitive mapping is not something that we
can give for granted
In the past three decades research on emotions has
progressively grown in scale and importance involving fields
such as psychology, neurophysiology, biological, social and
cultural anthropology, philosophy, history and sociology.
Powerful debates have arisen between confronting parties
establishing oppositions among
universalists/relativists,
biology/culture,
nature/nurture,
materialism/idealism,
positivism/interpretivism, individual/social, body/mind,
reason/passion, and rationalism/romanticism in a dualistic
approach that is deeply interconnected with the Cartesian
Dualism typical of the Western intellectual tradition [4].
This diffused attention to emotions is quite remarkable in
the contemporary era. Information and communication
domains have become the focal nodes of today's society,
enhancing the possibilities for interactivity and
interoperability among human beings, information and
communication systems, thus creating a new pattern-thatconnects, an explicit technological implementations of
Bateson's ecosystems of the mind [5].
The tendency is for technology to leave the screens of
desktop computers and to become embedded in cities,
architectures spaces, objects and bodies [6] turning all of
these elements into their augmented versions: fluid hybrids
of flesh/concrete/plastic and information/communication.
This change brings forth a progressive rise in the
strategic importance of emotional, empathic and relational
domains with entire economic models named after
"attention" and "relationship", and with the themes of
freedom of expression and of the dissemination and sharing
of knowledge and information becoming the focal nodes for
both social and economic innovation.
The scope of this paper is to document a research process
in which this mutation of the human body has been made
explicit by using wearable technologies that allow
individuals to capture, identify, represent and visualize
emotions (their own, those of other individuals' or calculated
emotions of groups of individuals), turning them into a
porous, communicational, visual, emergent skin.

Keywords-component;
emotions;
interaction
design;
wearable technologies; ethnography; cognitive sciences (key
words)

I.

INTRODUCTION

Emotions are the grammar of our social life. A widelyaccepted, transdisciplinary definition of emotion is described
as “organized psychophysiological reactions to news about
ongoing relationships with the environment” [1], which
refers to the specific configuration of a human being in terms
of desire, arousal, attention, tendency or an of the other
parameters that can be used to try and classify emotions
Throughout history several strategies have been used to
identify, classify and represent emotions. With the rise and
fall of empires, reigns and entire societies, the possibilities
for describing emotions – and for referring to, representing
and visualizing them using words or other formalisms - have
changed accordingly. Different populations speaking
different languages, living in different places and eras thus
produced different cultures. This variance is distributed
across several levels, the most challenging of which is the
possibility of referring to specific emotions in terms that are
isomorphic across time and space.
Catherine Lutz [2] has remarked that "the process of
translation involves much more than the one-to-one linking
of concepts in one language with concepts in another. Rather,
the process ideally involves providing the context in which
the words are used in each of the two languages between
which translation is attempted”.
1550-6037/10 $26.00 © 2010 IEEE
DOI 10.1109/IV.2010.38

200

II.

PROBLEM STATEMENT

III.

We intend to move beyond the "nature/nurture"
dichotomies and present approaches to emotions that account
for both biological and cultural factors, embracing and
leveraging approaches referring to various suggestions
coming from sociocultural anthropology and specific ones in
the domain of the biocultural approaches [7].
With the research process presented in these pages we
intended to observe and describe the possibilities offered by
the creation of technological wearable devices that
implement emotional skins: visualization surfaces that
represent emotional configurations of individuals or group of
individuals, according to specific strategies that can apply to
human activities.
There have been dramatic advances in experimental and
theoretical research on emotion, and there has been a sharp
rise in the significance that the emotional domains can reflect
on multiple areas such as design, process management and
medicine.
Donald Norman [8] emphasizes the importance of
emotions for the design processes and practices, arguing that
they can play a significant role in the ways in which people
solve problems.
Rosalind Picard [9] introduces affective computing,
describing it as the kind of technologies relating to, arising
from or deliberately influencing emotions. Focusing on the
fundamental importance of emotion in human
communication she argues that computers should be
equipped to recognize, express and respond to human
emotions, in order to improve education, learning and
knowledge-based processes.
Some other scholars, such as George Loewenstein [10],
highlight the importance of emotion in decisional processes.
Finucane, Peters and Slovic's researches [11] show how
mental images of different situations have been proved to
influence judgments and decisions through emotional
interactions.
These theoretical advancements can be combined with
the possibility of building innovative technological devices
that incorporate interactive, real-time visualization
functionalities for emotions. These devices can be wearable
or embedded/integrated in clothing, accessories or,
progressively, directly incorporated into the human body.
Through
this
process
it
is
possible
to
interconnect/interrelate several layers of the investigation,
giving rise to new insights. With specific reference to
Rosalind Picard's approaches in [9] we can highlight the
brain-body interactions letting emotions emerge, and we can
experiment on significative inter-domain mappings that can
be insightful in more than one direction: emotion generation,
recognition and expression/representation/visualization.
In this process, the implementation of significant,
cognitively-effective and expressive formalisms and
metaphors for emotion visualization is of fundamental
importance, as they will be the main tools used to
characterize the wearable representations, the new emotional
skin.

METHODOLOGY

The research process evolved across a three-year period
by iterating a design loop on different scenarios composed
by the following elements:
• a context (e.g.: relational; expressive;
decisional; a social interaction; game;
performative art; etc);
• a design concept for a wearable device to be
used to interact with the context's emotional
domains of all the components involved in the
process (individuals, groups, or indirect
emotions, meaning with this last term the
emotional domain that might be "recorded" in a
space/time through historical, cultural or
technological means)
• a formal identification and classification
methodology for emotions that could be
functional and significative for the specific
goals of the design concept
• a formalism for representation and visualization
The choice of contexts has been executed by envisioning
the scenarios in which investigation on emotion seemed to
have progressively raised impacts, using the points of view
of anthropology, architecture, cognitive sciences, robotics
and artificial intelligence, systems theory, games theory,
studies on sexualities and general communication science. In
this process, Information Aesthetics and Interaction Design
have played the role of connective disciplines, creating
dialogues among the other ones through cross-domain
mappings and trans-medial interconnections.
A series of fundamental issues had to be assessed from
the beginning and a precise strategy has been applied
throughout all the iterations of the process, to make them
comparable and coherent.
One of these issues had to do with identifying an
approach to be used in establishing universally valid emotion
recognition procedures. A clear manifestation of this problem
showed up, for example, whenever we would try to create
emotion interpretation systems based on voice: while we can
articulate observation points to identify emotional hints in
the vocal behavior of a single individual it is virtually
impossible to design a computer system that can perform
such a task universally, across the voice expressiveness of
different persons.
We identified this specific family of issues (availability
of universally valid emotion recognition schemes) as being
not really significant and not in line with the recent
advancements of social psychology and neurophysiology.
The general availability of devices that individuals
constantly carry with them and the specific target of the
research (the creation of wearable devices) marks as
perfectly feasible the scenarios in which the systems are
tuned to the peculiar expressive forms of the individual. We
therefore shifted the initial idea and allocated specific
research efforts in documenting the more effective patterns
in which the devices could be tuned to adhere to different
people's personal configurations.

201

Another fundamental issue to assess is a direct
consequence of the previous one, and it deals with the
possibility to compare different emotional profiles [12]. To
contextualize this issue, we might imagine two individuals
physically feeling the exact same way, but naming their
conditions in ways that are completely different; or, on the
other side, two people describing their emotional condition
in the same way but physically feeling in ways that are
different from one another. These complications arise both
from physiological difference among individuals and from
their different cognitive domains and interpretative
processes. These issues are particularly severe in the process
of connecting emotional investigation to digital systems, as
there actually doesn't seem to be any methodological way in
which a computer system can programmatically interact with
these interpretative processes of a human being. We
approached this issue by methodologically combining two
extreme approaches: on one side, to isolate the behavior and
context that is the object of the observation, thus limiting the
distortions created through the interpretative layers; on the
other side to design the systems to accept/require large
quantities/qualities of information on their users and usage
contexts, to have the availability of the higher possible
number of the relevant variables, allowing us to decline the
emotion recognition strategies according to the individual's
personal history and known contextual approach.
A common approach has been used to build the
components in all projects, allowing us to classify the
different parts of the designs according to various points of
view. Among the most significative, a pipelining approach
has been used to imitate cognitive processes in their
collaborative-hierarchical processing functionalities: the
signals, pattern and concept levels of the processing features
of the designs have been bidirectionally interconnected
through controlled feedback mechanisms allowing for
dynamic auto-tuning behaviors.
A general interaction design model has been used, with
specific declinations implemented for each project: the
systems were layered in sensor-comparator-actuator
domains. As a rule of thumb: the sensor layers took care of
low-level signal processing; the comparator levels were
structured to process the pattern and, then, concept levels of
emotional analysis; the actuator components implemented
the representation and visualization logics used in the
output/feedback stages of the devices.
Linear systems theory and digital signal processing
techniques have been used over time to process and interpret
the low-level signals. While the notion of measuring the
intensity of emotional states easily connects with human
common sense in an instinctual way, the formalization of the
set of signals to be processed to interpret each emotion is far
from being an unambiguous process. The selection of signals
to process gives rise, as of now, to an abstract representation
that can be approximated to a tolerable level by explicitly
associating ethnographic and psychological considerations to
the technical signal processing aspects. While many
advances have been achieved in identifying biological and
behavioral signals to use in classifying and measuring

emotions, no definitive answers still exist, adding further
strength to those strategies - like the ones chosen in this
research - of focusing on those systems that are thought for
tuning to individuals' specific configurations.
Going further up the processing hierarchy the need for
multidisciplinary interaction becomes even more evident.
Ethnographic approaches have been used in selecting to
choose emotion-classification schemes on a context-bycontext basis. Some patterns emerged, providing insights in
choosing the most expressive pattern-based and conceptbased schemes, adapting both to more social or intimate
contexts.
These observations had definite effects on the choice of
strategies and formalisms used in the actuator stages of the
systems, namely the emotion representation and visualization
components.
Several frameworks have been used over time to
approach the design and implementation of the visualization
components and to systematize and map the results of the
conceptual analysis taking place at the last stage of the
comparator level of the designs. Formalisms and software
products that have been used by researchers to investigate
decision making processes and cognitive response scenarios
have been proved to be very useful at this stage. These
frameworks actually describe cognitive architectures as
hybrid systems using different approaches. Some of these
include:
• adding symbolic processes to connectionist
representations (as in [13])
• adding sub-symbolic representations to
symbolic architectures (as in [14])
• adding genetic algorithms, fuzzy logic or other
representations to symbolic architectures ([15]
contains relevant scientific reviews and
evaluations of several solutions that we tried
out)
Several frameworks have been modified in the
process,cutting in early on the results of decision-making
oriented ones , and completely rewriting parts of code so that
they could be executed on the wearable devices or in direct
communication with them.
The most interesting models used along the process were
the Soar [16], ACT-R [17] and JACK [18], both for their
approaches and for their theoretical backgrounds, each of
which suggests the usage scenarios where they result as
being most effective: heuristic search, memory modeling,
prediction-oriented systems [19] and SME (Subject Matter
Experts) systems [20] have been intensively used.
These systems were employed to both prepare the
information coming from the conceptual analysis for
representation and visualization, and to implement learning
mechanisms through which auto-tuning functionalities could
be implemented to adjust the representation mechanism for
the specific individual and to provide insightful feedback to
the previous stages of the devices.
According to the implementation scenarios, these logic
and architectural components were directly used in/on the

202

device or delegated to centralized, more powerful systems,
only propagating to the device the visualization data.
IV.

embodied
representation/visualization.
Several
interpretations were used at a single time, simultaneously
showing different formalisms. A set of audio speakers
embedded into the suit expressed the overall emotional state
inferred from the audience, using the metaphor of the tag
cloud. The result, a wearable audio tag cloud, generated the
sounds of the words expressing the emotional states of the
audience with audio volumes varying according to emotional
intensities expressed (as a parallel to word-size in tag clouds,
linearly mapped to audio volumes).
Plutchik's angular distribution of basic emotions was
linearized as a periodical function (with one full 360 degrees
of classification considered as a complete period of the
emotional function), thought as being represented in the
frequency domain. Single points on the periodic curve were
weighted with the intensities of the emotions expressed by
the audience, thus implementing impulses with their position
along the frequency axis representing the type of emotion
and their amplitude representing its intensity. Reversetransforming the resulting impulse sequence produced a
signal that was isomorphic to the distribution of the
emotional conditions expressed by the audience. Several
frequency bands of these signal were distributed on the

RESULTS

The research process achieved several results across a
three year time span, in multiple iterations.
One of the most significant ones has been a wearable
device that could interconnect the live and online audience of
a dance performance creating multidirectional feedback
loops among all individuals involved and representing the
emotional states of the dancer and of the audience directly on
the body and through on-stage visualizations, thus creating a
hybrid body-architectural emotional display. The project was
called "Talkers" and it has been shown multiple times in
museums and new media arts festivals [21].
In the performance, emotions had been categorized
according to a simplified version of Plutchik's scheme [22]:
the angular disposition of emotions, across colors and
english words, was mapped to the interaction possibilities of
a series of web interfaces. Members of the audience were
allowed to interact with them, selecting words and colors
from metaphorical virtual bodies, streams of words and
geometrical arrangements inspired by the theories expressed
by Wassily Kandinsky in [23]. Other possibilities of
interaction were provided to an online audience, contributing
to the definition of a collaborative emotional profile by
means of textual interactions: these were analyzed using a
lexicon database that was created by extending Plutchik's
classification in an "emotional thesaurus": words were
structured in graphs showing weighted relationships, with
the arcs marked with the definition of the lexical constructs
describing the syntactical contexts in which specific words
could be considered as relevant in terms of the identification
of emotional states.
The emotional states of the audience, expressed through
interactions, were represented on stage according to several
strategies. The dance performer wore a latex suit with an
embedded cross-medial display device that was used for the

Figure 2.

Figure 1.

Talker Performance: visualizations and user interaction

Talker Performance: testing the device

203

performer's body by means of low-voltage electrical
stimulations. The performer, thus, experienced realtime
interferences on her body movement, and these assumed
specific configurations on her body according to the
emotional configuration: the body became an emotional
map. The dancer's movements were influenced (both
voluntarily and involuntarily) by these stimulations and,
thus, the resulting emergent choreography was to all effects a
visualization of the emotional states of the audience.
Several other formalisms were used to implement
sensorially immersive projections all around the stage
surface: emotional states were randomly picked from the
ones recorded through audience interactions; each of them
was mapped to generative geometries and projected on the
architectural spaces of the performance, creating a
continuous emotional trompe l'oeil. A technique similar to
the one used to create the electrical stimulations on the
dancer's body was used to generate sound waveforms that
were used in a generative audio composition that acted as an
additional part of the performance's soundtrack.
Another example created along this research process has
been the "O" mobile emotional device [24]. This bluetooth

gadget design was thought of as a low-information device
that could transmit emotional states to its peers. A tangible
interface was designed allowing users to configure their
social connections using their smartphones and to associate
minimalistic emotional representations for moods and simple
messages.
The devices interconnected through a centralized service
providing not only the simple information forwarding
mechanism required by the basic service, but also an
emotional network manager whose role was to identify and
represent social emotional patterns.
The information packets configured on the "O" system
by the users had to be initially associated to an emotional
interpretation (definitions were selectable using "pop",
communicative versions of more significant and formal ones,
also establishing side coherency monitoring mechanisms to
verify the selections performed by the users and weight them
accordingly). Therefore each message exchange was
connected with an emotional exchange both in semantics
(which emotion?) and in directionality ("I feel", "You feel",
"We feel", "I feel ... about you" etcetera). The social graph
was visualized according to several layers of emotional
classification, where possible values were emotion type,
intensity, relational status, duration, direction, level of social
aggregation.
The peculiar interaction methodology was also taken into
account for both emotion identification and later
representation/visualization. Users could interact with the
"O" device by squeezing it multiple times to issue specific
commands. The "squeezing" action was measured by the
device using appropriate pressure, compression and angular
sensors. Pressures and modalities were also recorded during
message transmission, and a pattern analysis in the
comparator stage of the service tried to recognize recurring
configurations. The identified patterns were used in multiple
ways. On one side they formed visualizations that could be
experienced and analyzed to infer additional insights on
these forms of interaction. On the other side they were used
to provide users with suggestive emotional feedback
messages ("are you feeling ok? You seem sad.") by using
their mobile phones. This multidirectional feedback

Figure 3.

Figure 4.

The generative audio tag cloud

“O”: architecture

204

mechanism has proven to be an outstanding, coherent,
measurable form of emotional investigation.
A more complex example was the "OneAvatar" [25]
project. Here a complex narrative strategy was enacted to
create a diffused public discussion on a controversial gadget
allowing people to connect their physical bodies to their
avatars in virtual worlds. OneAvatar was presented as a
jumpsuit featuring a body-wide network of sensors and
electrodes. Users could wear the suit while interacting with
virtual worlds (a Second Life version was released) to
connect their bodies to their avatars.
Sensors recorded physical variables from users' bodies,
such as heartbeat rate, sweat percentages on selected parts of
the body, arms and legs movement, pressure levels, being
able to discern if the individual was sitting, standing, lying
down, and so on.
Sensor data was used by a software component to
describe a physical profile which was used together with the
textual contents produced by users (such as chats and
messages) and with their movements in the virtual world to
infer real-time emotional profiles. This information was used
to interfere with the avatar online, causing effects and body
movements that the user could not control directly using

common interaction commands, as they were mapped to the
inferred emotional state. The same process was enacted
along the inverse direction, with software sensors monitoring
several parameters of the avatar's digital body and its social
interactions in the virtual world: a digital-physical and
digital-emotional profile was, thus, monitored in realtime on
the avatar, applying the same classification scheme used in
the analogical domain, but mapping it on best-effort-selected
virtual-world alternatives.
The digital-physical profile of the avatar was mapped on
its owner's body through electrical stimulations: if something
in the digital world, for example, hit the avatar on the leg, the
human's leg would be, accordingly, stimulated by a lowvoltage electrical shock. Furthermore, the avatar's digitalemotional state was mapped on the user's physical body with
led and audio based representations; led lights distributed on
the surface of the OneAvatar jumpsuit would flash in
patterns that were calculated from the avatar's digitalemotional state; audio signals would beep at disturbing
frequencies on the user's body when the avatar's emotional
configuration could be interpreted as restless or nervous,
while they would produce reassuring, soft sounds when
digital-peacefulness was the case. This process established a
Figure 5.

Figure 6.

Conference Biofeedback, the device

The OneAvatar suit

205

cross-loop running between physical and digital realities and
has proven to be quite disorienting to most people that had a
chance to use the device. As a matter of fact, the emotional
representation in both domains did not only have an aesthetic
function, but it also bore a highly active role, creating
interferences on both physical and cognitive levels that
enacted complex dynamics in the human/avatar system.
In a simplified version of the OneAvatar project, called
"Conference Biofeedback" [26], the same device was
embedded in a shirt-for-conference-presenters enabling such
interactions in the setting of a typical conference speech. In
this scenario the user did not cross-connect with an avatar,
but with his audience who ranked the speeches in real-time
through a web-based interface. In case of negative feedback
from the audience, the presenters received low voltage
stimulations.
V.

[2] Catherine Lutz, Unnatural emotions: everyday sentiments on a
Micronesian atoll & their challenge to western theory (University of
Chicago Press, 1988).
[3] David Konstan, The emotions of the Ancient Greeks: studies in
Aristotle and classical literature (Toronto, Canada: University of
Toronto Press, 2006).
[4] Catherine Lutz, Theodore Schwartz, and Geoffrey Miles White, New
directions in psychological anthropology (Cambridge: Cambridge
University Press, 1992).
[5] Gregory Bateson, Steps to an ecology of mind (Chicago: University
of Chicago Press, 2000).
[6] Adam Greenfield, Everyware: the dawning age of ubiquitous
computing (Indianapolis, USA: New Riders, 2006).
[7] Alexander Laban Hinton, Biocultural approaches to the emotions
(Cambridge: Cambridge University Press, 1999).
[8] Donald Norman, The design of everyday things (Basic Books, 2002).
[9] Rosalind W. Picard, Affective Computing (Boston: MIT Press, 2000).
[10] George Loewenstein, Colin Camerer, and Matthew Rabin, Advances
in behavioral economics (Princeton University Press, 2004).
[11] Melissa Finucane, E. Peters, and P. Slovic, Judgment and decision
making: The dance of affect and reason (New York: Cambridge
University Press, 2003).
[12] Klaus R. Scherer, Harald G. Walbott, and Angela B. Summerfield,
Experiencing emotion: a cross-cultural study (New York: Cambridge
University Press, 1986).
[13] Ron Sun and Lawrence A. Bookman, Computational Architectures
Integrating Neural and Symbolic Processes: A Perspective on the
State of the Art (Kluwer Academic, 1994).
[14] J. E. Laird, “Extending the Soar cognitive architecture,” in Artificial
general intelligence, 2008: proceedings of the First AGI Conference
(presented at the Artificial general Intelligence 2008, Memphis, TN:
IOS Press, 2008), 508.
[15] Abraham Kandel and Langhlz Gideon, Hybrid architectures for
intelligent systems (CRC Press, 1992)
[16] http://sitemaker.umich.edu/soar/home
[17] http://act-r.psy.cmu.edu/
[18] http://www.agent-software.com.au/products/jack/
[19] J. R. Anderson and G. H. Bower, Human associative memory
(Washington D.C.: Winston & Sons, 1973).
[20] Busetta, P., Rönnquist, R., Hodgson, A., & Lucas, A. (1999). JACK
intelligent agents - Components for intelligent agents in JAVA.
AgentLink
News
Letter,
2(Jan.),
http://www.agentsoftware.com/white-paper.pdf
[21] Talkers
Performance:
http://www.artisopensource.net/2006/12/21/talkers-performance/
[22] Robert Plutchik, The emotions (University Press of America, 1991).
[23] Wassily Kandinsky, Concerning the spiritual in art (Dover UK:
Courier Dover Publications, 1977).
[24] the "O" project: http://www.artisopensource.net/O/
[25] the OneAvatar project: http://www.artisopensource.net/OneAvatar
[26] the
"Conference
Biofeedback"
project:
http://www.artisopensource.net/2009/12/05/conference-biofeedback/
[27] Francisco J. Varela, Evan Thompson, and Eleanor Rosch, The
embodied mind: cognitive science and human experience (MIT Press,
1992).
[28] Marvin Minsky, Society of Mind (Simon & Schuster, 1988).
[29] Gunther R. Kress and Theo Van Leeuwen, Reading images: the
grammar of visual design (Routledge, 2006).

CONCLUSIONS

Francisco Varela argues [27] that the two extreme
approaches to cognition (external and internal
representations) ultimately amount to the same thing, and
highlights the opportunities found in the possibility of
redefining both of them: describing the mind mostly means
researching its state-to-state relationships which not only
represent the world, but also enact it. This is a crucial issue
in the research on emotions, and the mirroring loops that we
carried out can help in its investigation. Minsky [28]
describes the impossibility to find an "independent world"
and Varela extends his argument in [27] asserting the absence
of a true knowledge of the world, replaced by an
instantaneous knowledge of our representation of it. In [29]
Gunther Kress argues how visualizations convey the
processes of representation among the actors involved in a
visual communication.
The design loop carried out in our research project
created mechanisms for the fruition and measurability of the
connection between representation and visualization of
emotional configurations. The design and methodologies
presented in this research allow for the these domains to be
made explicit: the wearable technologies described in the
previous sections augment the body with this feedback loop.
ACKNOWLEDGMENT
The author wishes to thank all the research group at
FakePress Publishing, for the possibility of implementing a
truly collaborative research process and for their support in
the ethnographic research and in the design of the
components and performances: Luca Simeone (Design
Anthropology), Cary Hendrickson (Human Factors,
Sustainability), Oriana Persico (Communication Sciences),
Federico Ruberti (Market Analisys, Feasibility Research).
REFERENCES
[1] Richard Lazarus, Emotion and adaptation (New York: Oxford
University Press, 1991).

206

