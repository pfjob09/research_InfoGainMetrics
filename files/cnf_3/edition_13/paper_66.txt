Generating Synthetic Image Integrated With Real Images In Open Inventor
M. J. Abholo, F. J. Perales
Graphics and Vision Unit. Department of Mathematics and Computer Science
Balearic Islands University
Baleares, Espaha

Abstract
We describe a simple system for producing synthetic 3-D
scenes integrated with real images captured with a camera,
by using the graphic library Open Inventor. All the parts of
the system are being described: real image capture,
parameters measuring from the real scene, synthetic scene
formation and coherent integration between the synthetic
scene and the captured sequence. Finally we present some
results of the proposed system.
Keywords: Real images, 3-D Modelling, Synthetic images,
3D-Scene, Camera views, Virtual world

1.

Create a real scene (or use an existent one).

2.

Capture a sequence of images of the real scene with a
digital camera.

3.

Measure some needed parameters of the real scene and
the camera.

4.

Create 3-D synthetic scene with Open Inventor.

5.

Import the images and create a “screen” in the 3-D
synthetic scene for “watching” the images with Open
Inventor.

6.

Create a virtual camera with Open Inventor to visualise
the 3-D complete scene correctly integrated.

Subsection 2.1 describes the creation of a real scene used in
the tests done. Subsection 2.2 presents the capture of a
sequence of images of the real scene, besides the format
conversions needed to visualise it later. Subsection 2.3
describes the graphic library Open Inventor (Silicon
Graphics) and the constructions needed to represent the 3-D
“screen”. Finally, subsection 2.4 describes the process of
integration: measuring the needed parameters of the real
scene, constructing the 3-D synthetic scene according the
measures, and defining a virtual camera that allows
visualising the 3-D scene and the sequence of images
coherently integrated.

1. INTRODUCTION
There are sophisticated methods for integrating virtual and
real worlds to make an audiovisual program. [1][2][3][4]
describe complex virtual studios that automatically produce
images where real and synthetic 3-D objects are integrated.
It obtain high quality results, but both a complete process
of camera calibration and sophisticated hardware, such
sensors that allows to know the state of the camera with
accuracy, are necessaries.
We define a simple method for creating a 3-D scene which
contains both synthetic objects and images that were
captured with a camera from a real scene. There is no need
of additional hardware more than a digital camera
connected to a computer. The initial system that we
propose for integrating real and synthetic objects doesn’t
want to be a professional tool for video producing. A
possible application could be the construction of a
prototype of a 3-D ambient with the added realism of a real
scene as a backstage.

OPEN
i INVENTOR

REAL
SCENE

Section 2 describes the designed system and every step of
the manual or semiautomatic process. Section 3 presents
tests and results obtained.

DIGITAL

:

2. DESCRIPTION OF THE INTEGRATION

SYSTEM
FIGURE 1. Scheme of the process
Figure 1 shows a scheme of the integration process.
Basically it consists of the following steps:

O-7695-0210-5/99 $10.00 0 1999 IEEE

434

2.1.

Creating a real scene

To integrate a 3-D scene and a sequence of images captured
with a camera from a real scene, a reference in the real
scene is needed. A point in the real scene is taken as the
origin (O,O,O) and we have to know its position relative to
the camera. We take another two points to determine the
coordinate system. The position of the camera and every
object are determined relative to this coordinate system.
As an example, a simple real scene is created: a cone A, a
cube B and a rectangle parallelepiped C. The geometric
objects are on an (X,Z) horizontal plane. The origin (O,O,O)
or reference point is the centre of the base of the cone A.
Figure 2 a) shows the scene from the air. Figure 2 b) shows
how the camera c is translated (tx,fy,tz) relative to the
reference system and is rotated 8 respect to Y axe, with Y
perpendicular to the grid plane.

FIGURE 3. Image from the real scene of Figure 2-a

2.3.

2.3.1. Open inventor
Open Inventor [5] is a library of objects and methods
written in C++ that is based on Open GL, used to create 3D
graphic applications.
The node is the basic unit for constructing a 3-D scene.
Inventor objects include data base primitives such as shape
nodes that represent geometric 3-D objects; properties
nodes that represent qualitative characteristic of the scene;
group nodes that are containers that group other nodes and
as a consequence, create a hierarchical scene.

tzc

Nodes of a scene are structure in a graph. During rendering
process, the scene graph is visited starting with the root
node, in a left-right, and top-down way. The right (and
down) nodes of the graph inherit the state set by the left
(and top) nodes.

xc

a>

3-D Scene by using Open Inventor

b)

FIGURE 2. Example of a real scene

2.3.2. Image sequence visualization

2.2.

Capturing a sequence of images.

The goal is to visualise the images resulted from the
capturing process (section 2.2). A 3-D “image screen” is
created by using Open Inventor. Three kinds of Open
Inventor nodes are used: SoCube, SoTexture, SoBlinker.

With a digital camera (Jai 1510 Cosmicar) connected to a
PC by a Matrox Meteor RGB card, we capture a sequence
of images from the real scene described in the last
subsection. The software used for the video capturing is
Inspector 2.1. The technical aspects are not discussed here
because are irrelevant in this description.

SoCube
This class provides the shape for the “image screen”. We
construct a cube with minimal depth, i.e. width x high x 1.
SoTransfomtion

From the video capturing a video with AVI format is
obtained. The video is decomposed in separate images files
in a Open Inventor compatible format. This two steps are:

A derived of this base class is used to apply a geometric
transformation to a shape node. To rotate an object a
SoRotationXYZ node is used, and to translate it, a
SoTranslation node is used instead. In this case, this nodes
are used to set the exact position of the “screen”.

I- AVI-MPG conversion: Ulead MPEG Converter software
is used to convert from AVI to MPEG format.
2- I m a g e files g e n e r a t i o n from MPG format:
MediaConverter software is used to decompose the MPEG
video in the separate image files (i.e. in JPG format).

SoTexture
A node of this type contains a texture to be applied to an
object. For every image of the video sequence a SoTexture
node is built and the image file is associated to it. The
textures are applied to the “image screen” node to simulate
the image “projection” over the screen. In the Open

Figure 3 shows a captured image from the real scene of the
example of section 2.1. (Figure 2).

435

Objects Set(SoSepar&or)

Inventor scene graph the SoTexture node has to be on the
left of the SoCube node that represents the “image screen”,
for the texture to be applied to this object.

Object l(SoSeparrrto/h Object m(SoSeparator)

SoBlinker

The overall goal is to integrate the synthetic 3-D objects set

A node of this type has the property of switching between
all its children nodes at a specified speed. To simulate the
video projection over the screen, the SoTexture node are
grouped under a SoBlinker node that switches between the
images.

created and the real object reproduced in the image
sequence. Both of the subgraphs defined for the 3-D
synthetic objects and the image screen object respectively,
are grouping under a SoSeparator node forming the
integrated scene graph as follow:

The speed attribute represents the number of complete
cycles per second ( a cycle means visiting all the children
nodes). The user specify a required projection speed or
frame rate (i.e. 25 framesisec) and with the length n of the
image sequence, the speed attribute is determined as frame
rate I n.

Integrated Scene(SoSeparator)

/\

Objects Set

Image Screen(SoSeparator)

Figure 4 shows the 3-D objects created and the screen
object where the image sequence is “projected”, in a Open
Inventor scene examiner.

A SoSeparator node groups the textures node, the position
nodes and the “screen node”, in the correct order for the
textures and position changes be applied to the last node.
The subgraph is the following:
Image Screen (SoSeparator)

Image Sequence(SoBlinker)

(SoTranslation)

/\
Image l(SoTexture2) . . . Image n(SoTexture2)
FIGURE 4. 3-D synthetic objects and image screen object

This graph represent a subgraph in the complete scene
graph. The SoSeparator node isolates the effects of its
children, that is the nodes inside this group don’t affect
upper or right in the complete scene graph.
2.3.3. 3-D

2.4. Correct integration between 3-D
synthetic objects and a video sequence

synthetic objects creation

We create a synthetic scene by using the set of‘ Open
Inventor primitives of shape (i.e. SoCube, SoCone,
SoCylinder, SoSphere, etc.). A specific material can be
applied to the objects by the effects of a SoMaterial node.

The overall goal of the described process is to integrate the
3-D synthetic objects created and the image sequence
projection of the real objects captured with the camera. A
correct visual integration means a visualisation of the
global scene that gives the illusion of having both the real
objects of the images and the 3-D synthetic objects in the
same 3-D scene.

For every synthetic object that we want to create, we build
a SoSeparator n o d e t h a t g r o u p s SoMaterial a n d
SoTransformation node together with the node that defines
the shape. The subgraph is the following:
Object i (SoSeparator)

(SoTransformat~on)

(SoMaterial)

2.4.1.

Virtual camera definition

A virtual camera implemented in a graphic workstation,
makes a bidimensional projection of a 3-D synthetic world.
What the user visualises on the screen is always the
projection of a synthetic scene according a virtual camera.
Generally, the images are obtained from a perspective
projection based on the pinhole camera model [6]. As
Figure 5 shows, this model is characterised by an optic
center C and a plane P where the images are projected. Zc
is the camera optic axe, and is perpendicular to plane P. P
is situated at a distance f called focal distance. A point

3D-Shape

All the 3-D object defined are grouping under a
SoSeparator node, for the synthetic objects set be isolated
from the rest of the Inventor scene. The subgraph is the
following:

436

is needed to correctly position not only the virtual camera
relative to the scene but relative to the screen object. The
following subsection explains how to position the screen
object to correctly integrate the scene.

p(x,y,z) referenced in the camera coordinate system is
projected over the plane P in the point i(u,v,d).

w
.-

Yc

P(XJZ)

,_...’
,./’

___..’

_,..”
,/
_._..’
,...”
.,..”

zc

2.4.2. Distance from the screen object to the
synthetic scene

i(u,v 0

The screen object where the video sequence is projected
emulates the plane P. As a consequence, the screen object
must be centered in the optic axe and it must be parallel to
the view plane. Besides, the scene must be between the
virtual camera and the screen object, for the objects don’t
be hidden by the screen. In Figure 6, we can see that that
distance d from the screen object to the optical center must
be greater than the coordinate z of any point p(x,y,z)
belonging to an object of the 3-D synthetic scene, for the
virtual camera to project point p.

P

xc

FIGURE 5. Virtual camera model

To define a virtual camera in Open Inventor, there exists a
SoPerspectiveCamera node that emulates a real camera.
This node produces a hame from all the objects that are
after it in the scene graph. The complete scene graph is as
follows:
Root (SoSeparator)

/\
Camera (SoPerspectiveCamera)
Integrated Scene

At this time, a point pr(xnyr,zr) of the reference object and
the reference coordinate system are known. Applying
equations (1) and (2),

The position of the virtual camera relative to the 3-D scene

v, 1 Yr = f/z,

corresponds to the real camera position relative to the real
scene. Theoretically, having a synthetic scene equivalent to
the real one and the virtual camera positioned analogously
to the real one (Figure 2 b) the final image is equivalent to
the image obtained with the real camera from the real scene
(Figure 3). practically, there are errors caused by different
sources, such as the fact that Open Inventor virtual camera
model doesn’t emulate accurately the real camera used. As
it was said, the virtual camera follows the pinhole model,
that due to the great simplicity doesn’t introduce all the
parameters intervening in forming an image in a real
camera, classified as intrinsic and extrinsic. The intrinsic
parameters are those related with the intrinsic behaviour of
the camera, such as focal distance, geometric distortion,
optical center translation, etc. The extrinsic parameters
position the camera in the space. In the definition of a
realistic camera, a complete calibration of both intrinsic
and extrinsic parameters must be followed [7][8].

u,/ x, = f/z, ( 4 )

Screen object

xc
FIGURE 6. Screen object position
If we suppose f unknown some of the (u,,v,) coordinates
have to be known. Point i,(u,,v,) belongs to the image and
corresponds to pr. Because we can manipulate the captured
image it is easy to find the point ir(ur,vr). Note that this
coordinate are in pixels. In (1) or (2) we can replace f with
the result of (3) or (4), to obtain

In the method described in this paper, it is not a goal to
make an exact camera calibration. In contrast we follow the
pinhole model to evaluate the quality of the visual results
obtained with the simplest process.

v/Y = v, ZJ z Y,(S)
u/x = u, z,/ z x,(6)

The following lineal equations define the transformation
from p(x,y,z) to i(u,v,f):

f/z
u/x = f/z
v/y =

(3)

The projection of the screen object vertices must coincide
with vertices of the view plane of the camera. Applying
equations (5) and (6) with point p&x,,y,,d) unknown,
because still screen object dimension and distance to the
virtual camera are unknown, we obtain

(1)

(2)

In the experiment, the final image is the result of the
projections of both point of the 3-D objects of the synthetic
scene and all the points of the image that is projected on the
screen object. For all the elements to be coherently
integrated, that is being coherent in scale and perspective, it

vp/yP =v,z,/dyr

(7)

up I xp = u, z,l d x,

(8)

Coordinates (up ,vp) are deduced from the image
dimensions in pixels (width, high), with uP=widtW2 and vp

437

=higW2. Because of that, we have only two variables in
equation (5): distance d from the screen object to the
camera and coordinate yp. Analogously, in equation (6) we
have the variables d and xp,

First we evaluate subjectively the visual effect of the
integrated scene. Figure 7 b) shows how the illusion of the
integration between the real and synthetic object is
obtained.

Choosing any value d > z, for all p(x,y,z) that belongs to
the synthetic scene, we obtain (xp, yp), that determines the
size of the screen object. Determining only one dimension
of the screen we can obtain the other one with the Pxlv
proportion of the captured images, with Pxrc = width / high.
This works well when the real camera doesn’t produce any
distortion between width and high dimensions. We can
make a distortion testing by capturing a square positioned
in a plane parallel to the view plane and centered in the
optical axe of the real camera. We can then observe the
distortion Dx/y in the resulting image, and use this factor to
correct the proportion as P’wY =Pm / Dx/y.

G i v e n pr(xr,yrrzr), w e c a l l pP,(xPnyr’,,zPr) t h e p o i n t
corresponding to the first in the screen object projected
image. The integration is correct when the virtual camera
projects pr and ppr to the same point ip, (upr,vpr,f). A visual
checking in the scene of test 1, is drawing the line that joins
the camera center and the reference point pr(xr,yr,zr), and
then to observe the point where line cut the screen object.
Figure 8 shows the error in three points taken as reference
(cone highest extreme, corner of the cube and the
parallelepiped). There are error sources from the manual
phases and some assumptions of the method. From the
manual process, the errors can be:

3. EVALUATION OF RESULTS
The following tests were done:
I- The real scene of the example presented in section 2.1 is
reproduced by using the primitives of Open Inventor. It
implies the manual measuring of the real shapes and
determining the relative position to the origin. (Figure 4)

-

Erroneous measuring of the dimension or position of a
real object reproduced in the synthetic scene.

-

Erroneous measuring of the camera position relative to
the reference coordinate.

-

Erroneous positioning of a new 3-D synthetic object in
a non-free place, because uncertainty of the real scene.

-

Erroneous determining the corresponding pixel to a
reference point in the image.

2- Some of the 3-D objects are eliminated and new 3-D
synthetic objects are added in the scene, resulting the scene
showed in Figure 7 a).

FIGURE 8. Error visualisation

There are other errors from the assumptions of an ideal
camera model that doesn’t correspond exactly with reality,
such as:

b)
FIGURE 7. Synthetic scene that includes new 3-D objects

438

-

It is assumed a camera with an optical center that
intersect the view plane at its center, but in practice
there always exist a translation.

-

It is assumed an ideal lens following pinhole model,
but in practice, the lens is not ideal and not all the
points are projected according to the same optical
center C, and that is call geometric distortion.

4. CONCLUSSION

[6] Watt, Alan. “3D Computer Graphics”, Addison-Wesley
Publishing Company

We present a simple system to produce 3-D synthetic
scenes that integrate real image captured with a camera.
Particularly we use the graphic library Open Inventor, but
in the same manner we can use VRML or other 3-D tools.

[7] R. Tsai. “An Efficient and Accurate Camera Calibration
Technique for 3D Machine Vision”. Proc. IEEE Computer
Vision and Pattern Recognition, p. 68-75, 1986.

This initial system doesn’t want to be a professional tool
for video production. We just present the evaluation of a
very simple process, with no need of complex camera
calibration and no additional specific hardware more than a
computer and a digital camera. Quality of the result
depends of the depee of visual coherence of the integration
of 3-D synthetic objects and the images captured with the
camera, and that is depending to the precision with which
the manual measures are done.

[8] W. Grosky, L. Tamburino. “A Unified Approach to the
Linear Camera Calibration Problem”. Pattern Analysis and
Machine Intelligence, vol. 12, no. 7, p. 663-671, 1990.

Author(s):
Maria Jose Abasolo, the CONICET (Argentina) fellow and
postgraduate student of the Department of Mathematics and
Computer Science, Balearic Islands University (Baleares,
Spain).

Initially we consider simple scenes that include geometric
objects that correspond to 3D primitives in Open Inventor.
That is only to demonstrate the coherent visual integration
between the images of the real scene and an exactly
reproduced synthetic scene. In future applications, we hope
extend the type of integrated objects. Besides, realism
could be added with the inclusion of lights, shades,
transparencies, reflections, etc. Given a method that
integrate a synthetic scene with a sequence of images, it is
possible to animate the synthetic objects while the video of
the real images is projected. Another extension is to
visualise the scene from different angles considering that
the integration is correct when the scene is visualised with a
camera corresponding to the real one. We can capture the
real scene with different cameras to simulate the navigation
of the scene.

E-mail: abasolo@ipc4.uib.es
Dr. Francisco Perales, the professor of the Department of
Mathematics and Computer Science, Balearic Islands
University (Baleares, Spain).
E-mail: paco@anim.uib.es

5. REFERENCES
[l] S. Gibbs, C. Arapis, C. Breiteneder, V. Lalioti, S.
Mostafawy, J. Speier. “Virtual Studios: An Overview”.
IEEE Multimedia, vol. 5, no. 1, pag. 18-35, 1998.
[2] M. Hayashi. “Image Cornpositing Based on Virtual
Cameras”. IEEE Multimedia, vol. 5, no. 1, pag. 36-48,
1998.
[3] L. Blonde, M. Buck, R. Galli, W. Niem, Y. Paker, W.
Schmidt, G. Thomas. “A Virtual Studio for Live
Broadcasting: The Mona Lisa Project”. IEEE Multimedia,
vol. 3, no. 2, p&g. 18-29, 1996
[4] A. Iborra, M. Lazaro, P. Campoy, R. Aracfl. “Sistema
de integacion automatica de imagenes reales con imagenes
sinttticas”. CEIG’ 93, Granada, June 1993.
[5] Wernecke, Josie. “The Inventor Mentor”, AddisonWesley Publishing Company, 1993

439

