Bringing User Satisfaction to Media Access: The IST BUSMAN Project
1

Ebroul Izquierdo, 1Ivan Damnjanovic, 2Paulo Villegas, 3Li-Qun Xu, 4Stephan Herrmann
1
Queen Mary University of London, UK
2
Telefónica I+D, Spain
3
BT Research & Venturing, British Telecommunications PLC, UK
4
Technical University of Munich, Germany
Abstract

Interactive and seamless access to video content is
facilitated by technology that is able to annotate and
retrieve media data efficiently and automatically with the
minimal human interventions. Media should be accessible
by intended users quickly and independent of the database
size, regardless of the homogeneous or heterogeneous
nature of the delivery channels or the user computing
platform. These requirements are driving research and
development efforts in the European project BUSMAN; in
which a system for efficient annotation, seamless delivery
of and access to video components across heterogeneous
channels is being developed. The BUSMAN system
architecture is based on the needs and expectations of two
types of users: content providers and end users. In this
paper an overview of the BUSMAN system is presented
and its main functionalities are described.

1. Introduction
The convergence of multimedia systems technology,
telecommunications and the Internet is making it possible
to access large digital archives and video libraries across
the world. As a consequence, the need for enrichment in
modalities and capabilities of media services and more
crucially the provision of systems that are able to
catalogue, index and retrieve video components is
becoming more urgent than ever. The availability of a
wide range of digital recorders accessible to anyone, from
cheap digital cameras to complex professional movie
capturing devices, is making it happen that the stocks of
digital libraries already packed with visual content
continue rising rapidly. This trend has generated the acute
problem that the rapidly growing archives with terabytes
of totally unstructured visual content may be rendered
useless due to the lack of efficient annotation systems and
automatic search engines. To tackle this problem, the
BUSMAN project is working to enable efficient
annotation and seamless delivery of video components

across heterogeneous channels whilst ensuring userfriendly retrieval when short query-response time is
demanded. BUSMAN is designing, implementing,
validating, and testing an efficient system for both
delivery of and access to video content, as well as for
creation of and querying large databases. One main goal
of BUSMAN is to provide a testbed to assess the impact
of emerging technologies in the current business models
for digital content processing, annotation and distribution.
At the end of the project the implemented testbed will also
help to detect and test emerging products and services.
The research and development of BUSMAN are based
on the needs and expectations of two types of users:
content providers and end users. At the provider side,
advanced techniques for video analysis and indexing are
implemented to enrich content with metadata as well as
provide efficient search functionality. For end users,
efficient browsing and retrieval strategies are developed to
provide access to large databases across fixed and mobile
communication environments, whilst complying with the
main multimedia standards such as MPEG-7 and MPEG21.
The targeted final BUSMAN system features three
main functionalities:
x Retrieval, indexing, and browsing videos from
terminals of different capacities
x Video tagging and identification using Digital
Item Identification (DII) embedded in the content via
watermarking technology
x User friendly interfaces
Since the launch of the project in April 2002, the
consortium has made concerted efforts in addressing some
of the key issues behind the development of the prototype
system. Conducted work includes an extensive set of
human factors studies (use scenarios, user-centred design,
and usability [1]), novel video processing algorithms for
content analysis, modularised system design and
integrations [2], and user-friendly interfaces. Due to the
limited space, this paper only discusses the major
technical studies and system achievements so far,
focussing on logical and functional descriptions.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

This paper is organised as follows. In Section 2, related
research work and the usage context of the BUSMAN
system is outlined. The general system architecture is
presented in Section 3. The main functionalities and
system modules are described in section 4. The paper
concludes in Section 5 with a discussion on the remaining
works and planned enhancement for the delivery of a fully
functional prototype system for video content
management.

2.

Related Work and Project Context

Content-based video indexing and retrieval has been
the subject of active research in industry and academia
across the world. This reflects the commercial importance
of such technology and the fact that many problems
involved are still open. A comprehensive literature review
dealing with semantic audiovisual media analysis for
indexing and retrieval can be found in [3].
The ‘Multimedia Studio’ developed by the IST-project
SAMBITS provides functionalities to deal with selectable
arbitrary shaped video objects using MPEG-7 metadata
[4]. The digital-VCE funded project ‘Content Based
Recognition and Retrieval of Multimedia Information’
focused on strategies for key-frame selection and the
definition of basic video descriptors [5]. Several other
conventional image and video retrieval systems are
available to index and search video and images in the Web
that contain basic tangent points with the BUSMAN, e.g.,
QBIC [6], Photobook [7], Virage [8] and Webseek [9].
The BUSMAN project also deals with several issues
related to self-embedded metadata, persistent Digital Item
Identifiers and locators. This issue has also been

addressed by some international consortiums and
organisations, such as the ‘Content ID Forum’ (cIDf), and
the ‘DOI Foundation’, the latter has specified the ‘Digital
Object Identifier’, defined as a system for interoperable
identifying and exchanging intellectual property in the
digital environment. DOIs as defined are unique and
persistent. The MPEG Committee is currently working on
its MPEG-21 standard (dubbed Multimedia Framework).
Section 3 of the standard, called Digital Item
Identification, has been finalised in July 2002. The DII
specification contained in this standard addresses the issue
of content identifiers, also in the context of URIs and
URNs. A group within MPEG is developing further
initiatives in the field of ‘Persistent Associations of
information with Digital Items’. BUSMAN has
contributed to this work.
All these technological developments and standards
will have a significant impact on the usability and
accessibility of digital content through a new generation
of multimedia applications. The BUSMAN system is
contributing to speed up the development, deployment,
use, and commercialisation of these technologies.
One important feature of the BUSMAN system is its
generic approach to content management. It introduces a
new schema for enhancing multimedia delivery and
access from content creators down to end-users. It
provides increased knowledge about how to incorporate
advanced video processing technologies into a system in
which video resources can be transcoded and delivered
across a wide range of networks, using any required
format, and efficiently accessed from diverse terminals.

Fig. 1. Busman logical software architecture.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

By inserting pointers to metadata in the content itself,
the BUSMAN system guarantees the availability of such
information independently of the delivery channel. The
main innovation consists in its ability to handle metadata
for description and retrieval over heterogeneous delivery
channels Furthermore, the final system will contribute to a
better understanding of the effects of packet losses on the
embedded information in different environments.
In the BUSMAN environment any piece of digital
video can always be put into context: establishing its
source, obtaining the descriptions and identifying its
relationships to other video samples, are all tasks that the
BUSMAN makes possible. The result is a tagged content
that is easier to locate, retrieve, manage and track, all of
which contribute to an enhanced end-user experience.

3. System Architecture
The overall architecture of the BUSMAN conceptual
system diagram is shown in Fig. 1. It follows the popular
client-server model.
As illustrated in Fig. 1, the server comprises two
logical elements: the Input Unit (uploading and processing
a given media file), and the Information Server, which
delivers data (content & metadata) to end users. There are
two types of clients:
x The annotation client, which interacts with the
Input Unit, assisting a human annotator, and is
intended for professional users; and
x The end-user terminal, which is connected to the
Information Server for an end user to browse and
retrieve any desired contents, and for which two
versions are being developed: a fixed PC-based
desktop terminal and a mobile terminal.
The diagram also shows three different types of data
flows along the system, with the filled, empty, and empty
diamond arrows describing, respectively, the flow of
uncompressed media content, compressed media content,
and the metadata extracted or annotated.

much less capacity than that needed for complete
metadata.
Two different data embedding techniques are used in
BUSMAN to embed the BCT, they are as follows.
The first is a watermark embedded directly in the
MPEG-2 compressed stream, intended for the professional
use scenario defined in the project. The user will be
allowed to access metadata by extracting the BCT from
video, and further on linking the BCT to the database that
contains the metadata. Thus, the technical challenges to
meet are: embedding and decoding of watermark with
high imperceptibility, robustness to video editing
processes, and fast decoding speed.
To implement it, we use spread spectrum techniques.
Spread spectrum communication transmits a narrow-band
signal via a wide-band channel by frequency spreading
[10]. In the case of watermarking, it is to transmit the
watermark signal as a narrow-band signal via the video
signal acting as the wide-band channel. In the raw video
domain, the watermark signal is usually spread using a
large chip factor, modulated by the pseudo-noise
sequence, amplified afterwards, and finally added directly
to video pixels. Due to performance constraints in terms
of computing efficiency and degradation in re-encoding,
watermarking in pixel domain is not feasible in
BUSMAN. For this purpose, after an intermediate parsing
of the MPEG-2 stream a watermark signal prepared in the
similar fashion is directly added to the DCT coefficients
of the compressed video to accomplish the task.
The second data embedding functionality produces
MPEG-4 watermarked versions of the video content,
destined for streaming to clients. A smart client will be
able to extract the BCT, and through it locate the metadata
of the content. This watermarking is done in the raw video
domain, i.e. over a sequence of decoded images, taking
advantage of the transcoding operation. In order to
provide the best possible robustness for the watermark, it
is inserted after all video pre-processing tasks (image
resizing and/or frame rate adaptation) are completed.

4.2 Video Indexing and Retrieval

4. Main System Functionalities
4.1 Data Embedding
A novel feature of the system is the capability to
embed a Digital Item Identifier (DII) in compressed video.
A DII can be regarded as a serial number that uniquely
identifies a piece of content, and therefore can serve
conveniently as a pointer to the location of metadata
storage, which describes the syntactic and semantic
structures of the video content. For this system, a DII
specification called Busman Content Tag, or BCT for
short, has been defined. Embedding a BCT requires then

The metadata processing chain comprises the main
modules of video analysis and retrieval subsystem, which
takes as input the video content, and produces a range of
MPEG-7 compliant metadata, both high-level (semantic)
and low-level (media descriptors). The main processing
and interfacing modules include shot segmentation and
keyframe extraction; shot boundary editing; semantic
annotation; semantic search engine and the query-byexample subsystem (low-level video descriptor extraction
and descriptor search).
To be compliant to a standard, the ‘VideoSegmentDS’
from the MPEG-7 MDS standard was selected to
represent the shot information. These descriptions are the

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

central element of the BUSMAN system, because they
can store the required metadata information and relate
them to temporal units of the content. It can be seen as a
synchronization tool between metadata and media
information.

4.3 Metadata Model
The desirable functionality of the BUSMAN system
was revealed based on an extensive user centred study
[11]. This functionality was expressed in use case
scenarios. An analysis of the use scenarios was performed,
resulting in a set of processing steps that can be clustered
by their inputs and outputs. The resulting functionality of
the BUSMAN system supports search & retrieval on
various high level queries (e.g., genre, title and

keywords), refinement of the query, browsing of media
files lists, access to scenes of a movie, a report, or a
program, and content based search. It is a matter of the
metadata model that renders possible all the required
functionalities.
The BUSMAN metadata model is shown in Fig. 2. It
has been designed to be in full compliance with the
MPEG-7 standard. The central element of the metadata
model is the structural description (SegmentDS) allowing
navigation across the content. Each structural unit
(VideoSegment for scenes and shots) is annotated with
keywords (TextualAnnotation) and enriched with creation
and usage information. Furthermore, the VideoSegments
can have multiple key frames for which low-level visual
descriptors are extracted.

Top level
structural unit

Usage
Information
Creation
Information

Structural unit
Scene 1

Semantic
Information

usage-,
creation-,
semantic
information,
key frame

Structural unit
Shot 1

Structural unit
Shot 2

usage-,
creation-,
semantic
information,
key frame,
visual Ds

usage-,
creation-,
semantic
information,
key frame,
visual Ds

Structural unit
Scene 2

Key Frame

Fig. 2. Scheme of the BUSMAN metadata model.

4.4 Shot Detection and Video Summarization
Two efficient techniques for shot segmentation have
been investigated and implemented in the system. The
first one is specially adapted to the MPEG-2 streams used
as source material. For that the MacroBlock (MB) type
information is used. The MBType defines the character of
the MB prediction.
Since the MPEG sequence has a high temporal
redundancy within a shot, a continuously strong interframe reference will be present in the stream as long as no

significant changes occur in the scene. The amount of
inter-frame references in each frame and its temporal
changes can be used to define a metric, which measures
the probability of scene change in a given frame. Thus, we
can define a metric for the visual frame difference by
analysing the statistics of MBTypes in each frame.
The other implemented shot segmentation algorithm
derives visual content features from the uncompressed
image sequence, rather than from the MPEG-2 bit stream.
Despite this places an additional burden on the system
(decoding the video sequence); its impact is not

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

significant except on low-complexity systems. And it has
the additional advantage of potentially allowing any video
format to be used. The features chosen belong to the wellstudied MPEG-7 visual descriptors, thus with known
characteristics, such as scalability, small size and low-cost
matching.
In the current implementation, only one feature vector,
the ‘ColorLayout’, is used. This is a simple DCT-based
representation of the layout of colour within a frame. One
DCT is created for each colour plane (one luminance and
two chrominance components). A distance metric
involves taking the weighted Euclidean distance between
each DCT value in each colour component. This leads to
fast matching, and scalability can be improved by using
fewer DCT values at the expense of accuracy.

The semantic annotation model used in BUSMAN can
be mapped to a subset of the MPEG-7 ‘Multimedia
Description Schemes’ standard, selected as a compromise
between expressiveness and ease of annotation. Due to
this last requirement the ‘SemanticDS’ and friends were
discarded as unsuitable to be used by non-expert
annotators, since the work needed to create the underlying
abstraction model was deemed too complex.

4.5 Shot Boundary Editing
This is a Server application module that allows a
professional user to modify manually the shot boundaries
and keyframes automatically created by the system. The
interface is shown in Fig. 3, and it contains a special
timeline with a slider (to move the video around) and
where shot boundaries and keyframes are highlighted. By
using the control options, the annotator can change the
position of any shot boundary or select as keyframe a
different image than the one chosen by the system.

Fig. 3. The interface for shot boundary editing.

Fig. 4. Interface for shot-based semantic
annotation.
Instead, a simpler and more intuitive keyword-based
approach was adopted based on the MPEG-7
‘StructuredAnnotation’ descriptor. This descriptor allows
storing textual annotations in terms of seven basic
semantic concepts: animate objects – people and animals
(Who), actions (WhatObject), objects (WhatAction),
places (Where), time (When), purposes (Why), and
manner (How). An annotator can associate any number of
those semantic descriptors to a given shot. A dual
annotation scheme has been set up by making use of the
TermUse MPEG-7 datatype, so that any annotation can be
either a free text description (but still tagged to one of the
mentioned 7 concepts), or a dictionary-based term.
Fig. 4 presents the implemented interface for shotbased semantic annotations. A Shot Browser with
special controls (including a zoom bar) allows locating the
desired shot, which can then be played in the Shot Player.
The main section is the Semantic Annotation interface that
allows an easy editing of the annotation.

4.6 Semantic Annotation
4.7 Query by Example
This application allows the user to attach, edit or delete
semantic annotations. These can belong either to a
complete media file, in which case they will correspond to
global semantic descriptions of its content, or to a shot, in
which case they will describe only the content of that shot.
The same annotation scheme is used for these two types
(although the user interface has of course slight
variations).

This application module, the interface of which is
shown in Fig. 5, allows the user to search for similar video
shots via keyframe matching. For this purpose, the user
can select query images in the upper part of the window
when browsing through the shot keyframes. By clicking
on the result images, the user can navigate to the shots

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

corresponding to the result image, and obtain further
metadata information.
The search is based on measuring a similarity metric of
selected MPEG-7 visual features between the query and
candidate images. For extracting the visual features and
computing the distances, the MPEG-7 XM reference
software is used.
Six colour and texture MPEG-7 visual descriptors have
been used. The selection is based on previous
performance studies. Since the system model does not
include any spatial segmentation module, no shape
descriptors are used.
At present time, the distance values between individual
descriptors are combined using a weighted sum. To do
this, the distance functions have been normalised with
respect to their working point (WP). The WP was selected
using experiments, thus, for distance values below the
WP, images are perceived to be similar and vice versa. To
select the weighting factors in an intuitive way, a
relevance feedback function will be added in a future
release.

5

Conclusions and Future Work

In this paper a prototype implementation of the
BUSMAN system for effective video management and
delivery has been presented. The main functionalities and
system features have been described.
The remaining works of the project include
investigating tools for the chosen use scenarios and video
genres (which may provide automatic suggestions for
annotation), and the development of a novel relevance
feedback algorithm for query by example search. Also
high on agenda are the test and usability studies of the
server system and fixed clients. Finally, a mobile client is
to be fully developed and integrated into the overall
system.

Acknowledgements
This paper is based on the research and development
activities conducted within the IST project BUSMAN,
which is partially funded by the EU under grant Nr. ISTBUSMAN-2001-35152. URL: www.ist-busman.org.

References
[1] A. Evans. User-centred design of a mobile football video
database. Proceedings of 2nd Intl. Conference on Mobile
and Ubiquitous Multimedia, Norrkoping, Sweden, Dec
2003.
[2] P. Villegas et al. An Environment for Efficient Handling of
Digital Assets. Proceedings of WIAMIS' 2003, London, UK.
[3]
E. Izquierdo et al. State of the art in content-based
analysis, indexing and retrieval. Public deliverable
D2.1_V2, EU IST SCHEMA project, www.schema-ist.org,
2003.
[4] EU IST SAMBITS project - System for Advanced
Multimedia
Broadcast
and
IT
Services
www.ipsi.fhg.de/delite/Projects/SAMBITS/
[5] P. Hill. Review of Current Content Based Recognition and
Retrieval Systems. Technical report Theme T8, Nr. 99-05/1,
Virtual Centre of Excellence in Digital Broadcasting and
Multimedia Technology.
[6] M. Flickner et al. Query by image and video content: The
QBIC system. IEEE Computer, 28, pp 23-32, September
1995.
[7] A. Pentland, R.W. Picard, and S. Sclaroff. Photobook:
Content-based manipulation of image databases.
International Journal of Computer Vision, 18(3), pp 233254, 1996.
[8] A. Hampapur, et.al. Virage video engine. Proc. of SPIE,
Vol. 3022, pp 188-200, 1997.
[9] J.R. Smith and S.-F. Chang. An image and video search
engine for the world-wide web. Proc. of SPIE, Vol. 3022,
pp 85-95, 1997.
[10] F. Hartung, and B. Girod. Watermarking of uncompressed
and compressed video. Signal Processing, Vol. 66, no. 3,
pp. 283-302, 1998.
[11] Alyson Evans, Carsten Grünheit, Ana Maria Alarcon
Sanchez, Stephan Herrmann, Thomas Huber, Simon
Waddington, Paulo Villegas, Pedro Concejero, BUSMAN
Deliverable D2.2: Overall System Design and Reference
Model, February, 2004.

Fig. 5. The query-by-example search interface.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

