Color Model Based 3-D Self-Organizing Map
Kan LIU
School of Information, Zhongnan University of
Economics and Law, 430064, Wuhan, China
liukan@znufe.edu.cn

Ping LIU
Institute of Informatics, School of Computing,
University of Leeds, Leeds, LS2 9JT, UK
pliu@comp.leeds.ac.uk

Abstract

SOM runs slowly because the weight vectors of
neurons are initialized by random data, and this is the
reason why SOM is always used in only 2-D
representation space.
This paper focuses on the initial weight vectors
problem and proposes a new approach – a color model
based 3-D SOM. The remainder of the paper is
organized as follows. In Section 2 we present a brief
review of SOM (readers familiar with this may skip to
Section 3). In Section 3 we analyze the problem of
initializing neurons and introduce the related work on
solving this problem. Section 4 describes our approach
of applying the color model to the initialization of the
SOM neurons. In Section 5 we present the
experimental results of our approach applied to a
popular benchmark data set. Finally conclusions and
future work are presented in Section 6.

The Self-Organizing Map (SOM) is widely accepted
as a data visualization and cluster model for its ability
to map high dimensional data in a low dimensional
output space according to the data’s similar features.
However, this mapping process is time consuming and
a large amount of iterations are needed in order to
increase the accuracy of the data representation. This
paper describes how to apply the RGB color model to
the initialization of the SOM neurons. The major
feature is that the distribution of the neurons is closely
related to the data distribution during the initialization
of SOM. Therefore the iterations are greatly reduced
and efficiency and accuracy of SOM are much
improved. To evaluate our approach against
traditional approaches we have conducted an
experiment. The initial results show that the color
model based 3-D SOM is very promising in the
practical application.

1. Introduction
The Self-Organizing Map (SOM) is an unsupervised
competitive artificial neural networks technique
invented by Teuvo Kohonen [12, 13] which has
reduced the dimensions of data through the use of selforganizing neurons. This technique is particularly
useful for visualization [17] and cluster analysis [4, 15]
in a wide range of domains such as knowledge
discovery [2, 11], text classification [1, 16], pattern
recognition [3, 14], gene expression patterns [10], web
exploring [19], and so on.
However there are some problems in using SOM to
visualize data sets. Firstly, the weight vectors of the
neurons must be based on data that can successfully
group and distinguish inputs data. Secondly, it requires
sufficient data in order to develop meaningful results.
Thirdly, it is often difficult to obtain a perfect mapping
where groupings are unique within the map. Finally,

2. SOM overview
The self-organizing map model consists of
neurons, each of which is assigned a multi-dimensional
weight vector. There are two layers in the SOM
network where the sample data from the input layer
have to be mapped onto the neurons of the output layer.
The SOM map attempts to represent the topography of
the input data by the neurons where similar neurons are
close to each other and dissimilar neurons distanced
from each other.
Let the input data set be a d-dimensional data set
with n data: X={x1, x2, …, xn}, here xi=(xi1, xi2,…xid).
Let the output map have m d-dimensional neurons,
and each neuron has the same dimensions as the input
data: M ={m1, m2, …, mk}, here mj =(mj1, mj2,…mjd) is
the jth neuron weight vector.
Initially, pick up a random input vector x (sample
data). Then, compute the distance between x and mj by
the Euclidean distance formula, and find the best
matching unit mb:
(1)
||x-mb||=min{ ||x-mj||, j:1ĺk }

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

After that, update weight vector to mb and its
neighbors to reduce the distances between them and the
input vector according to the rule:
mi(t+l)=mi(t)+Į(t) hbi(t)[x-mi(t)]
(2)
Where t is the step index, Į(t) is an error-adjusting
coefficient (0 <Į(t) < 1) that decreases over time, and
hbi is the neighborhood function. One of the
neighborhood functions can be described as below:
hbi(t)=exp{-||rb-ri||/2ı2(t)}
(3)
This regression is usually reiterated over the
available samples and the fitting of the model vectors is
carried out by a sequential regressive process.
The process of the SOM algorithm is below: 1)
Initialize the weight vectors of the neurons on the
output layer; 2) Get one sample from input layer; 3)
Select the best matching unit with the minimum
distance to the sample as the winner; 4) Update the
weights of the neurons in the neighborhood of the
winner; 5) Continue (steps 2-4 are repeated) until the
weights stabilize.

3. The related work
The weight vectors of neurons in a SOM are usually
initialized with random values. The randomicity is
likely to cause similar-quality input data to spread at
different places on the output layer, thus the
neighboring neurons on the output layer do not have
their weight vectors point to the neighboring positions
in input space initially. So SOM is time consuming.
The more neurons in a SOM map, the more interactions
are needed in order to make the topology in output
space correlate with the topology in input space.
Furthermore, because a 3-D SOM network needs more
neurons, so the SOM is seldom used in 3-D space even
though the 3-D mapping can be more sensitive in
displaying the data set. Therefore it is important to
improve the initialization in order to enhance the SOM
in getting the result more quickly.
Several approaches have been proposed to improve
the initialization of the weight vectors [20]. The first
approach is to select a random data in the data set as
the initial value of weight vector. The second approach
is to randomly select a value between the minimum and
maximum value of the input data set rather than
randomly select from [0,1]. This change makes it easier
to find the proper weight The third approach is to give
an output threshold value hj to each neuron, with dj+hj
being the distance value of two neurons. In the learning
process, the times each neuron has been selected
should be recorded, and when a neuron has been
selected frequently, its threshold value should be raised
to increase the chances of other neurons being selected.

Therefore the weight vectors are utilized more
effectively. Although these three approaches can hasten
the learning process of SOM to some extent, the initial
value for each neuron is still assigned randomly and not
closely related to the data set. Thus the processing
efficiency of SOM has still not been greatly improved.
Rather than randomly selected the initialization,
Wlodzislaw and Antoinc [18] explored the mapping
results of Multi-Dimension Scales (MDS) to decide the
weight vectors of output neurons. This approach can
set up some relationships between data distribution and
neurons distribution. However, it can only deal with
data of 20 dimensions at its maximum, and the more
dimensions, the more complex the computation needed.
We propose a new approach (a color model based
SOM) to the initialization of SOM. It uses the datadriven neurons initialization to make the initialized
neurons closer to the data’s real distribution. As a
feature of our approach , the interactions are greatly
reduced; the quick and accurate visualization of a
multi-dimension data set is achieved. Another feature is
that it applies SOM to 3-D space after solving the
computationally expensive problem of SOM.

4. Color model based SOM method
4.1. The basic idea
In the study of colour image processing, each kind
of spectrum spreading within the range of visible lights
can show a certain color. The mapping between the
spectrum and the color can be reflected quantifiably by
the color matching function. If the high-dimensional
data is considered as the distribution of the spectrum
energy within the range of visible light, a certain color
can be found to show this high-dimensional data.
Assuming that the self-organization mapped neuron
network distributed according to the structure of a
color model in the initialization, the corresponding
neuron for the sample will be found in the network. At
this time, the neuron and the sample have the same
color and if we weight every dimension of the neuron
with the value of each dimension of the sample, a close
relationship will be formed between neurons and
samples. In this way, the iterative times of the learning
process will be greatly reduced and the mapping of the
data can be realized quickly.

4.2. RGB color model
The characteristics of light change according to the
length of the electromagnetic waves, ranging from
radio waves, through visible light to gamma rays. The

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

energy carried by the waves, which are visible light
(approximately 400-700 nm), will stimulate the
receptors in the human retina, producing natural color
[8]. This energy distribution is called the stimulation
spectrum ĭ(Ȝ). At the same time, the RGB color model
mixes the three primary RGB colors (red, green, blue)
to reproduce all colors found in nature (Figure1).

Energy
1.0

b(Ȝ)
g(Ȝ)

r(Ȝ)

ĭ(Ȝ)

0.5

400

500

600

700
Waveleng

Figure 2. The color matching functions
corresponding to the stimulation
spectrum function

4.3. Map high-dimensional data on color model

Figure1. RGB Color Model
Every stimulation spectrum function ĭ(Ȝ) can be
transformed into a point in RGB color space. The
quantitative relationship between the stimulation
spectrum ĭ(Ȝ) and RGB color coordinate can be
computed by the transformation equations below [5, 7]:
R = k×ȈȜ Ɏ(Ȝ) × r(Ȝ) ×ǻȜ;
G = k×Ȉ Ȝ Ɏ(Ȝ) ×g(Ȝ) ×ǻȜ;
(4)
B = k×ȈȜ Ɏ(Ȝ) ×b(Ȝ) ×ǻȜ;
In the equations, k is the ratio. Ȝ refers to the
wavelength of the visible light, ranging from 400nm to
700nm, ǻȜ=5nm. r(Ȝ), g(Ȝ) and b(Ȝ) stand for the color
matching function of red, green and blue, and the value
of r(Ȝ), g(Ȝ) and b(Ȝ) at every 5nm is measured by CIE
(Commission Internationale de l'Eclairage), which is
already known. The tristimulus values graphed in
Figure 2 describe the color matching function of the
CIE 1931 standard colorimetric system, where r(Ȝ),
g(Ȝ) and b(Ȝ) curves stand for color matching function
and ĭ(Ȝ) curve is stimulation spectrum function. Using
the three tristimulus measurements, any color can be
fully described.

Each data item Xi = (xi1, xi2, …, xij, …, xid) in highdimensional space can be plotted by a polygonal line in
parallel coordinates [9] as shown in Figure3. This
polygonal line can be viewed as a stimulation spectrum
curve if the axes in parallel coordinates are spread
evenly within the range of the wavelength of the visible
light. Then Xi can be regarded as the function of
wavelength Ȝ, the attribute values of Xi corresponds to
that of the stimulation spectrum function ĭ(Ȝ). For
example, if Xi is a 6-dimensional data (xi1, xi2, …, xi6),
then:
Xi(Ȝ) = xi1,
( 400Ȝ<450 )
Xi(Ȝ) = xi2,
( 450Ȝ<500 )
…… Xi(Ȝ) = xi6,
( 650Ȝ<700 )
According to formula (4), we get formula (5)
which can work out the 3-D coordinates of Xi in RGB
space.
XR = k’×ȈȜ Xi(Ȝ) ×r(Ȝ) ×ǻȜ;
(5)
XG = k’×ȈȜ Xi(Ȝ) ×g(Ȝ) ×ǻȜ;
XB = k’×ȈȜ Xi(Ȝ) ×b(Ȝ) ×ǻȜ;
In formula (5), k’ is the ratio, ǻȜ=5nm, and Xi (Ȝ)
corresponds to the relevant Ɏ(Ȝ). By adjusting the
value of k’, the mapping space can be not only the
RGB color space, but also any 3-D space. Therefore,
every high-dimensional data can be transformed into a
point in the RGB color space, which means each highdimensional data is related to a certain color. Since the
wavelength of visible light ranges from 400nm to 700
nm, and ǻȜ=5nm, so it is up to 60-dimensional data
that can be dealt with.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

xid

xi 2

Neurons arrayed by
RGB color model

xi 4

Xi

convergence of the neurons can be achieved rapidly.
The initialization process is shown in Figure 4 below:

xi1
Get sample
data

xi 3

x1

x2

x3

x4

Mapping to
RGB coordinate

Initialize the neuron
with the sample

xd

Figure 3. High-dimensional data plotted
by parallel coordinates, where

Begin SOM learning
process

xi ( i = 1,

2, …, d) is the parallel axes, and (xi1, xi2,
…, xid) are the attribute values of Xi

4.4. The initialization of the weight vectors
By applying the spectrum-color transformation
equations to high-dimensional data, each highdimension data corresponds to a certain color. If, in
SOM, the neuron network distributes according to the
RGB color model in the initialization, the highdimensional data and the neurons can be connected by
color. Let each weight vector of the neuron initialized
be the equal of each dimensional value of the sample
data that has the same color. Thus the relevant
information of the data has already been reflected in
the initialization of the neuron grid, and the similarquality data in the high-dimensional data space will
have similar mapping in the SOM network
initialization.
We extend the SOM neurons to a 3-D cubic grid.
This cubic grid can be regarded as a REG color model,
and the neurons spreading in different nodes of the grid
indicate different colors. In other words, each neuron
weight vector consists of two parts: one part refers to
the three basic colors R, G & B, shown by the 3-D
color model, and the other part is the weight vector
corresponding to the high-dimensional data. When the
output neurons array according to the color model, the
weight vectors are not initialized yet. When a sample is
picked up, its color is also received, so the next step is
to let each dimensional value of the sample equal the
matched neuron weight vectors (the sample and the
neuron have the same color), and then the initialization
of neurons is finished.
On all The initialization result of the weight vectors
is decided by the samples of input data set and
primarily reflects the data distribution, and then by
running SOM to readjust the data distribution, the
extent of adjustment is largely reduced, so the

Find the neurons
with the same color

Figure 4. Process of color model based SOM

4.5. The improved SOM algorithm
The improved SOM algorithm consists of 6 steps:
1) Array the neurons of SOM output layer equally in
the RGB color space;
2) Select sample data, calculate the R, G, B
coordinates of the sample by formula (5);
3) Scan the neurons on the output layer; for the
neuron that matches with the color of the sample,
initialize its weight vector with the value of the sample;
4) Go to step 2, get next sample, and initialize
another neuron;
5) After initialization, go on the left step of SOM;
6) After the convergence of the grid is achieved,
show the data distribution in the form of SOM.
The samples can be selected at random, and the
number of samples can be decided according to the size
of the data set. If the size of the data set is much larger
than that of the neurons, three or four times the size of
neurons can be the size of the selected samples.
Another condition to stop the initialization is that each
neuron weight vector has been given a value or the
whole data set has been scanned.

5. Experiment and results
The experiment is run on a PC with Pentium III 733,
256 MB RAM, and 40GB HDD. The output layer of
SOM is a three dimensional cube with 10×10×10
neurons. In the traditional SOM method, the initial
weight vector is given a randomly selected value from
(0, 1). Contrastively, in the color model based SOM,
the weight vectors scatter equally between (0,0,0) and
(255,255,255), with each neuron representing a color
(Figure 5).

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

white
blue

green

black

red

Figure 5. Initialized array of neurons
The following experiment is designed to test our
approach. We collected data of average temperatures
during 12 months in 42 randomly selected tourist cities
around the world. The original data forms a 12dimensional data set. The reason that we chose this
kind of data is that climate data is typical data used for
data mining, and it has high demands on visualization.
Normally the climate of cities that are geographically
alike should be relatively similar, thus these cities have
similar temperature distribution patterns and so the
temperature map of the cities should reflect the
geographical distribution.
Figure 6 shows the results of 300 times of iteration
using color model based 3-D SOM method. From
Figure 6 it can be seen that cities with lower
temperatures (such as Toronto, Muskvale, and
Helsinki) are located in the front bottom corner while
cities with higher temperatures (such as Hong Kong,
Cairo, and Houston) are clustered together in the back
upper corner. Cities in the southern hemisphere (such
as Sao Paulo, Sydney, and Melbourne) are separated
with in the northern hemisphere (such as Beijing,
Seoul, and New York). In this temperature map, similar
temperature patterns are grouped together. This result
is considered as effective when we compare the actual
physical position of these cities.

(b)
Figure 6. Display of the average temperature in
cities around the world, where (a) shows the
cities in whole and (b) shows some cities in
details
Although some similar results can be obtained when
we use the traditional approach (random initialization),
but the process needs more iterations. We compared
the accuracy of these two approaches by calculating the
error, and weighing the error of the data mapping result
with the sum of the squares of the differences between
the value of the data itself and the value of its best
matching unit, that is:
n

E = å ( xi − m i ) 2

(6)

i =1

In this formula, mi is the value of the best matching
unit corresponding to xi. The error comparison between
the two approaches is shown in Figure7, in which the
error of the traditional approach is represented by the
squares, the result of the optimized approach after the
initialization of color is represented by the diamonds,
and the iterative times changes from 50 to 500. From
Figure 7 we can see that on condition of the same times
of iterations, the error in the random initialization is
always larger than that in the color model based
initialization. For example with 200 iterations, the error
of our approach is 0.615 while that of the traditional
approach is 0.935. When the error is equal, the
iterations in the traditional approach are always in
excess of our approach. For example, when the error is
0.35, 300 iterations are needed in our approach, while
more than 500 in the traditional approach.

(a)

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

Error

Random initialization
Initialization using
RGB color model

2.0
1.5
1.0
0.5
100

200

300

400

500

Iterations
Figure 7. Comparison of iterative times and
errors in two methods

7. Conclusions
This paper presents how to apply the RGB color
model to initialize the neurons of SOM. In our
approach the RGB color model is used to connect the
neurons and the high-dimensional data since both the
neurons and the high-dimensional data can be mapped
to one point in the RGB color model. Based on this
initialization, the distribution of neurons weight vectors
is closer to the real distribution of the data set, and the
iterations of SOM learning process is reduced
dramatically. The improved SOM is used easily in a 3D space. This not only extends the display methods of
SOM, but also gives more vision angles to observe the
data set. We have demonstrated the advantages of
using color model based 3-D SOM in a case study. The
results of this initial experiment show that it improves
the performance against traditional SOM. In the future
further explorations will be carried out with other
techniques such as hue, saturation, luminance,
chromaticity, and so on in order to obtain more
accurate and effective results.

7. References
[1] K. Ahmad, T. A. Bale and D. Burford, “Text
classification
and
minimal-bias
training
vectors”,
Proceedings of International Joint Conference on Neural
Networks, Piscataway, NJ. IEEE Service Center, volume 4,
1999, pp. 2816–2819.
[2] D. Alahakoon, S.K. Halgamuge and B. Srinivasan,
“Dynamic self-organizing maps with controlled growth for
knowledge discovery”, IEEE Transactions on Neural
Networks, 11(3), 2000, pp. 601–614.
[3] N. M. Allinson, and H. Yin, “Self-organising maps for
pattern recognition”, Kohonen Maps, Elsevier, Amsterdam,
1999, pp. 111–120.

[4] A. Ciampi and Y. Lechevallier, “Clustering large, multilevel data sets: an approach based on kohonen selforganizing maps”, Principles of Data Mining and Knowledge
Discovery, Springer-Verlag, Berlin, 2000, pp. 353–358.
[5] J.D. Foley, A. van Dam, S.K. Feiner and J.F. Hughes,
Computer Graphics: Principles and Practice, AddisonWesley, 1990.
[6]
T.
Germano.
Self-organizing
Maps,
url:
http://davis.wpi.edu/~matt/courses/soms/
[7] R.C. Gonzales and R.E. Woods, Digital Image
Processing. Addison-Wesley, 1992.
[8] D. Hearn and M.B. Panline, Computer Graphics.
Prentice Hall Press, 1997.
[9] A. Inselberg and B. Dimsdale, “Parallel Coordinates: A
Tool For Visualizing Multidimensional Geometry”,
Proceedings of IEEE Conference on Visualization '90, Los
Alamitos, CA, 361-378, October, 1990.
[10] S. Kanaya, M. Kinouchi, T. Abe, Y. Kudo and Y.
Yamada, “Analysis of codon usage diversity of bacterial
genes with a self-organizing map (SOM): characterization of
horizontally transferred genes with emphasis on the coli o157
genome”, GENE, 276(1–2), 89–99, 2001.
[11] S. Kaski, "Data exploration using self-organizing maps",
Acta Polytechnica Scandinavica, Mathematics, Computing
and Management in Engineering Series No. 82. Technique
Thesis, Helsinki University of Technology, Finland. 1997.
[12] T. Kohonen, Self-Organizating Maps, New York,
Springer-Verlag, 1997.
[13] T. Kohonen, “Self-Organizing Maps”, Volume 30 of
Springer Series in Information Science, Berlin, Springer,
Heidelberg, 1995.
[14] M. A. Kraaijveld, J. Mao, and A.K. Jain, “A nonlinear
projection method based on Kohonen’s topogy preserving
maps”, IEEE Transactions Neural Networks, Vol. 6, 1995, pp.
548-559.
[15] D. Merkl and A. Rauber, “Alternative ways for cluster
visualization in self-organizing maps”, Proceedings of
Workshop on Self-Organizing Maps, Espoo, Finland, June
1997, pp. 106-111.
[16] J. Rozmus, “Information retrieval by self-organizing
maps”, Proceedings of 16th National Online Meeting,
Medford, NJ, USA, 1995, pp. 349-354.
[17] J. Vesanto, “SOM-based data visualization methods,”
Intell. Data Anal., Vol. 3, No. 2, 1999, pp. 111–126.
[18] D. Wlodzislaw and N. Antoinc, “On global selforganizing maps”, Proceedings of ESANN’96, Bruges,
Belgium, April 1996, pp. 367-370.
[19] C. C. Yang, H. Chen and K. Hong, “Exploring the
World Wide Web with Self-Organizing Map”, Proceedings of
the International World Wide Web Conference, Honolulu,
Hawaii, May, 2002, pp. 7-11.
[20] X.J. Yang, J.L. Zheng. Artificial neural networks.
Beijing, Science Press, 1992.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

