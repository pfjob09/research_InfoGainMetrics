Realistic Face Modeling With Robust Correspondences
Wang Kun, Zheng Nanning
The Institute of Artificial Intelligence and Robotics, Xi'an Jiao Tong University, China
{ kwang@aiar.xjtu.edu.cn}
Abstract
Finding robust correspondence is an important
problem in structure from motion algorithm. Because the
human face contains many low texture and homogeneous
areas, some algorithms such as corner matching are
unstable and may fail sometimes. We used the face
definition parameters and the symmetry of human face as
prior knowledge to find reliable correspondences between
two pictures, while most SFM algorithms use the generic
model as a modulator in the post-processing steps. This
paper proposes a whole scheme to construct textured 3D
face models from two views with a few user interactions.
According to the correspondences, a multistage SFM
approach is used to reconstruct the structure. Then we
use the RBFCS algorithm to interpolate more 3D points
according to the scattered feature points. A user with an
ordinary camera can use our system to generate his face
model in a personal computer.

1. Introduction
Realistic face modeling is useful to computer games,
movies, video communications and television etc.
Generating realistic 3D human face model is thriving
researched in computer vision whereas it is still a
persistent challenge. There are several factors that make
the problem so elusive [6].
·The human face is an extremely complex geometric form
to be reconstructed.
·The face exhibits countless tiny creases and wrinkles as
well as subtle variations in texture.
The modeling algorithm can be categorized according to
the acquisition of face images. The most popular
commercially available algorithms have utilized laser
scanners [1][11] such as the Cyber ware Color Digitizer.
But a 3D scanner is very expensive and the data are
usually quite noisy. E. Garcia utilized the structure light
[2] to model a face. Stereo algorithms use the geometric
relation over stereo images to recover the surface depth
[3][4] with the calibrated camera parameters. The
disparity map is apt to be disturbed in the low texture and

homogeneous areas while there are many low texture and
homogenous areas in human face. The camera calibration
for stereo algorithm is complex.
Recently increasing interests arise to utilize image
sequences to reconstruct the models [5][6][7]. Pighin et al.
[6] presented a technique with several un-calibrated
photographs and user interactions to generate the face
geometry. W.S. LEE [9] used two pictures from
orthogonal views to estimate the 3D face geometry.
Zhengyou Zhang et al. [7] developed a system that
constructs textured 3D face models from videos with user
interactions. The user is instructed to choose five feature
points in the two images. Then they use the corner
matching algorithm to find the correspondence and
determine the head motion and rotation between the two
images. Finally they use 3D fitting process to minimize
the distances from the reconstructed 3D points to the
neutral face model. Their work is very attractive and
inspired our research. In spite of its success, the system
fails sometimes because the corner points matching may
fail. The feature-tracking techniques often fail to produce
correct matches due to large motions, occlusions or
ambiguities. However, most of the “structure from
motion” algorithms are sensitive to the incorrect
correspondences. Finding robust correspondence becomes
a key step to the SFM approach.
The 3D structures of human face exhibit strong
symmetries and regularities. They are common
characteristics of human face that are described in face
definition parameter (FDP) in MPEG4. We utilize these
symmetries
and
regularities
to
find
robust
correspondences.
This paper presents a fast and efficient scheme to
construct a human face model with an ordinary camera
and a personal computer. We use two pictures and a few
user interactions to mark some feature points. Then we
use the face definition parameter in MPEG4 and the
symmetry of human face as prior knowledge to find more
correspondences. While most SFM algorithms use the
generic model which represents the common
characteristics of human face as a modulator in the postprocessing steps [7][5]. Compared with [7], the matching
results found by our algorithm are robust but very sparse

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

with respect to the number of vertices on the wire-frame.
So we use the radial basis function to interpolate more
points with the scattered feature points.
This paper is organized as follows. In section 2, we
introduce our whole scheme. Section 3 is the details of
the algorithm that includes four parts: feature points
matching, structure from motion, radial-based function
and texture mapping. The experimental results are shown
in section 4. Section 5 is the conclusion and future
research.

2. System Overview
Figure 1 illustrates the whole scheme of our system.
The first step is data acquisition. We take two frontal
views with upward head motion and two orthogonal
pictures. The user is instructed to choose a few feature
points in each of the two frontal images. We use the face
definition parameters and the symmetry of human face to
find more correspondences. With the correspondence
known, it could estimate the rotation and translation
matrix between the two images and reconstruct the
structures. Compared with vertices on the face mesh
[1][7], our reconstructed feature points are sparse but
these points could represent the specific face geometry. It
is a scattered data interpolation problem. We use the
compactly supported radial-based function to interpolate
more points [13][18][17]. The face texture is generated
from the two orthogonal images with the multi-resolution
image mosaics algorithm [15]. Finally we map the
textures into the 3D face model.

3. Proposed approach
3.1. Finding the Correspondences
The first stage is to take two frontal images with a
relative head motion upward such as in Figure 2. The
reasons are:
·Turning head upward is easy to be accomplished.
·With the head movement, the x coordinate of the 3D
points are not changed. We will use it to deduce the
correspondences.
·Because the head is symmetric in geometry structures,
we will reconstruct half part of the head and get the other
half with symmetries. The difference between the left and
right parts can be exhibited with textures.
Then the user is instructed to locate a wire frame into
the two images. The wire frame is consisted with 17
markers including inner eye corner, nose top, mouth
corner etc.

Figure 1 Overview of our system
They are denoted as P = { pi | i = 1,..., N} , where
N=17 in our algorithm. The white points in Fig. 2 are the
markers with the user’s indication.
The FDP in MPEG4 defines the appearance of the
human face. Table 1 is part of the FDP in MPEG-4. Now
we take the right eye as an example to illustrate our
matching algorithm as follows.
1) With the user’s indication, the coordinates of the eye
corners P1 and P2 are known. According to Table 1 and
the symmetry property, we find other two corners and the
center of the eye. Then we use an ellipse to describe the
eye.
2) Finding other 12 points in the ellipse whose x
coordinates are the equal divided points between points
P1 and P2 such as Fig. 3. Then the “eye” in Figure 3(a)
contains 17 points.
3) Do similar operation in Fig. 3(b).
4) Because the head turns upward, the x coordinates of
the feature points are not changed between these two
'

'

images. When ( P1 , P1 ) and ( P2 , P2 ) are matching
points with the user’s instruction, the other points are also
corresponding respectively. Other feature points are
selected similar to the eyes. For example, we use 9 points
in two parabolas to describe the mouth.
In our algorithm we use 55 feature points to make up
the coarse face mesh. They are denoted as the green
points in Fig. 2.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

4.1
4.2

Table 1
Feature points
Text description
Right corner of left eyebrow
Left corner of right eyebrow

Part of the feature points in FDP
Recommended location constraints
x

4.3

Uppermost point of the left eyebrow

4.4

Uppermost point of the right eyebrow

4.5
4.6

Left corner of left eyebrow
Right corner of right eyebrow

(a)
(b)
Figure 3 eyes correspondences between two views

3.2. SFM with known Correspondences

In section 3.1, we have obtained a set of
correspondences between the two frontal images. The
reconstruction problem becomes a SFM with known
correspondence problem. We use the following 6 steps to
estimate the essential matrix E and the motion parameters
in matrix t and R [10].
Step 1: Estimate the essential parameters E 1 with 8-point
algorithm.
Step 2: Imposing the zero-determinant constrain to refine
the initial estimate E 1 into E 2 .
Step 3: Refine the parameters by minimizing the sum of
squared distances between points and their epipolar lines.
The obtained results are denoted as E 3 .
Step 4: Estimate the R and t from E 3 .
·As E t = 0 , the matrix t is the solution of the
following problem.
T

2

min E T t ,
t

subject to t = 1

In this paper, we denote the homogeneous coordinate
of a vector x by ~
x . The homogeneous coordinate of a

·As

3D point M = [ x, y , z ] in a world coordinate system is

solving min E − [t]× R

T

~
M = ( x, y , z ,1) T . The homogeneous coordinate of its
T
~ = [u , v,1]T . They
retinal image point m = [u , v ] is m
are related by (1).

~

α

A=0
0


ν
β

(1)
are given by

u0 
 1 0 0 0
 R t
,


.
v 0  V =  0 1 0 0  , Ω =  T
1
0



0 1
 0 0 1 0
The elements of matrix A are the intrinsic parameters
of the camera. They are determined by camera calibration
[12]. Matrix Ω is the 3D transformation including
rotation matrix R and translation matrix t from the world
coordinate system to camera coordinate system. The
essential matrix [14][10] is denoted as E where
E = t ×R.

[]

z

(4.1.x+4.5.x)/2 or x coordinate of the uppermost
point of the contour
(4.2.x+4.6.x)/2 or x coordinate of the uppermost
point of the contour

Figure 2 two frontal images with head moving upward

~ = AVΩM
λm
where λ is a scale, and A, P and Ω

y

(2)

E = [t]× R by definition [14] [10], we find R by
2

R

subject to R T R = I and det(R) = 1
(3)
Step 5: Refine the motion and structure estimate with the
maximum likelihood (ML) estimation.
(4)
θˆ = arg max log( P ( X | θ ))
θ

where θˆ is the maximum likelihood (ML) estimate of the

motion and structure parameters of θ and X is the set of
corresponding points.
Step 6: Once the motion (R, t) is known, matched points
'

can be reconstructed in 3D space. Let (m, m ) be a
couple of matched points, and P be their corresponding
points in 3D space.
2

2

P = arg min ( m − mˆ + m ' − mˆ ' )

(5)

p

ˆ and
where m

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

mˆ ' are projection of P in both images.

Step 1 is accomplished by solving a least-squares problem.
Step 3, 5, 6 are nonlinear minimization problems. They
are done with the Levenberg-Marquardt algorithm. Step 4
can be solved using the quaternion representation of 3D
rotations. Fig. 4 shows the coarse mesh grid generated
with the reconstructed feature points.

φ (

x1 − x1 )

L

φ ( x1 − x N ) 

φ (


x N − x1 )

L

φ ( xN − x N

where A = 


C = (c1 , c2 ,

Lc

M

) , Y = ( y1 , y 2 ,
T

N

Ly

N



)

)T

−1

If A is positive definite, then U = A V .
The compactly supported radial basis function (RBFCS)
is discussed in [16][18][13][17]. It is widely used because
of the significant computation advantages and the good
properties they exhibit like the locality of their domain. A
classic structure of the RBF is as follows: [18]

Φ 3,0 = (1 − r ) 7 (5 + 35r + 105r 2 + 147r 3 + 101r 4
+ 35r 5 + 5r 6 ) ∈ C 6

Φ 3,1 = (1 − r ) 6 (6 + 36r + 82r 2 + 72r 3 + 30r 4

Figure 4 the coarse mesh

+ 5r 5 ) ∈ C 4

3.3. Data Interpolation

Φ 3, 2 = (1 − r ) 5 (8 + 40r + 48r 2 + 25r 3 + 5r 4 ) ∈ C 2

In section 3.2, the feature points are reconstructed in
3D space. But they are very sparse with respect to the
number of vertices on the wire-frame. We want to
interpolate more points according to these scattered feature
points [13][18].
Let us assume the sets of feature points are

Φ 3,3 = (1 − r ) 4 (16 + 29r + 20r 2 + 5r 3 ) ∈ C 0 (10)

X = { x1 , x 2 ,..., x N } ∈ R 3 . The problem is to define an
interpolation function F: R

F ( x) :=

∑
n

3

→ R 3 . Let’s define

C j ⋅φ(x − x j )

3

F :=

3

represents the weight associated with

and

∑F
t

k

.

(12)

k =1

φ : R 3 → R is positive
φ ( x) = φ ( x ) = φ ( r ) . We have

the j-th function. The function
definite on R

L

(6)

j =1

where C j ∈ R

In human face, the feature points are clustered in areas
such as eye and mouth, and they are sparse in other areas.
We subdivide the ensemble of feature points into a
number of subsets:
S 0 ⊂ S1 ⊂ ⊂ S t = S
(11)
Then use the RBFCS algorithm to interpolate at each of
these t levels. Finally, the interpolation function results:

In order to preserve the global surface smoothness,
high-order RBF are employed at the first iterations, while
low-order RBF are applied in the last iterations to keep
the global somatic details [18]. Fig.5 is the mesh grid
after data interpolation.

known the 3D coordinates of feature points. It can be
written as
(1 ≤ i ≤ N )
(7)
F ( xi ) = y i
The corresponding set Y = { y1 , y 2 ,..., y N } ∈ R
indicates the calibration feature points.
Then
 c1φ ( x1 − x1 ) + c 2 φ ( x1 − x 2 ) + + c N φ ( x1 − x N ) = y1

M



c φ ( x − x ) + c φ ( x − x ) +
N
1
2
N
2
 1

3

L

L+ c φ( x
N

N

− xN ) = yN

Fig. 5 The mesh grid after data interpolation

(8)
It can be denoted as

Ax ,φ C = Y

3.4. Face Texture
(9)
We utilize texture mapping to increase realism. In this
paper, we use the multi-resolution image mosaic
algorithm [15][9] to generate the texture with two

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

orthogonal pictures. The front and side images are
merged by using pyramid decomposition method with
Gaussian operator. It has two main operators, “REDUSE”
and “EXPAND”.
Let G0 be the original image.
Then
Gl = REDUCE(Gl −1 )
By which we mean

Gl (i, j ) =

∑∑ w(m, n) ⋅ G
5

5

m =1 n =1

l −1

(0<l<N)

(13)

( 2i + m,2 j + n) (14)

The pattern of weights w(m, n) used to generate each
pyramid level from its predecessor is called the
generating kernel. The weights are subject to the
constraints:
ˆ ( m) wˆ (n) ;
· w( m, n) = w

ˆ (0) = a ; wˆ ( −1) = wˆ (1) = b ; wˆ ( −2) = wˆ ( 2) = c ;
·w
· a + 2b + 2c = 1 ;
· a + 2c = 2b
(15)
EXPAND operation can be achieved by reversing the
REDUCE process. Let Gl ,k be the image obtained by
expanding Gl k times.
Then Gl , 0 = Gl , and for k>0,

Gl ,k = EXPAND[Gl ,k −1 ]
G l ,k (i, j ) = 4

∑ ∑G
2

2

m = 2 n = −2

l , k −1

(( 2i + m) 2 , ( 2 j + n) 2)

(16)
Here, only terms for which ( 2i + m) 2 and ( 2 j + n) 2
are integers contribute to the sum. Now define a sequence
of band-pass images L0 , L1 , …, LN . For 0<l<N

Ll = Gl − EXPAND[Gl +1 ] = Gl − Gl +1,1

Then we combine the Lk images on each level on the
defined edge. The results are shown in Figure 6.

(a) frontal image (b) side image (c) face texture
Figure 6 Face Texture

4. Experimental Results
We have used our system to construct face models.
Fig. 2 is the two frontal images with the relative head
movement. The user is instructed to locate the feature
points. Based on the FDP and the symmetry of human
face, we could find more correspondences, such as in Fig.
2. Fig. 4 is the coarse mesh grid. Fig.5 is the mesh grid
after data interpolation. Fig. 6(a) and (b) are the front and
side images by which we generate the face texture Fig.
6(c). In experiments, we used an ordinary camera to take
these pictures. Figure 7 shows comparisons between the
different views of reconstructed model and the real
images. Figure 8 is the animation of speaking. The model
has a nice shape in front and side views but it doesn’t
contain the hair, ear part and the back part. In our
algorithm, we add some 3D points into the model to
construct the neck. Because we use two frontal views to
reconstruct the model, the side structure of the model is
not complete, especially the back of the model. In
experiments, we have referenced the code of Geoface by
Keith Waters.

(17)

Figure 7 side by side comparison of the reconstructed model with the images

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

Figure 8 the model speaking

Conclusions
This paper proposes a whole scheme to reconstruct a
3D face model. The FDP and the symmetry of human
face are used as a prior knowledge to find the
correspondences. Experimental results prove the
effectiveness of our algorithm. It is very robust and has
no relation with the people’s age or gender. A user with
an ordinary camera can use our system to construct his
face model.
In spite of its robustness, there are still some areas for
improvement:
·The model does not contain the teeth, tongues and hair.
·The algorithm is semiautomatic. We are working at
automatic feature detection and correspondence.
·Facial expression synthesis.

References
[1] Yuen cheng Lee and Demetri Terzopoulos and Keith
Walters, “Realistic modeling for facial animation”,
SIGGRAPH95
[2] Emmanuel Garcia, Jean Luc Dugelay, “Low cost 3D face
acquisition and modeling”, International Conference on Coding
and Computing, 2001. Proceedings., 2-4 April 2001
[3] R.Lengagne, P. Fua, O.Monga, “3D stereo reconstruction of
human faces driven by differential constraints”, Image and
Vision Computing 18 (2000)
[4] Richard Lengagne, Jean Philippe Tarel, Olivier Monga,
“From 2D images to 3d face geometry”, Proceedings of IEEE
Second International Conference on Automatic Face and
Gesture Recognition
[5] A. Roy Chowdhury, R. Chellappa, S. Krishnamurthy, T.Vo,
“3D face reconstruction from video using a generic model”,
IEEE International Conference on Multimedia and Expo, 2002.
ICME '02. Volume: 1 , 26-29 Aug. 2002

[6] Fr´ed´eric Pighin, JamieHecker, Dani Lischinskiy, Richard
Szeliski, David H.Salesin, “Synthesizing Realistic Facial
Expressions from Photographs”, SIGGRAPH98
[7] Zicheng Liu, Zhengyou Zhang etc., “Rapid Modeling of
Animated Faces From Video”, Microsoft Technical Report,
2000
[8] Loris Ambrosini, Maurizio Costa, Fabio Lavagetto, Roberto
Pockaj, “3D head model calibration based on MPEG-4
Parameters”,
[9] Won-Sook LEE, NADIA MAGNENAT-THALMANN,
“Fast head modeling for animation”, Image and Vision
Computing 18 (2000) 355--364.
[10] Zhengyou Zhang, “A New Multistage Approach to Motion
and Structure Estimation by Gradually Enforcing Geometric
Constraints ”, ACCV'98
[11] Y.C.Lee, D.Terzopoulos, and K.Waters, “Constructing
physics-based facial models of individuals”, In proceedings of
Graphics Interface, Page 1-8, 1993
[12] Z. Zhang , “Flexible camera calibration by viewing a plane
from unknown orientations”, ICCV’99, pages 666-673, 1999
[13] R. Schaback, “Creating Surfaces from Scattered Data
Using Radial Basis Functions”, Mathematical Methods in
Computer Aided Geometric Design III, 1995
[14] H.Longuet-Higgins, “A computer algorithm for
reconstructing a scene from two projections”, Nature, 293:133135,1981
[15] P. J. Burt, E.H.Adelson, “A multiresolution spline with
application to image mosaics”, ACM Transaction on Graphics,
Vol. 2. No. 4, 1983
[16] O. Saligon, A. Le Mehaute, C. Roux, “Facial expression
simulation with RBF”, International Workshop on SNHC and
3D imaging, 1997
[17] Floater, M.S., and A. Iske, “Multistep Scattered Data
Interpolation using Compactly Supported Radial Basis
Functions”, Journal of Computational and Applied Mathematics
vol.73, no.5, pp. 65-78, 1996.
[18] F. Lavagetto, R. Pockaj, M. Costa, “Smooth surface
interpolation and texture adaptation for MPEG-4 compliant
calibration of 3D head models”, Image and vision computing,
1999

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

