A Novel Scalable Parallel Algorithm for Finding Optimal Paths Over
Heterogeneous Terrain
Carsten Maple and Jon Hitchcock
Department of Computing and Information Systems
University of Luton, UK.
jon.hitchcock@luton.ac.uk
carsten.maple@luton.ac.uk
Abstract
The area of path planning has received a great
deal of attention recently. Algorithms are required
that can deliver optimal paths for robots to take over
homogeneous or non-homogeneous terrain. Optimal
paths may be those that involve the shortest distance
travelled, the least number of turns or the least
number of ascents and descents. The often highly
complex nature of terrains and the necessity for realtime solutions have lead to a requirement for the
development of parallel algorithms. Such problems
have been notoriously difficult to parellelise
efficiently; indeed it has been said that an efficiency of
25-60% should be considered a success. In this paper
we present a parallel algorithm for finding optimal
paths
over
non-homogeneous
terrain
that
demonstrates superlinear speed-up.

1. Introduction
Recent trends show an increase in both the
deployment of autonomous robots and the number of
application areas in which such robots are utilised.
Current applications include exploration of active
volcanoes, planetary explorations and VLSI.
Furthermore, autonomous robots are becoming
increasingly used in operations in areas such deep-sea
mining and servicing tasks in toxic environments. The
increase in the use of these robots has lead to a great
deal of research effort being focussed on methods,
techniques and algorithms to ensure fast, robust, and
efficient deployment. One of the areas central to the
effective deployment of autonomous robots is that of
path planning. The robot path planning problem
requires the determination of an obstacle-free path for
a robot from a specified initial placement to a
specified final placement or goal. For a good survey of

methods for robot motion planning the reader is
referred to [10].
Algorithms for motion planning are centred on
minimising the cost of travelling from the starting
point to the goal. This cost of motion may depend
upon a number of parameters and constraints, such as
total distance travelled, elevation changes, forbidden
routes, velocity and acceleration. The motion planning
problem has extremely high complexity so that even
moderate problems can take a long time to compute
(see [1], for example) and though there exist faster
heuristic algorithms, these are not guaranteed to find a
solution even if one exists (see [6], for example).

1.1. Terminology
Many problems can be formulated as a search for
a sequence of moves that form a path from an “initial”
or “start” state (s) to a “goal” state (a member of a set
ī of goal states). If the search can start from one of a
number of start states, a single dummy state can be
created from which a move is allowed to any of the
start states [15]. If there are a number of goal states, a
similar dummy state could be created, but it is usual to
describe search algorithms as testing for a goal
condition rather than having a single goal state. Each
possible state can be considered as a vertex of a
directed graph, and each possible move can be
considered as an edge of the graph that joins the vertex
for the state before the move to the vertex for the state
resulting from the move. The graph is called a “state
space” [4, 5], the vertices are usually referred to as
“nodes”, the edges are called either “edges” or “arcs”,
and the problem becomes a “state-space search”. Each
directed edge joins a “parent” node to a “successor”
node [15]. The number of successors from a given
node is the “branching degree” (b) of the node. The
branching degrees are assumed to be finite, so that the
graph is “locally finite”. If two nodes are successors of

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

each other, the two directed edges can be replaced by
an undirected edge.

1.2. Terrain Approximation
Traditional ways of representing the surface of the
earth include pictorial views, hachures, contours
joining points of equal altitude, and spot heights at
specific locations. A more modern form is a Digital
Elevation Model (DEM), which records the elevation
of each location in a regular grid of points.
The data used in this implementation was
obtained from the United States Geological Survey
topographic data set. There are several programs
available that can read this data and provide a
graphical display of the terrain being modelled. For a
free
3D
package,
see
3DEM
at
www.visualizationsoftware.com/dem/downloads.html. It is
this package that is used to visualise the terrain in this
implementation. The format used in this
implementation was the XYZ format. This provides an
ASCII text representation of the x and y co-ordinates
of each data point, as integer values, and the height
value at that co-ordinate as a floating-point value. The
data was read to ascertain the minimum and maximum
x and y co-ordinate values and a regular array was
then constructed using this information. Minimum and
maximum height values were also extracted allowing
the values to be normalised such that height values
start at zero. This can then be converted into a
“boundary cubes” representation. Space is divided into
a three-dimensional array of cubes, and the boundary
cubes are those through which the surface passes, see
[13] for example.

2. Existing search algorithms
2.1. Branch-and-bound search
Branch-and-bound search originated in the 1960s
as a general way to solve problems in operational
research [8]. When applied to the search of a statespace graph, the general branch-and-bound algorithm
is:
1.
Create a list of open nodes, and initialise it
with the start node.
2.
[Selection] If the open list is empty, stop.
Otherwise, select a node on the open list to expand.
3.
[Branching] Generate the successors of the
selected node, and put them on the open list instead of
the selected node. In the original terminology of
branch-and-bound, the sub-problem of finding a path
that passes through the selected node is replaced by

the sub-problems of finding paths that pass through
the successors of the selected node.
4.
[Bounding] Prune the graph by eliminating
nodes from the open list that cannot lead to a desired
solution.
5.
Go to step 2.

2.2. A* search
If the costs are additive (as in a shortest-path
problem) and the edges have non-negative weights,
then the A* search algorithm can be used to find an
optimal solution. A* search is best-first branch-andbound search where the evaluation function f(n) used
in step 2 is the sum of the cost g(n) from the start to
the node plus an estimate h(n) of the cost from the
node to a goal. If this estimate never exceeds the true
cost (a so-called “admissible” function), then the first
path found to a goal node is an optimal solution. The
program can stop when this first solution is found, and
in step 4 the only nodes eliminated are those that are
duplicates of nodes previously generated.
Both functions f and h can be referred to as
heuristic functions. The g-value of a node generated in
step 3 can be easily computed by adding the weight of
the edge to the g-value of the parent node. For this
reason, the g-value of each node is stored in the data
structures.
When a duplicate of a node is detected in step 4,
its g- and f-values must be updated if the cost from the
start node is less than before. If the node was closed
and the g- and f-values are reduced, the node must be
reopened. The function h(n) is said to be “consistent”
or “monotone”, if it satisfies the triangle inequality
h(n)  w(n, n') + h(n') for every node n and every
successor n' of n, where w(n, n') is the weight of the
edge from n to n'. It can be shown that with a
consistent heuristic function, the first path found by
the A* algorithm to any duplicate node is an optimal
path, and so it is never necessary to reopen a closed
node in a sequential implementation. A consistent
heuristic function is also an admissible heuristic.

2.3. Parallel algorithms
In Cvetanovic and Nofsinger [3], the performance
of three parallel implementations of A* search are
compared using the Travelling Salesman problem with
up to 15 cities as an example. Using global lists of
open and closed nodes gives very little speedup; a
static distribution of nodes gives a speedup that never
exceeds 8; the best results are obtained with
continuous diffusion of open nodes from each
processor to four neighbouring processors. Mahapatra
and Dutt [12] state that parallel search with global

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

open and closed lists is only suitable for shared
memory machines; and that with local lists, load
balancing and pruning of duplicate nodes is important.
They propose a complicated scheme that combines
dynamic load balancing with a static partitioning of
the graph, where each node is allocated to a group of
processors. McKeown, Rayward-Smith, and Rush [11]
review branch-and-bound search, and discuss
detrimental, acceleration and efficiency anomalies.
They classify the methods of parallelisation of branchand-bound search as follows:
•
parallel expansion of the search tree with
different initial bound values
•
parallel search using different algorithms
•
parallel expansion of a single node
•
parallel evaluation of sub-problems (static
distribution of the search tree; dynamic distribution
with farming of the available work; dynamic
distribution with farming of large tasks; dynamic
distribution with a local priority scheme; randomised
algorithms).
Challou et al. [2] consider motion planning of
multi-jointed fixed-base robots for which it is
impractical to search the entire configuration space.
Instead they consider a randomised path planner,
which can be parallelised with no communication
overhead, and which stops when it finds a solution.
Such algorithms are efficient to parallelise but do not
attempt to find an optimal path. Ibrahim and Xinda [7]
describe the use of a dynamic load-balancing
algorithm with depth-first tree-search. They use the 8puzzle as an example, however the elapsed times in
the results are too small to draw useful conclusions.
The most relevant piece of recent work is that of
Lanthier, Nussbaum, and Sack [9]. They consider
finding shortest paths across a terrain. The terrain is
represented as a triangulated irregular network (TIN)
and they use a grid to partition the terrain statically
among the processors. Dijkstra’s algorithm is used to
solve single-pair and single-source problems, and their
results show that the efficiency reduces as the number
of processors increases.

nodes. The total time taken for operations on the
closed list is reduced from O(Ncf2) to O(Ncf), where Nc
is the number of closed nodes at the end of the search,
are this is the dominant part of the algorithm. We
further use a heap for the open list priority queue and
this can reduce the time of computation by a factor of
ten, because less time is needed for operations on the
open list. Contrary to some implementations of the A*
algorithm, we allow generation of the parent of the
node being expanded; it will be detected as a duplicate
node and the effect on computation time is marginal.
The implementation utilises the Manhattan or
city-block heuristic, |xn í xg| + |yn í yg| + |zn í zg|,
where the current position is given by (xn, yn, zn) and
the position of the goal is given by (xg, yg, zg). This
heuristic has been observed to give a significant
improvement over other heuristics such as the
Euclidean distance heuristic.
The parellelisation of the algorithm involves a
static partitioning of the terrain. The size of the tiles
that are used is uniform and constant. A static
partitioning is used as this reduces the overhead when
compared to a dynamic reconfiguration of the
partition. Each processor is assigned a set of tiles a
priori. The tiles are assigned in a sequential manner
as shown in Figure 2; no processor is assigned two
adjacent tiles.

5. Results
The algorithm described in the previous section
has been executed on a Beowulf Cluster. The cluster
consists of 13 computers, each with a 350 MHz AMD
K6-2 processor and 128 Mbytes of memory,
connected by a 10 Mb/s Ethernet hub. They run SuSE
8.1 Professional Linux, with the gcc 3.2 C++
compiler, and LAM 6.5.6 / MPI 2.
The binary file used in the tests has a file size of
15,725,270 bytes. The nodes (boundary cubes) can be
classified by their branching degree (the number of
adjacent cubes):

4. The proposed algorithm
A program has been written that converts the
USGS DEM data into a boundary cubes
representation. We then apply a parallel A* algorithm
to find the optimal paths. We utilise an indexed array
for the unopened/open/closed nodes. Experiments
show, see [14], that the application of indexed arrays
on a large problem can reduce computation time from
18 hours to 6 minutes, due to the fact that the time
taken for each operation on the closed list is reduced
from O(Nc) to O(1), where Nc is the number of closed

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

Branching degree

Total

Number of nodes

0

0

1

0

2

17

3

132920

4

406485

5

96726

6

18832
654980

Figure 1: Sequential search by a single
process

Figure 2: 13 processes solving the same
problem (tile size = 50 × 50)

Green – the start point
Red – an optimal solution path

A snapshot taken when process 10 had found a path
to the goal but the other processes were still
evaluating nodes to see if they lead to a better
solution

Bright yellow – frontier of “open” nodes being
evaluated when the solution was found

Red – paths traced back from the latest node that
has been expanded by each process

Blue – the goal

Pale yellow – “closed” nodes that have been
considered and eliminated from the search

All the timings in this section are for a search for
an optimal path from cube number 579814 (x,y,z =
420,64,45) to cube number 132952 (x,y,z =
70,314,67). In terms of UTM co-ordinates, this is from
point 654000E 4289010N to point 661500E
4278510N. The paths found each have a cost of 756
steps.

5.1. The effect of tile size
The effect of tile size is significant, as is shown in
Table 1. For a parallel programme, the total
computation time, T, can be represented by
T = tcomp + tcomm + toverhead.

That is, the total time for computation is the sum
of the time for computation, the time for
communication and any system overhead, such as
process generation and input-output. The total
computation time is the sum of essential and excessive
computation; often in parallel computations excessive,
or redundant, computation is undertaken. This may be
minimised by reducing the granularity of the problem;
that is, assigning smaller units of independent
computation to distinct processors. However, a likely
consequence is an increase in the amount of
communication overhead. If the tile size is too small,
the result is massive communication overhead due to
the fine granularity of the partition. This
communication then becomes an increasing factor in
total computation and can often exceed Ts, where Ts is
the total time taken on a single processor; that is the

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

Table 1. The effect of tile size on performance time and operation of the A* algorithm
Elapsed
Expansion
time
steps
(secs)

Tile size

Duplicate
nodes

Better
paths

Nodes sent

Nodes
reopened

Nodes
Comparisons
generated
made

(Sequential)

67.18

291456

873122

101140

..

0

295451

5124397

1

770.56

4706089

24952234

12361683

7357806

10897903

1258385

53814499

2

746.75

4392677

20258608

8459201

3595765

7153307

940290

56605711

5

74.44

2235543

8989687

2782379

740763

2314889

707881

26069364

10

44.94

2327326

8970909

2397069

389916

1991629

732695

23119847

20

5.67

789857

2806320

643253

72151

437695

432826

10030224

50

4.44

481140

1544229

243079

17464

102911

402629

6375021

100

8.85

544823

1801642

331037

9862

174266

392062

7605737

200

23.87

500736

1643585

297989

4450

143089

367401

7527345

Table 2. The performance of the parallel algorithm as the number of processors increases
Elapsed
Expansion Duplicate
time Speedup
steps
nodes
(secs)

Processors

Better
paths

Nodes
sent

Nodes
Nodes Comparisons
reopened generated
made

1

67.18

..

291456

873122

101140

..

0

295451

5124397

2

39.65

1.69

512828

1678974

277211

19343

143858

395862

7946361

3

20.32

3.31

430904

1364891

197250

16024

73671

378511

6462437

4

10.66

6.30

334383

1042229

149663

10918

40585

309749

4981536

5

8.47

7.93

349713

1091298

155489

11162

45365

322306

5055869

6

9.45

7.11

530592

1712861

265565

20595

122230

434557

7210876

7

6.42

10.46

428233

1353710

199117

14917

70815

378140

5869437

8

5.65

11.89

426010

1363429

213512

15301

86963

360003

5833448

9

5.27

12.75

444979

1438249

236975

15835

104654

362025

5941442

10

6.15

10.92

507818

1650409

269452

18441

128617

404186

6752417

11

4.74

14.17

506192

1618475

241219

18997

101360

429979

6532009

12

3.77

17.82

403033

1275178

190696

14628

66831

355675

5178930

13

4.44

15.13

481140

1544229

243079

17464

102911

402629

6375021

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

problem takes longer to execute on a parallel system
than on a single processor machine. Increasing the
granularity of the problem excessively (in this case
making the tile size excessively large) can result in an
increase in the amount of redundant computation;
furthermore, any synchronisation points can result in
an increase in the total time processors are idle. The
optimum balance for this problem appears to be a tile
size of 50 x 50, however the algorithm works
sufficiently well that a tile size of between 20 x 20 and
100 x 100 gives an efficiency of over 60%. It has been
noted in research papers that any efficiency factor of
between 25-60% should be considered a success, and a
200 x 200 tile size still delivers such success.

5.2. Parallel performance
The algorithm performs well and provides best
efficiency when using a 50 x 50 tile size. When such a
size is used, the performance exceeds linear speed-up
over 13 processors. Such factors of speed-up are often
due to memory caching effects, but in a stochastic
search it is possible that the effects could be due to a
reduced search space. In this case it is more likely that
caching effects are being witnessed.
When more than one processor is used, more
nodes are expanded and more duplicate nodes are
detected. The actual numbers vary with the number of
processors in a stochastic manner, and they have little
effect on the elapsed time. The speed-up is mainly
correlated (inversely) with the number of nodes that
are sent from one process to another. The full table of
results is presented in Table 2.

6. Conclusions
In this paper we have discussed a parallel search
algorithm for finding shortest paths; the algorithm
utilises an improved A* using a Manhattan heuristic.
The algorithm provides super-linear results; the
efficiency of the parallel algorithm exceeds 1 in all
cases other than for two processors (in which it is a
respectable 0.85). The efficiency, while not at its peak
for 13 processors, is certainly not dropping rapidly,
and so this algorithm appears to have reasonable
scalability.
Further ongoing work includes automatic
selection of tile size (using basic metrics of the graph
being searched). The algorithm is being further tested
on a large suite of problems and preliminary results
are encouraging.

7. References
[1] J.F. Canny, The Complexity of Robot Motion Planning,
MIT Press, 1998.
[2] D. Challou et al., “Parallel search algorithms for robot
motion planning,” Practical Motion Planning in Robotics:
Current Approaches and Future Directions, K. Gupta and A.
del Pobil, eds., John Wiley, 1998, pp. 115–131.
[3] Z. Cvetanovic and C. Nofsinger. “Parallel Astar search
on message-passing architectures,” Proc. 23rd Hawaii Int’l
Conf. System Sciences (HICSS), vol. 1, 1990, pp. 82–90.
[4] A. Grama et al., Introduction to Parallel Computing, 2nd
ed., Pearson Education, 2003.
[5] A. Grama and V. Kumar, “State of the Art in Parallel
Search Techniques for Discrete Optimization Problems,”
IEEE Trans. Knowledge and Data Engineering, vol. 11, no.
1, 1999, pp. 28–35.
[6] Y.K. Hwang and N. Ahuja, “Potential Field Approach to
Path Planning,” IEEE Trans. Robotics and Automation, vol.
8, no. 1, 1992, pp. 23–32.
[7] M.A.M. Ibrahim and L. Xinda, “Performance of dynamic
load balancing algorithm on cluster of workstations and
PCs,” Proc. 5th Int’l Conf. Algorithms and Architectures for
Parallel Processing (ICA3PP), 2002, pp. 44–47.
[8] V. Kumar and L.N. Kanal, “A general branch and bound
formulation for understanding and synthesizing And/Or tree
search procedures,” Artificial Intelligence, vol. 21, Elsevier,
1983, pp. 179–198.
[9] M. Lanthier, D. Nussbaum, and J.-R. Sack, “Parallel
implementation of geometric shortest path algorithms,”
Parallel Computing, vol. 29, Elsevier, 2003, pp. 1445–1479.
[10] J.C. Latombe, Robot Motion Planning, Kluwer
Academic, 1991.
[11] G.P. McKeown, V.J. Rayward-Smith, and S.A. Rush,
“Parallel branch-and-bound,” Advances in Parallel
Algorithms, L. Kronsjö and D. Shumsheruddin, eds.,
Blackwell Scientific, 1992, pp. 111–150.
[12] N.R. Mahapatra and S. Dutt, “Scalable glocal and local
hashing strategies for duplicate pruning in parallel a* graph
search,” IEEE Trans. Parallel and Distributed Systems, vol.
8, no. 7, 1997, pp. 738–756.
[13] C. Maple, “Using the Boundary Cubes algorithm for
terrain modelling and robot motion planning,” Proc. 9th Int’l
Conf. Methods and Models in Automation and Robotics
(MMAR), 2003.
[14] C. Maple and J. Hitchcock, “Algorithms for finding
optimal paths over heterogeneous terrain,” submitted to
Proc. 11th Int’l Conf. Methods and Models in Automation
and Robotics (MMAR).
[15] J. Pearl, Heuristics: Intelligent Search Strategies for
Computer Problem Solving, Addison-Wesley, 1984.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

