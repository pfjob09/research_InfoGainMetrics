User Evaluations of Interactive Multimodal Data Presentation
Veridiana Salvador
Institute of Mathematics and Computer Science
University of S˜ao Paulo, Brazil
veridiana.salvador@comcast.net

Rosane Minghim
Institute of Mathematics and Computer Science
University of S˜ao Paulo, Brazil
rminghim@icmc.usp.br

Haim Levkowitz
Institute of Mathematics and Computer Science
University of S˜ao Paulo, Brazil and
University of Massachussetts Lowell, USA
haim@cs.uml.edu

Abstract
A set of user evaluation studies were performed to test
hypotheses about the efﬁcacy and value of the use of soniﬁcation by itself and together with visualization to improve
the process of interactive data exploration. Our results
show that, in some cases, soniﬁcation yielded more accurate analysis results than visualization, and when combined
with visualization, yield much better results than either one
(visualization or soniﬁcation) yielded separately. That supports soniﬁcation as a valuable and promising tool that
should be further studied as a complement to visualization
systems.

1

Introduction

New data analysis techniques should be objectively evaluated using real subjects, performing both real tasks on real
data and synthesized data that represented well the characteristics of real-problem data, to verify their value and efﬁcacy, i.e., how well users can correctly and effectively analyze the data and come up with accurate conclusions from
it. This is particularly so for approaches that are still in early
stages of application, such as soniﬁcation, that is, the use of
sound to convey useful information.
In this work we have performed user evaluations to study
the efﬁcacy and value of soniﬁcation by itself and together
with visualization to improve the process of information
analysis. Our results identify cases when sound alone yields
even more accurate analysis results than visualization, and
cases when, combined with visualization, sound can yield
more correct results than either one (visualization or soniﬁcation) yielded separately.

In this paper, after a quick survey of previous work, we
provide a detailed description of our experiments’ design
and implementation, including details about data sets used,
subjects’ selection, and the tasks they were asked to perform. Then, we show our results, analyze and discuss them,
and ﬁnally, present the conclusions we have reached from
carrying out these studies.

2

Previous work

The soniﬁcation ﬁeld has reached a point where a considerable body of work has been built. From that work,
many tools are available to support data interpretation using
sounds and many guidelines have been put forward. One
of the most crucial factors affecting the potential success of
a soniﬁcation task is the sound mapping(s) employed. In
most cases, detailed tests should be carried out to validate
those choices.
Much of the recent soniﬁcation work has been accompanied by user evaluations to some extent, showing a trend
towards validating previous efforts to bring sound into the
data understanding scenario. Some of those evaluations are
quite speciﬁc to a particular application, such as audio ﬁle
browsing [2] and hypertext depth [4], offering general suggestions pertinent to the design of sound mappings. Other
user evaluations aim more directly at verifying user capabilities to interpret data-to-sound mappings, such as precision
in value estimation [15], scaling and polarity in magnitude
estimation through sound [13], the inﬂuence of cognitive
abilities and demographics on interpretation of dimensions
mapped to sound (such as magnitude) [16], and the inﬂuence of user preferences in performance [14]. Further work
on testing various properties of sound perception, such as
orientation and multiple sound stream interpretation for the

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

purpose of data interpretation, can be found in the literature.
Most of the above mentioned work has dealt with musical
listening, although there have also been validations of the
use of everyday sounds through user evaluation [11] [3].
A summary of the ﬁndings provided by user evaluations of
soniﬁcation, which is out of the scope of this text, can be
found in Minghim and Levkowitz [7]. Very few of the user
evaluations found in the literature have been run with simultaneous visual and aural stimuli, and even fewer from the
point of view of visualization applications. Adding sound
into an exploration task of a visualization tool brings new
dimensions to the evaluation of soniﬁcation for data interpretation.
In the work described here, we have added some perspective into mappings of sound pitch as a property for use
with interactive visualizations by asking subjects to perform
tasks involving simultaneous visual and aural interaction
with data.
In order to support our experiments’ set up, we followed
the work of Levkowitz, Pickett, Smith and Torpey [5],
which studied different schemas of sound representation.
Their work examines the nature of the data and the sound
tasks used. One can conduct studies using two sources of
data: (1) real data sets that represent real life problems, but
in which the investigator may not know exactly “the truth”;
(2) simulated data, which allow the investigator to specify
and control data features precisely, but such data might not
have any resemblance to real life data. The authors emphasize that an exhaustive suite of experiments can beneﬁt from
the use of both types. Three main tasks were discussed: detection, recognition, and quantiﬁcation. In a detection task,
the only requirement is to be able to detect the presence (or
announce the absence) of certain structures. This task allows us to evaluate the ability to perceive different patterns,
but not what their differences are. In a recognition task,
the subjects have to discriminate the differences among detected patterns. In a quantiﬁcation task, the user has to provide some quantitative measurements of patterns.
In a work by Bonebright and others [1] a methodology is
presented to evaluate perceptual properties of sound stimuli, which provides analysis techniques to ensure the effective use of sound in a variety of applications, including data
soniﬁcation and virtual reality. It offers general guidelines
on subject selection and sample size (representative of the
population to which the test is applied), and the number and
type of perceptual variables (for instance: pitch, intensity,
etc.). The authors further present three basic methods to determine the perceptual quality of simple sound events: an
identiﬁcation task, which questions the image that comes to
mind when a sound is heard; context-based classiﬁcation,
which provides a way to measure the perceptual veracity of
the sound within a certain context; and attribute classiﬁcation, which provides information on the perceptual qualities

important to a sound stimulus.
Mostly based on the guidelines of these two papers
([5],[1]), we have deﬁned the datasets employed, the tasks
the subjects were submitted to, and the degree of difﬁculty
entailed in the evaluations. Additionally, Bonebright et al.’s
work helped determine the time spent during an experiment,
the choice of the number of subjects for each experiment,
and the tasks to be performed. The experiments are described in the next section.

3

Experiments deﬁnition and implementation

The goals of our experiments were to evaluate the use
of sound (particularly pitch) and visual stimuli (particularly
color) together as a way to represent data. Users interacted
with projections of generic data onto surfaces using a 2D
Spider Cursor interaction tool, one of our system DSVol’s
interaction tools [9]. Figure 1 shows one screen shot of the
tool. The Spider Cursor can be moved on a surface of the
projected data and, for each central point, the cursor’s edges
show its neighbouring points. The user is presented with a
sound representing a value associated with the central point,
one sound at a time.

(a)

Figure 1. Spider cursor over a triangulation
representing a data set

With these experiments, we tried to (1) validate polarity of sound mappings, Direct and Inverse; (2) evaluate the
use of sound to support visualization tasks; and (3) evaluate three presentation modes: sound, visual, and joint visual
and auditory. Our mapping choices were made due to their
usage prevalence, not as a statement of their appropriateness
or promise.

3.1

Data sets deﬁnition

Two data sets were used: Quality of Life (QoL) and Rain
Volume (RV).

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

The Quality of Life data set provides the Human Development Index (HDI) of 174 countries listed by the United
Nations Development Program (UNDP). HDI tries to measure three dimensions of human development: a long and
healthy life, knowledge, and a decent standard of living.
For the visualization of QoL, countries were mapped to
points on a plane using multi-dimensional projection techniques and connected using a Delaunay Triangulation [12].
This visualization allows the identiﬁcation of countries and
their HDI features. Neighboring points do not represent geographical proximity, but point similarity based on HDI parameters. In one set of experiments, countries were plotted
on a plane, and sound was used to represent the HDI classiﬁcation. In another set, altitude (Z axis) and color represented the HDI overall classiﬁcation for each country.
The Rain Volume data set provides simulated information about altitude and Rain Volume at 164 points of a surface. Again, it was visualized with projection followed by
Delaunay triangulation. In the sound experiments, the altitude of a point was presented by the Z axis value, and the
Rain Volume by sound. In the color experiments, the altitude was presented by the Z axis value, and the Rain Volume
by color. In all experiments, the altitude of a point was also
displayed textually.
Each data set was used to validate one of the two sound
mappings used in DSVol: Direct and Inverse. Direct Sound
Mapping was used for the QoL data set because it is composed of quantitative values. Therefore, high QoL was
mapped to high frequency, and low QoL mapped to low frequency. Inverse Sound Mapping was used for Rain Volume,
which represents density information. Therefore, the higher
the RV, the lower was the frequency it was mapped to. The
correct choice between direct and inverse sound mapping
(or polarity) has not been entirely ascertained yet, and some
of the research on the subject shows that in various cases
where the value represents certain concepts such as size and
density the association of polarity is not obvious and even
user preferences may have an effect on performance [13].
The mapping choices for our work presented here had been
made based on previous experience and general user expectations [6] [10]. These experiments were meant to help validate these choices.

3.2

Subjects selection and task deﬁnition

Subjects were 84 Computer Science undergraduate students at ICMC (The Institute for Mathematical and Computer Sciences) at the University of S˜ao Paulo, Brazil. They
were divided into three groups, such that one group was assigned only soniﬁcation tasks, another one handled mixed
visualization-soniﬁcation tasks and the thirs group had only
visualization tasks. Table 1 summarizes the sequences of
experiments.

3.3

The experiments

The experiments were comprised of a 15-minutes-long
training session, in which they became familiar with sound
mappings to pitch and the spider cursor, separately. This
was followed by 5- and 10-minutes-long test sessions (see
Table 1). One of the authors explained the tasks and supervised their execution. In addition, instructions were presented on the screen. Subjects used earphones to listen to
sound stimuli, and were not allowed to communicate with
each other.
Subjects were allowed to change the instrument used in
the sound mapping. After completing all tasks, subjects
ﬁlled out a questionnaire about their previous experience
with visualization, music, and sound, and about the experiment.

4

Results

In the following we present a summary of the results obtained for each task. These were obtained using a program
implemented especially for this purpose. Raw results and
more details are described in [8].

4.1

Range identiﬁcation

QoL Task: identify the QoL of four countries A, B, C, D
as one of low, medium, high. Range separation was clear in
the data. Countries A, B, and C were well within the ranges
high, low and, medium, respectively. Country D was in the
upper low range, close to medium. RV tasks were similar to
those for the QoL, the only difference being the use of an Inverse Sound Mapping for the latter. During sound stimulus
experiments QoL was presented by sound, and during visual experiments it was presented by a rainbow color scale,
with high QoL presented as blue and low QoL presented as
red.
Percentage correct answers and average times are presented in Table 2.
These results show that, even though the average time for
a sound task was higher than for a visual one, the percentages of correct answers for both presentations were similar
when values were well deﬁned within each range. However,
when values were close to range boundaries (as in D), visual recognition was far better than sound recognition. Differences between Direct and Inverse sound mappings were
insufﬁcient to lead to any conclusions.

4.2

Sorting

Task: order ﬁve points A, B, C, D, E of the data sets.
Subjects in Groups 1 and 3 sorted the QoL of ﬁve countries
(lowest to highest). Subjects in Group 2 sorted the RV in

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

ﬁve regions (lowest to highest). For all groups, the correct
answer was C - E - A - D - B. Points B and D were deliberately set to be close to each other, allowing their comparison
without irrelevant sound stimulus between them. Points A
and E were set far apart from each other, forcing the presence of irrelevant sound stimuli between them. This was
done to test if irrelevant sound stimuli, which could be considered noise, would affect the sorting, possibly by inﬂuencing the memory of the subject.
Percentage correct answer and average times are presented in Table 3.
The results show that, even though the mean time for a
sound task was higher than for a visual task, sorting using
sound (both Direct and Inverse mappings) yielded a higher
percentage of correct answers than using color. Therefore,
on average, it seems that human beings have better perception of small value differences when comparing two values
when they are presented by sound. This perception advantage decreases when irrelevant sound stimuli are presented
between the two compared sound samples. The same does
not occur in a visual presentation because users can compare all value ranges simultaneously.

4.3

Quantiﬁcation

Task (performed by Groups 1 and 3 only): identify the
QoL in three countries: Australia, India and Guatemala. As
we had expected, exact values were difﬁcult to identify, and
answers close to the correct answers were rare. That is, it is
difﬁcult to attain precision using sound, only — perhaps —
approximation.
The percentage of correct answers, average for each
country, and average time taken to perform this task are presented in Table 4.
The results support our hypothesis that neither sound nor
color is a good representation for precise identiﬁcation of
exact values. The results of the visual presentation show
a constant standard deviation for all three countries. This
could be due to the visualization of the entire color range,
which might have helped subjects to approximate unknown
values. Using a sound presentation, subjects could only
hear one sound at a time.
These results also support the hypothesis that a sound
stimulus is good for identifying small changes in values.
On average, subjects in Group 1 (sound) correctly recognized the QoL in Guatemala to be better than that in India.
Subjects in Group 3 (color) were not able to detect this relationship.

4.4

Presentation mode — search task

The search task was developed to evaluate the effectiveness of three presentation modes: visual, sound, and multi-

Task (minutes)

Range Ident. (5)
Sorting (10)
Quantiﬁcation (10)
Search (10)

Group 1
(28 subjects)
(sound)
QoL
QoL
QoL
RV

Group 2
(27 subjects)
RV (sound)
RV (sound)
—–
RV
(sound + vis)

Group 3
(29 subjects)
(vis)
QoL
QoL
QoL
RV

Table 1. Groups and tasks
modal (visual and sound). Subjects were asked to search, on
a map, for the points with the lowest and the highest RV, respectively, and to report the altitudes of the regions (values
were displayed).
To analyze the results, we considered the three altitudes
with the highest values of RV and the three with the lowest
values. The percentage of correct answers and the mean
time are presented in Table 5.
The results show that visual-only tasks yielded the lower
mean time but also the lowest percentage of total correct
answers. The sound mode yielded a slightly higher total
correct percentage. Again, the sound presentation results
support the hypothesis that small changes in values are perceived better when presented using sound. However, multimodal (visual and sound) presentations yielded the best performance, presenting more than 43% total correct answers.
Using multimodal presentations, soniﬁcation can help conﬁrm results or eliminate doubts when used together with a
visual presentation

4.5

Questionnaire

After the conclusion of all required tasks, subjects ﬁlled
out a questionnaire about their visualization experience
None, Some, A lot, musical experience Just listen to music, Play instruments but no formal music education, Have
formal music education, problems they encountered during
the experiments, and their feelings about the use of sound
to represent information. The Tables 6 to 9 summarize subjects’ answers to the questionnaire.
Most subjects reported very little prior experience using
visualization, and more than half reported little previous
musical experience. Most subjects who had experienced
hardships performing the tasks (77%) also had little or no
musical experience. Most subjects (83.3%) felt conﬁdent
using sound as a way to represent data, and all found the
sound presentation useful.
Musical experience showed to be an important factor for
sound stimulus interpretation. In general, subjects with musical experience presented higher percentage of correct answers for all tasks, compared to those with little or no musical experience. Tables 10 to 13 show the analysis of the
results as related to musical experience.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

Country
A
B
C
D
Mean time
(min)

Sound:
Direct Mapping
84.6%
88.4%
88.4%
46.1%
4:42

Sound:
Inverse Mapping
86.3%
72.7%
95.4%
36.3%
3:57

Visual
83.8%
90.3%
93.5%
77.4%
2:49

Table 2. Range identiﬁcation task results

All
B-D
A-E
Mean time

Sounds:
Direct Mapping
65.3%
84.6%
76.9%
6:15

Sounds:
Inverse Mapping
68.1%
81.8%
90.9%
5:40

Visual
44.8%
86.2%
75.8%
2:36

Table 3. Sorting task results. B-D did not
present irrelevant sounds in-between. A-E
presented irrelevant sounds

Search

Visual

Conclusions

Highest Rain Volume
1st altitude
35.4% 38.4%
2nd altitude
0%
15.3%
3rd altitude
0%
0%
TOTAL
35.4% 53.8%
Lowest Rain Volume
Percentage
1st altitude
61.2% 26.9%
correct:
2nd altitude
6.4%
11.5%
3rd altitude
0%
0%
TOTAL
67.7% 38.4%
Percentage correct:
20%
23%
Mean time (min)
2:18
5:21

Australia:
Exact value:
102
India:
Exact value:
31
Guatemala:
Exact value:
33

Quantiﬁcation
Percentage correct answers
Mean value
Standard deviation
Percentage correct answers
Mean value
Standard deviation
Percentage correct answers
Mean value
Standard deviation
Mean time (min)

Sound
0%
95.5
5.42
0%
39.7
15.67
0%
41.74
16.5
3:56

Visual
0%
95.65
14.8
0%
41.68
14.1
0%
34.39
14.1
3:11

Table 4. Quantiﬁcation task results

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

51.3%
5.4%
0%
56.7%
54%
8.1%
0%
62.1%
43.2%
4:15

Table 5. Search task results
Visualization
experience

None
52

Some
44

A lot
4

Table 6. Subjects’ visualization experience

Musical
experience

We have described a series of experiments conducted using the 2D Spider Cursor interaction tool, to evaluate the
value and efﬁcacy of soniﬁcation to support data investigation tasks in interactive visualization.
The results of these experiments demonstrate the potential efﬁcacy of Direct Sound Mapping for qualitative values
and of Inverse Sound Mapping for density and volume features. They also support the hypothesis that soniﬁcation has
a value as a supplementary, support technique, when used
together with visualization.
The results of range identiﬁcation tasks conﬁrm that
sound presentation is as efﬁcient as visual presentation, under certain circumstances. Another result was the better
performance using sound presentation for sorting tasks, as
compared to the results using visual presentation for the
same tasks. On average, human beings perform better when
detecting differences between close values if they are presented by sound than by colors.

Visual
+ Sound

Percentage
correct:

Listen

5

Sound

Play
self-taught
7

65

Play
classes taken
28

Table 7. Subjects’ musical experience
Low
37

% Difﬁculty

Medium
59

High
4

Table 8. Subjects’ difﬁculty with experiments
Sound to represent
information
% yes

Feel
conﬁdence
83.3

Find
useful
100

Table 9. Subjects’ soniﬁcation conﬁdence
Country
Musical
experience
Range
identiﬁcation
% correct

A

B

C

D

Y

N

Y

N

Y

N

Y

N

90

80

90

65

95

80

25

41

Table 10. Range Id. v. Musical Experience
Sorting
Musical
experience
%

Correct answers
Y
N

Y

B-D
N

Y

N

80

85

75

95

65

45

A-E

Table 11. Sorting v. Musical Experience
Quantiﬁcation
Musical
experience
% close

Australia
Y
N

India
Y
N

Guatemala
Y
N

93

37

40

95

35

38

Table 12. Quant. v. Musical Experience

Musical
experience
% correct

Sound only
Y
N

Multi modal
Y
N

38

65

18

20

Table 13. Musical Experience Effect
As we expected neither sound nor color mapping was
proven good for quantiﬁcation tasks.
The results obtained here show that soniﬁcation is more
useful when used to support visualization tasks, but generally musical experience is an important issue for detection,
understanding, and evaluation of sound stimulus based on
musical listening.
These results encourage the use and evaluation of soniﬁcation together with visualization.
Further studies are to be carried out testing additional
capabilities of the spider cursor tool and of other tools.

Acknowledgments
This work has been funded by FAPESP, the S˜ao Paulo
(Brazil) Research Funding Agency, S˜ao Paulo, Brazil proc.
no. 04/12202-8, 98/13879-9 and 04/01756-2. Part of this
work was done while Haim Levkowitz was a U.S. Scholar
of the Fulbright Scholar Program in Brazil; he is grateful
for its support.

References
[1] T. L. Bonebright, N. E. Miner, T. E. Goldsmith, and
T. P. Caudell. Data collection and analysis techniques for evaluating the perceptual qualities of auditory stimuli. In Proc. ICAD’98, Glasgow, UK,
November 1998. http://www.dcs.gla.ac.uk/icad98/.
[2] E. Brazil and M. Fernstr¨om. Where’s that sound? exploring arbitrary user classiﬁcations of sounds for audio management. In Proc. ICAD’03, Boston, USA,
July 2003. http://www.icad.org/icad2003.
[3] T. Hermann, C. Niehus, and H. Ritter. Interactive visualization and soﬁnication for monitoring complex processes. In Proc. ICAD’03, Boston, USA, July 2003.
http://www.icad.org/icad2003.
[4] M.-S. K. J.-H. Lee, M.-H. Jeon. Auditory displays
on the depth of hypertext. In Proc. ICAD’03, Boston,
USA, July 2003. http://www.icad.org/icad2003.
[5] H. Levkowitz, R. M. Pickett, S. Smith, and M. Torpey. An environment and studies for exploring auditory representations of multidimensional data. In
G. Grinstein and H. Levkowitz, editors, Perceptual Issues in Visualization. Springer-Verlag, 1995.

[6] R. Minghim and A. R. Forrest. An illustrated analysis
of soniﬁcation for scientiﬁc visualisation. In Proc. of
IEEE Visualization’95, Atlanta, USA, October 2004.
IEEE CS Press.
[7] R. Minghim and H. Levkowitz. Soniﬁcation for support to visual data interpretation - a survey. in preparation, 2005.
[8] V. C. L. Salvador. Development of a Model for Exploration in Distributed Scientiﬁc Visualization using Sound (in Portuguese). PhD thesis, USP - Inst.
Math.Comput., S˜ao Carlos - SP, Brazil, August 2003.
[9] V. C. L. Salvador and R. Minghim. An interaction
model for scientiﬁc visualization using sound. In Proc.
Sibgrapi’03, S˜ao Carlos - SP, Brazil, 2003. IEEE CS
Press.
[10] V. C. L. Salvador, R. Minghim, and M. L. Pacheco.
Soniﬁcation to support visualization tasks. In Proc.
Sibgrapi’98), S˜ao Carlos - SP, Brazil, 1998. IEEE CS
Press.
[11] C. Stevens, D. Brennan, and S. Parker. Simultaneous manipulation of parameters of auditory icons to
convey direction, size, and distance: Effects on recognition and interpretation. In Proc. ICAD’04, Sydney,
Australia, July 2004. http://www.icad.org/icad2004.
[12] E. Tejada, R. Minghim, and L. G. Nonato. On improved projection techniques to support visual exploration of multi-dimensional data sets. Information Visualization J., 2(4):218–231, 2003.
[13] B. N. Walker. Magnitude estimation of conceptual
data dimensions for use in soniﬁcation. J. Exper. Psychol.: Applied, 8(4):211–221, December 2002.
[14] B. N. Walker, G. Kramer, and D. M. Lane. Psychophysical scaling of soniﬁcation mappings. In
Proc. ICAD 2000, Atlanta, Georgia, USA, July 2000.
http://www.icad.org/icad2000.
[15] B. N. Walker, J. Lindsay, and J. Godfrey. The audio
abacus: Representing a wide range of values with accuracy and precision. In Proc. ICAD’04, Sydney, Australia, July 2004. http://www.icad.org/icad2004.
[16] B. N. Walker and L. Mauney. Individual differences,
cognitive abilities and the interpretation of auditory
graphs. In Proc. ICAD’04, Sydney, Australia, July
2004. http://www.icad.org/icad2004.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

