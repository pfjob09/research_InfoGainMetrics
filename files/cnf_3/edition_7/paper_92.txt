Smart and Physically-Based Navigation in 3D Geovirtual Environments
Henrik Buchholz, Johannes Bohnet, Jürgen Döllner
University of Potsdam, Hasso-Plattner-Institute, Helmert-Straße 2-3, Potsdam, Germany
{buchholz, bohnet, doellner}@hpi.uni-potsdam.de

Abstract
This paper describes an approach for smart and
physically-based navigation, which aims at supporting
effective and intuitive user interactions with 3D
geovirtual environments (GeoVEs). The approach is
based on two aligned concepts: 1) All navigation
techniques are controlled by constraints that ensure user
orientation and avoid “getting lost” situations. 2) All
navigation techniques are handled in a time-coherent way
achieving steady, continuous user movements using a
physically-based motion model. Based on these concepts,
we demonstrate several ways to improve commonly used
navigation techniques for geovirtual environments.
Keywords: 3D Navigation, Physical Motion Model,
Geovisualization, Virtual Environments.

1. Introduction
One of the most important aspects of humancomputer interactions in virtual environments represents
the effectiveness and intuition of the underlying
navigation tools controlled by the users. Particularly in
geovirtual environments, navigation represents a key
functionality because “the acquisition of spatial
knowledge, essential for wayfinding, is primarily based
on direct environmental experience, which is usually
gained via movement” [6].
This paper presents an approach towards systemassisted 3D navigation based on smart constraints and a
physically-based motion model. The navigation
techniques presented are intended for real-time geovirtual
3D environments such as virtual terrain models, city
models, and landscape models. The techniques are
targeted at non-immersive environments and user
interaction by regular input devices such as 2D mouse,
space mouse, and keyboard.
As their key characteristics, the navigation techniques
can be classified as smart and physically-based. With
“smart” we refer to techniques that are aware of
confusing, disorienting viewing situations and provide
means to circumvent them. In addition, the navigation
techniques apply a physically-based model of the 3D
motion to ensure steady, continuous user movements and

to decouple the motion process from the handling of
hardware and user events.

1.1 Disorientation in Virtual 3D Environments
Disorientation represents one of the core problems
for the usability of virtual environments: Users frequently
get lost in the virtual space, that is, they lose their sense of
position, relations, and orientation. As Fuhrmann and
MacEachren point out [5], “core problems for users of
these desktop GeoVEs are to navigate through, and
remain oriented in, the display space and to relate that
display space to the geographic space it depicts.”
Particularly, the “end-of-world” problem is critical, that
is, if a user exceeds the virtual world’s boundaries and
reaches an undefined area. Disorientation affects both
inexperienced users as well as experienced users. The
former frequently need to restart or to cancel ongoing
activities, for the latter disorientation hinders effective
working in virtual environments. Burtnyk et al. [2]
describes further problems such as “a user may […] view
the model from awkward angles that present it in poor
light, miss seeing important features, experience
frustration at controlling their navigation, etc.”.
Smart navigation techniques provide a solution to the
disorientation problem. The key idea is to apply
constraints that limit camera control in a way that the
resulting view is guaranteed to contain a significant
amount of orientation-supporting information. In addition,
navigation techniques can take into account meta
information about the environment and its objects to
favorite important aspects, e.g., landmarks or areas of
interest.
Although smart navigation techniques constrain user
interactions, they must provide a sense of direct control to
the user and operate as comprehensible as possible.
Otherwise, constraints might cause the user to fight
helplessly against a limitation that the user does not
understand. Additionally, the user should realize the
camera control as time-coherent and physically sound –
unsteady and discontinuous motions, which would lead to
distraction, have to be avoided by an independent
physically-based camera model.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

1.2. Unsteady and Discontinuous Camera Motion
An unsteady and discontinuous camera motion
causes serious problems for interactive applications: If the
view of a subsequent frame is not visually coherent to the
previous frame, the user becomes not aware of the actual
camera movement. In the more harmless case of a jerky
motion the user is disturbed in performing a task within
the virtual environment. In practice, the problem of abrupt
changes is often circumvented by restricting the camera to
slow movements, which leads to a disadvantageous
compromise between smooth movement and the
capability of fast interaction.
To facilitate the design of navigation techniques and
constraints we propose an independent physically-based
camera motion subsystem, in which the camera is
modeled as a real, physical object situated in the virtual
world experiencing realistic physical forces. This
approach allows us to guarantee smooth movements when
abrupt changes in position or velocity occur, for example,
caused by directly mapping input events to camera
settings or by collisions between camera and scene
objects. Minimizing distracting discontinuities in the
camera movement as well as reducing delays due to
uncomfortable slow movement helps the user in
concentrating on the intended tasks.

2. Related Work
2.1 Navigation Techniques
There are several established navigation techniques
for virtual environments, which can be classified into
egocentric and exocentric frame of reference [8] and
distinguished according to the task they are suited for
[16].
With the Pedestrian navigation the user explores a
virtual environment from the point of view of a virtual
avatar, which can walk in four directions and rotate the
gaze around two axes using the mouse. This technique
has become quite popular due to its frequent use in
computer games. Similarly, the Flyer navigation controls
a virtual flying vehicle. The user manipulates the speed of
the forward/backward movement and the rotation around
the world’s up-vector. In addition, the flying vehicle
allows to change the height or to tilt the view direction.
The Flyer navigation is frequently used in geovirtual
environments. Both Pedestrian and Flyer navigations are
well suited for presentations and entertainment
applications. The Click-and-Fly navigation allows a
directed flight to a selected point of interest [14].
Navigation can be further improved by a Landmark
navigation, which calculates on-demand camera paths
from the current position to certain pre-defined
viewpoints [10,15].

For the task of examination, the Trackball navigation
is preferable. The user moves the camera on the surface of
a virtual sphere. It is usually combined with the Zoom
navigation, which allows the user to control the distance
of the camera to a focus point. This point is determined
by shooting a ray from the camera position towards the
view direction. With the Focus navigation the user selects
a point in the virtual environment that becomes the new
focus point. The technique rotates the camera in such way
that the focus point moves into the center of the
viewplane during a short animation period. There is a
large number of other useful techniques, see [4,8,12,16]
for an overview.

2.2 Constrained Navigation
Guided navigation has been applied successfully for
supporting user orientation, user experience, and the
creation of the user’s cognitive map. Galyean proposed
the river analogy [7]: The user is guided by a predefined
path and controls the gaze direction along with slight
deviations away from the path. Hanson and Wernert
extended this concept to guide manifolds [9]. Using 2D
input devices the user moves on a designer-provided
surface. The remaining degrees of freedom are controlled
by predefined guide fields according to the camera
position. In [20] this method has been extended and
applied to collaborative virtual environments. Burtnyk et
al. [2] developed an authoring tool for camera movement
called StyleCam that allows for continuous and seamless
transition between the camera’s spatial-control and its
temporal-control of animation; they apply user-defined
surfaces to constrain the camera movement. The attentive
camera [11] addresses the problem of guiding the view
direction without distracting the user from the intended
walk direction. Kiss and Nijhold [13] presented a system
that alters the view direction based on the terrain slope
and some objects of interest around the camera position.
Each object of interest is suggested to the user by shortly
focusing it. The system also ensures that the user is
always aware of obstacles that prevent the user from
moving. Bares et al. [1] introduce a heuristic constraint
solver to create optimal predefined camera paths.

2.3 Physical Motion Models
Physical camera models have shown to be useful in
previous approaches, e.g., in [17] a physical model is used
to achieve realistic camera movement using 3D input
devices. The concept of dynamic tethering uses a
physically-based approach to control the behavior of a
camera that traces a moving object or a virtual avatar
[3,18].

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

a) High orientation value

b) Reduced orientation value

c) Very low orientation value

Figure 1: 3D terrain views with varying orientation values ranging from high to zero.

3. Navigation Framework
The key concepts of our approach include:
●

●

Orientation Constraints: They support user
orientation by detecting confusing, disorienting view
specifications, and they provide functionality to
correct them.
Physical Motion System: It ensures steady,
continuous camera movements based on decoupling
the motion process from handling hardware and user
events.

As main input, the framework processes user and
time events. Depending on the current navigation
technique, it generates view specifications, that are
checked by orientation constraints and modified, if
necessary. The physical motion system uses these view
specifications to produce a series of smooth and steady
view specifications, that are sent to the underlying 3D
rendering system.

4. Orientation Constraints
We need to introduce a measure for the orientation
quality of a view specification, and we have to define
strategies that take over the control of the camera in
situations considered critical.

4.1 Orientation-Supporting Information
Orientation constraints ensure that a view contains a
significant amount of orientation-supporting information.
For this, we define an orientation value, which represents
a metric of the orientation-supporting information. This
value measures the capability of a view specification to
support the orientation of the user. Views with a low
orientation value are considered to be confusing and,
therefore, to be avoided.
One way to determine the orientation value of a view
is to measure the portion of the screen that is covered with

relevant parts of the scene. For GeoVEs, the orientation
value can be calculated by counting the screen pixels
covered by terrain (e.g., using the ARB extension for
occlusion query).
For specific application domains, we can define
priorities for different types of scene objects that estimate
the orientation-supporting contribution of the objects. In
the case of geovirtual environments, we can attribute
highest priority to landmarks and points of interest, high
priority to the terrain surface, and less priority to the sky.
This kind of meta information can be specified as
attributes in the scene specification.
Figure 1 shows an example of three situations with
different orientation values: a) The scene shows enough
visible scene content for orientation. b) A smaller portion
of the user’s sight is covered with orientation-supporting
scene objects. c) There is no orientation hint.

4.2 Orientation Maintenance Strategy
Each orientation constraint defines a maintenance
strategy to hold the orientation value at a sufficiently high
level, assuming that the previous view already had a high
orientation value. This strategy checks the view
specification proposed by the associated navigation
technique. If the resulting orientation value falls below a
certain threshold value indicating that the user is
approaching a disorienting situation, the orientation
constraint corrects the view specification. As long as
enough orientation-supporting information is provided,
the user can transit also through regions of lower interest.
As a general functionality, the maintenance strategy
checks for collisions of the virtual camera and its
environment.
The simplest method to keep a high orientation value
is to block the proposed movement by using the previous
view specification. Blocking the movement, however,
usually leads to disturbing effects.
We propose an improved method, which identifies
the user’s intention and guides the user away from
disorienting views based on the intention and outweighing

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

c)

view direction until a maximum angle is reached.
If no more tilting is possible, the strategy rotates the
flight direction parallel to the terrain to fly along the
terrain border.

4.3 Orientation Adjustment Strategy
Each orientation constraint defines an adjustment
strategy that ensures that the associated navigation
technique is capable of processing the current view
specification. While a navigation technique is active, its
orientation constraint never produces view specifications
that cannot be handled by the technique. After a switch
between navigation techniques, however, the newly active
navigation technique is confronted with arbitrary view
specifications. The orientation adjustment strategy
calculates a close applicable view specification if the
current one is not usable for the active technique.
Figure 3 illustrates an orientation adjustment strategy
for the Zoom navigation technique. After a switch from
the Pedestrian technique, no focus point is defined. Since
the Zoom technique needs a valid destination to zoom to,
the adjustment strategy tilts the view direction until a
focus point is hit. During this animation, however, the
user will not lose control. The user can interrupt the
animation at any time by using a different navigation
technique.

a) Strategy moves the camera away from terrain.

b) Strategy tilts down the view direction.

c) Strategy turns right.

Figure 2: Maintenance strategy for keeping
high orientation values.
the degraded orientation with corrections in orthogonal
movement degrees.
Figure 2 illustrates the maintenance strategy of the
Flyer navigation technique in the context of terrain
visualization. In three typical situations the user is about
to navigate into critical situations:
a)

The user rotates the flight direction and causes the
camera to look too far beyond the terrain border. The
rotation is accepted but outweighed by a slight rear
movement away from the border.
b) The user is flying forward beyond the terrain border.
The maintenance strategy temporarily tilts down the

a)

b)

5. Physical Motion System
The physical motion system regards the camera as a
realistic, inertial object within the virtual environment and
ensures steady and continuous movement, which is
achieved by a series of camera positions with smooth
accelerations and decelerations.

5.1 Autonomy of the Motion System
The problem of motion quality is solved at the end of
the whole navigation pipeline: the physical motion of the
camera is controlled by a system that is independent from
the navigation system.

c)

d)

Figure 3: Adjustment strategy for ensuring of feasible view specifications: a) No valid focus point
is defined; b) Adjustment strategy has tilted down the view direction; c) Zoom technique is now
able to work; d) Illustration of the adjustment process.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

Mathematically, it can be expressed by the ratio of
viscosity and spring strength.
Figure 4 illustrates the physical spring model in the
situation of zooming into an area of interest using the
mouse wheel as user input-device. Without applying the
physical model the camera approaches the terrain in
discrete steps (Figure 4 top). Adding the spring induces
the following effect (Figure 4 bottom): The unsteady
change in position is only applied to the spring’s origin.
The camera itself relaxes in a spatial-coherent way to its
new target position. Newton‘s physical laws guarantee
steadiness in position and velocity:

Mathematically, the behavior of the camera is obtained by
solving the resulting motion equation
Figure 4: Illustration of the physical spring
model applied to the camera.
This separation is motivated by the following reasons:
●

●

Ensuring smooth camera motion in all situations
makes up a significant portion of the development
effort of navigation techniques and constraints.
Encapsulated into a common module, the
implementation of navigation techniques and
constraints is facilitated.
The physical motion system is able to smooth the
movements that result from the interaction of the
camera and its environment. For example, navigation
techniques do not need to consider deceleration in the
case of an obstacle to which the camera is bound.

5.2 Physics of the Camera Model
We model the camera as an inertial 3D object that is
only controlled indirectly via a virtual spring. That is, the
user controls only the spring’s origin, whereas the camera
itself follows to the desired position in a time-delayed
way. Unsteady movements of the spring’s origin are
filtered due to the indirect power transmission and the
camera’s inertia.
Mathematically, the force of the spring on the camera
is described as
Spring strength S and the viscosity V of the medium in
which the camera is situated (e.g., vacuum, water, honey)
are the parameters, which define the camera’s behavior in
the physical environment. The viscosity value determines
the amount of friction the camera experiences
The mass of the camera can be neglected because as a
redundant parameter it does not augment the parameter
space by an additional degree of freedom.

Spring strength S and viscosity V determine the exact
behavior. The stronger the spring and the lower the
viscosity value the faster is the camera’s movement. The
two parameters also determine a) whether the camera
oscillates around the new target point, b) whether it
performs only a single oscillation (critical damping), or c)
whether the camera reaches the target without passing it.
The viscosity depending critical spring value Scritical
separates the domain of movement with oscillations
around the target point (S > Scritical) and the domain of
direct movement into the target without oscillation (S <
Scritical). It is given by the formula
The appropriate choice of the parameters S and V is
important for the user acceptance of a navigation
technique, because they define a compromise between the
degree of smoothness and the time lag between the user
input and the camera movement. Therefore, each
navigation technique requires its special physical
behavior of the camera. For the Trackball navigation
technique, e.g., the camera needs to follow the spring’s
origin in a direct and tight way. For it, a strong spring is
adequate. For the Flyer navigation technique, best user
acceptance can be achieved if the motion appears
exceptionally smooth, which is obtained by a weak
spring. A physical setting is associated with each
navigation technique in the navigation framework,
containing suitable parameters for the physical
environment. Navigation techniques switch parameters of
the physical system without reinitializing it. Hence, a
steady visual flow is guaranteed even in the case of
switches between techniques.

5.3 Resolving Conflicts During Movement
The camera specifications produced by the physical
motion system can be in conflict with the applied

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

constraints of the navigation system. However, these
conflicts rank below the motion quality because the
camera follows the target instantly, and potential
violations of constraints only occur for a short time.
In Figure 5 the camera is flying over a terrain surface
at a fixed height. If we would directly assign the intended
height to the camera, it would follow each bump of the
terrain surface. The resulting movement, of course, would
be very unpleasant. The physical motion system enhances
the quality of the camera motion by filtering the fast
changes in height due to physical inertia of the camera.
The resulting camera path is a smoothed version of the
terrain surface. The filtering process is time dependent,
i.e., it depends on the velocity of changes in height and
not only in the surface’s height profile. Only flying fast
will result in a smoothed camera trajectory; flying with a
low velocity will approximately reproduce the original
terrain surface. Additionally, we scale the velocity
proportional to the camera’s height above terrain, in
analogy to the Depth Modulated Flying of Ware and Fleet
[19]. Therefore, the impact of the terrain profile decreases
with increasing height.
To improve the sensitivity of the physical motion
system with respect to the camera-environment
interaction, we can also consider anticipating camera
movements. For example, the application of the physical
motion can interfere with the property of the Flyer
navigation technique to keep the camera at a fixed height
– temporary collisions are likely to occur because of the
camera’s inertia. The anticipatory method measures the
to-be-heights of the terrain in the flying path before the
camera reaches these points. It adjusts the camera’s
intended height to the maximum of all measured values.
As a result, collisions of the camera with the terrain are
prevented. Figure 5d shows the resulting camera path
with the anticipatory method.
Nevertheless, in rare cases the camera still might
temporarily collide with the terrain or objects in the 3D
world. A final collision test prevents collisions in this
case. The camera’s final position will then be modified.

5.4 Fast Slow-Down of the Physically-Based
Motion
A crucial situation occurs when the camera reaches
its destination and the user stops the interaction, for
example, if the camera is pointing onto an object of
interest. A disadvantage of the physical motion model is
the reaction time of the camera when choosing an
environment setting with a weak spring. While the camera
is already in its optimal position, the force’s origin is yet
one step further. As a result, the camera will come to rest
later than desired. To avoid this problem the physical
system has a built-in slow down control. When it is
activated by a navigation technique the viscosity of the

Figure 5: The Flyer navigation technique and the
interaction of camera and terrain surface. a)
Flight without the physical motion system. b)
Flight at high velocity. c) Flight at low velocity.
d) Flight based on anticipating the path.
medium in which the camera is situated is set to a very
high value. This is comparable to an exchange of the
current medium by a very viscous one like honey. Nearly
all of the motion energy is absorbed by the high friction
and the camera is slowed down rapidly, but still in a
steady and pleasant way. Additionally the spring-force’s
origin is moved to the actual camera position so that the
camera will not be pulled away from its optimal position.

6. Conclusions
Smart and physically-based navigations contribute to
solve typical usability problems in the scope of virtual
environments such as the “get lost” and “end-of-world”

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

interaction problems. We observed that orientation
constraints require domain-specific knowledge as
illustrated for the case of geovirtual environments. In
other domains, the framework can facilitate the
development
of
similar
constraints
improving
effectiveness and intuition.
In our experience, the physically-based motion model
improves the motion quality regardless of the frame rate
provided that 3D rendering is done in real-time. It handles
even complex situations, e.g., if switches between
different navigation techniques take place, or if events
such as collision with objects of the virtual world are
considered.
Our approach has been implemented as a part of our
3D geovisualization system LandXplorer. As future work,
we plan on doing usability studies to examine the effect
of different orientation constraints and physical settings.
Furthermore, we will specialize the framework for the
domain of 3D city models. As a major area of future
work, we want to work on a more sophisticated definition
of the orientation value as a metric of the orientation
support by considering the semantics of the 3D scene
objects.

Acknowledgements
We would like to thank the Remote Sensing GmbH,
specializing on 3D visualization of landscapes and cities
adapted from high-resolution digital aerial imagery, for
the city model of Berlin and for the aerial image of
Fuessen. We are also greatful to the German
environmental foundation Deutsche Bundesstiftung
Umwelt for supporting our research within the Lenne3D
project (www.lenne3d.de).

[6]

[7]

[8]
[9]

[10]

[11]

[12]

[13]

[14]

[15]

References
[1]

[2]

[3]

[4]

[5]

Bares, W., McDermott, S., Boudreaux, C., Thainimit, S.
Virtual 3D Camera Composition from Frame Constraints.
Proceedings of the 8th ACM international conference on
Multimedia, ACM Press, 2000, pp. 177-186.
Burtnyk, N., Khan, A., Fitzmaurice, G., Balakrishnan, R.,
Kurtenbach, G. StyleCam: Interactive Stylized 3D
Navigation using Integrated Spatial & Temporal Controls.
Proceedings of the 15th Annual ACM Symposium on User
Interface Software and Technology (UIST), ACM Press,
2002, pp. 101-110.
Colquhoun, H.W.J., Milgram, P. Dynamic Tethering for
Enhanced Remote Control and Navigation. Proceedings
of the IEA/HFES 2000 Congress, Human Factors and
Ergonomics Society, 2000, pp. 146-149.
Darken, R.P., Sibert, J.L. A Toolset for Navigation in
Virtual Environments. Proceedings of the 6th ACM
Symposium on User Interface Software and Technology
(UIST), ACM Press, 1993, pp. 157-165.
Fuhrmann, S., MacEachren, A.M. Navigation in Desktop
Geovirtual
Environments:
Usability
Assessment.
Proceedings of the 20th ICA/ACI International

[16]

[17]

[18]

[19]

[20]

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

Cartographic Conference, China ICA, 2001, pp. 24442453.
Gale, N., Golledge, R., Pellegrino, J.W., Doherty, S. The
Acquisition and Integration of Route Knowledge in an
Unfamiliar Neighborhood. Journal of Environmental
Psychology, 10, 1990, pp. 3-25.
Galyean, T.A. Guided Navigation of Virtual
Environments. Proceedings of the 1995 Symposium on
Interactive 3D Graphics, ACM Press, 1995, pp. 103 - ff.
Hand, C. Survey of 3D Interaction Techniques. Computer
Graphics Forum, 16(5), 1997, pp. 269-281.
Hanson, A.J., Wernert, E.A. Constrained 3D Navigation
with 2D Controllers. Proceedings of IEEE Visualization,
IEEE Computer Society Press, 1997, pp. 175-182.
Helbing, R., Strothotte, T. Quick Camera Path Planning
for Interactive 3D Environments. Smart Graphics 2000
Demo Session, 2000.
Hughes, S., Lewis, M. Attentive Interaction Techniques
for Searching Virtual Environments. Proceedings of the
Human Factors and Ergonomics Society’s 46th Annual
Meeting, Human Factors and Ergonomics Society, 2002,
pp. 2159-2163.
Igarashi, T., Kadobayashi, R., Mase, K., Tanaka, H. Path
Drawing for 3D Walkthrough. Proceedings of the 11th
Annual ACM Symposium on User Interface Software and
Technology (UIST), ACM Press, 1998, pp. 173-174.
Kiss, S., Nijholt, A. Viewpoint Adaptation during
Navigation based on Stimuli from the Virtual
Environment. Proceedings of the 8th International
Conference of 3D Web Technology, ACM Press, 2003, pp.
19-26.
Mackinlay, J.D., Card, S.K., Robertson, G.G. Rapid
Controlled Movement through a Virtual 3D Workspace.
Proceedings of the 17th Annual Conference on Computer
Graphics and Interactive Techniques, ACM Press, 1990,
pp. 171-176.
Salomon, B., Garber, M., Lin, M.C., Manocha, D.
Interactive Navigation in Complex Environments using
Path Planning. Proceedings of the ACM SIGGRAPH
Symposium on Interactive 3D Graphics, ACM Press,
2003, pp. 41-50.
Tan, D.S., Robertson, G.G., Czerwinski, M. Exploring 3D
Navigation: Combining Speed-Coupled Flying with
Orbiting. Proceedings of the CHI 2001 Conference on
Human Factors in Computing Systems, ACM Press, 2001,
pp. 418-425.
Turner, R., Balaguer, F., Gobbetti, E., Thalmann, D.
Physically-Based Interactive Camera Motion Control
Using 3D Input Devices. Scientific Visualization of
Physical Phenomena: Proceedings of CG International
Tokyo, Springer Verlag Inc., 1991, pp. 135-145.
Wang, W., Milgram, P. Dynamic Viewpoint Tethering for
Navigation in Large-Scale Virtual Environments. Human
Factors and Ergonomics Society’s 45th Annual Meeting,
Human Factors and Ergonomics Society, 2001.
Ware, C., Fleet, D. Context Sensitive Flying Interface.
Proceedings of the 1997 Symposium on Interactive 3D
Graphics, ACM Press, 1997, pp. 127ff.
Wernert, E.A., Hanson, A.J. A Framework for Assisted
Exploration with Collaboration. Proceedings of IEEE
Visualization, IEEE Computer Society Press, 1999, pp.
241-248.

