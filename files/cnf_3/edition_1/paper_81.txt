2011 15th International Conference on Information Visualisation

Evaluation of Volumetric Medical Images Segmentation
using Hidden Markov Random Field Model
Samy Ait-Aoudia, Ramdane Mahiou, Elhachemi Guerrout
ESI - Ecole nationale Supérieure en Informatique, BP 68M, O-Smar 16270 Algiers, Algeria
s_ait_aoudia@esi.dz, r_mahiou@esi.dz, e_guerrout@esi.dz

Abstract. Medical image segmentation is a crucial step
in the process of image analysis. An automatic aid in
interpretation of huge amount of data can be of great
value to specialists that hold final decision. Hidden
Markov Random Field (HMRF) Model and Gibbs
distributions provide powerful tools for image
modeling. In this paper, we use a HMRF model to
perform segmentation of volumetric medical images
handling inter-image similarity. This modelling leads to
the minimization of an energy function. This problem is
computationally intractable. Therefore, optimizations
techniques are used to compute a solution. We will use
and compare promising relatively recent methods based
on graph cuts with older well known methods that are
Simulated Annealing and ICM.

Annealing scheme (Metropolis-SA), the Iterated
Conditional Modes (ICM), and the Graph Cuts
promising method [16]. When we have the ground truth
images (a priori segmented images known), the
segmentation evaluation is made by calculating Dice
coefficients (Kappa Indexes) that gives the similarity
between segmented images and the a priori labeled
images.
This paper is organized as follows. We remind in
section 2 basis of Markov Random Field model. In
section 3, we give principles of Hidden Markov Field
model in the context of image segmentation. The
optimization techniques used are explained in section 4.
Experimental results on medical samples datasets are
given in section 5. Section 6 gives conclusions.

Keywords. Medical image segmentation, Hidden
Markov Random Field, Gibbs distribution, Simulated
Annealing, Gibbs sampler, Metropolis Sampling,
Iterated Conditional Modes, Graph cuts.

II.MARKOV RANDOM FIELD MODEL
In this section we remind some important notions
relative to Markov Random Field model and some
terms used in the context of image analysis issues.

I. INTRODUCTION

A. Neighborhood system and clique notion
The pixels of the image are represented as a lattice S of
M=n*m sites.
In an MRF, the sites (pixels in our case) in S are related
by a neighborhood system V(S).
The first and second order neighborhood systems are
the most commonly used. In these systems, a site has
four and eight neighbors respectively. When a site has
four or eight neighbors, we speak about a 4neighborhood (4-N) or an 8-neighborhood (8-N).
A clique c is a subset of sites in S relatively to a
neighborhood system. c is a singleton or all the distinct
sites of c are neighbors.

Classical medical exams can produce large sets of
images. The huge amount of produced data sets makes
the analysis and interpretation a tiresome and delicate
task. An automatic aid in interpretation can be of great
value to specialists that hold final decision. Automatic
medical image segmentation is thus an important phase
that can rapidly partition the images in different tissues
(normal or abnormal).
Several methods were used to perform this
segmentation. HMRF Model and Gibbs distributions
provide powerful tools for image modelling [10, 11, 14]
that lead to a solid basis to the final image
segmentation. Since the seminal paper of Geman and
Geman [7], Markov Random Fields (MRF) models for
image segmentation have been largely investigated [1,
9, 15, 17]. In this paper, we use a Hidden Markov
Random Field (HMRF) model to perform segmentation
of volumetric medical images (3D datasets). This
modelling leads to the minimization of an energy
function. This problem is computationally intractable.
Therefore, optimizations techniques are used to
compute a solution. Choosing a good optimization
method is a crucial task. A poor optimization process
can lead to “disastrous” results. We will use and
compare well-known optimization techniques that are
Gibbs Sampler with Simulated Annealing scheme
(Gibbs-SA), Metropolis Sampling with Simulated
1550-6037/11 $26.00 © 2011 IEEE
DOI 10.1109/IV.2011.83

A p-order clique noted cp contains
cardinal of the clique.

p sites i.e. p is the

B. Markov Random Field
Let X={X1,X2,…,XM} be a family of random variables on
the lattice S. Each random variable taking values in the
discrete space Λ={1,2,…,K}. The family X is a random
M
field with configuration set Ω = Λ .
A random field X is said to be an MRF on S with
respect to a neighborhood system V(S) if and only if :

∀ x ∈ Ω, P(x) > 0
∀s∈S,∀x∈Ω,P(Xs=xs/Xt=xt,t≠s)=P(Xs=xs/Xt=xt,t∈Vs(S))
513

III.HMRF MODEL

The Hammersley-Clifford theorem establishes the
equivalence between Gibbs fields and Markov fields.
The Gibbs distribution is characterized by the following
relation:
-1

P(x)= Z e−
Z = ¦e −

A. Hidden Markov Random Field
A strong model for image segmentation is to see the
image to segment as a realization of a Markov Random
Field Y={Ys}s∈S defined on the lattice S. The random
variables {Ys}s∈S have gray level values in the space
Λobs={0..255}. The configuration set is Ω obs.

U(x)
T

U(y)
T

y∈Ω

where T is a global control parameter called
temperature and Z is a normalizing constant called the
partition function. Calculating Z is prohibitive.
2097152
Card(Ω)=2
for a 512x512 gray level image. U(x) is
the energy function of the Gibbs field defined as :

The segmented image is seen as the realization of
another Markov Random Field X defined on the same
lattice S, taking values in the discrete space
Λ={1,2,…,K}. K represents the number of classes or
homogeneous regions in the image.

U(x)=¦U c(x)

In the context of image segmentation we have a
problem with incomplete data. Two different
information are associated to every site i∈S. Observed
information expressed by the random variable Yi and a
missed or hidden information expressed by the random
variable Xi. The Random Field X is said Hidden Markov
Random Field.

c∈C

C. Standard Markov Random Field
Standard Markov random fields have been used for
image analysis purposes.
Ising model
This model was proposed by Ernst Ising for
ferromagnetism studies in statistical physics. The Ising
model involves discrete variables si (spins) placed on a
sampling grid. Each spin can take two values, Λ={-1,1},
and the spins interact in pairs. The first order clique
potential are defined by –Bxs and the second order
clique potential are defined by :

The segmentation process consists in finding a
realization x of X by observing the data of the
realization y representing the image to segment.
B. MAP estimation
We seek a labeling x which is an estimate of the true
labeling x*, according to the MAP (Maximum A
Posteriori) [18] criterion (maximizing the probability
P(X=x|Y=y)).

U c ={s,t}(xs ,xt )=−β xs xt =­®−β if xs = xt
¯ β if xs ≠ xt

x =arg max{P(X = x|Y = y)}

The total energy is defined by :

x∈X

U(x)=− ¦βxs xt +¦Bxs
c ={s,t }

P(X = x | Y = y)=

s∈S

The coupling constant β between neighbor sites
regularizes the model and B represents an extern
magnetic field.

P(X = x | Y = y)= K e

U(x)
T

−Ψ(x, y)

probability P(X=x|Y=y) or equivalently by minimizing
the function Ψ(x|y).

x =arg min{Ψ(x,y)}
x∈X

The searched labeling x can be found using some
optimization techniques.

¦(2δ(x ,x )−1)
s

ln(P(Y = y| X = x)) -

P(X = x | Y = y)= K e
The labeling x can be found by maximizing the

Potts model
The Potts model is a generalization of the Ising model.
Instead of Λ={-1,1}, each spin is assigned an integer
value Λ={1,2,…,K}. In the context of image
segmentation, the integer values are gray levels or
labels. The total energy is defined by :

U(x)= β

P(Y = y|X = x)P(X = x)
P(Y = y)

t

s,t∈C 2

where δ is the Kronecker’s delta.
When β >0, the probable configurations correspond to
neighbor sites with same gray level or label. This
induces the constitution of large homogenous regions.
The size of these regions is guided by the value ofβ .

Assuming that the pixel intensity follows a Gaussian
distribution with parameters μk (mean) and σk2
(variance) and using the Potts model, we have :

Ψ(x,y)=

514

(ys - μ xs )²
β
+ln( 2π σ xs )+
(1-2δ(xs ,xt ))
2
T
2
σ
s
x
s,t∈C 2
s∈S

¦

¦

IV.OPTIMIZATION

Algorithm 3 : simulated annealing

1. Initialization: n=0 and T0=Tmax a high temperature;
the configuration x(0) is randomly chosen.
2. Repeat:
 Generate a perturbation x(n+1)
 Accept this state under condition
 Decrease the temperature T according to the
annealing function f(T)
Until reaching a minimum temperature Tmin that
guarantees the convergence to a global
minimum

The MAP estimation leads to the minimization of an
energy function. This problem is computationally
intractable. Therefore, optimizations techniques are
used to compute a solution. We will use and compare
relatively recent methods based on graph cuts with older
well known methods that are Simulated Annealing and
ICM.
Graph cuts algorithms are stemming from combinatorial
optimization. The two most popular graph cuts
algorithms used in our experiments are due to Boykov
et al. [3,4]. The two algorithms used (called swap-move
and expansion-move) find a local minimum with
respect to two types of moves that are swap moves and
expansion moves. These moves allow large number of
pixels to change their labels simultaneously. Algorithm
1 and algorithm 2 briefly describe these two methods.
Simulated annealing is a neighborhood based
optimization method inspired from a technique used to
have states of low energy of a material. This technique
is called metallurgic or physics annealing. Inspired from
the principles of the metallurgic annealing, Kirkpatrick
[13] proposed the well-known simulated annealing
optimization method shown by algorithm 3. We will use
in our experiments Gibbs Sampler with Simulated
Annealing scheme (Gibbs-SA) and Metropolis sampling
with Simulated Annealing scheme (Metropolis-SA).
The Iterated Conditional Modes (ICM) algorithm
proposed by Besag [2], is a deterministic relaxation
scheme with a constant temperature. Performances of
the ICM algorithm tightly depend on the initialization
process. It converges toward the local minimum close to
the initialization. Algorithm 4 summarizes the ICM
technique.

Algorithm 4 : ICM

1. Initialization: Start with an arbitrary labeling x0 and
let n=0.
2. At step n :
Visit all the sites according to a visiting scheme and
in every site s :

xsn +1 = arg min
U s ( xs = λ )
card ( S )

x∈Ω
, λ ∈Ω .
3. Increment n . Goto 2, until a stopping criterion is
satisfied.

V.EXPERIMENTAL RESULTS
The evaluation of the segmentation is made on
volumetric medical data samples. All images were graylevel, and were scaled to 8 bits/pixel.
The implementation of the segmentation model requires
estimating the parameters μ and σ for each class. Since
the segmentation is unsupervised as in [6,12], the
expectation-maximization (EM) algorithm [5,8,19] is
used to compute μ and σ.
The segmentation model needs also the estimation of
several parameters that are: the parameter β, the initial
temperature T0 for the simulated annealing process, the
constant τ and the neighborhood system. This is a

Algorithm 1 : α-β swap algorithm

1. Start with an arbitrary labeling x
2. Set success := 0
3. For each pair of labels {α, β} ⊂ L
3.1. Find x* = argmin E(x’) among x’ within one αβ swap of x
3.2. If E(x*) < E(x), set x := x* and success := 1
4. If success = 1 goto 2
5. Return x

non trivial task. We have conducted several tests with
different parameter choices. The choice of wrong
parameters can have “dramatic” consequences on image
segmentation quality and the time needed to do this
segmentation.
The convergence of Simulated Annealing algorithm is
tightly linked to the annealing process. The logarithmic

τ
(where τ. is a
log(1+ n)
constant) proposed by Geman and Geman [7] is very
slow. In practice, a geometric decrease T(n+1)=τ.T(n)
is used without noticeable degradation of results.
The following examples give some results showing the
results of the segmentation of medical and synthetic
images.
Figure 4 shows an IRM scan and its corresponding
segmentations in four classes varying the parameter β.
decrease of the temperature T(n)≥

Algorithm 2 : α expansion algorithm

1. Start with an arbitrary labeling x
2. Set success := 0
3. For each label α ⊂ L
3.1. Find x* = argmin E(x’) among x’ within one αexpansion of x
3.2. If E( x*) < E(x), set x := x* and success := 1
4. If success = 1 goto 2
5. Return x

515

β =0

β =1

β =2

β =10

T 0 = 10

T0 = 4

T0 = 1

Figure 6. Segmentation varying T0.

Figure 4. Segmentation varying β.
Figure 5 shows an IRM scan and its corresponding
segmentations in three classes with β=1 and T0=4
varying the parameter τ.
Figure 6 shows a synthetic image and its corresponding
segmentations in five classes with β=1 and τ=0.98
varying the parameter T0.

a
b
Figure 7. 6-neighborhood and 18-neighborhood systems

τ

Evaluating the quality of the segmentation can only be
made on synthetic images where the a priori
segmentation is known. For this purpose, we have used
the Brainweb1 database largely used in the evaluation of
brain segmentation. The Dice Coefficient DC or Kappa
Index given hereafter measures the quality of the
segmentation.

=0,98

DC =2×

τ

=0,5

τ

TP
2×TP + FP+ FN

where TP stands for True positive, FN False Negative
and FP False Positive.

=0,1

Figure 5. Segmentation varying τ.

The Dice coefficient equals 1 when the two
segmentations are identical and 0 when no classified
pixel matches the true segmentation.

Based on an evaluation phase, the following parameters
values were chosen: β=1, T0=4, τ =0,98, 6-neighborhood.
They give good results and a better compromise in
terms of segmentation quality and processing time: The
6-neighborhood system is a 3D neighborhood system as
shown in figure 7.a. The use of 3D 18- neighborhood

Figure 8 shows some slices and their corresponding
segmentation of the volumetric dataset for the subject
04 T1w (181 slices, 256x256 pixels per images) taken
from the Brainweb database.

(figure 7.b) and 26- neighborhood systems imply more
computing time without bringing noticeable quality in
segmentation.

1

516

http://www.bic.mni.mcgill.ca/brainweb/

Slice 52

Slice 72

Figure 10. Segmented images.

Slice 138
Figure 8. Brainweb Segmented examples.
Some slices of subject T1_ICBM (from Brainweb
database) are shown in figure 9. Their corresponding
segmentations are given in figure 10. The ground truth
segmentations for the white matter and gray matter are
given in figure 11 and 12 respectively.

Figure 11. Ground truth white matter images.

Figure 9. MR brain images sample.

4-N

6-N

18-N

ICM

0,84

0,85

0,85

Gibbs-SA

0,86

0,87

0,88

Metropolis-SA

0,87

0,89

0,89

GC α,β swap

0,92

0,93

0,94

GC α expansion

0,94

0,95

0,95

White Matter

The means of Dice coefficients calculated for these
slices considering white matter and gray matter are
given in table 1. The neighborhood systems used are 4neighborhood (4-N), 6-neighborhood (6-N) and 18neighborhood (18-N). With the 4-neighborhood, the
interactions between slices are not taken into account
since this is a 2D neighborhood. 6-neighborhood and
18-neighborhood are 3D neighborhood. The use of the
18-neighborhood don’t bring noticeable quality in
segmentation but requires more computing time.

4-N

6-N

18-N

ICM

0,79

0,80

0,81

Gibbs-SA

0,80

0,82

0,82

Metropolis-SA

0,81

0,83

0,83

GC α,β swap

0,85

0,87

0,87

0,86

0,87

0,88

GC α expansion

Gray Matter
TABLE I. Mean Kappa Index

517

[4] Y. Boykov and V. Kolmogorov, “An Experimental
Comparison of Min-Cut/Max-Flow Algorithms for
Energy Minimization in Vision”, In IEEE Transactions
on PAMI, Vol. 26, No. 9, pp. 1124-1137, Sept. 2004.
[5] A.P. Dempster, N.M. Laird, D.B. Rubin, “Maximum
likelihood from incomplete data via the EM algorithm”,
Journal Royal Stat. Soc., B1:1-38, 1977.
[6] H. Deng, D.A. Clausi, “Unsupervised image
segmentation using a simple MRF model with a new
implementation scheme”, Proc. of the 17th International
Conf. on Pattern Recognition, Aug. 2004, 691- 694.
[7] S. Geman, D. Geman, “Stochastic relaxation, Gibbs
distributions and the Bayesian restoration of images”.
IEEE Trans. Pattern Anal. Machine Intell., 1984, 6(6),
721-741.
[8] D.B. Gu, J.X. Sun, “EM image segmentation algorithm
based on an inhomogeneous hidden MRF model”,
Vision, Image and Signal Processing, IEE Proceedings,
Volume 152, Issue 2, 8 April 2005, 184 – 190.
[9] K. Held, E.R. Kops, B.J. Krause, W.M Wells, R.
Kikinis, H.-W. Muller-Gartner, “Markov random field
segmentation of brain MR images”, IEEE Tran. on
Medical Imaging, Dec. 1997, Vol. 16(6), 878-886
[10] .A. Huang, R. Abugharbieh, R. Tam, “Image
segmentation using an efficient rotationally invariant 3D
region-based hidden Markov model”, IEEE Computer
Vision and Pattern Recognition Workshops, June 2008,
Anchorage, AK, USA, 1-8.
[11] M. Ibrahim, N. John, M. Kabuka and A. Younis, “Hidden
Markov models-based 3D MRI brain segmentation”,
Image and Vision Computing Volume 24, Issue 10, Oct.
2006, 1065-1079.
[12] Z. Kato, J. Zerubia, M. Berthod, “Unsupervised parallel
image classification using Markovian models”, Pattern
Recognition 32 (1999) 591-604.
[13] S. Kirkpatrick, C.D. Gelatt, M.P. Vecchi, "Optimisation
by simulated annealing", Science, vol. 220 (4598), pp
671-680, 1983.
[14] S. Z. Li. Markov Random Field Modeling in Computer
Vision. New York: Springer-Verlag, 2001.
[15] J.L. Marroquin, B.C. Vemuri, S. Botello, E. Calderon, A.
Fernandez-Bouzas, “An accurate and efficient Bayesian
method for automatic segmentation of brain MRI”, IEEE
Trans. on Med. Imaging, vol. 21(8), Aug. 2002, 934-945.
[16] R. Szeliski, R. Zabih, D. Scharstein, O. Veksler, V.
Kolmogorov, A. Agarwala, M. Tappen, C. Rother, “A
Comparative Study of Energy Minimization Methods for
Markov Random Fields with Smoothness-Based Priors”,
IEEE Trans. on Pattern Analysis and Machine
Intelligence, Vol. 30(6), June 2008.
[17] K. Van Leemput, F. Maes, D. Vandermeulen, and P.
Suetens, “A Unifying Framework for Partial Volume
Segmentation of Brain MR Images”, IEEE Trans. on
Medical Imaging, vol. 22(1), pp.105-119, January 2003.
[18] P. Wyatt and J.A. Noble “MAP MRF joint segmentation
and registration of medical images”, Medical Image
Analysis, Volume 7, Issue 4, December 2003, 539-552
[19] Y. Zhang, M. Brady, S Smith, “Segmentation of Brain
MR Images through a Hidden Markov Random Field
Model and the Expectation-Maximization Algorithm”,
IEEE Trans. on Medical Imaging 20(1), Jan. 2001, 4557.

Figure 12. Ground truth gray matter images.
From table I, we see that the two graph cuts algorithms
(α-β swap-move and α expansion-move), give
remarkable best results than the older methods (ICM
and Simulated Annealing).
The implementation was also parallelized on a cluster of
PCs. The processing time was reduced to less than a
minute on a cluster of 16 PCs (Intel Pentium 4, CPU
3.60GHz, RAM 1 Go).
VI.CONCLUSION
This paper attempts to evaluate the segmentation of
Brain Magnetic Resonance Images using Hidden
Markov Random Field Model. Different optimization
techniques (Graph Cuts, Simulated Annealing and ICM)
were used and compared.
From the results obtained, Graph Cuts technique
outperforms the Simulated Annealing and ICM classical
techniques. When the ground truth is known, the
segmented images are very close (in similarity) to the a
priori segmented images when using graph cuts
techniques.
Nevertheless, further works must consider segmenting
sets of MR brain images taken from other sources. The
opinion of specialists must also be considered in the
evaluation when no ground truth is available to have a
more synthetic view of the whole segmentation process.
REFERENCES
[1] E.D. Angelini, T. Song, B.D. Mensh, & A.F. Laine,
“Brain MRI Segmentation with Multiphase Minimal
Partitioning: A Comparative Study”, Int. J. Biomed
Imaging, volume 2007, 15p.
[2] J. Besag, “On the statistical analysis of dirty pictures
(with discussion),” J. of Royal Statist. Soc., ser. B, 1986,
vol. 48, no. 3, pp. 259–302,.
[3] Y. Boykov, O. Veksler and R. Zabih, “Fast Approximate
Energy Minimization via Graph Cuts”, IEEE
Transactions on PAMI, vol. 23, no. 11, pp. 1222-1239

518

