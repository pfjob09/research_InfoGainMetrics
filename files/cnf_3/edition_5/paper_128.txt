PanoCAST: A Panoramic Multicasting System for Mobile Entertainment
Barnabás Takács1,2, Alex Beregszászi1, Gergely Komáromi-Mészáros1
1
SZTAKI Budapest, Hungary , 2 Digital Elite Inc., Los Angeles, USA
Btakacs@sztaki.hu
Abstract
This paper presents the design, architecture and

implementation of a novel panoramic broadcasting
system for Internet and mobile entertainment, where
real-time images obtained with spherical camera
are distributed to multiple viewers, each having
their own respective perspective and interactive
capabilities. The system, called PanoCAST
(Panoramic Broadcasting), was designed to deliver
telepresence to a mass media market. We discuss its
overall architecture and present examples of its
uses.

1. Introduction
The word telepresence is defined as “the experience of
or impression of being present at a location remote from
one’s own immediate environment” [1]. To achieve this
high level of immersion, a number of sensory stimuli,
such as visual, auditory, tactile, and perhaps olfactory,
need to be captured, encoded, transmitted and
subsequently presented or rendered to the user in a realtime and fully transparent manner. The level of
immersion in a telepresence system may be affected by
many variables and measured with the help of Presence
Questionnaires [2]. The ultimate goal of such technical
solutions, however, is to provide their users with the most
up-to date information and control over a remote
environment.
For the purposes of our current research we focused
on presenting visual and auditory stimuli to multiple users
at a time. Video-based solutions that employ panoramic
recording systems have recently become an important
field of research. Such off-line architectures employ
expensive multiple-head camera hardware and record data
to a set of digital tape recorders from which surround
images are stitched together in a tedious process [3]
These cameras are also somewhat large and difficult to
use and do not provide full spherical video (only
cylindrical), a feature required by many new applications.
More recently new advances in CCD resolution and
compression technology have created the opportunity to
design and build cameras that can capture and transmit
almost complete spherical video images [4,5], but these

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

solutions are rather expensive and can stream images only
to a single viewer.
To address these difficulties we have designed and
architecture that can redistribute spherical video to
multiple independent viewer each having control over
their own respective point of view. To achieve this goal
we built our solution on top of an advanced virtual reality
environment, called the Virtual Human Interface, which
is briefly discussed in the following section.

2. The Virtual Human Interface Platform
At the heart of our panoramic broadcasting solution
lies an advanced virtual reality platform, called the
Virtual Human Interface or VHI. The VHI is a complex
rendering system built up of many independent modules
explained as it is in Figure 1. Each of these modules
play a vital part in the process of delivering compelling
media content and thus create interactive experiences that
go beyond today’s form of communication. The VHI has
been successfully used in many different industries and
application areas [6-9]. In order to address the needs of
such diversity we devised a general architecture that can
be freely and easily configured on-the-fly. The principal
modules of the VHI are as follows:

Figure 1. Key modules of the Virtual Human Interface
used for panoramic broadcasting and telepresence.

1) Portable Virtual-Reality Platform: The VHI was
implemented to take advantage and always keep up with
the latest advances in low-cost consumer-level graphics
hardware and computer platforms. Despite the
requirements for high visual fidelity virtual environments,
media content, I/O devices, real-time vision, animated
virtual humans and many more features, the system runs
on an average portable platform. Portability is one of the
key factors in making VR accessible to many users.
One of the
2) High Fidelity Digital Humans:
cornerstones of the VHI system is its capability to model,
animate and render highly realistic virtual humans.
Building these photo-real virtual humans used for VRbased tutoring. The perceptual capabilities of the VHI
allow the algorithmic control mechanisms we
implemented to create a believable character who appears
to be paying attention to the user who is there to help him
or her guide through the interactive learning or
entertainment process.
3) Real-Time Panoramic Video, Compositing and
Augmented Reality: Depending of the exact needs of an
interactive application users set out to experience and
navigate in a Virtual Environment (VE) in which
seamlessly combines panoramic video elements with high
resolution 3-D models. To achieve the highest possible
level of fidelity and therefore immersion the virtual scene
is divided into two distinctive regions namely the
background containing elements in weak perspective, and
the foreground which holds visual objects close enough
to the user and therefore represented under strong
perspective transformations.
The background is
implemented as a spherical 360o panoramic video or
image. Foreground elements, on the other hand, are
synthetic 3D objects that appear under strong perspective
conditions.
4) Physical Simulation: Synthetic 3D digital objects
have physical properties, can move realistically when
interacted with using rigid-body dynamics and may be
subjected to environmental effects such as fog, fire,
smoke implemented via a built-in particle systems. In
addition, 3D speakers may be placed in the virtual scene
that can help guide the user in a specific direction. The
overall result is a unique blend of visual and acoustic
elements that create a truly immersive environment
capable of inducing emotions and reactions in a
controlled and programmable manner.
5) Low-Cost Input/Output Devices: To implement a
novel interaction model what we call closed-loop
dialogue, the VHI is continuously monitoring the users’
internal state by means of external sensors and input
devices. It then reacts to these measurements via a variety
of output channels. The input devices we interfaced with
range from simple web cameras and microphones to

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

capture video to more advanced devices such as an eye
tracker, biofeedback, data glove, touch screen, head
motion trackers, etc.
6) Virtual Studio Module: To further bridge the gap
between the synthetic environment and the real world of
the user the VHI contains a specialized image processing
module which works using the principles of virtual
studios. This functionality either allows another person to
enter the virtual world the user is experiencing, or
alternatively place the user himself or herself directly into
the virtual space.
7) Advanced Computer Vision and Facial Analysis:
Web cameras offer the least intrusive and perhaps the
most powerful means to understand a user’s behavior.
Therefore the VHI contains a special purpose computer
vision module that implements a multitude of image
processing and pattern recognition functions tailored
specifically to analyze faces and facial expressions.
8) Motion Capture (MOCAP) Support: Another special
purpose module implemented as a part of the vision
subsystem is a desktop motion capture solution which
employs low-cost USB cameras and reflective markers to
track the motion and deformation of objects. Its primary
goal is to provide an easy interface between physical
objects the user may hold in hand, and their virtual
counterparts representing them in the VE space.
Having reviewed the fundamental modules of the VHI
platform we next describe how these modules were used
to create a panoramic telepresence solution.

3. The PanoCAST System Architecture
The basic framework for telepresence is shown in Figure
2. A spherical video head located at a remote site first
captures, compresses and transmits images to a local hostcomputer in real-time, which subsequently re-transmits
(broadcasts) the raw image data through G-bit Ethernet to
multiple image servers. These servers are responsible for
creating and encoding image and sound streams that
arrive at the users’ remote computer for viewing and
manipulation. The user perceives the remote environment
via a low-cost head mounted display (HMD) [10] while
their head motion controls a virtual camera that maps
portions of the spherical video onto the flat display. Using
the advanced 3D sound and surround video capabilities of
the VHI Engine, one can experience being present at the
site of the camera only with very minimal delay. The key
technology behind the ability to address multiple users
each experiencing the same scene, but from a different
point of view, is the use of multiple individually
controlled virtual cameras as demonstrated in Figure 3.

Figure 2 Basic architecture of the PanoCAST system
for telepresence.

Figure 3 Using multiple individually controlled
virtual cameras to share a view of the same scenery.
First, a number of virtual cameras (shown in blue / upper
left hand corner) are placed inside a sphere (lower left
corner) on the surface of which the panoramic
surroundings are being mapped at speeds of up to 24
frames per second. Next, depending on the remotely
controlled rotation and field of view (FOV) settings of
each camera new views are generated for each user and

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

transmitted to their own viewing program for display.
During a PanoCAST session the head motion (or mouse
control) of captured from the user is relayed back to the
central rendering server and an new views are rendered
according to these new viewing parameters. Although the
VHI’s rendering architecture allows for creating an

unlimited number of virtual cameras , in practice, a single
server handles 6-12 different virtual cameras.
To setup a transmission stream each virtual camera is
first instructed to render its images into an addressable
memory buffer of programmable size and color accuracy.
These buffers are located on the graphics hardware (or
GPU) and their content sent back to the main memory of
the computer for further processing. This task is carried
out by a number of advanced fragment programs that take
advantage of the parallel architecture of the GPU in the
form of pixel shaders. As a result, all virtual views may
be not only displayed on the same computer monitor, but
also encoded and retransmitted into separate video
channels. This last step occurs with the help of virtual
device drivers. In our case, this allows the operating
system to handle the output from these view much like
any other computer device. Specifically, we “trick” the
operating system into believing that a number of physical
cameras are attached to it. These camera drivers,
however, receive their content not from a physical device,
but from the VHI engine itself. This is demonstrated in
Figure 4 where the operating system recognizes these
devices and therefore any program that requires a web
camera as an input device can use them without any
difficulty.

Figure 5 demonstrates the output from the rendering
pipeline for the case of 4 virtual cameras each being
oriented at a different portion of the same panoramic
scenery. The original panoramic view captured in a car
from the seat next to driver is shown in a stretched out
format on the top of the figure.

Figure 5 Rendered output from the same panoramic
scene using four virtual cameras with different
settings.
The final stage of sending these images to the respective
viewer who receive their assigned virtual camera they
have control over during the log-in process, is to encode
the sound and image data and display it on a remote
mobile device. To achieve this goal we have developed
multiple solutions using both publicly available video
conferencing and chat platform, such as Skype, MS
Messenger or Yahoo Messenger as well as our own
streaming solution for 3G mobile phones.

4. Application Areas

Figure 4 Virtual camera drivers appear as physical
devices for the operating system.

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

The PanoCAST system was successfully implemented
and tested in a number of real-world scenarios. Examples
from the field of entertainment include recording and
retransmitting a rock concert, for real estate the
representation of luxury homes, for tourism and education

the walk through in a building and city sites. The system
is capable of both recording panoramic scenery as well as
stream the spherical output to a number of distribution
servers in real-time. The main advantage of our solution
is that it is low-cost and capable of addressing many
users at the same time, yet each having total control over
what they see and how they view it. The PanoCAST
system is currently undergoing further development to
include mobile players and other forms of Internetenabled tele-presence applications. We argue that the
high level of flexibility it offers has the potential to bring
this special form of telepresence to large audiences and
therefore create a new infrastructure for education,
learning and entertainment. To underline these statements
we have created a number of demonstrations ranging
from popular rock concerts, to high-priced real state
walk-throughs, and visits to travel-related sites,
restaurants and hotels. In the education market
demonstrations for visiting other cultures and
experiencing classroom demonstrations in an e-Learning
context have now become possible. Finally, for
businesses, we created a new form shared workspaces in
which remote collaborations may well shape the future of
this industry.

5. Conclusion
In this paper we introduced a novel system for
transmitting panoramic surround videos to multiple
viewers each experiencing the flow of events from their
own personal perspective. We have developed an
architecture to transmit such imagery to a number of users
who use their own computer, low-cost head mounted
display (HMD) or even mobile phone device for display.
Using our solution we have created a number of
demonstrations that show the plausibility and usability of
our techniques and finally, we argued that such
PanoCAST systems may very well shape the future of
education, business and entertainment.

6. References
[1] Transparent Telepresence Research Group (2007),
http://www.telepresence.strath.ac.uk/telepresence.htm
[2] Witmer, B.G., M.J. Singer, (1998), “Measuring Presence
in Virtual Environments”, in Presence, 7 (3): pp.225240.
[3] Pryor, L., A.S. Rizzo (2000) “User Directed News”
http://imsc.usc.edu/research/project/udn/udn_nsf.pdf
[4] Immersive Media
Dodeca Camera (2007),
http://www.immersive-video.eu/en
[5]
Point Grey Research LadyBug2 Camera (2007),
http://www.ptgrey.com/products/ladybug2/index.asp

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

[6] Takacs, B. and B. Kiss (2003), “Virtual Human
Interface: a Photo-realistic Digital Human”, IEEE
Computer Graphics and Applications, 23(5):pp.38-45.
[7] Takacs B. (2005), “Special Education and Rehabilitation:
Teaching and Healing with Interactive Graphics”, IEEE
Computer Graphics and Applications, 25(5): pp.40-48.
[8] Takacs B. (2006), “Cognitive, Mental and Physical
Rehabilitation Using a Configurable Virtual Reality
System”, International Journal of Virtual Reality, 5(4):
pp.1-12.
[9] Digital Elite Inc. (2007), www.digitalElite.net.
[10] Imagin Z800 3Dvisor (2007), http://www.3dvisor.com/

