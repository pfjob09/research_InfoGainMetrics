Virtual Object Manipulation Using Physical Blocks
Satoshi Yonemoto, Takahiro Yotsumoto, Rin-ichiro Taniguchi
Kyushu Sangyo University, 2-3-1 Matsukadai, Fukuoka-city Fukuoka, 813-8503 Japan
Kyushu University, 744 Motooka Fukuoka-city Fukuoka, 8190395 Japan
{ yonemoto@is.kyusan-u.ac.jp rin@limu.is.kyushu-u.ac.jp }
Abstract

In this paper, we present a virtual object
manipulation tool that can directly control virtual
objects using physical objects. Physical objects can be
recognized by vision based sensing. Basic manipulations
using physical objects are translating, rotating, scaling
and replacing with a virtual object. Those manipulations
are performed by handling an object, a block. In addition
to this, complicated manipulations can be realized by
handling two objects with both hands. We have
implemented a prototype system and assessed the
effectiveness of the system by experimentally comparing
it with general modeling tool. The results suggest that
our object manipulation system can provide a more
effective means of handling than operations in general
modeling tool.
Keywords--- Tangible user interface, augmented
reality, motion capture.

1. Introduction
Realization of direct manipulation makes a user
interface easy to learn, to use, and to retain over time [1].
Tangible user interfaces (TUI) are a growing research
area of user interface that uses physical objects to
represent data. They were so named by Ishii and Ullmar
[2], and are extended in a wider range of practical
applications. TUI uses specific physical objects that can
embed knowledge of the physical world, i.e., how to
operate them [2][3]. Therefore, in order to utilize
everyday materials as such physical objects, a way to
observe them indirectly, for instance, vision-based
sensing is a useful way. In vision-based approach, AR
Toolkit [6] is known as a general-purpose augmented
reality tool, and its application, tangible AR is realized by
using visual tags.
Many researchers have developed real worldoriented interfaces using physical objects, toward
physical and virtual interaction. For example, AlgoBlock
[5] is a tool for collaborative learning, using specific
physical devices. Data Tiles [4] are special devices on
which display digital information. Moreover, in recent

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

years, low cost video game controller which has pointing
and motion-sensing abilities has also been developed [7].
We introduce a virtual object manipulation tool that
allows a user to construct and interact with a virtual
environment using physical objects. Virtual object
manipulation using physical objects is a solution to
reduce the difficulty of 3-D space cognition. A computer
recognizes positions of the physical objects in real time
by vision-based sensing, i.e., motion capturing technique.
To display real-virtual information, augmented reality
technique is employed, where virtual objects are
rendered by overlaying on real scene images. Basic
manipulations using physical objects are translating,
rotating, scaling and replacing with a virtual object. The
basic manipulations are performed by holding one object,
a block. In complicated manipulations, two blocks are
employed to handle with both hands.

2. Designing virtual object manipulation
In this section, we describe our system to realize
virtual object manipulation using physical objects. Figure
1 shows our system setup. Our system consists of work
space, physical objects, a monitor and two cameras for
vision based sensing. Two cameras are calibrated in
advance.

Figure 1 System setup.

2.1. Physical blocks
As physical objects, we employ a few color blocks.
A block consists of a 25mm x 25mm x 25mm cubic box
that is painted single color (Figure 2). Because each
block has just size for holding and keeps stable posture
on the work space, the user can easily handle it.
Moreover, the color is primary color, so it is useful to
stably detect the blocks by vision processing. We employ
at most five different color blocks (orange, red, yellow,
blue and green). Each color indicates the assigned object
manipulation such as translating, rotating and scaling.
For example, an orange block is assigned ‘‘translating’’,
which means to translate a virtual object, keeping the
same position.

2.2. Vision-based motion capturing
Blocks putting in the work space are immediately
detected. We employ HSI color models to detect block
regions in the captured image. Model colors of the block
regions are acquired by analyzing their color histograms
in advance. Centers of gravity of each block regions are
detected as 2-D block positions. Using the 2-D block
positions from two views, the 3-D position of each block
is calculated by stereo vision technique [8].

Instead of the above definition, the user can assign
his or her favorite color. Current parameters, such as
scale and pose are kept when the block is occluded for a
given period.

Figure 2 Physical blocks.

translating

replacing

2.3. Basic manipulations
Taking into account characteristics of direct
manipulation, basic manipulations such as translating,
rotating are assigned to handling physical blocks. Figure
3 illustrates the basic manipulations.
We designed the following manipulations using only
three pieces:
• Translating (orange block): An orange block
has the function of translating a virtual object,
keeping the same position. Figure 4 shows a
shot in translating a virtual object or a desk.
• Replacing (orange block): The orange block
has also another function of replacing a virtual
object to control. The type of the virtual object
is changed by swinging the block up and down.
Figure 5 shows a shot in replacing a desk with a
chair.
• Rotating (red block): A red block has the
function of rotating a virtual object. The virtual
object to handle is placed by the orange block
operation above mentioned. Rotation parameters
(roll and pitch) change, depending on the
distance from the orange block. Figure 6 shows
a shot in rotating a virtual object (pitch up).
• Scaling (yellow block): A yellow block has the
function of scaling a virtual object. The object
scale changes depending on the displacement of
the block position.

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

scaling

rotating

Figure 3 Basic Manipulations.

Figure 4 Translating a desk.

Figure 5 Replacing an desk with a chair.

2.4. Complicated manipulations
Using two blocks, complicated manipulations are
realized. For example, human-like character motions can
be controlled by handling two blocks. Figure 7 shows an
example of character motion control. Arm motions are
coupled with blue and green block operation. In short,
two blocks have motion sensing capability, which allows
the user to interact with and manipulate objects on virtual
scene via movement and pointing. Various manipulations
can be added according to their applications because
required hand gestures depend on the virtual scene
contents.

Figure 6 Rotating a desk.

2.5. Adding virtual objects
Occluding the orange block for a given period, a
new virtual object can be generated. The original object
is left and then the new object is ready to handle. The
type of the new object is determined by replacing
operation. Figure 8 shows a shot in adding virtual
objects. Replacing operation includes null object mode,
which indicates that no objects are selected. Thus the
user can pick up the desirable object to handle, bringing
the orange block to it in the null object mode. Figure 9
shows a pickup operation in the mode. Line circle
appears around the closest object to the orange block.

Figure 7 Character motion control.

Figure 8 Add virtual objects.

3. Implementation and evaluation
In this section, we describe implementation details
of our system and performance evaluation of the
effectiveness of it.

3.1. Implementation details
As a prototype system, we have implemented a realtime system that consists of a regular PC with two
ieee1394 digital cameras (we used pointgray research
Flea). The two cameras must be calibrated in advance.
Using two cameras, 3-D positions of the physical blocks
can be simultaneously detected. Real-virtual scene is
rendered by overlaying on the live image. The image is
obtained from one of the vision processing cameras or
the other fixed camera. In the prototype system, video
display devices such as head-mounted displays and
wearable glasses are not necessarily required. In the
prototype system, the work space is limited (400mm x
400mm x 300mm) and has simple background condition.

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

Figure 9 Pickup a virtual object.

3.2. Evaluation tasks
We tried to evaluate our tool, comparing with
mouse-and-keyboard manipulation in a general modeling
tool. Evaluation tasks are defined as follows: the user
allocates virtual objects with the aim of a target layout.
The user can watch a monitor which displays the same
view as one of the vision processing cameras. Concretely,
we have compared the manipulations in our system with
mouse-and-keyboard GUI in a general modeling tool,
lightwave3DTM. Since the GUI based manipulation is
most popular, we present such comparison.
Figure 10 (a)(b) shows two target layouts (Task 1
and Task 2). Task 2 is more difficult to allocate virtual
objects than Task 1 because recognition of the spatial

configuration is not easy (two chairs are kept on a table
or on a desk). Figure 10 (c) shows the final shots in our
manipulations in Task 1 and in Task 2. Figure 10 (d)
shows the final shots in lightwave3D operations.
We used 9 subjects, who are beginners with 3-D
virtual object manipulation. First, we introduced them on
how to manipulate each block. They tried the
manipulation test for several minutes. Next, we
explained the tasks to the subjects giving our manual.
Then, we asked the subjects to perform the two tasks.
The user starts the tasks when no objects are generated in
the work space.
Our system measured elapsed time to perform the
task. Figure 11 and Figure 12 show two graphs with time
to complete the tasks. We found that subjects operated
our system more quickly. We then asked the subject to
fill out a questionnaire about the tasks. We collected
subject's questionnaire answers and comments.

(a) Target layout 1

(b) Target layout 2

(c) Our system (Task1 / Task2)

Concretely, the following questions are given:
• Do you think translating, rotating, scaling and
replacing are easily operated? (Q1-4)
• Do you think our system is easy-to-use to
complete Task 1/Task 2? (Q5-6)
• Do you think our system realizes direct
manipulations? (Q7)
Figure 13 shows the questionnaire results (the
proportion of YES, NO Answer and NO). No answer
implies that the subject did not give the decision. In the
figure, the answer to the first question (includes Q1-Q4)
corresponds to "Translation", "Rotation", "Scaling" and
"Replacement". The second question (Q5-6) is "Task 1"
and "Task 2". The last question is "Direct".
From the results, we found that rotating and scaling
operations were easier. However, it was not clear that
translation and replacement operations were easier.
Several subjects took time to allocate the chairs and to
keep them in Task 2 because 3-D space cognition was
difficult without a 3-D viewing device such as HMDs.
From the subject’s comments, we confirmed that fixed
user’s viewpoint is not suitable for 3-D manipulation.
We have to introduce adaptive viewpoint control
mechanism, employing fixed display.
78% of the subjects answered "yes" to the last
question. Most subjects were attracted to the object
manipulation by physical blocks because they had never
seen such a system. In this paper, we evaluated our
system by using the first impressions for the users, and
then we have to investigate the usability for the
experienced users.

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

(d) General modeling tool (Task 1 / Task2)
Figure 10 Evaluation tasks: (a)(b) two target
layouts, (c)(d) shots in completing the Tasks.

time [ms]
400

our system
general modeler

Task 1

350
300
250
200
150
100
50
0

1

2

3

4

5

6

7

8

9 subject

Figure 11 Elapsed time for Task 1.

time [ms]
400

our system
general modeler

Task 2

0%

350

Translation

300

Rotation

20%

40%

60%

80%

100%

Scaling

250

Replacement

200

Task1

150

Task2

100

Direct

YES
NOA
NO

50
0

1

2

3

4

5

6

7

8

Figure 13 Questionnaire results.

9 subject

Figure 12 Elapsed time for Task 2.

[3]

Ullmar, B. and Ishii H.: Media Blocks: Tangible
interfaces for online media, Extended Abstracts of
CHI’99, 31-32, May 1999.

[4]

Rekimoto, J., Ullmar. B., Oba. H., Data Tiles: A Modular
Platform for Mixed Physical and Graphical Interactions,
in Proceedings of Human Factors in Computing Systems,
CHI’01, 2001.

[5]

H.Suzuki and H Kato. Integration-level support for
collaborative
learning:
AlgoBlock
-An
open
programming language, In Proceedings of CSCL’95,
349-355, 1995.

[6]

H. Kato and M. Billinghurst. Marker tracking and hmd
calibration for a video-based augmented reality
conferencing system. International Workshop on
Augmented Reality, 1999.

[7]

Wii Remote, http://wii.nintendo.com/controller.jsp .

[8]

S. Yonemoto and R. Taniguchi, Human Figure Control
Software for Real-Virtual Application, Proceedings of
8th
International
Conference
on
Information
Visualization, pp.858-862, 2004.

4. Conclusions
In this paper, we presented a virtual object
manipulation tool that can directly control virtual objects
using physical blocks. Physical blocks can be recognized
by vision based motion capturing. Basic manipulations
such as translating and rotating are intuitively performed
by handling one block. In addition to this, complicated
manipulations can also be realized by handling two
blocks with both hands. The remarkable point is that our
interface has the application scalability, because it works
as a kind of motion capture devices. We have
implemented the prototype system and assessed the
effectiveness of the system by experimentally comparing
it with general modeling tool for several subjects. The
results suggest that our object manipulation system can
provide a more intuitive means of handling. We
confirmed that virtual object manipulation using physical
blocks is a solution to reduce the difficulty of 3-D space
cognition. We plan to apply our system for various realvirtual applications. In this paper, though we assume
desktop applications with small blocks, the extend
system, which would be coupled with whole body
motion capture system, can be realized by using larger
objects.

References
[1]

Ben Shneiderman. Designing the user interface strategies for effective human-computer interaction -,
Allyn and Bacon, 2004.

[2]

Ishii, H. and Ullmer, B. Tangible Bits: Towards
Seamless, Interfaces between People, Bits and Atoms, In
Proceeding Conference on Human Factors in Computing
Systems. CHI’97, ACM Press, 234-241. March 1997.

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

