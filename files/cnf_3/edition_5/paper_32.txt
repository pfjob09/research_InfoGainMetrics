Semantic Map Based Web Search Result Visualization
Mohamed Salah Hamdi
University of Qatar
mhamdi@qu.edu.qa
Abstract
The problem of information overload has become
more pressing with the emergence of the increasingly
more popular Internet services. The main information
retrieval mechanisms provided by the prevailing
Internet Web software are based on either keyword
search (e.g., Google and Yahoo) or hypertext browsing
(e.g., Internet Explorer and Netscape). The research
presented in this paper is aimed at providing an
alternative concept-based categorization and search
capability based on a combination of meta-search and
self-organizing maps. Kohonen's self-organizing map
is very well known as a clustering and dimension
reduction tool. Clustering can be used for
categorization of search results. Dimension reduction
can be used for visualization and for reducing
information in order to ease search.

1. Introduction
Search engines rank the retrieved documents in
descending order of relevance to the user’s information
needs according to certain predetermined criteria. The
usual outcome of the ranking process applied by a
search engine is a long list of document titles. The
main drawback of such an approach is that the user is
still required to browse through this long list to select
those that are actually considered to be of interest.
Another shortcoming is that the resultant list of
documents from a search engine does not make
distinctions between the different concepts that may be
present in the query, as the list inevitably has to be
ranked sequentially. As the list of documents grows
longer, the amount of time and effort needed to browse
through the list to look for relevant documents
increases.
What is needed are systems, often referred to as
information customization systems [1] [2] [3] that act
on the user’s behalf and that can rely on existing
information services like search engines that do the
resource-intensive part of the work. These systems will
be sufficiently lightweight to run on an average PC and
serve as personal assistants. Since such an assistant has

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

relatively modest resource requirements it can reside
on an individual user’s machine. If the assistant resides
on the user’s machine, there is no need to turn down
intelligence. The system can have substantial local
intelligence.
We propose an information customization system
that combines meta-search and unsupervised learning.
A meta-search engine simultaneously searches multiple
search engines and returns a single list of results. The
results retrieved by this engine can be highly relevant,
since it is usually grabbing the first items from the
relevancy-ranked list of hits returned by the individual
search engines. The Kohonen Feature Map [4] is then
used to construct a self-organizing semantic map such
that documents of similar contents are placed close to
one another. The goal is conceptualize an information
retrieval approach which uses traditional search
engines as information filters and the semantic map as
a browsing aid.
In general, using neural networks to cluster a
collection of documents represented using term vectors
is computationally expensive because of the high
dimensionality resulting from the large number of
terms in the collection. In the case of a meta-search
engine, however, the high dimensionality problem does
not seem to be a severe issue because the system is
usually grabbing only the first few items from the
relevancy-ranked list of hits returned by the individual
search engines. Hence, the resulting total number of
documents returned will be relatively small. The metasearch engine SOMSE described in this paper, for
example,
returns
maximum
60
documents.
Additionally, as there is no time to download the
original documents off the Web, the system will take
short snippets returned by the individual search engines
as input for clustering. The resulting vocabulary will
therefore be relatively small. Additional operations
applied on the vocabulary terms such as stemming,
stop word removal, and removal of high and low
frequency words will make the vocabulary and hence
the dimensionality still smaller. Thus, we can afford
some more complex processing, which can let us
achieve better results.

2. Self-organizing semantic maps
Kohonen’s feature map algorithm [4] takes a set of
input objects, each represented by a N-dimensional
vector, as input and maps them onto nodes of a twodimensional grid. Initially the components of the
weight vectors assigned to nodes of the twodimensional grid are small random values. They are
adjusted through the following learning process:
1. Select an input vector X(t) = (x1(t), x2(t), …, xN(t))
randomly from the set of all input vectors.
2. Find the winning node s of the grid, i.e., the node
that produces the smallest distance ds = ||X(t) Ws(t)|| = mink ||X(t) - Wk(t)|| where Wk(t) = (wk1 (t),
wk2(t), …, wkN(t)) is the weight vector for node k at
time t.
3. Adjust the weights of the winning node and the
weights of its neighboring nodes in the grid so that
they become still closer to the input vector in the
N-dimensional space as follows: Wk(t+1) = Wk(t)
+ η(t) * (X(t) – Wk(t))
where η(t) is an
error-adjusting (learning) coefficient (0 < η(t) <
1) that decreases over time.
This process goes through many iterations (each
input vector is presented many times) until it
converges, i.e., the adjustments all approach zero. Each
input vector is then mapped to a grid node closest to it
in the N-dimensional space.
The process corresponds to a projection of the input
space onto the two-dimensional grid. The result is
called feature map. Two main properties of such a
feature map are [5]: i) The feature map preserves the
distance relationships between the input data as
faithfully as possible; ii) The feature map allocates
different numbers of nodes to inputs based on their
occurrence frequencies.
Kohonen initially defined the coefficient η(t) over
geographic neighborhoods: at time t, η(t) is a small
constant within a given neighborhood, and 0
elsewhere. A more recent version of the feature map
adopts the Gaussian function to describe the
neighborhood and η(t). In this paper we adopt a
Gaussian function for η(t) similar to that used in [6]:

1

η (t , k , s) = A1 ⋅
e

t
A2

1

⋅
e

t ⋅d ( k , s )
A3

(1)

where d(k,s) is the Euclidian distance between the node
k and the winning node s in the two-dimensional grid.
A1, A2, and A3 are three parameters. In the formula,
the first Gaussian function controls the weight update
speed and the second Gaussian function defines the
neighborhood shrinkage. Note that η(t,k,s) depends on

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

the time t and on the distance of the node k from the
winning node s.

3. More intuitive parameters
Since, in the context of search result clustering, fast
interaction with the user is required, the system should
be able achieve convergence within a training period of
limited length. It is therefore convenient to assume a
training period of a certain given length C, and try to
shape the function η appropriately in order to achieve
convergence within this period. η should therefore be
expressed as a function of C. The best way to do this is
through the parameters A1, A2, and A3. We will
express these 3 parameters as functions of C and 3
other additional parameters that are more intuitive than
A1, A2, and A3, and hence, easier to tune. The 3 new
parameters are:
o ηstart: 0 < ηstart ≤ 1, is the starting value (value at
time t = 0) for η for the winning node s. Note that
the time t goes from 0 to (C-1).
o ηend: 0 < ηend < ηstart, is the final value (value at
time t = (C-1)) for η for the winning node s.
o G2end: 0 < G2end < 1, is the final value (value at
time t = (C-1)) for the second Gaussian function in
equation (1) for a node k that is situated at a
maximum distance from the winning node s. The
maximum distance in the map is the Euclidian
distance between two opposite corners. Note that
the starting value (value at time t = 0) for the
second Gaussian function in equation (1) is equal
to 1 for any node k.
From equation (1), it is clear that at time t = 0,
η(0,k,s) = A1. Hence A1 = ηstart (2).
From equation (1), it also follows that

1

η ((C − 1), s, s) = η end = A1 ⋅
e
Hence, A2 =

(C − 1)

η
ln( start )
η end

( C −1)
A2

⋅1

(3).

Furthermore, according to the definition of G2end we

1

have G 2 end =

e

( C −1)⋅ Dmax
A3

,

where

Dmax

is

the

maximum distance in the map, i.e., the Euclidian
distance between two opposite corners in the map:

Dmax = ( M r − 1) 2 + ( M c − 1) 2 , where Mr is the

number of rows and Mc is the number of columns in

(C − 1) ⋅ Dmax
the map. Consequently, A3 =
(4).
1
ln(
)
G 2 end
With ηstart and ηend it is possible to set the speed
with which η decreases within the learning period of
length C, and hence, control the weight update speed.
With G2end it is possible to set the speed of
neighborhood shrinkage within the learning period of
length C. The speed of neighborhood shrinkage grows
as G2end becomes smaller. Note that at time t = 0, the
neighborhood is the whole map, i.e., the value of η is
the same for all nodes k in the map. Hence, G2start is
always equal to 1.

4. SOMSE
SOMSE (Self-Organizing Meta-Search Engine)
works according to the following general algorithm:
o Get the user query
o Get the collection of search results from the
underlying individual search engines
o Build an inverted index for this collection (from
the snippets)
o Determine the vocabulary (set of unique terms in
the collection)
o Represent the documents (snippets) in the
collection as N-dimensional vectors and use them
as input for the self-organizing feature map
o Train the self-organizing feature map
o Draw the map and make it useful for browsing
SOMSE queries the three most popular search
engines Google, Yahoo, and Msn. By default, SOMSE
is configured to return the first 20 results from Google
(first 2 pages), the first 20 results from Yahoo, and the
first 20 results from Msn. It is possible that there may
be fewer than 60 results returned by the search. This
can also happen when SOMSE automatically removes
identical results from the list. This choice for the
number of results from each search engine is justified
since users do not usually search through pages other
than the first few ones, because most of them consider
these pages irrelevant to their search.
There are two possible modes of clustering Web
search results. The system can either respond in
seconds by clustering the snippets returned by the
underlying search engines, or it can download the
original documents off the Web and cluster them,
requiring more time as downloading the documents can
be quite slow. On the other hand, the clustering quality
of the latter mode is higher since more information is
present. However, the degradation in the quality of the
clusters is usually moderate when snippets are used

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

instead of the original documents [7]. Additionally,
most search engines are well designed to facilitate
users' relevance judgment only by the snippet. We can
therefore assume that the snippet contents are
informative enough. SOMSE therefore performs the
clustering on the returned snippets, allowing fast
interaction with the user.
First an inverted index of the returned collection of
search results is produced. During indexing, stop
words are omitted and some basic stemming rules are
applied [8]. Additionally, the most frequently
occurring words and the least frequently occurring
words are excluded. High frequency words usually
occur in most of the documents and have therefore no
discriminative value. Rare words are omitted on the
argument that they will produce very small clusters.
Removing high and low frequency words will also
reduce the dimensions of the vector space i.e., the size
of input vectors and weight vectors, and hence speed
up processing.
The remaining unique terms (stems) are retained
and used as the set of indexing words (vocabulary) for
the collection of search results. These words and the
documents of the collection form a matrix of
documents versus indexing words, where each column
is a N-dimensional document vector and each row
corresponds to a word (stem) of the vocabulary. A
document vector contains 1 in a given row if the
corresponding word occurs in the document snippet
and 0 otherwise.
The document vectors are used as input to train a
feature map of N features and a two-dimensional grid
of M output nodes (say a 10-by-14 map of 140 nodes).
Following the Kohonen’s algorithm:
o Each feature corresponds to a selected word.
o Each document is an input vector.
o Each node on the map is associated with a vector
of weights which are assigned small random
values at the beginning of training.
o During the training process, a document is
randomly selected, the node closest to it in Ndimensional vector space according to the
Euclidian distance is chosen; the weight of the
node and weights of its neighboring nodes are
adjusted accordingly;
o The training process proceeds iteratively for a
certain number of training cycles. It stops when
the map converges.
o When the training process is completed, submit
each document as input to the trained network
again and assign it to a particular grid node
(concept) in the map.
A semantic map of documents that contains very
rich information is then constructed (see Figure 1). The
map displays on each node a number that indicates the

number of documents mapped to that node. These
numbers collectively reveal the distribution of the
documents on the map. Clicking on a node will cause
the corresponding document list to be shown to the
user and the user can browse through that cluster (see
Figure 2).

to them. Because of the cooperative feature of the
neighboring nodes in the map, the areas are assured of
continuity. Therefore, concept regions that are similar
(conceptually) appear in the same neighborhood.
Similar documents are assigned into same or similar
concepts.
Node0_0
Jaguar US - Home
Jaguar USA Official Home Page. ... Accolades. Build Your XK. Jaguar USA
Home. Build Your Jaguar. Request Brochure. Get Email Updates. Locate a
Dealer ...
http://www.jaguarusa.com/
Source: Google, Yahoo
Jaguar US - Home
Jaguar USA Official Home Page ... Build Your XK. Build Your Jaguar.
Request Brochure. Get Email Updates. Locate a Dealer. Search Site Map
Contact Us Privacy Policy ...
http://www.jaguar.com/us/en/home.htm
Source: Yahoo
Jaguar US - Home
Jaguar USA Official Home Page
http://www.jaguarusa.com/us/en/home.htm
Source: MSN

Figure 2. Cluster of documents associated
with node (0,0) of the map of Figure 1.

5. Some characteristics of the maps
generated by SOMSE

Figure 1. A 10x14 map generated by SOMSE
for the query 'jaguar'.
Of course, it is not enough for a clustering system to
create coherent clusters, but the system must also
convey the contents of the clusters to the users
concisely and accurately. The system is most useful
when the user can decide at a glance whether the
contents of a cluster are of interest.
The map is divided into concept areas (word areas)
or regions. The area to which a node belongs is
determined as follows: compare the node to every unit
vector (containing only a single word), and assign to
the node the nearest unit vector (or the word it
represents). The same effect can also be achieved as
follows: assign a word to each node by choosing the
one corresponding to the largest weight in the weight
vector of the node (winning term).
Neighboring nodes which contain the same winning
terms belong to the same concept/topic region (group,
area). The resulting map thus represents regions of
important terms/concepts with the documents assigned

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

The size of the areas on the map corresponds to the
frequencies of occurrence of the words. Usually, the
word that appears most often in the collection will have
the largest area. The second most frequent word will
have the second or third largest area, etc. For example,
the largest area in the map of Figure 1 is "car", which
by frequency counts (document frequency: DF)
appears most often in the collection (see Table1).
"Inform" appears second most frequently (DF) in the
collection and its area is the third largest. However, as
the mapping is a nonlinear one, the sizes and the
frequencies do not have a linear relationship. There is a
tendency that the large ones look even larger, and the
small ones sometimes simply disappear.
Not only the frequencies of occurrence of the words
but also the frequencies of word co-occurrence
influence the map. Words that more often co-occur
than others will be assigned to neighbor areas. "Car" is
next to "free" and "inform" in the map of Figure 1
because it more often co-occurs with these two terms
than with any others (see Table 2: diagonal numbers
are document frequencies of each stem. ). "xj" is next
to "xk" for the same reason.
Frequency of word co-occurrence visualized on the
map by the neighborhood property of areas may

compensate for inconsistency and incompleteness in
the indexing of documents (see [6] [9] [10]). This is
especially helpful because of the problem that
originates in the fact that snippets contain little to no
redundancy in terms of the information presented in
the snippets as well as in the choice of words. Due to
their limited length and condensed structure, word
repetition and clarification of the most important
aspects within the text are usually not present, resulting
in less specific vector representations of the
documents. Thus using only the snippets provides a
somewhat more challenging task than using the
complete documents.
Table 1. Vocabulary generated by SOMSE
for the query 'jaguar' (map of Figure 1).
NO
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

STEM
Office
Car
Page
Home
Usa
Build
Xk
Dealer
Amp
model
Price
wikipedia
Free
encyclopedia
panthera
World
Inform
Fact
Big
Cat
New
Video
search
Latest
Auto
Xj
Type
Ford
Naia
Part

TF
5
17
5
8
4
4
7
5
6
7
4
5
8
6
4
4
10
4
4
5
5
7
4
4
7
10
9
4
4
5

DF
5
13
5
4
3
2
6
5
3
7
3
5
7
6
2
4
9
4
4
5
5
3
4
4
4
4
4
3
2
2

SIZE_OF_AREA
6
79
0
0
0
0
7
0
0
0
0
0
19
0
0
4
15
0
0
8
0
0
0
0
0
1
0
0
1
0

As there is no time to download the original
documents off the Web, the system should produce
high quality clusters even when it only has access to
the snippets returned by the search engines. This is
assured by the above property of the self-organizing
map. Additionally, using words representing the areas
and neighbor areas on the map, the user can look for
possible combinations of words to form terms relevant
to his request. This allows flexibility and encourages
the user to search for terms that best describe his
request.
The labels themselves aid in identifying the most
important features within every node and thus help to
understand the information represented by a particular
node. In spite of the little redundancy present in
snippets, the labels turn out to be informative in so far

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

as they help the user to understand the map and the set
of search results as such. Especially in cases where
little to no knowledge on the set of search results itself
is available (e.g., when the user is new to a subject area
and doesn't know the key terms) the resulting
representation can lead to tremendous benefits in
understanding the characteristics of the collection of
search results.
Table 2. Co-occurrence data for the stems that
appear as node labels in the map of Figure 1.
0
1
6
12
15
16
19
25
28
NO

Office
Car
Xk
Free
World
Infom
Cat
Xj
Naia
STEM

5
1
2
0
0
0
0
1
0
0

13
2
3
0
4
0
1
0
1

6
0
0
0
0
3
0
6

7
1
2
0
0
0
12

4
0
1
0
0
15

9
2
0
0
16

5
0
0
19

4
0
25

2
28

6. Evaluation of SOMSE maps
The self-organizing semantic map is a technique for
visual display of information that looks to be very
suited for an interactive environment like Web search.
It represents the document space two dimensionally
and provides rich information on the display. It creates
a visual effect through geographic features of the areas
on the map and preserves certain structures of the
document space. It keeps its simplicity even in the case
of a large number of documents.
The map looks to be a good aid to searchers because
it enables users to determine at a glance whether the
contents of a cluster are of interest. It may save the
searcher time and effort in assessing the variety of
possible meanings and aspects of a long list of search
results, and provide quick identification of the clusters
that best match interests. This is especially helpful to
people who are new to a subject area and don't know
the key terms. Additionally, it seems to succeed in
disambiguating words that have multiple meanings
depending on the context such as "jaguar" (Figure 1).
According to [11], one of the central issues
concerning visual display for information retrieval
involves creating the ability to spark understanding,
insight, imagination, and creativity through the use of
graphic representations and arrangements. The
semantic maps generated by SOMSE may help create
such ability.
The evaluation of a clustering interface is
notoriously difficult, particularly in the context of Web
search engines, which are used by a heterogeneous user
population for a wide variety of tasks: from finding a
specific Web document that the user has seen before
and can easily describe, to obtaining an overview of an
unfamiliar topic, to exhaustively examining a large set

of documents on a topic, and more. A clustering
system will prove useful only in a subset of these
cases.
In a first study, we asked 2 human evaluators to
cluster and label the search results returned by SOMSE
as ranked lists (before clustering) for 30 queries (Table
3). We specially selected three types of queries:
ambiguous queries, entity names, and general terms,
since these queries are more likely to contain multiple
sub-topics and will benefit more from clustering search
results. The idea was to consider maps generated by
people and compare them to the maps generated by
SOMSE. The task given to evaluators is to put the
documents (snippets) on the grid based on their
perceived document similarities. It is emphasized that
snippets can be put on any locations of the grid, and
that relative distances among documents are more
important than the locations. Evaluators are told that
the purpose of such a map is to make browsing and
selection of documents from the map easier.
Table 3. Queries used in preliminary study.
Type
Ambiguous queries
Entity names
General terms

Queries
jaguar, apple, Saturn, jobs, jordan, tiger,
trec, ups, quotes, matrix
susan dumais, clinton, iraq, dell, disney,
world war 2, ford
health, yellow pages, maps, flower, music,
chat, games, radio, jokes, graphic design,
resume, time zones, travel

From the results of this experiment, it was clear that
there are both similarities and differences between the
maps generated by SOMSE and the maps generated by
the evaluators. There were also some similarities
between the processes of map generation.
Other preliminary experimental results demonstrate
that we can generate correct clusters with meaningful
short (and hopefully more readable) names, that could
improve users' browsing efficiency through search
results. Also, the time for building the self-organizing
is acceptable (few seconds).
To what extent SOMSE produces coherent clusters,
and if it actually outperforms, in this respect, other
clustering algorithms in the Web search domain is
being currently investigated. We will also further
investigate several other problems on search result
clustering such as the ability of building hierarchical
maps.

7. Conclusions
SOMSE exploits the obvious advantage of the two
dimensional map, namely, the fact that it can be
displayed on a screen and uses it as an interface that
replaces long lists of ranked documents. Since the map
makes underlying structures of the document space
visible, the semantic map interface will likely allow
more efficient browsing and selection of documents

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

from the document space. SOMSE allows the user to
have a different perspective of searching. It gives a
lateral way of looking at the results. With the search
results obtained and the way they are presented, the
users will hopefully get a better idea of what they are
searching for, and hence learn to issue more accurate
queries.

8. References
[1] M.S. Hamdi, "MASACAD: A Multiagent-Based
Approach to Information Customization", IEEE Intelligent
Systems, 21(1), 2006,pp. 60-67.
[2] M.S. Hamdi, "Information Overload and Information
Customization", IEEE Potentials, 25(5), 2006, pp. 9-12.
[3] M.S. Hamdi, "MASACAD: A multi-agent approach to
information customization for the purpose of academic
advising of students", Applied Soft Computing, 7, 2007, pp.
746–771.
[4] T. Kohonen, Self-organization and associative memory,
3rd edition, Springer-Verlag, Berlin, 1989.
[5] H. Ritter, and T. Kohonen, "Self-organizing semantic
maps", Biological Cybernetics, 61, 1989, pp.241-254.
[6] S. Lin, D. Soergel, and G. Marchionini, "A SelfOrganizing Map for Information Retrieval", In Proceedings
of the 14th Annual ACM SIGIR International Conference on
Research and Development in Information Retrieval,
Chicago, Illinois, 1991, pp 262-269.
[7] O. Zamir, and O. Etzioni, "Web document clustering: a
feasibility demonstration", In Proceedings of the 19th
International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR'98), 1998, pp.
46-54.
[8] M.F. Porter, "An algorithm for suffix stripping",
Program, 14(3), 1980, pp. 130-137.
[9] M.C. Mozer, Inductive Information Retrieval Using
Parallel Distributed Computation, Research Report,
University of California at San Diego, CA, June 1984.
[10] R.K. Belew, Adaptive information retrieval: machine
learning in associative networks, Doctoral dissertation,
University of Michigan, 1986.
[11] R.H. Veith, Visual information systems: the power of
graphics and video, G.K. Hall, Boston, Mass., 1988.

