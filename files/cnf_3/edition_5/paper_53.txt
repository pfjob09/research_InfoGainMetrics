Fused Exploration of Temporal Developments and Topical Relationships in
Heterogeneous Data Sets
Vedran Sabol, Michael Granitzer, Wolfgang Kienreich
Know-Center Graz, Austria
vsabol@know-center.at, mgrani@know-center.at, wkien@know-center.at
Abstract
In many application areas which deal with
heterogeneous data sets temporal developments and
topical relationships both play an important role. A
number of visual representations have been developed
which separately address each of these two aspects of
the data. However, a simultaneous analysis of both
aspects is also often required. In this paper we
introduce an approach which utilizes a combination of
visual representations, where some are targeting the
temporal behaviour while others are focusing on
topical relationships. The views are fused in the sense
that they are tightly coupled with respect to the
visualised data set, considered time interval, and user
actions. The resulting user interface enables the user
to simultaneously explore and discover patterns in
both temporal and topical aspects of a heterogeneous
data set. As a proof of concept a demo application was
developed and applied for visualising temporal and
topical patterns extracted from meeting recordings.

1. Introduction
In general data sets are characterized by the
diversity of data types they contain. Topical, temporal
and (geo-)spatial information, wide variety of different
metadata, as well as explicit and implicit relationships
arising from that data are some examples. For each of
these data types a number of different visual
representations have been conceived, each having
different properties targeting the characteristics of the
data type it was designed for. These visual
representations are specialized at
revealing
relationships and patterns only for a single aspect of
the data.
In the past, tools for visualising complex
relationships have been applied on large, static data
sets and focused mostly on a single type of data,
typically topical relationships (i.e. degree of topical
similarity between a number of entities) covered by

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

text documents. Such visual tools gained acceptance in
areas such as news publishing, digital libraries and
document management in general. Other tools were
successfully applied for
revealing temporal
developments (i.e. behaviour of an entity over time
with respect to some activity) but did not address other
aspects of the data (such as the topical relationships).
Data sets have always contained heterogeneous
information: they include content of different
modalities (text, audio, video, numeric data, etc.),
contain rich metadata, address entities of different
types (e.g. persons, organizations, places, etc), and
may posses a pronounced dynamic behaviour. In cases
when the analysis of a data set necessitates considering
more than just a single type of the data, visual tools
capable of simultaneously handling different types of
the data will be required.
This paper describes a work in progress comprising
of several visualisation components, some focusing on
temporal aspects while other focusing on topical
relationships. These components can be flexibly
configured to handle different entity types found in
heterogeneous data sets. The novelty introduced by this
work is an approach for fusing the visual components
into an integrated, temporal-topical visualisation
system. Fusion of components into a unified temporaltopical user interface is achieved through a framework
including:
1. A software architecture for coordinating different
visual components in response to user actions
such as, zooming, selection and filtering;
2. Unified data model which is shared by all
components and includes a representation for
temporal events as well as a high-dimensional
vector space representation for describing highdimensional (typically topical) features.
Fusion of views is based on coordinated multiple
views (CMVs) techniques [12], which in general allow
users to create multiple-view interfaces that, are
customized to their needs, and allow for coordination
of filtering, selecting and navigating over different data

types. However, we have chosen the term “fused
visualisation” because of the particularly tight, predefined integration of the visual components which is
aimed at simultaneous exploration and analysis of only
two distinguished data aspects: the temporal and the
topical aspects.
To illustrate the proposed fused visualisation of
temporal and topical data, the proposed framework was
used to build a demo application which was applied on
data delivered by the MISTRAL system [14].
MISTRAL (Measurable Intelligent and Reliable
Semantic Extraction and Retrieval of Multimedia Data)
is a service-oriented architecture for extraction of
semantic metadata from multimedia data sets. In
particular it is designed for extracting metadata from
meeting videos. However, it must be emphasized that
the proposed fused visualisation of temporal and
topical data is not limited only to data provided by
MISTRAL, but can also be applied on other data sets
which feature both topical and temporal aspects (such
as highly dynamic document repositories).
The rest of the paper is organised as follows: In
chapter 2 related work is discussed. In chapter 3 the
MISTRAL system is briefly introduced. Chapter 4
describes different visual components, while in chapter
5 an approach for fusing those components into an
integrated temporal-topical visualisation is proposed.
In chapter 6 the implemented visualisation framework,
and the demo application built upon it, are described.
Also, the benefits of the fused visualisation are
outlined. Finally, chapter 7 discusses ongoing work as
well as plans for further development.

2. Related Work
A well-known metaphor for data with a pronounced
temporal component is the ThemeRiver [4]
visualisation. For large document collections it
visualises thematic changes over time and puts them
into the context of some external events.
FeretBrowser [16] is a visual tool for browsing
meetings, which also puts it into the category of
temporal-oriented tools. It supports different types of
data such as media, transcripts as well as results of
automated video processing, most prominently speaker
segmentations (i.e. speaking activity).
MacSHAPA [1] is a video analysis tool capable of
for applying a number of sequential data analysis
techniques. It is can annotate, manipulate, and also
visualize data in a number of different ways.
A metaphor suitable for representing complex
relationships, such as those which occur in highdimensional data sets, is the information landscape,
with SPIRE [17] and VxInsight [1] being prominent

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

examples. In this representation objects are positioned
in a 2D layout so that the distance in the layout is a
measure for relatedness.
Fused visualisation approaches attempt to address
several aspects of the data, such as temporal, spatial or
topical aspects, at once. This may happen in a single
visual representation which is capable of visualising
different aspect of the data, or through tight coupling
of a number of different views where each view is
specialised for a single type of data. In the second case
the need arises to coordinate the different views. Snap
Together [12] is an example of such an approach, in
which the users dynamically combines and binds
different visualisations on-the-fly to produce a
customised coordinated user interface.
Oculus GeoTime [6] is an example of a fused geotemporal representation in which the geospatial
information and the temporal developments (using the
z-axis) are both presented within a single 3D view.
In [3] multiple coordinated views are employed for
analysis of geo-referenced, high-dimensional data sets.
Besides a geographical representation and a thematic
map, the tool also offers a parallel coordinate views.
The Idiom project [5] also falls into the group of
geospatial-topical systems as it employs a geo-spatial
view and a thematic landscape for browsing of large
document sets within a single, coordinated interface.

3. MISTRAL
MISTRAL is a service-oriented, multi-modal
system for extracting a variety of semantically relevant
metadata from one media type and integrating it with
concepts extracted from other media types. The role of
the system within this paper is to extract metadata from
recordings of meeting videos and to supply that data to
visualisation components discussed in the rest of the
paper.

3.1. MISTRAL Architecture
The system is composed out of a number of
services, which are subdivided into two different
groups: processing and supporting services. Processing
services define a flow of actions for extracting features
and concepts from the recorded materials (multimedia
data). Supporting services are those which do not offer
any analysis or extraction capabilities, but are
necessary for the system to operate.
Horizontal services begin with uni-modal extraction
units (1) which analyse the multimedia data and extract
feature and concepts separately for each modality.
Multi-modal merging unit (2) integrates uni-modal
extraction results. Semantic enrichment unit (3) applies

an inference model is to check the validity of extracted
metadata. The final extraction results are used by a
number of semantic applications (4). Supporting
services include a storage subsystem for multimedia
data and extraction results, a benchmarking module,
and several GUI and visualisation components.

3.2. Extraction Functionality
Considering technological and resource aspects it is
clear that a completely generic system would be
unrealistic. Therefore, a “Meetings” application
domain was chosen with the goal of focusing the
development of MISTRAL’s extraction algorithms.
The “Meetings” domain defines a scenario showing
one or more persons sitting around a table, discussing
about a subject and using PowerPoint slides for the
presentation. Meetings are recorded on video.
Accompanying documentation (in the form of
PowerPoint and Word documents) is also considered
by the system.
The capabilities of each service are only described
briefly
without
describing
algorithm
and
implementation details as this would be outside of the
scope of this paper. For more details MISTRAL
project’s homepage [10] should be consulted.
• Video Unit: real-time face recognition, face
(position) tracking and activity recognition
(speaking, turning head, etc.).
• Audio Unit: Voice activity detection, speaker
indexing, concurrent (multiple) speaker tracking,
position (angle) estimation, gender recognition.
• “Click” data (sensory data) unit: Capturing user
input (mouse clicks, keystrokes), tagging them
with temporal and application information,
grabbing text data from currently displayed
PowerPoint slide.
• Text unit: Extracting named entities, topics and
concepts from PowerPoint slides, speech-to-text
transcripts, and external documentation. Provides
indexing and searching functionality for text.
• Multi-modal merging unit: Fuses and synchronizes
features extracted from different modalities,
produces combined extraction results.
• Semantic enrichment unit: provides inference
capabilities
for
detecting
and
solving
contradictions and inconsistencies introduced by
extraction units.
• Data access and storage: Integration of units
through a shared MPEG7 data structure. Includes a
benchmarking server and client applications for
data access and annotation.

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

• Semantic applications: Several applications for
retrieving and browsing results provided by
MISTRAL.

4. Visual Components
This chapter describes components for visualizing
topical or temporal data, which are employed to realise
the fused temporal-topical visualisation.

4.1. Temporal Activity View

Figure 1: Temporal Activity View
Temporal activity view visualises presence and
duration of events (i.e. temporal segments) produced
by a number of entities over a defined period of time.
Each row corresponds to a different entity and time is
represented by the x-axis. The view provides of clear
representation of event boundaries (for example
speaking activity), but is not suitable for overlapping
events. In this particular case (Figure 1) the component
is configured to display meeting participants as
entities. Each participant is coded by a different colour,
and each rectangle represents an event, which, in this
case, is an action performed by that person. Events are
primarily periods of speech activity, but may also be
gestures, movements and similar. The width of a
rectangle represents the temporal duration of the
corresponding action, whereby a zero duration event is
be represented by a line. With mouse-over on a
rectangle details on that action are obtained.

4.2. Temporal Intensity View
Temporal intensity view visualises the intensity of
events caused by a number of entities over a defined
period of time. As in the previous example each row
corresponds to a different entity, and time is
represented by the x-axis. The height of the “hills”
indicates the intensity of events, whereby overlapping
events are stacked up and added together to yield
higher “hills”. The view is designed for representing
overlapping events, but is not suitable for exact
representation of event boundaries. In this particular
case (Figure 2) the component was configured to
display “topics of discourse” as entities. These are

predominantly topical clusters, but may also be
extracted persons, organisations or places. Each topic
is coded by a different colour, whereby topics
belonging to the same group (e.g. topical cluster,
person, organisation, etc.) are displayed in different
shades of the same colour.

Figure 2: Temporal Intensity View
Note that when temporal and intensity views are
employed within the same application, and because
they will typically visualise temporal developments for
different types of entities, intensity view uses less
intensive, pastel colours while activity view employs
bright, intensive colours for colour coding.

4.3. Information Landscape View

objects, an information landscape can be computed by
a layout algorithm which produces a 2D distribution of
the object set in such a way that related objects are
placed close to each other, while unrelated objects are
positioned far apart. As a consequence related objects
form clearly discernible groups (or islands). We have
selected a force-directed placement algorithm for
layout computation because of its simplicity and the
possibility to tune its parameters in order to achieve a
layout with desired properties. To produce the map
texture (i.e. background) each object is represented by
a small spherical cap. By stacking the caps over each
other and adding their heights, peaks emerge at points
with highest object density. Details on employed
algorithms can be found in [8] and [13] .

5. Fusion of Temporal and Thematic Views
This chapter provides motivation for, and describes
the principle of fusing the described visual components
into a single user interface for simultaneous
exploration of temporal and topical data.

5.1 Motivation

Figure 3: Information Landscape View
Information landscapes are used to visualise
complex relationships which occur in highdimensional data sets. Typically they are employed to
give insight into thematic relationships for large sets of
documents. However, in this particular case (Figure 3)
the information landscape was instead configured to
visualise relationships between meeting participants
depending on the topics they discussed in a meeting.
To achieve this a high-dimensional term-vector is
constructed for each participant from his cumulated
utterances. As a consequence speakers discussing
about similar topics are placed close to each other,
while those addressing different topics are placed
further apart.
Given a similarity measure (e.g. cosine coefficient)
capable of expressing the relatedness between a pair of

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

While temporal activity and temporal intensity
views are suitable for discovering temporal behaviour
of entities, they can not express relationships between
the entities themselves. On the other hand, information
landscapes, due to the fact that spatial proximity in the
2D layout is a measure for relatedness between objects,
are capable of presenting complex relationships in the
data set. However, temporal behaviour can not be
visualised in a static landscape.
Application domains with data sets which are
characterized by a variety of data types (i.e. different
aspects of data) may resist analysis unless different
aspects of data can be analysed simultaneously.
Therefore, we propose the following approach for
fusing the visual components described in chapter 4.

5.2 Approach
If the landscape view is placed so that the temporal
axis of the temporal view is orthogonal to the
landscape (as in the figure bellow), it is not hard to
imagine the landscape as a slice of a temporal view for
a chosen interval in time.

Time Axis

Non-temporal components are:
• Information Landscape (described in 4.3)
• Colour-coding definition: A tree component used
to specify the colour coding of meeting
participants and of extracted topics and entities.
The described framework, including all logical and
visual components is implemented in Java 1.5 and
makes use of the Swing toolkit, Java2D library and
Java Media Framework (JMF) 2.1.1e.

6.2. Visual Conversation Analysis Tool
Interval defining the width of slice

In this way the landscape view shows relations
between entities which are shown in the temporal
view, but only depending on the data from the selected
time interval. As the user slides the chosen time
interval along the time axis, or modifies the width of
interval, the landscape view updates itself to reflect the
selected subset of the data. Note that the behaviour of
the temporal views is not modified by the fusion.

6. Demo Application
6.1. Architecture
In order to tightly couple and coordinate different
views, a software architecture for synchronizing the
visual components, as well as a shared data model,
which describes both the events (segments) and the
high-dimensional data, were developed along the
principles of multiple coordinated views [12] [1] . The
User Action Manager ensures that all user actions, such
as panning, zooming, selecting and filtering are
coordinated over all registered visual components.
When requested by a component Data Manager fetches
the data from the back end (in this case MISTRAL data
storage subsystem) and supplies the components with
shared data structures.
Besides the already described visual components
some additional views are also integrated in the
framework. All of them can be subdivided into
temporal and non-temporal components. Temporal
components are:
• Activity View (described in 4.1).
• Intensity View (described in 4.2).
• Components for temporal visualisation of icons
(i.e. image thumbnails), such as PowerPoint slides
and captured video frames.
• Interval selection bar: Selection of the time
interval, navigation back and forth in time.
• A search result bar: Displays found events spread
along the time axis.
• Video player: Plays meeting videos.

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

The Visual Conversation Analysis (VCA) tool
(Figure 4) was built upon the described framework to
demonstrate the proposed fused analysis of temporal
and topical aspects of meeting data. It is designed for
offering answers to questions such as: who talks about
which topics; who engages in discussions with whom;
who is inactive; which participants have similar
interests; which topics cause heaviest discussions; who
has similar/different views during such discussions.
The GUI is organised as follows: On the left hand
side there is a video player for reproducing the
materials recorded during the meetings. Under the
player there is a tree component which is used to
specify the colour coding of meeting participants and
of extracted topics and entities. Meeting participant are
colour-coded by bright, intensive colours. Extracted
topics and entities are encoded by less intensive, pastel
colours for better differentiation.
On the right hand side all components have shared
time axis (the x-axis) which flows from left to right.
On the top-right there is an interval selection bar which
is used for selecting the time interval which is
considered by the visualisation components (actually
offering a temporal zooming function). All
visualisation components, including the landscape
view, display only data within the currently chosen
time interval. For moving back and forth in time the
user can slide the interval bar, whereby all temporal
visualisation components will smoothly scroll to the
new temporal position. Under the already described
activity and intensity views (both placed bellow the
interval selection bar) there are two instances of a
component for temporal thumbnail visualisation, the
upper one visualising PowerPoint slide transitions, the
lower one showing the storyboard.
There is also a simple search facility for searching
after events. Found results are displayed in the search
result bar (just underneath the interval selection bar) as
rectangles occupying their respective temporal
positions (a mouse-over effect caused one found event
to be highlighted in red). Clicking on a result will

trigger a smooth, animated navigation to the time
interval covered by that event.

Introduction of a temporal dependency view is also
being considered. This component should explicitly
visualise dependencies and causal relationships
between events. Last but not least are numerous tweaks
and improvements of visual appeal which are
continuously integrated into all visual components.

7.2. Alternative application Domains

Figure 4: Visual Communication Analysis Tool
Although the information landscape is coordinated
with other views, is not yet integrated into the VCA
tool in a satisfactory manner: due to technical
considerations (it is imported from a different
visualisation framework [8] ) the landscape view is
currently shown in a separate window. Another
shortcoming is that currently, while other components
adjust themselves smoothly while the user is
navigating in time, the landscape performs a single,
somewhat erratic update once the user stops
navigating.
In this application the proposed “fused” approach
proved to be integrative in the sense that different
aspects of the data (i.e. temporal and topical) can be
explored and analysed simultaneously, and that
relations and patterns not only within each aspect but
also between the aspects can be understood. These are
for example:
• Temporal patterns in communication activities of
meeting participants.
• Patterns in temporal development of topics.
• Topical relationships between participants
depending on their cumulated utterances.
• Relationships between different extracted topics.

7. Future Work
7.1. Improvements of Visual Components
The highest priority on the to-do list is tighter
integration of the thematic landscape with the rest of
the framework. In addition to this, we are looking at
possibilities of providing smooth, animated transitions
in the landscape view when the user navigates along
the time axis. This poses a substantial challenge due to
a very intensive processing involved, even if the
number of entities is relatively small.

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

The fused temporal-topical visualisation can also be
applied on application domains, other than meetings,
which posses a highly dynamic character. So far we
have identified two application areas, which provide
interesting opportunities for fused temporal-topical
visual analysis:
1. Social networks in the context of Web 2.0:
Newsgroups and discussion forums, blogs, wikis,
instant messaging, email traffic, and similar
services, are all sources of lively communication
and information exchange, and therefore potential
application targets.
2. Dynamic document repositories: In the news
publishing sector, the diffusion and resonance of
news, as well as effects of the published
information (for example on stock prices) could be
examined. Visual identification of new topics and
trends, “hotness” of topics, and relations of topics
to external events could be of interest for analysis
of scientific literature.
Currently our efforts are directed towards the
second application domain and work is underway to
apply the fused visualisation on highly dynamic,
metadata-enriched document repositories.

7.3. Evaluation
Due to the fact that the presented visual
representations and tools are still in development, we
did not yet evaluate their performance by conducting
user tests. However, we have been collecting feedback
periodically during the project by presenting mock-ups
and prototype implementations to groups of experts at
different stages of development. For example: we
received feedback on layout of components and
orientation of the time axis; a component displaying
the seating plan was cancelled, instead a temporal
dependency view should be introduced (see 7.1); a 3D
display in which the activity and intensity view were
orthogonal to each other was rejected because reading
of time was difficult; the fused temporal-topical
visualisation approach generally received positive
feedback.
Results of these heuristic evaluations were
subsequently incorporated into the improved versions

of visual components. Performing of extensive user
tests is planned for a later time point, when the
prototype reaches a more mature state, and when
integration of all components is completed.

Conclusions

[4]

[5]
[6]

In this paper we have introduced an approach for
combining several visual representations, with the goal
of fusing temporal and topical analysis of
heterogeneous data sets. As a proof-of-concept a demo
application was developed and applied on data
extracted form meeting videos. In our opinion, the
proposed approach is capable of addressing
simultaneous analysis of temporal and topical aspects
of the data. As this work is still an ongoing effort,
some shortcomings could be identified and, based on
this, guidelines for future work were proposed.

[7]
[8]

Acknowledgements

[12]

Presented results have been partly developed in the
MISTRAL project. MISTRAL is financed by the
Austrian
Research
Promotion
Agency
(http://www.ffg.at) within the strategic objective FITIT under the project contract number 809264/9338.
We gratefully acknowledge the following resources for
meeting data provision: The AMI Meeting Corpus,
which has been released publicly under the Creative
Commons Attribution NonCommercial ShareAlike 2.5
License. For further information see also [7] and [11] .
We also gratefully acknowledge Helmut Mader for
contributing the Information Landscape View.
Supervised by Christian Gütl he is currently working
on his Master Theses [8] on multi-dimensional
information visualisation.

[9]

[10]
[11]

[13]

[14]

[15]

[16]

References
[1]

[2]

[3]

Boukhelifa, N., Roberts, J. C., Rodgers, P. A
Coordination Model for Exploratory Multi-View
Visualization. In Proceedings of the International
Conference on Coordinated and Multiple Views in
Exploratory Visualization (CMV 2003), p. 76-85.
IEEE, 2003.
Boyack, K. W., Wylie, B. N., & Davidson, G. S.,
Domain visualization using VxInsight for science and
technology management, Journal of the American
Society for Information Science and Technology,
53(9), 764-774. 2002.
Brodbeck, D., and Girardin, L., Design Study: Using
Multiple Coordinated Views to Analyse Georeferenced Highdimensional Datasets, In Proceedings
of the Coordinated & Multiple Views in Exploratory
Visualization (CMV’03). IEEE, 2003.

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

[17]

Havre, S.; Hetzler, E.; Whitney, P.; Nowell, L.,
ThemeRiver: visualizing thematic changes in large
document collections, IEEE Transactions on
Visualization and Computer Graphics, Volume 8,
Issue 1, Page(s):9 – 20, Jan/Mar 2002.
IDIOM Project url: http://www.idiom.at
Kapler, T., W. Wright, GeoTime Information
Visualization, IEEE InfoVis, 2004.
M4 Project url: http://www.m4project.org
Mader, H., Visualizing Multidimensional Metadata Development of the Visualization Framework
MD2VS, Master’s Thesis at Graz University of
Technology, 2007.
I. McCowan, S. Bengio, D. Gatica-Perez, G. Lathoud,
F. Monay, D. Moore, P. Wellner, and H. Bourlard,
"Modeling Human Interaction in Meetings", in Proc.
IEEE Int. Conf. on Acoustics, Speech and Signal
Processing (ICASSP), Hong Kong, April 2003.
MISTRAL url: http://www.mistral-project.at
MultiModal
Media
File
Server
url:
http://www.idiap.ch/mmm/
North,
C.,
Shneiderman,
B.,
Snap-together
visualization: Coordinating multiple views to explore
information. Technical Report CS-TR-4020, University
of Maryland Computer Science Department, 1999.
Sabol
V., Visualisation
Islands: Interactive
Visualisation and Clustering of Search Result Sets,
Master’s Thesis at Graz University of Technology,
2001.
Sabol, V., Granitzer, M., Tochtermann, K., Sarka W.:
MISTRAL – Measurable, Intelligent and Reliable
Semantic Extraction and Retrieval of Multimedia Data.
2nd European Workshop on the Integration of
Knowledge, Semantic and Digital Media Technologies,
London, UK, 2005.
Sanderson, P., Scott, J., Johnston, T., Mainzer, J.,
Watanade, L., James, J. Macshapa and the enterprise of
exploratory sequential data analysis (esda).
International Journal of Human-Computer Studies,
41(5):633–681, 1994.
Wellner, P., Flynn M., & Guillemot M., Browsing
Recorded Meetings With Ferret, MLMI'04 Workshop,
Martigny, Switzerland, June 2004.
Wise, J.A., J.R., Thomas, J.J., Pennock, K., Lantrip,
D., Pottier, M., Schur, A., Crow, V., Visualizing the
non-visual: spatial analysis and interaction with
information from text documents, In Proceedings,
Information Visualization, October 30-31, 1995,
Atlanta, Georgia, USA. IEEE Computer Society Press,
Los Alamitos, California, 1995.

