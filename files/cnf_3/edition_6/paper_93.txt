Building Reactive and Emotionally Responsive Virtual Children
for Edutainment and Interactive Art
Barnabás Takács
SZTAKI Budapest, Hungary / Digital Elite Inc., Los Angeles, USA
Btakacs@sztaki.hu
Abstract
This paper presents the design, architecture and
implementation of an interactive virtual child
animated in real-time using low-cost and portable
computers. The system was created for edutainment,
i.e. using entertainment and multi-modal
interactivity to help learning in a virtual reality
environment. We advance the state-of-the-art by
using a novel model for emotional modulation
implemented in the form of closed-loop dialogues.
Using the above techniques we demonstrate the
capabilities of our solution by means of real-world
examples of four separate applications fields,
namely rehabilitation, education, interactive art and
mobile telephony in popular culture.

1. Introduction
To mimic the quality of everyday human communication,
future computer interfaces in general, and interactive
entertainment system in particular must combine the
benefits of high visual fidelity animated human agents
with conversational intelligence and the ability to
modulate the emotions of their users in a personalized
manner [14,17,18]. During the past several decades,
researchers have conducted countless studies on
conversational agents [5,6,10, 11-14,17,18] and human
animation [1,2,4,11,13,14] in order to create a humanoid
interface that works by utilizing the natural means of
interaction, e.g. words, gestures, glances and body
language instead of traditional computer devices, such as
the keyboard and mouse.
From a computer science and interface design perspective
one way to fulfill these requirements requires the ability
to create believable digital humans capable of expressing
the finest shades of emotions in a controllable manner
[16-18]. In addition, the same system must also be able
to read the users’ emotional reactions and adapt the
behavior of the digital human accordingly in real-time.
For the past years we have been implementing this
concept as a step towards creating a novel interactive
edutainment system called the Virtual Human Interface

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

[14,16,17,18]. Our solution builds upon many years of
interdisciplinary research to create a closed-loop model of
interaction [17,18] whereas the user’s internal state
(emotion, level of attention, etc.) is constantly monitored
and driven directly by the animated character with the
purpose of creating emotional bonding [8,9,14,17,18].
This emotional bonding then not only entertains but acts
as a catalyst to help turning information into knowledge.
In other words, our advanced user interface draws on
emotions to help its users (most frequently students) in
the learning process in a playful and exciting manner by
intelligently tailoring its workload and constantly
adapting its presentation strategies, hence its name
affective intelligence [14,17]. We argue that photorealistic virtual humans due to their similarities to their
real-life counterparts make a powerful affective interface
and as such they will likely and quickly become the
primary means of communication between computers and
humans for future decades to come. Furthermore, they
represent “The Face of a New Generation” whereas the
boundary between entertainment, learning, special
education and many more new interactive experiences
stem from a single integrated system. In this paper we
elaborate on the basic model and advance the state-of-the
art by showing how such an architecture can be used to
address real-world interaction tasks. Specifically, we
created an interactive virtual child capable of expressing a
broad range of non-verbal reactions and use its repertoire
to interact with people via perceptual channels, such as
vision and tactile interaction. We used this virtual child to
implement many different applications demanding realtime interaction, game-like interfaces and high visual
fidelity.

2. Interactive Animated Agents
The research field of Animated Conversational Agents
[3,5,7-9] focuses on creating autonomous agents and
virtual characters which exhibit rich personalities, and
interact in their virtual worlds inhabited by other
characters or directly with the user. To achieve this goal
various layers of meta-communication, and non-verbal
signals need to be modeled and delivered in an automatic
fashion in conjunction and synchronized with what the
virtual character speaks and acts. To generate these

signals and provide an illusion of a life-like characters,
detailed emotional and personality models need to be
developed that are capable of controlling the multitudes
of animation channels as a function of the virtual human’s
personality, mood and momentary emotions.
The real-time delivery and interaction with these virtual
characters poses an extra set of technical challenges both
in terms of the speed, computational power and most
importantly the visual quality required to make the user
believe that he/she is interacting with a living creature. To
achieve this goal pre-crafted animation graphs and
animated actions may be replaced with intelligent
behavior modules that attempted to control speech,
locomotion, gaze, blinks, head- and body gestures
(including various postures) and object manipulations to
interact with the environment [3,8,9]. The following
chain of logic explains how the ongoing revolution in
real-time computing, interaction, and advanced virtual
human technology leads to a broad range of novel
applications:
x Interaction: Input from the user prompts the character
to generate actions. These actions may be implemented
as scripted reactions, but for full immersion and
believability they need to be generated on-the-fly
leading to some form of autonomous behavior >>.
x Behavior: Artificial intelligence and artificial emotion
techniques are necessary to create the reactions of a
virtual human to its changing dynamic environment
including input from the user. To simulate reactiveness
the digital character is governed by the “stimuli” it
receives from its environment in the form of digital
perception >>.
Much like natural humans digital
x Perception:
characters possess multiple sensory channels to
perceive their environment. These channels primarily
include vision (web camera), hearing (microphone) and
tactile input (collisions, external forces, touch screen).
Based on these modalities the virtual human is capable
of recognizing emotions and gauging the attentive state
or mood of the user and generate its own responses
accordingly. Thus, interaction leads to a closed loop
dialogue >>.
x Closed-loop dialogue: Having a dialogue with a digital
human assumes it has a personality designed
specifically to meet the needs of the application in
hand. Personality - as well as moods, emotions and
intentions - is primarily communicated towards the user
via non-verbal signs also called body language >>.
x Body language: Body language speaks volumes
regarding the internal state, personality and intent of a
virtual human. It comprises of subtle events requiring
high fidelity and frequently subliminal motion data that
is difficult to hand animate or even capture. In addition

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

culture specific aspects including conventions for hand
signals and socially governed behaviors also needs to
be layered seamlessly on top of functional motion such
as the actions of the digital characters and/or speech.
The timing and quality of these layered and integrated
motions ultimately affect the virtual human’s
believability >>.
x Believability: Life-like virtual human characters hold
the promise of blurring the boundary between real and
virtual by being able to create interactive experiences
similar to those between two people. For this the notion
of presence and believability are of critical importance.
The believability of a digital human (to pass as another
person) largely depends on the variability and
consistency of its responses, the speed of feedback and
reactions it provides and ultimately its geometric
resolution and fine details determining visual fidelity
>>.
x Visual fidelity: The new generation of hardware
supported pixel- and vertex shaders offer photorealistic skin, fur, hair and dynamic cloth simulation.
Secondary surface deformations such as micro
expressions, involuntary muscle twitches and low-level
reflexes can be combined with texture effects for
blushing, goose bumps or the glinting eyes of a
character in love while global illumination algorithms
place these virtual characters in fully realistic digital 3D
environments. All the above, today are available in the
form of high performance graphics cards and sold at a
low cost >>.
x Low cost: The broad accessibility of consumer level
graphics hardware and the software tools that aid easy
content creation for interactive animated virtual human
characters bring new ideas, market niches and business
opportunities. These niches draw investment and spur
yet unimaginable wealth of novel applications >>.
x Novel applications will bring a renaissance period for
virtual humans and establish them as part of our
everyday lives in such areas as edutainment, personal
reality, cyber training, and various medical fields, to
mention a few.
The chain of logic above demonstrates how complex the
problem domain and the design/implementation of a
virtual human interface system becomes. In the following
sections we describe our solution to address these
difficulties.

3. The Virtual Human Interface (VHI)
Entertainment systems based on Virtual Reality (VR)
offer seemingly endless possibilities to create novel and
engaging context and utilize the power of interaction and
game-play. Yet to date, there are practically no low-cost

commercially available solutions that bring this
technology to the realm of the general public or even the
class room. Therefore our goal was to set out and design a
system fulfilling this need and deliver real-time and
interactive multi-modal experiences that involve all
human senses. To achieve our goal we developed an
open-architecture platform, called the Virtual Human
Interface or VHI. The system comprises of several stateof-the-art key modules, as shown in Figure 1. Each of
these modules play a vital part in the process of
edutainment and the resulting interactive experience it
yields. We devised a general architecture that can be
freely and easily configured to address the various needs
of a large variety of virtual-reality-based applications.
The principal modules are as follows:

Figure 1. Key modules of the Virtual Human Interface
used for virtual reality entertainment.
1) Portable Virtual-Reality Platform: The VHI was
implemented to take advantage and always keep up with
the latest advances in low-cost consumer-level graphics
hardware and computer platforms. Despite the
requirements for high visual fidelity virtual environments,
media content, I/O devices, real-time vision, animated
virtual humans and many more features, the system runs
on an average portable platform. Portability is one of the
key factors in making VR accessible to many users.
One of the
2) High Fidelity Digital Humans:
cornerstones of the VHI system is its capability to model,
animate and render highly realistic virtual humans.
Building these photo-real virtual humans used for VRbased tutoring is a complex process that involves detailed
skeletal modeling, body construction with underlying
muscle deformation systems, hand gesture libraries, facial
animation, hair and cloth which is added later. These
virtual humans are used as the primary means to modulate
the user’s emotions via non-verbal feedback, such as
facial expressions, body gestures and secondary signs of

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

emotional displays. The actions and reactions of these
characters are a generated dynamically as a combination
of produced content (e.g. motion patterns and) and
reactive behavior layered on top of each other. The
perceptual capabilities of the VHI allow the algorithmic
control mechanisms we implemented to create a
believable character who appears to be paying attention to
the user who is there to help him or her guide through the
interactive learning or entertainment process.
3) Real-Time Panoramic Video, Compositing and
Augmented Reality: Depending of the exact needs of an
interactive application users set out to experience and
navigate in a Virtual Environment (VE) in which
seamlessly combines panoramic video elements with high
resolution 3-D models. To achieve the highest possible
level of fidelity and therefore immersion the virtual scene
is divided into two distinctive regions namely the
background containing elements in weak perspective, and
the foreground which holds visual objects close enough
to the user and therefore represented under strong
perspective transformations.
The background is
implemented as a spherical 360o panoramic video or
image. Foreground elements, on the other hand, are
synthetic 3D objects that appear under strong perspective
conditions.
4) Physical Simulation: Synthetic 3D digital objects
have physical properties, can move realistically when
interacted with using rigid-body dynamics and may be
subjected to environmental effects such as fog, fire,
smoke implemented via a built-in particle systems. In
addition, 3D speakers may be placed in the virtual scene
that can help guide the user in a specific direction. The
overall result is a unique blend of visual and acoustic
elements that create a truly immersive environment
capable of inducing emotions and reactions in a
controlled and programmable manner.
5) Low-Cost Input/Output Devices: To implement a
novel interaction model what we call closed-loop
dialogue, the VHI is continuously monitoring the users’
internal state by means of external sensors and input
devices. It then reacts to these measurements via a variety
of output channels. The input devices we interfaced with
range from simple web cameras and microphones to
capture video to more advanced devices such as an eye
tracker, biofeedback, data glove, touch screen, head
motion trackers, etc. During the design phase of the VHI
special emphasis was placed on integrating only low-cost
and affordable sensors (typically supplied by the
computer game development community) in order not to
limit the general usability of the system. The role of the
camera is to enable visual perception, i.e. vision, and
primarily to analyze facial actions (see below) of the user.

6) Virtual Studio Module: To further bridge the gap
between the synthetic environment and the real world of
the user the VHI contains a specialized image processing
module which works using the principles of virtual
studios. This functionality either allows another person to
enter the virtual world the user is experiencing, or
alternatively place the user himself or herself directly into
the virtual space.
7) Advanced Computer Vision and Facial Analysis:
Web cameras offer the least intrusive and perhaps the
most powerful means to understand a user’s behavior.
Therefore the VHI contains a special purpose computer
vision module that implements a multitude of image
processing and pattern recognition functions tailored
specifically to analyze faces and facial expressions. Using
this information the system attempts to estimate the
internal state, such as level of attention, excitement or
fatigue, as it relates to the interactive process and helps
learning.
8) Motion Capture (MOCAP) Support: Another special
purpose module implemented as a part of the vision
subsystem is a desktop motion capture solution which
employs low-cost USB cameras and reflective markers to
track the motion and deformation of objects. Its primary
goal is to provide an easy interface between physical
objects the user may hold in hand, and their virtual
counterparts representing them in the VE space.
9) Open Scripting Interfaces and External MIDI
Control: Finally, the VHI system was designed as a
completely open architecture to supporting the needs of
many different projects. It contains multiple levels of
scripting (TCL/TK, LUA, HTML, XML) can be
controlled in a client-server mode using TCP/IP or UDP
protocol, and may accept MIDI formatted data streams
allowing its visual, acoustic and olfactory output to be
controlled from musical instruments or a simple
keyboard.
Perhaps the most challenging aspect of this process is the
need to model faces that can express the mimics and
subtle nuances and/or involuntary reactions characteristic
of human non-verbal communication. To bring the ability
of expressing subtleties (micro-expressions) and
secondary signs of internal state, such blushing as a sign
of embarrassment, glinting eyes for excitement, or
slightly sweating forehead as a mark of nervousness, etc.
we take advantage of graphics hardware capabilities in
the form of pixel- and vertex shading technology.
Specifically, secondary non-verbal cues are implemented
via dynamic texture maps. As an example, blushing, is
visualized by blending multiple region-based redness
maps into a single blush map to modify the appearance of
the character. The weight of combining these maps is a

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

function of the virtual child’s internal state and can be
used to express embarrassment, anger, concern, etc.
Similar methods may be used to change the reflective
properties of the skin in order to create the effect of a
sweat layer, also to indicate the high stress or anxiety in
our virtual character. Another example of the
implementation of subtle non-verbal cues is to increase
the expressiveness of the eyes. The size of the pupils have
been linked to levels of interest and attention. Large
pupils indicate high levels of interest and are generally
experienced as warm, friendly and open. On the other
hand, small pupils signal a withdrawn character who is
not to be trusted. Figure 2 demonstrates this effect with
the large and dilated pupils shown on the left and the
small pupils on the right. Finally, the cornea is also an
important channel of visual feedback. Depending of its
reflectivity it may be used to show dry eyes or eyes in
tears. This effect can be achieved using dynamic texture
maps which feed the live video image captured by the
web camera amounted facing the user directly into the
environment map the shading algorithm uses (Figure 3).

Figure 2. Modulating pupil size to express interest,
attention, trust and friendliness (see text).

Figure 3. The cornea is used to reflect the user’s image

4. Practical Applications
Using the modeling pipeline and the real-time delivery
architecture of the Virtual Human Interface system we
created a virtual child capable of sensing its user and
reacting to accordingly. Specifically, we enabled the
system with the ability to see and track faces and also, by
means of a touch-screen, supported direct tactile
interaction, i.e. the face of the virtual child can be
touched. We deployed our solution in four different
scenarios. The first one, shown in Figure 4, was
developed to help autistic children. In this application the
autistic child takes a gentle, slow movement ride while a
virtual friend “Buddy” (shown on the right) is displayed
on the built-in monitor screen. A hidden eye tracker and

video camera are placed to measure and train a child’s
social attention in order to help its healthy development.
The behavior reinforced in this learning environment is
gaze, in particular looking at the informative area of the
animated face from which communicative, emotional as
well as gender and age information can be obtained.
Figure 5 demonstrates some of the many faces the virtual
child can express. First order expressions include facial
shape changes caused by the underlying musculature and
controlled easily via Temporal Disc Controller (TDC)
interfaces (see above). Secondary effects include pupil
dilation, look at targets for the eyes (the user, camera, or
other objects in the scene), and the ability to dynamically
change skin tones and reflection parameters for the
cornea as a function of emotional state (blushing, slight
perspiration, glinting eyes, etc.). Finally, animations can
be rendered out to create streamable video snippets
thereby allowing small, animated reactions to be included
for home-based care and applications over the Internet
[17].

“FacePlay” utilizes the baby face model and creates an
interactive experience using the VHI’s vision system and
touch screen capabilities. Its goal is to demonstrate this
new kind of user interface in a playful manner for
children and adults alike.

Figure 5. Facial expressions of the virtual child.

Figure 6. Implementation of the BabyTeach program
to help students with learning.

Figure 4. Virtual “Buddy” for autistic children.
In the second application the same virtual child has been
integrated into interactive learning environment, called
“BabyTeach”. The BabyTeach program package builds
upon the emotional modulation and non-verbal feedback
techniques implemented in the Virtual Human Interface
to teach geography and help students of all ages to take
tests.
The system operates by prompting a series of questions
and asking the user to indicate the correct answer with
moving a pointer using the mouse. The animated face
appears in the upper left hand corner and expresses
encouraging or disappointed facial expressions according
to the answers. As an example, when the mouse moves
toward the proper answer it gently leans forward or nods
as a form of acknowledgment. Once a selection is made,
immediate feedback regarding the choice is displayed as a
smile or tweaking mouth. Figure 6 demonstrates this
process by showing a test map of Europe. During learning
the student may make many attempts while the baby’s
face gently guides him or her towards the correct answer.
Apart from the positive effects of emotional modulation
on learning, this application also involves the motor
system thereby further helping to memorize spatial
locations.
Thirdly, we designed and implemented a playful and
artistic interactive experience. This application, called

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

At the center of this application is our virtual child’s face
that reacts to the visitors with its elaborate repertoire of
facial expressions. Here the VHI system is configured to
use two cameras placed at different heights. The first one
is at the same level as that of a visitor child’s head sitting
on top of the monitor showing the life size digital face.
The second one is located higher to be able to collect
images from the accompanying adults. The system runs
face recognition algorithms on both of these image
streams to determine not only if there are users to interact
with, but also whether it is a child alone, with an
accompanying adult or just a grown up standing in front
of the installation. Based on this information the VHI uses
different strategies (e.g. child like behavior or more adult
actions) to address the people who are standing in front of
it. The facial expressions of the digital character are
linked to the faces as they move in within the field of
view of the cameras. This from of non-verbal feedback
based on visual perception is supplemented by the tactile
subsystem in the form of a touch screen located in front
of the virtual face. This latter type of interaction is
demonstrated in Figure 7. As the user reaches and touches
the virtual baby’s face, it notices when doing so and
reacts with head motion, facial expressions and by
simulating “face painting” with changing the color and
properties of the underlying skin.
Finally, our virtual child also exists on mobile phones.
The application shown in Figure 8 uses a Java-version of
our VHI player and a mobile phone to deliver funny and
entertaining animated faces over 3G digital mobile

network. Users can set the facial expressions of the
virtual child using the keys of the phone and create
animated sequences which subsequently can be
transmitted via MMS or sent to a central server for
distribution.
The above examples serve to demonstrate the versatility
and and flexibility of the VHI system. Its modular
architecture and multiple input/output-modalities offer a
unique platform building upon which it is easy to
implement a broad range of interactive entertainment and
educational applications.

Figure 7. Implementation of a face painting interface
using the tactile subsystem of the VHI.

Figure 8. Virtual Baby on a mobile phone.

5. Conclusion
In this paper we introduced a novel application of
animated and photo-realistic virtual humans for real-time
and interactive entertainment, education and training. We
described in detail a general architecture, called the
Virtual Human Interface, to support multi-modal virtual
reality experiences on a portable platform. Using this
system we implemented a closed-loop interaction model
complete with emotional modulation aimed to maximize
the emotional engagement of the user. Built upon the VHI
architecture and using our virtual child, we created four
practical applications that have been successfully used in
interactive entertainment, education, rehabilitation and
even pop-culture.

6. References
[1]Badler, N.I., C.B. Phillips, B.N. Webber (1993),
Simulating Humans: Computer Graphics Animation and
Control, Oxford University Press.
[2] Badler, N.I, M.S. Palmer, R. Bindiganavale (1999),
“Animation Control for Real-time Virtual Humans”,
Communications of the ACM, 42(8).

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

[3] Cassell, J., Vilhjálmsson, H., Bickmore, T.(2001)
“BEAT: the Behavior Expression Animation Toolkit”, in
Proceedings of SIGGRAPH '01, 477-486.
[4] Digital Elite Inc. (2005), www.digitalElite.net.
[5] Hendrix, J., Zs. Ruttkay, P. ten Hagen, H. Noot, A.
Lelievre, B. de Ruiter (2000), “A Facial Repertoire for
Avatars”, in Proc. of the W.p on Interacting Agents.
[6] Johnson, W.L. (2003), “Interaction Tactics for Socially
Intelligent Pedagogical Agents”, Intelligent User
Interfaces, pp. 251-253.
[7] Kapoor, A, S. Mota, R.W. Picard (2001), “Towards a
Learning Companion that Recognizes Affect”, Proc. of
Emotional and Intelligent Agents II: The Tangled Knot of
Social Cognition, AAAI Fall Symposium.
[8] Kiss, B., B. Benedek, G. Szijarto, B. Takacs (2004),
“Believable Intelligent Agent: Face-to-Face Interaction
with a Digital Human in a Virtual Environment”,
C.European Intl. Multimedia and VR Conference.
[9] Kiss, B., B. Benedek, G. Szijarto, B. Takacs (2004),
“Closed Loop Dialog Model of Face-to-Face
Communication with a Photo-Real Virtual Human”, SPIE
Electronic Imaging Visual Communications and Image
Processing, San Jose, California.
[10] Kort, B., R. Reilly, R.W. Picard (2001), “An
Affective Model of Interplay Between Emotions and
Learning: Reengineering Educational Pedagogy –
Building a Learning Companion” in Proc. Intl. Conf. on
Advanced Learning Technologies.
[11] Marsella, S. J. Gratch (2002), “A Step Toward
Irrationality: Using Emotion to Change Belief” in Proc.
Of 1st Intl. Conf. on Autonomous Agents and Multi-agent
Systems, Bologna, Italy.
[12] Pasquariello, S., C. Pelachaud (2001), “Greta: A
Simple Facial Animation Engine”, 6th Online World
Conference on Soft Computing in Industrial Appications,
Session on Soft Computing for Intelligent 3D Agents.
[13] Rickel, J., S. Marsella, J. Gratch, R. Hill, D.R. Traum
and W.R. Swartout (2002), “Toward a New Generation
of Virtual Humans for Interactive Experiences”. IEEE
Intelligent Systems, 17(4):pp.32-38.
[14] Ruttkay, Zs., C. Pelachaud (Eds.) (2004), From
Brows to Trust: Evaluating Embodied Conversational
Agents, Kluwer’s Human-Computer Interaction Series
[15] Schlossberg, H. (1952), “The Description of Facial
Expressions in Terms of Two Dimensions” in, J of
Experimental Psychology, 44(4).
[16] Takacs, B. and B. Kiss (2003), “Virtual Human
Interface: a Photo-realistic Digital Human”, IEEE
Computer Graphics and Applications, 23(5):pp.38-45.
[17] Takacs B. (2005), “Special Education and
Rehabilitation: Teaching and Healing with Interactive
Graphics”, IEEE Computer Graphics and Applications,
25(5): pp.40-48.
[18] Takacs B. (2005), “Affective Intelligence: A Novel
User Interface Paradigm”, in 1st Int. Conference on
Affective Computing & Intelligent Interaction, Beijing,
China.

