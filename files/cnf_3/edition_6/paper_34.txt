Text Map Explorer: a Tool to Create and Explore Document Maps
Fernando Vieira Paulovich, Rosane Minghim
Institute of Mathematics and Computer Science
University of S˜ao Paulo, S˜ao Carlos, SP, Brazil
{paulovic, rminghim}@icmc.usp.br
Abstract
This paper presents a tool, called Text Map Explorer,
which can be used to create and explore document maps
(visual representations of document collections). This tool
is capable of grouping (and separating) documents by their
contents, revealing to the user relationships amongst them.
This paper also presents a novel multi-dimensional projection technique for text that reduces the quadratic time com3
plexity of our previous approach to O(N 2 ), keeping the
same quality of maps. The technique creates a surface that
reveals intrinsic patterns and support various kinds of exploration of a text collection.

1. Introduction
In a quest for information on a particular subject in a
document data base, it is a fact that the exploration of the results obtained has become a tiresome task due to the lack of
comprehensive structure in their responses. Most search engines, and other supporting tools for data interpretation, return the information about documents that, in spite of been
progressively enhanced, does not allow the identiﬁcation of
higher level structure possibly revealed by the documents,
such as sub-areas.
A number of different techniques for visualization of textual results from Web and other searches have being deployed [1, 17, 27, 2]. While these techniques are capable
of displaying large document bodies, they tend to make location of relevant reading material more troublesome.
Other techniques for document visualization that, in order to help locate neighboring similarities between texts
and groups of texts, search for a representation of the
content of an individual document [20, 24], of document
sets [17, 4, 32], or of themes approached in documents
[13, 33, 34].
The most common way to extract structure from a document collection is by applying some sort of dimensional
reduction technique over the document collection vector

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

representation [25]. This is the case of systems based on
Multi-dimensional Scaling (MDS), Force-direct Placement
[7, 12, 21], Principal Component Analysis (PCA) or Latent Semantic Indexing (LSI)[18], that work with statistical measures for subspace reduction, and Self-Organizing
Maps (SOM), that employ neural computation [32, 33, 4,
16, 34]. Those techniques can be used to plot the original data in bidimensional (2D) space, when dimension is
reduced to 2.
Another common strategy to dealing with the organization of information from a document collection is document
clustering [6, 23], many times employed in combination
with dimensionality reduction and SOM [14, 32, 17]. When
clustering techniques are applied, the intra-cluster relations
are not given as a result. However, they are very useful
to provide general overviews of large collections, although
they usually have to be interpreted by users with certain
level of expertise.
Maps resulting from the techniques mentioned above are
meant to analyze a number of properties of documents, including similarity, co-citation, term co-occurrence and various others. We refer to the work of B¨orner and others [5] for
a detailed description of the available techniques for document mapping and its applications, systems and challenges.
A few systems are being developed dedicated to viewing
maps from multi-dimensional data and some of them are
particularly dedicated to document collections. One recently published system [12] adds representational power
to the conventional ways of plotting document as points in
2D by separating their contents in thematic areas and handling levels of interaction by hierarchical organization. In
general, the methods discussed above lack the ability to determine levels of associations between texts contents. Others are computationally expensive.
This paper presents a new technique for content-based
text mapping based on projection of clusters (called ProjClus), as well as tool, called Text Map Explorer (TME)1 .
1 The TME tool is written in Java and it is freely available at
www.lcad.icmc.usp.br/∼paulovic/tme.htm where the color pictures of this
paper can also be found.

Through the ProjClus technique, the text map explorer software can create visual representations of text collections
and support their exploration. The tool can be used to
overview of a set of documents, to locate important documents in the corpora, or to ﬁnd useful associations between
documents. ProjClus is much faster than the techniques previously developed for text mapping and is capable of representing by proximity the similarity between documents and
groups of documents.
The next section describes the full process for document
map generation as well as its motivation. Section 3 discusses the results in terms of quality of the maps, performance and precision. Conclusions and further work follow.

2. Process Overview
The overall process followed here to generate contentbased document maps is shown the Figure 1. This process is
composed by three main steps: (1) Corpora pre-processing;
(2) Projection and triangulation; and (3) Mapping of additional information.

used to reduce the dimensionality of these vectors and plot
the collection onto its map.

2.1. Document Collection Pre-processing
In order to represent a document collection in a vector
space, ﬁrst each document is converted in a vector with coordinates based on the frequency of certain terms or combinations of terms. After that, these vectors are united to
form a matrix of documents x terms in which each term
is weighted according to term-frequency inverse-documentfrequency (tﬁdf) measure [26]. The steps towards generating
the matrix from the text collection are:
1. Stopwords (irrelevant words, such as prepositions, articles and such) are eliminated from all texts;
2. Stemming is applied to extract word radicals (here we
employ the Porter’s stemming algorithm [22]);
3. A frequency count is performed, and the Luhn’s [19]
cut applied, eliminating terms that occur less than n
times, and are therefore not representative within the
collection. The value of n must be established on a
empirical way, since there is not a theoretical basis for
its determination [31].
4. Terms are weighted according to their frequency. Here
tﬁdf (term frequency inverse document frequency) is
used. With tﬁdf if a term appears on too many documents, its ’representation power’ is decreased, since
it cannot distinguish individual elements of the collection.

Figure 1. Process overview
According to this process, a ﬁrst, pre-processing step,
converts all documents of the collection into a vector representation on a multi-dimensional space based on the number of terms (or n-grams) within the collection. After that,
the dimensionality of these vectors is reduced to 2 in order to represent each document as a point on a plane. Once
these points are projected on a plane, they are triangulated,
to reﬂect the proximity relationship between them and also
to generate a surface view of the map. Finally, it is possible to map further information over the surface by choosing
visual attributes such as color or size of each point, representing additional information about the documents, such
as the presence or frequency of a particular word or a set of
words in each document.
The next sections describe each step of this process in
turn, presenting the way on each document is converted into
a vector, and the novel distance-based projection technique

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

In the documents x terms matrix, each line is a vector
representing a document and each ﬁnal term is a dimension,
with tﬁdfs of the terms as coordinates.
The second step in the mapping generation process reduced the dimension of the set to two, using the resulting
coordinates to plot the points in 2D.

2.2. Document Layout by Projection of
Clusters
The projection technique developed to generate the maps
is outlined in this section. Its goal is to achieve a fast point
positioning in 2D that preserves similarity given by a metric
in the original vector space. Table 1 lists the symbols and
their deﬁnitions used to describe this technique.
Let X be a set of points in Rn with d : Rn → R a
criterion of proximity between points in Rn , and P be a
set of points in R2 with dˆ : R2 → R a proximity criterion in R2 . A Projection Technique can be described
as a function α : X → P which tries to approximate

Table 1. Summary of symbols and deﬁnitions.
Symbol
X
P
α(∗)
d(∗, ∗)
ˆ ∗)
d(∗,
S
C

Deﬁnition
a set of points in the original space.
a set of points in the projected space.
projection function.
a proximity criterion between points in the
original space.
a proximity criterion between points in the
projected space.
a set of points of X (cluster).
clusters’ centroids.

ˆ
|d(xi , xj ) − d(α(x
i ), α(xj ))| as close as possible to zero,
∀xi , xj ∈ X [29].
Normally, when a projection technique is applied directly over the document vector representation, the ﬁnal
result is not very useful due to the amount of information
that is lost during the projection [21]. To remedy that, an
improvement technique must be applied to re-arrange the
positions of the points in order to recover this information.
One approach which combines projection and improvement
techniques and that brings good results for document maps
was presented in [21]. In this approach, called IDMAP,
the well known Fastmap [9] technique was applied together
with a simpliﬁed force-direct placement technique, called
Force [29]. Although this technique generates good layouts, it cannot be applied to large document collections due
to the time complexity of Force, O(N 2 ).
Trying to keep the same quality of IDMAP’s layouts,
but reducing its time complexity we devised a new method
ˆ
based on the idea that the |d(xi , xj )−d(α(x
i ), α(xj ))| must
be as close as possible to zero, ∀xi ∈ X, but with xj ∈ Si ,
where Si is a set of a limited number of points of X related
with xi .
Once our focus is to improve the representation of similarity between documents, putting the most similar documents of xi near to α(xi ) when they are projected, the set
Si must be composed by the points which belong to a neighborhood of xi , i.e., Si = {x : d(xi , x) < ε, x ∈ X}, with
ε deﬁned in order to limit the number of points in Si . Although the obvious choice to determine the set Si would be
a nearest neighbors search, its complexity (O(N 2 )) is prohibitive to our targets. Instead we employ a faster clustering
technique to deﬁne the sets Si , allowing the complexity to
3
be reduced to O(N 2 ).
In order to create the clusters, the bisecting k-means
technique is employed due to its time complexity (O(k ×
N )) and because it is less sensitive to the initial choice of
clusters than the original k-means [28].
Once the clusters have been found, the centroid of each

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

cluster is calculated, resulting on a set of points C =
{c1 , ..., ck } ∈ Rn . After that, each cluster Si is individually
projected using Fastmap and Force, and these projections
are normalized between [0, λi ]. The λi can be calculated
as λi = max{d(x, ci ), ∀x ∈ Si }/F, F > 0, where F is a
factor of clusters density in R2 added to ProjClus. As the
value of F increases the clusters formed get more dense on
the projection.
As the last steps, the centroids C are projected themselves using Fastmap and Force, and the projection of each
cluster is positioned in the ﬁnal layout according to its relative position to the centroid. This ﬁnal positioning guarantees that similar clusters be projected close, and dissimilar
ones be projected far apart, so similar documents will be
grouped, and dissimilar ones will be separated. Figure 2
outlines this new technique.
√
1. Split the points into N clusters S1 , ..., Sk
2. Calculate the clusters centroids C = {c1 , ..., ck }.
3. Project the clusters centroids.
4. For each clusters Si .
4.1. Project its points.
4.2. Normalize the cluster projection between [0, λi ].
5. Position each cluster projection in the ﬁnal layout.
Figure 2. The novel projection technique.
The overall complexity of ProjClus can be calculated as
O(H + Pk + Pc + J), where H is the complexity to create
the clusters, Pk is the complexity to project these clusters,
Pc is the complexity to project the clusters’ centroids, and J
is the complexity to join√all the projection of all clusters into
3
a ﬁnal
√ layout. Creating N clusters is O(N 2 ). Positioning
the N centroids is O(N ). Generating the ﬁnal
√ layout is
O(N ). Finally, if we have in each cluster r × N points,
with r = 1, the procedure to project each cluster will be
3
O(N ), and the projection of all clusters will be O(N 2 ). It
is a fact that not all cluster will have the same size when
k-means is applied [15], but since we employ a bisection
technique to generate the clusters, no cluster will have r
3
3
2, so the Pk complexity is O(r2 × N 2 ) = O(N 2 ), since r
is a small constant. The ﬁnal complexity of this technique
3
3
3
will be therefore O(N 2 + N 2 + N + N ) = O(N 2 ).

2.3. Triangulation and Mapping Additional
Information
Once the projection is performed, we generate a triangulation of the points. This triangulation is used as a way to
support user’s location of the nearest neighbors of a point
on R2 . Again, although the obvious choice to create such
triangulation be a nearest neighbor search, since its complexity is O(N 2 ) it cannot be applied. Here we employ a
Delaunay triangulation which runs on O(N logN ) [8].

The Delaunay triangulation will give an approximation
of the nearest neighbors of a point in R2 , but it is also interesting to represent the nearest neighbors of a point in
Rn . To create such representation we only execute a nearest
neighbors search inside the clusters created by the projec3
tion technique. This kind of approximation has O(N 2 ) if
the clusters have the same size (similar of the Pk complexity
discussed above).
Besides representing the similarity between documents,
the TME tool makes it possible to map additional information of the documents in two ways thus far: by coloring
the points which represent the documents, and changing
points’ sizes. Currently, the tool allows a word be searched
in the corpora and the documents (points) which contains
such word be colored/sized according to the frequency of
its occurrence.

3. Experimental Results
We have tested our tool using different corpora. Table 2
provides the number of documents of these data sets, the
Luhn’s cut which was applied in each data set, and the resulting dimensionality of the documents x terms matrix after the pre-processing phase. The ﬁrst data set includes title, authors, abstract and references from scientiﬁc papers
in four different subjects: Case-Based Reasoning (CBR),
Inductive Logic Programming (ILP), Information Retrieval
(IR), and Soniﬁcation (SON). Those were all collected by
members of our team. The set news-3 is composed by
messages from three different news discussion groups, and
reuters is composed by documents which appeared on the
Reuters newswire in 1987. These two sets were obtained
from an internet repository2 . The infoviz04 data set was
made available for the 2004 IEEE Information Visualization Contest [10]. Finally, the kdvis data set was obtained
from an Internet repository and includes ﬁles on the subjects of Bibliographic Coupling (BC), Co-citation Analysis
(SC), Milgrams (MG) and Information Visualization (IV)3 .
Table 2. Corpora description.
Number
Luhn’s cut
Number
documents
dimensions
cbr-ilp-ir-son
675
150
285
reuters
4988
300
564
news-3
300
100
85
infoviz
515
100
299
kdviz
1624
170
367
Corpora

2 Hettich, S. and Bay, S. D. (1999).
The UCI KDD Archive
[http://kdd.ics.uci.edu]. Irvine, CA: University of California, Department
of Information and Computer Science
3 ella.slis.indiana.edu/∼katy/outgoing/hitcite/{bc,sc,mb,iv}.txt

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

In the following examples the distance between documents (the d of section 2.2) is calculated using the cosine
metric presented in [9].
The ﬁrst example aims at verifying if ProjClus could
keep the projection of points that are considered to be in
the same general area close to one another. Figures 3(a),
3(b) and 3(c) show the results for the corpora news-3, kdviz
e cbr-ilp-ir-son. On these ﬁgures the colors of the points indicate the general area it belongs to. In case of articles, what
research area it is associated with, and, in the case of news,
which discussion group it belongs to. With this test it is
possible to see that the technique can separate well general
areas of knowledge or discussion.
Figure 3(d) shows a zoomed view of a part of Figure 3(c).
In these ﬁgures the blue points are IR articles, the brown
points are ILP articles, the green points are SON articles,
and the pink points are CBR articles. In this zoom it is possible to see a sub-group of IR articles inside the SON group.
Closer examination reveals that these IR articles deal with
”Audio Information Retrieval”, and almost all of them belong to the same author. A similar result can be observed in
Figure 3(b), where blue points are placed together with pink
ones. It occurs because these documents belongs to similar
areas: Bibliographic Coupling and Co-citation Analysis.
In order to show what happens when the factor F is
changed, Figures 4(a) and 4(b) present two maps for the
reuters corpora with different factors. It can be seen that as
the factor is increased, clusters density also increases, supporting location of groups of documents. In these ﬁgures
the documents are colored after a search of the word ’drug’.
Although in Figure 4(a) it is possible to distinguish just one
cluster, Figure 4(b) shows that there are two different clusters which present the word ’drug’. This separation happens because in one cluster almost all documents deal with
AIDS medicines, and in the other cluster the documents
deal mostly with medicines in general. Both facts can be
easily checked by the user with a very quick examination of
the map using TME. Factor F , allied to the speed in which
the mapping is performed, can be used to provide a hierarchical examination of the data set, moving from overview
to detail in order to analyze the map. To support that type
of multi-level exploration the tool allows a part of the map
to be cut and be browsed separately.
To evaluate projection quality for multi-dimensional data
visualization, it is usual to employ a quantitative measure
called Stress [7]. However, due to the high dimensionality
inherent of the document vectorial representation of texts,
and since high dimensionality tend to make distance calculations unstable [3], this kind of metric has been unable to
reﬂect the visual properties of grouping and separation of
groups of a particular technique.
We therefore employ a different scheme to assess the visual quality of projections according to document similarity,

(a) news-3 map.

(b) kdviz map.

(c) cbr-ilp-ir-son map.

(d) cbr-ilp-ir-son
zoom.

Figure 3. Some examples of maps.
time consuming task is pre-processing. In the case of vector representations of generic text collections (that is, with
no ﬁxed vocabulary), that process is not incremental. Other
measures of similarity can be used, either with ﬁxed vocabulary over vector representation or using other approaches
for text comparison, such as approximations of Kolmogorov
complexity [30].
(a) Using the factor F = (b) Using the factor F =
4.5
10.5

Figure 4. Example of changing the factor F
over reuters map.

our ﬁrst concern. Precision here is therefore calculated by
taking the k nearest neighbors of a point in the projection
and counting the percentage of its neighbors are in the same
pseudo-class of this point. Figure 5 shows the precision of
some the maps that were pre-classiﬁed. It can be seen that
under this criterion the projection presents a very good precision rate overall.

Figure 5. Precision of some maps.
The times taken to generate the maps, using a Pentium
IV 3.2GHz (2GB of RAM), are shown in Table 3. In
this table the process is separated into 2 main tasks: preprocessing and projection. According to this table the most

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

Table 3. Times taken to generate the maps (in
seconds).
Corpora
news-3
infoviz
cbr-ilp-ir-son
kdviz
reuters

Pre-processing
0.938
2.531
3.828
7.188
23.453

Projection
0.109
0.282
0.421
1.499
8.843

Total
1.047
2.813
4.249
8.687
32.296

Figure 6 shows the Text Map Explorer tool. This ﬁgure presents a document map for the infoviz04 corpora. On
this map the documents (points) color/size are given by the
number of occurrences of the word ’graph’, composing a
cluster on the map. It indicates that even on a homogeneous
corpora this tool can help to locate areas of interest. The
tool can also locate a document by its name; if a document
is selected on the map, its name and a list of its neighbors
is shown. With a double click over a document, its content
and the content of its neighbors are displayed in a separate
window.
Finally, although we have designed and tested ProjClus
in the context of text mapping, this projection technique
(and corresponding tool) can also be applied to other multidimensional data sets. Figure 7 presents a projection for the
Quadruped Mammals data set [11]. This data set is composed by 4 classes (dogs, cats, horses, and giraffes), and has
100,000 72-dimensional instances. The time spent to create
such map was 82.843s on the same computer setup of the
previous tests, indicating added degree of scalability when
comparable with other effective multi-dimensional projec-

Figure 6. Text Map Explorer tool.
tion techniques.

Figure 7. Quadruped Mammals map.

4. Conclusions
This paper has presented a technique, called ProjClus,
and a tool, named Text Map Explorer (TME), used to create and explore document maps. These maps can offer both
overview and detailed view of a document collection, improving the user’s capability to locate relevant material from
the data set. In such maps the documents are represented as
points on a plane, where the distance between them indicate
a similarity relationship, i.e., if two points are close on the
map the content of the documents which they represent are
supposed to be similar.
The new ProjClus dimensional reduction technique takes
as basis our previous approach IDMAP [21], but reduces
3
its quadratic time complexity to O(N 2 ) keeping the same
quality of the maps. Also, this new technique includes a
mechanism to increase or decrease the density of the clusters on the map, so it is possible for the user to explore the
maps hierarchically: ﬁrst looking for possible clusters on

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

the collection, after that exploring the documents within the
cluster and their relationships. Inter-cluster relationships
are provided by performing distance based projections locally, revealing document similarity by proximity on the
map, a feature not offered by conventional clustering techniques. Also opposite to clustering approaches, that do not
give intra-clusters relationships, here the frontiers between
different clusters are also represented, relating clusters with
one another.
Regarding to the scalability of the tool, it was shown that
the projection technique is very fast even for 100,000 points,
but the pre-processing time should be improved for treatment of very large data sets. In this sense we are working
on a new approach which avoids the vector representation
by comparing text against text and also on particular applications with ﬁxed vocabulary and incremental calculations.

Acknowledgements
This work is funded by FAPESP research ﬁnancial
agency, S˜ao Paulo, Brazil (proc. no. 04/07866-4 and
04/09888-5). We wish to acknowledge the work of our undergraduate and research students as well as research colleagues in processing some data and discussing various issues of the work.

References
[1] O. Alonso and R. Baeza-Yates. Alternative implementation
techniques for web text visualization. In Proc. of the First
Latin American Web Congress (LA-WEB 2003), pages 202–
203, Santiago, Chile, November 2003. IEEE Computer Society, IEEE Press.

[2] M. R. Baeza-Yates. Visualizing large answers in text
databases. In Int. Workshop on Adv. User Interfaces
(AVI’96), pages 101–107. ACM Press, 1996.
[3] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft.
When is nearest neighbor meaningful? In Lecture Notes
in Computer Science, volume 1540, pages 217–235, 1999.
[4] A. Booker, M. Condliff, M. Greaves, F. Holt, A.Kao,
D. Pierce, S. Poteet, and Y.-J. Wu. Visualizing text data
sets. Computing in Science and Eng., 1(4):26–35, 1999.
[5] K. Borner, C. Chen, and K. Boyack. Visualizing knowledge
domains. Annual Review of Informtion Science & Technology, 37:1–51, 2003.
[6] M. Carey, D. C. Heesch, and S. M. Ruger. A visualization
tool for document searching and browsing. In Proc. of Intl
Conf. on Distri. Multimedia Sys., 2003.
[7] M. Chalmers. A linear iteration time layout algorithm for
visualising high-dimensional data. In Information Visualization 1996, pages 127–132, San Francisco - CA, USA, 1996.
IEEE CS Press.
[8] H. Edelsbrunner. Geometry and Topology for Mesh Generation. Cambridge Monographs on Applied and Computational Mathematics. Cambridge, 2001.
[9] C. Faloutsos and K. Lin. Fastmap: A fast algorithm for indexing, datamining and visualization of traditional and multimedia databases. In ACM SIGMOD Proc. Intl Conf. on
Manag. of Data, pages 163–174, San Jose-CA, USA, 1995.
ACM Press: New York.
[10] J.-D. Fekete, G. Grinstein, and C. Plaisant.
IEEE
InfoVis 2004 Contest, the history of InfoVis.
www.cs.umd.edu/hcil/iv04contest, 2004.
[11] J. Gennari, P. Langley, and D. Fisher. Models of incremental concept formation. In Artiﬁcial Intelligence, volume 40,
pages 11–61, 1989.
[12] M. Granitzer, W. K. V. Sabol, K. Andrews, and W. Klieber.
Evaluating a system for interactive exploration of large, hierarchically structured document repositories. In Information Visualization 2004, pages 127–132, Austing- TX, USA,
2004. IEEE CS Press.
[13] S. Havre, E. Hetzler, P. Whitney, and L. Nowell. Themeriver: Visualizing thematic changes in large document
collections. IEEE Trans. Visual. and Comp. Graphics,
8(1):9–20, Jan-Mar 2002.
[14] S. Iritano and M. Ruffolo. Managing the knowledge contained in electronic documents: a clustering method for text
mining. In 12th DEXA Workshop, pages 454–458. IEEE CS
Press, 2001.
[15] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: A
review. ACM Computing Surveys, 31(3), September 1999.
[16] S. Kaski, T. Honkela, K. Lagus, and T. Kohonen. Websom
- self-organizing maps of document collections. Neurocomputing, 1(1-3):110–117, 1998.
[17] A. Leuski and J. Allan. Lighthouse: Showing the way to relevant information. In InfoVIS, pages 125–130. IEEE Computer Society Press, 2000.
[18] A. Lopes, R. Minghim, V. Melo, and F. Paulovich. Mapping texts through dimensionality reduction and visualization techniques for interactive exploration of document collections. In IST/SPIE Symposium on Electronic Imaging
- Workshop on Visualization and Data Analysis, San Jose,
California, 2006.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

[19] H. Luhn. The automatic creation of literature abstracts. IBM
Journal of Research and Development, 2(2):159–165, 1968.
[20] N. E. Miller, P. C. Wong, M. Brewster, and H. Foote. Topic
islands - a wavelet-based text visualization system. In
Proc. of Visualization ’98, pages 189–196, Research Triangle Park, North Carolina, United States, 1998. IEEE CS
Press.
[21] R. Minghim, F. Paulovich, and A. Lopes. Content-based
text mapping using multi-dimensional projections for exploration of document collections. In IST/SPIE Symposium on
Electronic Imaging - Workshop on Visualization and Data
Analysis, San Jose, California, 2006.
[22] M. F. Porter. An algorithm for sufﬁx striping. Program,
14(3):130–137, 1980.
[23] M. Rasmussen and G. Karypis. gCLUTO - an interactive
clustering, visualization, and analysis system. Technical Report CSE/UMN TR 04-021, Univ. of Minnesota, Dep. of
Computer Science and Engineering, 2004.
[24] R. M. Rohrer, D. S. Ebert, and J. L. Sibert. The shape of
shakespeare: Visualizing text using implicit surfaces. In
IEEE InfoVis’98, pages 121–129. IEEE Press, 1998.
[25] G. Salton. Developments in automatic text retrieval. Science, 253:974–980, 1991.
[26] G. Salton and C. Buckley. Term-weighting approaches in
automatic text retrieval. Inf. Process. Manage., 24(5):513–
523, 1988.
[27] M. Sebrechts, J. Cugini, S. Laskowski, J. Vasilakis, and
M. Miller. Visualization of search results: A comparative
evaluation of text, 2d, and 3d interfaces. In 22nd ACMSIGIR Conf. Research and Development in Information Retrieval, pages 3–10. ACM Press, 1999.
[28] P. Tan, M. Steinbach, and V. Kumar. Introduction to Data
Mining. Addison-Wesley, 2006.
[29] E. Tejada, R. Minghim, and L. Nonato. On improved projection techniques to support visual exploration of multidimensional data sets. Information Visualization Journal,
2(4):218–231, 2003.
[30] G. P. Telles, R. Minghim, and F. V. Paulovich. Visual
mapping of text collections using an approximation of kolmogorov complexity. Technical report, Instituto de Ciˆencias
Matem´aticas e de Computac¸a˜ o, University of S˜ao Paulo,
2005.
[31] C. J. van Rijsbergen. Information Retrieval. Butterworths,
second edition edition, 1979.
[32] E. Weippl. Visualizing content based relations in texts. In
Proc. of the 2nd Australian conference on User interface,
pages 34–41, Queensland, Australia, 2001. IEEE Computer
Society, IEEE Computer Society.
[33] J. A. Wise. The ecological approach to text visualization.
J. of the American Soc. for Inf. Sci., 50(13):1224–1233,
November 1999.
[34] J. A. Wise, J. J. Thomas, K. Pennock, D. Lantrip, M. Pottier,
A. Schur, and V. Crow. Visualizing the non-visual: spatial analysis and interaction with information for text documents. In Readings in information visualization: using vision to think, pages 442–450, San Francisco, CA - USA,
1995. Morgan Kaufmann Publishers Inc.

