Visualisation Based Feedback Control for Multiple Sensor Fusion
Gui Yun Tian
Duke Gledhill
School of Computing and Engineering, University of Huddersfield, UK, HD1 3DH
d.gledhill@hud.ac.uk
g.y.tian@hud.ac.uk

Abstract
This paper presents a new approach for
adaptive weighting schemes for multiple sensor
fusion by using visualisation feedback control. A
visualisation schema is used to coordinate the
information content capture of different features
from multiple sensors. The end-users can
interact with the training sample distribution to
obtain weights for the sensor data fusion and
decision making. Based on the proposed fusion
approach, some applications such as audiovisual biometric authentication and multi-modal
surface measurement are discussed.
Key words: Sensor fusion, Multi-modal data,
Virtual
reality,
Adaptive
weighting,
Visualisation, Feedback control
1. Introduction
In recent years, sensor research has been undergoing a
quiet revolution, promising to have significant impact on a
broad range of applications relating to national security,
health care, the environment, energy, food safety, and
manufacturing. The convergence of the internet,
communications, and information technologies with
techniques for miniaturization now provides vast
opportunities for the development and application of
sensor systems to meet these needs. Emerging technologies
have the potential to decrease the size, weight and cost of
sensors and sensor arrays by orders of magnitude, as well
as to increase their spatial and temporal resolution and
accuracy.
Unfortunately, mono-modal recognition techniques
are likely to reach in the near future a saturation in
performance. A potential way overcoming such
limitations, consists in combining results from several
modalities. For example, multi-cue biometrics helps
improve system reliability: While background noise has a
detrimental effect on the performance of voice biometrics,

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

it does not have any influence on face biometrics. On the
other hand, while the performance of face recognition
systems depends heavily on lighting conditions, lighting
does not have any effect on the voice quality. Therefore,
soon large numbers of sensors may be integrated into
systems to improve performance and lifetime, and decrease
life-cycle costs. Sensors are also beginning to incorporate
active elements for self-test, calibration and maintenance.
The term Sensor Web was first proposed by NASA Sensor
Web Applied Research Planning Group. Sensor Web
includes both in situ sensors and remote sensors. Sensors
can be mobile or stationary. Such sensors include, for
example wireless sensor networks, flood gauges, weather
towers, air pollution monitors, stress gauges on bridges,
mobile bio-sensors, webcams, and satellite-borne earth
imaging devices. ‘In the next century, planet Earth will
don an electronic skin. It will use the Internet as a scaffold
to support and transmit sensations. The skin is already
being stitched together. It consists of millions of embedded
electronic measuring devices: thermostats, pressure
gauges, pollution detectors, cameras, microphones, glucose
sensors, EKGs, electroencephalographs. These will probe
and monitor cities and endangered species, the atmosphere,
our ships, highways and fleets of trucks, our conversations,
our bodies – even our dreams’ [1]. This leads to massive
information fusion from multiple sensors for decision
making.
Multisensor data fusion is a technology concerned
with the problem of how to combine data and information
from multiple sources/sensors in order to achieve
improved accuracies and better understanding of the
observed target than could be achieved by the use of a
single source/sensor alone. In recent years, multi-sensor
data fusion has been extensively considered for military
and non-military applications, such as target and pattern
recognition [2, 3].
Fusion is performed at two different sensor levels
(sensing modality): intramodal and intermodal [4]. In
intramodal fusion, the scores of multiple samples (e.g.
utterances or video shots) obtained from the same modality
are linearly combined, where the combination weights are
dependent on the difference between the score values and a
user-dependent reference score obtained during
enrollment. This is followed by intermodal fusion in which
the means of intramodal fused scores obtained from

different modalities are fused. The final fused score is then
used for decision making.
There are three kinds of fusion that are associated with
object identification: Data level fusion, Feature level
fusion and Decision level fusion. Data level fusion deals
with each sensor independently according to an
identification scheme [5]; Feature level fusion involves
extracting feature vector information and creating a single
feature vector for identifying an object [6, 7]; Decision
level fusion deals with inference techniques such as the
Dempster-shafer method [8, 9]. This paper will investigate
feature level fusion using adaptive weighting integration.
After the introduction, section 2 presents the visualization
based feedback control for feature fusion; section 3
discusses some applications and section 4 reports the
conclusion and further work.

2. Adaptive weight fusion and feedback
control

different fusion under different circumstances (adaptive
fusion).
Before the feature fusion, z-score normalization
technique is used, using the arithmetic mean and standard
deviation of the given data. This schema needs to estimate
the mean and the stand deviation of the scores from a
given set of matching scores. The normalized scores are
given by

sk  P

s k'

(2)

V

Where μ is the arithmetic mean and ı is the standard
deviation of the given data. However, both mean and
standard deviation are sensitive to the outliers. Hence, a
process of removing outliers is applied to improve the
robustness of z-score normalization.
Colour

An approach for adaptive weight fusion using
visualization based feedback control is proposed. In the
previous publication [7], as illustrated in Figure 1, a VRbased approach to the search for and retrieval of an image
from an image database is illustrated. The image database
is categorised by the image attributes of colour, texture,
shape and text title. The features are extracted by using
different multimedia information representations. Then,
Multidimensional Scaling (MDS) techniques are used for
feature fusion. MDS is a non-linear competitor to Principal
Component Analysis (PCA) – i.e. projecting highdimensional data into lower dimensions (e.g. 3D) whilst
attempting to retain the distance between pairs. The 3D coordinated images can then be visualised in a virtual
environment. Following the work, the paper will
concentrate on the feature fusion.
Instead of assigning an equal weight to all scores,
different weights are assigned to different features. The
approach splits a score sequence into K sub-sequence. The
frame-level fused scores are computed as
K

st( m )

¦D

( m ,k ) ( m ,k )
t
t

s

(1)

k 1

Where t=1, …, T

(m)

/ K , and D t( m ,k )  [0,1] represents
( m ,k )

the confidence (reliability) of the feature score s t
fusion weights

D

( m,k )
t

. The

, whose sum will be 1, are made

dependent on both the training data and recognition data
(scores). We will apply manipulating VR visualization of
the training data for optimizing the weighting to create

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

Colour
image

Texture

Shape

Off-line

Feature
fusion

VR-based
Sorted image
database
visualisation

User
Interaction

Text title

On-line
Inquiring
image

Feature
extraction

Heuristic
Search

Results
Visualisation

Figure 1. VR-based colour image retrieval
Virtual Reality (VR) technology and techniques have
the potential to deliver intuitive user interfaces with more
natural styles of interaction between the user and the
application which will enhance productivity and
performance. Facilities for user interface and interaction
development within today’s VR systems are rudimentary,
limited and limiting.
Using visual feedback techniques (such as
highlighting objects when the selection cursor passes
through them) and generic input devices (such as the
glove) the end-users can manipulate the distribution of
training data, moving the positions of the information from
pre-defined weighting fusion. The modification of
coordinates of the training data in the VR environment will
change the transformation matrix to re-calculate the
weighting. The process will be repeated by the end-users in
a closed loop system as illustrated in Figure 2. The system
can be integrated into Figure 1 for content based image

retrieval.

Normalised
features

Weight
Matrix

Feature
fusion

VR based visualisation

User
manipulation
Figure 2. VR visualization based adaptive feature fusion

3. Experimental study
A previously used image database contained 80
colour-texture images. To visualise the image database [7],
their colour, texture or/and titles were used to represent the
image data. The process can be summarised as:
1) Feature extraction by using colour histogram, texture
histogram or both;
2) Computing Euclidean distances to produce similarity
matrix. In our colour-texture image database, the
distance matrix is an 80*80 symmetric matrix [7].

In addition to content based image retrieval, this
approach can be extended for multimodal biometrics such
as person authentication [2, 3, 10] and speech recognition.
Different kinds of sensors such as stereo cameras,
ultrasonic sensors, infrared sensors have been installed on
the remote mobile robot, which can be controlled by a host
PC. Audio-visual signals returned by the robot to the host
could be visualised and weighted to be fused for tasks such
as person recognition. Extending the current system from
audio-only or visual-only to audiovisual is nontrivial;
questions about how best to integrate the audio and visual
data streams, and how best to exploit the visual
information have to be carefully addressed. The usercentral feedback control in collaboration with
normalisation techniques is important for our on-going
research on system application of detection, classification
and tracking for crime watch.
More sensors such as electromagnetic sensors for
metal detection and fingerprint sensors will be
incorporated in the sensor rich system. Based on the
system and developed software, selective feature
extraction algorithms and fusion will be investigated and a
demonstration of biometrics based security system will be
constructed. A feature integrative matrix and uncertainty
estimation will be developed based on normalised
techniques.

4. Conclusion and further work
Visualisation based feedback control for multiple
sensor fusion is aiming at optimal weighting schemes for
audio-visual fusion or colour-shape-texture fusion for
improving recognition and pattern inspection. The userdependent fusion through visualization navigation and
manipulation has been applied for adaptive weighting
fusion. The on-going project will evaluate large datasets
for different applications including multimodal sensors for
security and automatic ‘early warning’ systems.

References
1.

Figure 3. Colour-Texture pattern visualisation in a VR
environment
Figure 3 illustrates the colour-texture pattern
visualisation in a VR environment. End-users can
manipulate the distribution by moving the training data for
integration of colour and texture Z-score normalised
features.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

2.

3.

4.

Steve H.L. Liang, Arie Croitoru and C. Vincent Tao, “A
distributed geospatial infrastructure for Sensor Web”,
Computers & Geosciences, Volume 31, Issue 2, March
2005, pp. 221-231.
A. Ross and A. Jain, “Information fusion in biometrics”,
Pattern Recognition Letters, Volume 24, Issue 13,
September 2003, pp. 2115-2125.
A. Jain, K. Nandakumar and A. Ross, “Score normalization
in multimodal biometric systems”, Pattern Recognition,
Volume 38, Issue 12, December 2005, pp. 2270-2285.
M. Cheung, “Intramodal And Intermodal Fusion For AudioVisual Biometric”, url = citeseer.ist.psu.edu/694500.html

5.

L.B. Becker, E. Nett, S. Schemmer and M. Gergeleit,
“Robust scheduling in team-robotics”, Journal of Systems
and Software, Volume 77, Issue 1, July 2005, pp. 3-16.
6. G. S. W. Klein and T. W. Drummond, “Tightly integrated
sensor fusion for robust visual tracking”, Image and Vision
Computing, Volume 22, Issue 10, 1 September 2004, pp.
769-776.
7. G. Y. Tian and D. Taylor, “Colour image retrieval using
Virtual Reality”, Proceeding IV2000 (IEEE), pp 221-226.
8. R. J. Stanley, P. D. Gader and K. C. Ho, “Feature and
decision level sensor fusion of electromagnetic induction
and ground penetrating radar sensors for landmine detection
with hand-held units”, Information Fusion, Volume 3, Issue
3, September 2002, pp. 215-223
9. F. Rottensteiner, J. Trinder, S. Clode and K. Kubik, “Using
the Dempster–Shafer method for the fusion of LIDAR data
and multi-spectral images for building detection”,
Information Fusion, Volume 6, Issue 4, December 2005, pp.
283-300.
10. B. Duc, E. S. Bigun, J. Bigun, G. Maitre, and S. Fischer.
“Fusion of audio and video information for multi modal
person authentication”. Pattern Recognition Letters,
18(9):835--843, 1997.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

