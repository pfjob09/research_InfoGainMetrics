Visual Mapping of Text Collections through a Fast High Precision Projection
Technique
Fernando Vieira Paulovich, Luis Gustavo Nonato, Rosane Minghim
Institute of Mathematics and Computer Science
University of S˜ao Paulo, S˜ao Carlos, SP, Brazil
{paulovic, gnonato, rminghim}@icmc.usp.br
Haim Levkowitz
University of Massachusetts Lowell, USA
haim@cs.um.edu

Abstract
This paper introduces Least Square Projection (LSP),
a fast technique for projection of multi-dimensional data
onto lower dimensions developed and tested successfully in
the context of creation of text maps based on their content.
Current solutions are either based on computationally expensive dimension reduction with no proper guarantee of
the outcome or on faster techniques that need some sort
of post-processing for recovering information lost during
the process. LSP is based on least square approximation,
a technique originally employed for surface modeling and
reconstruction. Least square approximations are capable
of computing the coordinates of a set of projected points
based on a reduced number of control points with deﬁned
geometry. We extend the concept for general data sets. In
order to perform the projection, a small number of distance
calculations is necessary and no repositioning of the ﬁnal
points is required to obtain a satisfactory precision of the ﬁnal solution. Textual information is a typically difﬁcult data
type to handle, due to its intrinsic dimensionality. We employ document corpora as a benchmark to demonstrate the
capabilities of the LSP to group and separate documents by
their content with high precision.

from high computational times, while faster projections
generate higher degrees of information loss that have to
be recovered via repositioning of the points, usually with
a force-based placement strategy.
The application of projections for textual document mapping and visualization poses an extra challenge, also tackled
by many, due to the fact that vector space representations
of texts can reach thousands of dimensions easily, even for
data sets with a few hundred documents. However, extracting meaning of document contents and their relationships
to other documents automatically is an application with important ramiﬁcations in visual analysis. Our goal is to create a surface-based map of documents that can be explored
to ﬁnd patterns and relevant information within text collections.
This paper presents a solution for multi-dimensional projection that, besides being fast, results in a high precision of
ﬁnal positioning of points, avoiding force-based placement
and also avoiding the calculation of the full distance matrix
between points in the original space. The technique is capable of grouping related documents as well as separating
well those groups making the display very useful for further
user exploration.

1. Introduction
The problem of multi-dimensional projection has been
the concern of many researchers, due to the large variety of
applications that could make use of a bi-dimensional (2D)
positioning by similarity of points deﬁned in a space with
many dimensions.
Techniques based on sub-space decomposition suffer

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

The next section reviews the work on text mapping for
comparison with the application targeted here. It also highlights the classes of techniques previously employed for
the formation of visual representations of text sets. LSP
is presented in Section 3. Our results are presented in Section 4. Those are followed by conclusions and further developments of the present work.

2. Related Work
Several different techniques for visualization of textual
results from the Web and other searches have been proposed
([1, 14, 20, 3]). While these techniques are capable of displaying large document bodies, they tend to make the location of speciﬁc relevant reading material difﬁcult. We focus
here on tools to support mapping of documents in a way
that helps locate neighboring similarities between individual text documents and groups of document. We therefore
assume a pre-ﬁltering task that reduces the universe of targeted documents to a few hundreds (maybe thousands) of
documents in a few areas of interest (not necessarily predetermined).
Usually text processing and visualization tasks employ
the vector space model [19] whereby individual texts are
represented as vectors and terms (or n-grams) are the dimensions. The coordinates of a text in that space are
the weights of the terms based on their frequencies. Text
documents collections’ dimensions tend to be very high.
Typically, those dimensions reach the thousands even for
small to medium data bases. Transformation of a document
collection into a vector space is preceded by elimination
of non-inﬂuential words (such as stopwords), reduction of
words to their radicals (stemming), and some sort of frequency counting — various exist.
The most common way to extract structure from a text
document collection is by applying some dimension reduction technique over the resulting vector representation. Systems that implement such approaches include those based
on Multi-dimensional Scaling (MDS), Principal Component Analysis (PCA) or Latent Semantic Analysis ([15]),
which work with statistical measures for subspace reduction, as well as Self-Organizing Maps (SOM), which employ neural computation ([25, 26, 5, 13, 27]).
Point placement strategies, including force-based point
placement([17, 9]), are commonly used for multidimensional data presentation, and perform adjustment of
point position through attraction by similarity. Some approaches [8, 2, 18] can avoid, partially or completely, the
extensive calculations needed in dimension reduction techniques by starting with a semi-random point placement.
However, our experience trying to map text sets using their
application to text content has been that the precision of the
placement is limited, probably due to the subtleties of text
representation.
Techniques that result in document clustering, such as
SOM, are capable of reﬂecting inter-cluster relationships to
some extent, but do not directly support identiﬁcation of
intra-cluster associations. The same is true for other clustering techniques for text. MDS, PCA and LSI can be used
to generate text maps in two dimensions. However, when
the reduction is employed to reach 2D, results are only good

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

when there is clear separation between up to three sub-areas
within the collection. Beyond that (and therefore also in
collections with unknown class assignment) the grouping
capabilities of the technique cannot be guaranteed and must
be supported by additional processing to reach reasonable
results.
Maps resulting from the techniques mentioned above are
meant to present various properties of documents, including
content similarity, co-citation, term co-occurrence and attribute value matchings, such as author or publication date.
For a detailed description of available techniques for text
document mapping and its applications, systems and challenges, we refer the reader to B¨orner et al. [6].
Minghim et al. [17] have proposed faster mapping approaches that posses the ability to associate documents by
similarity, based on dimension reduction by fast projections
(such as Fastmap [11]) and Nearest Neighbor Projection
(NNP) [23]). Those, however, are still not capable of avoiding point re-adjustment after projection, a procedure that
can carry high computational costs, and impair the scalability of the method for extremely large data sets.
A few systems dedicated to viewing maps from multidimensional data are being developed. Some of those are
particularly dedicated to text document collections. One of
these systems [2] adds representational power to the conventional ways of plotting documents as points in 2D by
separating their contents in thematic areas and handling levels of interaction by hierarchical organization.
The technique presented here is a new multi-dimensional
projection based on least square meshes[21], an approach
originally employed to satisfactorily reconstruct geometrical meshes from control points. The quality of the resulting
mesh in the modeling application—allied to the facts that it
is based on neighborhood preservation and that, to be built,
the number of distance calculations is reduced—has suggested its usefulness for the purpose of multi-dimensional
data mappings and in particular for the case of text mappings. In fact our results show that the maps produce a very
satisfactory position of the points based on their distances in
the original space, thus not requiring re-positioning in 2D.
The resulting map displays the text as points connected
through a triangular mesh, which lends itself to surface
based views and attribute mappings for adding dimensions to the visualization. Landscape plots have been the
choice of many useful presentations of documents before
([25, 26, 7, 9]). The map is interactive under surface exploration tools, such as the Spider Cursor [16].

3. Least Square Projection
Given a set of points S = {p1 , . . . , pn } in Rm , the LSP
algorithm aims at representing the points of S in a lower
dimensional space Rd , d < m so as to preserve the neigh-

borhood relationship among the points. Two main steps are
involved in the projection process. First, a subset of points
in S, called control points, are projected onto Rd by a conventional projection method, such as Fastmap, for example.
Making use of the neighborhood relationship of the points
in Rm and the cartesian coordinates of the control points in
Rd it is possible to build a linear system whose solutions are
the cartesian coordinates of the points pi in Rd .
The idea of generating the coordinates of a set of points
from control points has already been explored in the context of mesh recovering and mesh editing by Sorkine and
Cohen-Or [21, 22]. In fact, the LSP method generalizes the
ideas of Sorkine and Cohen-Or in order to deal with high
dimensional spaces.
In the following we present the details of the LSP
method, emphasizing the construction of the linear system
involved in the projection strategy and how to deﬁne the
control points that are essential to the process.

3.1. Laplacian Matrix
Let Vi = {pi1 , . . . , piki } be a set of ki points in a neighborhood of a point pi and p˜i be the coordinates of pi in Rd .
Suppose that p˜i is given by the equation:
p˜i −

1
ki

p˜j = 0

(1)

pj ∈Vi

If equation (1) holds for the points in S then each pi is
the centroid of the points in Vi . Other important properties
regarding equation (1) can be found in [12, 24].
Equation (1) gives rise to a set of linear systems from
which it is possible to compute the coordinates of points pi ,
that is:
Lx1 = 0, Lx2 = 0, · · · Lxd = 0
(2)
where x1 , x2 , . . . , xd are the vectors containing the cartesian coordinates (x1 , . . . , xd ) of the points and L is the n×n
matrix whose entries are given by:
⎧
i=j
⎨ 1
1
pj ∈ V i
− ki
lij =
⎩
0
otherwise
Matrix L is usually called the Laplacian matrix and its
rank depends on the neighborhood relationship among the
points. When a mesh is given, the neighborhood of the
points can be obtained from the incidence relationship of
the mesh. In this case, the rank of L is n − r, where r
is the number of connected components of the mesh [21].
In our case, where no mesh exists, it is important to deﬁne
the neighborhood of the points so as to ensure the overlap
condition stated bellow:
Deﬁnition: Let S = {p1 , . . . , pn } be a set of points and
V = {V1 , . . . , Vn } be the set of neighborhood relationships

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

of the points in S. The set V is said to satisfy the overlap condition if for every two points pi and pj there exists
a sequence of neighborhoods V1ij , . . . , Vqij so that V1ij =
ij
Vi , Vqij = Vj and Vkij ∩ Vk+1
= ∅, k = 1, . . . , q − 1.
It is not difﬁcult to see that the overlap condition ensures
the same properties on L as a mesh with a single connected
component, that is, L has rank equal to n − 1, ensuring
a non-trivial solution. The problem is that any geometrical information is contained in L, thus the solutions of the
linear systems may be not useful. In order to render the solutions of the linear systems more attractive, we must add
some geometrical information into the system. This is done
through control points that can be obtained by projecting
some points of S onto Rd .
The control points are built into the linear system as new
rows in the matrix. The cartesian coordinates of the control
points are added to the right side of the system, giving rise
to a non-zero vector. Thus, given a set of control points
Sc = {pc1 , . . . , pcn c }, we can re-write (2) in the form
Ax = b

(3)

where A is a rectangular (n + nc) × n matrix given by:
A=

L
C

,

cij =

1 pj is a control point
0 otherwise

and b is the vector
bi =

0
xpci

i≤n
n < i ≤ n + nc

where xpci is one of the cartesian coordinates of the control
point pci .
The linear system with the control points has full-rank
and it can be solved in least-square sense, which means that
we shall ﬁnd x that minimizes ||Ax − b||2 , that is, x =
(AT A)−1 AT b. The system AT Ax = AT b that must be
solved is symmetric and sparse, facilitating its solution [21].
Another important characteristic of the above construction is that if a new point pn+1 is added to S, the linear
system (3) can easily be updated by only adding a new line
in the L part of A. This strategy makes it possible to project
points onto Rd interactively, making it necessary only to deﬁne a neighborhood relationship to each new added point.

3.2. Control Points
In order to determine the set of control points, nc points
are carefully chosen from the set of points S. These points
must be chosen so as to represent properly groups (classes)
of interest in Rm . In order to select the control points,
the space Rm is split into nc clusters using the k-medoids

method [4], where the most representative point of a cluster is the one closest to the centroid of the cluster. The kmedoids method was chosen due to its ability to deal with
outliers [4]. Furthermore, only the distances to the medoids
are needed, thus avoiding the need to compute all the distances among the points.
Once the control points are chosen (the medoids), these
points are projected onto Rd through a fast dimensionality reduction method, such as Fastmap [11] or NNP [23]
(the latter can only be used if d = 2). If the target dimension is R2 a projection improvement method, such as
Force [23], can be used together with the dimensionality reduction method to enhance the arrangement of the control
points.

4. Results
In order to test our approach to multi-dimensional projection we have applied it on various data sets. First we
looked at the capability of the technique to keep close to
each other the projections of points that are considered to be
in the same general area. Second, we looked at the positioning of papers within the general groups to see whether papers previously known to tackle similar subjects have been
mapped close to each other. In both tests we were satisﬁed with the results, particularly because our technique has
reached a precision level close to those of other mapping
techniques in one pass (that is, without force-based repositioning of the points), and without the need to calculate all
the distances between points, thus resulting in a very fast
projection technique.
In the following examples and text, the vector-based
distance refers to the cosine distance over a vector-based
representation of the documents and the Kolmogorov distance refers to the Normalized Compression Distance
(NCD) following the formulation proposed by Cilibrasi and
Vit´anyi [10].
Figure 1 illustrates the steps to obtain the document map
using LSP showing the two options of distance calculations.
Table 1 provides details of the documents used for the experiments. The ﬁrst two data sets include scientiﬁc papers
with title, authors, abstract and references from a number
of text documents. CBR (Case-based Reasoning) and ILP
(Inductive Logic Programming) subsets were taken from
journals on those subjects. The IR (Information Retrieval)
and SON (Soniﬁcation) subsets were articles obtained as a
result of Internet searches pre-ﬁltered to comply to those
pseudo-classes. Those were all collected by members of
our team. CBR+2 data sets include the ﬁrst three subjects (CBR+ILP+IR) and the CBR+3 data set also includes
SON. The kdvis set was obtained from an Internet repository and includes ﬁles in the ISI format on the subjects
of Bibliographic Coupling (BC), Cocitation Analysis (SC),

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

Figure 1. Document map generation process
Milgrams (MG) and Information Visualization (IV)1 . The
set news is messages from news discussion groups obtained
from an internet repository2 .
Table 1. Datasets used in the tests
Set
General
Files
CBR+2
Scientiﬁc papers
575
Scientiﬁc papers
675
CBR+3
kdvis
Scientiﬁc papers
1624
infovis04
Scientiﬁc papers
535
all
Scientiﬁc papers
2834
Discussion messages 300
news

Figure 2 presents the LSP of the data set CBR+2. It can
be seen that the projection was able to position members
of the same group together and also to separate the groups,
with very few mismatches. The effect of changing the number of control points can be seen in Figure 3. Although the
number of control points in this case actually decreased, the
solution was even better, probably because of the choice of
the control points. In those cases, CBR is displayed in red,
ILP in green, and IR in blue.
Figure 4 shows the inclusion of a new set of ﬁles, this
time belonging to a new subject (soniﬁcation). This time
more mismatches are present than in the previous example,
but the separation is still very good. Besides including these
ﬁles we have also included six of our own papers to see how
they would be placed inside their group and amongst themselves. Five of those papers were on soniﬁcation and shared
1 ella.slis.indiana.edu/∼katy/outgoing/hitcite/{bc,sc,mb,iv}.txt
2 Hettich, S. and Bay, S. D. (1999).
The UCI KDD Archive
[http://kdd.ics.uci.edu]. Irvine, CA: University of California, Department
of Information and Computer Science

(a) surface.

(b) wire-frame.

Figure 2. 3-class dataset (CBR+2) data set,
with 55 control points and 10 the neighborhood size.

(a) surface.

(b) wire-frame.

Figure 3. 3-class dataset (CBR+2) data set,
with 50 control points

two authors. The sixth also shared two authors but was on
a completely different subject (image reconstruction). They
are displayed in green in Figure 4. It can be seen that our
technique has placed them well, positioning all the soniﬁcation papers within the soniﬁcation group, and close to
one another. Meanwhile the paper on the other subject was
pulled to the border between two groups (ILP and SON).
The analysis of the maps can be further improved by providing a high level of interaction, additional mappings on
top of the surface (such as color, height, texture, ghyphs),
and other visual attributes.
Figure 5 shows the addition of visual attributes to the
map in Figure 3. Hierarchical clustering of the projected
data was performed, and it was mapped to color. The darker
the color, the closer the points in that region are.
Figures 6(a) and (b) show other text maps, pre-classiﬁed
in pseudo colors, demonstrating the capability of the technique to map related text close together. They have been obtained using distances based on NCD. It can be seen that the
news map is more incoherent than the others maps already
presented. This is natural, since everyday language patterns,
which are normally used in newsgroups, repeat across different subjects, rendering it more context dependent. Also
the two citation subsets of the kdvis data set (red and blue in

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

(a) surface.

(b) wire-frame.

Figure 4. 4-class dataset (CBR+3) data set,
with 50 control points

(a) height and color
mapped to levels of
clustering.

(b) height mapped to levels of clustering and color
mapped to class.

Figure 5. 2-class dataset (CBR+2) maps
the picture) are not distinguishable since they treat the same
subjects, so they are plotted together, an expected outcome
for a proper mapping.

(a) kdvis.

(b) news data set.

Figure 6. Further text maps.
Finally, the map of a data set with all scientiﬁc papers
used above put together is shown in Figure 7, also showing
a very good separation between the kdvis data set (blues
to white), and also CBR (red), ILP (yellow), IR (orange),
infovis04 and SON (green and lighter green).
Figure 8 shows the precision of the mappings. It was
calculated by taking the k neighbors of a point in the projection and checking whether it belongs to the same class.
Those them that did were plotted. It can be seen that under
that criterion the projection presents a very good precision

5. Conclusions

Figure 7. All scientiﬁc data sets put together

rate overall.

Figure 8. Precision for the maps presented in
previous pictures.

We have presented LSP, the Least Square Projection, a
dimension reduction technique that is able to project large
data sets in a satisfactory computational time. Besides preserving neighborhood relationships properly, LSP does not
require all distance measures to work, rendering is incremental, that is, new points can be added iteratively.
Computing neighborhood relationships is one of the
most expensive operations involved in our approach, which
may require sophisticated metric trees in order to be effective when dealing with large data sets. We are currently
developing ways to incorporate a metric tree (an M-tree)
in our implementation, which should further improve our
technique’s performance.
Another aspect that should be further investigated is how
to deﬁne the parameters involved in LSP. For example, how
to ﬁnd the optimal number of neighbors and control points
for a given data set. In fact, the choice of the control points
affects signiﬁcantly the quality of the projection, making it
an interesting challenge to design an efﬁcient algorithm to
perform such a task.
We have tested our approach on text document analysis
and visualization, and have demonstrated its ability to effectively and efﬁciently segment and cluster documents based
on their contents. This makes our technique a promising
tool for visual analysis of large bodies of text documents,
with far-reaching applications in various ﬁelds.

Acknowledgements
Table 2 shows the computational times for the projection,
using an AMD Athlon XP 2.16GHz (512 MB of RAM).
Since the steps for distance calculation and visual attribute
mapping are necessary in other text mappings as well, we
only present the time to generate the projection.

Table 2. Times in seconds to build the maps
Set
Building the Solving the
Total
matrix
system
CBR+2
0.046
0.0781
0.1241
CBR+3
0.046
0.1250
0.1710
kdvis
1.609
1.1563
2.7653
infovis04
0.046
0.0938
0.1398
2.7200
2.3125
5.0325
all
news
0.016
0.046875
0.0629

Those times represent an improvement of many times
over the fastest solution we had accomplished thus far [17].

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

This work has been supported by FAPESP Grants
03/02815-0, 04/01756-2, 04/09888-5 and 04/07866-4 as
well as CNPq Grants 300531/99-0 and 521931/95-5. Haim
Levkowitz’s research was partially conducted while he was
a Fullbright US Scholar to Brazil, August 2004 - January
2005.

References
[1] O. Alonso and R. Baeza-Yates. Alternative implementation
techniques for web text visualization. In Proc. of the First
Latin American Web Congress (LA-WEB 2003), pages 202–
203, Santiago, Chile, November 2003. IEEE Computer Society, IEEE Press.
[2] K. Andrews, W. Kienreich, V. Sabol, J. Becker, G. Droschl,
F. Kappe, M. Granitzer, P. Auer, and K. Tochtermann. The
infosky visual explorer: exploiting hierarchical structure and
document similarities. Information Visualization, 1:166–
181, 2002.
[3] R. Baeza-Yates. Visualizing large answers in text databases.
In Int. Workshop on Adv. User Interfaces (AVI’96), pages
101–107. ACM Press, 1996.

[4] P. Berkhin. Survey of clustering data mining techniques.
Technical report, Accrue Software, San Jose, CA, 2002.
[5] A. Booker, M. Condliff, M. Greaves, F. Holt, A.Kao,
D. Pierce, S. Poteet, and Y.-J. Wu. Visualizing text data
sets. Computing in Science and Eng., 1(4):26–35, 1999.
[6] K. Borner, C. Chen, and K. Boyack. Visualizing knowledge
domains. Annual Review of Informtion Science & Technology, 37:1–51, 2003.
[7] M. Chalmers. Using a landscape methaphor to represent a
corpus of documents. In A. U. Frank and I. Campari, editors,
Spatial Information Theory: A Theoretical Basis for GIS,
volume 716 of Lecture Notes in Computer Science, pages
377–390. Springer, 1993.
[8] M. Chalmers. A linear iteration time layout algorithm for
visualising high-dimensional data. In Information Visualization 1996, pages 127–132, San Francisco - CA, USA, 1996.
IEEE CS Press.
[9] M. Chalmers and P. Chitson. Bead: explorations in information visualization. In 15th International ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR’92), pages 330–337, 1992.
[10] R. Cilibrasi and P. Vit´anyi. Clustering by compression. IEEE
Trans. Information Theory, 51(4):1546–1555, 2005.
[11] C. Faloutsos and K. Lin. Fastmap: A fast algorithm for indexing, datamining and visualization of traditional and multimedia databases. In ACM SIGMOD Proc. Intl Conf. on
Manag. of Data, pages 163–174, San Jose-CA, USA, 1995.
ACM Press: New York.
[12] M. Floater. Parametrization and smooth approximation of
surface triangulations. Computer Aided Geometric Design,
4(13):231–250, 1997.
[13] S. Kaski, T. Honkela, K. Lagus, and T. Kohonen. Websom
- self-organizing maps of document collections. Neurocomputing, 1(1-3):110–117, 1998.
[14] A. Leuski and J. Allan. Lighthouse: Showing the way to relevant information. In InfoVIS, pages 125–130. IEEE Computer Society Press, 2000.
[15] A. Lopes, R. Minghim, V. Melo, and F. Paulovich. Mapping texts through dimensionality reduction and visualization techniques for interactive exploration of document collections. In IST/SPIE Symposium on Electronic Imaging
- Workshop on Visualization and Data Analysis, San Jose,
California, 2006.
[16] R. Minghim, H. Levkowitz, L. G. Nonato, L. Watanabe,
V. Salvador, H. Lopes, S. Pesco, and G. Tavares. Spider
cursor: A simple verstile interaction tool for data visualization and exploration. In Proceedings of GRAPHITE 2005,
pages 307–314, Dunedin, New Zeland, 2005. ACM Press.
[17] R. Minghim, F. Paulovich, and A. Lopes. Content-based
text mapping using multi-dimensional projections for exploration of document collections. In IST/SPIE Symposium on
Electronic Imaging - Workshop on Visualization and Data
Analysis, San Jose, California, 2006.
[18] A. Morrison, G. Ross, and M. Chalmers. A hybrid layout
algorithm for sub-quadratic multidimensional scaling. In
INFOVIS ’02: Proceedings of the IEEE Symposium on Information Visualization (InfoVis’02), page 152, Washington,
DC, USA, 2002. IEEE Computer Society.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

[19] G. Salton. Developments in automatic text retrieval. Science, 253:974–980, 1991.
[20] M. Sebrechts, J. Cugini, S. Laskowski, J. Vasilakis, and
M. Miller. Visualization of search results: A comparative
evaluation of text, 2d, and 3d interfaces. In 22nd ACMSIGIR Conf. Research and Development in Information Retrieval, pages 3–10. ACM Press, 1999.
[21] O. Sorkine and D. Cohen-Or. Least-squares meshes. In Proceedings of Shape Modeling International, pages 191–199.
IEEE Computer Society Press, 2004.
[22] O. Sorkine, Y. Lipman, D. Cohen-Or, M. Alexa, C. R¨ossl,
and H. Seidel. Laplacian surface editing. In Proceedings of
the Eurographics/ACM SIGGRAPH symposium on Geometry processing, pages 179–188. Eurographics Association,
2004.
[23] E. Tejada, R. Minghim, and L. Nonato. On improved projection techniques to support visual exploration of multidimensional data sets. Information Visualization Journal,
2(4):218–231, 2003.
[24] W. Tutte. How to draw a graph. Number 13, pages 743–768,
1963.
[25] E. Weippl. Visualizing content based relations in texts. In
Proc. of the 2nd Australian conference on User interface,
pages 34–41, Queensland, Australia, 2001. IEEE Computer
Society, IEEE Computer Society.
[26] J. A. Wise. The ecological approach to text visualization.
J. of the American Soc. for Inf. Sci., 50(13):1224–1233,
November 1999.
[27] J. A. Wise, J. J. Thomas, K. Pennock, D. Lantrip, M. Pottier,
A. Schur, and V. Crow. Visualizing the non-visual: spatial analysis and interaction with information for text documents. In Readings in information visualization: using vision to think, pages 442–450, San Francisco, CA - USA,
1995. Morgan Kaufmann Publishers Inc.

