POLARMAP - Efficient Visualisation of High Dimensional Data
Frank Rehm, Frank Klawonn, Rudolf Kruse
German Aerospace Center, Braunschweig, Germany
University of Applied Sciences of Braunschweig/Wolfenbuettel Germany,
Otto-von-Guericke-University of Magdeburg, Germany
frank.rehm@dlr.de, f.klawonn@fh-wolfenbuettel.de, kruse@iws.cs.uni-magdeburg.de
Abstract
Multidimensional scaling provides low-dimensional visualisation of high-dimensional feature vectors. This is a
very important step in data preprocessing because it helps
the user to appraise which methods to use for further data
analysis. But a well known problem with conventional MDS
is the quadratic need of space and time. Beside this, a
transformation of MDS must be completely recomputed if
additional feature vectors have to be considered. The POLARMAP algorithm, presented in this paper, learns a function, similar to NeuroScale, but with lower computational
costs, that maps high-dimensional feature vectors to a 2dimensional feature space. With the obtained function even
new feature vectors can be mapped to the target space.
Keywords— Visualisation, Multidimensional Scaling, Sammon’s Mapping.

1 Introduction
Visualisation is an important step as a part of data preprocessing, since the amount of data and their dimensionality
is growing fast. A visual assessment of a high dimensional
feature space is not possible without the help of appropriate methods. Many such techniques have been published so
far. Some of these methods provide dimension reduction,
the use of icons or rearrangement of the dimensions. Principal component analysis (PCA) is a very efficient method
for dimension reduction that unfortunately is not appropriate for manifolds. Multidimensional scaling (MDS) [1, 6]
is very powerful, mapping data to a low-dimensional feature space. Many modifications of MDS are published so
far, but high computational costs prevent their application
to large datasets [3, 11]. In recent years some research has
been done in this regard [2, 5, 8, 12].
In this paper we present a new approach for dimension
reduction. Our approach is a modification of published
MDSpolar . Instead of trying to preserve the distances between feature vectors as for MDS, our algorithm transforms
high-dimensional feature vectors into 2-dimensional feature
vectors under the constraint that the length of each vector is

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

preserved and the angles between vectors approximate the
corresponding angles in the original space as good as possible. As an improvement of MDSpolar , we will present an
algorithm that learns a function that enables the user to map
even new feature vectors to the target space. Finally, we will
describe a technique to learn such mappings from data with
O(n · log n) space- and time-complexity.
This paper is organised as follows: in section 2, we recall the conventional MDS method. In section 3, we will
briefly describe MDSpolar , which is the base algorithm for
our method that we will present in section 4. In section
5 we will discuss some experimental results. Finally, we
conclude with the last section.

2

Multidimensional Scaling

Multidimensional scaling (MDS) is a method that estimates the coordinates of a set of objects Y = {y1 , . . . , yn }
in a feature space of specified (low) dimensionality that
come from data X = {x1 , . . . , xn } ⊂ Rp trying to preserve the distances between pairs of objects. Different ways
of computing distances and various functions relating the
distances to the actual data are commonly used. These distances are usually stored in a distance matrix
Dx = dxij ,

dxij = xi − xj ,

i, j = 1, . . . , n.

The estimation of the coordinates will be carried out under
the constraint that the error between the distance matrix Dx
of the dataset and the distance matrix Dy = dyij , dyij =
yi − yj , i, j = 1, . . . , n of the corresponding transformed dataset will be minimised.
Thus, different error measures to be minimised were proposed, i.e. the absolute error, the relative error or a combination of both. A commonly used error measure, the so-called
Sammon’s mapping
E=

n

1
n

n

i=1 j=i+1

dxij

n

i=1 j=i+1

dyij − dxij
dxij

2

describes the absolute and the relative quadratic error. To
determine the transformed dataset Y by means of minimising error E a gradient descent method can be used. By
means of this iterative method, the parameters yk to be optimised, will be updated during each step proportional to the
gradient of the error function E. Calculating the gradient of
the error function leads to
∂E
=
∂yk

2
n

n

i=1 j=i+1

dxij

j=k

dykj − dxkj yk − yj
.
dxkj
dykj

After random initialisation for each projected feature vector
yk a gradient descent is carried out and the distances dyij as
∂dy

well as the gradients ∂yijk will be recalculated again. The algorithm terminates when E becomes smaller than a certain
threshold.
The complexity of MDS is O(c · n2 ), where c is the
(unknown) number of iterations needed for convergence of
the gradient descent scheme. Thus, MDS is usually not
applicable to larger datasets. Another problem of MDS
is that it does not construct an explicit mapping from the
high dimensional space to the lower dimensional space, but
just tries to position the lower dimensional feature vectors
in a suitable way. Therefore, when new data have to be
considered, they cannot be mapped directly to the lower
dimensional space, but the whole MDS procedure has to
be repeated. NeuroScale [7] is a scheme that tries to construct an explicit mapping for MDS in the form of a neural
network. However, it does not reduce the complexity of
MDS. In [3] a more efficient, but still iterative approach
was proposed making use of a step-by-step reduction by
one dimension based on determining the best projection in
each step. In [10], a different algorithm is proposed, not
needing any iterative scheme and whose complexity can be
reduced to O(n · log n).

3

MDSpolar - Multidimensional Scaling with
Polar Coordinates

Multidimensional scaling suffers from several problems.
Besides the quadratic need of memory, MDS, as described
above is solved by an iterative method, expensive with respect to computation time. Furthermore, a completely new
solution must be recalculated, if a new object is added to the
dataset.
For a p-dimensional dataset X MDSpolar determines
a 2-dimensional representation in polar coordinates Y =
{(l1 , ϕ1 ), . . . , (ln , ϕn )}, where the length lk of the original vector xk is preserved and only the angle ϕk has to be
optimised. This solution is defined to be optimal, if all angles between pairs of data objects in the projected dataset Y
coincide as well as possible with the angles in the original
feature space X.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

The use of polar coordinates in the space Y preserving
the length of each feature vector has various advantages.
On the one hand, the number of parameters to be optimised
is reduced, while the length presevervation already guarantees a roughly correct placement of the feature vectors in
the lower dimensional space. On the other hand, this property will enable us to reduce the complexity of the algorithm
based on the following idea. It is important to map feature
vectors that are close to each other in the original space X
to close vectors Y , whereas feature vectors with a large distance between them should also be mapped far away from
each other. However, for vectors with a large distance, it is
not important that we match the original distance exactly,
but it is sufficient to make sure that they will not be mapped
close to each other. When the length of two vectors differs significantly, we already guarantee a certain distance
between the projected vectors, even if the angle between
them is not matched at all. Using this property it will not
be necessary to consider the angles between all vectors. For
those vectors with a significant difference in length, we can
neglect the angle.
A straight forward definition of an objective function to
be minimised for this problem, taking all angles into account for the moment, would be
n k−1

E=

(|ϕi − ϕk | − ψik )2

(1)

k=2 i=1

where ϕk is the angle of yk , ψik is the positive angle between xi and xk , 0 ≤ ψik ≤ π. E is minimal, if the difference of the angle of each pair of vectors of dataset X and
the corresponding two vectors in dataset Y is zero. The absolute value is chosen in equation (1) because the order of
the minuends can have an influence on the sign of the resulting angle. As discussed in [10] equation (1) is not suitable
for finding an analytical solution or a gradient descent technique, since functional E is not differentiable.
In the mentioned paper it is proposed to minimise the
following functional instead
n k−1

(ϕi − ϕk − ψik )2 .

E=

(2)

k=2 i=1

The problem that arises with equation (2) is, that a simple
minimisation would not lead to acceptable results. This is
the case because an angle between yi and yk that might perfectly match the angle ψik , ϕi − ϕk can either be ψik or
−ψik . Therefore, when minimising functional (2) in order
to actually minimise functional (1), one can take the freedom to choose whether we want the term ϕi − ϕk or the
term ϕk − ϕi to appear in (2).
When we are free to choose between ϕi −ϕk and ϕk −ϕi
in equation (2), we take the following into account
(ϕk −ϕi −ψik )2 = (−(ϕk −ϕi −ψik ))2 = (ϕi −ϕk +ψik )2 .

Therefore, instead of exchanging the order of ϕi and ϕk ,
one can choose the sign of ψik , leading to

MDSpolar to minimise the following differentiable objective function

n k−1

(ϕi − ϕk − aik ψik )2

E=

(3)

k=2 i=1

with aik = {−1, 1}. In order to solve this modified optimisation problem of equation (3) we take the partial derivatives of E, yielding
k−1

∂E
= −2
(ϕi − ϕk − aik ψik ).
∂ϕk
i=1

(4)

To fulfil the necessary condition for a minimum one sets
equation (4) equal to zero and solves it for the ϕk -values,
which leads to
k−1
i=1

ϕk =

ϕi − k−1
i=1 aik ψik
.
k−1

˜=
E

n−1

n

Since according to our definition ψij ≥ 0, it is obvious that
˜min . Thus, a minimum of E
˜ might not be the
Emin ≤ E
minimum of E, but it can be used as a conservative estimation. Albeit, f might be any function, we discuss in this
work the following function style
˜,
f (x) = aT · x

(8)

where a is vector whose components are the parameters to
be optimised and x
˜ is feature vector x itself or a modification of x. In the simplest case we use
x
˜
a

= x
= (a1 , a2 , . . . , ap )T

(9)

where f describes in fact the linear combination of x.
Assuming that a certain component of x affects the transformation not linearly but quadratically or exponentially, it
may be useful to compute some additional components from
x with the objective to gain more coefficients, which could
improve the transformation.
An example for quadratic components derived from x is
described by the following choice:

4 POLARMAP
As an extension of MDSpolar we propose in this work
a method that learns a function f that provides for any pdimensional feature vector xk the corresponding angle ϕk
that is needed to map the feature vector to a 2-dimensional
feature space. As for MDSpolar the length of vector xk is
preserved. With the obtained function also angles for new
feature vectors can be computed. A 2-dimensional scatter
plot might be not suitable, when visualising mappings for
large datasets. With the computed function it is simple to
produce information murals, which allow more comprehensive visualisations [4].
Analogous to functional (1) we define our objective function E as follows:

(7)

i=1 j=i+1

(5)

Thus, on the one hand, neglecting that we still have to
choose aik , our solution is described by a system of linear equations which means its solution can be calculated
directly without the need of any iteration procedure. On
the other hand, as described above, one has to handle the
problem of determining the sign of the ψik in the form of
the aik -values. A greedy strategy with O(n · log n) for this
is also proposed in [10].

2

(f (xi ) − f (xj ) − ψij ) .

x
˜ =
=

a

(x1 , . . . , xp , x1 x1 , . . . , x1 xp ,
x2 x2 , . . . , x2 xp , . . . , xp xp )T

(10)

(a1 , . . . , ap , a11 , . . . , a1p ,
a22 , . . . , a2p , . . . , app )T .

(11)

Replacing term f by the respective function we obtain
n−1

E˜

n

˜j − ψij
aT x˜i − aT x

=

2

(12)

i=1 j=i+1
n−1

n

=

aT (˜
xi − x˜j ) − ψij

2

.

(13)

i=1 j=i+1

˜j by x
˜ij and obtain
For a better readability we replace x
˜i − x
n−1

n

(|f (xi ) − f (xj )| − ψij )2 .

E=

(6)

i=1 j=i+1

E is minimal, if, for each pair of feature vectors, the difference of the two angles, which are computed by the respective function f is equal to the measured angle of the
two vectors in the original space. Since functional (6) is not
differentiable, we propose analogous to the procedure for

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

˜
E

n−1

n

=

aT x
˜ij − ψij

2

.

(14)

i=1 j=i+1

The derivative of E˜ w.r.t. a can by easily obtained
˜
∂E
∂a

n−1

=

n

2

˜ij − ψij x
aT x
˜ij .
i=1 j=i+1

(15)

Setting equation (15) equal to zero to fulfil the necessary
condition for a minimum we end up with
n−1

n

0=

aT x
˜ij
˜ij − ψij x

(16)

i=1 j=i+1

which results in a system of linear equations in a =
(a1 , a2 , . . . , ap )T .
As mentioned already, angles computed by f (xi ) −
f (xj ), might be positive or negative, while ψij is always
positive by definition. Thus, in the case where aT x
˜ij < 0
holds, E˜ might be minimal, but our original objective funcxij
tion E might not be minimal. Hence, replacing x
˜ij by −˜
in this case might lower the error. Consequently, finding the
appropriate sign for x˜ij is a crucial step when minimising
˜ For common datasets determining the exact solution for
E.
this problem is too expensive regarding computation time.
In the following section we describe a greedy strategy that
approximates a relaxation of this problem.

4.1

Greedy Approximation of POLARMAP

Determining the sign for each x˜ij requires exponential
need of computation time in the number of feature vectors.
For real-world datasets this is unacceptable. When relaxing the problem in favour to an approximation of the exact solution one can reduce the time complexity down to
O(n·log n). In this section we begin with a very fast greedy
algorithm that finds rather poor approximations of the exact
solution, which are suitable for initialisation purposes for
more complex approximation schemes.
In the following, we use an n × n-matrix Θ with θij = 1
when the sign for x
˜ij is positive and θij = −1 when the
sign for x˜ij is negative. As algorithm (1) shows this greedy
algorithm does not change these signs for the respective x
˜ij
until θij aT x˜ij < 0 is satisfied and computes afterwards the
updated components of a by solving the revised system of
linear equations. Usually, this algorithm converges after a
few iterations. This approach is very efficient and simple at
the same time.
Algorithm (2) shows another greedy algorithm. Always,
when θij changes, a will be recomputed immediately and
the next iteration starts. Θ changes during one iteration at
most in one point – namely θij , otherwise the algorithm
ends without changing any θ. Thus, the algorithm greed˜ij < 0 is
ily changes the first θij , when condition θij aT x
satisfied. From this it follows that the algorithm only finds
˜ which is the reason why we speak
a local minimum of E,
about a relaxation of the problem. It is advisable to initialise Θ with algorithm (1). Otherwise, too many of iterations will be needed until convergence. With algorithm (2)
very accurate transformations will be found – indeed computational costs are fairly high. In the following subsection,
we describe a technique that reduces the computation cost
drastically.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

4.2

Generalisation of POLARMAP

Although the above greedy algorithm is efficient, for
large datasets too many iterations will be needed until convergence. Its time and space complexity are also quadratic
in the number of data, so that it is not applicable to larger
datasets. In order to evaluate the objective function, all x˜ij and all ψij -values must be computed in advance, causing
already the quadratic complexity.
˜ = {˜
˜2 , . . . , x
˜n }
X
x1 , x
let Ψn×n be a matrix with the pairwise angles ψij between all (xi , xj )
let Θn×n be a matrix where θij = 1, ∀i, j

n−1
n
T˜ −ψ
a ← solve
˜ij = 0
ij
ij x
j=i+1 a x
i=1
˜
compute E
repeat
˜
˜ ←E
E
for i = 1 to n − 1 do
for j = i + 1 to n do
if θij aT x
˜ij < 0 then
θij ← −θij
end if
end for
end for
n−1
n
T˜ −ψ
a ← solve
˜ij = 0
ij
ij x
j=i+1 θij a x
i=1
˜
compute E
˜
until E˜ ≤ E

Algorithm 1: Greedy POLARMAP
˜ = {˜
˜2 , . . . , x
˜n }
X
x1 , x
let Ψn×n be a matrix with the pairwise angles ψij between all (xi , xj )
let Θn×n be a matrix where θij = 1, ∀i, j
n−1
n
T˜ −ψ
˜ij = 0
a ← solve
ij
ij x
j=i+1 a x
i=1
˜
compute E
repeat
˜
˜ ←E
E
for i = 1 to n − 1 do
for j = i + 1 to n do
if θij aT x
˜ij < 0 then
θij ← −θij

a ← solve
˜
compute E
GOTO: check
end if
end for
end for
LABEL: check
˜
until E˜ ≤ E

n−1
i=1

n
j=i+1

˜ij − ψij x
θij aT x
˜ij = 0

Algorithm 2: Greedy POLARMAP
The greedy algorithm must also compute many (again
˜ij that
quadratic in the number of data) scalar products aT x
contribute not or only little to the quality of the transformation. Thus, if we had a measure to decide whether an adaptation of θij might be target-oriented or not, we could save
computation time by skipping the computation of nonessential scalar products.

˜ acAs a matter of fact, the computation of the error E
counts for the greatest part of the computation resources.
In the following we will discuss the problem how to reduce computation time due to dropping hopefully dispensable terms.
As for MDSpolar we provide for POLARMAP a generalisation by introducing weights wij for our objective func˜ that results in
tion E
˜=
E

n−1

n

wij a x
˜ij − ψij
T

2

might be useful if a certain threshold can be provided. To
decrease the computational complexity, weights should be
chosen in such a way that for feature vectors with a certain
(large) distance the respective weights become zero. The
following function describes a simple weighting function
that behaves as just mentioned:

w(zr ) =
.

(17)

⎧
⎨
⎩

zr −ϑ
1−ϑ

, if

0

, otherwise

zr ≥ ϑ

(21)

i=1 j=i+1

Again, we obtain the following system of linear equations
˜
after taking partial derivatives of E
˜
∂E
=2
˜ij
wij aT x
˜ij − ψij x
∂a
i=1 j=i+1
n−1

n

(18)

and setting equation (18) to zero to fulfil the necessary condition for a minimum which leads to
n−1

n

wij aT x˜ij − ψij x˜ij .

0=

(19)

i=1 j=i+1

The introduction of weights opens new ways to define
and handle the objective function. We cannot only assign
a weight to individual errors, controlling in this way how
much influence single errors have on the final result. We
can also consider relative instead of absolute errors. For
2
corresponds to relative
example, choosing wij = 1/ψij
MDSpolar . The difference between the angle in the original
space and the corresponding angle in the target space, will
not account directly to the computation of a but weighted
with ψij .
Weights can be chosen in such a way that only feature
vectors, which are similar to a certain degree will be taken
into account, when computing θij . Since our transformation
preserves the length of each data vector, it is guaranteed that
vectors with a large difference in length will not be mapped
to close points in the plane, even though their angle might
not be matched at all. Therefore, we propose to use a small
or even zero-weight for pairs of data vectors that differ significantly in their length. The weight could be defined as a
function of the difference between the length values li and
lj of two data vectors:
wij = w(li , lj ) = w(z).

(20)

We can use the absolute difference for z, i.e. z = za =
|li − lj | . This might be useful if certain information about
the structure of the data is known in advance. The argument
zr for relative weighting functions
z = zr = min

li lj
,
lj li

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

where ϑ ∈ [0, 1]. With the threshold ϑ one can control indirectly the fraction of the data that will be used to affect
a. Thus, small values for ϑ lead to many non-zero weights
which comes along with high computational complexity.
Values near 1 for ϑ lead to a quickly decreasing weighting
function and to low computational complexity, respectively.
Any other function can be used as weighting function.
For reasons of an easy implementation and low computational complexity a decreasing function which leads to a
more or less large fraction of zero weights should be used.
For an efficient implementation it is useful to sort the feature vectors by means of their length. Note that this can be
achieved with O(n · log n) time complexity.
It is obvious, when the feature vectors are sorted in that
way, weights for a given i are decreasing while incrementing j because the length of x
˜ij is increasing. Therefore, the
inner f or-loop in algorithm (2) can be terminated if w(z)
becomes zero for the first time. Since the weighting function is decreasing, a further iteration would lead to zero,
too. In cases where clusters with a large amount of data are
expected in a dataset, it might be rather useful to limit the
maximum number of pairs to consider for the calculation
of a and θ than setting a larger threshold. It might also be
useful to reduce ϑ locally, when only few vectors satisfy the
condition in equation (21). With a limitation of the number
of weights w > 0 and a moderate ϑ at the same time, it can
be achieved that the number of weights considered for the
calculation of ϕk does not differ too much for different ϕk
and limited computation time can be guaranteed.
Algorithm (3) is a modification of the previous algorithm
regarding the mentioned aspects. Beneath sorting X, the
inner f or-loop now contains the condition to terminate the
loop if the weighting function indicates that for the given i
no further x
˜ij has to be considered.
Note that we reduce computation time drastically, if
we choose an appropriate weighting function. Instead of
O(c·n2 ) with c as number of iterations, we obtain O(c·n·m)
where m is the maximum bin size. The bin size for a feature vector xi refers to the number of non-zero weights wij .
With this bin-strategy we do not only reduce the number of

pairs x
˜ij to be considered, much more important is the ef˜ With algorithm (3) we
fect on the computation of a and E.
introduced the array ni , i = 1 . . . n that is initialised with
ni = min(i + maxbinsize, n), ∀i. When computing a and
˜ it is no longer necessary to sum up the difference beE,
tween the target angle and ψij for vectors which are out of
the bin, since it will be weigthed with wij which is zero.
˜ = {˜
˜2 , . . . , x
˜n }
X
x1 , x
sort X
set maxbinsize
initialise ni
let Ψn×n be a matrix with the pairwise angles ψij between all (xi , xj )
let Θn×n be a matrix where θij = 1, ∀i, j

ni
n−1
T˜ −ψ
˜ij = 0
a ← solve
ij
ij x
j=i+1 wij a x
i=1
˜
compute E
repeat
˜
˜ ←E
E
for i = 1 to n − 1 do
for j = i + 1 to ni do
wij ← w(li , lj )
if wij > c then
ni ← j − 1
GOTO: check1
end if
if θij aT x
˜ij < 0 then
θij ← −θij

a ← solve
˜
compute E
GOTO: check
end if
end for
LABEL: check1
end for
LABEL: check
˜
until E˜ ≤ E

n−1
i=1

ni
j=i+1

˜ij = 0
wij θij aT x
˜ij − ψij x

Algorithm 3: Greedy POLARMAP
Of course, if the bin size is only limited by a weighting
function that leads only to few weights wij = 0, the gain of
computation time tends to zero.

5 Results
In this section, we discuss the results of POLARMAP
on two synthetic 3-dimensional datasets, on two benchmark
datasets and on a real world dataset as well.

5.1

Demonstrative Examples

Figure 1 shows two synthetic datasets. The cube dataset
(a) is about a dataset, where data points scatter around the
corners of an imaginary 3-dimensional cube. Thus, the
cube dataset contains eight well separated clusters. The coil
dataset (b) contains a 1-dimensional manifold comparable
to a serpentine. Furthermore, we apply POLARMAP to the
well known iris dataset and on the wine dataset as well.
A Sammon’s mapping of the 4-dimensional iris dataset is

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

shown in figure 4 (a). We split this dataset into a training dataset and a test dataset to demonstrate the capability
POLARMAP to generalise. The wine dataset results from
a chemical analysis of wines grown in the same region in
Italy but derived from three different cultivars. The analysis
determined the quantities of 13 constituents found in each
of the three types of wines. A Sammon’s mapping of the
13-dimensional wine dataset is shown in figure 4 (c).
Our tests have shown that algorithm (1) is a good initialisation for algorithm (2) and (3). Since algorithm (2)
and (3) compute the actual coefficients immediately after
changing one sign, without initialisation, many iterations
would be needed for large datasets until convergence. For
that reason it is advisable to initialise algorithm (2) and
(3) with algorithm (1). Note that algorithm (1) does not
have quadratic complexity, when we introduce corresponding weights leading to moderate bin sizes. The following
transformations result from this procedure.
Figure 2 shows some results on the cube dataset. The
greedy algorithm (1) converges already after three iterations
when using x
˜ according to equation (9) (subfigure (a)). Similarly, greedy algorithm (1) converges after five iterations,
when using x
˜ according to equation (10) (subfigure (b)).
For the relatively simple cube dataset it is not of much importance to generate additional components for x
˜ and additional components a, respectively. The results from applying algorithm (2) and algorithm (3) are comparable and
omitted here for reasons of restricted space. These transformations are based on the x
˜ = x cube dataset. Obviously, a
linear function with three coefficients a is sufficient to map
the cube dataset to a 2-dimensional feature space. The eight
clusters are clearly separated in the target space, too. Using weights according to algorithm (3) and the following
weighting function
w(za ) =

1
0

if za < c
otherwise

with c = 35 leads to the transformation shown in subfigure
(d). The results with algorithm (2) and algorithm (3) are
quite similar for the cube dataset, even though algorithm (3)
needs less computation time.
The results for the coil dataset are shown in figure 3.
Again, subfigure (a) results from algorithm (1) with x
˜=x
and subfigure (b) results from x
˜ according to equation (10).
The results for both representations of the dataset are similar regarding a majority of the characteristics. Algorithm
(1) converges after few iterations for both datasets. Subfigure (c) and (d) show the results of algorithm (3) on the
dataset (˜
x = x) without initialising Θ other than 1. Subfigure (c) results from using a bin size of 10. Subfigure (d)
2
for all wij inside the bin of
results from using wij = 1/ψij
size 10. Chosing maximum bin size 10 leads to the fact that
no sign will be changed and thus the algorithm stops after

(a) Cube dataset

(b) Coil dataset

Figure 1: Synthetic datasets

(a) Algorithm 1 (˜
x = x)

(b) Algorithm 1 (˜
x according to eq.(10))

Figure 2: Results on the cube dataset
one iteration. Designing the weighting function such that
a larger bin size has to be taken into account, one can observe that already after only three signs have changed, the
transformation gets the major characteristics as the one in
subfigure (a). Even if this transformation is very simple,
some requirements, i.e. preserving distances between feature vectors, are fulfilled.

target space by means of the learned function. The mapping
of the training dataset is plotted with the different symbols
again, each for the corresponding class. The mapped feature
vectors of the test dataset are additionally marked with a circle around the corresponding symbol. As the figure shows,
the learned function maps the feature vectors in the right
way.

Since a function is learned by POLARMAP it becomes
possible to map new vectors in the target space. To demonstrate the power of POLARMAP, we applied it on the well
known iris dataset. Figure 4 (a) shows the Sammon’s mapping of the iris dataset. The different classes are represented
by different symbols. Subfigure (b) shows the transformation with POLARMAP. For this example, the iris dataset is
split into a training dataset and a test dataset. The training
dataset consists of 80% of each class. This part of the data
is used to learn the desired coefficients. The test dataset that
contains the remaining 20% of the data, is mapped to the

Figure 4 (c) shows the Sammon’s mapping of the wine
dataset. The three classes are marked with different symbols again (class 1: +, class 2: ✷, class 3: ✸). Based on
the Sammon’s mapping, the three classes cannot be separated linearly. Notably class 2 and class 3 cannot be distinguished. The transformation of the wine dataset with POLARMAP is shown in figure 4 (d). Both transformations are
similar regarding the scattering of the different classes. The
mapping of the new feature vectors (marked with a circle
around the respective symbol) meets the expectations from
the mapping of the training dataset.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

(a) Algorithm 1 (˜
x = x)

(b) Algorithm 1 (˜
x according to eq. (10))

(c) Algorithm 3 (absolute)

(d) Algorithm 3 (relative)

Figure 3: Results on the coil dataset

5.2

Visualisation of complex Weather Data

Recently, a study concerning the prediction of aircraft
flight durations was published [9]. The study is based
on two combined data sets, respectively concerning the
weather conditions and the traffic at Frankfurt Airport.
The weather data come from the ATIS (Automatic Terminal Information Service) weather data set of the year
1998: different sensors present in the airport capture several
weather characteristics and define a weather report. Such
a report is released every 30 minutes (in case of rapidly
changing weather, the frequency is increased); this corresponds to 11878 data. Each weather report contains information such as the temperature, the air pressure and precipitation information, e.g. the presence of snow, rain or hail.
All these features are numerical except for the precipitation
attribute which is categorical and can take 30 different values. As a preprocessing step, this precipitation attribute is
replaced by a set of binary attributes, respectively associated
with each of its possible values. As a part of data preprocessing, several other transformations are made which are

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

explained in [9] in detail.
In addition to the weather data set, information about the
traffic is available in the form of a data set that contains
the arrival times of all aircraft at Frankfurt Airport for the
observed time period. Since the delay that is caused by the
weather factors in the vicinity of the airport is of interest, the
point in time of the aircraft’s entrance in the airport vicinity
(the so-called Terminal Management Area, TMA) and the
time when the corresponding aircraft is landing are considered. The difference between these two times corresponds
to the flight duration in the TMA. This quantity is independent of delay that may be caused at the departure airport
or during the flight. In average, the TMA flight duration is
about thirty minutes. The aim is to study delays corresponding to longer flight durations in the TMA. Lastly a traffic
attribute is derived from these data: it has been noticed that
the traffic in the airport has an important influence on the
flight duration, it is thus added to the weather attributes to
predict the flight duration.
POLARMAP is applied to the data to visualise the re-

(a) Sammon’s mapping of the iris dataset

(b) Mapping of the iris dataset with POLARMAP

(c) Sammon’s mapping of the wine dataset

(d) Mapping of the wine dataset with POLARMAP

Figure 4: Results on the iris dataset and the wine dataset
lationships between flight duration and weather factors. An
early study has shown that the weather can be easily divided
into cloudy and non-cloudy weather since the data form two
distinct clusters that can be separated almost linearly [9].
For the purpose of visualisation this procedure is followed
and both subsets of data are transformed separately. Due to
restricted amount of space we discuss in this work just the
non-cloudy weather. In the mentioned study, a strong effect
of the traffic on the travel time could be determined. Therefore, this section shows visualisation results of the weather
data both including the traffic attribute and excluding it.
Figure 5 (a) shows a mapping of the non-cloudy weather
including the traffic attribute using POLARMAP. In order
to examine the relationships between the weather factors
and the flight duration, three classes of travel times are defined: one class represents weather data associated with
short travel times with up to one minute more than the average travel time, it is represented in green. A second class,
represented in blue, corresponds to medium travel times
with up to eight minutes delay as compared to the average

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

travel time value. The last class, in red, stands for weather
data associated with even later flights. Early flights prevail,
less than 10% of the flights are more than eight minutes
later than the average. The figure shows clearly that the
three classes overlap to some degree. Whereas the green
class is spread allover the feature space, the blue class and
the red class take the upper right region of the feature space
mainly. This effect is mainly due to the traffic information.
Figure 5 (b) shows the transformation of the same data set
excluding the traffic information. This figure reveals that,
without traffic information, the different classes are even
harder to distinguish.

Conclusion
In this paper we have presented a powerful data visualisation method. Under the constraint to preserve the
length of feature vectors, it was our aim to find a mapping
that projects feature vectors from a high-dimensional to the
plane in such a way that we minimise the errors in the angles between the mapped feature vectors. The solution is

(a) Transformation with POLARMAP of the non-cloudy weather data including traffic information

(b) Transformation with POLARMAP of the non-cloudy weather data
excluding traffic information

Figure 5: Mapping of the weather dataset
described by a system of linear equations. To overcome the
problem in high-dimensional feature spaces that no differentiation between positive and negative angles can be made
as for a 2-dimensional feature space, three algorithms are
provided to obtain the desired signs for the angles. With the
bin-algorithm, we presented an algorithm that lowers the
computation complexity down to O(n · log n).
With the function, learned by means of POLARMAP it
is possible to map even new feature vectors to the target
space without any extra costs. Experimental results demonstrate the power of the approach. For very large datasets, it
is also possible to construct a mapping from a sample of a
reasonable size with acceptable computation time and then
project the whole datasets using the constructed mapping.
For large datasets it is also recommended, not to display
them as a scatter plot with a dot or symbol for each feature
vector, but to use density-based scatter plots [4] that our
method also supports.

References
[1] Borg, I., Groenen, P.: Modern Multidimensional Scaling: Theory and Applications. Springer, Berlin, 1997.
[2] Chalmers, M.: A Linear Iteration Time Layout Algorithm for Visualising High-Dimensional Data. Proceedings of IEEE Visualization 1996, San Francisco, CA,
127–132, 1996.
[3] Faloutsos, C., Lin, K.: Fastmap: A Fast Algorithm
for Indexing, Data-Mining and Visualization of Traditional and Multimedia Datasets. Proceedings of ACM
SIGMOD International Conference on Management of
Data, San Jose, CA, 163–174, 1995.
[4] Jerding, D.F., Stasko, J.T.: The information mural: a
technique for displaying and navigating large informa-

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

tion spaces. Proceedings of the 1995 IEEE Symposium
on Information Visualization, 43–50, 1995.
[5] Jourdan, F., Melancon, G.: Multiscale hybrid MDS.
Eighth International Conference on Information Visualisation (IV’04), 388–393, 2004.
[6] Kruskal, J.B., Wish, M.: Multidimensional Scaling.
SAGE Publications, Beverly Hills, 1978.
[7] Lowe, D., Tipping, M.E.: Feed-Forward Neural Networks Topographic Mapping for Exploratory Data
Analysis. Neural Computing and Applications, 4, 83–
95, 1996.
[8] Morrison, A., Ross, G., Chalmers, M.: Fast Multidimensional Scaling through Sampling, Springs and Interpolation. Information Visualization, 2, 68–77, 2003.
[9] Rehm, F., Klawonn, F: Learning Methods for Air Traffic Management. Lecture Notes in Computer Science,
Springer, 992-1001, 2005.
[10] Rehm, F., Klawonn, F., Kruse, R.: MDSpolar - A
New Approach for Dimension Reduction to Visualize
High Dimensional Data. In: Advances in Intelligent
Data Analysis VI: 6th International Symposium on Intelligent Data Analysis (IDA 2005), Springer, 316–327,
2005.
[11] Tenenbaum, J.B., de Silva, V., Langford, J.C.: A
Global Geometric Framework for Nonlinear Dimensionality Reduction. Science, 290, 2319–2323, 2000.
[12] Williams, M., Munzner, T.: Steerable, Progressive
Multidimensional Scaling. 10th IEEE Symposium on
Information Visualization, Austin, TX, 57–64, 2004.

