DOI: 10.1111/j.1467-8659.2011.01972.x

COMPUTER GRAPHICS

forum

Volume 30 (2011), number 8 pp. 2231–2245

Context-Based Coding of Adaptive Multiresolution Meshes
Christoph von Tycowicz, Felix K¨alberer and Konrad Polthier
Freie Universit¨at Berlin, Germany
{Christoph.vonTycowicz, Felix.Kaelberer, Konrad.Polthier}@fu-berlin.de

Abstract
Multiresolution meshes provide an efficient and structured representation of geometric objects. To increase the mesh
resolution only at vital parts of the object, adaptive refinement is widely used. We propose a lossless compression
scheme for these adaptive structures that exploits the parent–child relationships inherent to the mesh hierarchy.
We use the rules that correspond to the adaptive refinement scheme and store bits only where some freedom of
choice is left, leading to compact codes that are free of redundancy. Moreover, we extend the coder to sequences of
meshes with varying refinement. The connectivity compression ratio of our method exceeds that of state-of-the-art
coders by a factor of 2–7. For efficient compression of vertex positions we adapt popular wavelet-based coding
schemes to the adaptive triangular and quadrangular cases to demonstrate the compatibility with our method.
Akin to state-of-the-art coders, we use a zerotree to encode the resulting coefficients. Using improved context
modelling we enhanced the zerotree compression, cutting the overall geometry data rate by 7% below those of the
successful Progressive Geometry Compression. More importantly, by exploiting the existing refinement structure
we achieve compression factors that are four times greater than those of coders which can handle irregular
meshes.
Keywords: multiresolution, subdivision surfaces, progressive mesh coding, geometry compression, wavelets,
zerotree coding, hierarchical representations, level-of-detail
ACM CCS: I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling—Geometric algorithms,
languages, and systems.

1. Introduction
Multiresolution meshes that arise through successive subdivision of a coarse base commonplace in a variety of areas such
as the movie industry, computer aided design and in numerical simulations. The computing power of todays computer
systems and the availability of advanced modelling software
make it easy to generate grids with up to several million
vertices. Storing those meshes in a raw data format is notoriously expensive due to the sheer amount of data. This is
where compression comes into play.
Adaptive refinement, that is the introduction of detail only
where it is needed, is an essential strategy to master the processing, rendering, transmission and storage of such meshes.
For uniform refinement, the connectivity of the mesh can
be represented by a coarse base complex and the number
c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics
Association and Blackwell Publishing Ltd. Published by
Blackwell Publishing, 9600 Garsington Road, Oxford OX4
2DQ, UK and 350 Main Street, Malden, MA 02148, USA.

of subdivision levels. In contrast, adaptively refined meshes
exhibit a non-trivial hierarchical structure. To the best of our
knowledge, the lossless compression of adaptive hierarchies
has not been researched into before.
Generally, compression comes in two stages: The first is
a lossy stage, where essential information of the input is
extracted and negligible data is dropped. The data decimation
is then followed by lossless encoding in which the remaining
data are transcoded into a compact byte stream, typically
using entropy coding such as Huffman or arithmetic coding.
In view of mesh coding, the mesh data consists of connectivity, geometry and possibly attribute data such as
colours and texture coordinates. Three-dimensional (3D)
mesh coders are often referred to as lossless if they preserve the original connectivity of the mesh, even if floating

2231

2232

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes

point data of coordinates and attributes are truncated to a
fixed precision. This tolerance within the “lossless” category
may be due to the fact that geometry data will never be free
from errors, and errors introduced by truncating the least significant bits of a float value are often negligible compared
to noise, discretization and quantization errors during mesh
acquisition.

The FreeLence [KPRW05] and Angle Analyzer [LAD02]
algorithms exploit correlation between connectivity and geometry by accessing already encoded geometry data when
encoding connectivity and vice versa, allowing it to push the
bit rates below that of [IS06] and [Szy02]. Although FreeLence is especially performant in the triangular case, Angle
Analyzer is optimized for quadrangular meshes.

Lossy mesh coders consider the connectivity of the mesh
as auxiliary information that does not contribute to the shape
of the model. In the same manner, tangential positions of
vertices within the surface are regarded as negligible. The
intention of those coders is usually the best reproduction of
the shape with respect to some distance norm within a given
byte limit. The mesh is often remeshed to a semi-regular
mesh that allows wavelet analysis to compress the geometry
data.

For progressive transmission, models are often simplified
or remeshed [LSS*, GVSS00] to generate a simple base mesh
from arbitrary connectivity meshes. In this context, waveletbased coding has proven itself as the approach for efficient
compression. Wavelet transforms recursively construct lower
resolution approximations of a given input model, decorrelating high- and low-frequency geometry data. The difference of the details to predictions based on the coarse data
are stored as wavelet coefficients. These typically feature a
smaller entropy than the original data yielding superb compression rates.

We consider connectivity compression a vital issue because the outcome of many algorithms from geometry processing and numerical analysis depend on an exact reproduction of connectivity—think of animation or morphing.
In our work, we assume that the data reduction has already
taken place. Our input models are hierarchical models that are
adaptively refined. We assume that somebody has carefully
selected important details and pruned the negligible ones by
some criterion, be it by preservation of shape, normals, visual
impact or by some numerical criteria. The use of a lossy black
box encoder is prohibitive if no further detail should be lost.
Such situations arise for example in optimal control problems with time-dependent PDEs where several frames with
varying refinement structure have to be stored. In this case,
the base mesh is stored just once, with different refinement
stages for each time step. The storage of view-dependent refinements of objects in virtual environments creates a similar
situation.

1.1. Related work
Numerous compression schemes for surface meshes have
been developed for single-rate coding (compressing the
whole mesh in a region-growing fashion) as well as progressive coding (encoding the model from coarse to fine). On
the single-rate side Edgebreaker [Ros99] and the method of
Touma and Gotsman [TG98] are the most prominent coders
for triangle meshes which have spawned a wealth of variants
and improvements. Among the best-performing variants for
connectivity compression is the early-split coder of Isenburg and Snoeyink [IS06] and the optimized Edgebreaker
encoding of Szymczak [Szy02]. These coders profit from
mesh regularity and are able to push the bit rate well below the Tutte limit [Tut62] of roughly 3.24 bits per vertex.
Many triangle mesh coders have been generalized to polygonal meshes, such as Martin Isenburg’s method [Ise02] which
extends the Touma-Gotsman coder.

Wavelet-based coding schemes exist for both irregular and
semi-regular meshes. The first group is based on mesh simplification methods that progressively remove vertices causing
the smallest distortion. Bits are written to identify a vertex
within the mesh as well as its geometric position in order to
be able to reconstruct the original mesh. Prominent coders of
this group are [IS99, AD01, VP04].
The best results for geometry compression however have
been achieved for semi-regular meshes. Based on stencils
from subdivision schemes, efficient wavelet transforms have
been derived. The best known wavelet-based coder in this category is the progressive geometry compression (PGC) codec
by Khodakovsky et al. [KSS00] adapting the renowned zerotree coding scheme [Sha93] from image compression. A
wealth of descendants have been proposed extending PGC
to different types of meshes [KG03], resolution scalability [AMG05] and efficient embedded quantization [PA05].
However, these coders only aim at the compression of the
geometry and do not allow lossless reconstruction of the
connectivity even for meshes generated through adaptive refinement. (Although the normal mesh compression by Khodakovsky et al. [KG03] is able to generate such meshes,
the adaptivity is controlled by the coder, thus neglecting any
original criteria.)
Adaptively refined hierarchies also play a major role in
point cloud compression. In [BWK02] and many follow-ups,
a bounding box is adaptively refined up to a certain degree,
and the leaf cells at the finest level then represent the point
positions. Although these methods also compress adaptive
trees, we propose optimization strategies specialized for the
situations on surface meshes, such as non-trivial root domains, red-green conformity and balanced refinement. On
the contrary, point cloud compression schemes utilize characteristics of surface geometry. For instance, close-by points
are expected to be nearly coplanar, as done in [SK06].

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes

2233

Figure 1: Adaptive refinement of a bone model. Elements are coloured according to our coding scheme. We store a bit for each
blue and red triangle, specifying whether it is refined or not. These bits are sufficient to reconstruct the connectivity of the model.
All other triangles can be reconstructed using rules of the refinement scheme as explained in the text.
1.2. Contribution
Our major contribution is a connectivity compression scheme
that is adapted to the special characteristics of adaptive multiresolution meshes. We convert the tree-like hierarchical
structure to a binary stream, and use the refinement scheme’s
rules, to prune redundant bits. With context-based arithmetic
coding and an adapted traversal of the mesh we can furthermore take advantage of structural regularities that are
typically present in real-world data. In combination, these
measures push the data rates significantly below those of
general single-rate and progressive coders, outperforming
state-of-the-art by factors of 2–7. In addition, we present extensions to our compression that exploit correlations of the
refinement structure in sequences of time-dependent grids.
To show that our connectivity compression for adaptive
hierarchies works seamlessly with leading geometry compression schemes, we have extended the well-proven,
wavelet-based progressive geometry coding (PGC) of Khodakovsky et al. [KSS00] to adaptive triangle- and quad-based
hierarchies. As in connectivity coding we applied contextbased modelling for geometry compression. For uniform hierarchies we could achieve gains of 7% on average over PGC
for the coding of wavelet coefficients. Furthermore, we surpass non-specialized coders (Wavemesh [VP04] and PMC
[Ise02]) by average factors of 2.8 for adaptive triangular and
4.5 for quadrilateral meshes.
All our methods are described for triangular and quadrilateral surface meshes and we provide connectivity and geometry compression results for both cases. Although we illustrate
our scheme for adaptive surface hierarchies, the methods are
not restricted to these meshes, and an extension to adaptive
tetrahedral or hexahedral meshes is straightforward.
Our scheme is fast and can be implemented to run in linear
time. The code is progressive, that is after decoding the base

mesh, the topology and coarse shape is reconstructed. Further
details will be added from coarse to fine levels as more code
is processed.

2. Hierarchy Coding
In this section, we explain how we encode the hierarchical
structure of adaptive multiresolution meshes and thus their
connectivity. First, we will explain the concepts of adaptive
triangular and quadrilateral refinement, before we outline
our encoding procedure in Section 2.2. Sections 2.3–2.6 then
elaborate on the details.

2.1. Adaptive refinement schemes
In the field of subdivision surfaces and FEM, a variety of
refinement schemes have been devised. Among them, the
dyadic split (cf. butterfly [DLG90] or Loop [Loo87]) for
triangle surfaces and the face split (cf. Catmull and Clark
[CC78]) for polygonal surfaces are widely spread. The dyadic
split operation divides each triangle into four congruent subtriangles. First, new vertices are introduced at each edge
midpoint, dividing the edges into two. Connecting the three
edge midpoints within each triangle then splits the triangle
into four. Consequently, this scheme is often referred to as
1-to-4 split. The face split, on the other hand, can be applied
to elements of arbitrary degree. New vertices are inserted
not only at edge midpoints but also at the centre of the element. Then the vertices at the edge midpoints are connected
to the centre vertex, thus splitting an n-gon into n quadrilaterals. Figure 2 illustrates both the dyadic and the face split
refinements.
Unlike global mesh refinement where all elements in the
mesh are subdivided to obtain a finer mesh, adaptive or
local mesh refinement performs the split operation only for

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2234

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes

Figure 4: Base mesh with refinement trees and its corresponding red-green refined hierarchical mesh.

Figure 2: Uniform refinement of coarse domains using the
dyadic split (top) and face split (bottom).

Figure 3: Conformization of hanging nodes. Up to symmetry, these are the only configurations that can occur in
balanced meshes.

selected elements. The transition between elements of different refinement levels requires extra care because a new
vertex is inserted on an edge of the refined element, but
the coarse neighbouring face still contains the edge undivided. These irregular vertices are called hanging nodes. To
resolve the non-conforming situations between elements of
various refinement grades, the adjacent unrefined elements
must also be refined. To maintain locality of the conformization, non-conformal elements must be split without introducing additional hanging nodes (Figure 3, top). In the finite
element community triangles introduced to resolve hanging nodes are called green triangles, whereas elements that
are generated by the dyadic split are called red triangles
(hence the name red–green refinement [BSW83]). Several
conformization schemes exist for quadrilateral hierarchies,
some of them introducing non-quadrilateral elements. We implemented the scheme described in [SMFF04] (Figure 3, bottom). Following the aforementioned terminology elements
inserted by these schemes will also be referred to as green
elements.
Because green elements are less shape regular than their
parents the adaptive refinement is usually restricted to balanced meshes where the refinement level of neighbouring
elements must not differ by more than one level. This bounds
the number of hanging nodes on each edge to one, preventing
the refinement of green elements and therefore ever thinner
faces.

The refinement strategy yields a canonical hierarchical
structure where each split element acts as a parent for the
new sub-elements. Therefore, each element of the coarse
base mesh will have an associated tree that specifies its refinement. We refer to the entities of the trees as nodes to
underline the parental relationship between elements at different resolutions of the mesh. For a split element we assign
the sub-element incident to the nth vertex as the nth child of
the related node. In the triangle setting, the remaining central
sub-element will be the fourth child. Figure 4 shows three
root elements with associated refinement trees as well as the
represented adaptive grid.

2.2. Encoding
Akin to existing progressive coders we separately encode
the base domain from the hierarchy. Typically, the root grid
is described by a small, carefully laid out mesh that can be
compressed well using single-rate coders. Our prototype uses
FreeLence [KPRW05] and PMC [Ise02] to losslessly encode
the triangular and quadrilateral root grids, respectively. This
compression will generally alter the order of the base domain
elements as well the as their local indexing, that is the ordering of references to vertices. To ensure the compatibility
of the refinement hierarchy, the root grid and its associated
refinement forest is matched to the reconstructed deterministic output of the FreeLence decoder so that refinement trees
can be bijectively mapped to root elements without coding
of further information.
Starting from the base domain, encoding the refinement
hierarchy is sufficient to reconstruct the connectivity of the
mesh at any level of detail. Provided that the encoder and
decoder agree on a common node traversal strategy, the refinement hierarchy can be stored with one bit per node where
each bit specifies whether the node has children (is refined) or
not. Exceptions are made when the conformization schemes
leaves freedom of choice, for example triangles with two
hanging nodes (Figure 3). The other possibility to resolve
this non-conforming situation arises by flipping the diagonal
edge. In practice, the concrete conformization is often determined exclusively by local indexing. Because we change
the local indexing during the compression of the base mesh,
the original indexing is lost. Therefore, we need to store additional symbols to determine the conformizations. For the
triangle hierarchies these will be one bit per triangle with

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes

two hanging nodes. Using a conformization scheme that introduces only quadrilaterals (see e.g. [Sch96]) at most one bit
for each border between regions of different refinement must
be coded for quadrilateral hierarchies. The conformization
scheme of Settgast et al. [SMFF04] on the other hand has
no such ambiguities. We entropy code these bits, but found
that they where virtually incompressible without knowing
the exact implementation of the grid manager. If, however,
a geometric criterion is used to determine the conformizations, we can omit these bits altogether. The same is true
if the application does not expect any green elements and
the conformization is constructed on the fly, for example the
Progressive Geometry Compression software from Caltech
[KSS00].
Because of the deterministic conversion of the hierarchical
structure to a bit stream we can estimate an upper bound for
the code size. Let us begin with hierarchies generated by face
splits. Except for the root level, each parent is split into four
children. Therefore, the number of bits in the stream will
sum to one bit per leaf plus one 14 bit for their parents, 412 bit
1
for their grandparents and so on until reaching 4d−1
bit for
the first level, where d is the depth of the leaf in the tree. In
1 1
bit for the n-gon contained in
addition, we have to add 4d−1
n
the root level. We can write this sum as
d−1

i=0

4
1
1
1 1
+ d−1 = + d−1
4i
4
n
3 4

1
1
− +
3 n

≤

4
.
3

This bound also holds for the triangle hierarchies because
they are composed entirely of quad trees so the n in the last
term will be 4. Because the number of leaves in the hierarchy
is no greater than the number f of elements of the finest
resolution, our stream will contain no more than 43 f bits.
So far we did not account for symbols needed to resolve
multiple possibilities of conformization. In these cases we
have to store additional bits, but in fact, we already counted
multiple green elements in f that where represented by just
one node in the hierarchy.
To maintain the progressivity of the generated bit code, we
traverse the hierarchy breadth-first, so that we successively
visit the nodes at a certain depth in all trees, before switching
to the next finer level. Finally, the generated bit stream is
entropy encoded. In the following sections, the algorithm is
explained in more detail.
2.3. Redundant symbols
We converted the refinement structure into a compact bit
stream. Nevertheless, the knowledge of the structure can be
used to further improve the hierarchy compression by culling
out nodes from the bit stream whose state can be implicitly
reconstructed. Because the hierarchy is directly related to the
mesh, the mesh constraints implied by the refinement scheme
are mirrored the hierarchy’s structure. These dependencies
are exploited by the following extensions:

2235

2.3.1. 1-Regularity
As mentioned earlier, adaptive refinement produces balanced meshes.
There will be at most one hanging
node per side of an element. Moreover, because the nodes of the hierarchy are conquered level-wise, we already know whether the neighbours Marked triangles can’t
of the node in question are green el- be further refined due
ements that resolved a nonconform- to green triangles in
ing situation in the parent level. As the parent level.
a consequence, nodes representing
faces adjacent to coarse green elements cannot be refined and
can thus be skipped by the encoder.
2.3.2. Hanging nodes
Some implementations of adaptive refinement prohibit more
than a certain number of hanging nodes per element. In these
setups, the hanging nodes are conformized by applying the
usual split operation, introducing hanging nodes in adjacent
coarse elements. During compression, if all neighbours of the
current node are already processed, the number of hanging
nodes within the element will be known to the coder. Hence,
elements that exceed the maximum number of allowed hanging nodes can be split immediately and no symbol has to be
coded. Anyhow, we do not need to treat these cases explicitly
because they will be handled by our coder without overhead,
cf. Section 2.4.
2.3.3. Uniform refinement
Uniformly refined meshes exhibit a featureless hierarchical
structure—the whole forest can be described by a single
scalar that specifies the overall height of the refinement trees.
Because many meshes in practice are uniformly refined to a
certain degree, we exploit this property to reduce the number
of code symbols. We store a single byte encoding the degree
of uniform refinement separately, allowing the coder to skip
all nodes on coarser levels.
2.3.4. Stream truncation
Note that decoding a 0 from the stream has no effect on the
hierarchy whereas a 1 causes a refinement of the current node
(or its associated element, respectively). As the refinement
forest is conquered in a breadth-first manner, nodes at the
finest level are visited last, thus concluding the output stream.
These are all 0 entries and are only needed for closing the
forest, that is generating a valid hierarchical structure. This
structure can be constructed by the decoder without using
these entries. Therefore, the encoder omits the finest nodes
from the output and even truncates 0’s written after the last 1
as these can be implied, cf. [Sai04]. The decoder thus simply
skips nodes for which no bit was stored (i.e. the code contains

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2236

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes

Table 1: Removal of redundant symbols.

Model

f

#nodes

Drop 0s

1-Regul.

Uniform

bones
fandisk3
feline
femur
heat_transfer
horse
rabbit
venus
blade

5622
86092
254044
8944
96412
96368
68506
138672
51157

5438
95532
303072
10608
115034
113228
78570
154792
35905

8%
34%
37%
26%
5%
37%
23%
40%
16%

16%
23%
16%
13%
15%
18%
21%
24%
36%

0.0%
0.0%
0.8%
0.0%
0.6%
0.2%
1.3%
0.0%
0.0%

76%
43%
46%
60%
79%
46%
55%
36%
49%

(4133)
(40770)
(138523)
(6417)
(91111)
(51545)
(43368)
(56112)
(17425)

1520
23208
60096
2392
28632
25368
21496
35560
9960

(1904)
(40520)
(138512)
(4680)
(82840)
(51440)
(42928)
(50320)
(17488)

fandisk4
fertility
shoulder
rockerarm
torso

24595
192635
108361
30747
54918

30662
129420
77219
27160
38583

24%
14%
16%
30%
12%

3%
37%
34%
21%
36%

0.0%
0.0%
0.4%
6.8%
0.7%

73%
49%
49%
42%
52%

(22521)
(63513)
(38120)
(11482)
(19929)

1424
42848
23448
5392
11968

(17888)
(63592)
(38192)
(11360)
(19968)

72111

86802

23%

22%

0.8%

54%

(43212)

20951

(41545)

Average

#left

Code size

Column 2 contains the number of faces and column 3 the number of tree nodes, that is the number of binary decisions the decoder has to
make. Columns 4–6 list percentage of bits that can be omitted by dropping trailing zeros, exploiting 1-regularity, storing the number of levels
of uniform refinement. Column 7 lists the percentage and actual number of bits that have to be stored, and the last row shows the size of the
compressed code in bits, with (and without) the use of context groups.

no further symbols that can be read). Encoding the degree
of uniform refinement in combination with the omission of
trailing zeros guarantees that not a single symbol ends up
in the output when a uniformly refined mesh is compressed.
The results of the number of symbols that have to be coded
for our benchmark models are shown in Table 1. Among the
described optimizations stream truncation and 1-regularity
perform best and contribute most to the reduction of symbols.
Because uniform refinement only affects the coarsest levels
only a few symbols are omitted for our adaptive test models. Overall, the number of symbols that have to be coded
averages out at nearly half the number of nodes.

split information of two adjacent nodes is highly correlated,
so the refinement state of the neighbour node is a valuable
hint. For instance, 23k of the 96k nodes of the fandisk model
have children, which gives each hierarchy triangle the probability of 24% of being split. Given the knowledge that a
particular neighbour is a leaf, the probability of being subdivided drops to 7%. If all three direct neighbours are leaves,
that probability is a mere 1.2%.
Let X be the binary stream of split data. As shown by
Shannon [Sha48], the entropy
1

−p(i) log(p(i)),

H (X) =
i=0

2.4. Context groups
With the steps in the last section we used the rules of the
refinement scheme to eliminate code symbols in cases where
the refinement scheme leaves no room for choice. The steps
above reduce the binary representation of the refinement tree
to a compact, redundancy free representation, without even
looking at the characteristics of the particular input model.
Models that appear in practice, however, do show certain
characteristics. Just like two adjacent pixels in a digital photograph are likely to be similar, the refinement grades in
hierarchical meshes typically tend to be locally similar.
Luckily, our forest structure admits the definition of neighbours, which lets us easily determine the effects of locality.
We call two nodes within one level of the hierarchy adjacent,
if their element counterparts in the mesh of that level share
an edge. Because of the locality of the refinement depth, the

measured in bits per symbol, is the information content of X.
It poses a lower bound for the code length of any encoding
of the message X, where p(0) and p(1) are the probabilities
of X being 0 or 1, respectively. Good entropy coders, such as
arithmetic coding [WNC87], approach this lower bound in
the limit and are thus optimal in that sense.
If we do have additional information about X, for instance
the state of the neighbour elements, the code length can be
reduced. Let Y be an information source that is correlated to
X (in our case y ∈ Y describes a particular configuration of
refinement states of the neighbour elements). The amount of
information that actually has to be stored is measured by the
conditional entropy
1

p(Y = y)

H (X | Y ) =
y∈Y

−p(i | Y = y) log p(i|Y = y),
i=0

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2237

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes
Table 2: Population of the context groups for the na¨ıve and improved traversal strategy on the feline model.

Na¨ıve traversal

Improved traversal

Group
, ,?

#symb

%1’s

bits/symb

#symb

%1’s

bits/symb

(0, 3, 0)
(1, 2, 0)
(2, 1, 0)
(3, 0, 0)
(0, 2, 1)
(1, 1, 1)
(2, 0, 1)
(0, 1, 2)
(1, 0, 2)
(0, 0, 3)

13606
5603
5913
12146
14314
6373
14130
17256
18520
30855

2%
37%
82%
100%
6%
43%
99%
10%
94%
55%

0.12
0.95
0.69
0.00
0.34
0.99
0.05
0.49
0.34
0.99

681
30
91
685
29789
12079
33227
40996
20944
1

10%
50%
78%
100%
5%
72%
98%
23%
96%
100%

0.52
1.40
0.95
0.04
0.29
0.86
0.13
0.77
0.22
5.00

Culled
Total

164356
303072

0
0.23

164549
303072

0
0.20

For each strategy we provided the number of symbols in each context group, the percentage of 1’s among those symbols and the efficiency in
terms of bits per symbol.

that is the amount of new information in X, given that we
already know Y. If X and Y are correlated, H(X|Y) is strictly
less than H(X).
In our implementation we use context groups as a simple
measure to profit from the correlation of hierarchy elements.
Recall that we specify with one bit whether the current node
is refined as we traverse the nodes in the trees level by level.
Whenever a binary code symbol is produced, we check the
status of the neighbours. If the neighbour has already been
visited during traversal, its refinement status will be known
to the decoder. Thus, we can use the refinement status of
already processed neighbours as the definition of contexts: a
neighbour can either be refined ( ), not refined ( ), or it has
not been processed before (?).
The status of the neighbour elements define the context
group in which the symbol is encoded. We write symbols of
different context groups in separate arrays, which are entropy
coded independently. With arithmetic coding, each context
group y will compress to
1

−p(i | Y = y) log(p(i | Y = y))

H (X | Y = y) =
i=0

in the limit. The total code size per symbol is obtained by
averaging the entropies individual contexts weighted by their
respective probabilities,
p(Y = y)H (X | Y = y) = H (X | Y ),
y∈Y

which proves that contexts are an appropriate tool to capture
all the mutual information that is inherent in the correlation
of neighbouring elements.

So far we have not specified exactly how we define the
contexts. The contexts arise from the number of , and ?
neighbours of a hierarchy node. We write (x, y, z) to denote
the context with x situations, y times and z times ?. For
the triangle hierarchy setting the all possible cases are listed
in Table 2.
2.5. Traversal order
In this section, we review the compression rates within the
single context groups and discuss the impact of the hierarchy
traversal strategy on them.
The level-wise traversal of the hierarchy is an important
factor in our coder. Yet it leaves freedom to choose any
iteration of nodes within each level. This choice directly
affects the distribution of the symbols over the context groups
as the context of a node solely depends on the refinement state
of its neighbours and therefore on the fact whether these have
been already visited or not.
A customary traversal scheme would visit the children of
each node in a fixed ordering. Table 2 shows the symbols
distribution in each context group for one of our test models.
Here na¨ıve traversal refers to a strategy where children are
visited in order.
In our implementation, elements incident to the parent’s
vertices are visited first. In addition, local indexing of parent
elements determines the order of its children. Hence context
group (0, 0, n), where none of the neighbours are known,
contains the most entries. This group, although, is virtually
incompressible as no advantage can be taken of mutual information. The same holds for context groups where the
extra information is rather ambiguous, for example (1, 1, 1),

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2238

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes

(1, 2, 0) and (2, 1, 0) in the triangle setting. Contrarily, the
other context groups perform very well but are less populated.
The positive effects of an adaptive traversal order have
been observed in Angle-Analyzer [LAD02] for surface
meshes. Also, Schnabel et al. [SK06] optimize the octree
traversal in each level. In this spirit, we designed a new traversal scheme (Improved traversal in Table 2) to redistribute the
symbols and maximize the overall compression. Instead of
ordering the children in a fixed manner we first iterate over
every tree and collect all nodes at a certain depth. This allows
a global optimization of the level-wise node traversal.
The principle of our algorithm is to maximize the mutual
information that can be exploited for encoding each node.
For that purpose we prioritize each node by the entropy of
its current context. Therefore, nodes that already profit from
encoded neighbours will be conquered first, which in turn
provides more information to its unprocessed neighbours.
Clearly, all nodes that are skipped by the coder due to optimizations from Section 2.3 feature a zero entropy and will
hence be traversed before all other nodes. A promising candidate for prioritization of the remaining nodes is to use the
actual entropies of the individual contexts. The greedy strategy traverses nodes in the context with lowest entropy first to
minimize the overall compression rate. Experiments showed
that this strategy already achieves significant improvements
in compression. Learning the entropies of the contexts, however, is expensive in terms of compression performance as
well as computational cost. Once the learning phase is settled,
the algorithm sticks with a fixed prioritization of contexts. To
remove all effects of the learning process of the contexts’ entropies from the traversal, we additionally investigated fixed
priorities for the nodes, that is a fixed order of contexts during the search for the next node to conquer. We looked at
different orderings:
• increasing by the learned (settled) entropies of contexts,
• increasing by the number of unknown neighbours, and
• decreasing by the difference of known coarse and known
refined neighbours.

of one symbol if the mesh represents a connected surfaces.
The nodes are thus conquered in a region-growing manner,
so nodes whose neighbours are all known become extremely
rare [cf. group (3, 0, 0), (2, 1, 0) and (1, 2, 0) in Table 2]. Furthermore, the traversal directly affects the nodes’ order which
causes the change in the number of culled out symbols. Reviewing the final compression rates on our test models shows
an average improvement of 7%.
2.6. Time-varying sequences
As we observed, we can profit well from the correlations
between adjacent elements. We can profit in the same way
when we have a time-varying series of refinements of a common base mesh. Here, we assume that the degree of refinement varies only a little from one frame to another, just as
one assumes smooth transitions for animated geometries or
videos.
When processing a hierarchy node in a series, we query the
state of the corresponding node in the previous time frame,
which can give us one of three states:
• it was a node with children,
• it is a leaf node, or
• it didn’t exist
in the previous frame. Thus, the number of contexts triples,
if we also include the status of the previous frame in the
contexts.
If the refinement trees do not vary much between the time
steps, then contexts corresponding to the first case will be
mainly filled with ones, while the latter two will primarily
contain zeros. Thus, grids which equal their preceding frame
can be stored at no cost, aside from a small overhead due to
the use of more contexts. At the contrary, if the grids are not
correlated, the entropy of the individual contexts can never
be worse than in the static setting, because symbols from one
context are simply spread to three, maintaining or improving
symbol possibilities. Table 3 shows the results of the time
series adaption applied to three time-varying hierarchies.

In the case of ties we also tried all possible permutations.
The tests revealed that we can substantially improve the
performance of the traversal by skipping the learning process. Although we could not identify a single strategy that
performed best on the entire set, ordering the contexts as
in Table 2 (increasing by number of unknown neighbours
and breaking ties increasing by the number of known refined
neighbours) constantly provided good results. Therefore, we
chose this ordering as the default for our coder and thus for
all the presented tests.
As a result of our choice, the context group (0, 0, n) contains almost no entries—in fact, it will always be comprised

Table 3: Compression results for time-varying sequences.

Model

#Frames

Average

Total

Diff.

bunny
heat
cloth

21
11
15

585 (1002)
420 (1125)
459 (472)

12280 (21046)
4621 (12376)
6888 (7085)

−42%
−63%
−3%

Left to right: Model, number of frames in the sequence, average
code size per frame in bytes for dynamic (static) coder, total code
size in bytes for dynamic (static) coder and the difference in code
size between dynamic and static coder.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2239

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes

(a) feline_mean
v=127020, f0 =504

(f) bones
v=2809 f0 =3882

(b) horse
v=48186, f0 =220

(g) venus
v=69338, f0 =388

(k) blade
v=30866, f0 =497

(h) fandisk3
v=43048, f0 =4828

(l) rockerarm
v=22171, f0 =368

(c) heat_transfer
v=48652, f0 =2

(i) fandisk4
v=23312, f0 =258

(m) torso
v=33334, f0 =265

(d) femur
v=4474, f0 =532

(e) rabbit
v=34k, f0 =210

(j) shoulder
v=66366, f0 =293

(n) fertility
v=113097, f0 =265

Figure 5: The test set we used, the number v of vertices at the finest resolution, and the number f 0 of elements of the base mesh.
3. Geometry Compression
Thus far, we have described our coder in terms of connectivity compression. This section covers the compression of the
geometry of the adaptive hierarchies by extending progressive coding schemes from the uniform setting. The following
text will cover wavelets and zerotrees only very briefly as extensive literature on these topics exist. We refer the reader
to [KSS00] for a discussion on wavelet transforms of semiregular meshes and to [SPM96] for a detailed description of
zerotrees.

3.1. Wavelet transform
Wavelet transforms convert a mesh hierarchy into a coarse
base mesh and nested sets of wavelet coefficients representing the mesh details. The coefficients are computed as
differences to predictions derived from the parent level. Thus,

any resolution of the mesh can be reconstructed recursively
from the base mesh and the coefficients from all intermediate resolutions. Various subdivision schemes can be used to
predict new points from the parent level.
In our implementation, we compute the wavelet transform based on interpolatory subdivision schemes. Although
lifted wavelet transforms have been reported to provide better compression, we used unlifted schemes for simplicity.
We applied butterfly subdivision [DLG90] for triangle hierarchies and Leif Kobbelt’s scheme [Kob96] for quadrilateral
hierarchies. An exception is the uniformly refined triangular
case, where we used the PGC software for the lifted butterfly
wavelet transform. This way, we can ensure the exact same
quantization errors, which simplifies comparison.
We represent the vector-valued wavelet coefficients in a
local frame induced by the tangent plane of the surface.
Because normal errors have more impact on the distortion,

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2240

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes

Figure 6: Parent–child relationships in triangular and
quadrilateral hierarchies. Solid circles indicate parents;
empty circles indicate children.
we increase the precision of the normal component. We chose
a factor of 4, which has been reported as being reasonable in
the literature. Each component is encoded individually using
zerotree coding.
Once the coefficients in level j are computed, we adjust
their scaling by a factor of 2−j . Such scaling arises from L2 normalizing the subdivision basis functions. It is commonly
applied in compression to compensate for their shrinking
support on finer levels. Note that the scaling provides higher
accuracy on coarser levels and causes stronger quantization
for coefficients on finer levels.

Figure 7: The neighbours (squares) that are used for
context-based coding of significance bits. The black circle
marks the current coefficient and gray circles represent sibling coefficients that are grouped in the SPIHT coder.
geometry data. This enables the decoder to reconstruct an
intermediate mesh from any prefix of the stream. Therefore, a common mesh traversal strategy has to be chosen for
the geometry and hierarchy compression. Nevertheless, the
traversal used by our coder can be changed to any progressive conquering of the mesh in order to facilitate a special
geometry coder.
3.3. Entropy coding

3.2. Zerotrees
For decreasing thresholds Tk = T 0 /2k , the zerotree coder
successively transmits the location and sign of coefficients
that become significant, that is when |ci | > Tk . To transmit this information efficiently, coefficients are grouped in
a tree structure implementing the parent–child relationship
depicted in Figure 6.
For triangle hierarchies, coefficients have a one-to-one
association with edges of the coarser level. For quadrilateral hierarchies there are two types of associations: vertices
inserted at face midpoints are associated with these faces,
whereas vertices on edge midpoints are associated to these
edges. Based on this structure we use the SPIHT encoding
[SPM96] to transmit significance information either for a
single coefficient or for sets of coefficients, requiring only a
single bit if all coefficients in a set are insignificant. Thus,
the zerotree coder exploits the fact that the distribution of
the wavelet coefficients is centred around zero and that their
magnitudes tend to decrease at finer subbands depending on
the smoothness of the input surface.
In addition to the significance and sign data, refinement
bits have to be sent in each pass for coefficients that became
significant in earlier passes. The initial threshold T 0 and the
number of zerotree passes determine the quantization of the
coefficients. In our experiments, we choose T 0 to be the greatest magnitude of all coefficients’ components. The number
of bit planes was chosen to achieve L2 errors that are comparable to those caused by uniform 12-bit quantization.
For the progressive transmission of multiresolution
meshes it is important to interleave the connectivity and

Although very efficient, the compression performance of the
zerotree coder can be further improved by applying entropy
coding to significance bits. Refinement and sign bits, on the
other hand, are much harder to compress.
As Denis et al. recently observed [DSM*10], there is a
strong correlation between neighbouring coefficients within
each level (intraband correlation) and a correlation between
parents and their child coefficients (interband correlation),
with the former being stronger than the latter. The SPIHT
encoder (and therefore PGC) exploits the interband correlation by transmitting the significance information for children
of a parent as a j-bit symbol, where j ∈ {1, 2, 3, 4} is the
number of insignificant children.
However, as seen in Figure 7, there are coefficients in the
same level (squares) that are closer than the siblings defined
by the zerotree (light grey). Because there is a higher correlation between direct neighbours within a subband, we decided
to use their significance information for entropy coding.
Following the ideas of our connectivity coding, we define
contexts based on the number of significant, insignificant,
and unprocessed neighbour coefficients. Because the wavelet
coefficients of different subbands are of different magnitudes,
we consider only neighbours of the same subdivision level
to guarantee similarity of the data. We devise four contexts
representing the neighbour status:
• all processed neighbours are significant
• all processed neighbours are insignificant
• all neighbours are unprocessed.
• there are significant and insignificant neighbours

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes

2241

Figure 8: The three test sequences used. Top: planar domain refined driven by heat transfer computations (showing temperature
as z-component). Middle: View-dependent refinement taking curvature, visibility and silhouette into account (as proposed in
[SMFF04]). Bottom: cloth simulation with curvature-dependent refinement.

This choice of contexts can be used for various mesh types
and is robust if the number of neighbours varies due to boundaries, adaptivity or irregular face degrees.
The significance bits in the SPIHT algorithm occur not
only for single coefficients but also for sets of coefficients,
both contributing roughly the same amount to the code size.
Following the context construction for coefficients we also
apply context-based coding for type A and type B set significance bits separately, tripling the number of contexts to 12
(see [SPM96] for a definition of set types). For the set contexts, neighbours are counted as significant if their respective
set significance test is positive.
To also exploit interband correlation, we double the number of contexts for coefficients and type A sets, increasing the
total number to 20. For a coefficient, the context decision is
based on the significance value of its parent. For a type A set,
we check the parent of the coarsest coefficients in the set and
switch the context if it is significant. For type B sets, we do
not need to store a bit if the parents are insignificant. In this
case, the set will always be significant. Hence, no doubling
of contexts is needed for type B sets.

3.4. Adaptive zerotree coding
So far, we have only described geometry encoding of uniformly refined hierarchies. Using adaptive meshes, we can
profit from the fact that we do not have to store any bits
for coefficients that are not present in the adaptive mesh.
In our context model for the significance bits, we consider
non-existent coefficients as unprocessed.
An alternative is to fill up the zerotree with zero coefficients producing a uniformly refined tree structure, as done
in Khodakovsky’s and Guskov’s normal mesh compression
[KG03]. Because of the efficiency of the zerotree coder, the
impact on the code size is small. Nevertheless, to reconstruct
the original adaptivity, the refinement information must be
transmitted, which is more costly than our proposed method,
especially if the mesh refinement is not geometry related.
4. Results and Conclusion
We measured the performance of our coder using the 14 models in Figure 5 as well as the three time-varying sequences
in Figure 8. The uniform feline, horse, rabbit and venus

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2242

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes

Table 4: Connectivity and geometry rates (columns conn and geom ) for triangle hierarchies in bits per vertex.

PMC
Triangle mesh

Wavemesh

PGC

Our

v

geom

conn

L2

geom

conn

L2

geom

conn

L2

geom

conn

L2

Adaptive

bones
fandisk3
feline
femur
heat_trans
horse
rabbit
venus

2809
43048
127020
4474
48652
48186
34255
69338

20.63
11.15
10.18
17.12
2.22
11.30
12.94
12.60

2.34
1.53
1.23
1.32
1.21
1.32
1.47
1.50

47
38
37
38
0
40
43
33

21.60
11.94
10.33
18.53
3.57
11.39
13.28
12.39

4.44
2.87
2.40
2.75
2.52
2.37
2.81
2.60

47
37
37
38
0
40
43
33

–
–
–
–
–
–
–
–

–
–
–
–
–
–
–
–

–
–
–
–
–
–
–
–

18.98
6.06
2.39
13.19
0.10
3.10
5.12
4.30

1.84
0.58
0.48
0.71
0.64
0.53
0.64
0.52

46
32
35
31
4
40
32
31

Uniform

fandisk3
feline
horse
rabbit
venus

154498
258046
112642
107522
198658

4.47
5.34
5.51
5.65
5.67

0.03
0.01
0.01
0.01
0.01

40
37
40
43
35

4.40
2.64
3.45
3.90
3.63

0.04
0.01
0.01
0.01
0.01

28
31
32
43
40

1.99
1.14
1.35
1.93
2.04

0.01
0.00
0.00
0.00
0.00

26
34
40
32
30

1.67
1.08
1.30
1.89
1.96

0.01
0.00
0.00
0.00
0.00

26
34
40
32
30

Avg. (adaptive)
Avg. (uniform)

47222
166273

12.27
5.33

1.49
0.01

35
39

12.88
3.54

2.85
0.02

34
35

–
1.69

–
0.00

–
32

6.66
1.58

0.74
0.00

31
32

Column v lists the number of vertices, and column L2 lists the root mean square errors reported by metro in units of 10−6 w.r.t. the bounding
box diameter.

models are from the PGC data set, courtesy of Khodakovsky
et al. We constructed the respective adaptive models by local
coarsening according to various curvature measures. Bones
and heat_transfer—courtesy of Zuse Institute Berlin—result
from heat transfer simulations. The femur model, courtesy of Oliver Sander, was refined according to criteria of
a two-body contact problem in a human knee joint. The
quadrilateral hierarchies are remeshes based on QuadCover
[KNP07] parameterizations. The fandisk4 and the rockerarm refinements have been constructed based on heat diffusion and thin shell simulation, respectively. The other quad
models were coarsened based on curvature. Note that we
included the entropy coded orientation bits for the conformization of triangle hierarchies as well when necessary.
In particular, this concerns bones, heat_transfer, and femur,
as these models were not generated with our own hierarchy
manager.
Table 1 shows the efficiency of our strategies for redundancy removal from the binary representation of the refinement hierarchy. Here especially two approaches contribute
to the overall result: stream truncation and 1-regularity. 1regularity heavily profits from the fact that the refinement
scheme only allows the subclass of balanced meshes. Stream
truncation exploits the rather simple observation that trailing 0 symbols can be omitted as these cause no change
in the reconstructed hierarchy. The effect of stream truncation cannot be achieved by the context based entropy coder
alone. Keeping the zeros expanded our codes by 17%. The
strategies of Section 2.3 nearly halve the number of symbols that have to be coded. If the mesh is not balanced, the
1-regularity optimization can not be applied. In this case,

refinement bits can be entropy coded using the same context models as above, counting the green neighbours as
unrefined.
Arithmetic coding is another vital part of our compression
scheme. Without context groups, the compact binary representation of the hierarchy is almost incompressible due to a
rather uniform distribution of the symbols, as confirmed by
values in parentheses in Table 1. Again, knowledge about
the mesh structure is used to apply context based arithmetic
encoding. The introduced context groups reduced the code
length by approximately 50%.
An evaluation of compression rates within each context group is given in Table 2 and revealed huge gaps between the performance of individual groups. These gaps can
be attributed to the mutual information inherent to each
context group. Therefore, we devised a hierarchy traversal scheme that shifts the distribution of symbols over the
contexts and thus balances the mutual information available for the coding of each node. As a result, an average gain of 7% for the overall compression rates could be
achieved.
In Tables 4 and 5 we provide compression results for the
connectivity as well as the geometry of our test models. Although the connectivity is trivial in the uniform case, we
provide results for all models for which the finest level geometry information was also available.
Table 4 summarizes our results for the triangle multiresolution meshes. We are comparing our results to the singlerate Polygonal Mesh Compression [Ise02] (PMC) and to the

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2243

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes
Table 5: Connectivity and geometry rates for quadrilateral hierarchies in bits per vertex.

PMC

Our

Quad mesh

v

geom

conn

L2

geom

conn

L2

Adaptive

blade
fandisk4
fertility
rockerarm
shoulder
torso

36358
23657
134351
24460
77573
39086

9.31
5.50
8.11
8.52
8.80
9.23

2.14
0.33
2.22
1.46
2.08
2.10

55
58
55
61
56
58

3.63
2.70
1.61
3.49
1.75
2.72

0.28
0.07
0.32
0.23
0.31
0.31

52
45
53
49
61
54

Uniform

blade
fandisk4
fertility
rockerarm
shoulder
torso

127234
263682
274426
300546
94208
269826

4.48
3.61
4.15
5.06
4.10
4.03

0.00
0.00
0.00
0.00
0.00
0.00

55
52
55
61
56
58

1.02
0.72
0.68
1.20
0.42
0.35

0.00
0.00
0.00
0.00
0.00
0.00

58
40
58
70
62
46

8.11
4.24

1.72
0.00

57
56

2.65
0.73

0.25
0.00

52
56

Avg. (adaptive)
Avg. (uniform)

progressive wavelet based coders Wavemesh and PGC. In
the connectivity case, our scheme significantly outperforms
PMC by an average factor of 2.0 and Wavemesh by an average factor of 3.8 for the adaptively refined meshes.
For geometry compression, we used uniform 12-bit quantization for PMC for all models. This is also true for the coarse
base meshes needed by PGC and our coder. For Wavemesh,
we used the current version 2.1 and chose for each model the
options that provided the best results: uniform 12-bit quantization for the adaptive models, and zerotree encoding with
lifting radius 1 for the uniform ones. Wavemesh was able
to detect the subdivision connectivity of the uniform models
and was thus able to handle the uniform models much better
than the adaptive ones.
Overall, we exceed PGC codec by 7% on average for
uniform multiresolution models, due to the use of improved
context models for significance bits and we surpass PMC
and Wavemesh by average factors between 2.0 and 3.0 for
the uniform as well as the adaptive models.
Similarly, Table 5 presents compression results for quadrilateral hierarchies for our coder and PMC. One reason for the
drastic drop in the connectivity rate is due to the fact that we
encode face-based symbols and quadrilateral meshes have
half as many faces per vertex. Evaluating the connectivity
compression results, we exceed PMC by an average factor of
6.9 for the connectivity and 2.8/3.4 for the adaptive/uniform
geometry, which demonstrates the efficiency of our scheme
for quadrilateral multiresolution meshes.
Table 3 shows compression results for time-varying sequences. Overall, using the proposed extensions to the our
static compression scheme, can improve compression rates
significantly. Even though the extension is only on par with

the static version if the refinement varies a lot (see cloth), we
recommend using the extension due to its simple implementation.
Even though we have a non-optimized prototype implementation, the execution times in our tests were dominated
by the times for loading and parsing the uncompressed models from the hard disk. After processing the small base mesh,
connectivity encoding and decoding of a model is performed
by a single traversal of the hierarchy in which each node is
touched only once. Connectivity processing of a node merely
involves finding its neighbours and children, arithmetic coding of one bit, and finding a node with highest priority. Each
of those operations is done in constant time. This applies
even to finding a highest priority node if the nodes of each
priority are collected in separate doubly linked lists.
When coding the wavelet coefficients, every coefficient is
visited once for each bit plane. Thus, geometry coding time
is linear in the number of coefficients and the number of bit
planes.

4.1. Further extensions and future work
We took an in-depth look at connectivity compression, but
only briefly covered geometry compression. Progressive geometry compression has been an active field of research and
many tweaks have been reported to further increase geometry
compression performance, but consideration of all of them
is beyond the scope of this paper. Many of those tweaks can
be transferred from the uniform to the adaptive case. For
instance, second generation wavelets derived via the lifting
scheme [SS95] have often been applied and we expect improved rate-distortion curves when using them in our scheme.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2244

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes

However, a thorough treatment of lifting schemes on adaptive mesh hierarchies is still lacking. Empirical results of
[DSM*10] further show that there are correlations between
sign bits that can be exploited. Furthermore, we observed
that the components of vector-valued wavelet coefficients
are also correlated, making them suitable for further context
modelling.
Extending the context model based on the significance of a
coefficient in a prior time frame also enables us to take advantage of correlation in animated sequences. We expect similar
gains for the geometry code as for our connectivity part.

[DSM*10] DENIS L., SATTI S., MUNTEANU A., CORNELIS J.,
SCHELKENS P.: Scalable intraband and composite waveletbased coding of semiregular meshes. IEEE Transactions
on Multimedia, 12, 8 (2010), 773–789.
[GVSS00] GUSKOV I., VIDIMCˇ E K., SWELDENS W., SCHRO¨ DER
P.: Normal meshes. In SIGGRAPH ’00 Proceedings of
the 27th Annual Conference on Computer Graphics and
Interactive Techniques (New York, NY, USA, 2000),
ACM Press/Addison-Wesley Publishing Co., pp. 95–
102.
[IS06] ISENBURG M., SNOEYINK J.: Early-split coding of triangle mesh connectivity. In Graphics Interface Proceedings
(Quebec, Canada, 2006), Canadian Information Processing Society, pp. 89–97.

Acknowledgements
This work was supported by the DFG Research Center
MATHEON “Mathematics for key technologies” and mental images GmbH. We thank Martin Weiser and Sebastian G¨otschel
of the Zuse Institute Berlin as well as Oliver Sander for providing their multiresolution data. Models are courtesy of
Stanford University, Caltech, AIM@SHAPE, Polygon Technologies and the Visible Human Project.

[Ise02] ISENBURG M.: Compressing polygon mesh connectivity with degree duality prediction. In Graphics Interface Conference Proceedings (Calgary, Alberta, Canada,
2002), pp. 161–170.
[IS99] ISENBURG M., SNOEYINK J.: Mesh collapse compression. In Proceedings of SIBGRAPI’99 (S˜ao Paulo, Brazil,
1999), pp. 27–28.

References
[AD01] ALLIEZ P., DESBRUN M.: Progressive compression for
lossless transmission of triangle meshes, In Proceedings
of the 28th Annual Conference on Computer Graphics and
Interactive Techniques (2001), ACM Press, pp. 195–202.
[AMG05] AVILE´ S M., MORA´ N F., GARC´IA N.: Progressive lower trees of wavelet coefficients: efficient spatial
and SNR scalable coding of 3D models. Advances in
Multimedia Information Processing-PCM 2005: 6th
Pacific-Rim Conference on Multimedia Jeju Island,
Korea, November 13–16, 2005 Proceedings (2005),
Springer, pp. 61–72.
[BSW83] BANK R., SHERMAN A., WEISER A.: Some refinement algorithms and data structures for regular local mesh
refinement. Scientific Computing, Applications of Mathematics and Computing to the Physical Sciences (1983).
[BWK02] BOTSCH M., WIRATANAYA A., KOBBELT L.: Efficient
high quality rendering of point sampled geometry. In Proceedings of the Eurographics Workshop on Rendering
(Pisa, Italy, 2002), Eurographics Association, pp. 53–64.
[CC78] CATMULL E., CLARK J.: Recursively generated
b-spline surfaces on arbitrary topological meshes.
Computer-Aided Design, 10, 6 (1978), 350–355.
[DLG90] DYN N., LEVINE D., GRAPHORY J. A.: A butterfly
subdivision scheme for surface interpolation with tension control. ACM Transactions on Graph., 9, 2 (1990),
160–169.

[KG03] KHODAKOVSKY A., GUSKOV I.: Compression of normal meshes. In Geometric Modeling for Scientific Visualization. Guido Brunnett, Bernd Hamann, Heinrich M¨uller
and Lars Linsen (Eds.). Springer-Verlag, Berlin, Heidelberg (2003), pp. 189–206.
[KNP07] K¨ALBERER F., NIESER M., POLTHIER K.:
QuadCover—surface parameterization using branched
coverings. Computer Graphics Forum, 26, 3 (2007),
375–384.
[Kob96] KOBBELT L.: Interpolatory subdivision on open
quadrilateral nets with arbitrary topology. In Computer
Graphics Forum 15, 3 (1996), 409–420.
[KPRW05] K¨ALBERER F., POLTHIER K., REITEBUCH U.,
WARDETZKY M.: Freelence—coding with free valences.
Computer Graphics Forum, 24, 3 (2005), 469–478.
[KSS00] KHODAKOVSKY A., SCHRO¨ DER P., SWELDENS W.: Progressive geometry compression. In SIGGRAPH ’00 Proceedings of the 27th Annual Conference on Computer
Graphics and Interactive Techniques (New York, NY,
USA, 2000), ACM Press/Addison-Wesley Publishing Co.,
pp. 271–278.
[LAD02] LEE H., ALLIEZ P., DESBRUN M.: Angle-Analyzer:
A triangle-quad mesh codec. Computer Graphics Forum
21, 3 (2002), 383–392.
[Loo87] LOOP C.: Smooth Subdivision Surfaces Based on
Triangles. Utah University, USA.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

C. von Tycowicz et al. / Context-Based Coding of AMR Meshes

[LSS*] LEE A. W. F., SWELDENS W., SCHRO¨ DER P., COWSAR
L., DOBKIN D.: Maps: multiresolution adaptive parameterization of surfaces. In SIGGRAPH ’98 Proceedings of the
25th Annual Conference on Computer Graphics and Interactive Techniques (New York, NY, USA, 1998), ACM
Press, pp. 95–104.
[PA05] PAYAN F., ANTONINI M.: An efficient bit allocation for
compressing normal meshes with an error-driven quantization. CAGD, 22, 5 (2005), 466–486.
[Ros99] ROSSIGNAC J.: Edgebreaker: Connectivity compression for triangle meshes. IEEE Transactions on Visualization and Computer Graphics 5, 1 (1999), 47–61.
[Sai04] SAID A.: Introduction to arithmetic coding-theory
and practice. Hewlett Packard Laboratories Report
(2004).
[Sch96] SCHNEIDERS R.: Refining quadrilateral and hexahedral element meshes. In Proceedings of the 5th International Conference on Grid Generation in Computational
Field Simulations (MS, USA, 1996), CRC Press, pp. 679–
688.

2245

[SMFF04] SETTGAST V., M¨ULLER K., F¨UNFZIG C., FELLNER D.:
Adaptive tesselation of subdivision surfaces. Computers
& Graphics, 28, 1 (2004), 73–78.
[SPM96] SAID A., PEARLMAN W. A., MEMBER S.: A new
fast and efficient image codec based on set partitioning in hierarchical trees. IEEE Transactions on Circuits and Systems for Video Technology, 6 (1996), 243–
250.
[SS95] SCHRO¨ DER P., SWELDENS W.: Spherical wavelets:
Efficiently representing functions on the sphere. In
Proceedings of the 22nd Annual Conference on Computer
Graphics and Interactive Techniques (1995), ACM Press,
Ney York, NY, pp. 161–172.
[Szy02] SZYMCZAK A.: Optimized edgebreaker encoding for
large and regular triangle meshes. In DCC ’02 Proceedings
(Washington, DC, USA, 2002), IEEE Computer Society,
p. 472.
[TG98] TOUMA C., GOTSMAN C.: Triangle mesh compression.
In Graphics Interface Conference Proceedings (Vancouver, BC, Canada, 1998), pp. 26–34.

[Sha48] SHANNON C. E.: A mathematical theory of communication. The Bell System Technical J., 27 (1948).
243379–473 and 633–656.

[Tut62] TUTTE W.: A census of planar triangulations.
Canadian Journal of Mathematics, 14 (1962), 21–
38.

[Sha93] SHAPIRO J. M.: Embedded image coding using zerotrees of wavelet coefficients. IEEE Transactions of Signal Processing, 41 (1993), 3445–3462.

[VP04] VALETTE S., PROST R.: Wavelet-based progressive
compression scheme for triangle meshes: Wavemesh.
IEEE Transactions on Visualization and Computer Graphics, 10, 2 (2004), pp. 123–129.

[SK06] SCHNABEL R., KLEIN R.: Octree-based point-cloud
compression. In Proceedings of the Symposium on PointBased Graphics 2006 (July 2006), M. BOTSCH and B. CHEN
(Eds.), Eurographics.

[WNC87] WITTEN I. H., NEAL R. M., CLEARY J. G.: Arithmetic coding for data compression. Communications of
the ACM, 30, 6 (1987), 520–540.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

