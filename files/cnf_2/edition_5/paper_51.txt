DOI: 10.1111/j.1467-8659.2011.01874.x
EUROGRAPHICS 2011 / M. Chen and O. Deussen
(Guest Editors)

Volume 30 (2011), Number 2

Computer-Suggested Facial Makeup
Kristina Scherbaum1
1 MPI

Informatik

Tobias Ritschel2,3

2 Télécom

Wi
t
houtma
k
eup

Matthias Hullin1

ParisTech / LTCI / CNRS

S
ugges
t
edma
k
eup

3 Intel

Thorsten Thormählen1,5

Visual Computing Institute

Not
s
ugges
t
edma
k
eup

Volker Blanz4

4 University

of Siegen

Hans-Peter Seidel1
5 University

of Tübingen

I
nt
er
a
c
t
i
v
ea
ndr
el
i
tpr
ev
i
ewdet
a
i
l

Abstract
Finding the best makeup for a given human face is an art in its own right. Experienced makeup artists train for
years to be skilled enough to propose a best-fit makeup for an individual. In this work we propose a system that
automates this task. We acquired the appearance of 56 human faces, both without and with professional makeup. To
this end, we use a controlled-light setup, which allows to capture detailed facial appearance information, such
as diffuse reflectance, normals, subsurface-scattering, specularity, or glossiness. A 3D morphable face model is
used to obtain 3D positional information and to register all faces into a common parameterization. We then define
makeup to be the change of facial appearance and use the acquired database to find a mapping from the space of
human facial appearance to makeup. Our main application is to use this mapping to suggest the best-fit makeup for
novel faces that are not in the database. Further applications are makeup transfer, automatic rating of makeup,
makeup-training, or makeup-exaggeration. As our makeup representation captures a change in reflectance and
scattering, it allows us to synthesize faces with makeup in novel 3D views and novel lighting with high realism. The
effectiveness of our approach is further validated in a user-study.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Color, shading, shadowing, and texture I.5.4 [Computer Graphics]: Applications—Computer
vision

1. Introduction
For many centuries people have changed facial reflectance
by different means to achieve pleasant facial appearance. In
current everyday life, facial makeup is mostly used to mimic
healthy and attractive facial geometry and reflectance. Finding the best makeup for a given human face can be considered
an art form, and nowadays many people trust the advice of
professional makeup-artists.
This paper introduces a model of facial makeup using a
database of example faces with and without makeup. The
makeup model allows, for example, to provide computersuggested makeup for new subjects, which are not part of
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

the database. It is also possible to rate an applied makeup
in order to give an objective feedback. For all these applications we make the assumption that the choice of human
facial makeup is based on the more or less conscious process of mapping facial features to reflectance and scattering
changes. This is of course just an approximation of reality, as
there are some widely accepted standards but no strict rules
for makeup [Tho05]. Furthermore, the mapping from facial
appearance to makeup might change over culture, ethnics,
and history. To be practical, we limit our investigations to
western, 21st century, female facial makeup.
In contrast to existing approaches [GS09, TTBX07], our

486

Scherbaum et al. / Computer-Suggested Facial Makeup

approach does not only allow to transfer makeups between
subjects but provides a computer-suggested makeup. Furthermore, we perform our makeup analysis and synthesis in 3D,
whereas related approaches worked in the 2D domain. Nevertheless, our 3D analysis can provide makeup suggestions, if
only a 2D photograph of the subject is available. The proposed 3D synthesis allows to simulate the suggested makeup
under different lighting conditions.
The paper is structured as follows: After reviewing previous work in Section 2, we describe our approach to build a
makeup model in Section 3 that leads to a number of example
applications described in 4 that are validated in a perceptual
study. After a discussion 5, we conclude in Section 6.
2. Related Work
Facial Appearance Convincingly modeling the appearance
of the human body and especially the human face has been
a long-standing goal in computer graphics. Modeling facial
appearance, which consists both of geometry as well as reflectance and scattering, is challenging, because human observers are well-tuned to perceive faces [HHG00] and even
subtle imperfections may easily lead to an unpleasant impression [SN07]. To capture the essence of facial geometry,
Blanz and Vetter [BV99] introduced a morphable face model.
Besides geometry, the appearance (i.e. reflectance and scattering) of human skin was captured [MWL∗ 99, WMP∗ 06]. The
effect of aging, alcohol consumption and also foundation cosmetics can be predicted by a model based on the separation
of hemoglobin and melanin [TOS∗ 03].
Makeup The main purpose of makeup is to temporarily
change the facial appearance. Most people apply makeup
to either cover unwanted facial structures, such as wrinkles or
pores, or to emphasize specific features. Others apply makeup
to express themselves and to play around with different styles
and looks. However, makeup is a very subjective matter. The
common understanding of makeup varies between different
cultures and changed over history [Cor72]. Further, cosmetic
makeup is, and has been historically, much more common
among women [Cor72] and we will hence limit our investigation to female, present time makeup for simplicity. Russel [Rus03] studies the relation of contrast between different
facial parts, indicating that luminance changes as found in
cosmetics are more effective on female faces.
Given a certain facial appearance, makeup is used to emphasize certain features or deemphasize others. Therefore,
makeup is a change of facial appearance that depends on the
initial appearance. This change is not limited to color, but also
gloss and scattering are affected. To our knowledge, makeup
was neither rendered nor captured in previous work and only
considered in an image-based context [TTBX07, GS09]. In
this work, we will argue how makeup is a change of facial
appearance as a function of facial appearance, i. e. it is a
mapping, that can be modeled given some examples.
Adding or emphasizing existing makeup can approxi-

mately be achieved for live footage using simple image filters [NRK99]. Both Numata et al. [NRK99] and Tsumura et
al. [TOS∗ 03] change the melanin and hemoglobin mixture to
indirectly mimic the effect of foundation cosmetics, which in
reality does not change the melanin and hemoglobin mixture.
The system of Tong et al. [TTBX07], allows to transfer
cosmetic when given a before-after image pair. Guo and
Sim [GS09] transfer makeup and other facial details from
a single image to another image but without an explicit notion of makeup itself. Finding a suitable “decoration” of a
face is similar to automated caricature as done by Chen and
colleagues [CXS∗ 01].
Facial Attractiveness Facial attractiveness is perceived consistently between observers of different age, sex and cultural background [LR90] and is believed to relate to averageness [LR90], symmetry [PBP∗ 99] and sexual dimorphism
[PLP∗ 98].
Mappings from human shape to simple attributes were investigated by [EDR06] (attractiveness) or [WV09] (social
judgments). The model of Eisenthal et al. [EDR06] was
later used for facial image beautification by Leyvand and coworkers [LCODL08]. While all previous work has analyzed
real attractiveness (and synthesized virtual beauty in the case
of Leyvand’s [LCODL08] work), we conduct an analysis,
complemented by a synthesis step that improves real physical
attractiveness when applying our suggested makeup.
3. Analysis and Synthesis of Makeup
To extract the essence of makeup, we model it as a mapping
from a domain of facial appearance to a range of appearance changes, i.e. makeup. To this end, we first acquire facial
appearance including makeup then, extract the makeup, construct the mapping, and finally generate a suggested makeup.
3.1. Facial Appearance Capture
We acquired a database of N = 56 female faces in two states:
without makeup and with professional makeup. Subjects were
placed in a light tent surrounded by six projectors. To capture
advanced facial properties, we project a number of structured
light patterns in each state: Gradients along X, Y and Z and
vertical stripe patterns of increasing frequencies (8 octaves)
(cf. Fig. 1). The patterns were successively projected in less
than a second and each was captured using a Canon EOS
5D Mark II camera. First, we used the stripe patterns to perform a coarse point-based 3D reconstruction of the subject’s
face surface. Next, we fit a morphable face model [BV99]
both to all “full-on” (cf. Fig. 1) images with and without
makeup. Using the registered face model we converted all
images into a common parameterization, i. e. we make sure
that the tip of the nose is in the same location in all images
(cf. Fig. 2). Finally, we use manual 2D image deformation
to make the registration pixel-accurate. Remaining holes, i.e.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

487

Scherbaum et al. / Computer-Suggested Facial Makeup

F
ul
l
on

Xgr
a
di
ent

Ygr
a
di
ent

Zgr
a
di
ent

Poi
ntl
i
ght

S
t
r
i
pepa
t
t
er
n

Figure 1: The controlled-light setup used for facial appearance capture.

We use the 3D surface model to perform inverse lighting
i.e. using the “full-on” image (cp. Fig. 1) to compute spatially
varying diffuse reflectance [WMP∗ 06]. An individual specularity and glossiness term for each subject is computed from
the “single light source” image and for different segments,
such as the lips. The change of specularity and glossiness after
applying makeup is most noticeable on the lips. Fig. 3 shows
a typical example, where the lips show stronger specularity
and gloss after the makeup is applied.
Finally, we compute spatially varying subsurface scattering strength from the stripe patterns by direct-indirect separation [NKGR06]. We approximate scattering using a sum
of Gaussians, which has shown to be perceptually plausible [JSG09]. Comparing the subsurface scattering with and
without makeup, it can be observed that makeup reduces
subsurface scattering of the skin. The makeup on the lips
(typically lipstick) mostly eliminates the subsurface scattering.
In summary, for subject i in state X we acquired an appearance image Ai,X that stores per-pixel: diffuse color RGB (3),
3D position XYZ (3), 3D normal XYZ (3), specularity (1),
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

S
c
a
t
t
er
i
ng

Gl
os
s

S
pec
ul
a
r

Pos
i
t
i
on

From the gradient images we acquire photometric normals [MHP∗ 07]. We did not use polarized light to separate specular and diffuse normals. Finally, we remove
low-frequency normal bias using Nehab et al.’s [NRDR05]
method.

Makeup:
Mor
e,
a
nds
ha
r
perhi
ghl
i
ght
s

Figure 3: The measured specularity and glossiness changes
after applying makeup. This is most noticeable on the lips:
(Left) rendered lips without makeup, (Right) rendered lips
with makeup.

Nor
ma
l

parts of the face not seen in the image are filled using texture
in-painting [Tel04]. Please note that these manual steps are
performed while building the model, but no manual intervention is required when using the model.

Nomakeup:
l
es
shi
ghl
i
ght
s

Di
ﬀus
e

Figure 2: Registered diffuse appearance and makeup.

Figure 4: Appearance images (here: a single subject) have
a common parametrization for all subjects and all bands.
glossiness (1), scattering strength RGB (3), and scattering
size RGB (3) in a common (for all faces) parameterization
(cf. Fig. 4). The values in brackets state the number of required bands. Thus, an appearance image has a total of 17
bands.
3.2. Makeup
Once acquired, we compute the change in facial appearance
and call this change the makeup. The makeup of subject i
from state X (without makeup) to state Y (with makeup) is
denoted as Mi,X→Y . Further, we define the ratio of appearance
with and without makeup to produce makeup Mi,X→Y =
Ai,Y
Ai,X . Consequently, the multiplication of appearance without
makeup Ai,X and makeup Mi,X→Y produces appearance with
makeup Ai,Y = Ai,X ·Mi,X→Y . Note, that Mi and Ai are images
and multiplication or division must be done per pixel and
band. We assume that makeup has no geometric effects, i.e.
normals and positions are not affected by makeup and are
omitted. The change in diffuse color is most important and

488

Scherbaum et al. / Computer-Suggested Facial Makeup

we use the ratio in RGB space [LSZ01] to express the effect.
Other color spaces like LAB performed worse. Glossiness
and specularity is modeled as scalar addition, all changes to
scattering as monochromatic multiplication.
3.3. Appearance-to-Makeup Mapping
Let A be the space of all possible facial appearances and M
the space of all possible makeups. We want to find the best
mapping from a facial appearance A ∈ A to a makeup M ∈
M. For each of our 56 examples in the database this mapping
is given, because a particular makeup Mi to go from a nomakeup-state X into a makeup-state Y is only a function of the
subject’s appearance Ai . If we want to determine a makeup
Mquery for a new subject Aquery , which is not in our database,
we can perform nearest neighbor matching in the space of
facial appearance, i. e.
Mquery = M j

(1)

where
j = argmin d(Aquery , Ai ).

(2)

i

A naïve distance function d(Aquery , Ai ) for nearest neighbor
matching would be the sum of absolute differences of all pixels and bands of the two appearance images. However, this
approach would assume that each pixel and each band contains the same amount of information. Instead, we perform a
PCA of all facial appearances Ai using the classical Eigenface
approach [TP91]. The only difference is that compared to the
classical approach our appearance images contain a larger
number of bands, as for example the 3D coordinates. The
Eigenface approach allows to use the Mahalanobis distance
of the corresponding facial appearance PCA coefficients as
the distance function d(Aquery , Ai ). By determining the nearest
neighbor in PCA space, it is ensured that the most descriptive
pieces of information are used to differentiate between the
subjects and the subject with the most similar appearance is
selected.
However, we found during our experiments that the nearest
neighbor matching using the Mahalanobis distance of the
PCA coefficients does only to some extent find the makeup
that is most aesthetic for a given face. A human observer
focuses strongly on certain features of the face, in particular
on the color of the eyes, hair, or skin. These inter-subject
differences can be also found in the PCA coefficients, but
are not weighted as high in the Mahalanobis distance as they
are weighted by a human observer. Consequently, we extend
our distance function for nearest neighbor matching with the
following heuristic:
d(Aquery , Ai ) = w1 dpca + w2 deye + w3 dskin + w4 dhair

Figure 5: The two images on the right side show the query
face after makeup transfer of the best match. By applying
PCA compression our approach allows the re-synthesis of
makeup that automatically omits unwanted transfer of personal details, like freckles or moles. (The makeup is applied
5 times for better visibility.)

(3)

where dpca is the Mahalanobis distance between the PCA
coefficients of Aquery and Ai , and deye , dskin , and dhair are the
distances between the eye, skin, and hair color vectors of
Aquery and Ai in RGB space, respectively. The eye, skin, and
hair color for each subject can easily be extracted from the

Figure 6: The first five Eigen-makeups for the eye segment
and the respective average. (The contrast was enhanced for
better visibility.)
rectified appearance images. We set the weights to w1 = 0.6,
w2 = 0.2, w3 = 0.1, w4 = 0.1 in all our experiments. Experimenting with several combinations we found these heuristic
weight parameters to yield the most convincing results.
3.4. Re-Synthesis of Makeup
Now that we have determined by nearest neighbor matching
which makeup we want to copy, we could apply this makeup
to the query appearance with Aquery,Y = Aquery,X · Mquery,X→Y .
However, as shown in Fig. 5 this would result in a transferal of personal details, like freckles or moles. If the nearest
neighbor subject has freckles or moles and these are covered by makeup, these details appear as inverted freckles or
moles in the transfered makeup of the query subject. Thus,
we need to find a way to regularize our makeup. In other
words, we want to transfer only the essence of that particular
makeup compared to other makeups, but not the personal
details. This can be achieved by applying a PCA on all makeups Mi . Again, we use the Eigenface approach and calculate
the corresponding Eigen-makeups (cf. Fig. 6). The resulting
N = 56 Eigen-makeups are sorted according to their eigenvalues. The Eigen-makeups with large eigenvalues are likely
to contain variations that can be observed between many
makeups, whereas the Eigen-makeups with small eigenvalues contain the personal details. We then re-synthesize each
makeup M˜ i by using the 10 Eigen-makeups with the 10 largest
Eigenvalues. As shown in Fig. 5 these makeups are regularized to omit personal details and can be applied to the the
query appearance with
Aquery,Y = Aquery,X · M˜ query,X→Y

(4)

We can improve upon this approach by performing individual PCAs for different parts of the face. This allows the PCA
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

489

Scherbaum et al. / Computer-Suggested Facial Makeup

Figure 7: We compose the makeup merging three regions
(eyes, lips, skin). Each of the regions is reconstructed from
its own Eigenspace using 35,15,10 Eigenvectors respectively.
Note that the shown makeup is squared for better visibility.
to identify those differences between makeups that are typical
for these different parts. We segment the face into eyes, lips
as well as skin and create individual Eigenspaces for each
makeup segment (cf. Fig. 7). The eye and lip regions have
soft borders to fade into white, i.e. the identity makeup. We
then re-synthesize a complete makeup M˜ i by re-synthesizing
each part individually and compose the individual regions
to a complete makeup. As a result, the regularization is performed on the individual parts rather than the whole makeup,
which we found to perform better in practice. Using the new
approach we can for example automatically remove wrinkles
all over the face while preserving the full variance of possible
eye makeups.
3.5. Rendering
Having acquired the facial appearance from the subject in
question, we can show a visualization of novel synthesized
makeup from novel viewpoints under novel lighting, including effects such as subsurface scattering [JSG09] and specular
lighting in real-time.
4. Applications and Results
4.1. Applications
Computer-Suggested Makeup The key application of our
model is computer-suggested makeup. Given a 2D image
(e.g., a photograph) of a subject as a query, we fit the morphable face model [BV99] to the face in the image. This gives
us the facial appearance without makeup AX,query . However,
this appearance image has only 6 bands (diffuse color RGB
and position XYZ) in contrast to the 17 bands we have acquired for the appearance images Ai in the database, which
were captured in the light tent. Consequently, we can use only
the first 6 bands to perform nearest neighbor matching, as
described in Section 3.3 (cf. Fig. 8). Once the nearest neighbor is found, we just copy the missing bands from the nearest
neighbor to the query appearance, except the normals. This
allows us to perform convincing real-time 3D renderings of
the subject without makeup under varying view position and
lighting setups. We then re-synthesize the suggested makeup
from the nearest neighbor makeup (cf. Section 3.4) and generate AY,query . The user can now inspect the query subject
with suggested makeup in real-time under varying viewpoints
and different illuminations (Fig. 9).
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Figure 8: The computer-suggested makeup is found by a
nearest neighbor search that involves the 3D-shape PCA
coefficients of the facial appearance Ai as well as skin-, eye-,
and hair-color.
Makeup Transfer Makeup transfer (as done image-based
by Guo and Sim [GS09]) can be done more robustly in 3D.
The 3D information allows to perform robust inverse lighting
and, thus we can render the subject under different viewpoints
and with novel lighting The disadvantage of our approach is
that we require a with-and-without-makeup image pair to capture the makeup. The advantage of our approach is, that we
use the PCA compression as a regularizer, whereas Guo and
Sim’s approach relies on low-pass filters to remove personal
details that should not be transferred.
Our PCA approach allows to transfer finer details that are
shared by all subjects, i. e. in proximity of the eyes, which are
blurred away even for advanced filters (bilateral, non-local
means).
To perform a makeup transfer, the user provides a facial
appearance of one subject without makeup Asource,X , the same
subject with makeup Asource,Y , and a second subject without
makeup Atarget,X . As we already know which makeup we need
to transfer, we can skip the nearest neighbor search and just
need to perform the re-synthesis step from Section 3.4 to
generate Atarget,Y . A user can then inspect the effect of the
first subject’s makeup on the second subject.
Automatic Rating of Makeup A variant of computer-suggested makeup is the automatic rating of existing makeup.
Here, a user provides a with-and-without-makeup appearance
pair (AX,query , AY,query ) to the system. Given this appearance
pair the query makeup is given by
Mquery =

Aquery,Y
Aquery,X

(5)

In contrast to the computer-suggested makeup the nearest
neighbor search is now performed in the makeup space M
and not in the appearance space A. To be more specific,
we use the Mahalanobis distance of the makeup PCA coefficients as the distance function d(Mquery , Mi ) to find the
nearest neighbor in makeup space (instead of d(Aquery , Ai )
in appearance space as before). Once we found the nearest neighbor makeup Mnearest , we know the corresponding
appearanceA(Mnearest ) in the database. We can then return the
Mahalanobis distance d(A(Mnearest ), AX,query ) in appearance
space as a rating for the makeup. Thereby, a small distance is
considered a better makeup.

490

Scherbaum et al. / Computer-Suggested Facial Makeup

Wi
t
houtma
k
eup
S
ugges
t
edma
k
eup
E
x
per
tma
k
eup
F
a
r
t
hes
tma
k
eup

S
ubj
ec
t32

S
ubj
ec
t2

S
ubj
ec
t6

S
ubj
ec
t25

S
ubj
ec
t10

Figure 9: Our main application is computer-suggested makeup. Given a subject (One in each colum), starting from a face
without makeup (First Row), we suggest a makeup (Second row), according to a mapping acquired from a makeup artist (his
result shown in the third row). We include the makeup that fits the least (Fourth row) for comparison.
Inverse Computer-Suggested Makeup In inverse computer-suggested makeup, our system generates a facial appearance A that would be best for a given makeup Mquery . The
approach is similar to the automatic rating of makeup, except
that A(Mnearest ) is returned by the system instead of a rating.
This can be used for didactic purposes to study what facial
appearance corresponds to which makeup (cf.Fig. 10).

Makeup Exaggeration Given a facial appearance without
makeup A and its makeup M our model can be used for
(de)-exaggeration. The naïve approach to this problem is to
multiply M by a constant, which indeed increases the effect
of M. However, best results are achieved when moving away
form the intra-subject average (Fig. 11).

Quer
yf
a
c
e

Quer
yma
k
eup

Bes
tt
wi
t
houtma
k
eup

Bes
tt
wi
t
hma
k
eup

Figure 10: Given a query Makeup (Left), the user can now
search for the facial appearance (Right) that would fit best to
his makeup. This could be useful training makeup artists.
4.2. Perceptual study
We evaluated the effectiveness of our model for the
“computer-suggested makeup” application in a perceptual
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

491

Scherbaum et al. / Computer-Suggested Facial Makeup

All
Female
Male
Signif.

Ma
k
eup

Na
ï
v
es
c
a
l
i
ng

E
x
a
gger
a
t
ed

Figure 11: Starting from a facial appearance with makeup
(Left), naïve makeup scaling (Middle) leads to an overall
scaled makeup, e.g. the eyeshadow – which is found in many
makeups – is enhanced. When scaling the Eigenmakeup
instead (σeyes = 2.0), features typical for this individual
makeup are exaggerated (Right).
study. Participants compared three screenshots of rendered
faces that were placed next to each other in one row: Iq (query
subject without makeup), Ia (this subject with makeup), and
Ib (this subject with a different makeup). The query image of
the subject without makeup was placed in the middle of both
images with makeup.
We asked the same question at all times: “What makeup
fits best for the query appearance in Iq : The makeup in image
Ia or the makeup in image Ib ?”. In each trial we randomized the position of the query image Ia and Ib . We ran three
experiments, which used
1. the re-synthesized makeup applied by the professional
makeup artist for Ia and a randomly selected makeup for
Ib ,
2. our suggested makeup for Ia and the farthest neighbor
makeup for Ib . (The farthest neighbor makeup is the
makeup with the largest distance d(Aquery , Ai )),
3. our suggested makeup for Ia and a randomly selected
makeup for Ib .
For each of these three experiments the participants were
shown 17 image triplets. Thus, in total, each participant had
to rate 51 image triplets.
As we ran the three experiments on the Amazon Mechanical
Turk platform, we decided for two mechanisms avoiding
random answers. First, within each set of 17 images we
showed three images twice. Consequently each user rating
these three images differently was excluded from the final
results. Secondly, we evaluated the time users needed to
answer the three experiment tasks. Heuristics showed 30
seconds to be the minimal time for answering a single task.
Thus, users taking less than 30 seconds were excluded
as well. Overall these restrictions reduced the number of
valuable results from initially 210 to 146, which is 69%.
Another side effect of the filtering is that the distribution of
male and female participants differs in each experiment.
Table 1 summarizes the results of the three experiments.
It can be observed that 62 % of all participants preferred the
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Experiment 1
prof.
rand.

Experiment 2
NN
FN

Experiment 3
NN
rand.

62 %
38 %
64 %
36 %
58 %
42 %
p < 10−8

67 %
33 %
66 %
34 %
67 %
33 %
p < 10−15

58 %
42 %
62 %
38 %
55 %
45 %
p < 10−4

Table 1: Results for the three experiments of the study are
shown. The upper 3 rows state the percentage of participants that voted for a particular makeup (prof. = professional
makeup, rand. = random makeup, NN = nearest neighbor
makeup, FN = farthest neighbor makeup). The last row shows
the p-values of the corresponding Pearson’s chi-square test.
professional makeup over a random makeup. If we formulate
the null hypothesis that professional and random makeup are
equal, this null hypothesis can be rejected with a Pearson’s
chi-square test. The probability of the observed rating under
the null hypothesis is < 10−8 (p-value). Consequently, it is
extremely likely that professional and random makeup are
not equal. In the second experiment, 67 % of all participants
preferred our suggested nearest neighbor makeup over the farthest neighbor makeup. Again, it can be shown with Pearson’s
chi-square test that this difference is statistically significant
(p-value < 10−15 ). Finally, in the third experiment, 58 % of
the participants liked our suggested makeup better than a
randomly selected makeup (p-value < 10−4 ).
These results allow some interesting implications. The
small advantage a professional makeup expert can improve
upon randomness is at best 62 % − 50 % = 12 % and serves
as a baseline of what can be achieved. Overall, the computersuggested makeup does perform only slightly worse than the
professional makeup (58 % vs. 62 %). This can be considered a very good result. Much higher percentages cannot be
expected as we have modeled our mapping from the professional makeup which constitutes an upper bound. Examples
for the stimuli used as well as the full ratings are found in the
supplemental material.
5. Discussion
In all results presented, we use nearest-neighbor sampling
over M, though more advanced reconstructions can improve
the results. Further, we use multiplication [LSZ01] of diffuse
colors to simulate the effect of makeup, while the more advanced Kubelka-Munk-theory could be used. Linear blending
of multiple components using the first three Eigenvectors that
are sufficiently orthogonal creates in-between makeups, but
becomes implausible for others. Here, more than 56 samples
would be required. Assuming uniform distribution, we can
infer from the given Eigenvector distribution that our makeup
space is sufficiently dense for lips and skin while for the eyes
more samples would be needed. Within our current setup
we expect 100 samples to be suitable. In our approach, we
currently assume that there is no bad example in the database

492

Scherbaum et al. / Computer-Suggested Facial Makeup

and the expert was always right. To generalize to arbitrary
non-expert input (e. g. from community photo databases), a
rating of the individual makeup would need to complement
the approach. We base our model on makeup performed by a
single professional expert makeup artist. This introduces the
artist’s personal taste as a bias which could be reduced by using more artists. Consequently this would require even more
samples in order to guarantee a densely sampled makeup
space. We re-synthesize new makeups always from a single best-match makeup. This is on the one hand suitable as
makeup artists focus on creating styles, where eye, skin and
lip makeup homogeneously match. However, combining different makeups could lead to new and interesting makeup
styles, such as mixing the facial regions of different makeups.
In this case more sophisticated blending algorithms would be
mandatory.
6. Conclusion and Future Work
We have presented a first step to computationally model the
relation of facial appearance and facial makeup. Using a
database of examples, we built a model that allows applications such as machine-suggested, individualized makeup that
can improve perceived attractiveness as shown in a perceptual
study. Despite previous approaches our machine-suggested
makeup involves 3D information for both, analysis and synthesis. Such, results can be relighted and inspected in arbitrary
poses and under arbitrary lighting conditions.
In future work, the ideas presented in this paper could
be extended to model many other aspects of human “style”
that might depend in a more or less observable way on individual human properties. Examples are hairstyle from facial
appearance and color or clothing style from body appearance
or human gait. Also external parameters could be included:
makeup for a sunny day might have other requirements than
the one for a nightclub.
References
[BV99] B LANZ V., V ETTER T.: A morphable model for the
synthesis of 3D faces. In Proc. SIGGRAPH (1999), pp. 187–194.
2, 5
[Cor72] C ORSON R.: Fashion in makeup: From Ancient to Modern
Times. Peter Owen Publishers, 1972. 2
[CXS∗ 01] C HEN H., X U Y.-Q., S HUM H.-Y., Z HU S.-C.,
Z HENG N.-N.: Example-based facial sketch generation with
non-parametric sampling. Proc. ICCV 2 (2001), 433. 2

[LCODL08] L EYVAND T., C OHEN -O R D., D ROR G., L ISCHIN SKI D.: Data-driven enhancement of facial attractiveness. ACM
Trans. Graph. (Proc. SIGGRAPH) 27, 3 (2008). 2
[LR90] L ANGLOIS J. H., ROGGMAN L. A.: Attractive faces are
only average. Psychological Science 1 (1990), 115–121. 2
[LSZ01] L IU Z., S HAN Y., Z HANG Z.: Expressive expression
mapping with ratio images. In Proc. SIGGRAPH (2001), pp. 271–
276. 4, 7
[MHP∗ 07] M A W.-C., H AWKINS T., P EERS P., C HABERT C.-F.,
W EISS M., D EBEVEC P.: Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination.
In Proc. EGSR (2007), pp. 183–194. 3
[MWL∗ 99] M ARSCHNER S. R., W ESTIN S. H., L AFORTUNE E.
P. F., T ORRANCE K. E., G REENBERG D. P.: Image-based BRDF
measurement including human skin. In Proc. EGWR (1999),
pp. 139–152. 2
[NKGR06] NAYAR S. K., K RISHNAN G., G ROSSBERG M. D.,
R ASKAR R.: Fast separation of direct and global components of
a scene using high frequency illumination. ACM Trans. Graph.
25, 3 (2006), 935–944. 3
N EHAB D., RUSINKIEWICZ S., DAVIS
MAMOORTHI R.: Efficiently combining positions and

J., R A normals
for precise 3D geometry. ACM Trans. Graph. (Proc. SIGGRAPH)
24, 3 (2005). 3

[NRDR05]

[NRK99] N UMATA K., R I K., K IRA K.: HD E-make: A real-time
HD skin-make-up machine. Eizo Joho Media Gakkai Gijutsu
Hokoku 24 (1999), 45–50. 2
[PBP∗ 99] P ERRETT D. I., B URT D. M., P ENTON I., L EE K.,
ROWLAND D., E DWARDS R.: Symmetry and human facial attractiveness. Evolution & Human Behavior (1999), 295–307. 2
[PLP∗ 98] P ERRETT D., L EE K., P ENTON I., ROWLAND D.,
YOSHIKAWA S., B URT D. M., H ENZI S. P., C ASTLES D., A KA MATSU S.: Effects of sexual dimorphism on facial attractiveness.
Nature (1998), 884–887. 2
[Rus03] RUSSEL R.: Sex, beauty, and the relative luminance of
facial features. Perception 32 (2003), 1093–1107. 2
[SN07] S EYAMA J., NAGAYAMA R. S.: The uncanny valley:
Effect of realism on the impression of artificial human faces.
Presence: Teleoper. Virtual Environ. 16, 4 (2007), 337–351. 2
[Tel04] T ELEA A.: An image inpainting technique based on the
fast marching method. J. Graphics Tools 9, 1 (2004), 23–34. 3
[Tho05] T HOMSON K.: Sometimes Less Is Not More: A Fabulous
Guide to Fabulous Makeup. Lipstick Press, 2005. 1
[TOS∗ 03] T SUMURA N., O JIMA N., S ATO K., S HIRAISHI M.,
S HIMIZU H., NABESHIMA H., A KAZAKI S., H ORI K., M IYAKE
Y.: Image-based skin color and texture analysis/synthesis by
extracting hemoglobin and melanin information in the skin. In
Proc. SIGGRAPH (2003), pp. 770–779. 2
[TP91] T URK M., P ENTLAND A.: Face recognition using eigenfaces. Proc. CVPR (1991), 586–591. 4

[EDR06] E ISENTHAL Y., D ROR G., RUPPIN E.: Facial attractiveness: Beauty and the machine. Neural Comput. 18, 1 (2006),
119–142. 2

[TTBX07] T ONG W.-S., TANG C.-K., B ROWN M. S., X U Y.-Q.:
Example-based cosmetic transfer. In Proc. PG (2007), pp. 211–
218. 1, 2

[GS09] G UO D., S IM T.: Digital face makeup by example. In
Proc. CVPR (2009). 1, 2, 5
[HHG00] H AXBY J. V., H OFFMAN E., G OBBINI M. I.: The
distributed human neural system for face perception. Trends Cogn
Sci. 4 (2000), 223–233. 2

[WMP∗ 06] W EYRICH T., M ATUSIK W., P FISTER H., B ICKEL
B., D ONNER C., T U C., JANET M C A NDLESS J. L., N GAN A.,
J ENSEN H. W., G ROSS M.: Analysis of human faces using a
measurement-based skin reflectance model. ACM Trans. Graph.
(Proc. SIGGRAPH) (2006), 1013–1024. 2, 3

[JSG09] J IMENEZ J., S UNDSTEDT V., G UTIERREZ D.: Screenspace perceptual rendering of human skin. ACM Trans. on Applied
Perception 6, 4 (2009). 3, 5

[WV09] WALKER M., V ETTER T.: Portraits made to measure:
Manipulating social judgments about individuals with a statistical
face model. J. Vision 9, 11 (2009), 1–13. 2

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

