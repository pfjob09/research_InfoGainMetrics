DOI: 10.1111/j.1467-8659.2011.02032.x
Eurographics Symposium on Geometry Processing 2011
Mario Botsch and Scott Schaefer
(Guest Editors)

Volume 30 (2011), Number 5

A Multiscale Approach to Optimal Transport
Quentin Mérigot
Laboratoire Jean Kuntzmann, Université de Grenoble and CNRS

Abstract
In this paper, we propose an improvement of an algorithm of Aurenhammer, Hoffmann and Aronov to find a least
square matching between a probability density and finite set of sites with mass constraints, in the Euclidean plane.
Our algorithm exploits the multiscale nature of this optimal transport problem. We iteratively simplify the target
using Lloyd’s algorithm, and use the solution of the simplified problem as a rough initial solution to the more
complex one. This approach allows for fast estimation of distances between measures related to optimal transport
(known as Earth-mover or Wasserstein distances). We also discuss the implementation of these algorithms, and
compare the original one to its multiscale counterpart.
Categories and Subject Descriptors (according to ACM CCS): I.3.5 [Computer Graphics]: Computational Geometry
and Object Modeling—Geometric algorithms, languages, and systems

1. Introduction
Engineer and mathematician Gaspard Monge proposed the
following problem [Mon81]: what is the cheapest way to
transport a pile of sand into a hole with minimum cost, knowing that moving an individual particle from a position x to
another position y has a cost c(x, y) ? This problem gave
birth to the field of optimal transport, which has been very
vivid in the past twenty years, with applications in geometry,
probability and PDEs (see e.g. [Vil09]).
However, Monge’s problem was an engineering problem,
and it is not very surprising that various form of optimal
transport appeared in many applied fields. In computer vision,
distances defined using optimal transport have been used as a
way to compare the color histograms of images in [RTG00]
under the name of Earth mover distance. Optimal transport
on the circle has been used for transferring the hue of an
image to another [DSS10]. In combination with Lloyd’s algorithm, optimal transport seems a interesting tool for optimal
quantization [BSD09]. More recently, methods inspired by
(or relying on) optimal transport have been proposed as a
tool in several parts of geometry processing: surface comparison [LD11], surface reconstruction from data corrupted
with outliers [CCSM10, MDGD∗ 10], construction of optimized primal-dual triangulations [MMD11], reconstruction
with sharp corners and edges [dGCSAD11].
Yet, the lack of a practical method to compute optimal
transports maps except in 1D has hindered the development
c 2011 The Author(s)
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd. Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ,
UK and 350 Main Street, Malden, MA 02148, USA.

of many of these applications. Even in the simplest planar
situation, namely with c(x, y) = x − y 2 , there is a lack of
reasonably fast and widely usable method able to efficiently
compute optimal transport maps.
1.1. L2 optimal transport
In the remaining of the paper, we will deal with the L2
optimal transport, i.e. where the cost for moving a certain
amount of mass from a point x to a point y is proportional
to square of the Euclidean distance x − y 2 . This model is
well-understood theoretically, and has several nice properties (such as uniqueness of the solution) that follow from
strict concavity of the cost. Numerical schemes have been
proposed by Brenier-Benamou [BB00], Loeper [LR05] and
Angenent-Haker-Tannenbaum [AHT03] to solve L2 optimal
transport. However, numerical instabilities makes them difficult to use for general problems: for instance, [LR05] requires
a lower bound on the density of the source measure, while the
gradient descent algorithm of [AHT03] suffers from a drift
effect which produces optimal maps that are not transport
plans. Another possibility to find optimal transport plans is by
discretizing the source and/or target measure. These discrete
approach include linear programming, the Hungarian method,
and a variant known as Bertsekas’ auction algorithm [Ber88].
These methods work for general cost functions, and are often
unable to take advantage of the geometric simplifications that
occur when working specifically with the squared Euclidean
metric.

1584

Q. Mérigot / A Multiscale Approach to Optimal Transport

A promising approach, both from the practical and the theoretical point of view has been proposed in [AHA98]. In this
approach, the source measure has a density ρ while the target
measure is given by a sum of Dirac masses supported on a
finite set S. The fact that the source measure has a density
ensures the existence and uniqueness of the optimal transport map. In this case, solving the optimal transport problem
amounts to finding a weight vector (w p ) p∈S such that the
power diagram of (S, w) has the following property: for every
point s in S, the proportion of the mass of ρ contained in the
corresponding power cell should be equal to the mass of the
Dirac mass at s (see Section 2.2). In the same article, the
authors also introduced a convex function whose minimum
is attained at this optimal weight vector, thus transforming
the optimal transport problem into an unconstrained convex
optimization problem on RN where N is the number of points
in the set S.

1.2. Contributions
We revisit the approach of [AHA98] with implementation
in mind. Our main contribution is a multiscale approach for
solving the unconstrained convex minimization problem introduced in [AHA98], and thus to solve L2 optimal transport.
Let us sketch briefly the main idea. In order to solve the
optimal transport problem between a measure with density
µ such as a grayscale image and a discrete measure ν, we
build a sequence ν0 = ν, . . . , νL of simplifications of ν using Lloyd’s algorithm. We start by solving the much easier
transport problem between µ and the roughest measure νL
using standard convex optimization techniques. Then, we
use the solution of this problem to build an initial guess for
the optimal transport between µ and νL−1 . We then proceed
to convex optimization starting from this guess to solve the
optimal transport between µ and νL−1 . This is repeated until
we have obtained a solution to the original problem. If the
original target measure has a density, we use Lloyd’s algorithm to obtain the first discretization ν. Note that this idea
was independently proposed in [Bos10].
This procedure provides a significant speedup, up to an
order of magnitude, for computing optimal transport plans.
Moreover, at every step of the algorithm it is possible to obtain a lower and upper bound on the Wasserstein distance
(also known as Earth-mover distances [RTG00]) between the
source measure µ and the original target measure ν. Using this
approach, one can obtain rough estimates of Wasserstein distances between two images with a speedup of up to two order
of magnitude over the simple convex optimization approach.

2. Background
We briefly recap the few concepts of measure theory and
optimal transport that we use, before explaining the relation
between the L2 optimal transport and power diagrams. We

also recall how optimal transport can be turned into an unconstrained convex optimization problem.

2.1. Measure theory and Optimal transport
A non-negative measure µ on the space Rd is a map from
(measurable) subsets of Rd to a non-negative numbers, which
is additive in the sense that µ (∪i∈N Bi ) = ∑i µ(Bi ) whenever
(Bi ) is a countable family of disjoint (measurable) subsets.
The total mass of a measure µ is mass(µ) := µ(Rd ). A measure µ with unit total mass is called a probability measure.
The support of a measure µ, denoted by spt(µ) is the smallest
closed set whose complement has zero measure.
The optimal transport problem involves two probability
measures: a source measure µ, and a target measure ν. We
will always suppose that the source measure µ has a density,
i.e. there exists a non-negative function ρ on Rd such that for
every (measurable) subset B of Rd ,
µ(B) :=

ρ(x)dx.
B

On the other hand, we will assume that the target measure ν
is discrete, supported on a finite set S of Rd . This means that
there exists a family of positive coefficients (λ p ) p∈S such
that for every subset B,
ν(B) =

∑

λp.

p∈S∩B

The above formula is equivalent to writing ν as the sum
∑ p∈S λ p δ p , where δ p is the unit Dirac mass at p. The integral
of a continuous function φ with respect to these two measures
is Rd φ(x)dµ(x) := Rd φ(x)ρ(x)dx, and Rd φ(x)dν(x) :=
∑ p∈S λ p φ(p).
In Section 3.1 we will see how to adapt the proposed
method to the case where the source and target measures both
have density.

Transport map. The pushforward of a measure µ by a map
T : Rd → Rd is another measure T# µ defined by the equation
T# µ(B) := µ(T −1 (B)) for every subset B of Rd . A map T is
called a transport map between µ and ν if the pushforward of
µ by T is ν. We denote by Π(µ, ν) the set of transport maps
between µ and ν.
For instance, a map T is a transport map between the
source measure µ and the target measure ν described at the
beginning of this paragraph if and only if for every point p in
the support S of ν,
λ p = µ(T −1 ({p}))

=

T −1 ({p})

ρ(x)dx .

Optimal transport maps. The cost of a transport map T
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1585

Q. Mérigot / A Multiscale Approach to Optimal Transport

Figure 1: Interpolation between the standard pictures Photograph and Peppers obtained with our algorithm (see Section 5.3).
The target image was quantized with 625 Dirac masses for the first row, and 15625 Dirac masses for the second row. The cells in
these pictures are interpolation between power cells (on the left) and Voronoi cells (on the right), obtained by linearly varying
the weights from their value on the left picture to zero. Their color vary so that the product of the area of a cell multiplied by its
gray level remains constant over time.
between the source measure µ with density ρ and the target
measure ν is defined by:
c(T ) :=

Rd

x − T (x) 2 dµ(x)

=
Rd

x − T (x) 2 ρ(x)dx

The problem of optimal transport, also called Monge’s problem, is to find a transport map Topt whose cost is minimal
among all transport maps between µ and ν, i.e.
Topt := arg min{c(T ); T ∈ Π(µ, ν)}
The non-convexity of both the cost function c and of the set
of transport plans makes it difficult, in general, to prove the
existence of a minimum. However, in this specific case where
µ has a density and the cost is the squared Euclidean norm
the existence follows from [Bre91].

Wasserstein distance. The Wasserstein distance between
two probability measures µ and ν, µ having a density, is the
square root of the minimal transport cost. We denote it by
Wass2 (µ, ν). Intuitively, the Wasserstein distance measures
the minimum global cost of transporting every bit of µ onto
ν, supposing that moving an infinitesimal amount dµ(x) from
x to y is equal to x − y 2 dµ(x).
Note that our definition above is not symmetric, as it requires the source measure to have a density. However, this
restriction can be leveraged using the notion of transport
plan instead of transport map, leading to a definition of
Wasserstein distance between any pair of probability measures [Vil09, Ch. 6]).
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2.2. Power diagrams
Let S be a finite set of points in Rd , and w : S → R be a
given weight vector. The power diagram or weighted Voronoi
diagram of (S, w) is a decomposition of the ambient space in
a finite number of cells, one for each point in S, defined by
the property that a point x belongs to Vorw
S (p) iff for every
q in S, one has x − p 2 − w(p) ≤ x − q 2 − w(q). Note
that if the weights are all zero, this coincides with the usual
Voronoi diagram.
Given such a decomposition, we will consider the application TSw which maps every point x in the power cell Vorw
S (p)
to its center p. This map is well-defined everywhere except
on the boundary of the power cells, but since this set has
zero Lebesgue measure this has no consequence for us. The
pushforward of the source measure µ by the map TSw is a sum
of Dirac masses centered at every point p in the set S, whose
mass is the µ-mass of the corresponding power cell:
TSw # µ =

∑ µ(VorwS (p))δ p ,

p∈S

In [AHA98], the following theorem was proven. Note that
this result, as well as Theorem 2, can be also obtained as a
consequence of Brenier theorem [Bre91].
Theorem 1 For any probability measure µ with density, and
a weighted set of points (S, w), the map TSw is an optimal
transport map between the measure µ and the pushforward
measure TSw |# µ. Consequently,
1/2

Wass2 µ,

TSw # µ

=

∑

w
p∈S VorS (p)

2

x − p ρ(x)dx

.

1586

Q. Mérigot / A Multiscale Approach to Optimal Transport

This theorem gives examples of optimal transport plans
and the discrete probability measures supported on S that can
be written as the pushforward TSw |# µ, where w is a set of
weights on S. As we will see in the next section (Theorem 2),
it turns out that every probability measure supported on S can
be written in this way. Said otherwise, the optimal transport
maps between µ and a measure supported on S can always be
written as TS,w for some weight vector w.
2.3. An unconstrained convex optimization problem
Adapted weight vectors. Let µ be a probability measure with
density ρ, and ν a discrete probability measure supported on a
finite set S, with ν = ∑ p∈S λ p δ p . A weight vector w : S → R
on S is called adapted to the couple of measures (µ, ν) if for
every site p in S, one has
λ p = µ(Vorw
S (p))

=

VorwS (p)

ρ(x)dx .

Moreover, Theorem 2 below asserts that finding a weight
vector adapted to the couple (µ, ν) amounts to finding a global
minimum of the function Φ below, thus turning the very constrained original problem (minimization among convex maps)
into an unconstrained convex optimization problem. Note
that this theorem follows from the discussion in Section 5
of [AHA98].

∑

λ p w(p) −

p∈S

VorwS (s)

( x−s

2

− w(p))dµ(x)
(2)

Theorem 2 Given a measure µ with density ρ on Rd , and ν =
∑s∈S λ p δ p , the following three statements are equivalent:
(i) the power map TSw realizes an optimal transport between
the measures µ and ν ;
(ii) w is adapted to the couple (µ, ν);
(iii) w is a global minimizer of the convex function Φ.
We will recall briefly how the value of the gradient of the
function Φ can be computed at a weight vector w. Incidentally, the steps necessary to this computation almost sketch a
complete proof of the theorem. Consider the map
Ψ(w) :=

∑

w
p∈S VorS (p)

( x− p

2

− w(p))dµ(x)

2

− w(p) ≤ x − T (x)

T

ΨT (w) :=

( x − T (x)

2

− w(T (x)))dµ(x).

R2

Since the functions ΨT all depend linearly on w, the function
Ψ is concave, and Φ is convex. Moreover, it is easy to check
that the values of the functions Ψ and ΨTSw coincide for the
weight vector w. Consequently, their gradient coincide to at
that point, and a simple computation shows :
∂Ψ
(w) =
∂w(p)

VorwS (p)

∂Φ
(w) = λ p −
∂w(p)

ρ(x)dx

VorwS (p)

ρ(x)dx

(3)
(4)

In particular, the second equation shows that the gradient ∇Φ
vanishes at a weight vector w if and only if w satisfies Eq. (1).
3. Multiscale Approach for Minimizing Φ
The efficiency of an optimization technique relies on two
important choices. The most important one is the choice of
descent algorithm, as it is well-known that the difference
in efficiency between (for instance) the first order simple
gradient descent algorithm and the second order Newton
methods can be tremendous [Fle87].
The second one is the choice of the position from where
the optimization is started. Its importance shouldn’t be disregarded, even for convex optimization, as the second-order
convergence in Newton’s descent does only happen in a small
basin around the global minimizer.
In this section, we introduce our multiscale algorithm for
finding a global minimum of Φ. We start by building a decomposition of the target measure ν, i.e. a sequence of discrete
measures ν0 := ν, ν1 , . . . , νL that are simpler and simpler as
L increases. The level of the decomposition is then used to
construct a good initial weight vector for the optimal transport
problem (µ, ν −1 ), in a hierarchical way.
3.1. Decomposition of the target measure
A decomposition of the target measure ν = ∑ p∈S λ p δ p is a
sequence of discrete probability measures (ν ) ≥0 such that
ν0 = ν and ν is supported on a set S :
ν =

∑

λ p, δ p .

p∈S

which is related to the our function by the equation Φ(w) =
∑ p∈S λ p w(p) − Ψ(w). The map Ψ is concave, as we will
show by writing it as an infimum of linear functions. Consider
any map T from Rd to the finite set S. By definition of the
power cell, for every point x in Vorw
S (p) one has
x− p

Ψ(w) = inf ΨT (w), where

(1)

By Theorem 1, if the weigh vector w is adapted to the couple
(µ, ν) then the optimal transport between µ and the discrete
measure ν is given by the map TS,w .

Φ(w) :=

As a consequence, the function Ψ can be rewritten as

2

− w(T (x))

Moreover, for every level we are given a transport map
between ν and ν +1 , that is a map π from the support S of
ν to the (smaller) support S +1 of ν +1 with the additional
property that for every point p in S +1 ,
λ p,

+1

=

∑

λq,

(5)

q∈π−1 (p)

c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1587

Q. Mérigot / A Multiscale Approach to Optimal Transport

The decomposition that we will consider in practice are constructed using Lloyd’s algorithm, as explained in Section 4.2.
This means that the transport map π maps every point p in
S to its nearest neighbor in S +1 .

black spots in the interpolated pictures (see Section 5.3). The
choice of . 1 plays a different role which we describe in the
next paragraph.

We remark that having access to such a transport map
between ν and ν +1 allows to bound the Wasserstein distance
between these two measures. By considering the composition
of transport maps, it is also possible to bound the distance
between e.g. ν and νL . Letting π = πL−1 ◦ . . . ◦ π0 one has:

3.3. Computation of Wasserstein distances

1/2

Wass2 (ν, νL ) ≤

∑ λp

p − π(p)

2

(6)

p∈S

3.2. Algorithm

The multiscale descent algorithm is summarized in Algorithm 1. Note that the algorithm does not depend on the
choice of the convex optimization scheme (L-BFGS), which
we will discuss later in Section 4.
Algorithm 1 Multiscale minimization of Φ := Φ0
wL := 0
for = L − 1 to 0 do
set w ,0 (p) := w +1 (π (p)) for every p ∈ S
k := 0
repeat
compute w ,k+1 from w ,k using L-BFGS on Φ
set vk+1 := ∇Φ (w ,k+1 ), k := k + 1
until vk q > ε
set w := w ,k
end for
In the stopping condition . q denotes the usual Lq -norm
where q > 1 or q = +∞. In particular,
∞

= sup |λ p − µ(Vorw
S (p))|
p∈S

∇Φ(w)

Proposition 1 Let w be the a weight vector on S, and consider the image measure ν˜ := TS,w # µ. Then,
Wass2 (ν, ν˜ ) ≤ D × ∇Φ(w)

We are given the source probability measure µ, and a decomposition (ν )0≤ ≤L with L levels of the target measure. The
goal is to use the solution of the optimal transport problem
from µ to ν +1 at level + 1 to construct the initial weight
vector for the optimal transport problem between µ and ν at
level . As before, we will consider the weight vectors at level
as functions from the support S of µ to R. The function
Φ that we optimize at step is given by the same formula as
in Eq. (2).

∇Φ(w)

Simple lower and upper bounds on Wasserstein distance can
be obtained at every step of the multiscale algorithm, using
the fact that ∇Φ(w) 1 corresponds to the twice amount
mass that has been misaffected. This follows from the following proposition:

1

=

∑ |λ p − µ(VorwS (p))|

2/1
1 ,

where D is the diameter of the support S of ν.
Proof By definition, both this measure ν˜ and the target measure ν are supported on the same set S. Moreover,
m := ∇Φ(w)

1

=

∑

λp −

p∈S

VorwS (p)

ρ(x)dx

corresponds to the amount of mass that has been mistransported. The cost of transporting
this mass back at the right
√
place in S is at most mD2 , where D = diam(S).
As a consequence of this proposition, stopping Algorithm 1
at level with weight vector w yields the following estimation of Wass2 (µ, ν):
Wass2 (µ, ν) −

Rd

x − TS ,w (x) 2 ρ(x)dx

≤ D × ∇Φ (w )

1/2

1/2
1 + Wass2 (ν, ν

)

(7)

Said otherwise, if one wants to compute the Wasserstein
distance between µ and ν up to a certain error ε, it is not
necessary to consider the levels of the decomposition below
the first level 0 such that Wass2 (ν 0 , ν) < ε. Note that this
quantity can be estimated thanks to Eq. (6). The effectiveness
of this approach is discussed in Section 5.2.
Proof of Eq. (7) By Theorem 1, the map TS ,w is an optimal
transport between the measure µ and ν˜ = TS ,w # µ. This
means that the Wasserstein distance Wass2 (µ, ν˜ ) is equal to
c . Moreover, by the reverse triangle inequality,
| Wass2 (µ, ν) − Wass2 (µ, ν˜ )| ≤ Wass2 (ν˜ , ν ) + Wass2 (ν , ν).
One then concludes using the previous proposition.

p∈S

The first quantity measures the maximum error that has been
made by considering the weight vector w instead of the optimal one. In particular, if ∇Φ(w) ≥ min p∈S λ p , then one is
sure that all the cells in the power diagram of (S, w) intersect
µ non-trivially. This is important especially for visualization
purpose, as the existence of cells with zero µ-mass lead to
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

3.4. Convergence of Optimal Transport Maps
In this paragraph, we discuss the soundness of constructing an
initial weight vector for the optimal transport problem (µ, ν )
from an adapted weight vector for the problem (µ, ν +1 ). The
result of this section are summarized in the following theorem.
Note that the definition of zero-mean convex potential is given

1588

Q. Mérigot / A Multiscale Approach to Optimal Transport

below, and is necessary to define uniquely the adapted weight
vector: without this, adapted weight vectors are defined up to
an additive constant.
Theorem 3 Let ν and (νn )n≥1 be discrete probability measures supported on finite sets S and (Sn )n≥1 respectively, such
that limn Wass2 (ν, νn ) = 0. Let:
wn :Sn → R be adapted to (µ, νn )
w :S → R be adapted to (µ, ν)
Suppose that both weight vectors yield zero-mean convex
potentials (see below), and that the assumptions of Proposition 2 are satisfied. Then, for every sequence of points pn ∈ Sn
converging to a point p in S, one has w(p) = lim wn (pn ).

and Tn (resp. T ) denotes the optimal optimal transport map
between µ and νn (resp. ν). Then, for every positive ε,
lim µ(∆ε (T, Tn )) = 0

where ∆ε (T, Tn ) := {x ∈ Rd ; T (x) − Tn (x) ≥ ε}.
Proof of Proposition 2. For almost every x in Ω, the gradient
∇φn (x) = Tn (x) is included in the support of νn , hence in
the ball B(0, L) by (iii). The same holds for T , so that the
inequality T − Tn ≤ 2L holds for almost every x in Ω. For
every p ≥ 1,
T − Tn

L p (µ)

T (x) − Tn (x)

:=

1
2

x

2

− min x − p
p∈S

2

− w(p)

1
= max x|p + (w(p) − p 2 )
2
p∈S

(8)
(9)

where v|w = ∑i vi wi denotes the usual Euclidean scalar
product. From these two formulations, it is easy to see that
w
the function φw
S is convex, and that its gradient ∇φS coinw
cides with the transport map TS . We call such a function a
convex potential for the optimal transport plan. Since adding
a constant to the weight vector (or to φw
S ) does not change
the transport plan, we consider the zero-mean convex potential which is uniquely defined by the extra assumption that
w
Rd φS (x)ρ(x)dx = 0.
Proposition 2 Let µ and ν be two probability measures, µ
having density ρ and ν supported on a finite set S. Let (νn ) be
a sequence of probability measures supported on finite sets
(Sn ), s.t. limn Wass2 (νn , ν) = 0. Assume that:
(i) the support of ρ is the closure of a connected open set Ω
with regular (piecewise C 1 ) boundary ;
(ii) there exists a positive constant m such that ρ ≥ m on Ω;
(iii) the support of all the measures νn is contained in a fixed
ball B(0, L) ;
wn
Denote φ := φw
S (resp. φn := φSn ) the zero-mean convex potential of the optimal transport between µ and ν (resp. νn ).
Then, φn converges to φ uniformly on Ω as n grows to infinity.

This proposition is similar to [Bos10, Theorem 1], but
without the requirement that the source and target measure
have to be supported on convex sets. It relies on the following
result (cf [Vil09], Corollary 5.23):
Fact. Let νn be a sequence of measures converging to µ,

ρ(x)dx

T (x) − Tn (x)

=

p

ρ(x)dx

Ω\∆ε (T,Tn )

T (x) − Tn (x)

+

φw
S (x) :=

p

Ω

Before proving this theorem, we need to introduce a few
definitions and auxiliary results.

Convex potential. Let ν be a probability measure supported
on a finite set S, and let w denote the weight vector adapted
to the optimal transport problem between µ and ν. Set

(10)

n→+∞

p

ρ(x)dx

∆ε (T,Tn )
p
p

≤ ε + (2L) µ(∆ε (T, Tn ))
Using Eq. (10), we obtain the convergence of Tn to T in the
L p (µ)-sense. Thanks to the the assumptions (i) and (ii), we
can apply the Poincaré inequality on the domain (Ω, µ) to the
zero-mean potentials φn and φ to get:
φn − φ

L p (µ)

:=

Rd

φn (x) − φ(x) ρ(x)dx

≤ const(Ω, ρ) Tn − T

L p (µ) .

p

In other words, φn converges to φ in the L (µ)-sense. Since
the support of all target measures is contained in a ball of
size L, ∇φn ≤ L, and φw
S is L-Lipschitz. Hence, φn also
converges uniformly to φ on the support of µ.
Proof of Theorem 3. We begin by proving that there for every point p in S, there exists a sequence of points qn ∈ Sn
converging to p such that wn (qn ) also converges to w(p). Applying Eq. (10) with ε equal to half the minimum distance
between two points in S ensures that Tn converges to T on
a set F with full Lebesgue measure in Ω. Choose a point x
in the intersection of the cell Vorw
S (x) and of F, and consider
the sequence qn = Tn (p). This sequence converges to p, and
by definition one has :
1
φn (pn ) − φ(p) = ( x − p 2 − x − pn 2 + w(p) − w(pn ))
2
Using the uniform convergence of φn to φ, one deduces that
w(qn ) converges to w(p).
We now prove by contradiction that if (pn ) converges to
p, then lim sup wn (pn ) is at most w(p). Suppose not: taking
subsequence if necessary, the limit of wn (pn ) is larger than
w(p) by a positive η. For every point x in Ω, we use the
triangle inequality to get
x − pn

2

− wn (pn ) ≤ x − qn

with rn := qn − pn

2

2

− wn (qn ) + rn

(11)

+ 2D pn − qn + wn (qn ) − wn (pn )

and D is the maximum distance between a point in Ω and a
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1589

Q. Mérigot / A Multiscale Approach to Optimal Transport

point in the ball B(0, L) defined by the assumption (iii). Using
the convergence of (pn ) and (qn ) to the same point p, and the
assumption on the limits of wn (pn ) and wn (qn ), we obtain
limn→+∞ rn ≤ −η. Combining this with Eq. (11) shows that
n
the cell of pn in the power diagram Vorw
Sn does not intersect
Ω for n large enough. This contradicts the hypothesis that pn
is a Dirac with positive mass in the support of νn .
The proof that lim inf wn (pn ) is larger than w(p) is very
similar but a bit longer, as it requires the use of the zero-mean
assumption for the convex potentials. These two bounds for
lim inf / sup wn (pn ) conclude the proof.

In practice, we only use it the second query for the functions
f : x → x − x0 2 . We developped two different models of
measure with density.
The first one is the uniform measure on a convex polygon
R. In this case, computing the mass of a polygon P amounts
to computing the area of the intersection P ∩ R of two convex polygons. The integral of the squared distance function
x → x − x0 2 over the polygon P ∩ R is computed by triangulating P and summing the integral over each triangle
T . The integral on T can be obtained in closed-form: if one
denotes by cov(T, x0 ) the covariance matrix of T with base
point x0 , then

4. Implementation
In the first paragraph, we give some details our implementation of the convex optimization method proposed in [AHA98]
for a fixed target measure. Then, we explain how we compute
the hierarchical decomposition of the target measure needed
for the multiscale algorithm.

T

P

Solving optimal transport between a probability measure
µ with density ρ and a discrete measure ν = ∑ p∈S λ p δ p
amounts to finding the minimum of the convex function Φ
given in Theorem 2.(iii):

∑

λ p w(p) −

p∈S

with

∂Φ
(w) = λ p −
∂w(p)

VorwS (p)

( x− p

VorwS (p)

2

− w(p))ρ(x)dx

ρ(x)dx

We need three ingredients to achieve this goal: an efficient and
robust implementation of power diagram computation, robust
numerical integration functions, and convex optimization
software. In this paragraph, we discuss and motivate our
choices regarding these three aspects.
Power diagrams. We use the Regular_triangulation_2 package from CGAL [cga]. It is tempting to try to avoid recomputing the whole power diagram for every evaluation of the function Φ by using the same approach that was used in [MMdCTAD09] to maintain the Delaunay triangulation. However,
as shown in Figure 3(a), the topology of the power diagram
keeps changing until the very last steps of the optimization,
thus discarding this approach.
Numerical integration. In our C++ implementation, a measure µ with density ρ is represented by an object which can
answer the following two queries. Given a convex polygon
P = [a0 , . . . , aN = a0 ], and a function f from P to R, the class
should provide a way to compute:
1. the mass of P, i.e. P ρ(x)dx ;
2. the integral of f over P, i.e. P f (x)ρ(x)dx.
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2

dx = cov(T, x0 )11 + cov(T, x0 )22

The second model corresponds to the density obtained
from a grayscale image. We assume that the density ρ is
constant on each square pixel pi, j = [i, i + 1) × [ j, j + 1),
equal to the value ai, j . We then consider:

4.1. For a fixed target measure

Φ(w) =

x − x0

ρ(x)dx = ∑ ai, j area(P ∩ pi, j )

f (x)ρ(x)dx
P

(12)

i, j

∑ ai, j area(P ∩ pi, j ) f (i, j)

(13)

i, j

Note that it is not possible to simply replace the area of
P ∩ pi, j by zero or one depending on whether P intersects pi, j
or not, thus disallowing a more efficient GPU implementation.
However, since the area of P ∩ pi, j needs to be computed only
for pixels containing edges or vertices of P the algorithm we
use remains rather efficient. Pixels on edges are dealt with
while applying Bresenham’s algorithm to raster the polygon.
The coverage of pixels containing vertices of P is obtained
simply by computing the intersection of the polygon P with
the square pi, j .
Convex optimization. We tried several approaches for the
actual convex optimization. All of these methods use the
following rough scheme to construct the sequence of weight
vectors (wk ):
(i) Determine a descent direction dk ;
(ii) Determine a timestep sk , and set wk+1 = wk + sk dk .
Methods to choose the descent direction dk include gradient
methods, where dk is simply −∇Φ(wk ), Newton methods
for which dk = −[D2 Φ(wk )]−1 ∇Φ(wk )) and quasi-Newton
methods. In quasi-Newton methods D2 Φ(wk ) is not computed exactly, but estimated from previous evaluations of the
gradients. We chose the widely used low-storage version of
the BFGS scheme [Fle87], implemented in C in libLBFGS.
The timestep sk is determined by a search along the line
starting from wk with direction dk . Here again, the literature
is very vast, as there is a trade-off between finding a good
step size (the best choice would be to minimize the function
s → Φ(wk + sdk )) and requiring as few functions evaluations

1590

Q. Mérigot / A Multiscale Approach to Optimal Transport

as possible — recall that in our case a function evaluation
requires the construction of a complete Power diagram!
Figure 2.(a) shows that gradient descent methods are outperformed by quasi-Newton ones, regardless of the choice
of line search. It also shows that the choice of line search
method is not as important — barring the fixed-step scheme.
For all remaining experiments, we use the low-storage BFGS
method with Moré-Thuente line search [MT94].

source/target
1
2 U / U10000
1
4 U / U10000
1
8 U / U10000
U / L10000

original
577s
1180s
1844s
216s

multiscale
143s
189s
241s
52s

speedup
4.0
6.2
7.6
4.1

Table 1: Running time of the original and multiscale algorithm to find a weight vector such that ∇Φ(w) ∞ < 10−6 .
5. Results

4.2. Decomposition of the target measure
Suppose for now that the measure ν is discrete; we will explain in the next paragraph how to convert an image to such
a measure. From this measure, we construct a sequence of
discrete probability measures (ν ), with
ν =

∑

λ p, δ p

p∈S

such that ν0 = ν, and that the number of points of the support of ν decreases as increases. The parameters of our
algorithm are the number L of levels in the decomposition,
and for each level , the number of points n( ) in the support of the measure ν . In practice, we found that choosing
n( ) = n(0)/k with k = 5 usually provides good results.

Lloyd’s algorithm. Theorem 3 suggests that if we want to be
able to construct a good initial weight vector for the problem
(µ, ν ) from a weight vector adapted to (µ, ν +1 ) we need
to have ν +1 as close as possible to ν in the Wasserstein
sense. Given the constraints that ν +1 is supported on n( + 1)
points, this means
ν

+1

∈ arg min{Wass2 (ν¯ , ν ); | spt(ν¯ )| ≤ n( + 1)}.

This minimization problem is equivalent to a weighted kmeans problem, with k = n( + 1). Since it is hopeless to
solve this problem exactly, we use the standard Lloyd’s iterative algorithm to find a good local minimum.
We initialize the algorithm using a random sample S0+1
of n( + 1) points drawn independently from ν . We then
apply Lloyd’s descent step to Sn+1 to obtain Sn+1
+1 , stopping
when the points do not move more than a given threshold
between two successive steps. This procedure provides us
with the support S +1 of our measure. We define π to be the
application which maps a point p in S to its nearest neighbor
in S +1 . The values of (λ p, +1 ) p∈S +1 are defined by Eq. (5).

Initial quantization of the target measure. Often, the target
measure is not a discrete measure but a measure νI with
density σ : Ω → R (such as a grayscale image). In this case
we apply Lloyd’s algorithm to the measure ρ in order to
obtain an initial quantization ν = ∑ p∈S λ p δ p of the original
measure νI with a prescribed number of points N.

We will use the following datasets in our experiments. We
denote by λU the uniform probability measure on the square
λS = [0, λ512] × [0, λ512]. For λ = 1 we will simply write
U and S. By L, we denote is the standard grayscale picture
of Lena on the square S. Given a measure with density D,
we will denote by DN a quantization of this measure with
N points, obtained using Lloyd’s algorithm. The decomposition of measures we work with are all obtained with the
same parameters: 5 levels in the decomposition (including the
original one), and level being made of N/5 Dirac masses.
5.1. Comparisons with the original approach
In Figure 2(b) and 2(c) we show the evolution of the . ∞
distance between the weight vector obtained at a given time,
and the optimal one wsol . This optimal weight vector had
been previously obtained by running the algorithm with a
target accuracy of ∇Φ(w) ∞ < 10−9 .
The advantage of our multiscale method over the original
convex optimization is especially important when the source
and target measure are far from each other. Table 1 compares
the running time of the original and multiresolution algorithms to compute a weight vector adapted to the problem
of optimally transporting λU to U1000 with a given accuracy
∇Φ(w) ∞ < ε. The speedup increases as λ goes to zero,
i.e. as the measure λU becomes more concentrated around
the lower-left corner of the original square S.
5.2. Computation of Wasserstein distances
We use the approach described in Section 3.3 to obtain lower
and upper bounds on the Wasserstein distance between µ and
ν at every step of the algorithm. Figure 3(b) and 3(c) compare
the evolution of these two bounds as a function of the runtime
of the original and the multiscale algorithm.
5.3. Displacement interpolation of images
The concept of displacement interpolation of two probability measures was introduced in [McC97]. It uses optimal
transport maps as a replacement for the linear interpolation
µt = (1 − t)µ + tν. Displacement interpolation can be a useful tool for the interpolation of grayscale image, when the
gray value of a pixel can be interpreted as a density of some
quantity (e.g. satellite views of clouds, preprocessed so that
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1591

wk − wsol

1
0.1
0.01

(a) Gradient
(U/L3000 ).

descent

vs

t=60
1000
10
0.1
0.001

quasi-Newton

at time t

t=30

t=0

t=20

t=40
1000
100
10
1
0.1
0.01

∞

10

t=0

wk − wsol

100

at time t

400

∞

200

wk − wsol

0

∞

at step k

Q. Mérigot / A Multiscale Approach to Optimal Transport

(c) Single vs. multi-scale ( 12 U/L3000 )

(b) Single vs. multi-scale (U/L3000 )

100

200
100%
90%
80%
70%

(a) Number of active sites (U/L3000 )

0s

10s

20s

150
100
50
0

Wass2 (µ, ν) at time t

0

Wass2 (µ, ν) at time t

% active sites at step k

Figure 2: Speed of convergence, measured by the L∞ distance between the weight vector at a given time/step and the optimal
one. (a) Comparison of simple convex optimization algorithms: gradient descent (red) with fixed step (solid) or strong Wolfe line
search (dashed), and low-storage BFGS algorithm (blue) with strong Wolfe (solid) or Moré-Thuente line-search (dashed). (b)
and (c) Comparison between the original algorithm of [AHA98] (red) and the multiscale one (blue).
0s

10s

20s

30s
400

200

0

(b) Bounds on Wasserstein distance (U/L3000 ) (c) Bounds on Wasserstein distance ( 12 U/L3000 )

Figure 3: (a) Percentage of points in the support of the target measure whose Power cell intersects the support of the source
measure during the execution of various convex optimization algorithms (colors are the same as in Fig. 2(a)). (b) and (c)
Estimation of Wasserstein distance: in red (resp. blue), the lower and upper bounds obtained by the multiscale (resp. original)
algorithm as a function of time, and in green the correct value.
the gray level ranges from black to white depending on the
thickness of the cloud). We make use of the transport map
computed using the multiscale algorithm. Recall that in order to apply this algorithm to a target measure with density
σ : Ω → R, we had to compute a first quantization of σ,
ν = ∑ p∈S λ p δ p using Lloyd’s algorithm. By construction of
ν, and by definition of the optimal weight vector ω, one has
for every point p in S
σ(x)dx = λ p =
VorS (p)∩Ω

ρ(x)dx.
VorS,w (p)∩Ω

This suggests a way to construct an interpolation between σ
and ρ. Given a time t, consider the weight vector wt = tw, and
the corresponding Power diagram (VorS,wt ). Now, we define
the interpolant ρt at time t as the only piecewise-constant
function ρt on Ω obtained by spreading the mass of λ p on
the intersection of the cell VorS,wt (p) with Ω, i.e. for every
point x in VorS,wt (p), define ρt (x) := λ p / area(VorS,wt (p)).
An example of this interpolation is presented in Figure 4.
6. Discussion
In this paper we have presented a simple way to increase the
efficiency of the convex optimization algorithm introduced
in [AHA98] to solve the optimal transport problem. We also
discussed how our multiscale approach can be used to obtain
fast estimation of Wasserstein distances between images.
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

This first step suggests that, in order to obtain faster computations of optimal transport, one has to better understand
the geometry of the function Φ. For instance, it is currently
not possible to obtain complexity estimates for this approach
because: (i) nothing is known about the shape and size of
the basin around the minimizer where Newton’s method has
quadratic convergence and (ii) the stability result (Theorem 3)
is not quantitative. Understanding these two problems could
open the way to even more efficient computations of optimal
transport maps.

We also believe that this multiscale approach can be useful
in the solution of more geometric problems with a similar
structure. An example of such a problem is Minkowski’s
problem: given a set of normals n1 , . . . , nN and a set of areas
λ1 , . . . , λN such that ∑i λi ni vanishes, find a convex polytope
whose facets normals are among the (ni ), and such that the
facet corresponding to ni has an area of exactly λi . This
problem has a similar multiscale structure as optimal transport, and can be also solved by minimizing a convex functional [LRO06], and would probably benefit from a multiscale
approach. A second example is the problem of designing a
reflector antenna with prescribed image measure at infinity,
which can also be formally cast as an optimal transport problem (Section 4.2.5 in [Oli03]).

1592

Q. Mérigot / A Multiscale Approach to Optimal Transport

Figure 4: First and second rows: An interpolation between a picture of G. Monge and photograph of B. Riemann (with N =625
and 15k respectively). The intermediary steps are obtained using McCann’s displacement interpolation [McC97] of the two
corresponding measures, which can be computed from the L2 optimal transport.
Acknowledgements. ANR grant GIGA ANR-09-BLAN0331-01 and Université Grenoble I MSTIC grant GEONOR.

[LD11] L IPMAN Y., DAUBECHIES I.: Conformal Wasserstein
distances: comparing surfaces in polynomial time. Advances in
Mathematics (2011). 1

References

[LR05] L OEPER G., R APETTI F.: Numerical solution of the
Monge-Ampère equation by a Newton’s algorithm. Comptes
Rendus Mathematique 340, 4 (2005), 319–324. 1

[AHA98] AURENHAMMER F., H OFFMANN F., A RONOV B.:
Minkowski-type theorems and least-squares clustering. Algorithmica 20, 1 (1998), 61–76. 2, 3, 4, 7, 9
[AHT03] A NGENENT S., H AKER S., TANNENBAUM A.: Minimizing flows for the monge-kantorovich problem. SIAM journal
on mathematical analysis 35, 1 (2003), 61–97. 1
[BB00] B ENAMOU J., B RENIER Y.: A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem.
Numerische Mathematik 84, 3 (2000), 375–393. 1
[Ber88] B ERTSEKAS D.: The auction algorithm: A distributed
relaxation method for the assignment problem. Annals of Operations Research 14, 1 (1988), 105–123. 1
[Bos10] B OSC D.: Numerical approximation of optimal transport
maps. Preprint, 2010. 2, 6
[Bre91] B RENIER Y.: Polar factorization and monotone rearrangement of vector-valued functions. Communications on pure and
applied mathematics 44, 4 (1991), 375–417. 3
[BSD09] BALZER M., S CHLÖMER T., D EUSSEN O.: Capacityconstrained point distributions: a variant of Lloyd’s method. ACM
Trans. Graph. 28 (2009), 86:1–86:8. 1
[CCSM10] C HAZAL F., C OHEN -S TEINER D., M ÉRIGOT Q.: Geometric inference for probability measures. Foundation of Computational Mathematics (2010). to appear. 1
[cga] C GAL, Computational Geometry Algorithms Library.
http://www.cgal.org. 7
[dGCSAD11] DE G OES F., C OHEN -S TEINER D., A LLIEZ P.,
D ESBRUN . M.: An optimal transport approach to robust reconstruction and simplification of 2D shapes. Preprint, 2011. 1
[DSS10] D ELON J., S ALOMON J., S OBOLEVSKI A.: Fast Transport Optimization for Monge Costs on the Circle. SIAM Journal
on Applied Mathematics 70, 7 (2010), 2239–2258. 1
[Fle87] F LETCHER R.: Practical methods of optimization. John
Wiley & Sons, 1987. 4, 7

[LRO06] L ACHAND -ROBERT T., O UDET É.: Minimizing within
convex bodies using a convex hull method. SIAM Journal on
Optimization 16, 2 (2006), 368–379. 9
[McC97] M C C ANN R.: A Convexity Principle for Interacting
Gases. Advances in Mathematics 128, 1 (1997), 153–179. 8, 10
[MDGD∗ 10] M ULLEN P., D E G OES F., D ESBRUN M., C OHEN S TEINER D., A LLIEZ P.: Signing the Unsigned: Robust Surface
Reconstruction from Raw Pointsets. Computer Graphics Forum
29, 5 (2010), 1733–1741. 1
[MMD11] M ULLEN P., M EMARI D E G OES F., D ESBRUN M.:
Hodge-Optimized Triangulations. In Proceedings of ACM SIGGRAPH 2011 (2011). 1
[MMdCTAD09] M ACHADO M ANHÃES DE C ASTRO P.,
T OURNOIS J., A LLIEZ P., D EVILLERS O.: Filtering relocations
on a Delaunay triangulation. In Computer Graphics Forum
(2009), vol. 28, Wiley Online Library, pp. 1465–1474. 7
[Mon81] M ONGE G.: Mémoire sur la théorie des déblais et de
remblais. In Histoire de l’Académie Royale des Sciences de Paris,
avec les Mémoires de Mathématique etde Physique pour la même
année (1781), pp. 666–704. 1
[MT94] M ORÉ J., T HUENTE D.: Line search algorithms with
guaranteed sufficient decrease. ACM Transactions on Mathematical Software (TOMS) 20, 3 (1994), 286–307. 8
[Oli03] O LIKER V.: Mathematical aspects of design of beam
shaping surfaces in geometrical optics. In Trends in Nonlinear
Analysis. Springer Verlag, 2003, p. 193. 9
[RTG00] RUBNER Y., T OMASI C., G UIBAS L.: The earth mover’s
distance as a metric for image retrieval. International Journal of
Computer Vision 40, 2 (2000), 99–121. 1, 2
[Vil09] V ILLANI C.: Optimal transport: old and new. Springer
Verlag, 2009. 1, 3, 6

c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

