DOI: 10.1111/j.1467-8659.2011.01854.x

COMPUTER GRAPHICS

forum

Volume 30 (2011), number 6 pp. 1667–1680

Efficient Depth-of-Field Rendering with Adaptive Sampling
and Multiscale Reconstruction
Jiating Chen1,2,3,4 , Bin Wang1,3,4 , Yuxiang Wang1,2,3,4 , Ryan S. Overbeck5 , Jun-Hai Yong1,3,4 and Wenping Wang6

1 School of Software, Tsinghua University, Beijing, China
of Computer Science and Technology, Tsinghua University, Beijing, China
3 Key Laboratory for Information System Security, Ministry of Education of China, Beijing, China
4 Tsinghua National Laboratory for Information Science and Technology, Beijing, China
5 Columbia University, USA
6 The University of Hong Kong, Hong Kong
{chenjt04,roverbeck,jdit89}@gmail.com, {wangbins,yongjh}@tsinghua.edu.cn, wenping@cs.hku.hk
2 Department

Abstract
Depth-of-field is one of the most crucial rendering effects for synthesizing photorealistic images. Unfortunately,
this effect is also extremely costly. It can take hundreds to thousands of samples to achieve noise-free results
using Monte Carlo integration. This paper introduces an efficient adaptive depth-of-field rendering algorithm that
achieves noise-free results using significantly fewer samples. Our algorithm consists of two main phases: adaptive
sampling and image reconstruction. In the adaptive sampling phase, the adaptive sample density is determined
by a ‘blur-size’ map and ‘pixel-variance’ map computed in the initialization. In the image reconstruction phase,
based on the blur-size map, we use a novel multiscale reconstruction filter to dramatically reduce the noise in the
defocused areas where the sampled radiance has high variance. Because of the efficiency of this new filter, only
a few samples are required. With the combination of the adaptive sampler and the multiscale filter, our algorithm
renders near-reference quality depth-of-field images with significantly fewer samples than previous techniques.
Keywords: depth-of-field rendering, adaptive sampling, multiscale reconstruction
ACM CCS: I.3.3 [Computer Graphics]: Picture/Image Generation;
Dimensional Graphics and Realism

1. Introduction
Depth-of-field is an important tool in photography, especially
for portraiture. Professional photographers adjust the camera
lens to draw the viewer’s attention to specific visual details
by bringing some elements into sharp focus while blurring
others. This technique often produces impressive results for
both still images and films.
Over the past few decades, it has become possible to simulate this effect in computer generated imagery as well. Monte
Carlo integration is generally considered the most robust
method to compute images with depth-of-field. However, it
can take an inordinate amount of time because Monte Carlo
c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics
Association and Blackwell Publishing Ltd. Published by
Blackwell Publishing, 9600 Garsington Road, Oxford OX4
2DQ, UK and 350 Main Street, Malden, MA 02148, USA.

I.3.7 [Computer Graphics]: Three-

often requires hundreds to thousands of samples to converge
to a noise-free result. Fortunately, the defocused areas which
are difficult for Monte Carlo are smooth in the image space.
Several adaptive methods have been proposed to leverage this
smoothness. However, even with these methods, the number
of samples required to achieve converged results still remains
prohibitively large for some applications.
This paper presents an efficient adaptive Monte Carlo approach for depth-of-field rendering that achieves noise-free
results with significantly fewer samples than previous approaches. One of the main contributions is a novel multiscale
filter for image reconstruction, as illustrated in Figure 1. It is
a multiscale smoothing filter, where each scale corresponds

1667

1668

J. Chen et al. / Efficient Depth-of-Field Rendering with Adaptive Sampling and Multiscale Reconstruction

Scale 4

Scale 4

Scale 1

Scale 3

Scale 1

Scale 2

(a) noisy image

Scale 3
(b) the filter’s representation

Scale 2

(c) reconstruction result

Figure 1: Efficient image reconstruction using the multiscale filter. For a noisy image (left), the filter can dramatically reduce
the noise, achieving a nearly noise-free result (right). Filters of different scales are applied to different image areas based on
the amount of blurriness (middle).
to repeated applications of a two-dimensional (2D) smoothing filter. It means that each successive scale uses a filter with
2 × the support of the preceding scale. When applied to a
noisy depth-of-field image (Figure 1a), it allows us to apply a
larger filter to the more out-of-focus regions (the yellow and
blue regions in Figure 1b), and a smaller filter to the in-focus
regions (the red and green regions in Figure 1b). As shown
in Figure 1(c), the new filter dramatically reduces the noise
in the defocused areas with high variance.
To produce better results and further reduce sample counts,
we also adaptively sample the image, targeting at the problematic image areas for the filter. We first calculate the image
sample density to adaptively assign samples. Then we optimize the sample locations in both image space and lens
space, further reducing the variance of the pixel radiance.
As expected, experimental results show that our algorithm
allows us to render noise-free depth-of-field images with far
fewer samples than previous approaches.

2. Related Work
2.1. Image-based versus Monte Carlo methods
Recent years have witnessed important progress in the techniques that efficiently simulate the depth-of-field effect. In
general, they can be classified in two categories: imagebased rendering [PC81, KMH95, BHK*03, KS07, ZCP07]
and Monte Carlo sampling-based synthesis [Mit87, VG97,
BM98, BWG03, WABG06, RMB07]. The former is more
efficient, whereas the latter targets at producing better results. Our method can be viewed as a combination of the

two. We seek to combine the efficiency of the imagebased methods with the high quality of the Monte Carlo
methods.
The image-based methods rely on the thin-lens model
[PC81, CPC84]. They often first compute a pinhole image
and its depth map, and then postprocess the image using a
dynamic filter which varies spatially based on the computed
depth map. For example, the recent image-based rendering
method by Lee et al. [LES09] decomposes the image into
multiple layers based on the scene content. They then filter
each layer separately, using a larger filter for out-of-focus
layers and a smaller filter for the more in-focus layers. Our
multiscale reconstruction filter instead works on an accurate
Monte Carlo sampling-based image rather than blurring layers from a flat pinhole camera image. Therefore we avoid
some of the inherent artefacts of the image-based methods,
such as colour bleeding, sharpened or darkened silhouettes,
and especially incorrect visibility.
High-quality offline synthesis methods focus on adaptively
sampling in the areas of rapid change. There are two methods
commonly used for adaptive sampling. One can either determine a total sample budget up front, based on some measure
of variance [Mit87, SFWG04, RMB07, SSD*09], or proceed iteratively, by repeatedly re-measuring the variance and
re-distributing samples [Whi80, BM98, HJW*08, ODR09].
The former non-iterative approach tends to be more efficient
than the iterative approach, but it often cannot reliably determine suitable sample density for various situations. Our
approach is non-iterative, which uses a sampling strategy
that is adequate for the smoothness requirements of the multiscale filter. Also, this filter allows us to use less samples,

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

J. Chen et al. / Efficient Depth-of-Field Rendering with Adaptive Sampling and Multiscale Reconstruction

1669

and enables our method to converge faster than other Monte
Carlo based approaches.
2.2. More general sampling-based methods
Two recent adaptive sampling methods are capable of producing general combinations of effects: multi-dimensional
adaptive sampling (MDAS) [HJW*08], and adaptive wavelet
rendering (AWR) [ODR09]. Similar to our method, both of
them use an adaptive sampling stage followed by a reconstruction filtering stage to produce high-quality images with
low sample rates. Although these methods apply to effects
besides depth-of-field, we find that by focusing on the depthof-field and using depth information, we achieve the same
quality results using fewer samples than any of these methods.

reference (625 spp)

our method (16 samples/pixel)

our method (16 spp)

AWR (32 spp)

2.3. Frequency-based methods
Egan et al. [ETH*09] developed a novel algorithm for rendering motion-blurred images based on light transport in the
frequency domain [DHS*05]. They use frequency analysis
of several aspects of motion to compute local band-limits and
hence sample rates. Their algorithm measures the velocities
of the samples in each pixel and shears the reconstruction
filter to match the sample motion. Similar to their algorithm,
our algorithm measures the depth of the samples in each pixel
to re-shape the reconstruction filter to match the lens’s focus.
Soler et al. [SSD*09] also built their method on frequency
analysis of light transport [DHS*05], but use it for depthof-field simulation. Their method uses more image space
samples to the focused regions and more lens space samples to the defocused regions in order to adapt to the highfrequency components of light transport. Primary rays can be
reduced to tens to hundreds of samples per pixel on average.
Our method only adaptively samples image space and does
not directly adapt to the signal in lens space. However, our
method can render near-reference quality images with lower
sample rates. Even for complex scenes, our method uses only
about 16 samples per pixel to produce high-quality smoothed
results, as shown in Figures 2–4.

LDS (64 spp)

Figure 2: Ancient Chinese architecture rendering with background focus. In the yellow highlighted region, AWR tends
to lack fine details in the areas where small defocused flowers are over the focused architecture, and LDS has difficulty
in reducing noise in these areas. Our method generates a
near-reference smoothed result at lower sample counts.

reference (625 spp)

our method (16 samples/pixel)

our method (16 spp)

AWR (16 spp)

3. Overview
In this section we will give a brief overview of our algorithm;
more details will be given in Section 4. The flow chart of the
algorithm is shown in Figure 5. It is a three-stage algorithm:
initialization, sampling, and reconstruction, corresponding to
the three rows of Figure 5.
3.1. Initialization
In the initialization step, we first compute a depth map
(Figure 6a) for the scene as if viewed through a pinhole

LDS (32 spp)

Figure 3: Ancient Chinese architecture rendering with foreground focus. According to the sample density in Figure 7, the
local average sample densities of the defocused regions highlighted in yellow are 11 samples/pixel and 33 samples/pixel
for our method and AWR, respectively.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1670

J. Chen et al. / Efficient Depth-of-Field Rendering with Adaptive Sampling and Multiscale Reconstruction

reference (625 spp)

(a) depth map
our method (16 samples/pixel)

(b) blur-size map

(c) chess scene

our method (16 spp)

Figure 6: The depth map and the blur-size map of the chess
scene. In the blur-size map, the green and gray regions denote
the foreground and background respectively. The blur-size
|r|grows with the increase of brightness.
AWR (64 spp)

LDS (64 spp)

Figure 4: The plants scene with high geometric complexity.
Because many of the triangles are smaller than a pixel, the
wavelet reconstruction filter of AWR fails to keep sharp in the
focused regions, while our small scale filter can successfully
reconstruct sharp and antialiased results.

(a) pixel-variance map

(b) our method

(c) AWR

Figure 7: Computing the adaptive sample density for the
scene in Figure 3. Middle: our sample density. Right: AWR’s
sample density. Our sampler assigns more samples to those
boundary areas of each scale filtering region (green), rather
than to the large and continuous defocused areas (yellow).

3.2. Adaptive sampling
depth map

pixel-variance map

blur-size map

adaptive sampling

Initialization

predicting image
sample density

result image
g

pixel means
p

Sampling

Reconstruction

multiscale filter

Figure 5: The overview of our algorithm.

camera, yielding a corresponding blur-size map (Figure 6b),
which defines the amount of blur caused by lens defocus.
Then we trace four rays per pixel for depth-of-field simulation, and use this result to approximate a pixel-variance map
(Figure 7a). For a better understanding of our multiscale reconstruction, we will describe the use of the blur-size map
at the reconstruction stage (Section 4.1). The pixel-variance
map is described in the sampling stage (Section 4.2) as it is
closely related to the sample density.

After the initialization, we perform a Monte Carlo sampling
of the image. We use the three maps (i.e. the depth map,
blur-size map and pixel-variance map) to determine a perpixel sample density (Figure 7b) and trace rays accordingly.
The sample distribution assigns rays adaptively, helping the
filter robustly handle the boundary issues. The output of this
stage is a ‘pixel-mean’ image. This stage is described in more
detail in Section 4.2.

3.3. Reconstruction
Finally, we use the multiscale filter to reconstruct a highquality smoothed image from the ‘pixel-mean’ image. Based
on the ‘blur-size’ map, the filter applies larger smoothing filters to blurrier defocused regions and smaller filters to sharper
focused regions. The process of applying the multiscale filter
to a noisy pixel-mean image is shown in Figure 8.

4. Depth-of-Field Rendering Algorithm
We start by reviewing the basics of using Monte Carlo integration for simulating the depth-of-field effect. Then we
describe our adaptive sampler and multiscale filter.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

J. Chen et al. / Efficient Depth-of-Field Rendering with Adaptive Sampling and Multiscale Reconstruction

1671

mask for Scale 1

mask for Scale 2

mask for Scale 3

Scale 1 filtering

Scale 2 filtering

Scale 3 filtering

input image

result of Scale 1

result of Scale 2

mask for Scale 4

Scale 4 filtering

final result without masking

final result

result of Scale 3

Figure 8: The process of the multiple scales filtering. When image regions reach their optimal smoothness, we should mask
them to stop them from the following scales filtering. The coloured part of the mask image in each scale is the mask regions for
the corresponding scale filtering. Without masking, the focused image regions would become incorrectly smoothed.
Synthesizing depth-of-field effect requires the solution of
a 4D integral at every pixel in the image
L(px , py ) =

px +1

py +1

1

1

f (x, y, u, v) dx dy du dv.
px

py

0

0

(1)

Here L(px , py ) is the final image radiance at pixel p. The
x, y denote the image dimensions, and u, v denote the lens
dimensions. The function f (x, y, u, v) is a 4D function,
which varies over the pixel and over the camera lens.
Usually, the Monte Carlo method is used to estimate this
integral, and a ray tracer is adopted to sample the function
f (x, y, u, v). Using the brute force Monte Carlo method
would require many samples to reduce variance to acceptable levels. So instead, we sparsely sample the integral, and
employ the multiscale filter to reconstruct a smoothed result
over the image dimensions.

benefit from the reconstruction filter with smaller or larger
support. Smaller scale filters work better on the focused regions, removing edge aliasing artefacts while keeping the
focused scene sharp. Larger scale filters are best for the defocused regions, smoothing those areas with high variance.
Although in the boundary areas of each scale filtering region,
incorrect filtering results would be introduced by direct application of the multiscale filter. To achieve accurate rendering
results, we try to reduce the filtering effect in these problematic areas, and assign more samples there in the adaptive
sampling stage, which is discussed in Section 4.2.
In practice, the multiscale filter performs on the image
space from small scale to large scale, as shown in Figure 8.
Taking a noisy image as input, the image is further smoothed
and noise is reduced as each larger scale filter is applied. At
each scale and at each pixel, we decide whether to continue
or stop filtering based on the amount of defocus at that pixel.
We use what we call a blur-size map to make this decision.

4.1. Multiscale reconstruction
In the image reconstruction stage, we propose a multiscale
filter to remove the noise. Particularly, with depth-of-field
rendering, we found that different parts of the image can

4.1.1. Blur-size map
The blur-size map encodes the expected amount of blurriness
at each pixel, which is used to determine the size of the

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1672

J. Chen et al. / Efficient Depth-of-Field Rendering with Adaptive Sampling and Multiscale Reconstruction

Image Plane

Lens Plane

Focal Plane

O
r

Q
u
uf

rl

P

2.3

2.4

2.5

2.3

2.3

2.5

2.2

2.3

2.4

2.2

2.3

2.3

2.1

2.2

2.3

21
2.1

22
2.2

23
2.3

Yellow: 1 |r| < 2;
2 ||r|| < 33.
Green:
G

v
df

Figure 10: Illustration of adjusting the blur-size map. Left:
before adjusting. Right: after adjusting. Each grid cell denotes a pixel. The number in each cell is the pixel’s depth.

rl: the lens radius;
uf, df: the distance of the image plane and the focal plane to the
lens plane respectively;
v: the distance of the scene point P to the lens plane, computed
by ray detecting;
u: the distance between the center of lens plane and the
intersection ppoint of the lens axis and the refraction rayy from P;;
r: the radius of the blur imaging circle for the scene point P.

Figure 9: The thin-lens model.

reconstruction filter for that pixel. The process of how a scene
point gets blurred is depicted in Figure 9. When an out-offocus scene point, P , gets projected through a physical lens,
it gets blurred and projects to a circle on the image plane.
This is called the circle of confusion and its radius r can be
computed using the thin-lens model
⎧
1
1
1
⎪
⎪
+
=
⎪
⎪
u
d
f
f
f
⎪
⎪
⎪
⎨
1
1
1
+ =
⎪
u
v
f
⎪
⎪
⎪
⎪
⎪r
⎪
⎩ = uf − u .
rl
u

Our adjustments to the blur-size map are illustrated in
Figure 10. The number in each cell is the pixel’s depth, and
the colour represents the pixel’s blur-size. For each pixel in
the image (e.g. the pixel in the red rectangle), we compare
its depth against the depth of every other pixel within its
blur-size (the pixels in red circles). If its depth is less than
that of one of its surrounding pixels, then its blur-size and
depth will replace the blur-size and depth in that other pixel.
After adjusting in Figure 10, we have copied the blur-size
and depth from the pixel in the red rectangle, to the pixels
above and to the right of it, because they have larger depth
values.

4.1.2. Multiscale reconstruction filter
(2)

Accordingly, r = ( d1f − v1 )uf rl . When v < df , this equation is still valid, and r < 0 (the case of the foreground scene
point Q). In practice, the camera model is a little more complicated. We compute the radius r using the PBRT [PH04]
camera system in Appendix A.

Once the blur-size map is computed, we use it to guide our
multiscale filter. In the following we first explain how the
filter directly works for image regions with no sudden change
of blur-size, and then describe the complete design of our
filter by considering the boundary issues involving sudden
change of the blur-size or the filter size.
A multiscale filter is a low-pass filter, which transforms
the image one scale at a time
Lk,x,y =

cn1 cn2 Lk−1,x+2k−1 n1 ,y+2k−1 n2 .
n1

Taking occlusion into account: We first initialize
the blur-size map with the radius of the circle of confusion,
|r|, at each image pixel. If there was no occlusion in the scene,
this would be sufficient. However, with occlusion, we must
make sure that objects in the foreground blur over objects in
the background rather than the other way around.
The blur-size map is adjusted in order to account for occlusion. We use Figure 6 to help describe our solution. For an
occluded region, like the one inside the red highlighted region
in Figure 6, there is no occlusion between the two objects
in the pinhole image (Figure 6a). However, the foreground
object blurs over the background object in the synthesized
image (Figure 6c). Therefore, we spread the blur-size of each
pixel in the foreground to the pixels in the background according to the circle of confusion (Figure 6b).

(3)

n2

Here c0,±1,±2,... denotes a group of symmetric low-pass filter
coefficients, and k defines the scale number. Lk,x,y is the
pixel radiance at (x, y) after k scales filtering, and L0 is the
pixel-mean image. For better efficiency, we alternate between
applying the 1D transform on the image rows, and then on
the columns. Based on the experiments, we use at most four
scales for all of the examples in our paper (1 ≤ k ≤ 4).
To obtain the optimal smoothness for different image areas, we need to determine in which scale the defocused areas
should stop further filtering. The small Scale 1 is executed
on the whole image, removing the edge aliasing artefacts
while keeping the focused scene sharp. For the following increasingly larger scales, different areas achieve their optimal
smoothness at different scales. Based on the experiments, we
found that Scale 2 works well in the areas with |r| ≥ 2, and

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1673

J. Chen et al. / Efficient Depth-of-Field Rendering with Adaptive Sampling and Multiscale Reconstruction

ns rapid change in blur-size

nl

0

0

1

15

15

16

16

18

18

c-4

c-3

c-2

c-1

c0

c1

c2

c3

c4

small blur-size area

current pixel

large blur-size area

Case 2

Figure 12: 1D filter transform across the area of rapid
change in blur-size. Each cell denotes a pixel. The number in each cell denotes the blur-size. Each ci denotes the
filter coefficient. If the scale k = 4, the weight is reduced to
1
× wk,x,y .
24

Case 3
Case 1

Figure 11: Three cases of boundary problems. Case (1):
smoothing discontinuity (close-up in green box, left) and fixed
result (close-up in green box, right); Case (2): defocused
background adjacent to focused foreground (close-up in red
box, left) and fixed result (close-up in red box, right); Case
(3): defocused foreground over focused background (closeup in yellow box, left) and fixed result (close-up in yellow
box, right).

|r| ≥ 4 for Scale 3, |r| ≥ 8 for Scale 4, respectively. With these
simple settings, most image areas do achieve their sufficient
smoothness. Some intuitive explanations on the soundness
of the multiscale filter solving the depth-of-field problem are
provided in Appendix B.
The simple form of the above filter works well only inside the filtering region of each scale. We now consider how
to cope with boundary issues if there is a sudden change
of the filter size (which increases by a factor of 2 in every scale step) or the blur-size due to depth discontinuity.
In all these cases, we can achieve improved results by interpolating the results between the successive filter scales
using:
Lk,x,y = wk,x,y × Lk,x,y + (1.0 − wk,x,y ) × Lk−1,x,y .

(4)

Here the weight w k,x,y is set to 1.0 as default, and Lk,x,y
is computed using Equation (3). We shall discuss how to
adjust the weights wk,x,y to reduce the filtering effects in the
following three different cases (Figure 11).
Case 1: The filter size may change abruptly by a factor of
2 even if the blur-size changes only gradually, causing oversmoothing results. Consequently, seams may be introduced
at the transition between different filter scales (the green
highlighted region in Figure 11). To fix this problem, we
linearly reduce the weight in Equation (4) according to the
blur-size as follows:

wk,x,y

⎧
|r | − 2.0
⎪
⎪ x,y
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
|rx,y | − 4.0
⎪
⎪
⎨
2.0
= wk,x,y ×
⎪
⎪
⎪
⎪
⎪
⎪
⎪
|rx,y | − 8.0
⎪
⎪
⎪
⎪
⎪
4.0
⎪
⎪
⎩

if k = 2
and 2 ≤ |rx,y | < 3
if k = 3
and 4 ≤ |rx,y | < 6

(5)

if k = 4
and 8 ≤ |rx,y | < 12.

Case 2: Defocused background is adjacent to focused
foreground (see the red highlighted region in Figure 11).
This problem is illustrated in 1D in Figure 12. At the junction between two very different blur-sizes, the radiance from
the focused foreground (small blur-size) may bleed into the
defocused background (large blur-size), leading to visible
ringing. Therefore, when we start filtering the defocused
background areas, we dampen the filter when it overlays
pixels with focused foreground, according to the distance
between the updated pixel and the focused foreground, and
further reduce it for the larger scale k. Specifically, when k =
3 and the filter passes over a pixel with |r| ≤ 2, or when k =
4 and the filter passes over a pixel with |r| ≤ 4, w k,x,y is set
to
wk,x,y =

wk,x,y × nl
.
(ns + nl ) × k 2

(6)

Here ns and nl are the numbers of the filter coefficients for
the pixels with the small blur-size and pixels with the large
blur-size, respectively.
Case 3: Focused background is adjacent to defocused
foreground (see the yellow highlighted region in Figure 11).
Incorrect filtering results would be introduced if the focused
background gets blurred together with the defocused foreground by the larger filter according to the blur-size map.
Depending on the blur-sizes of the foreground rf , and background rb , this problem normally happens when rf < −3 and
|rb | ≤ 3.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1674

J. Chen et al. / Efficient Depth-of-Field Rendering with Adaptive Sampling and Multiscale Reconstruction

To resolve this problem, for the areas with blur-size r <
−3, we need to count per pixel how many samples hit the
foreground nf and background nb . Whether the samples hit
the foreground or the background is determined by the blursize of the samples, computed using the method in Appendix
A. We reduce the weight more if more samples hit the focused background. Specifically, the weight in Equation (4) is
adjusted to be
wk,x,y =

wk,x,y × nf
.
nb + n f

pixel means

sample density

our method

reference

(7)

With these adjustments to the filter weights in different
cases, our filter is able to satisfactorily accommodate different situations. See Figure 11 for the improved results produced by the filter. When two or all of the above three cases
occur simultaneously, we use the minimum one of the computed weights wk,x,y to reduce the filtering effects. Note that
the noise in these problematic areas will be resolved by our
adaptive sampler, which is presented in the next section.

our method

Figure 13: The region highlighted in the green box is sufficient smoothed before filtering, because more samples are
used. Therefore, although the filtering effect is reduced, our
method can still produce high-quality smoothed results.

4.2. Adaptive sampling
Before image reconstruction, previous adaptive approaches
distribute more samples to the defocused areas with high
variance in the adaptive sampling stage. These areas are often
large and continuous. Because the multiscale filter works
well in these areas, it is not necessary to focus many samples
there using our method (e.g. the yellow highlighted areas in
Figure 7). Instead, our adaptive sampler is designed to focus
more samples to the problematic areas, which are often the
boundary areas of each scale filtering region (pointed out in
Section 4.1), helping the filter robustly handle the boundary
issues as well as the noise. Fortunately, these areas are much
smaller than the whole defocused areas with high variance.
Therefore, only a few samples are required in the adaptive
sampling stage.
The adaptive sample density is determined according to the
pixel-variance map, each of its pixel unit recording the variance of the four initial samples, σ 2 (f ). We use the square of
the contrast metric to estimate the variance used by [Mit87]:
σ 2 (f ) = (Imax − Imin )2 /(Imax + Imin )2 .

Using the depth map and blur-size map, for each pixel
pi we add its weighted variance to the pixels if they are
within pi ’s blur-size, and have greater depths than pi . Then
we average the sum of variance as the sample density for
each pixel
wpi σp2i (f )
dp =

N

.

Finally, we allocate a small number of samples for the
adaptive sampling stage. In most cases, 4–12 samples/pixel
on average are enough for the adaptive sampling stage. The
sample counts for each pixel are computed according to the
sample density. We adopt low discrepancy sampling [PH04]
to optimize the sample locations in both image-space and
lens-space, thus further reducing the variance of the pixelmean image.
After the adaptive sampling stage, the problematic areas of
the pixel-mean image are sufficiently smoothed (Figure 13).
Then we perform the multiscale filter on the pixel-mean image, as described in Section 4.1. In this way, our method
robustly produces near-reference noise-free results.

(8)

Here I max and I min are the maximum and minimum sample
intensity, respectively.

pi

Here p is within pi ’s blur-size, and p’s depth is greater than
that of pi . N is the total count, and wpi is the weight of the
variance σp2i (f ), with wpi = 1 as default. In the problematic
areas, we set wpi = 10, focusing more samples to those areas to achieve high-quality smoothed results. The computed
adaptive sample density is shown in Figure 7(b).

(9)

5. Results and Discussion
For ease of comparison between our method and previous
approaches, all the methods were implemented in the general PBRT system [PH04], and all the results were rendered
on a 2.50 GHz Intel Core 2 Q8300 and 2 GB of RAM using one thread. Experimental results show that our method
works well on full depth-of-field simulation at low sample
counts. We compare our technique to previous methods, especially the state-of-the-art adaptive rendering approaches,
in the following paragraphs.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

J. Chen et al. / Efficient Depth-of-Field Rendering with Adaptive Sampling and Multiscale Reconstruction

5.1. Comparison to sampling-based methods
5.1.1. Comparison to Monte Carlo, Mitchell, and low
discrepancy sampling (LDS)
The basic Monte Carlo is an unbiased method, but requires
many samples to reduce noise. Mitchell [Mit87] uses two
passes of adaptivity to sample in the image space. It is hard
for these two methods to reduce noise to acceptable levels
even using tens to hundreds of samples, whereas our method
can render a noise-free image with only 8 samples/pixel,
as shown in the first row of Figure 14. LDS optimizes the
sample locations to reduce the sample variance. However,
without efficient image reconstruction, it is still a little difficult for LDS to remove noise in the defocused areas, shown
in Figure 2. Because LDS is an effective sampling method to
reduce the sample variance, we adopt it, rather than random
sampling, as our sampler.
5.1.2. Comparison to multidimensional adaptive sampling
(MDAS) and adaptive wavelet rendering (AWR)
MDAS [HJW*08] and AWR [ODR09] are two of the state-ofthe-art adaptive approaches. Their reconstruction filters can
produce high-quality images at low sample counts. When
applied to depth-of-field simulation, our technique is more
efficient than these two approaches. In the second row of
Figure 14, to produce smoothed images with the similar
mean square error (MSE), MDAS and AWR methods use
two times and four times more samples than our method,
respectively. In the bottom two rows of Figure 14, using
roughly the same number of samples, our method produces
high-quality smoothed effect with an MSE that is one third
of that by AWR, and nearly one order of magnitude lower
than that by MDAS. The reconstruction process of MDAS is
rather time-consuming, taking nearly half the total rendering
time. Moreover, MDAS requires to store and maintain the
samples, causing excessive memory consumption. We had
to divide each image into several tiles to meet its memory
requirements in our implementation of MDAS. In the AWR
method, visible artefacts can be easily introduced in the defocused areas due to insufficient samples, as shown in the
bottom two rows of Figure 14.
In the following, we provide several complex scenes
(Figures 2–4) to demonstrate the efficiency of our method.
We compare our method with a uniform sampling LDS and
the AWR (one of the best adaptive methods) using different numbers of samples. Experiments show that our method
converges faster to the reference result as the sample count
increases and produces better smoothed quality than both
LDS and AWR. Using 16 samples/pixel (spp), the MSE of
our result is about 1.0 × 10−4 , which can be regarded as the
near-reference quality, outperforming both AWR and LDS.
Compared with AWR, our method not only effectively removes noise, but also reaches an acceptable low MSE at
low sample counts. Compared with LDS, even with a sim-

1675

−4

ilar MSE 1.0 × 10 using 64 samples/pixel in Figure 2,
there still exists visible noise in the defocused areas, whereas
our method is quite effective in removing the noise. We include the whole rendering results of these complex scenes
in the Supplementary Information to objectively show the
efficiency of our method.
Figure 2 compares our method with both LDS and AWR
in the case of small defocused flowers over the focused architecture, which is quite difficult for depth-of-field simulation. In the difficult areas highlighted in yellow frame, the
AWR’s result lacks fine details of the focused background,
and there still exists visible noise in LDS’s result. In contrast,
our method produces a near-reference smoothed result using
only 16 samples/pixel.
Figure 3 demonstrates the efficiency of using our adaptive sampling scheme. Our sampler uses more samples in
the problematic areas of rapid change in blur-size, rather
than in the whole large and continuous defocused areas with
high variance. Inside the yellow frame in Figure 3 (large
and continuous defocused areas), the local average sample
densities are 11 samples/pixel and 33 samples/pixel for our
method and AWR, respectively. Because of the efficiency
of the multiscale filter, these areas can easily achieve nearreference smoothness at low sample rates using our method.
Although it uses three times more samples than our method in
these areas, AWR still produces some subtle wavelet artefacts
and over-smoothing. In the problematic areas highlighted
with the green frame in Figure 3, because our sampler uses
more samples there (Figure 7), our method can consistently
render near-reference smoothed results, as shown inFigure 15. In comparison, AWR’s result is overly smoothed and
would introduce some wavelet artefacts due to insufficient
sampling.
Figure 4 is a difficult scene for depth-of-field rendering
due to the high geometric complexity. There are over 19
million triangles in the scene, many of which are smaller
than a pixel. In the defocused areas, the large-scale filters
can accurately remove the noise. There is little error in the
defocused areas shown in Figure 16. In the focused areas,
the small scale filter can successfully reconstruct sharp and
antialiased results. Because many triangles are smaller than
a pixel, the sample variance in the focused areas is often as
large as that in the defocused areas. Therefore, AWR tends to
improperly blur these areas according to the sample variance,
failing to keep sharp features in the focused areas, as shown
in Figure 4. Compared to LDS, our small-scale filter is able
to remove edge aliasing artefacts by slightly filtering these
areas instead of using many samples. Of course, without
oversampling, only applying small scale filter to these areas
may not achieve accurate results. Therefore, in our method,
there is still a small amount of error in the focused areas,
as shown in Figure 16. Nevertheless, this error is usually
negligible. Therefore, our result is still very close to the
reference image in these areas.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1676

J. Chen et al. / Efficient Depth-of-Field Rendering with Adaptive Sampling and Multiscale Reconstruction

our method

reference

our method

Monte Carlo

Mitchell

750×1000 pixels

625 samples/pixel
34028 secs.

8 samples/pixel
459 secs.
MSE: 3.05 × 10−5

128 samples/pixel
6149 secs.
MSE: 1.14 × 10−4

65 samples/pixel
3937.7 secs.
MSE: 9.71 × 10−5

our method

reference

our method

MDAS

AWR

1000×875 pixels

625 samples/pixel
33907 secs.

8 samples/pixel
522 secs.
MSE: 1.91 × 10−5

our method

reference

our method

MDAS

AWR

1000×424 pixels

1024 samples/pixel
28434 secs.

8 samples/pixel
220 secs.
MSE: 1.56 × 10−4

16 samples/pixel
1539 secs(recons. 669s).
MSE: 1.26 × 10−3

8 samples/pixel
207 secs.
MSE: 5.71 × 10−4

our method

reference

our method

MDAS

AWR

1000×424 pixels

1024 samples/pixel
40038 secs.

16 samples/pixel
611 secs.
MSE: 1.98 × 10−4

16 samples/pixel
1646 secs(recons. 704s).
MSE: 1.04 × 10−3

16 samples/pixel
592 secs.
MSE: 5.92 × 10−4

16 samples/pixel
3426 secs(recons. 1830s).
MSE: 2.59 × 10−5

32 samples/pixel
1951 secs.
MSE: 3.02 × 10−5

Figure 14: Comparison with previous approaches. In the first row, both Monte Carlo and Mitchell methods have difficulty in
reducing noise to acceptable levels even using tens to hundreds of samples, while our method renders a high-quality smoothed
result using only 8 samples/pixel. In the second row, to produce smoothed images with the similar MSE, MDAS and AWR
methods use two times and four times more samples than our method does, respectively. The rendering time of MDAS is six times
longer than that of our method. In the bottom two rows, using the similar number of samples, our method is able to simulate
high-quality depth-of-field with an MSE that is one third of that by AWR, and is nearly one order of magnitude lower than MDAS.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

J. Chen et al. / Efficient Depth-of-Field Rendering with Adaptive Sampling and Multiscale Reconstruction

reference

our method

AWR

1677

LDS

Figure 15: The green highlighted region in Figure 3. Our
method consistently renders near-reference quality results.

Figure 17: Comparison between our method (top panel) and
a recent image-based approach [KS07] (bottom panel). The
image-based method introduces some rendering artefacts,
such as biased smoothing and incorrect visibility. However,
our method produces a more accurate result.
our method (16 spp)
AWR (64 spp)
MSE: 1.64 × 10−4 MSE: 3.70 × 10−4

LDS (64 spp)
MSE: 3.58 × 10−4

Figure 16: MSE visualizations for the plants scene in Figure 4. Using 4 times less samples, our method produces an
image with an MSE that is 2 times lower than AWR and LDS.

5.2. Comparison to image-based methods
Our method bears certain similarity to the image-based methods, such as filtering in the image space according to the
depth map. However, the image-space filtering solution of the
image-based methods fails in some situations because they
are based on the pinhole images, which often lack visibility
information. Although various improvements have been proposed to improve the quality [ZCP07, KS07, LES09], their
results are still biased, especially for complex scenes. Our
multiscale filter works on the sampling-based pixel-mean images, which can provide a correct estimation for the visibility.
In the difficult image areas, we use more samples, rather than
resorting to filtering, to generate more accurate results. We
compare our method with one of the recent image-based
methods [KS07] in Figure 17.
One of the recent image-based methods [LES10] produces
high-quality depth-of-field effect in real-time. However, our
method is simpler and more general. For example, the method
in [LES10] cannot simulate antialiasing because it is based on
the image-based representation, while our method can easily
simulate antialiasing. Furthermore, our method can simulate
some other effects such as physically based environment
lighting, and can robustly render complex scenes, as shown
in Figures 2–4, which is difficult or even impossible for the
image-based methods.

5.3. Discussion and Limitations
Plenty of tests have demonstrated the efficiency of the multiscale filter in noise removal in the defocused areas with
high variance. However, there is still room for improvement.
We reduce the filtering effect to avoid introducing incorrect filtering results. But visible noise may be retained in
these areas due to the lack of sufficient blurring. Fortunately,
this problem is alleviated by our adaptive sampler, which
complements the filter, as shown in Figure 13. In general,
the multiscale filter helps the adaptive sampler to achieve
smoothed effect at low sample rates, whereas the sampler
helps the filter to robustly eliminate the boundary problems
with a small cost. They are practically inseparable, with one
fixing the problems of the other.
Because the multiscale filter relies on the depth map, our
method may fail to handle specular materials. For example,
when the reflected image is in focus and the reflecting object
is out of focus, our method would incorrectly blur the focused
reflected image. A possible solution to this problem is to
remove the filtering effect in the specular areas, and only use
the sampler to approximate the depth-of-field.
6. Conclusions and Future Work
We have presented an efficient adaptive rendering technique
for simulating the depth-of-field effect. Our technique combines the efficiency of the image-based methods with the
high quality of Monte Carlo methods. The multiscale filter removes noise effectively in the defocused areas with
high variance, whereas the adaptive sampler helps the filter
to robustly handle the boundary issues as well as the noise.
Combining the adaptive sampler and the multiscale filter, our
approach dramatically reduces the sample counts. Our test results show that our method renders near-reference smoothed

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1678

J. Chen et al. / Efficient Depth-of-Field Rendering with Adaptive Sampling and Multiscale Reconstruction

depth-of-field images with significantly fewer samples than
previous approaches.
Our method suggests the possibility of efficiently simulating high-quality depth-of-field effect on the GPU using
Monte Carlo methods for future research. Recent advances
have exploited parallel computation on the GPU for realtime depth-of-field rendering [LES09, LES10, YWY10].
But they are only for image-based methods as far as we
know. Because our method greatly reduces the sample
counts, and each step of our method can be carried out in
parallel, we plan to implement our method on the GPU.
We also plan to incorporate ray packets [WSBW01] into
our method for further performance enhancement. Efficient
or even interactive rendering of depth-of-field effect on
the GPU can then be achieved using our sampling-based
method.

Appendix A: The Blur-Size Computed in PBRT
float PerspectiveCamera::BlurSize(
float x, float y, float depth) const {
// x, y: sample location in image space
Point Pras(x, y, 0);
Point Pcamera;
RasterToCamera(Pras, &Pcamera);
Vector dir = Normalize(
Vector(Pcamera.x, Pcamera.y,
Pcamera.z));
float v0 = 1.f / (FocalDistance
- ClipHither);
float v1 = 1.f / ((depth - ClipHither)
* fabsf(dir.z));
float u0 = fabsf(Pcamera.z);
float u1 = 1.f / (1.f/u0 - (v1 - v0));
float deltaY = fabsf(1.f - u0/u1)
* LensRadius;
Point pc1(Pcamera.x,
Pcamera.y + deltaY,
Pcamera.z);
Point p1 = CameraToRaster(pc1);
if (v0 > v1)
return fabsf(y - p1.y);
else
return -fabsf(y - p1.y);
}

Appendix B: The Soundness of the Basic Multiscale
Filter Solving the Depth-of-Field Problem
The basic multiscale filter is partly inspired by downsampling, illustrated in Figure B1. There is a defocused wall
textured by a complex brick pattern in the image. For simplicity, here we assume that the scene depth is constant. In the
top two rows, for a noisy image rendered with sparse samples

(leftmost, 8 samples/pixel), the noise is gradually reduced as
the image is down-sampled to a coarser scale level. We found
that when the image is down-sampled to the next coarser scale
(1/4), it is equivalent to an image with the same size (1/4)
directly rendered with four times more samples/pixel. Specially, for the coarsest scale image (rightmost), we directly
render another image with the same size (64 × 64) with 512
samples/pixel, which can be regarded as a reference image.
The MSE of the coarsest scale down-sampled image is 2.1
× 10−5 compared to the reference image. That means the
down-sampled result not only becomes smoother, but also
achieves more accurate. The multiscale filter is partly similar
to the down-sampling, applying a low-pass filter to the whole
image. However, to keep the image size unchanged, we do
not discard any image data. In practice, we adopt the same
filter as the down-sampling, the low-pass filter of Daubechies
9/7 [CDF92], performing on different sparse grid domains
for different scales (Equation 3), which is demonstrated as
one of the optimal filters to remove noise in the experiments.
We found that the noise is also gradually reduced as a series
of filters are performed on the image successively, shown in
the bottom row (in some way, the result of Scale 1 Filtering
is approximate to the result of down-sampling to 1/4, and
Scale 2 Filtering is approximate to down-sampling to 1/16,
and so on). The final filtering result is not only smoothed, but
also accurate in comparison with the reference image in our
experiment (rightmost, MSE: 4.2 × 10−5 ).
As known, due to the circle of confusion, the sample of
a defocused scene point p would spread to some certain
image area which is usually larger than the size of a pixel
grid. When down-sampling the image successively, this area
may be reduced to a pixel size. According to the Nyquist
Limits, once this area is reduced to a pixel size during downsampling, we accordingly stop the corresponding following
scales filtering for our multiscale reconstruction. Otherwise,
this area would become overly smoothed (Figure 8). This is
why in our method Scale 2 is required to work on the areas
with blur-size |r| ≥ 2, and |r| ≥ 4 for Scale 3, |r| ≥ 8 for
Scale 4, respectively.
Finally, we use at most four scales for all the examples
in the paper, even for the strongly defocused areas. From
the above analysis, when the image rendered with 8 samples/pixel is down-sampled to 1/128, it means that 1024
samples/pixel are used to directly render an image with the
same size (1/128). The reconstruction result after four scales
filtering can achieve the similar smoothness as the image is
down-sampled to 1/128. In other words, the result after four
scales filtering can achieve the similar smoothness of directly
rendering the image with 1024 samples/pixel, which is usually considered as reference. From the experiments, we also
found that it is difficult to further reduce the MSE for more
than four scales filtering. Instead, it would often introduce a
little overly smoothed and therefore increases the MSE. This
is the main reason why we use at most four scales for all the
examples in this paper.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

J. Chen et al. / Efficient Depth-of-Field Rendering with Adaptive Sampling and Multiscale Reconstruction

1679

Figure B1: The similarity between the down-sampling and the multiscale reconstruction filtering.
Acknowledgements
The authors thank Peng Liu, Guidu Chen and the anonymous reviewers for their valuable comments and suggestions. The research was supported by Chinese 973 Program
(2010CB328001), the National Science Foundation of China
(61035002), the ANR-NSFC(60911130368) and Chinese
863 Program (2010AA186002). Prof. Yong was supported
by Tsinghua University Initiative Scientific Research Program(2009THZ0) and the Fok Ying Tung Education Foundation (111070). The chess and pool scenes are from Hachisuka
et al. [HJW*08], dragons and plants from PBRT[PH04], and
the tree model in Figures 2 and 3 from Evermotion team
(www.evermotion.org).
References
[BHK*03] BARSKY B. A., HORN D. R., KLEIN S. A., PANG
J. A., YU M.: Camera models and optical systems used

in computer graphics. Part II: Image-based techniques.
In Proceedings of ICCSA (Montreal, Canada, 2003), pp.
256–265.
[BM98] BOLIN M. R., MEYER G. W.: A perceptually based
adaptive sampling algorithm. In Computer Graphics (SIGGRAPH’98 Proceedings) (Orlando, USA, 1998), pp.
299–310.
[BWG03] BALA K., WALTER B., GREENBERG D. P.: Combining edges and points for interactive high-quality rendering. ACM Transactions on Graphics 22, 3 (2003), 631–
640.
[CDF92] COHEN A., DAUBECHIES I., FEAUVEAU J.-C.:
Biorthogonal bases of compactly supported wavelets.
Communications on Pure and Applied Mathematics 45,
5 (1992), 485–560.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1680

J. Chen et al. / Efficient Depth-of-Field Rendering with Adaptive Sampling and Multiscale Reconstruction

[CPC84] COOK R. L., PORTER T., CARPENTER L.: Distributed
ray tracing. Computer Graphics (SIGGRAPH’84) 18, 3
(1984), 137–145.
[DHS*05] DURAND F., HOLZSCHUCH N., SOLER C., CHAN
E., SILLION F. X.: A frequency analysis of light transport. ACM Transactions on Graphics 24, 3 (2005), 1115–
1126.
[ETH*09] EGAN K., TSENG Y.-T., HOLZSCHUCH N., DURAND
F., RAMAMOORTHI R.: Frequency analysis and sheared reconstruction for rendering motion blur. ACM Transactions
on Graphics 28, 3 (2009), 1–13.
[HJW*08] HACHISUKA T., JAROSZ W., WEISTROFFER R. P.,
DALE K., HUMPHREYS G., ZWICKER M., JENSEN H. W.: Multidimensional adaptive sampling and reconstruction for
ray tracing. ACM Transactions on Graphics 27, 3 (2008),
1–10.
[KMH95] KOLB C., MITCHELL D., HANRAHAN P.: A realistic
camera model for computer graphics. In Computer Graphics (SIGGRAPH’95 Proceedings) (Los Angeles, USA,
1995), pp. 317–324.
[KS07] KRAUS M., STRENGERT M.: Depth-of-field rendering
by pyramidal image processing. Computer Graphics Forum 26, 3 (2007), 645–654.
[LES09] LEE S., EISEMANN E., SEIDEL H.-P.: Depth-of-Field
Rendering with Multiview Synthesis. ACM Transactions
on Graphics 28, 5 (2009), 1–6.
[LES10] LEE S., EISEMANN E., SEIDEL H.-P.: Real-Time Lens
Blur Effects and Focus Control. ACM Transactions on
Graphics 29, 3 (2010), 1–7.

[PC81] POTMESIL M., CHAKRAVARTY I.: A lens and aperture
camera model for synthetic image generation. In Computer Graphics (SIGGRAPH’81 Proceedings) (Dallas,
USA, 1981), pp. 297–305.
[PH04] PHARR M., HUMPHREYS G.: Physically Based Rendering: From Theory to Implementation. Morgan Kaufmann
Publishers Inc., San Francisco, USA, 2004.
[RMB07] RAMAMOORTHI R., MAHAJAN D., BELHUMEUR P.: A
first-order analysis of lighting, shading, and shadows.
ACM Transactions on Graphics 26, 1 (2007), 2.
[SFWG04] STOKES W. A., FERWERDA J. A., WALTER B.,
GREENBERG D. P.: Perceptual illumination components: a
new approach to efficient, high quality global illumination
rendering. ACM Transactions on Graphics 23, 3 (2004),
742–749.
[SSD*09] SOLER C., SUBR K., DURAND F., HOLZSCHUCH N.,
SILLION F.: Fourier depth of field. ACM Transactions on
Graphics 28, 2 (2009), 1–12.
[VG97] VEACH E., GUIBAS L. J.: Metropolis light transport. In
Computer Graphics (SIGGRAPH’97 Proceedings) (Los
Angeles, USA, 1997), pp. 65–76.
[WABG06] WALTER B., ARBREE A., BALA K., GREENBERG
D. P.: Multidimensional lightcuts. AACM Transactions on
Graphics 25, 3 (2006), 1081–1088.
[Whi80] WHITTED T.: An improved illumination model for
shaded display. Commun. ACM 23, 6 (1980), 343–349.
[WSBW01] WALD I., SLUSALLEK P., BENTHIN C., WAGNER M.:
Interactive rendering with coherent ray tracing. Computer
Graphics Forum 20, 3 (2001), 153–164.

[Mit87] MITCHELL D. P.: Generating antialiased images at
low sampling densities. In Computer Graphics (SIGGRAPH’87 Proceedings) (Anaheim, USA, 1987), pp.
65–72.

[YWY10] YU X., WANG R., YU J.: Real-time depth of field
rendering via dynamic light field generation and filtering.
Computer Graphics Forum 29, 7 (2010), 2099–2107.

[ODR09] OVERBECK R. S., DONNER C., RAMAMOORTHI R.:
Adaptive wavelet rendering. ACM Transactions on Graphics 28, 5 (2009), 1–12.

[ZCP07] ZHOU T., CHEN J. X., PULLEN M.: Accurate depth of
field simulation in real time. Computer Graphics Forum
26, 1 (2007), 15–23.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

