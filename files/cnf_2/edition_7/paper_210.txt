DOI: 10.1111/j.1467-8659.2009.01418.x

COMPUTER GRAPHICS

forum

Volume 28 (2009), number 8 pp. 2104–2116

Semi-Supervised Learning in Reconstructed Manifold Space
for 3D Caricature Generation
Junfa Liu1,5 , Yiqiang Chen1 , Chunyan Miao2 , Jinjing Xie1 , Charles X. Ling 3 , Xingyu Gao1 and Wen Gao4,1
1 Institute

of Computing Technology, Chinese Academy of Sciences, China
of Computer Engineering, Nanyang Technological University, Singapore
3 Department of Computer Science, The University of Western Ontario, Canada
4 Institute of Digital Media, Peking University, China
5 Graduate School, Chinese Academy of Sciences, China

2 School

Abstract
Recently, automatic 3D caricature generation has attracted much attention from both the research community and
the game industry. Machine learning has been proven effective in the automatic generation of caricatures. However,
the lack of 3D caricature samples makes it challenging to train a good model. This paper addresses this problem
by two steps. First, the training set is enlarged by reconstructing 3D caricatures. We reconstruct 3D caricatures
based on some 2D caricature samples with a Principal Component Analysis (PCA)-based method. Secondly,
between the 2D real faces and the enlarged 3D caricatures, a regressive model is learnt by the semi-supervised
manifold regularization (MR) method. We then predict 3D caricatures for 2D real faces with the learnt model.
The experiments show that our novel approach synthesizes the 3D caricature more effectively than traditional
methods. Moreover, our system has been applied successfully in a massive multi-user educational game to provide
human-like avatars.
Keywords:
ACM CCS: Computer Graphics [I.3.5]: Computational Geometry and Object Modelling-Hierarchy and geometric
transformations

1. Introduction
With the development of 3D virtual environment technology, an unprecedented number of applications that employ
3D caricatures have emerged, including virtual communities,
online games, cartoon movies and so on. For instance, people
can use their 3D caricatures as avatars to interact with other
people in the games such as Second Life.
Currently, 3D caricatures are mainly created manually
with the help of specialized software packages such as
MAYA or 3DS MAX. The creation process is usually very
time-consuming, and requires substantial professional skills.
Therefore, finding efficient and automatic method of generating 3D caricatures is becoming an interesting and meaningful
research topic.

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and
Blackwell Publishing Ltd. Published by Blackwell Publishing,
9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main
Street, Malden, MA 02148, USA.

There are generally two types of methods in researches
to generate 3D caricatures. The first type of methods creates 3D caricatures interactively. In the work of Akleman
[AR04], the user first identifies the unique facial features,
then creates an abstract caricature using some disconnected
pieces, which are NURBS or subdivision surfaces supported
by the MAYA software. Once the 3D shape is confirmed to
be a likeness of the person, the user worksout an integrated
subdivision surface to approximate the final 3D caricature.
The second type of methods is capable to automate the generation of 3D caricatures. A system named PICASSO has
been developed in the series of works in [KMK97, FNT∗ 99,
FKF∗ 01, FKF∗ 02], which introduced some approaches for
caricature generation. Under the ‘Average Face’ assumption,
the system exaggerates the differences between the new face

2104

J. Liu et al. / 3D Caricature Generation

and the average face. Convenient algorithms are designed
to locate the facial parts from the photograph. The system
can generate greyscale 2D and 3D caricatures efficiently and
automatically. Shadbolt [Sha03] presented a system which
can synthesize 3D caricature from two facial photographs.
The system first reconstructs the 3D face using two orthogonal photographs, and subsequently deforms the generic head
model according to the difference between the input face
and the average face. Yeong-Kyu Lim et al. [LFK07] implemented a 3D caricature system on handset, which generates
3D caricature based on a single facial photo. The algorithm
is efficient and automatic. However, due to the constraints
such as CPU power and screen sizes, the quality of the output model is limited. It is a weighted combination of some
predefined face forms.
In our opinion even the state-of-art of 3D caricature generation is far from satisfactory. There are two main aspects to
be improved. The first is that some works such as [AR04] are
not automatic enough for practical application. The second
one is that some works such as [KMK97, FNT∗ 99, FKF∗ 01,
FKF∗ 02, Sha03] cannot provide complex caricature styles.
Those algorithms work by exaggerating the differences between the input face and the standard face, and cannot simulate the styles of the caricatures created by artists. In this
paper, we attempt to realize a system which fully automates
caricature generation and provides complex caricature styles
by employing machine learning methods.
In recent years, machine learning has been adopted successfully for 2D caricature generation. It can represent the
caricature styles created by artists very well. Chen et al.
[CXS∗ 01] learnt a flexible sketch model from samples created by an artist. By non-parameteric sampling from that
model, the cartoon face with the specified style can be generated. Liang et al. [LCXS02] proposed a prototype-based
method to capture the artist’s understandings. Such prototypes are learnt by analyzing the correlation between the
image caricature pairs using partial least-squares algorithm.
This method performs well to exaggerate the face to caricature. Shet et al. [SLEC05] used a neural network to learn the
artist’s drawing styles and help to produce personalized caricatures. Liu et al. [LCG06] learnt a mapping model between
real faces and 2D caricatures in PCA eigenspace, and by
this model, input face photographs were transformed into
2D caricatures accordingly. It seems that machine learning is a promising solution for automatic 3D caricature
generation.
However, all machine learning methods require sufficient
number of training samples to ensure quality of the learnt
model. On the other hand, there are very limited 3D caricature samples available, because almost all the artists create
solely 2D caricatures. Although we have created manually a
3D caricature data set with 100 samples, it is obviously insufficient for a successful training. More 3D caricatures are
needed to increase the sample density.

2105

This paper proposes a novel approach for automatic 3D
caricature generation. Our system takes as input 2D facial
photographs, and the output is 3D models. First, a PCAbased reconstruction algorithm for 3D caricature is introduced. The reconstruction serves to enlarge the 3D training
set and ensure the effectiveness of learning. All 3D samples
are reconstructed according to corresponding 2D caricatures,
which are created manually by artists. After that, based on
the extended 3D training set, a regressive model is learnt between 2D real faces and 3D caricatures. In order to learn the
regressive model effectively, we adopt manifold regularization, a semi-supervised learning algorithm which takes advantage of the manifold structure existing in the data. Before
the regressive learning, Locally, Linear Embedding (LLE)
is performed to translate both the 2D real faces and the 3D
caricatures into manifold features.
The rest of this paper is organized as follows. Section 2
describes data collection and processing. Section 3 introduces the algorithm for 3D sample reconstruction in details.
In Section 4, regressive learning framework based on the enlarged training set is demonstrated. In Section 5, we improve
the results by two steps, namely, weighted combination and
texture synthesis. The experiments of 3D caricature generation and their evaluation are presented in Section 6. We
finally draw the conclusions in Section 7.

2. Data Preparation and Processing
In this section, we first introduce data preparation, and then
present the alignment algorithm for 2D faces, which adjusts
all 2D faces to a same scale.

2.1. Data preparation
Our data set contains three parts. Some samples of each
part are shown in Figure 1. Figure 1(a) is a 2D real face,
Figure 1(b) is corresponding 2D caricature and Figure 1(c)
and (d) are different views of the 3D caricature.
Each data set and their relationships are represented in
Figure 2, and are explained as follows:
1. Data A, 2D real faces. We collected 2000 photographs
of human faces. This part of data is used as the training
set for manifold regularization. A 1 is a subset of A,
containing 100 faces.
2. Data B, 2D caricatures. We collected about 1000 artistic
2D caricatures from magazines, professional books and
the Internet. This part of data is used as the prototypes
to reconstruct the 3D caricatures. B 1 is a subset of B,
containing 100 caricatures. A 1 and B 1 are matched as
100 pairs. Each pair consists of a caricature and a real
face of the same person.

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2106

J. Liu et al. / 3D Caricature Generation

used to train the regressive model between real faces
and 3D caricatures.

Let us give a summary to the relationships of three data
sets. A and B are collected manually, while A 1 and B 1 constitute 100 pairs. C consists of two parts. C 1 is modelled
manually according to B 1 and further utilized to build a PCA
subspace. C − C 1 is reconstructed according to B − B 1
based on the PCA subspace. A and C will be used as the pairwise inputs to train the regressive model while C 1 is regarded
as the label of A 1 .

2.2. Feature extraction
The features are extracted from the 2D pictures and 3D caricature models. For the 3D caricatures, we output the vertex
coordinates as the features. This function is supported by a
MAYA plug-in we have developed. There are in total 1337
vertices of the 3D mesh, so the feature of the 3D caricature
is represented as 4011 dimensional vector. All 3D caricature
models have the same number of vertices, edges, and faces.
Therefore, all the feature vectors are in the corresponding
form, which makes the learning method practical.

Figure 1: Three types of samples in the data set. (a) shows
the real face, (b) shows the corresponding 2D caricature,
(c and d) show the 3D caricature model in frontal and side
views.
2D real faces
A
2D caricatures
B

La

C

s

be

Pa
ir

B1

3D caricatures

A1

PCA Space

ls

C1

Figure 2: Relationship among three data sets.
3. Data C, 3D caricatures. This data set comes from two
sources. One is C 1 , which contains 100 3D caricatures
created manually. We took 100 2D caricatures as prototypes and asked skilful modellers to create 3D caricatures with MAYA. The modellers were required to
make those 3D models according to the 2D caricatures
strictly. An artist helped them adjust the shape on the
side and back of the caricatures. C 1 is used to build a
PCA subspace based on which the 2D caricatures in
B − B 1 are transformed into 3D caricatures. These 3D
caricatures constitute C − C 1 . The data set C and A are

For the 2D pictures, we use the points of the facial shapes
as the features. A dedicated tool named ShapeExtracter has
been developed to extract the 2D facial shape efficiently. In
order to adjust the facial points accurately, friendly Graphical
User Interface (GUI) is designed. Moreover, automatic face
detection is provided to initially extract the facial shapes from
the photographs. The automatic face detection is realized by
Active Shape Model (ASM) [CTCG95], which is a wellknown algorithm for shape extraction. For the 2D caricatures,
we processed most of them manually. There is a total of 118
vertices for the 2D mesh resulting in a representation of
the 2D face with a 236 dimensional vector. The extracted
results represented by the red points and lines are shown in
Figure 1(a) and (b).
We refer the extracted feature parameters from facial photograph as the 2D real face, the parameters from a caricature
picture as the 2D caricature, and the parameters of a MAYA
model as the 3D caricature.

2.3. Face alignment
Before using the 2D face parameters, it is necessary to align
all the faces to a uniform scale, for the photographs and
the caricature pictures come from various sources. There
are some methods for aligning the face data, such as affine
transformation [CTCG95] and procrustes analysis [DM98].
We choose the former due to its good performance in our
previous research and its popularity in the computer vision
field.

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

J. Liu et al. / 3D Caricature Generation

2107

The process of alignment comprises of translation, rotation
and scaling. The average face is calculated as the target scale.
All other faces are aligned to it. Let X i = (x i0 , y i0 , x i1 ,
y i1 , . . . , x i(n−1) , y i(n−1) ) represent the original face, and X i is
the target face to be calculated. We first get the average face
as Equation (1).
X=

1
n

n

Xi .

(1)

i=1

Then define the transformation matrix Z and the preprocessing matrix A i for X i
⎡

s cos θ

⎢
⎢ s sin θ
⎢
Z=⎢
⎢ tx
⎣

⎤
⎥
⎥
⎥
⎥
⎥
⎦

(2)

ty
⎡

xi0

⎢
⎢ y
⎢ i0
⎢
⎢ .
Ai = ⎢
⎢ ..
⎢
⎢
⎢ xi(n−1)
⎣
yi(n−1)

−yi0
xi0
..
.
−yi(n−1)
xi(n−1)

1 0

⎤

⎥
0 1⎥
⎥
⎥
.. .. ⎥
⎥
,
. .⎥
⎥
⎥
1 0⎥
⎦
0 1 2n×4

(3)

where, n is the sum number of the samples. t x and t y are the
translations, s is the scaling coefficient, and θ is the rotation
angle of the original face.
The transformation matrix Z can be calculated according
to A i and the average face X
X = Ai Z ⇒ Z = (Ai Ai )−1 Ai X.

(4)

Then compute the alignment result X i for X i
Xi = Ai Z = Ai (Ai Ai )−1 Ai X.

(5)

In Figure 3(a), the facial parameters represented by 118
feature points are extracted from the face photograph. Those
118 feature points can be linked logically as a facial mesh.
Figure 3(b) shows some facial meshes, which are in different
scales. Figure 3(c) is the average facial mesh calculated as
Equation (1). Figure 3(d) shows the new facial meshes after
alignment by taking the average mesh as the target scale. The
same processing is applied to the 2D caricatures. For 3D caricatures, the alignment is ignored, because all the samples are
modelled based on a standard MAYA model. The modellers
just rearrange the vertices of the 3D model to make it like
the 2D caricature prototype, so its scale remains unchanged
from the beginning.

Figure 3: The alignment of the face data: (a) shows the
feature points extracted from the 2D real face, (b) shows
the original meshes in different scales, (c) shows the average mesh, (d) shows the meshes in a uniform scale after
alignment.

3. Enlarging the Training Set
Different caricatures look different for their distinctive types
of eyes, noses, mouths, chins and so forth. Exaggerating
those facial components with different degrees and combining them together can lead to plentiful caricature patterns. To
generate 3D caricatures from 2D faces, it is crucial to predict their caricature patterns. Nevertheless, to our knowledge,
even the artists themselves find it very difficult to precisely
point out a pattern for a face they painted. However, we suppose there is an underlying pattern in the artist’s products,
and it can be retrieved by machine learning methods. Hence,
we need to enlarge the limited 3D caricature data set to make
the learning method effective.
A PCA-based method for 3D caricature reconstruction
is adopted. PCA had been successfully applied in 3D face
reconstruction problems such as [JHY∗ 05, BV99, BAHS06].
In [JHY∗ 05], PCA is adopted to model a 3D face data set,
and further used to reconstruct 3D face for a single frontal
facial photograph. The reconstructed 3D face with multiple
poses improved face recognition rate significantly. In [BV99,
BAHS06], PCA is used to represent the faces in terms of
linear combinations, and output a 3D morphable model. The
morphable model then can be transformed into various types

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2108

J. Liu et al. / 3D Caricature Generation

of faces as the user required. In our work, we use PCA in the
similar way with [JHY∗ 05] to model the 3D caricature data
and do 3D caricature reconstruction.

3.1. PCA of 3D caricatures
Since PCA is a well-known statistical analytic method, we
only explain the crucial parts.
The principal target is to find the component space. Suppose the caricature sample set is {X i }N
i=1 , whose size is N,
and dimension of the element is M. The average is defined
as X = N1 N
i=1 Xi . Each sample differs from the average
by the vector i = Xi − X. Now, we get the covariance maT
trix: C = N1 N
i i . Then, M orthonormal vectors {μ i }
i=1
and variables {λ i } of C are computed, where {μ i } denotes
the eigenvectors and {λ i } the respective eigenvalues, and λ i
represents the importance of vector μ i . The feature space
supported by {μ i } is called PCA subspace. Once the eigenvectors are obtained and the subspace is built, we select some
of the most important vectors (i.e. the principal components).
This results in a dimensional reduction. To preserve a ratio
R of the information, a total of k vectors need to be used,
according to Equation (6).
k

shape can be expressed as
Sf = Sf + Pf α,

(8)

where Sf and Pf are the X, Y parts of the feature vertices on
S and P, respectively. Then, the coefficients α can be derived
from Equation (8) as
α = PfT Pf

−1

PfT Sf − Sf .

(9)

To avoid the outliers, we modify the Equation (9) as

M

λi ≥ R.

λi
i=1

Figure 4: The reconstruction result: (a) is the original 2D
caricature, (b, c) are the frontal and side views of the 3D
result, respectively.

(6)

i=1

To reconstruct 3D caricatures, we perform PCA for 100
3D caricature samples, and build the eigenspace, then map
an input 2D caricature into 3D caricature by sampling in
the eigenspace. The computation is formulated briefly in the
following sections.

3.2. PCA based reconstruction
Since 100 face samples are used to build the PCA subspace
successfully in [JHY∗ 05], in our experiment, 100 3D caricature models are prepared for PCA as well. Let S = (X 1 , Y 1 ,
Z 1 , X 2 , Y 2 , Z 2 , . . . , X n , Y n , Z n )T denote a 3D caricature
model. After PCA, each 3D caricature can be represented
as
S = S + P α,

(7)

α = PfT Pf + λ

−1 −1

PfT Sf − Sf ,

(10)

where = diag(v1 , v2 , . . . , vm ), vi is the ith eigenvalue,
and λ is the weighting factor. According to Equation (10),
taking a 2D caricature as the input, we compute the geometry
coefficient α. By applying α to Equation (7), we obtain the
whole 3D geometry S. Because Equation (10) is an estimation
solver with some information lost, the points’ coordinates of
S f are somewhat different from the coordinates of the corresponding feature points on the reconstructed caricature. To
ensure all the feature points are exactly correct, the coordinates of t feature points of the 3D caricature are forced to
be aligned to that of the feature points of the 2D caricature.
The coordinates of the rest 3D points are computed by interpolating according to these feature points. Figure 4 gives a
reconstructed 3D caricature.
3.3. Training samples selection

where S is a mean vector of the 3D caricature shape; P is
the matrix of the top k eigenvectors (in descending order
according to their eigenvalues); α is the coefficients of the
eigenvectors for sample S, and it is also explained as the
projection of the features on principal components.

Through the reconstruction experiments, we find that some
results are not reasonable and artistic enough according to
subjective evaluation. The features of those models look
over exaggerated in some degree. In order to ensure the
quality of the enlarged training set, those models should be
discarded.

Let Sf = (X1 , Y1 , X2 , Y2 , . . . , Xt , Yt )T denote a 2D shape
containing t feature points. It is assumed that there are t
corresponding points in the target 3D model. Since Sf is
the subvector of S, according to Equation (7), the caricature

Since there is no objective standard to evaluate the 3D
caricatures, we resort to subjective examination of all our
results. Two artists are employed to manually examine the
quality of each reconstructed caricature. Only those deemed

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2109

J. Liu et al. / 3D Caricature Generation

2D faces

2D Face
Manifold

3D Caricatures

MR Training
MR Predicting

2D face

3D Caricature
Manifold

3D Caricature

Figure 5: The framework of training and testing. The routine
with the blue solid lines represents the training procedure.
The routine with the red dotted lines represents the testing
procedure.

of high quality will be added to the training set. Fortunately,
we find that the flawed ones only constitute a small part of
the result set, and most of the results are acceptable. Finally,
about 10 percent of the results is removed from the result set
through strict examination.

4. Regressive Manifold Learning
As mentioned before, various types of caricatures are identified by unique features, such as big eyes, high nose, long
chin and so on. Combination of those features constructs
different caricature patterns. In 3D caricature synthesis, the
primary task is to predict such pattern for the new face.
Since the 3D caricatures are distributed in manifold, we can
get the caricature pattern by reducing the high dimensional
caricatures to a 2D manifold map. Locally Linear Embedding (LLE) [RS00] is an efficient manifold method which
can discover the non-linear structure of high dimensional
data. This paper adopts LLE to get manifold features for
both 2D real faces and 3D caricatures. Based on the manifold features, we designed the method of regressive manifold
learning.
We first introduce the framework of regressive manifold
learning, then in the following sections, give the details of
each routine. The framework is shown in Figure 5. The routine with the blue solid lines is the training procedure while
the routine with the red dotted lines is the testing.
In the training phase, LLE is performed to build each
manifold subspace for both 2D real faces and 3D caricatures.
As partial 2D samples have corresponding 3D caricatures,
it inspired us to perform semi-supervised learning between
two manifolds. The output is the regressive model f (x). In
the testing phase, the input 2D face is first transformed into
2D facial manifold, and then mapped into 3D caricature’s
manifold by prediction of f (x). The final 3D caricature is
obtained by dimension increasing.

Figure 6: Low dimensional manifold of 2D real faces. The
top four faces have transitionary poses. The bottom four faces
have transitionary expressions.
4.1. LLE dimension reduction
We use LLE to discover the local embeddings for both 2D
real faces and 3D caricatures. According to the principle of
LLE, each face can be expressed as the linear combination of
its neighbours. It means that a face can be reconstructed by
a weight matrix W, which describes the distances between
that face and its neighbours. The reconstruction errors are
measured by Equation (11).
⎛
⎞2
⎝Xi −

ε(W ) =
i

Wij Xj ⎠ .

(11)

j

The weight matrix W is unknown. To compute it, we need
to minimize the reconstruction errors. There are two constraints: (1) j Wij = 1 and (2) W ij = 0, if X j is not one
of the k nearest neighbours of X i . Once the weights are obtained, we can begin the procedure of dimension reduction.
We suppose that a face X in the high dimension maps to a
low dimension feature Y which represents the global internal
coordinates on the manifold. The feature Y can be computed
by minimizing the cost function
⎛
⎞2
⎝Yi −

(Y ) =
i

Wij Yj ⎠ .

(12)

j

Here, we fix the weights W ij , and specify the dimension
to d for the vector Y . The minimizing problem can be transformed into an eigenvector problem, which is well posed to
solve in linear algebra.
According to the principle, LLE assumes that the data lies
on a low dimensional geometric structure. Some previous
works such as [RS00] have conducted experiments which
show that human faces are in manifold distribution. We also
make a test for 2D faces and 3D caricatures to see if they meet
this assumption. The results in Figures 6 and 7 show that both
real faces (with 236 dimensional features) and caricatures

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2110

J. Liu et al. / 3D Caricature Generation

Equation (13)
1
l

f ∗ = arg min

f ∈HK

l

V (xi , yi , f ) + γ f

2
K

, (13)

i=1

where f 2K is the norm of the possible solutions f in the
kernel space. This penalizing item is to reflect the smoothness conditions of f ; V is the cost function. Generally, it is
computed by the difference between Y and f (X). X denotes
the 2D real face data and Y is the corresponding 3D caricature pattern, which is represented by the points on the 2D
manifold map.
The classical Representer Theorem states that the solution
to this minimization problem exists in RKHS and can be
written as:
l

f ∗ (x) =
Figure 7: Low dimensional manifold of 3D caricatures. The
bottom four faces have transitionary expressions.

(with 4011 dimensional features) behave regularly as analogous triangles when they are reduced to two dimensions.
Figures 6 and 7 also show some faces at different position
marked on the map with crosses. Those faces appear with
transitionary features such as expression.
In our research, we take the point on the triangle map
in Figure 7 as the training label y, consequently, the point
y(y 1 , y 2 ) is a 2D vector. As a regressive model, standard
MR algorithm is designed to train a single value function
f (x). Therefore, we changed the standard MR algorithm to
make it available for a multidimensional vector regressive
training.

4.2. Regressive manifold regularization
Regularization is a well built mathematical approach for solving ill-posed inverse problems. It is widely used in machine
learning tasks [RS00]. Many popular algorithms such as Support Vector Machine (SVM), splines and Radial Basis Function (RBF) can be broadly interpreted as special cases of
the regularization algorithm. Those algorithms correspond
to the regularization algorithm with different empirical cost
functions and complexity measures. In this paper, we take
manifold regularization [BNS06] as semi-supervised pattern
learning approach to predict the caricature pattern for the
target face.
We give a brief introduction to manifold regularization.
Details can be found in literature [BNS06]. Given a set of data
with labels {X i , Y i }li=1 , the standard regularization [EPP00]
in Reproducing Kernel Hilbert Space (RKHS) is to estimate the function f ∗ by solving the minimization problem in

αi K(xi , x),

(14)

i=1

where K(x i , x) is a kernel matrix. Substituting Equation (14)
in Equation (13), it is transformed to the optimization of α. In
the case of the squared difference function of V (x i , y i , f ) =
(y i − f (x i ))2 , we arrive at the convex differentiable objective
function in Equation (15), and get the final regularized least
squares (RSL) solution in Equation (16)
1
α ∗ = arg min (Y − Kα)T (Y − Kα) + γ α T Kα (15)
l
α ∗ = (K + γ lI )−1 Y .

(16)

Belkin et al. [BNS06] extended the standard framework to
MR, which is described in Equation (17)
f ∗ = arg min

f ∈HK

1
l

l

V (xi , yi , f ) + γA f

2
K

+ γI f

2
I

.

i=1

(17)
The difference between the manifold regularization and
the standard framework lies in two aspects. The first is that
the former incorporates geometric structure of the marginal
distribution P χ in the minimizing, which is reflected by the
third item γ I f 2I . While minimizing, the coefficient γ I
controls the complexity of the function in the intrinsic geometry of P χ , and γ A controls the complexity of the function
in ambient space. Belkin et al. [BNS06] employ Laplacian
manifold to represent the geometric structure embedded in
high dimensional data. In this work, the real faces and the
caricatures are distributed in the manifold structures, so it is
very suitable to apply the extended regularization to train the
regressive model.
The second difference is that manifold regularization imports the unlabelled data {X i }ui=l+1 . While the lost function
is calculated only by the labelled samples as before, when
calculating the third penalizing item f 2I , a large mount of
unlabelled samples are taken into account. Those unlabelled
samples are beneficial to recover the manifold structure. In

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2111

J. Liu et al. / 3D Caricature Generation

fact, there are just 100 labelled samples in our case, so we take
advantage of MR to import many other unlabelled 2D faces,
which can make contribution for discovering the manifold
structure.
Importing the labelled and unlabelled data, the solution
of f is changed as Equation (18) to include the labelled and
unlabelled samples.
l+u

f ∗ (x) =

αi∗ K(xi , x).

(18)

W, it also can be represented linearly by those k neighbours
with weight W in low dimensional manifold space.
Therefore, the reverse mapping f −1 can be described as:
Given Y , searching within the 2D manifold map, we get
total k nearest neighbours Y i of Y , and compute the weights
W i (i = 1, 2, . . . , k) by minimizing the reconstruction errors
according to Equation (11). We can get Y = ki=1 Wi Yi . As
each Y i maps to an original 3D caricature X i in the sample
space, the result X can be constructed by Equation (21)

i=1

k
2
I

T

The third penalizing item f can be represented as f Lf
approximately. L is the Laplacian graph. Subsequently, the
convex solution is changed with a new form as Equation (19),
and the solution of α as Equation (20).
1
α ∗ = arg min (Y − Kα)T (Y − Kα) + γA α T Kα
l
γI
α T KLKα
+
(19)
(l + u)2
γI l
LK)−1 Y ,
(20)
α ∗ = (J K + γA lI +
(l + u)2
where J = diag(1, 1, . . . , 0, 0) with the first l diagonal entries
as 1 and the rest 0. We notice that if there is no unlabelled
sample, which means γ I = 0 and JK = K, the Equation (20)
will be identical to Equation (16).

X =

Wi Xi .

(21)

i=1

From the above equation, we know that the neighbourhood
in terms of neighbours and weights are very important for
3D reconstruction.
5. Result Improvement
Once the preliminary result is obtained, two steps are employed to improve the final caricature. One is to combine the
result with its real 3D face, the other is to generate texture
according to the original facial photograph.
5.1. Weighted combination with real face

The solution of α is l dimensional vector in the standard
form. Whereas, in our research, for the element y i of Y is
a 2D vector (y i1 , y i2 ) which is the coordinates on the 3D
caricatures’ manifold map, α is changed to l× 2D matrix.

In order to improve the similarity of the resulting caricature
with its real 3D face, a weighted combination procedure is
designed as Equation (22).

Once regressive learning of manifold regularization is
completed, we can test the learnt function f ∗ . As shown
in Figure 5, in the test phase, a new 2D real face shape X
in the photograph is extracted by ASM. We suppose its 3D
caricature is X which is needed to be computed. First, we
reduce the dimension of X by f to get a 2D facial manifold
feature Y, then predict its 3D manifold feature Y by f ∗ .
Since the output of Y is low dimensional manifold feature,
we need to increase its dimension to the sample space by
f −1 . This task can be done conveniently taking advantage of
the characteristics of LLE.

X∗ is the final result. X 3DT is a real 3D face reconstructed
by the method in [JHY∗ 05]. λ is an empirical parameter, and
can be tuned manually. Generally, we set it as 0.7.

X∗ = λX + (1 − λ)X3DT .

(22)

We find that by performing Equation (22), the final results
get higher similarity with the original faces than those without weighted combination. In addition, some artifacts in the
resulting caricatures caused by extreme exaggeration of the
local facial feature are eliminated by this operation.
5.2. Texture synthesis

4.3. LLE dimension increasing
Dimension reduction of LLE defines a mapping function f
from high dimensional face to low dimensional manifold
feature, which is f : X → Y . Similarly, we define a reverse
mapping f −1 : Y → X , which transforms the 2D manifold
feature Y to the target 3D caricature X .
As we know, one characteristic of LLE is that the neighbourhood of the samples remains unchanged during the dimension reduction. It means if a sample in high dimensional
space can be represented linearly by k neighbours with weight

To further improve the visualization of the results, we make
a schedule for texture synthesis according to the input 2D
facial photograph.
As we know, a usual way to map the 2D texture to 3D
face is using the UV coordinates. Since all the 3D caricatures have uniform geometric topology in terms of vertices,
triangles and quads, we designed a texture template and a
set of predefined UV coordinates. Figure 8 gives the texture
template and UV example. Each 3D vertex of a 3D model has
corresponding UV coordinates on the texture template. As
the triangle can be represented by three vertices, each triangle

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2112

J. Liu et al. / 3D Caricature Generation

6. Experiments
The experiments consist of three parts. The first part is 3D
caricature reconstruction which is to enlarge the training set.
The second part is 3D caricature generation which is based
on the enlarged training set. Some results are presented and
relative parameters are discussed. The third part is 3D caricature evaluation where we design two experiments to evaluate
the results by objective and subjective manner.

6.1. 3D caricature reconstruction
Figure 8: UV mapping. (a) shows the texture template, and
(b) shows the UV coordinates.

Figure 9: Transformation of texture template for individual
face: (a) shows the extracted face from Figure 3(a), (b) shows
the face which is warped and coped to the facial area in the
texture template, (c) shows the final texture after sampling in
texture subspace.

of the 3D model can correspond to a triangle on the texture
template. Sequentially, a mesh warping algorithm [WOL90]
is employed to map texture from the triangle of the texture
template to the triangle of the 3D model. The same case is
available for a quad of the 3D model. Therefore, the whole
3D model which is represented by triangles and quads can
obtain its texture by the UV mapping procedure.
To reflect the original facial texture of the input face, the
texture template is transformed for individual face. Figure
9 shows an example of texture transformation. First, once
the facial feature points are detected as shown in Figure
3(a), the facial texture is extracted as Figure 9(a). Secondly,
by warping algorithm [WOL90], the extracted texture is
warped and copied to fill the facial area in texture template as
Figure 9(b). Finally, in order to realize seamless and smooth
texture as Figure 9(c), we use a texture sampling method. A
PCA subspace is built based on a predefined texture database,
and then Equation (7), that is S = S + P α, is used for two
times. The first time is to calculate α according to S and P,
where S is the input texture shown in Figure 9(b), and P is the
principal component parameters. The second time of using
Equation (7) is to calculate a new texture S according to P
and α which has just been computed.

When performing PCA for 100 3D caricatures, we set parameter R as 98%, which is large enough to count the contribution
of the principal components. Correspondingly, the number
of the principal components is calculated as 40 according to
Equation (6).
There are totally 1000 2D caricatures in the data set, and
100 of them had been taken as prototypes to manually create the 3D caricatures. Therefore, 900 2D caricatures are
involved in reconstruction. Two artists are asked to further
evaluate the resulting caricatures, and Similarity, Artistry and
Exaggeration are employed as metrics. Finally, 810 reconstructed caricatures are selected successfully into the training
set. The success rate is 90%. One of the reconstructed 3D
models is given in Figure 4.
Generally speaking, a data set with 100 samples is too
sparse to build the manifold space. However, after reconstruction, the new data set with 910 samples gets dense
enough to reflect the manifold, which can be well observed in
Figure 7.

6.2. Model training and 3D caricature generation
Before training, we built the manifold space for both 2D
real faces and 3D caricatures by LLE dimension reduction.
The number of neighbours k should be specified first. We
compute the chart of k versus the LLE reconstruction error,
which is obtained from Equation (11). According to the chart
in Figure 10, we specify k as 6 to keep balance between the
reconstruction error and computational complexity, which
raises dramatically as k increased.
Then, to specify the dimension of manifold for 2D real
faces, we compute the chart of dimension versus reconstruction error in Figure 11. The dimensional reduction is also a
tradeoff between computational complexity and reconstruction error, and we find 56 dimensions for the representation
of the 2D faces to be a good choice.
For 3D caricature, the dimension is reduced to 2 for some
reasons. The first reason is that LLE allows us to retain the
neighbourhood relationships after the reduction of dimensions. Although there is information lost while the dimension
is reduced from 4011 to 2, most parts of the neighbourhood

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

J. Liu et al. / 3D Caricature Generation

2113

need specify some other empirical parameters for MR, such
as γ A (0.02), γ I (0.8), the kernel function (‘Radial Basis
Function’), and so on.
In total, 2000 real face samples are involved as training
set {X}, and 910 3D caricatures are processed as training set
{Y }. One hundred of them that had been made manually are
regarded as labels.

Figure 10: Specifying the number of neighbours according
to the reconstruction error curve.

Figure 12 gives some final results. The first line represents the original 2D faces. The second, third and forth lines
represent respectively the frontal and side snapshots of the
resulting 3D caricatures generated by the trained regressive
function f (x), and the fifth line shows the results generated
by the traditional method. The traditional method adopts the
strategy of exaggerating the differences between the input
face and the average face, which is a common technique in
the literatures, and is employed in, for example, [KMK97,
FNT∗ 99, FKF∗ 01, FKF∗ 02, Sha03]. More specifically, the
principle of the traditional method can be described as
Equation (23).
∗
= X3D + (b − 1)(X3D − X3D ),
X3D

(23)

where X 3D is the input 3D face, X3D is the average face,
X∗3D is the output 3D caricature and b is the coefficient of
exaggeration. When b is equal to 0, the output is the average
face; b is equal to 1, the output is the original face. In the
experiment of this paper, b is assigned as 1.8.
6.3. 3D caricature evaluation

Figure 11: Specifying the dimension of 2D real faces according to the reconstruction error curve.
remain intact. As mentioned in Section 4.2, we care most the
neighbourhood. In our problem, we use the neighbourhood
to generate the 3D caricature by weighted combination as
Equation (21), that is X = ki=1 Wi Xi . As the neighbourhood remains unchanged while performing dimension reduction, we can compute the neighbourhood in low dimension
and further transfer the neighbourhood to high dimension to
calculate the final result 3D caricature. The second reason is
that in the 2D feature space, 3D caricatures still behave well
with manifold characteristics as shown in Figure 7. The third
reason is that a small number of dimension can obviously
reduce the computational complexity of MR.
So the features are reduced from 236(118 × 2) to 56 for
2D real faces, from 4011(1337 × 3) to 2 for 3D caricatures.
They are prepared as the training input for MR. We also

We compared the results of our method to those of the traditional method subjectively. On the whole, our method can
generate globally harmonious caricature, while reflecting
complex features of the faces. For instance, it can be observed from Figure 12 that our method can output various of
chins in (a–c), complex nose in (d) and the convex cheek in
(e).
We also carried out objective evaluation on all the results.
Similarity, Artistry and Exaggeration were employed as three
metrics. Ten persons were invited to evaluate all the results,
and assigned scores to them on a scale of 5. In the evaluation process, the evaluators were kept ignorant about the
method to generate each result. Table 1 shows the average
score of each item. ‘MR’ represents MR-based method in
our research, and ‘Trad.’ represents the traditional method.
The scores in Table 1 show that the MR-based approach
outperforms the traditional approach.
To further verify the similarity of the results, a recognition
experiment was also conducted. We prepared 5 female and
15 male facial photographs and their 3D caricatures generated by our approach. Twenty-three persons who had never
seen the caricatures before were invited to recognize 3D results and assign each one to the original photograph. The
recognition time was recorded. Table 2 shows the success

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2114

J. Liu et al. / 3D Caricature Generation

Figure 12: The results generated by our method and traditional method. The first line shows the original 2D faces. The second,
third and forth lines show the frontal and side snapshots of the result 3D caricatures. The fifth line shows the results of the
traditional method.

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2115

J. Liu et al. / 3D Caricature Generation
Table 1: The evaluation for the results generated by MR-based method and traditional method.

Result1

Result2

Result3

Result4

Result5

Average

Evaluation
metrics

MR

Trad.

MR

Trad.

MR

Trad.

MR

Trad.

MR

Trad.

MR

Trad.

Similarity
Artistry
Exaggeration
Average

4.04
4.15
3.84
4.01

3.77
3.62
3.38
3.59

4.35
4.17
4.38
4.30

4.00
3.69
3.50
3.73

3.73
4.12
3.80
3.88

3.65
3.46
3.54
3.55

4.58
4.27
4.22
4.35

4.08
3.47
3.46
3.67

3.91
4.39
3.94
4.08

3.54
3.69
3.67
3.63

4.12
4.22
4.04
4.13

3.81
3.59
3.51
3.64

Table 2: The average recognition rate and recognition time.

Items
Female average
Male average
Total average

Rec. rate (%)

Rec. time (s)

95.65
92.17
93.04

8.20
11.33
10.55

recognition rate and recognition time. It is interesting that
the female caricatures were recognized more quickly and
more accurately than the male caricatures. Maybe there is
more distinctive features existing in female faces, such as the
sharp chin of the first result in Figure 12.

7. Conclusions and Future Works
This paper proposed a novel approach for automatic 3D caricature generation. Through reconstructing 3D caricatures
by a PCA-based method, the training set is enlarged. It performed effectively when samples are too scarce to train a
good model. The paper also proved both the 2D real faces
and the reconstructed 3D caricatures are lying on a low
dimensional manifold in high dimensional feature spaces.
Moreover, based on the manifold characteristics, the regressive Manifold Regularization performs well to learn a mapping model between 2D real faces and 3D caricatures. It is
through this mapping model that the new 2D real face is transformed into 3D caricature. The experiments and evaluation
show that our method outperforms the traditional method. In
addition, the synthesized 3D caricatures have been applied
successfully in Virtual Singapore River City, an massive multiuser educational game. With caricature avatars, the young
users prefer to spend more time to learn various professional
knowledge in the virtual world.
As this paper is a first attempt to synthesize 3D caricatures
by machine learning, there are a number of issues which
remain open for further researches. As mentioned in Section 2, not all the reconstructed caricatures are good enough
for training. We will investigate 3D caricature reconstruction
task using other non-linear methods instead of PCA, such as
manifold based methods. We will also build more accurate
relationship between 2D and 3D caricatures in manifold sub-

space than in PCA subspace. Moreover, in our experiments,
the similarity always disagrees with the exaggeration of the
result caricatures. Learning local exaggeration model while
keeping similarity as much as possible maybe the potential
solution in future research. In addition, it remains unclear
how artistry and exaggeration would impact human judgment of the results. We will design quantitative measures to
represent artistry and exaggeration to discover the relationship between these metrics and human judgment.

Acknowledgements
We would like to thank Mr. Dong Wang and Wang Hu for
their works to collect and process the caricature data. We
also thank Dr. Zhiqi Shen and Mr. Tao Qin for their helpful
suggestions on our work. This work is supported by National Natural Science Foundation of China (60775027) and
(60575032).
References
[AR04] AKLEMAN E., REISCH J.: Modeling expressive 3d
caricatures. In Proceedings of the SIGGRAPH’04 (2004),
pp. 61–61.
[BAHS06] BLANZ V., ALBRECHT I., HABER J., SEIDEL H.-P.:
Creating face models from vague mental images. Computer Graphics Forum 25, 3 (2006), 645–654.
[BNS06] BELKIN M., NIYOGI P., SINDHWANI V.: Manifold
regularization: a geometric framework for learning from
labeled and unlabeled examples. The Journal of Machine
Learning Research 7, 11 (2006), 2399–2434.
[BV99] BLANZ V., VETTER T.: A morphable model for
the synthesis of 3d-faces. In Proceedings of the SIGGRAPH’99 (1999), pp. 187–194.
[CTCG95] COOTES T. F., TAYLOR C. J., COOPER D., GRAHAM
J.: Active shape models–their training and application.
Computer Vision and Image Understanding 61, 1 (1995),
38–59.
[CXS∗ 01] CHEN H., XU Y., SHUM H., ZHU S., ZHENG N.: Example based facial sketch generation with non-parametric

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2116

J. Liu et al. / 3D Caricature Generation

sampling. In Procedings of the ICCV’01 (2001), vol. 2,
pp. 433–438.

the International Conference on Recent Advances in 3-D
Digital Imaging and Modeling (1997), pp. 283–288.

[DM98] DRYDEN I. L., MARDIA K. V.: Statistical Shape
Analysis. John Wiley & Sons (1998).

[LCG06] LIU J., CHEN Y., GAO W.: Mapping learning in
eigenspace for harmonious caricature generation. In Proceedings of the ACM International Conference on Multimedia (2006), pp. 683–686.

[EPP00] EVGENIOU T., PONTIL M., POGGIO T.: Regularization networks and support vector machines. Advances in
Computational Mathematics 13, 1 (2000), 1–50.
[FKF∗ 01] FUJIWARA T., KOSHIMIZU H., FUJIMURA K., KIHAM
H., NOGUCHI Y., ISHIKAWA N.: 3D modeling system of human face and full 3d facial caricaturing. In Proceedings of
the Seventh International Conference on Virtual Systems
and Multimedia (2001), pp. 625–633.
[FKF∗ 02] FUJIWARA T., KOSHIMIZU H., FUJIMURA K., FUJITA
G., NOGUCHI Y., ISHIKAWA N.: A method for 3d face modeling and caricatured figure generation. In Proceedings of
the ICME’02 (2002), vol. 2, pp. 137–140.
FUJIWARA T., NISHIHARA T., TOMINAGA M., KATO
[FNT∗ 99]
K., MURAKAMI K., KOSHIRNIZU H.: On the detection of feature points of 3d facial image and its application to 3D
facial caricature. In Proceedings of International Conference on 3-D Digital Imaging and Modeling (1999), pp.
490–496.
[JHY∗ 05] JIANG D., HU Y., YAN S., ZHANG L., ZHANG H.,
GAO W.: Efficient 3D reconstruction for face recognition.
Pattern Recognition 38, 6 (2005), 787–798.
[KMK97]
KONDO T., MURAKAMI K., KOSHIMIZU H.: From
coarse to fine correspondence of 3-dfacial images and its
application to 3-d facial caricaturing. In Proceedings of

[LCXS02] LIANG L., CHEN H., XU Y.-Q., SHUM H.-Y.:
Example-based caricature generation with exaggeration.
In IEEE Proceedings of the 10th Pacific Conference on
Computer Graphics and Applications (2002), pp. 386–
393.
[LFK07] LIM Y.-K., FEDOROV A., KIM S.-D.: 3d caricature generation system on the mobile handset using a single photograph. In Proceedings of the ICPPW’07 (2007),
pp. 37–37.
[RS00] ROWEIS S. T., SAUL L. K.: Nonlinear dimensionality reduction by locally linear embedding. Science 290,
5500 (2000), 2323–2326.
[Sha03]
SHADBOLT A.: From 2D Photographs to 3D Caricatures. Department of Computer Science, University of
Sheffield, 2003.
[SLEC05] SHET R. N., LAI K. H., EDIRISINGHE E. A.,
CHUNG P. W.: Use of neural networks in automatic
caricature generation: an approach based on drawing
style capture. In Proceedings IbPRIA’05 (2005), pp.
343–351.
[WOL90]
WOLBERG G.: Digital Image Warping. IEEE
Computer Society Press (1990).

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

