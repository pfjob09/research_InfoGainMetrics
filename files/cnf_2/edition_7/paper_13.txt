DOI: 10.1111/j.1467-8659.2008.01171.x

COMPUTER GRAPHICS

forum

Volume 28 (2009), number 1 pp. 161–171

Exposure Fusion: A Simple and Practical Alternative to High
Dynamic Range Photography
T. Mertens1 , J. Kautz2 and F. Van Reeth1
1 Hasselt

University — EDM transationale Universiteit Limburg, Belgium
tom.mertens@uhasselt.be
2 University College London, UK

Abstract
We propose a technique for fusing a bracketed exposure sequence into a high quality image, without converting
to High dynamic range (HDR) first. Skipping the physically based HDR assembly step simplifies the acquisition
pipeline. This avoids camera response curve calibration and is computationally efficient. It also allows for including
flash images in the sequence. Our technique blends multiple exposures, guided by simple quality measures like
saturation and contrast. This is done in a multiresolution fashion to account for the brightness variation in the
sequence. The resulting image quality is comparable to existing tone mapping operators.
Keywords: high dynamic range photography, image fusion, image pyramids
ACM CCS: I.4.8 [Image Processing]: Scene Analysis – Photometry, Sensor Fusion

Exposure fusion is similar to other image fusion techniques
for depth-of-field extension [OABB85] and photomontage
[ADA∗ 04]. Burt et al. [BHK93] have proposed the idea of
fusing a multi-exposure sequence, but in the context of general image fusion. We introduce a method that can more
easily incorporate desired image qualities, in particular those
that are relevant for combining different exposures.

1. Introduction
Digital cameras have a limited dynamic range, which is lower
than one encounters in the real world. In high dynamic range
(HDR) scenes, a picture will often turn out to be under- or
overexposed. A bracketed exposure sequence [DM97, MP95,
RWPD05] allows for acquiring the full dynamic range, and
can be turned into a single HDR image. Upon display, the
intensities need to be remapped to match the typically low
dynamic range of the display device, through a process called
tone mapping [RWPD05].

Exposure fusion has several advantages. First of all, the
acquisition pipeline is simplified, no in-between HDR image
needs to be computed. Since our technique is not physically based, we do not need to worry about calibration of
the camera response curve, and keeping track of each photograph’s exposure time. We can even add a flash image to
the sequence to enrich the result with additional detail. Our
approach merely relies on simple quality measures, like saturation and contrast, which prove to be very effective. Also,
results can be computed at near-interactive rates, as our technique mostly relies a pyramidal image decomposition. On the
downside, we cannot extend the dynamic range of the original pictures, but instead we directly produce a well-exposed
image for display purposes.

In this paper, we propose to skip the step of computing
a HDR, and immediately fuse the multiple exposures into
a high-quality, low dynamic range image, ready for display
(like a tone-mapped picture). We call this process exposure
fusion; see Figure 2. The idea behind our approach is that we
compute a perceptual quality measure for each pixel in the
multi-exposure sequence, which encodes desirable qualities,
like saturation and contrast. Guided by our quality measures,
we select the ‘good’ pixels from the sequence and combine
them into the final result.

c 2008 The Author
Journal compilation c 2008 The Eurographics Association and
Blackwell Publishing Ltd. Published by Blackwell Publishing,
9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main
Street, Malden, MA 02148, USA.

161

This paper was selected as one of the best from the Pacific
Graphics 2007 program

162

T. Mertens et al. /Exposure Fusion

Figure 1: Exposure fusion is guided by weight maps for each input image. A high weight means that a pixel should appear in
the final image. These weights reflect desired image qualities, such as high contrast and saturation. Image courtesy of Jacques
Joffre.

calibration step can be computed from the input sequence
and their exposure settings.

Figure 2: Demonstration of exposure fusion. A multiexposure sequence is assembled directly into a high-quality
image, without converting to HDR first. No camera-specific
knowledge, such as the response curve, had to be accounted
for. Total processing time was only 3.3 seconds (1 megapixel).
Image courtesy of Jacques Joffre.

2. Related Work
HDR imaging assembles a HDR image from a set of low
dynamic range images that were acquired with a normal
camera [DM97, MP95]. The camera-specific response curve
should be recovered in order to linearize the intensities. This

Most display devices have a limited dynamic range and
cannot directly display HDR images. To this end, tone mapping compresses the dynamic range to fit the dynamic range
of the display device [RWPD05]. Many different tone mapping operators have been suggested with different advantages and disadvantages. Global operators apply a spatially
uniform remapping of intensity to compress the dynamic
range [DMAC03, LRP97, RD05]. Their main advantage is
speed, but sometimes fail to reproduce a pleasing image. Local tone mapping operators apply a spatially varying remapping [DW00, DD02, FLW02, LSA05, RSSF02, TR93], i.e.
the mapping changes for different regions in the image. This
often yields more pleasing images, even though the result
may look unnatural sometimes. The operators employ very
different techniques to compress the dynamic range: from
bilateral filtering [DD02], which decomposes the image into
edge-aware low- and high-frequency components, to compression in the gradient domain [FLW02]. The following
two local operators are related to our method. Reinhard
et al. [RSSF02] compute a multi-scale measure that is related
to contrast and rescales the HDR pixel values accordingly.
This is in a way similar to our measures. However, our measures are solely defined per pixel. The method by Li et al.
[LSA05] uses a pyramidal image decomposition, and attenuates the coefficients at each level to compress the dynamic
range. Our method is also pyramid-based, but it works on the
coefficients of the different exposures instead of those of an
in-between HDR image. Other tone mappers try to mimic the
human visual system, e.g., to simulate temporal adaptation
[PTYG00]. Instead, we aim at creating pleasing images and
try to reproduce as much detail and colour as possible.
Image fusion techniques have been used for many years.
For example, for depth-of-field enhancement [OABB85,

c 2008 The Author
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Mertens et al. /Exposure Fusion

163

Figure 3: We fuse differently exposed images using a Laplacian decomposition of the images and a Gaussian pyramid of the
weight maps, which represent measures such as contrast and saturation. Image courtesy of Jacques Joffre.

Figure 4: Weighted blending. The input sequence is shown
in (a). Naive per-pixel blending (b) yields obvious seams due
to sharp variations in the weight map. Blurring the weight
map using a Gaussian kernel (c) removes the seams, but
introduces halos around edges. Cross-bilateral filtering (d)
is able to avoid some of the halos, but not all. Multiresolution
blending (e) creates the desired result.

Hae94], multimodal imaging [BHK93], and video enhancement [RIY04]. We will use image fusion for creating a highquality image from bracketed exposures. In the early 1990s,
Burt et al. [BHK93] have already proposed to use image fusion in this context. However, our method is more flexible
by incorporating adjustable image measures, such as contrast
and saturation. Goshtasby [Gos05] also proposed a method to
blend multiple exposures, but it cannot deal well with object

Figure 5: Comparison with other pyramid-based fusion
techniques [OABB85, BHK93]. These methods select the
most salient Laplacian pyramid coefficients in the input sequence (a), whereas our technique does blending. The results
(b,c) are too dark, and exhibit colour shifts. Our technique
(e) produces a more faithful result compared to the input
sequence (a). Image courtesy of Jesse Levinson.

boundaries. A more thorough discussion of these techniques
is presented in Section 3.3.
Grundland et al. [GVWD06] cross-dissolve between two
images using a pyramid decomposition [BA83a]. We use a
similar blending strategy, but employ different quality measures.
We demonstrate that our technique can be used as a simple way to fuse flash/no-flash images. Previous techniques
for this are much more elaborate [ED04, ARNL05] and are
specifically designed for this case, whereas our method is
flexible enough to handle that case as well.

c 2008 The Author
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

164

T. Mertens et al. /Exposure Fusion

3.1. Quality Measures
Many images in the stack contain flat, colourless regions
due to under- and overexposure. Such regions should receive
less weight, while interesting areas containing bright colours
and details should be preserved. We will use the following
measures to achieve this:
• Contrast: we apply a Laplacian filter to the greyscale
version of each image, and take the absolute value of the
filter response [MP90]. This yields a simple indicator C
for contrast. It tends to assign a high weight to important
elements such as edges and texture. A similar measure
was used for multi-focus fusion for extended depth-offield [OABB85].

Figure 6: A spurious low-frequency change in brightness
might occur due to the difference in exposure among the input
images. The result (a) appears too bright towards the bottom,
which seems unnatural compared to the input images. One
of the input images is shown in (b) for reference.

Figure 7: Comparison with several tone mapping techniques: closeups from Figure 8. The results produced by our
algorithm exhibit high contrast and good colour reproduction.

3. Exposure Fusion
Exposure fusion computes the desired image by keeping only
the ‘best’ parts in the multi-exposure image sequence. This
process is guided by a set of quality measures, which we
consolidate into a scalar-valued weight map (see Figure 1). It
is useful to think of the input sequence as a stack of images.
The final image is then obtained by collapsing the stack using
weighted blending.
As with previous HDR acquisition approaches [MP95,
DM97], we assume that the images are perfectly aligned,
possibly using a registration algorithm [War03].

• Saturation: As a photograph undergoes a longer exposure, the resulting colours become desaturated and eventually clipped. Saturated colours are desirable and make
the image look vivid. We include a saturation measure S,
which is computed as the standard deviation within the
R, G and B channel, at each pixel.
• Well-exposedness: Looking at just the raw intensities
within a channel, reveals how well a pixel is exposed. We
want to keep intensities that are not near zero (underexposed) or one (overexposed). We weight each intensity
i based on how close it is to 0.5 using a Gauss curve:
2
, where σ equals 0.2 in our implementaexp − (i−0.5)
2σ 2
tion. To account for multiple colour channels, we apply
the Gauss curve to each channel separately, and multiply
the results, yielding the measure E.
For each pixel, we combine the information from the different measures into a scalar weight map using multiplication. We opted for a product over a linear combination, as
we want to enforce all qualities defined by the measures at
once (i.e. like an ‘AND’ selection, as opposed to an ‘OR’
selection, respectively). Similar to weighted terms of a linear
combination, we can control the influence of each measure
using a power function:
Wij ,k = (Cij ,k )ωC × (Sij ,k )ωS × (Eij ,k )ωE
with C, S and E, being contrast, saturation, and wellexposedness, respectively, and corresponding ‘weighting’
exponents ω C , ω s and ω E . The subscript ij, k refers to pixel
(i, j) in the k-th image. If an exponent ω equals 0, the corresponding measure is not taken into account. The final pixel
weight W ij ,k will be used to guide the fusion process, discussed in the next section. See Figure 1 for an example of
weight maps.

3.2. Fusion
We will compute a weighted average along each pixel to
fuse the N images, using weights computed from our quality

c 2008 The Author
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Mertens et al. /Exposure Fusion

165

Figure 8: Comparison with several popular tone mapping techniques. Our algorithm yields image quality that is competitive
with the other results. See Figure 7 for a more detailed inspection.
measures. To obtain a consistent result, we normalize the
values of the N weight maps such that they sum to one at
each pixel (i, j):
−1

N

Wˆ ij ,k =

Wij ,k

Wij ,k.

k =1

The resulting image R can then be obtained by a weighted
blending of the input images:
N

Rij =

Wˆ ij ,k Iij ,k
k=1

(1)

with I k the k-th input image in the sequence. Unfortunately,
just applying Equation (1) produces an unsatisfactory result.
Wherever weights vary quickly, disturbing seams will appear (Figure 4b). This happens because the images we are
combining, contain different absolute intensities due to their
different exposure times. We could avoid sharp weight map
transitions by smoothing the weight map with a Gaussian
filter, but this results in undesirable halos around edges, and
spills information across object boundaries (Figure 4c). An
edge-aware smoothing operation using the cross-bilateral filter seems like a better alternative [PSA∗ 04, ED04]. However,
it is unclear how to define the control image, which would
tell us where the smoothing should be stopped. Using the

c 2008 The Author
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

166

T. Mertens et al. /Exposure Fusion

Figure 9: Combining a flash/no-flash image pair using our technique. Notice how the highlight is removed, while detail and
contrast has been transferred to the face. Images taken from Agrawal et al. [ARNL05].

original greyscale image as control image does not work
well, as demonstrated in Figure 4d. Also, it is hard to find
good parameters for the cross-bilateral filter (i.e. for controlling the spatial and intensity influence).
To address the seam problem, we use a technique inspired
by Burt and Adelson [BA83b]. Their original technique
seamlessly blends two images, guided by an alpha mask,
and works at multiple resolutions using a pyramidal image
decomposition. First, the input images are decomposed into
a Laplacian pyramid, which basically contains band-pass filtered versions at different scales [BA83a]. Blending is then
carried out for each level separately. We adapt the technique
to our case, where we have N images and N normalized
weight maps that act as alpha masks. Let the l-th level in a
Laplacian pyramid decomposition of an image A be defined
as L {A}l , and G {B}l for a Gaussian pyramid of image
B. Then, we blend the coefficients (pixel intensities in the
different pyramid levels) in a similar fashion to Equation (1):
N

L{R}lij =

G{Wˆ }lij ,k L{I }lij ,k.
k=1

Thus, each level l of the resulting Laplacian pyramid is computed as a weighted average of the original Laplacian decompositions for level l, with the l-th level of Gaussian pyramid of
the weight map serving as the weights. Finally, the pyramid
L{R}l is collapsed to obtain R. An overview of our technique
is given in Figure 3.

Multiresolution blending is quite effective at avoiding
seams (Figure 4), because it blends image features instead
of intensities. Since the blending Equation (1) is computed
at each scale separately, sharp transitions in the weight map
can only affect sharp transitions that appear in the original
images (e.g. edges). Conversely, flat regions in the original
images will always have negligible coefficient magnitude,
and are thus not affected by possibly sharp variations in the
weight function, even though the absolute intensities among
the inputs could be different there.
For dealing with colour images, we have found that carrying out exposure fusion in each colour channel separately
produces good results.

3.3. Discussion
Seamless blending is an often-encountered problem in image processing and graphics. We use a multiresolution technique based on image pyramids [BA83b], but other methods
are available as well. In particular, gradient-based blending
[PGB03] has shown to be effective, and it has been applied
to image fusion as well [ADA∗ 04, RIY04]. Gradient methods convert images to gradient fields first, apply the blending
operation, and reconstruct the final image from the resulting
gradients. However, reconstruction requires solving a partial
differential equation, which can be costly for high-resolution

c 2008 The Author
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Mertens et al. /Exposure Fusion

167

Figure 10: Comparison of our quality measures. Exposure fusion is performed with each measure turned on separately. See
Section 4.4 for a discussion. Bottom images courtesy of Jacques Joffre.
images. In addition, gradient-based fusion incurs a scale and
shift ambiguity, and is prone to colour shifting [RIY04].
Tone mapping operators may also cause colour shifts like
oversaturation [LSA05], and possibly reduce contrast (see
Figure 8). Our blending is robust against changes in appearance, as it can be seen as a selection process. Even though
we select based on contrast and saturation, we do not directly
change pixels to meet these constraints.
Our work bears similarity to early work on image fusion,
where the Laplacian (or another) pyramid decomposition
is used as well [OABB85, Toe90, BHK93]. These methods work directly on the coefficients by retaining only those
pyramid coefficients that are most salient. For instance, the
coefficients with the largest magnitude are kept [OABB85]:
L{R}lij = argmax |L{I }lij ,k .|
L{I }lij ,k

Burt and Kolczynski’s exposure fusion technique
[BHK93] is based on the same principle. These approaches
compound all details present in the sequence, but they do

not necessarily produce an appealing result; see Figure 5.
Instead, we blend the pyramid coefficients based on a scalar
weight map, but do not directly process individual coefficients at different levels. Measures like saturation and wellexposedness are hard to evaluate directly from pyramid coefficients. Our technique basically decouples the weighting
from the actual pyramid contents, which enables us to more
easily define quality measures. In fact, any measure that can
be computed per-pixel, or perhaps in a very small neighbourhood, is applicable. Alternatively, we could also compute the measures (and thus the weights) at multiple resolutions in a Gaussian pyramid decomposition of the input
images. However, this would increase the computational cost,
as more pixels need to be weighted, while the benefit remains
unclear.
Goshtasby’s technique [Gos05] selects the optimal exposure on a per-block basis, and smoothly blends between
blocks. Since blocks may overlap with different objects, information will spill over object boundaries, similar to the
artifacts related to Gaussian blurring of the weight map
(Figure 4c).

c 2008 The Author
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

168

T. Mertens et al. /Exposure Fusion

Figure 11: More examples. The input images are shown as thumbnails above each result. Note how each image captures all
details from the input sequence. Bottom right image courtesy of Jesse Levinson.
4. Results
All of our examples were constructed from JPG-encoded
photographs, with unknown gamma crrection and camera
response curve. We used equally weighted quality measures
(ω C = ω S = ω E = 1) in most examples, except where mentioned otherwise.

4.1. Quality
Figures 2 and 1 show a typical bracketed exposure shot: underexposed, normally exposed and overexposed. Every ex-

posure contains relevant information that is not present in the
other exposures. Our technique is able to preserve finescale
details of the buildings, and the warm appearance of the sky.
More results can be found in Figure 11.
In Figures 8 and 7, we compare our result to tone
mapping. A rigorous comparison is hard, due to the operators’ implementation-specific differences and parameter
settings. We therefore limit ourselves to an informal comparison with a few popular tone mappers. Compared to
Durand and Dorsey [DD02] and Reinhard et al. [RSSF02],
our method offers better contrast. Li et al.’s approach
[LSA05] produces quite similar results to ours in terms of

c 2008 The Author
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Mertens et al. /Exposure Fusion
Table 1: Computation times for our technique. We computed results for 12 , 1 and 2 megapixel images. N is the number of images in
the stack. The initialization builds the Laplacian pyramids for each
input image. The update step computes the weight maps, the corresponding Gaussian pyramids and the blending. For small image
sizes (half to one megapixel), the user gets interactive interactive
feedback (about 1 second).
w×h×N
864 × 576 × 3
1227 × 818 × 3
1728 × 1152 × 3
864 × 576 × 7
1227 × 818 × 7
1728 × 1152 × 7

init. (s)

update (s)

total (s)

0.75
1.5
3.0
1.5
3.0
6.0

0.82
1.6
3.2
1.5
3.1
6.0

1.6
3.2
6.2
3.0
6.1
12.0

169

4.3. Including Flash-Exposures
A flash exposure can fill in a lot of detail, but tends to produce
unappealing images, and it may include spurious highlights
and reflections. Recent work on flash photography has introduced techniques for combining flash/no-flash image pairs
[ED04, PSA∗ 04, ARNL05]. Our technique can be used here
as well, as our quality measures are also relevant in this
case. Figure 9 shows how our technique has successfully removed a highlight and filled in details, similar to Agrawal
et al. [ARNL05]. However, it cannot remove flash shadows
[ED04] or unwanted reflections [ARNL05]. We used the contrast measure for these examples (ω C = 1 and ω S = ω E =
0).
4.4. Comparison of Quality Measures

contrast, but it also exhibits slight oversaturation. We had
to tweak the saturation parameter in their implementation to
correct the colours.
The multiresolution blending technique discussed in
Section 3.2 is not without its problems. In Figure 6, our
result contains a spurious low frequency brightness change,
which is not present in the original image set. It is caused by a
highly varying change in brightness among the different exposures. Intuitively speaking, this artifact can be considered
as a very blurred version of the seam problem, illustrated in
Figure 4b. Constructing a higher Laplacian pyramid partially
solves this problem. However, the pyramid height is also
limited by the size of the downsampling/upsampling filter
[BA83a].
Another issue concerns out-of-range artifacts. The pyramid reconstruction does not guarantee that the resulting intensities lie within [0, 1], even if the original intensities were
restricted to this domain. E.g. in Figure 8, the area below the
monitor has become overly dark as the intensities fall below
0. One can simply fix this issue by shifting and scaling the
intensities, at the risk of reducing contrast.
4.2. Performance
Our unoptimised software implementation performs exposure fusion in a matter of seconds; see Table 1. After
building the Laplacian pyramids, our technique can provide
near-interactive feedback (see timings of update step). This
enables a user gain more control over the fusion process,
as he or she can adjust the weighting of quality measures. Additional controls on the input images, such as linear and non-linear intensity remappings are also possible
(like brightness adjustment or gamma curves). This can be
used to give certain exposures more influence. Motivated
by the work of Strengert et al. [SKE06], we expect that
our algorithm could eventually run in real time on graphics
hardware.

Figure 10 shows a comparison of our quality measures. Exposure fusion is performed with either contrast, saturation
or well-exposedness. The desk scene in the first row comes
out better with saturation turned on. Contrast makes the background a bit dark, and well-exposedness darkens the centre of
the monitor, making the result look unnatural. For the house
scene on the next row, saturation and well-exposedness produce vivid colours, which is less so for contrast. Finally, the
last row shows how contrast retains details, which are not
present in the saturation image (e.g. in the water, and the
buildings’ windows). Well-exposedness yields a more surrealistic result.
In general, we found that well-exposedness by itself produces fairly good images. However, in some cases it tends
to create an unnatural appearance, because it always favours
intensities around 0.5. Saturation and contrast does not have
this problem. But then again, the results from those measures are not always as interesting as those produced by
well-exposedness.
5. Conclusion
We proposed a technique for fusing a bracketed exposure sequence into a high-quality image, without converting to HDR
first. Skipping the physically based HDR assembly step simplifies the acquisition pipeline. It avoids camera response
curve calibration, it is computationally efficient, and allows
for including flash images in the sequence. Our technique
blends images in a multi-exposure sequence, guided by simple quality measures like saturation and contrast. This is done
in a multiresolution fashion to account for the brightness variation in the sequence. Quality is comparable to existing tone
mapping operators. Our approach is controlled by only a few
intuitive parameters, which can be updated at near-interactive
rates in our unoptimised implementation.
We would like to investigate different pyramidal image
decompositions, such as wavelets and steerable pyramids.
Also, we would like to include more measures, in particular

c 2008 The Author
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

170

T. Mertens et al. /Exposure Fusion

one that would detect camera noise. An optimised Graphics Processing Unit (GPU) implementation would enable the
user to interactively control the fusion process, but could also
be used to display a multi-exposure video stream [MMC∗ 07]
in real time. Finally, we would like to look into the applicability of our technique to other image fusion tasks, such as
depth-of-field extension [OABB85] and multimodal imaging
[BHK93].
Acknowledgments
Thanks to Jacques Joffre, Jesse Levinson, Min H. Kim and
Agrawal et al. [ARNL05] for sharing their photographs. Part
of the research at Expertise Centre for Digital Media (EDM)
is funded by the European Regional Development Fund.
References
[ADA∗04] AGARWALA A., DONTCHEVA M., AGRAWALA M.,
DRUCKER S. M., COLBURN A., CURLESS B., SALESIN D.
and COHEN M. F.: Interactive digital photomontage. ACM
Trans. Graph 23, 3 (2004), 294–302.
[ARNL05] AGRAWAL A., RASKAR R., NAYAR S. K. and LI Y.:
Removing photography artifacts using gradient projection
and flash-exposure sampling. ACM Trans. Graph. 24, 3
(2005), 828–835.
[BA83a] BURT P. and ADELSON T.: The Laplacian Pyramid
as a Compact Image Code. IEEE Transactions on Communication COM-31 (1983a), 532–540.
[BA83b] BURT P. J. and ADELSON E. H.: A multiresolution
spline with application to image mosaics. ACM Transactions on Graphics 2 (October 1983), 217–236.
[BHK93] BURT P. J., HANNA K. and KOLCZYNSKI R. J.: Enhanced image capture through fusion. In Proceedings of
the Workshop on Augmented Visual Display Research
(December 1993), NASA – Ames Research Center, pp.
207–224.
[DM97] DEBEVEC P. E. and MALIK J.: Recovering high dynamic range radiance maps from photographs. In SIGGRAPH ’97: Proceedings of the 24th annual conference
on Computer graphics and interactive techniques (New
York, NY, USA, 1997), ACM Press/Addison-Wesley Publishing Co., pp. 369–378.
[DW00] DICARLO J. and WANDELL B.: Rendering high dynamic range images. In Proceedings of SPIE (January
2000), vol. 3965.
[DMAC03] DRAGO F., MYSZKOWSKI K., ANNEN T. and CHIBA
N.: Adaptive logarithmic mapping for displaying high
contrast scenes. Computer Graphics Forum 22 (2003),
419–426.

[DD02] DURAND F. and DORSEY J.: Fast bilateral filtering for
the display of high-dynamic-range images. ACM Trans.
Graph 21, 3 (2002), 257–266.
[ED04] EISEMANN E. and DURAND F.: Flash photography
enhancement via intrinsic relighting. In ACM Transactions on Graphics (Proceedings of Siggraph Conference)
(2004), vol. 23, ACM Press.
[FLW02] FATTAL R. and LISCHINSKI D., WERMAN M.: Gradient domain high dynamic range compression. ACM Transactions on Graphics 21, 3 (July 2002), 249–256.
[Gos05] GOSHTASBY A.: Fusion of multi-exposure images.
Image and Vision Computing 23 (2005), 611–618.
[GVWD06] GRUNDLAND M., VOHRA R., WILLIAMS G. P. and
DODGSON N. A.: Cross dissolve without cross fade: Preserving contrast, color and salience in image compositing.
Computer Graphics Forum 25, 3 (September 2006), 577–
586.
[Hae94] HAEBERLI P.: A multifocus method for controlling depth of field. http://www.graficaobscura.
com/depth/index.html, 1994.
[LRP97] LARSON G. W., RUSHMEIER H. E. and PIATKO C. D.:
A visibility matching tone reproduction operator for high
dynamic range scenes. IEEE Trans. Vis. Comput. Graph
3, 4 (1997), 291–306.
[LSA05] LI Y., SHARAN L. and ADELSON E. H.: Compressing and companding high dynamic range images with subband architectures. ACM Transactions on Graphics 24, 3
(August 2005), 836–844.
[MP90] MALIK J. and PERONA P.: Preattentive texture discrimination with early vision mechanism. Journal of the
Optical Society of America 7, 5 (May 1990), 923–932.
[MP95] MANN S. and PICARD R.: Being ‘undigital’ with
digital cameras: Extending dynamic range by combining
differently exposed pictures. In Proceedings of IS&T 46th
annual conference. (May 1995), pp. 422–428.
[MMC∗ 07] MCGUIRE M., MATUSIK W., CHEN B., HUGHES
J. F., PFISTER H. and NAYAR S.: Optical splitting trees for
high-precision monocular imaging. Computer Graphics
and Applications 27, 2 (March–April 2007), 32–42.
[OABB85] OGDEN J. M., ADELSON E. H., BERGEN J. R. and
BURT P. J.: Pyramid-based computer graphics. RCA Engineer 30, 5 (1985).
[PTYG00] PATTANAIK S. N., TUMBLIN J. E., YEE H. and
GREENBERG D. P.: Time-Dependent visual adaptation for
realistic Real-Time image display. In Proceedings of SIGGRAPH 2000 (2000), Computer Graphics Proceedings,
Annual Conference Series, pp. 47–54.

c 2008 The Author
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Mertens et al. /Exposure Fusion

171

[PGB03] P´EREZ P., GANGNET M. and BLAKE A.: Poisson
image editing. In SIGGRAPH ’03: ACM SIGGRAPH 2003
Papers (New York, NY, USA, 2003), ACM Press, pp.
313–318.

[RWPD05] REINHARD E., WARD G., PATTANAIK S. and
DEBEVEC P.: High Dynamic Range Imaging: Acquisition,
Display and Image-Based Lighting. Morgan Kaufmann
Publishers, Los Alamitos, CA, USA, Dec. 2005.

[PSA∗ 04] PETSCHNIGG G., SZELISKI R., AGRAWALA M.,
COHEN M. F., HOPPE H. and TOYAMA K.: Digital photography with flash and no-flash image pairs. ACM Trans.
Graph 23, 3 (2004), 664–672.

[SKE06] STRENGERT M., KRAUS M. and ERTL T.: Pyramid
Methods in GPU-Based Image Processing. In Workshop
on Vision, Modelling, and Visualization VMV ’06 (2006),
pp. 169–176.

[RIY04] RASKAR R., ILIE A. and YU J.: Image fusion for
context enhancement and video surrealism. In Proceedings of the 3rd Symposium on Non-Photorealistic Animation and Rendering (2004), pp. 85–152.

[Toe90] TOET A.: Hierarchical image fusion. Mach. Vision
Appl. 3, 1 (1990), 1–11.

[RD05] REINHARD E. and DEVLIN K.: Dynamic range reduction inspired by photoreceptor physiology. IEEE Trans.
Vis. Comput. Graph 11, 1 (2005), 13–24.

[TR93] TUMBLIN J., RUSHMEIER H. E.: Tone reproduction for realistic images. IEEE Computer Graphics and Applications 13, 6 (November 1993), 42–
48.

[RSSF02] REINHARD E., STARK M., SHIRLEY P. and FERWERDA
J.: Photographic tone reproduction for digital images.
ACM Transactions on Graphics 21, 3 (July 2002), 267–
276.

[War03] WARD G.: Fast, robust image registration for compositing high dynamic range photographcs from handheld exposures. Journal of Graphics Tools: JGT 8, 2
(2003), 17–30.

c 2008 The Author
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

