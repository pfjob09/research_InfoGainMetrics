Pacific Graphics 2009
S. Lee, D. Lischinski, and Y. Yu
(Guest Editors)

Volume 28 (2009), Number 7

Expression Synthesis and Transfer in Parameter Spaces
Hyun Joon Shin and Yunjin Lee
Division of Digital Media, Ajou University, Korea

Abstract
In this paper, we introduce a novel framework that allows users to synthesize the expression of a 3D character
by providing a intuitive set of parametric controls. Assuming that human face movements are formed by a set of
basis actuation, we analyze a set of real expressions to extract this set together with skin deformation due to the
actuation of face. To do this, we first decompose the movement of each marker into a set of distinctive movements.
Independent component analysis technique is then adopted to find a independent set of actuations. Our simple
and efficient skin deformation model are learned to reproduce the realistic movements of facial parts due to the
actuations. In this framework, users can animate characters’ faces by controlling the amount actuation or by
directly manipulating the face geometry. In addition, the proposed method can be applied to expression transfer
which reproduces one character’s expression in another character’s face. Experimental results demonstrate that
our method can produce realistic expression efficiently.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism—Animation I.6.5 [Simulation and Modeling]: Model Development—Modeling methodologies

1. Introduction
In computer animation production, animating facial expressions represents one of the most time-consuming processes
as deformable face geometry involves many degrees of freedom and because it is not easy to define a reasonable underlying structure such as a skeletal structure in character body
animation. Therefore, in many cases, even an experienced
animator typically spends countless hours placing vertices
(or control points) in a realistic and consistent manner.
Assuming that human face movements are formed by a set of
basis actuations, we can define a parameter space as a vector
space in which each basis vector corresponds to a basis actuation of the face. In this parameter space, an expression
can be represented with a vector and the expression corresponding to a given parameter vector can be synthesized
uniquely. Through this parameter space, facial expressions
can be controlled efficiently only by specifying the coefficients of the basis vectors indirectly. However, since it is not
trivial to define basis vectors for facial expressions, a number of implicit and explicit parameterization schemes have
been proposed [EF78, Par82, LTW95, Wat87, WF95, Pla81,
CDB02, SNF05, SSRMF06].
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

In this paper, we introduce a novel parameterization method
that extracts a set of basis actuations automatically from a
set of real human expression data. In our method, we aim
at representing facial expressions in a database with a small
number of and intuitive parameters covering a reasonable
range of human expression. To achieve this, we decompose
the expressions into a set of distinct and independent movements using ICA (independent component analysis). Since
ICA finds a set of statistically independent components automatically and each independent component represents statistically coherent movements, we can represent facial expressions with a small number of independent movements
without user intervention.

ICA can produce a new facial expression as a linear combination of the basis actuations directly with a parameter
set corresponding to basis actuations. However, such linear
representation can sometimes cause unnatural facial expressions because vertices on a face move nonlinearly. To reproduce realistic movements of facial parts due to the actuations, we provide a simple and efficient method to learn
a nonlinear deformation function based on a set of realistic
facial expression data.

1830

H. Shin / Expression Synthesis and Transfer in Parameter Spaces

With these parameterization and deformation methods, we
also offer effective tools to synthesize facial animation and
flexible adaptation to a range of applications. In addition to
parametric control of facial expressions, we provide a direct manipulation capability to produce a realistic expression by specifying a few geometric constraints, particularly
those that involve pinning or dragging of points on the face
surface. Expression transfer is also easily achieved by transferring a source expression to a target model through the parameter space.
This paper makes the following contributions.
• Automatic extraction of basis actuations: Our parameterization method can find intuitive basis actuations of a face
automatically.
• Efficient nonlinear deformation: The proposed deformation method is simple enough to provide efficient direct
manipulation capability while faithfully capturing nonlinearity of facial movement.
• Practical applications: We provide applications of our parameterization and deformation function including direct
manipulation and expression transfer.
The remainder of the paper is organized as follows. Section 2
briefly reviews related work and Section 3 gives an overview
of our system. Each step of our system is presented in details
in Sections 4 and 5. In Section 6, applications of the proposed method are discussed with the experimental results.
We discuss the problems and limitations of the proposed
method and conclude the paper in Section 7.
2. Related Work
2.1. Expression synthesis and cloning
It is not easy to synthesize realistic facial animations with
direct control over the face geometry by moving control
points or clusters because there are too many degrees of freedom. Instead, many approaches represent facial expressions
as a linear combination of basis shapes [TYAB01, CLK01,
JTDP03, CB05]. This blend shapes method is widely used
in the industry. PCA has been used to compute weights
for a combination of blend shapes [BV99]. Our data-driven
method produces basis shapes automatically from the real
performance. Deformation of the face is also improved by
non-linear deformation model.
Another widely used method in the industry can be classified
into performance-based approaches [Wil90, KMMTT92,
LRF93, EBDP96, EPMT98, FNK∗ 00]. In these approaches,
an animation is synthesized based on the live performance
of actors. Using a variety of capture techniques, these approaches successfully synthesize realistic facial animation.
Zhang et al. proposed a data-driven face inverse kinematics
system considering the expression of entire face, so that, for
example, pulling up on one corner of the mouse causes the

entire face to smile [ZSCS04]. An interesting approach has
been proposed by Lau et al. in which a realistic expression
can be modeled based on a large statical model of feasible
human expressions [LCXS07]. Similarly to this approach,
we provide a compact representation of the data while sacrificing little quality together with an intuitive parameterization of the human expression that can be used for expression
transfer.
Expression retargeting is an interesting area in view of the
fact that this enables the reuse of desired expressions for
other models. Noh and Neumann copied an expression sequence of a source face to a target model based on the
geometric difference between them [NN01]. Their method
represents powerful means cloning expressions to various
types of models. Vlasic et al. estimated a multilinear model
of human faces that parameterizes the space of geometric
variations according to different attributes (e.g., identity, expression, and viseme) from scanned data [VBPP05]. Focusing the detail movement of the face, our framework allows
the expression to be transfered to the target character in an
straightforward manner through a parameter space, resulting
more semantical transfer.
2.2. Expression parameterization
Facial expressions can be parameterized in many ways. One
of the most popular schemes categorized into semantic parameterization is FACS [EF78], which was originally created for psychological uses. FACS defines 46 units of facial
poses and movements called Action Units. They are very intuitive and can be used as basic parameters for some methods
such as blend shapes. Since only the small parts of face (e.g.,
eyes or lips) were considered for each Action Unit, it cannot
represent the correlations of the regions effectively.
Geometric parameterization is another approach [Par82,
LTW95, Wat87, WF95, Pla81, CDB02]. As those are considered to be lack of anatomical accuracy, there has been a research using physically based facial model [SNF05]. Later,
Sifakis et al. focused especially on speech and visemes
which need high accuracy and they pointed out that since
a person’s face is driven by muscle activations, an anatomically faithful model of actual human facial muscle exactly
spans the space of facial expression [SSRMF06].
While those parameterization methods rely on human experience and knowledge, ours uses an automated blind-source
separation technique. Therefore, it can yield a parameterization that covers the data completely, while merging any
correlated movements together. We also believe that the corresponding deformation model is a reasonable trade-off between realism and efficiency.
Feng et al. proposed a data-driven deformation method very
similarly to the our method [FKY08]. This adopts PCA to
choose control points and builds deformation models using
canonical correlation analysis while we try to extract a set
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

H. Shin / Expression Synthesis and Transfer in Parameter Spaces

1831

Data-driven parameterization
Marker #1

Classification

Actuation Signals #1

Marker #2

Classification

Actuation Signals #2

Marker #3

Classification

Actuation Signals #3

đ

Classification

Actuation Signals #N

Basis Actuation #1
Basis Actuation #2
Basis Actuation #3

đ

đ

đ
Marker #N

ICA

Basis Actuation #K

Deformation function generation
Deformation
function

Examples

Deformation model
for parameters

Figure 1: Overview of the parameterization method

Figure 2: Test expression set: the expressions in the set cover
all unit actions described in FACS.

of independent parameters from the data to provide users
with an intuitive control together with efficient deformation
model.

• data-driven parameterization: We roughly find basis actuations of each marker using mean-shift clustering. A set
of basis actuations over face are then extracted using ICA.

2.3. ICA in animation

• deformation function generation: We model nonlinear deformation of the face due to the control parameters so as
to establish more precise deformation model.

ICA (Independent Component Analysis) is used to extract
independent features from a set of data [HO00]. Mori and
Hoshino proposed the idea of applying ICA to a motion
data to separate meaningful motion features [MH02]. Cao
et al. used ICA to separate speech and emotion from captured data [CFP03]. In the same way, Shapiro et al. separated styles from body motion data using ICA [SCF06]. For
example, a clumsy walking motion can be decomposed into
the clumsy style and the walking pattern. With those systems, a user can edit motions in intuitive ways.

As the test data, a facial animation sequence of an actor was
captured. The actor wore 75 markers on the face including
seven markers for head tracking. The actor was asked to attempt a set of 20 predefined expressions in approximately 30
seconds. The test set was designed to contain all unit actions
described in FACS. The expressions in the test set are shown
in Figure 2. To capture fast actions such as eye blinking, the
frame rate of the animation are set to 120Hz.

Fidaleo et al. analyzed co-articulation regions by classifying gesture intensity in ICA space [FN04]. Facial images are
partitioned into a set of local regions and each region is parameterized by the activation level of a pre-defined set using
ICA. With an inputted video of facial expressions, the system generates flip-book style animation.
To adopt ICA for parameterization of expression, our approach hire a sophisticated preprocessing step. By doing
this, we can extract more meaningful basis expressions.
Moreover, applying ICA onto the entire face yields parameterization in which a fundamental movement of face and its
accompanied minor movements are represented with a single parameter. This allows the users to synthesize a realistic
expression quickly since one only need to specify a fewer
parameter to control the entire face.

3. Overview
Our analysis method consists of two steps: data-driven parameterization and a deformation function generation step
as follows (see Figure 1).
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

It is worth noticing that our proposed method can be applied
to a variety form of animation sequence. Although we describe our method taking marker data as the input for simplicity, the proposed method can be applied to the other
animation sequences including manually crafted animation
clips of a blend shape and captured animations of a denser
geometry with fine detail. As long as they can be represented
as displacements of points on a face, the method presented
in this paper can be adapted.
4. Data-Driven Expression Parameterization
As the first step of parameterization, we roughly estimate the
actuation amount of each unit from the marker data. Each
marker is affected by one or more actuation units, each of
which moves the marker in a unique direction. Therefore,
we can roughly estimate the actuations by dividing its trajectory into unit movements and classifying them based on
their major directions. To do this for each marker, we first
find the frame where the marker is maximally displaced from
its neutral position locally. Each maxima are then projected
onto the unit sphere centered at the neutral position and classified using mean-shift clustering method [CM02]. Figure 3

1832

H. Shin / Expression Synthesis and Transfer in Parameter Spaces

Figure 3: The result of classification: The small dots are the
projections of the local maxima of the movement colored by
the classes. The Orange dots show the average directions of
the classes.

Figure 4: The signals colored by the class. Each curve
shows the approximated amount of the corresponding actuation.
exhibits the three classes of a marker on the left eyebrow.
The classes roughly correspond to the three major muscles
attached: corrugator, frontalis, and orbicularis oculi. Notice
that even if a class contains effects of two more actuation
units or the actuation of a unit appears two or more classes,
the final separation is not much degraded, since the following ICA step process the signals of all markers to produce a
compact set of independent signals.
Once the classes of every maxima of a markers have been
decided, we project the marker trajectory between a pair of
consecutive maxima onto the axis of the class, which is computed by simply averaging and renormalizing the projections
of the maxima in the class onto the sphere. If the piece of the
trajectory connects two maxima in a class, we take the magnitudes of the projections as the actuation signal of the class.
Otherwise, the magnitudes are multiplied to a smooth transition function so that the signal of the former class fades
out smoothly to zero and vice versa. Figure 4 shows the signals for the three actuations of the marker on the eyebrow.
Each of curves shows the movement of the marker in each
distinctive direction.
The signals of a particular marker include those due to individual actuations and their mixtures. Moreover, a set of
markers are affected by a single actuation units. To separate
the mixtures into the individual signals and merge sets of coherent signals, we adopt standard ICA technique for the signals obtained from all markers. Although the detail of ICA
lies out of the scope of this paper, here is a brief description.
Given an input vector signal y(t), ICA computes a source
vector signal s(t) and the corresponding mixing matrix A:
y(t) = As(t).

(1)

Among many solutions, ICA chooses s(t) in such a way
that each element signal is mutually independent as much

Figure 6: Expressions corresponding to principle components: each of expression is not meaningful and a reasonable
expression is a combination of them.
as possible, while PCA guarantees the orthogonality of the
columns of A. To do this, PCA is first applied to reduce dimensionality of the signals followed by the process that decreases Gaussianity of each signal in s(t). Further detail can
be found the article written by Hyarinen [HO99].
Figure 5 demonstrates the expressions corresponding to the
actuations analyzed automatically. We set the number of the
actuations as 13, which has been chosen experimentally. Notice that the expressions are not quite local, since the actor
moved parts of her face simultaneously throughout the motion capture session. For example, in the motion capture session, she smiled by the whole face a few times, but never
smiled by the eyes nor by the mouth only. In such a case,
we merge these movements into one actuation by assuming
that the actor cannot separately move those parts to smile.
A local parameterization can be done if an actor moves all
the local parts individually. In the system with 2.66 GHz Intel Core 2 Duo CPU and 2GB ram running Mac OS X, the
preprocessing and ICA have taken 10.2 and 8.1 seconds, respectively. Figure 6 shows the result of PCA for comparison.
It effectively reduces the number of movements, however the
expressions corresponding to the principle axes are not intuitive, since the second and the following movements must be
orthogonal to the previous ones.
5. Deformation Function Generation
Given the actuation signals corresponding to the motion captured animation, the relation between the signals and the
movement of each marker needs to be established. Here, the
movements of markers actuated by a signal may not be linear to each other. To model those nonlinearity, we employ
a simple nonlinear function that maps a particular actuation
signal to the movement of a marker and train the parameters
of the function based on the actuation signal and the marker
movement in the corresponding animation.
For the displacement of a marker in a cartesian axis direction
xi (t) from its neutral position, we approximate its movement
due to an actuation signal s j (t) using a cubic polynomial:
xi = ∑ ai j s3j + bi j s2j + ci j s j ,

(2)

j

since the net displacement of a marker is summation of the
displacement due to every actuation signal. Here, ai j , bi j ,
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

H. Shin / Expression Synthesis and Transfer in Parameter Spaces

1833

Figure 5: Expressions corresponding to each actuation: neutral, raising lips, raising lower lip, pulling cheek up (Zygomaticus),
closing eyes, wrinkling nose (Corrugator), raising eyebrows (Frontalis), opening mouth, smiling, pulling lips to left (negative
for right), pushing mouth out, opening eyes, lowering jaw, raising the center of eyebrows (inner Frontalis)

Figure 7: Effect of the nonlinear deformation model: from
left to right, the expression is synthesized by increasing one
control parameter from zero to one. (Top row) Linear model
from ICA, and (Bottom row) Learned nonlinear model
and ci j are unknown coefficients to compute. It is important to note that the constant term of the cubic polynomial is
omitted such that the neutral expression is represented with
the zero control vector, s j = 0.
Equation (2) can be rewriten in a matrix form:
xi = · · · s3j s2j s j · · ·

· · · ai j bi j ci j · · ·

T

= s˜ j ci .

(3)

For the time varying marker position and the actuation signals, we have
⎡
⎤ ⎡
⎤
..
..
.
.
⎢
⎥ ⎢
⎥
⎢ xi (t − 1) ⎥ ⎢ s˜ j (t − 1) ⎥
⎢
⎥ ⎢
⎥
⎢ xi (t) ⎥ = ⎢ s˜ j (t) ⎥ ci = Sci
(4)
⎢
⎥ ⎢
⎥
⎢ xi (t + 1) ⎥ ⎢ s˜ j (t + 1) ⎥
⎣
⎦ ⎣
⎦
..
..
.
.

Figure 8: Expressions of a cartoonic character synthesized
by parametric control
from a particular motion capture data. To do this, one first
have to produce 3D models of the face with variety expressions using the blend shape. Each expression are then mimicked by the parametric model to yield many parameter-3D
model pairs. Those pairs can be directly used as the input of
Equation (4) to compute the deformation functions. The expressions of the cartoonic face in Figure 8 are produced by
controlling the actuation parameters. The expression can be
controlled intuitively while preserving the style of the face
since the expressions of the face is mimicked not geometrically but semantically. Given a set of blend shapes, it took 2
hours for a naive user to build a deformation model for this
particular character.
6. Applications & Results

Here, matrix S represents the control signals, which is invariant for target marker, and vector ci purely contains the
unknown coefficients. By finding the least squares solution
of this linear system, we can compute the best fitting nonlinear function from the actuation signal to the marker displacement. Even though S is N × K matrix where N is the number
of frames in the data and K is the number of IC’s, which are
3500×13, one can efficiently compute coefficients in linear
time for all markers by multiplying the pseudo inverse of S
of which size is K × N to each xi . The system described earlier needs 45 seconds to compute the deformation functions.

6.1. Direct Manipulation

Figure 7 exhibits the effect of the nonlinear deformation.
Comparing to the linear movement obtained directly from
ICA (top row), the nonlinear version (bottom row) yields
more realistic and smooth movement of the mouth. The proposed learning method can be used to find the deformation
model of the other faces. For example, we can find a set
of deformation functions for a conventional blend shape, so
that we can control the face using the parameters obtained

where K is the set of indices of the constrained vertices,
pˆ k is the target position of the k-th vertex, p0k is its position
at the neutral expression, and fk is net displacement of the
k-th vertex due to the control parameter s. This problem is
similar to a typical nonlinear IK problem, and the Conjugate
Gradient method is adopted to solve s. As fk are the cubic
polynomials of the variable s, and because the coefficients
of the high-order terms are relatively small, the solution can

c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

For controlling expressions directly in 3D space, the proposed system solves the parameter vector that deforms the
geometry in accordance with the user-specified geometric
constraints. In the proposed direct manipulation framework,
the geometric constraints are limited to the positional constraints of the vertices on the face. The direct manipulation
problem can be formulated as
arg min
s

∑

||pˆ k − p0k + fk (s) ||2 ,

(5)

k∈K

1834

H. Shin / Expression Synthesis and Transfer in Parameter Spaces

Figure 9: Expressions generated in direct manipulation
framework
be found with few iterations, providing real-time interactive
performance.
Figure 9 demonstrates the experimental result of direct manipulation. In the first three examples, only one point on the
face is constrained. As expected, the manipulating of a single point affects the shape of every coherent parts of the face
and produces a realistic expression. Therefore, it is relatively
easy to synthesize a desired expression with the proposed
direct manipulation framework. The remaining examples in
the figure show the expression constrained at more than one
point. With a few constraints, a desired expression can be
generated efficiently. With one constraints, the expression
is solved in 1 ms. Solving the expression for a few (three
to five) takes 5 to 7 ms. With more constraints, it generally
takes more time to solve, and since it is easier to become
ill-posed with more constraints, we have found a few frames
requires a number of iteration taking 20-30 ms, which is still
reasonable for interactive applications.
6.2. Expression Transfer
With the proposed parameterization scheme, it is also possible to clone expressions through a parameter space, as the
deformation model can be independently obtained from the
data used in the parameterization process. If a user has an animation sequence of a source character, it can be transferred
to a target character through the parameter space. The predefined animation of the source face can be converted into
the parametric representation using the aforementioned direct manipulation method. Once an animation sequence is
represented with the parameters, it is also straightforward to
synthesize the corresponding expression sequence of the target character. Moreover, the transferred animation can further be edited directly in 3D space and the parameter space.
The major advantage of the proposed expression transfer
method is that if the geometric shapes of the target face are
prepared such that the expressions contain its characteristic
styles, they can be also reproduced in the cloned animation.
Figure 10 provides a side-by-side comparison of the source
expressions and the corresponding cloned expression of the
target face. Here, the blend shape lacks some expressions
that are required to reproduce the source expressions, although the number of blend shapes is larger than the number
of ICs (i.e., 20 shapes). The result shows that the proposed
expression transfer method faithfully transfers the expressions reflecting the style of the target character.

Figure 10: Expressions transfer: the captured expression sequence of the human character is transfered through the parameter space to the cartoon character
7. Discussions and Limitations
In this paper, we presented a novel parameterization method
together with its applications. Based on ICA, we separate
the control parameter signals from a realistic sequence of
an actor. To improve the effectiveness of the parameterization process, a sequence of preprocessing steps is provided.
The face geometry then deforms in accordance with the userspecified control parameter based on a nonlinear deformation model learned from the example. This parameterization
and deformation scheme allows users to control the expressions of characters using the techniques of parametric control, direct manipulation, and transfer.
The results shown in this paper is not compatible to the previous approaches mainly because the expression data we use
are not the dense geometric data but marker trajectory. However, the proposed method can be scaled for such denser
data. The time complexities of the preprocessing, analysis,
and deformation function learning steps are linear to the
number of vertices (or markers). Although the time required
for the first and the last step increases proportionally to the
squared number of frames, we believe that it is not a problem
since those steps are need to be done once as preprocessing.
At runtime, deformation can be done in time linear to the
number of vertices.
The major drawback of the proposed method is that it cannot separate the actuations that appear coherently in the data.
This prevents us from parameterizing facial expression locally, and the conventional animators might have some problem to use the parametric control and direct manipulation.
However, naive users can easily get accustomed to the system, since realistic expressions can be generated by changing a small number of parameters. Moreover, the direct manipulation framework can be adapted into the conventional
facial animation pipeline. In the framework, the parts that
an animator wants not to move can be effectively fixed by
pinning. Therefore, we believe that we can develop more inc 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

H. Shin / Expression Synthesis and Transfer in Parameter Spaces

1835

tuitive user interfaces in this framework that provide useful
local control.

[JTDP03] J OSHI P., T IEN W. C., D ESBRUN M., P IGHIN F.:
Learning controls for blend shape based realistic facial animation. In 2003 ACM SIGGRAPH / Eurographics Symposium on
Computer Animation (Aug. 2003), pp. 187–192.

Acknowledgements

[KMMTT92] K ALRA P., M ANGILI A., M AGNENATT HALMANN N., T HALMANN D.: Simulation of facial muscle
actions based on rational free form deformations. Kilgour A.,
Kjelldahl L., (Eds.), vol. 11, pp. 59–69.

This work was supported by the new faculty research fund
of Ajou University and partially supported by Defense Acquisition Program Administration and Agency for Defense
Development under the contract (UD060048AD). This paper was completed with Ajou university research fellowship
of 2008.
References
[BV99] B LANZ V., V ETTER T.: A morphable model for the
synthesis of 3d faces. In Proceedings of SIGGRAPH 99 (Aug.
1999), Computer Graphics Proceedings, Annual Conference Series, pp. 187–194.
[CB05] C HUANG E., B REGLER C.: Mood swings: expressive
speech animation. ACM Transactions on Graphics 24, 2 (Apr.
2005), 331–347.
[CDB02] C HUANG E. S., D ESHPANDE F., B REGLER C.: Facial
expression space learning. In 10th Pacific Conference on Computer Graphics and Applications (Oct. 2002), pp. 68–76.
[CFP03] C AO Y., FALOUTSOS P., P IGHIN F.: Unsupervised
learning for speech motion editing. In 2003 ACM SIGGRAPH /
Eurographics Symposium on Computer Animation (Aug. 2003),
pp. 225–231.

[LCXS07] L AU M., C HAI J., X U Y.-Q., S HUM H.-Y.: Face
poser: Face poser: Interactive modeling of 3d facial expressions
using model priors. In ACM SIGGRAPH / Eurographics Symposium on Computer Animation (2007).
[LRF93] L I H., ROIVAINEN P., F ORCHEIMER R.: 3-d motion estimation in model-based facial image coding. In IEEE Transactions on Pattern Analysis and Machine Intelligence (June 1993),
vol. 15, pp. 545–555.
[LTW95] L EE Y., T ERZOPOULOS D., WATERS K.: Realistic
modeling for facial animation. In Proceedings of SIGGRAPH
95 (Aug. 1995), Computer Graphics Proceedings, Annual Conference Series, pp. 55–62.
[MH02] M ORI H., H OSHINO J.: Independent component analysis of human motion. In Proceedings of International Conference on Acoustics Speech and Signal Processing (2002), vol. VI,
pp. 3564–3567.
[NN01] N OH J.-Y., N EUMANN U.: Expression cloning. In
Proceedings of ACM SIGGRAPH 2001 (Aug. 2001), Computer
Graphics Proceedings, Annual Conference Series, pp. 277–288.
[Par82] PARKE F. I.: Parameterized models for facial animation.
IEEE Computer Graphics & Applications 2, 1 (Nov. 1982), 61–
68.

[CLK01] C HOE B., L EE H., K O H.-S.: Performance-driven
muscle-based facial animation. The Journal of Visualization and
Computer Animation 12, 2 (May 2001), 67–79.

[Pla81] P LATT S. M.: Animating facial expressions. In Computer
Graphics (Proceedings of SIGGRAPH 81) (Aug. 1981), pp. 245–
252.

[CM02] C OMANICIU D., M EER P.: Mean shift: A robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence 24, 5 (2002), 603–619.

[SCF06] S HAPIRO A., C AO Y., FALOUTSOS P.: Style components. In Graphics Interface 2006 (June 2006), pp. 33–39.

[EBDP96] E SSA I., BASU S., D ARRELL T., P ENTLAND A.:
Modeling, tracking and interactive animation of faces and heads:
Using input from video. In Computer Animation ’96 (June 1996),
pp. 68–79.
[EF78] E KMAN P., F RIESEN W. V.: Facial action coding system :
A technique for the measurement of facial movement. Consulting
Psychologist Press: Palo Alto (1978).
[EPMT98] E SCHER M., PANDZIC I., M AGNENAT-T HALMANN
N.: Facial deformations for mpeg-4. In Computer Animation ’98
(June 1998).
[FKY08] F ENG W.-W., K IM B., Y U Y.: Real-time data-driven
deformation using kernel canonical correlation analysis. ACM
Transactions on Graphics 26, 3 (2008), 91.
[FN04] F IDALEO D., N EUMANN U.: Analysis of co-articulation
regions for performance-driven facial animation. Computer Animation and Virtual Worlds 15, 1 (2004), 15–26.
[FNK∗ 00] F IDALEO D., N OH J.-Y., K IM T., E NCISO R.,
U.N EUMANN: Classification and volume morphing for performance driven facial animation. In International Workshop on
Digital and Computational Video (2000).
[HO99] H YVÄRINEN A., O JA E.: Fast and robust fixed-point
algorithms for independent component analysis. Transactions on
Neural Networks 10, 3 (1999), 623–634.
[HO00] H YVÄRINEN A., O JA E.: Independent component analysis: Algorithms and applications. Neural Networks 13, 4–5
(2000), 411–430.
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

[SNF05] S IFAKIS E., N EVEROV I., F EDKIW R.: Automatic determination of facial muscle activations from sparse motion capture marker data. ACM Transactions on Graphics 24, 3 (Aug.
2005), 417–425.
[SSRMF06] S IFAKIS E., S ELLE A., ROBINSON -M OSHER A.,
F EDKIW R.: Simulating speech with a physics-based facial muscle model. In 2006 ACM SIGGRAPH / Eurographics Symposium
on Computer Animation (Sept. 2006), pp. 261–270.
[TYAB01] T ORRESANI L., YANG D. B., A LEXANDER E. J.,
B REGLER C.: Tracking and modeling non-rigid objects with
rank constraints. In 2001 Conference on Computer Vision and
Pattern Recognition (CVPR 2001) (Dec. 2001), pp. 493–500.
[VBPP05] V LASIC D., B RAND M., P FISTER H., P OPOVI C´ J.:
Face transfer with multilinear models. ACM Transactions on
Graphics 24, 3 (Aug. 2005), 426–433.
[Wat87] WATERS K.: A muscle model for animating threedimensional facial expression. In Computer Graphics (Proceedings of SIGGRAPH 87) (July 1987), pp. 17–24.
[WF95] WATERS K., F RISBIE J.: A coordinated muscle model
for speech animation. In Graphics Interface ’95 (May 1995),
pp. 163–170.
[Wil90] W ILLIAMS L.: Performance driven facial animation. In
Proceedings of SIGGRAPH 90 (1990), pp. 235–242.
[ZSCS04] Z HANG L., S NAVELY N., C URLESS B., S EITZ S. M.:
Spacetime faces: high resolution capture for modeling and animation. ACM Transactions on Graphics 23, 3 (Aug. 2004), 548–
558.

