Volume 28 (2009), Number 2

EUROGRAPHICS 2009 / P. Dutré and M. Stamminger
(Guest Editors)

Predicting Display Visibility Under Dynamically Changing
Lighting Conditions
Tunç Ozan Aydın, Karol Myszkowski and Hans-Peter Seidel
MPI Informatik, Germany

Abstract
Display devices, more than ever, are finding their ways into electronic consumer goods as a result of recent trends
in providing more functionality and user interaction. Combined with the new developments in display technology
towards higher reproducible luminance range, the mobility and variation in capability of display devices are
constantly increasing. Consequently, in real life usage it is now very likely that the display emission to be distorted
by spatially and temporally varying reflections, and the observer’s visual system to be not adapted to the particular
display that she is viewing at that moment. The actual perception of the display content cannot be fully understood
by only considering steady-state illumination and adaptation conditions. We propose an objective method for
display visibility analysis formulating the problem as a full-reference image quality assessment problem, where
the display emission under “ideal” conditions is used as the reference for real-life conditions. Our work includes
a human visual system model that accounts for maladaptation and temporal recovery of sensitivity. As an example
application we integrate our method to a global illumination simulator and analyze the visibility of a car interior
display under realistic lighting conditions.
Categories and Subject Descriptors (according to ACM CCS): Computer Graphics [I.3.3]: Picture/Image
Generation—Computer Graphics [I.3.6]: Methodology and Techniques—Image Processing and Computer Vision
[I.4.8]: Scene Analysis—

1. Introduction
Display devices with different characteristics are ubiquitously integrated to our daily lives: from the technically limited displays on cell phones, mobile gaming platforms and
similar peripheral devices, to projectors, and the high end
plasma and experimental high dynamic range (HDR) displays, there exists a large spectrum of capabilities in contrast and color reproduction. Two consequences of this trend
are the motivation of our work. Firstly, an observer might
receive information simultaneously from multiple displays
with different characteristics, such as when taking notes on a
PDA while watching a presentation from the projector. Secondly, displays integrated to mobile devices might be observed under uncontrolled lighting, for example a cell phone
display exposed to direct sunlight. These observations suggest that in real life scenarios we can no longer assume the
observer’s visual system to be fully adapted to a single display, and the display emission to be not distorted by rec 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

flections. But how do we perceive the display content under these conditions? In this study we present an objective
method to predict the visual perception of information conveyed through different displays under spatially and temporally varying lighting conditions.
Evaluating the perception of images that have been processed by some algorithm has been an active field of research. Subjective experiments and objective metrics have
been used to assess the quality of images subjected to various
distortions and tone-mapping algorithms. The main point of
interest in those studies is the almost “ideal” case where the
observer is in a oratory setup with controlled lighting, sitting
directly in front of the display in a perfectly adapted state.
While such conditions are necessary for benchmarking different algorithms by assessing the quality of their outcomes,
it is equally important to consider lighting conditions and
human visual system (HVS) states that are likely to be observed in real life.

174

Tunç Ozan Aydın, et al. / Predicting Display Visibility Under Dynamically Changing Lighting Conditions

We formulate the display visibility analysis (Section 3)
as a full-reference image quality assessment problem, where
the observer’s actual field of view at a given time is the test
image, and the display emission under ideal conditions (perfect adaptation, no reflections) is the reference image (Figure 1). Our method accounts for the decrease in sensitivity
due to maladaptation (adaptation to a different level than
the background luminance) that may be caused by abrupt
changes in lighting as well as the observer directing her
gaze to a brighter or darker object. The magnitude of maladaptation is a temporal mechanism: sensitivity of the human visual system (HVS) is gradually recovered in time
(Section 3.1). Our metric, on the one hand predicts the spatially varying magnitude of visibility of the reference and
associates pixels with easily interpretable visibility classes
like informative, warning&caution, etc. (Section 3.2), and
on the other hand detects structural distortions due to the
loss of details with respect to the reference (Section 3.3).
We present results for various lighting conditions and visual
system states in Section 4, and apply our method to a car interior display, where the lighting of both the car interior and
the display is computed by a global illumination simulator
(Section 5).

Figure 1: Images of a car display with and without reflections. Note that gamma corrected images should be converted to physical luminance before being processed by our
method.

2. Previous Work
Given a sufficient number of test subjects, a reliable way of
assessing image quality is through subjective experiments.
This method has been used to determine the effects of simple distortion types (blur, white noise, compression artifacts,
etc.) to the perceived image quality under identical settings
for each trial [SSB06]. More recently, [AMS08] performed
a similar study comparatively on a regular LCD and a prototype HDR display, where the latter is approximately an
order of magnitude brighter than the former. Similar work
has also been done in the context of tone-mapping algorithm
evaluation [YMMS06]. Although display brightness and image dynamic range appear as variables in these studies, they
are performed in vitro under highly controlled lighting (either ambient or no light) where the observer is directly looking at the display and has been given enough time for her
visual system to adapt. Similar assumptions are taken for

granted even in more extreme cases of luminance adaptation mismatches when real world scenes are directly compared against rendered [MRC∗ 86, DM01] or tone mapped
ˇ
YBMS07] images.
[CWNA06,
On the other hand, objective methods rely on mathematical HVS models instead of human subjects. Well established metrics visible difference predictor (VDP) [Dal93]
and visible difference metric (VDM) [Lub95] detect the discriminability between the test and reference images. This
approach works fine for a simple distorted image - reference image comparison, but a higher level image structure
[WB06] concept can also capture the fact that not all visible
differences are distortions (e.g. contrast enhancement). Recently, [AMMS08] proposed a dynamic range independent
metric that detects visible structural distortions, and can be
considered a combination of both approaches. These metrics
have been used to replicate the results of subjective studies
assuming the same controlled settings, without considering
temporal factors.
While the objective metrics discussed so far are mostly
used to measure fidelity in reproduction of images with respect to the reference, they can be also successfully applied to evaluate visual performance in specific tasks such
as reading or target detection [Lub95]. Also, more specialized metrics targeted for displays used in automotive and
aerospace applications have been developed. For example,
the symbol discriminability on such displays can be measured as the Euclidean difference between the corresponding visible contrast images and scaled in the just noticeable
difference (JND) units [ASMG06]. Dreyer [Dre07] measures the visibility level (VL) defined as the ratio between
the actual luminance difference (e.g., between a symbol and
its background) and Adrian’s threshold luminance [Adr89],
which is a function of the symbol size and exposure time.
In both approaches the exposure time is considered only by
means of simple proportionality coefficient. The temporal
aspect of adaptation is modeled in the time-to-visibility metric [KS92], which takes into account display contrast (with
ambient illumination) and the adaptation luminance to determine the time when a given spatial frequency pattern becomes visible. Mantiuk et al. [MDK08] proposed a quality
metric which takes into account ambient illumination conditions to optimize perceived detail reproduction in a tone
mapping algorithm.
The last two methods are the closest to our goals. We
differ from them, in that we consider spatially and temporally varying real world lighting including specular reflections from the display surface, and model the loss of sensitivity due to maladaptation over time. We also focus on
providing human interpretable visibility and distortion maps
at any moment of time in the course of adaptation.
Our maladaptation model is strongly influenced by timedependent tone mapping algorithms [PTYG00, IFM05],
which consider temporal adaptation models to reproduce
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Tunç Ozan Aydın, et al. / Predicting Display Visibility Under Dynamically Changing Lighting Conditions

scene visibility for realistic display of the original HDR image appearance. This involves contrast manipulation (usually compression) in the HDR image. Our goals are clearly
different as we measure discriminability and detect perceivable structural changes between a pair of images as a function of time. Also, we model the eye optics, contrast sensitivity and visual masking for various spatial frequencies, which
are usually ignored in tone mapping (a notable exception is
the work of Pattanaik et al. [PFFG98], but they ignore the
time domain).

3. Visibility Analysis
We use two measures in our display analysis: first, we determine the visibility classes of the display emission image
as a function of the perceived contrast magnitude, and second, we detect the loss of details as a result of the distortions
in image structure due to spatially varying illumination. The
former measure is computed at contrast levels well above the
threshold (supra-threshold), while the latter happens at the
vicinity of the visibility threshold (near-threshold). We employ separate methods to model both tasks, each specialized
in modeling the corresponding contrast range.
Since we refrain from making a priori assumptions on
possible illumination conditions, meaning the reflections
can be arbitrarily bright, our entire model is designed
for high luminance ranges. Two recent HDR capable image quality metrics, HDR-VDP [MDMS05] and a dynamic
range independent metric [AMMS08] use very similar nearthreshold HVS models, extended over the well established
VDP [Dal93]. Both metrics model luminance masking by
assuming the eye is perfectly adapted to the image at single pixel resolution. Another recent supra-threshold vision
model [MDK08] used in an HDR context, relies on a transducer function for measuring the HVS response to contrast
[Wil80] similarly assumes perfect adaptation.
In reality, the visual system rarely achieves such levels
of sensitivity and will be maladapted up to some degree. In
this respect, the perfect adaptation assumption corresponds
to the “ideal case”, where the observer’s sensitivity is maximal. From a distortion analysis point of view, this approach
provides a convenient worst case measure since distortion
visibility can only decrease if the observer is adapted to a
different luminance level. But the goal in our application is
the exact opposite, namely the display content should be visible even if the observer is strongly maladapted, and thus the
perception under perfect adaptation is not representative of
practical conditions. The main contribution of this work is to
introduce the building blocks necessary to extend the HDR
image quality models of steady-state human vision to work
under temporally changing real world conditions.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

175

3.1. Temporal Adaptation
The sensitivity variation at a certain adaptation state is commonly modeled as a sigmoid response profile centered at the
corresponding luminance level [NR66]. To cope with temporally and spatially changing real world luminance, the adaptation state is continuously readjusted. In scenarios with dynamically changing lighting conditions, the relatively slow
pace of temporal adaptation plays a significant role in visual perception. The threshold luminance when the HVS is
maladapted to the adaptation luminance La , while the actual (background) luminance is L, is typically given as a
threshold versus intensity and adaptation function (∆L =
tvia(L, La )).
Almost all known adaptation mechanisms operate within
retina, each of which having their own time course suggesting that they should be tracked separately [PTYG00]. The
fast but less effective neural mechanisms and slower but
more effective photochemical mechanisms are responsible
for shifting the response profile across the visible luminance
range for both cone and rod systems. We adopt Irawan’s
[IFM05] approach, where adaptation due to pigment bleaching (σb ), slow neural adaptation (σc ) and fast neural adaptation (σn ) are modeled separately, and Equation 1 gives the
adaptation state as a function of adaptation luminance La .

σ(La ) = σb (La ) σc (La ) σn (La )

(1)

The sigmoid shaped retinal response function R for this
adaptation state as a function of background luminance L
and a sensitivity control parameter n is given in Equation 2.

R(L, σ(La )) =

Ln
Ln + σ(L

n
a)

(2)

To obtain the threshold luminance ∆L at an adaptation
level given by the tvia function, first the differential retinal
response ∆R = R(L + tvi(L), L) − R(L, L), that produces a
unit JND, is computed assuming perfect adaptation (La = L).
The tvi (threshold versus intensity) function returns the visibility threshold of the fully adapted visual system given
the background luminance. We derive the tvi function from
VDP’s contrast sensitivity function by iteratively computing the maximum sensitivity for each adaptation luminance
along all spatial frequencies. Finally, the difference between
the luminance value that generates the response R + ∆R and
L gives the threshold luminance of the maladapted visual
system.
In Figure 2, we plot the threshold contrasts (1/sensitivity)
for three adaptation states at La equals 1, 100 and 10, 000
cd/m2 (blue curves) along with the threshold contrasts for
perfect adaptation (red curve). The perfect adaptation curve
is approximately the envelope of all adaptation states. We

176

Tunç Ozan Aydın, et al. / Predicting Display Visibility Under Dynamically Changing Lighting Conditions
0.4

6

Visibility Class
Attention Getter

PJND
120

5

Warning & Caution

90

4

Dynamic Complex

70

3

Static Complex

60

2

Status

50

1

Informative

40

Contrast ∆L/L

0.3
∆C10,000
0.2

∆C1

0.1

∆C100

0
0.0001

0.01
1
100
10,000 1,000,000
Background Luminance L [cd/m2]

Figure 2: Threshold contrasts (calculated by normalizing
tvia by background luminance L) for perfect adaptation (red
curve) and for adaptation luminances (La ) of 1, 100 and
10, 000 cd/m2 (blue curves). At L = 100 cd/m2 , threshold contrast (∆C) is lowest for adaptation luminance 100
cd/m2 , whereas for maladapted states with La equals 1
cd/m2 and 10, 000 cd/m2 the threshold increases notably.

can think of the blue curves shifting horizontally as the visual system adjust to a new adaptation state. The time course
of neural adaptation in the case of an abrupt change in lighting from luminance L0 at time t = 0, to La is modeled as the
exponential decay function (Equation 3) for neural adaptation of both rods and cones.

−t

La−current = La + (L0 − La ) e t0

(3)

Temporal change in adaptation is modeled by updating
the tvia at each time step with the current adaptation level
La−current . We set t0 to 0.08 seconds for cones, and 0.15
seconds for the rods as given in [IFM05]. We consider only
the steady-state behavior of relatively slow pigment bleaching, since we observed that detail visibility is almost entirely
recovered within the first few seconds.
In the rest of this section, we discuss the supra– and near
threshold measures we employ in our analysis and introduce
new building blocks based on the tvia function, that extend
those measures by modeling adaptation over a time course.
3.2. Visibility Classes
In this section we relate luminance of the reference display
emission to magnitude of contrast visibility scaled in JND
units. Contrast of typical display content is well over the
visibility threshold. Thus we use a transducer based suprathreshold HVS model, that accurately predicts the magnitude of HVS response by taking into account visual masking.

Description
Attention getting quality must be maintained
beyond the para-foveal
limit and into the peripheral vision areas
Warning or cautionary
information
requiring
predominate attention
Complex formats with
small
alphanumeric
characters and/or fine
line analogue or graphic
presentations. This data
is not fixed in location
Same as Dynamic complex, but data is fixed in
location
Dual State (On-Off) information. Location is
fixed
Fixed format single state
information. Provides
background information
supporting controls or
more complex presentations

Figure 3: The visibility classes and associated PJND values.

A numerical response value computed by the model alone is
not descriptive (e.g. how much visible are 50 JNDs?). Instead, a classification of HVS response intervals into visibility classes (VC) is easier to interpret by humans. The perceptible just noticeable difference (PJND) model introduces
6 classes of visibility (Figure 3) and has been applied to airplane cockpits [SCG∗ 03]. The method has been calibrated
by subjective experiments on airplane-pilots and civilians in
separate studies, and similar values are obtained for both.
The PJND value is defined as the geometric mean of luminance and chrominance JNDs. According to [Dre07], the
effect of chrominance is relatively small, therefore we consider only luminance contrast.
In the original PJND method, luminance to JND conversion is done be normalizing the logarithmic contrast by an
experimentally found constant assuming that the observer is
adapted to 10, 000 cd/m2 . In environments subject to strong
sunlight (such as airplane cockpits), it is reasonable to assume logarithmic HVS response and high adaptation luminance. But under dimmer lighting this model will severely
underestimate observer sensitivity. Additionally, the significant effect of visual masking on supra-threshold contrast
perception is neglected. In our work, we employ a multiscale
luminance contrast perception model [MDK08] to compute
the hypothetical supra-threshold HVS response (Figure 4).
First, we calculate the logarithmic contrast G across scales
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

177

Tunç Ozan Aydın, et al. / Predicting Display Visibility Under Dynamically Changing Lighting Conditions

method to compute S in Equation 4 is designed for steadystate adaptation. Using the tvia function from Section 3.1,
we derive Equation 5 that also accounts for temporal adaptation.

S = OT F(ρ, p)

given the image luminance, by computing the logarithm of
image luminance and building a Gaussian pyramid. The logarithmic contrast at level l is then given by the difference
between levels l and l + 1 of the pyramid, where larger numbers indicate coarser scale. Considering the high frequency
nature of the information conveyed through display devices
(text, symbols, etc.), we focus on the loss of local details
rather than distortions in the global contrast. Thus, consistent with the original method, we only consider frequencies higher than 3cy/deg. Next, Wilson’s classical transducer [Wil80] is used to compute the HVS response given
contrast W = ∆L/L and sensitivity S as input (Equation 4).
Note that logarithmic contrast can then easily be converted
to Weber contrast (W = 10|G| − 1).

T (W, S) =

3.291 [(1 + (SW )3 )1/3 − 1]
0.2599 (3.433 + SW )0.8

(4)

(5)

The normalized contrast sensitivity nCSF is modelled as
a function of spatial frequency ρ, adaptation level La and
viewing distance d. The (OT F) models the disability glare
due to the reflections in optics of the human eye, where p denotes observer’s pupil diameter (See [AMMS08] appendix
for formulas). Although not commonly observed in low dynamic range imaging, disability glare has a significant effect
on our perception of HDR images. The effect of changing
adaptation conditions to the HVS response is shown in Figure 6.
20
La = 1000 cd/m2
15
Response

Figure 4: Building blocks of the visibility class (VC) analysis. See text for details.

nCSF(ρ, La , d)
tvia(L, La )/L

10

La = 10 cd/m2
La = L

5

0
0.001

0.01
Contrast ∆L/L

0.1

Figure 6: Supra-threshold human visual system response for
background luminance L = 100 cd/m2 , at adaptation levels
La : 100 (red), 10 (blue) and 1000 (green) cd/m2 at 4 cy/deg.
The HVS responses to luminance contrast across all scales
are summed up using a Minkowski summation with exponent 2. In our visualization, the test image is shown in grayscale while corresponding visibility classes are color-coded
according to the scale at the bottom (Figure 5).

3.3. Loss of Details

Figure 5: Visibility classes of the emission of a car display.
Refer to Figure 1-right to see the original image.
The contrast sensitivity function used in the original
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

In real life scenarios a reflection component, that is a function of the display BRDF and lighting conditions, is added
to the light emitted by the display. If the display receives
only ambient illumination, the resulting effect on display
content, namely a uniform decrease in contrast, can be assessed simply comparing the visibility class maps of the real
world image and the reference display emission. But spatially varying illumination, such as specular highlights, will

178

Tunç Ozan Aydın, et al. / Predicting Display Visibility Under Dynamically Changing Lighting Conditions

not simply decrease the overall display contrast, but also introduce new contrast to the image perceived by the observer.
The metric in Section 3.2 cannot differentiate between the
distortions due to contrast introduced by the reflections, and
the contrast lost because of reflections occluding the display
emission. Only the latter of those two distortions are relevant
to our application. Consequently, our metric should incorporate the higher level concept of image structure, since the
traditional visible differences approach does not allow such
a classification of distortions. It is also likely that the display content with reflections to have a higher dynamic-range
compared to the reference. Thus, our method should not only
work in HDR, but should also handle images with different
dynamic ranges. We base our approach on the near-threshold
dynamic range independent metric [AMMS08] that satisfies
both aforementioned conditions.

from [AMMS08] for the case when La = L, that are obtained through psychophysical experiments on the modelfest
dataset [Wat00].

Figure 8: JND values for perfect adaptation (red curve), and
La equals 1, 100 and 10, 000 cd/m2 (blue curves from left to
right). Note the large differences between JND values for the
same background luminance in different adaptation states.
Figure 7: Main processing steps of the detail loss analysis.
The luminance masking step is modified as in Equation 6.
See the appendix of [AMMS08] for formulas of other components.
The input to the metric are the luminance values of the
test image (display emission and reflections) and the reference image (display emission only), both given in cd/m2
units (Figure 1). The main processing steps of the method
are depicted in Figure 7, where both input images undergo
the same processing separately, until the final distortion detection step.
To model temporal adaptation we introduce a mapping
from luminance to a perceptually uniform space scaled in
JND units of a maladapted visual system. Unlike the original method that assumes perfect adaptation, we derive the
mapping for a given adaptation luminance La by iteratively
adding threshold values at the maladapted state, starting
from the minimum luminance L1 (10−3 cd/m2 ) until the
maximum luminance LN (1010 cd/m2 ).
Li = Li−1 + tvia(Li−1 , La ) i ∈ {2, 3, . . . , N}

(6)

The index i of luminance Li gives the corresponding
JND value for the maladapted visual system. The JND values at arbitrary luminance levels are interpolated from the
two closest neighbors. The resulting mapping from luminance to JNDs is shown for perfect adaptation and three
adaptation levels at 1, 100 and 10, 000 cd/m2 in Figure 8.
We calibrate both components using the calibration values

Figure 9: Detail loss due to the reflections on the car display. Refer to Figure 1-left to see the original image. The
loss of visibility of the symbols and characters on the right
display side are detected by the metric. On the other hand
the increase of luminance on the display background in this
region due to the reflections is ignored by our metric because
it does not lead to any structural changes in the image.
We use the same optical transfer function (OTF) and normalized contrast sensitivity function (nCSF) as discussed
in Section 3.2. The orientation and spatial frequency selectivity of the neurons in the visual cortex are modeled at
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Tunç Ozan Aydın, et al. / Predicting Display Visibility Under Dynamically Changing Lighting Conditions

179

Figure 10: 1st and 3rd rows, from left to right: the reference display emission, and three levels of reflections in increasing order.
The 2nd and 4th rows: the visibility class maps of the display emission (1st column) and maps of detail loss due for three levels
of reflections (2nd , 3rd and 4th columns). The visibility maps are computed for a perfectly adapted observer. For detail loss
analysis, images with reflections are compared to the corresponding display emission.
0 sec

0.2 sec

0.4 sec

0.6 sec

0.8 sec

Figure 11: Visibility class (first row) and detail loss (second row) maps for L0 = 5664 cd/m2 , after time steps indicated at the
top.

the channel decomposition step through the cortex transform [Wat87] with modifications as in [Dal93]. Consistent
with Section 3.2, we use the cortex bands down to mean frequency 3 cy/deg, while additionally performing processing
for 6 orientations.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

In order to predict only the detail loss due to the reflections, we calculate the detection probability of the case
where visible contrast in the reference becomes invisible in
the test image, separately at each frequency and orientation.
The detail loss map is generated by combining distortions

180

Tunç Ozan Aydın, et al. / Predicting Display Visibility Under Dynamically Changing Lighting Conditions

across frequencies and orientations (through regular probability summation [AMMS08]). We take a similar in-context
map approach to visualization of detail loss as in visibility
classes (refer to Figure 9). Note that the other two types of
distortions, namely amplification and reversal of contrast are
not detected, since the former roughly corresponds to the reflections on the display device, and the latter is irrelevant for
our purposes.
4. Results
In this section we test our method on a Barco Coronis 3MP
LCD display (max. luminance 400 cd/m2 ) under multiple
levels of reflections. Firstly, for each image in our test set we
generate a scene referred HDR image of the corresponding
display emission. HDR images are generated by combining
multiple shots from a Canon 5D camera with different exposures using the open source pfsCalibration package. Next,
in the same way we capture a reflection component generated by the camera flash. A test set is created by amplifying
the reflection components to three separate levels and combining them with captured emissions for three test images.
Resulting visibility class and detail loss maps for an adapted
observer are shown in Figure 10.
Our method associates high contrast regions such as icons
and text to the highest visibility classes, whereas background
regions are predicted to have lower importance. The detail
loss analysis detects more structural distortions with each
increase in reflections. Note that we correctly differentiate
between the contrast introduced by the reflection component
and the image contrast occluded by the reflection component, and detect only the latter. In Figure 11, we show how
our measures respond to temporal recovery of sensitivity. In
this scenario, the observer first adapts to an image with reflections (Figure 10: 3rd row, 3rd column). The adaptation
luminance at time t = 0 is calculated as 5664 cd/m2 by averaging over a 1 visual degree area near the brightest center
part of the reflection. Next, the reflections are removed, leaving the display emission fully visible (Figure 10: 3rd row, 1st
column) while the observer is still adapted to the luminance
of the highlight. Our analysis shows that after 0.8 seconds
nearly all details become visible, and visibility classes are
improved.

been computed off-line using precise final gathering with
importance sampling driven by the bi-directional reflectance
distribution function (BRDF), which was measured for the
actual car display covered with antiglare/antireflection layer.
Our metric predicts higher visibility classes (up to dynamic Complex) along the high contrast left and bottom borders (Figure 12). The navigation information at the center,
however, is not well visible. Thus, our metric predicts a perceived contrast magnitude lower than the informative class,
correctly identifying a bad design practice. Similar to the
previous section, the detail loss increases with increasing
amounts of reflection (Figure 13).
These results also demonstrate how visibility class and detail loss analysis complement each other. In high reflection
cases our metric detects detail loss in the display’s center
region that contains the navigation information (Figure 13:
2nd row, 2nd and 3rd columns). The near-threshold detail loss
measure detects simply the loss of all visible details, even if
they are very close to the visibility threshold. But such low
levels of contrast do not always ensure the legibility of the
display content. In fact, the visibility class map for the same
display region shows the perceived contrast does not reach
even the level of the lowest visibility class (Figure 12 right).
By combining information from both maps, one may choose
to discard the loss of details that were not legible in the first
place.

Figure 12: The simulated car display without reflections
(left) and predicted visibility classes (right).

5. Automotive Application
To demonstrate a possible application, we integrate our
method to a global illumination simulator that models a car
interior containing a navigation panel display [DAK∗ 04].
HDR environment maps captured using an HDR camera
with a fisheye lens mounted on the roof of driving car have
been used to illuminate the virtual car model. Since the car
geometry is static in our application, the precomputed radiance transfer (PRT) technique has been used to efficiently
compute global illumination in the car interior for each environment map frame. The reflectance from the display has

Figure 13: Detail loss (second row) increases with increasing reflections (first row).
To demonstrate the effect of temporal adaptation, we inc 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Tunç Ozan Aydın, et al. / Predicting Display Visibility Under Dynamically Changing Lighting Conditions

vestigate a scenario where the driver subject to direct sunlight (Figure 14) points her gaze to the display inside the car.
At first, the driver’s visual system is adapted to direct sunlight (61, 580 cd/m2 , averaged over 1 visual degree). Due to
the high thresholds at this adaptation level, almost all details
in both the dashboard panel and the display are lost (Figure 15). After nearly 0.8 seconds, the driver slowly regains
her sensitivity and the detail loss on the display decreases.
But details at the darker panel region are still mostly invisible. After a second the image is perceived almost as in perfect adaptation conditions, even though the sensitivity is not
fully recovered yet.

181

One limitation of our work is that we use a single adaptation luminance for the entire image when modeling maladaptation. A better approximation to real adaptation luminance would be found by averaging over a region at each location. However, the exact support size and type of such an
averaging kernel is unknown to us. We also assume that the
display content to be static. A higher perceived contrast can
be achieved by introducing temporal variations to display
content (e.g blinking lights). Our model can be improved by
taking into account the change in contrast sensitivity due to
temporal variance. It would also be interesting to compare
our method to reaction time based visual performance studies such as [RO91] [TJC85].
7. Acknowledgements
The authors would like to thank Thomas Annen for graciously providing the lighting simulation software and
modifying it to adjust our needs. We would also like to
thank Grzegorz Krawczyk, Shinichi Kinuwaki, Rafał Mantiuk, Louis Silverstein, Hendrik P. A. Lensch, Kristina
Scherbaum, and the anonymous reviewers for their constructive comments.
References
[Adr89] A DRIAN W.: Visibility of targets: Model for calculation.
In Lighting Res. Technol. (1989), vol. 21, pp. 181–188.
[AMMS08] AYDIN T. O., M ANTIUK R., M YSZKOWSKI K.,
S EIDEL H.-P.: Dynamic range independent image quality assessment. In Proc. of ACM SIGGRAPH (2008), vol. 27(3). Article
69.

Figure 14: Rendering of the car cockpit. Original HDR image is tone-mapped for displaying purposes.

6. Conclusions
We introduce a method for display visibility analysis that
works under spatially and temporally varying illumination
conditions and accounts for the temporal adaptation of the
observer’s visual system. Our method consists of two parts:
A supra-threshold metric that associates visible contrast
of the display emission with visibility classes, and a near
threshold metric that detects the visible detail loss due to reflections. We extend current methods, that assume the eye
is perfectly adapted at single pixel resolution, by deriving
the components necessary to model the temporal change in
sensitivity. The performance of our method is demonstrated
on an LCD display illuminated by spatially varying ambient
light of different intensity. We also integrate our method to a
global illumination simulator and present visibility analysis
of a car cockpit display under various lighting and adaptation
conditions.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

[AMS08] AYDIN T. O., M ANTIUK R., S EIDEL H.-P.: Extending
quality metrics to full luminance range images. In Human Vision
and Electronic Imaging XIII (2008), vol. 6806, pp. 68060C–10.
[ASMG06] A HUMADA A. J., S AN -M ARTIN M., G ILLE J.: Symbol discriminability models for improved flight displays. In
Proc. of SPIE: Human vision and electronic imaging XI (2006),
vol. 6057.
ˇ ADÍK M., W IMMER M., N EUMANN L., A RTUSI
ˇ
C
[CWNA06]
A.: Image attributes and quality for evaluation of tone mapping
operators. In Proceedings of the 14th Pacific Conference on Computer Graphics and Applications (2006), pp. 35–44.
[DAK∗ 04] D MITRIEV K., A NNEN T., K RAWCZYK G.,
M YSZKOWSKI K., S EIDEL H.-P.: A CAVE system for interactive modeling of global illumination in car interior. In ACM
Symposium on Virtual Reality Software and Technology (VRST
2004) (2004), pp. 137–145.
[Dal93] DALY S.: The Visible Differences Predictor: An algorithm for the assessment of image fidelity. In Digital Images and
Human Vision (1993), Watson A. B., (Ed.), MIT Press, pp. 179–
206.
[DM01] D RAGO F., M YSZKOWSKI K.: Validation proposal for
global illumination and rendering techniques. Computers &
Graphics 25, 3 (June 2001), 511–518.
[Dre07] D REYER D.: Kontrastschwellensimulation für Sichtbarkeitsuntersuchungen an Displays. Dissertation, Technische
Universität München, München, 2007.

182

Tunç Ozan Aydın, et al. / Predicting Display Visibility Under Dynamically Changing Lighting Conditions

0 sec

0.2 sec

0.4 sec

0.6 sec

0.8 sec

1.0 sec

Figure 15: Recovery of temporal adaptation, from t = 0 to t = 1 seconds with 0.2 time steps.

[IFM05] I RAWAN P., F ERWERDA J. A., M ARSCHNER S. R.:
Perceptually based tone mapping of high dynamic range image streams. In Eurographics Symposium on Rendering (2005),
pp. 231–242.

of simulated neural images. Comp. Vision Graphics and Image
Processing 39 (1987), 311–327.
[Wat00] WATSON A.: Visual detection of spatial contrast patterns: Evaluation of five simple models. Optics Express 6, 1
(2000), 12–33.

[KS92] K RANTZ J., S ILVERSTEIN L.: Visibility of transmissive
liquid crystal displays under dynamic lighting conditions. Human
Factors 34, 5 (1992), 615–632.

[WB06] WANG Z., B OVIK A. C.: Modern Image Quality Assessment. Morgan & Claypool Publishers, 2006.

[Lub95] L UBIN J.: Vision Models for Target Detection and
Recognition. World Scientific, 1995, ch. A Visual Discrimination
Model for Imaging System Design and Evaluation, pp. 245–283.

[Wil80] W ILSON H.: A transducer function for threshold and
suprathreshold human vision. Biological Cybernetics 38 (1980),
171–178.

[MDK08] M ANTIUK R., DALY S., K EROFSKY L.: Display
adaptive tone mapping. In Proc. of ACM SIGGRAPH (2008),
vol. 27(3). Article 68.

[YBMS07] YOSHIDA A., B LANZ V., M YSZKOWSKI K., S EIDEL
H.-P.: Testing tone mapping operators with human-perceived
reality. Journal of Electronic Imaging 16, 1 (2007), 013004–1–
14.

[MDMS05] M ANTIUK R., DALY S., M YSZKOWSKI K., S EIDEL
H.-P.: Predicting visible differences in high dynamic range images - model and its calibration. In Human Vision and Electronic Imaging X (2005), vol. 5666 of SPIE Proceedings Series,
pp. 204–214.

[YMMS06] YOSHIDA A., M ANTIUK R., M YSZKOWSKI K.,
S EIDEL H.-P.: Analysis of reproducing real-world appearance
on displays of varying dynamic range. In Proc. of Eurographics
2006 (2006), pp. 415–426.

[MRC∗ 86] M EYER G. W., RUSHMEIER H. E., C OHEN M. F.,
G REENBERG D. P., T ORRANCE K. E.: An experimental evaluation of computer graphics imagery. ACM Transactions on Graphics 5, 1 (1986), 30–50.
[NR66] NAKA K. I., RUSHTON W. A. H.: S-potentials from luminosity units in the retina of fish (cyprinidae). Journal of Physiology 185 (1966), 587–599.
[PFFG98] PATTANAIK S. N., F ERWERDA J. A., FAIRCHILD
M. D., G REENBERG D. P.: A multiscale model of adaptation
and spatial vision for realistic image display. In Proc. of ACM
SIGGRAPH (1998), pp. 287–298.
[PTYG00] PATTANAIK S. N., T UMBLIN J. E., Y EE H., G REEN BERG D. P.: Time-dependent visual adaptation for fast realistic
image display. In Proc. of ACM SIGGRAPH (2000), pp. 47–54.
[RO91] R EA M., O UELLETTE M.: Relative visual performance:
a basis for application. Lighting Research and Technology 23, 3
(1991), 135–144.
[SCG∗ 03] S HARPE R., C ARTWRIGHT C. M., G ILLESPIE W. A.,
VASSIE K., C HRISTOPHER W. C.: Sunlight readability of displays: a numerical scale. In Proc. SPIE (2003), vol. 4826.
[SSB06] S HEIKH H. R., S ABIR M. F., B OVIK A. C.: A statistical evaluation of recent full reference quality assessment algorithms. In IEEE Transactions on Image Processing (2006),
vol. 15, pp. 3440–3451.
[TJC85] TAKEHIRO U., J OE P., C. S. V.: Reaction times to chromatic stimuli. Vision Research 25, 11 (1985), 1623–1627.
[Wat87]

WATSON A.: The Cortex transform: rapid computation
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

