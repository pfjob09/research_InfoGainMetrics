DOI: 10.1111/j.1467-8659.2009.01431.x

COMPUTER GRAPHICS

forum

Volume 28 (2009), number 8 pp. 2057–2067

Autostereoscopic Rendering of Multiple Light Fields
M. Escriv´a, J. Blasco, F. Abad, E. Camahort and R. Viv´o
Departamento de Sistemas Inform´aticos y Computaci´on, Universidad Polit´ecnica de Valencia, Valencia, Spain
{mescriva, jblasco, fjabad, camahort, rvivo}@dsic.upv.es

Abstract
Light fields were introduced a decade ago as a new high-dimensional graphics rendering model. However, they
have not been thoroughly used because their applications are very specific and their storage requirements are too
high. Recently, spatial imaging devices have been related to light fields. These devices allow several users to see
three-dimensional (3D) images without using glasses or other intrusive elements.
This paper presents a light-field model that can be rendered in an autostereoscopic spatial device. The model is
viewpoint-independent and supports continuous multiresolution, foveal rendering, and integrating multiple light
fields and geometric models in the same scene.
We also show that it is possible to examine interactively a scene composed of several light fields and geometric
models. Visibility is taken care of by the algorithm. Our goal is to apply our models to 3D TV and spatial imaging.
Keywords: computer graphics models, light fields, autostereoscopic display
ACM CCS: I.3.3 [Computer Graphics]: Picture/Image Generation–Display algorithms; I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling–Curve, surface, solid, and object representations; I.3.6
[Computer Graphics]: Methodology and Techniques–Graphics data structures and data types

1. Introduction
Light fields are an image-based modelling technique. They
store the radiance that emanates from a synthetic or real
scene, sampled along the four-dimensional (4D) set of oriented lines. Light fields can also be extended with depth
information or geometric proxies to allow a more faithful
reconstruction of the radiance function during the rendering
stage.
Light fields present many advantages. They allow modelling complex objects from the real world without having to
capture their geometry. Their rendering time only depends
on the complexity of the final image and not on the complexity of the scene. They support hybrid models based on
geometry and textures, as shown in this paper. They also
enable the interactive rendering of models that cannot be
visualized by modern graphics processing units (GPUs) in
real time because of their geometric complexity, or because
advanced lighting effects are required. Finally, light fields
are the inherent representation model in digital video and
c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and
Blackwell Publishing Ltd. Published by Blackwell Publishing,
9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main
Street, Malden, MA 02148, USA.

in certain three-dimensional (3D) display devices, specially
volumetric and autostereoscopic devices. An important feature of light-field models is their high storage cost and their
high memory requirements on the rendering system. To alleviate this problem we apply both multiresolution techniques
and foveal rendering. We also show that our models support
continuous levels of detail. To guarantee a minimum interactivity during rendering, we store a working set of images
in the GPU. Later, we load the rest of the images on-demand
until the desired level of detail is achieved.
This paper presents a light-field representation model that
combines all the properties mentioned above. It has been
tested with high-definition light fields of up to 81 920 images
of 512 × 512 pixels. The model also allows storing geometric
information about the images. Our tests show that the storage requirements are reduced between eight and ten times
without reducing the quality of the results. Furthermore, the
extension of the light field with geometric information allows
combining several light-field models with regular geometric
models in the same scene.

2057

2058

M. Escriv´a et al. / Autostereoscopic Rendering of Multiple Light Fields

Our results confirm that this representation model renders interactively high-quality images that used to require
between half a minute and two minutes to generate with advanced global illumination software. These results can be
applied to the generation, storage, compression, transmission, retrieval and rendering of 3D and 4D images for spatial
devices (both autostereoscopic and volumetric) and for 3D
TV. These display devices allow several users to simultaneously perceive correct 3D images without using glasses or
other intrusive elements. This paper also presents an example
of how to use our models in an autostereoscopic display.
The rest of the paper is structured as follows. We start surveying the previous work in light fields and spatial devices.
Then, we describe our representation model and its rendering
algorithms. Next, we explain how to extend our model with
geometric information, its advantages and how it modifies
the rendering algorithm. Later we show that this information allows light-field models and geometry to be correctly
integrated in the same scene. Finally, we explain how to render our models in an autostereoscopic display. The last two
sections present our results, conclusions and future work.

2. Previous Work
The light field is formally defined as a function that captures
the radiance R that flows through every point (x, y, z) in
every possible direction (θ, φ) of 3D space [GGSC96, LH96].
This definition is also equivalent to the plenoptic function
described by Adelson and Bergen [AB91]. If wavelength
and time are fixed, then the light field becomes a 5D scalar
function R(x, y, z, θ, φ) that gives a radiance value for each
position and direction. The support of that function is the set
of rays L(x, y, z, θ , φ) of 3D space.
This support is reduced to 4D if an occlusion-free 3D
volume is modeled, because the radiance does not change
along the rays. Therefore, the support of R becomes the set
of oriented lines in 3D space. The radiance function R has to
be sampled to build a discrete model of the light field. This
requires the oriented line space to be described, using either
a structured or an unstructured parametrization.

2.1. Light-field models
Structured parametrizations sample the rays using regular
patterns in their parameters. There are two types of structured parametrizations: planar and spherical. Planar models
parametrize each line in the light field with two intersection
points P and Q with two parallel planes (see Figure 1(a)).
Therefore, a line L is defined by parameters (s, t, u, v).
Each pair of planes is known as a light slab. At least six
light slabs are required to model a complete object [LH96].
This parametrization was inspired by holography [HH92].
On the other hand, Gortler et al. used geometric proxies to
improve the reconstruction of the continuous radiance func-

Figure 1: Two parametrizations of the lines that define the
light-field support.

tion [GGSC96]. Finally, Sloan et al. studied how the number
of samples, the number of slabs, the rendering speed and
the quality of the images are related for planar light fields
[PPS97].
There are two types of structured spherical parametrizations: two spheres, and direction and point [CLF98]. One advantage of both models is that they produce a (quasi)uniform
sampling of the line space where the light field is defined.
Figure 1(b) shows the direction and point parametrization,
where the ray L is given by its direction D and its intersection
P with the support plane of circle C. In this work we use this
last parametrization, described in detail in Section 3.
One advantage of these parametrizations with respect to
planar parametrizations is that the latter needs several slabs to
enclose the object to model. This produces discontinuities in
the radiance function and noticeable artefacts in viewpoints
where more than one slab is visible. Furthermore, spherical
parametrizations present interesting hierarchical multiresolution properties.

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

M. Escriv´a et al. / Autostereoscopic Rendering of Multiple Light Fields

Another alternative parametrization is surface light fields
[WAA∗ 00, CBCG02] that use a surface that approximates
the object closely as a sampling surface. A similar approach to these techniques is view-dependent texture mapping [DTM96], where only a small set of sampling directions
are used to capture the appearance of the object.
Finally, unstructured parametrizations do not impose restrictions on the viewpoints where the images of the objects
are captured. Several authors have proposed techniques based
on approximate geometric models [BBM∗ 01], some type of
depth information [SVSG01] and a measurement of the focus
of the points in the object [TN05].

2059

In the following sections we present a light-field implementation that is compatible with these devices.

3. Modelling and Rendering Light Fields
We have developed a system that supports planar, spherical
and unstructured light-field models [BEA∗ 08]. The system
abstracts the common features of all three types and defines a
compact interface to interact with it. We have concluded that
the spherical model is the best suited to interactively examine
isolated objects, because its representation does not depend
on the camera parameters. Planar models are adequate for
viewpoints located on one side of the object. Unstructured
models are appropriate for models captured with a hand-held
video cameras that freely samples the scene.

2.2. Spatial visualization
The use of light fields in spatial display devices is appropriate, given that they capture the light at every 3D point in
every direction of a 3D volume. Spatial display devices allow
several users to perceive a real 3D image at the same time
without using glasses. Light fields store the images that later
will be shown in each view of the spatial device.
Isaksen et al. propose a reparametrizable model of the light
field [IMG00]. It allows extracting view-dependent images to
compose integral images of the light field. Isaksen et al. show
how these integral images can be displayed using lenticular
display devices. A lenticular device is a spatial device that
uses a matrix of lenses that redirect the light coming out of
the screen. This allows several users to see different images
for each eye.
A similar technology uses several projectors to project
multiple views on a lenticular screen [YCH∗ 05]. There are
integral video capture systems that record high-resolution
video using a spherical set of cameras. The video can be
used to create content for that technology [KOK∗ 06]. The
problem of lenticular displays is that they present very few
views, typically less than ten. Back-projected holographic
displays were design to alleviate this problem [BFB∗ 05].
They are expensive, because they use tens of projectors, but
they allow near 45 views to be displayed, five times more
than a lenticular display.
Other spatial display devices use parallax barriers made
of two overlapped LCD panels [PPK00]. The front panel
displays alternating black and transparent vertical lines. The
back panel displays a stereo pair, where each eye sees a
different image through the columns of the front LCD. These
devices do not require using special glasses, but they require
some kind of user location device.
Finally, volumetric displays show simultaneously several
views of a model projecting them onto a spinning surface
[FNH∗ 02, JMY∗ 07]. The device synchronizes the screen rotation and the proper projection of the different views.

This section describes the spherical light-field representation and its multiresolution, continuous level of detail and
foveal rendering features.

3.1. Spherical representation model
In this representation each ray is parametrized by its direction
D = (θ, φ) and by its intersection P = (u, v) with a plane
orthogonal to D (see Figure 1(b)). To generate a light field
from a geometric model, it is scaled and fitted into the unit
sphere. Then, lines that intersect with the sphere and their
radiance are sampled, thus obtaining the images of the light
field [CLF98].
In order to build this model, it is necessary to discretize
the set of directions (θ, φ) in 3D space. This is done first by
triangulating the unit sphere. Then, the centre of each triangle
defines a sample direction. Each sample thus approximates
the pencil of directions that starts at the centre of the sphere
and intersects with the triangle area.
Every line parallel to a directional sample that intersects
the sphere is discretized to sample the light-field support
lines. These are the lines that intersect the great circle C
shown in Figure 1(b). In practice, the light-field radiances are
sampled by rendering the object, using a parallel projection
from each viewpoint defined by the directional samples. Each
rendered image is associated to the pencil of directions that
approximate the directional sample and it is stored as a texture
linked to its corresponding triangle.
Our rendering algorithm is based on the Lumigraph’s algorithm [GGSC96]. It is shown in Figure 2. Initially, we compute the pencils that intersect the view volume by locating
this volume inside the tessellated sphere. If necessary, both
the sphere and the volume can be scaled. Both of them should
be centred at the origin. The intersected pencils correspond
to the triangles of the sphere visible from that viewpoint (see
Figure 2(a)). Next, each triangle is rendered, textured with its
associated image in the light field. The texture coordinates

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2060

M. Escriv´a et al. / Autostereoscopic Rendering of Multiple Light Fields

Figure 2: Rendering algorithm for spherical light fields.

are computed by projecting its vertices onto the support plane
of the texture (shown in red in Figure 2(b)). Figure 3(a) shows
a light field rendered using this algorithm. The triangles used
in the rendering process are shown on top of the image.

3.2. Multiresolution model
To build a multiresolution model, we consider separately the
positional and the directional dependencies of the light field.
The positional sampling of the radiance is done by creating
textures from parallel projections. To support multiresolution
in this space, textures are stored as mipmaps.
In the directional space, we build a quasi-uniform sampling starting by approximating the sphere with an icosahedron (Figure 4(a)). Next, each of the 20 initial triangles is
divided into four subtriangles and its vertices are reprojected
onto the surface of the sphere (Figure 4(b)). This generates
80 triangles and 80 directional samples, each passing through

Figure 3: Example of foveal rendering of a light field: triangles are subdivided deeper in the foveal area.
the centre of one triangle. This process is applied recursively
to obtain 320, 1280, etc., samples. Seams between adjacent
triangles may appear when a model with few directional samples is rendered (see the head of the gargoyle in Figure 5(a)).
With our representation these problems can be overcome by
using interpolation or higher levels of detail.
3.3. Continuous level of detail
The previous discretization enables a logic and simple multiresolution representation. On the other hand, popping may
appear when changing levels of detail. This problem can be
reduced or solved using smooth transitions between different
levels of detail, both in the positional and in the directional domains. A simple interpolation between consecutive

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

M. Escriv´a et al. / Autostereoscopic Rendering of Multiple Light Fields

2061

the object surface to the image plane. Without compression
this increases the required storage between 50 and 100%
depending on the pixel and depth resolution. However, our
tests show that directional samples can be reduced up to 16
times (two levels of detail) without affecting the quality of
the result.

Figure 4: Partitioning of 3D directional space (each triangle represents a pencil of directions): (a) initial partition
based on the icosahedron; (b) first subdivision computed by
dividing each triangle on the left in four subtriangles.

mipmap levels is enough to reduce the artefacts in the positional domain. In the directional domain, we linearly interpolate between the triangle associated to a given sample
and the triangles associated to its children, using α-blending.
Figure 5 shows an example.
3.4. Foveal rendering
Our system also supports foveal rendering by reducing the
level of detail in areas located further from the centre of
the display [AR03]. This allows a speed-up of the rendering
affecting visual quality only in the image periphery. Given an
angle that defines the foveal area, the algorithm computes the
angle between the view direction and each pencil direction.
If this angle is less than the foveal angle, the triangle that
represents the pencil is subdivided, using its four children
instead. If the angle is greater, the triangle is shown without
further subdivision.
Triangles that are outside of the foveal area are rendered
using a lower level of detail (see Figure 3). This technique
can be extended to more than one foveal area and can be
applied to larger displays and to theater and IMAX screens.
In these displays, the user usually pays more attention to the
centre of the image and does not notice the lower rendering
resolution near the borders of the screen.
4. Use of Depth Information
This information allows the storage costs of the light field
to be substantially reduced without affecting the rendering
quality. It also allows the light fields to be efficiently integrated as another primitive of a conventional rendering
engine [DEA∗ 06].
The depth information can be provided mainly using two
methods: a geometric proxy that approximates the object
using a simplified polygonal model, or with a set of depth
maps that associates a depth value to each sample in the
light field. Our representation stores a depth map for each
image in the light field. For each pixel in the image, the
map stores the distance (normalized between [0, 1]) from

Figure 6 clearly shows the advantage of using a light-field
model extended with depth information. The light field used
to generate the image on the left has 16 times more images
than the light field used to render the image on the right. The
quality of both images is similar, as can be seen comparing
both images. In fact, when zooming into both images, we can
conclude that the image on the left is blurrier. The image in
the centre shows the result of using the same light field used to
render the image on the right, but without depth information.
It can be seen that there is not enough information to properly
reconstruct the light field.
Both the size of the images and the number of images in
each light field have to be taken into account to compare the
storage requirements of the best two options. Each image in
the extended light field may require up to twice the space
of an image in the light field without depth information. In
the extended model each pixel stores both the color information (24 or 32 bits) and the depth information (8, 24, 32,
or typically, 16 bits). On the other hand, the light field without depth information requires up to 16 times more images.
Taking into account all these sizes, we can conclude that the
light field with depth information requires about one-tenth of
the storage of the light field without depth information.

4.1. Rendering with depth information
This algorithm is different from the one presented in previous sections because it renders the triangles on top of the
surface of the object. It intersects the rays that pass through
the vertices of the triangle with a 3D reconstruction of the
corresponding depth map (see Figure 7). The coordinates of
that intersection are then used to compute more precisely
the texture coordinates and the approximate location of the
triangle on the object surface. In practice, this implies a reconstruction of that surface.
The reconstruction presents problems when the depths associated to each vertex of a triangle are very different, and
when one of the rays of a triangle does not intersect with the
object. The triangle is recursively subdivided in those cases.
Three tolerance values control the subdivision process: the
first one subdivides a triangle if it is greater than a given
number of pixels, the second one subdivides a triangle whose
vertices have very different associated depth values, and the
third one limits the subdivision process to avoid subdividing
triangles smaller than one pixel.
Our tests show that a value of 40 pixels for the first tolerance produces good results. Smaller values require too many

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2062

M. Escriv´a et al. / Autostereoscopic Rendering of Multiple Light Fields

Figure 5: Light fields rendered with continuous levels of detail 2, 2 13 and 3.

Figure 6: A model rendered with the following directional resolutions: (a) 20 480 without depth; (b) 1280 without depth (d)
1280 with depth. The first two images have been rendered with interpolation, and the third without interpolation. The image in
the middle suffers from ghosting.
subdivisions and increase rendering time without producing
perceptible quality improvements. We use a value of 0.05 for
the second tolerance parameter (with respect to a normalized
range of depths between [0, 1]). This value produces better
results in those edges of the object that do not belong to its
silhouette. Finally, the third tolerance parameter is set to one
because it makes no sense to draw triangles smaller than a
pixel. Larger values produce bites along the silhouettes of
the objects.
The images shown in this paper have been rendered using
the parameters given in the previous paragraph. Furthermore,
we use the depth information to compute more precisely
the texture coordinates of the triangles. This reduces the
seams that appear at shared edges of adjacent triangles (see
Figure 5(a)).

Finally, rendering the triangles on the object surface allows multiple light fields and regular geometric objects to be
integrated in the same scene.
4.2. Integration of multiple light fields and geometric
models
A simple approach to integrate multiple light fields in a scene
without using depth information is to store more than one
image per directional sample. Each image is located at a
different distance along each direction. However, this simple solution may produce noticeable rendering artefacts if
models overlap (see Figure 8).
This problem is solved using the depth information to
compute the exact vertex position of the rendered triangles.

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2063

M. Escriv´a et al. / Autostereoscopic Rendering of Multiple Light Fields

on graphics hardware. Both geometric model and texture
sizes usually grow faster than the available memory in GPUs.
Historically, several algorithmic solutions have been proposed to overcome the gap between the hardware capabilities
and the data complexity of the scenes. Examples of those solutions are (i) multiresolution rendering, (ii) view volume
culling, (iii) occlusion-based selection and (iv) image-based
rendering.
The culling methods can easily be combined with multiresolution models to select the appropriate level of detail
according to criteria like viewpoint location. However, multiresolution schemes usually increase the memory footprint
of the models.
Figure 7: 2D analogy of how depth maps are used while
rendering the light-field model: during the rendering stage,
rays are cast from the viewpoint through the vertices of the
triangle; instead of intersecting with plane P and obtaining
points A, B, C, . . . , the rays intersect at points A , B , C , . . . ,
on the depth map.

For example, consider a light field with an image resolution
of 512 × 512 pixels and 20 480 directions. The uncompressed
model requires about 40 GB of storage. A 10:1 compression
ratio can be achieved using a lossless algorithm (assuming
that most of the images have a significant number of pixels
set to the background color, see Table 1). Up to a 40:1 ratio
(depending on the amount of background color) could be obtained using a lossy compression algorithm, assuming image
quality losses.
Both an efficient cache and compression algorithms are
required to properly manage the computer memory hierarchy to obtain a responsive application. Dealing with such
a huge amount of data imposes severe requirements on the
architecture of the application.
5. Results

Figure 8: The cube is closer than the tetrahedron along
the green projector; however, the image plane that represents the cube is behind the tetrahedron’s image plane.
Therefore, the tetrahedron will wrongly appear on top of the
cube in the final image.

This information allows the triangles to be rendered on the
surface of the original object. That way the visibility of the
scene is properly solved by the GPU.
This algorithm also allows light field and geometry to
be integrated in the same scene. Thus, light-field models
become a regular graphics primitive in a geometric scene.
This also allows attaching labels or annotations on the lightfield models.

4.3. Large volume storage light-fields
Recent advances in the fields of 3D design, 3D scanning of
real objects and simulation have increased the requirements

To generate the light-field models shown in this paper,
we have used three geometric models of the AIM@Shape
Project, rendered using Blender and Yafray. The models
called Gargoyle, Grog (the warrior) and Lion are shown
in Figure 9b and their storage requirements are shown in
Table 2. To render the images that compose the light field,
Table 1: Space required to store both the color images and the
depth maps of one of our data sets.

Number of directions
Res. 256 × 256
20 480
5120
1280
Res. 512 × 512
20 480
5120
1280
Res. 1024 × 1024
20 480
5120
1280

Uncompressed

Compressed

10 GB
2.5 GB
640 MB

1.3 GB
336 MB
85 MB

40 GB
10 GB
2.5 GB

6.4 GB
1.6 GB
446 MB

160 GB
40 GB
10 GB

25.5 GB
7.1 GB
1.5 GB

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2064

M. Escriv´a et al. / Autostereoscopic Rendering of Multiple Light Fields

Figure 9: Simultaneous rendering of multiple light-field models.
Table 2: Characteristics of the models shown in this paper: generation times are shown assuming a single PC, and model sizes are shown
without compression, except in disk, where images are stored in PNG format.

Geometric model

Light-field model
Image size

Name
Gargoyle
Warrior
Lion

Number of polygons

Time per image

Directional samples

Total time

Disk

Memory

Storage maps

1 726 372
1 752 348
1 311 956

122 seg
64 seg
48 seg

1 280
1 280
20 480

43h 22m
22h 45m
11d 13h 52m

201 MB
161 MB
3.42 GB

1.25 GB
1.25 GB
20.00 GB

640 MB
640 MB
10.24 GB

we have used ambient occlusion, Cook-Torrance specular
illumination and eight samples per pixel. The positional resolution of the generated models is 512 × 512 pixels and
the directional resolution is 1280 directions, except the lion
model that has 20 480 directions. A render farm made of up
to 100 computers in the teaching labs of our department was
used to render the models. The number of polygons in each
model has been limited, to allow each light-field model to be
built in a few days, during weekends. Table 2 shows the time
required by a computer to build each light-field model and
their sizes.

used to render the light fields. Figure 9 shows a scene with
multiple light fields. The gargoyle model has been instantiated three times to model the scene shown in Figure 9(a).
The system cache allows the same images to be shared by all
the instances of the same light field. Figure 9(b) shows three
different light field models in the same scene: two copies of
the gargoyle model, a lion and a warrior. Figure 10 shows
how the visibility between the geometric objects and the
light-field models, and between different light-field models,
is properly solved. Finally, Table 3 shows the size of those
scenes and its rendering speed.

Videos that show how our models are rendered both
in conventional displays and in autostereoscopic displays can be downloaded from the web page http://sigmarron.dsic.upv.es/ALF/papers/cgf2009/.

This shows that our system is a competitive alternative
to interactive rendering of complex objects using a GPU.
When geometry and illumination are so complex that it is not
possible to render the object in real time with the available
hardware, it may be possible to generate its light field off-line
and render it in real time using the GPU. Furthermore, future
improvements on the GPU technology will speed up both the
rendering of geometric complex models and our light-field
models.

5.1. Monoscopic rendering
A 2.4 GHz Intel Core 2 Quad with 2 GB of RAM and an
NVIDIA GeForce 8800 GT with 512 MB of memory was

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

M. Escriv´a et al. / Autostereoscopic Rendering of Multiple Light Fields

2065

Figure 10: These scenes with geometric and light-field models show how the visibility can be properly solved.
5.2. Autostereoscopic visualization
Currently, spatial display devices accept two types of input.
The first one is a geometric description of the scene using a
standard format (e.g. OpenGL). These devices provide wrappers or special drivers that internally generate the required
views for autostereoscopic visualization. The second type of
input format is a set of images, one per available view.
Light-field models are convenient for this type of devices
because they allow the required views from their precomputed images to be easily obtained. On the other hand, very
few implementations can generate geometry from light fields.
Our system can provide both, and therefore can be used
in any autostereoscopic display device using the described
technologies.
We have used a 20 Philips WOWvx autostereoscopic display to test our light-field model. This monitor uses lenticular
technology and provides nine views with horizontal parallax
and a touch screen [BP00]. It also allows two modes of operation: rendering of OpenGL models and rendering of images
with depth information attached. The result is an integral
image made of nine views that covers a small field of view.
Figure 11 shows a photograph of the display showing a light
field. Due to the aperture of the lens of the camera used to
take the photograph, double images corresponding to different views can be seen.

6. Conclusions and Future Work
We have presented a spherical light-field model with features such as multiresolution, continuous levels of detail and
foveal rendering. The model provides interactive rendering
of complex objects that cannot be otherwise rendered using
traditional rendering techniques.
We have added depth information to the model, which
allows their storage requirements to be reduced between eight
and ten times, without affecting the quality of the results. We

Figure 11: Close-up of a photograph of a light field displayed on the autostereoscopic device: a double image can
be seen because more than one of the display views is visible.

also show that for the first time we can render scenes made of
several light fields and regular geometric objects, correctly
solving visibility.
Finally, we have shown how to use our light-field models
in autostereoscopic display devices. The results show that
these models are suitable for lenticular displays and that nine
views are insufficient to give a good 3D sensation to multiple
users. We are studying other non-lenticular displays with a
greater number of views.
We are also working on the implementation of planar and
unstructured light fields. Specifically, our goal is to capture light fields with an off-the-shelf hand-held camera and
converting them to a structured and potentially non-uniform

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2066

M. Escriv´a et al. / Autostereoscopic Rendering of Multiple Light Fields

Table 3: Storage requirements of the scenes shown in Figures 9 and 10: the lion model uses only 1280 of the 20 480 directional samples
available.

Scene

Description

Size in disk

Memory size

Speed

Scene 1
Scene 2

Three gargoyles in circle
Two gargoyles, a lion and a warrior

841 MB
2.44 GB

1.87 GB
5.62 GB

4.2 FPS
5.3 FPS

Scene 3
Scene 4
Scene 5

Three gargoyles and a sphere
Gargoyle and teapot intersecting
Lion and gargoyle interpenetrating

841 MB
841 MB
974 MB

1.87 GB
1.87 GB
3.75 GB

4.2 FPS
32 FPS
13 FPS

representation. We are working on non-uniform models and
on conversion algorithms.

VR 2005 Workshop on Emerging Display Technologies
(March 2005).
[BP00] BERKEL C. V., PARKER D. W.: Austostereoscopic
display apparatus. U.S. Patent No. 6,118,584, 2000.

Acknowledgements
We want to thank the reviewers for their comments and suggestions. We also want to thank the staff and direction personnel of our department for allowing us to use the teaching
labs (more than 100 computers) to generate the light fields
shown in this paper. Jos´e Luis Hidalgo helped us with the
generation.
The Lion model is provided courtesy of INRIA. The Gargoyle and Grog models are provided courtesy of VCG-ISTI
by the AIM@SHAPE Shape Repository. Finally, our work
has been partly supported by the Vicerrectorado de Investigaci´on of the Universidad Polit´ecnica de Valencia and by the
Project TIN2005-08863-C03-01 of the Ministry of Science
and Innovation of Spain.

References
[AB91] ADELSON E. H., BERGEN J. R.: The plenoptic function and the elements of early vision. Computational Models of Visual Processing (1991), 3–20.
[AR03] ASHDOWN M., ROBINSON P.: The escritoire: A personal projected display. In Proceedings of WSCG’2003
(2003), pp. 33–40.
BUEHLER C., BOSSE M., MCMILLAN L., GORTLER
[BBM∗ 01]
S., COHEN M.: Unstructured lumigraph rendering. In SIGGRAPH ’01 (2001), ACM, pp. 425–432.
[BEA∗ 08] BLASCO J., ESCRIVA´ M., ABAD F., QUIRO´ S R.,
CAMAHORT E., VIVO´ R.: A generalized light-field API
and management system. In Proceedings of WSCG’2008
(Plzen – Bory, Czech Republic, 2008).
BALOGH T., FORGA´ CS T., BALET O., BOUVIER
[BFB∗ 05]
E., BETTIO F., GOBBETTI E., ZANETTI G.: A scalable holographic display for interactive graphics applications. IEEE

[CBCG02] CHEN W.-C., BOUGUET J.-Y., CHU M. H.,
GRZESZCZUK R.: Light field mapping: Efficient representation and hardware rendering of surface light fields. In SIGGRAPH 2002 Conference Proceedings (2002), Hughes J.
(Ed.), Annual Conference Series, ACM Press/ACM SIGGRAPH, pp. 447–456.
[CLF98] CAMAHORT E., LERIOS A., FUSSELL D.: Uniformly
sampled light fields. In Proc. Eurographics Rendering
Workshop ’98 (1998), pp. 117–130.
[DEA∗ 06] DOMINGO A., ESCRIVA´ M., ABAD F., VIVO´ R.,
CAMAHORT E.: Introducing extended and augmented light
fields for autostereoscopic displays. In Proceedings of
3rd Ibero-American Symposium in Computer Graphics
(2006), pp. 64–67.
[DTM96] DEBEVEC P. E., TAYLOR C. J., MALIK J.: Modeling and rendering architecture from photographs: a hybrid geometry- and image-based approach. In SIGGRAPH
’96: Proceedings of the 23rd annual conference on Computer graphics and interactive techniques (New York, NY,
USA, 1996), ACM, pp. 11–20.
[FNH∗ 02] FAVALORA G. E., NAPOLI J., HALL D. M., DORVAL
R. K., GIOVINCO M. G., RICHMOND M. J., CHUN W. S.: 100
million-voxel volumetric display. In Proceedings of the
SPIE (2002), vol. 4712, pp. 300–312.
[GGSC96]
GORTLER S. J., GRZESZCZUK R., SZELISKI R.,
COHEN M. F.: The lumigraph. In Proc. SIGGRAPH ’96
(1996), pp. 43–54.
[HH92]
HAINES K., HAINES D.: Computer graphics for
holography. IEEE Computer Graphics and Applications
12 (January 1992), 37–46.
[IMG00] ISAKSEN A., MCMILLAN L., GORTLER S. J.: Dynamically reparameterized light fields. In Proc. SIGGRAPH ’00 (2000), pp. 297–306.

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

M. Escriv´a et al. / Autostereoscopic Rendering of Multiple Light Fields
∗

[JMY 07]
JONES A., MCDOWALL I., YAMADA H., BOLAS
M., DEBEVEC P.: An interactive 360◦ light field display. In
SIGGRAPH ’07 (2007), ACM.
[KOK∗ 06] KOIKE T., OIKAWA M., KIMURA N., BENIYAMA F.,
MORIYA T., YAMASAKI M.: Integral videography of highdensity light field with spherical layout camera array. In
Proceedings of SPIE (2006).
[LH96] LEVOY M., HANRAHAN P.: Light field rendering. In
Proc. SIGGRAPH ’96 (1996), pp. 31–42.
[PPK00] PERLIN K., PAXIA S., KOLLIN J. S.: An autostereoscopic display. In Proc. SIGGRAPH ’00 (2000), pp. 319–
326.
[PPS97] PETER-PIKE SLOAN MICHAEL F. COHEN S. J. G.:
Time critical lumigraph rendering. In Proc. 1997 Symposium on Interactive 3D Graphics (1997).
[SVSG01] SCHIRMACHER H., VOGELGSANG C., SEIDEL H.,
GREINER G.: Efficient free form light field rendering. In

2067

Proceedings of Vision, Modeling, and Visualization 2001
(2001).
[TN05] TAKAHASHI K., NAEMURA T.: Unstructured
light field rendering using on-the-fly focus measurement. ICME 2005. IEEE International Conference on Multimedia and Expo (July 2005), 205–
208.
˜
WOOD D. N.,
AZUMA D. I., ALDINGER
[WAA∗ 00]
K., CURLESS B., DUCHAMP T., SALESIN D. H.,
STUETZLE W.: Surface light fields for 3d photography. In SIGGRAPH ’00: Proceedings of the 27th
annual conference on Computer graphics and interactive techniques (New York, NY, USA, 2000),
ACM Press/Addison-Wesley Publishing Co., pp. 287–
296.
[YCH∗ 05] YANG R., CHEN S., HUANG X., LI S., WANG L.,
JAYNES C.: Towards the light field display. IEEE VR 2005
Workshop on Emerging Display Technologies (March
2005).

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

