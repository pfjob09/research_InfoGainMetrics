Pacific Graphics 2009
S. Lee, D. Lischinski, and Y. Yu
(Guest Editors)

Volume 28 (2009), Number 7

SecondSkin: An interactive method for appearance transfer
A. van den Hengel† , D. Sale & A. R. Dick
University of Adelaide, Australia

Abstract
SecondSkin estimates an appearance model for an object visible in a video sequence, without the need for complex
interaction or any calibration apparatus. This model can then be transferred to other objects, allowing a nonexpert user to insert a synthetic object into a real video sequence so that its appearance matches that of an existing
object, and changes appropriately throughout the sequence. As the method does not require any prior knowledge
about the scene, the lighting conditions, or the camera, it is applicable to video which was not captured with this
purpose in mind. However, this lack of prior knowledge precludes the recovery of separate lighting and surface
reflectance information. The SecondSkin appearance model therefore combines these factors. The appearance
model does require a dominant light-source direction, which we estimate via a novel process involving a small
amount of user interaction. The resulting model estimate provides exactly the information required to transfer the
appearance of the original object to new geometry composited into the same video sequence.
Categories and Subject Descriptors (according to ACM CCS): I.3.6 [Computer Graphics]: Methodolgy and
Techniques—Computing Methodologies; I.4.8 [Image Processing and Computer Vision]: Scene Analysis —
Shading

1. Introduction
The insertion of computer generated (or synthetic) objects
into video sequences captured using real cameras is a critical
aspect of many video processing operations. These include
the visualisation of otherwise impossible scenarios such as
dinosaurs roaming the earth, but also less spectacular effects
such as the removal of unwanted objects from video.
Realistically inserting synthetic objects into a real video
requires a particular set of information about the video sequence. Knowing the path of the camera, and at least partial
scene geometry, is essential if the synthetic object is to be
realistically embedded in the scene. Similarly, some information about scene illumination is required if the synthetic
object is to be convincingly rendered into the sequence.
One approach to recovering the required illumination information is to seek a full description of the lighting conditions at the time the video was taken. This then allows a syn-

† This research has been funded by the Australian Research Council and the South Australian Premier’s Science and Research Fund
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and 350
Main Street, Malden, MA 02148, USA.

thetic material applied to a synthetic object to be realistically
rendered. However this typically requires the filming of specific calibration objects with known shape and reflectance
properties, which may be impractical for some scenes or if
the video has already been shot.
An alternative approach is to use the information available
from objects that are already visible in the video sequence
to inform the rendering process. Toward this goal SecondSkin presents an interactive method by which a user may estimate specific lighting and material properties from a video
sequence without the use of additional apparatus.
Given the interplay of lighting and material properties in
the image formation process it is not possible to fully recover
either from a general image sequence alone. Under certain
assumptions, however, it is possible to recover enough information to render a synthetic object with a material that is already present in the viewed scene. This does not require absolute information about either the lighting or material properties, but rather an understanding of their combined effect.
We call the process of transferring the combined lighting and

1736

A. van den Hengel D. Sale & A. R. Dick / SecondSkin: An interactive method for appearance transfer

Figure 1: Frame from a video of a bronze sculpture (left) whose appearance is captured (middle) and applied to synthetic object
inserted into the video (right).

material properties from one object to another appearance
transfer. An example is shown in Figure 1.
Appearance transfer using SecondSkin proceeds as follows:
• Estimate the camera path and projection properties using a
camera tracker such as Voodoo [TB] or PFTRACK [The]
• Create a 3D model of relevant parts of the viewed scene
(Section 2.3)
• Interactively estimate the predominant light-source direction (Section 2.3)
• Solve for the combined lighting and material properties of
an object in the scene (Section 3)
• Render the required synthetic object into the original sequence using a rendering engine such as that in
Blender [Ble].
This process has a number of steps, but is simple, robust and
flexible. The first and last steps are required by any process
to insert synthetic geometry into an existing video sequence.
SecondSkin has been implemented as a Blender plugin.
1.1. Related work in lighting estimation
As demonstrated by Nakame et al. [NHIN86], identifying
the predominant light source direction in a scene is critical to the realistic insertion of synthetic objects into real
images. A number of methods have been developed to do
this in particular circumstances. Cao et al. [CF07] for example, describe a method for calibrating a camera and determining light source direction from an image with known
scene points and corresponding shadow points. Weber and
Cipolla [WC01] describe a method for determining light
source direction on the basis of known scene geometry without requiring the presence of shadows in the image set, but
subject to a Lambertian reflectance constraint.
The lighting conditions under which a video sequence was
captured may be recovered through the use of a specific object which is inserted into the scene. Light-probe images, for
example, may be acquired by recording the reflection of the

scene in a mirrored sphere using high-dynamic range imaging techniques [DM97, TSE∗ 04]. These images may then
inform the rendering process in order to achieve a realistic
result. However they require the photographer make specific
preparations prior to capturing the sequence. We avoid this
constraint.
One approach which does not require the addition
of an additional object to the scene is that of Sato et
al. [SSI99] which is able to identify illumination distribution by analysing the shadows cast by an object of known
shape upon another of known reflectance properties.
1.2. Related work in reflectance estimation
A bidirectional reflectance distribution function (BRDF),
parameterised by incident and reflected direction of light,
provides a complete description of surface reflectance. A
number of parametric BRDFs such as the Blinn [Bli77],
Phong [Pho75] and Ward [War92] models have also been
developed. SecondSkin is capable of using any parametric
BRDF that can be implemented in a raytracer in order to effect appearance transfer. We use the term appearance function to refer to functions from both classes.
Several techniques for recovering the reflectance properties of a material have been developed which rely on the use
of various forms of calibrated apparatus such as the gonioreflectometer [MCAS90, KMG96]. More recent approaches
do not require the placement of the object to be measured
in a specific apparatus, but rather analyse images of the objects in situ [YDMH99, WSL04]. These techniques do, however, require information about the scene lighting and camera properties.
An alternative approach has been proposed by Drettakis et al. [DRB97] by which structure-from-motion analysis [PGV∗ 04] (also known as camera tracking) is used to
recover the camera properties at each frame, and an interactive process is used to model scene geometry [FRL∗ 98].
The scene is then imaged multiple times with a single light
source placed at a different location in each image. By analysis of the images the authors recover surface reflectance for
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

A. van den Hengel D. Sale & A. R. Dick / SecondSkin: An interactive method for appearance transfer

multiple objects in the scene. This approach is similar to that
presented here, except that it requires the addition of a movable light source to the scene.
Loscos et al. [LDR00] have extended Drettakis et al. to
allow the lighting and geometry of a scene to be modified.
The method they propose shares the requirement of multiple
reflectance images of the scene, however. The SecondSkin
approach is similar in that it requires the application of a
camera tracker to the original footage, and that some modelling of the scene is required. The SecondSkin approach is
more flexible in that it does not require the use of a set of
reflectance images, but as a result less information about the
material properties of the scene is recoverable.
Closer to the SecondSkin approach, Boivin and Gagalowicz [BG01] describe a method by which material properties
may be estimated on the basis of known scene geometry.
Their method assumes that a full scene model is available,
but may be adaptable to the case where only part of the scene
is modelled. A process of estimating the BRDF of each material in the scene is carried out on the basis of the scene
geometry and known lighting parameters.
Both SecondSkin and the MatCap [mat] tool included in
ZBrush attempt to estimate combined material and lighting
parameters in order to transfer the appearance of an material from an image set to a synthetic object. MatCap requires
that the user provide a set of surface normals, the predominant lighting direction, and approximate material properties
in order to perform this task.
Although the technical details are not available, it would
appear that MatCap models the appearance of a material
by interpolating in 2D between patches sampled from the
source image. When adding a source patch the user is required to provide an estimate of the surface normal at the
patch location. The appearance properties associated with
normals not sampled may then be calculated by interpolating between known patches on the basis of their normals.
This approach is fundamentally 2-dimensional, and limited
to synthesis from and transfer to images rather than video.
1.3. Contribution
This paper thus makes the following contributions:
• An interactive method for estimating the direction of the
predominant light source in a scene from a set of images.
The method requires no prior knowledge of the characteristics of the scene, or the relationship between the camera and the light source. The method is fast enough to
support user interaction, and requires minimal user input
while still allowing full user control in order to achieve
the required estimate.
• A method for estimating the appearance of an object visible in a video sequence. The method does not require any
particular properties of the video sequence and thus can
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

1737

be applied to video captured for other purposes, or in situations in which a complete estimate of scene or lighting
conditions is not practical. The only assumption is that the
scene has a dominant light direction, and that surface appearance can be modelled to a close approximation with
an available BRDF.
These contributions combine to produce a system by which
a new object can be realistically rendered into an existing
video sequence, by taking on the appearance properties of
an object that already exists in the sequence.
2. Estimating light source direction
Rather than attempt to extract separate lighting and material
information from the image set, these will be estimated in
tandem through the process described in Section 3. The dependency of the position of specular highlights upon the predominant lighting direction, however, requires that we extract this information separately.
The light source direction estimator we describe requires
that the light source be far enough away, relative to the scale
of the scene, to be approximated as a single directional light.
This is often the case [Lov97], as human observers are in
practice unlikely to identify small deviations in the distance
to the dominant light source across an image. For this reason, the approach is not suitable for soft-lit indoor environments (lighting from large fluorescent lighting for example)
or overcast days.
In order to make the process as simple as possible for the
user, all that we require is a model of the objects which cast
and recieve shadows (this can be obtained easily using a tool
such as VideoTrace [vdHDT∗ 07]), and a few strokes indicating sample regions of shadowed and unshadowed pixels.
2.1. Lighting direction as a segmentation problem
To recover the dominant lighting direction, we use an approach based on the PoseCut method proposed by Bray et
al. [BKT06]. PoseCut aims to simultaneously recover both
the segmentation and pose of a human body model from a
set of images by embedding the problem in a Markov Random Field (MRF). The pose recovered is that which results
in the least cost segmentation of the image.
SecondSkin is similar in that it simultaneously estimates
the segmentation corresponding to the shadow of an object in the scene, and the light-source direction which corresponds to the best shadow segmentation. This ensures that
only segmentations corresponding to feasible parameter estimates are considered.
2.2. Min-cut algorithms
It has been shown that a certain class of labelling problem
defined over a Markov Random Field can be solved using
the min-cut approach [BVZ01].

1738

A. van den Hengel D. Sale & A. R. Dick / SecondSkin: An interactive method for appearance transfer

A two label Conditional Random Field (CRF) may be represented as a graph G = (V, E,C) where V is the set of vertices or nodes, E is the set of edges and C is an edge cost
function that maps all edges in E to positive real values. In
the case of the segmentation problem, one node is typically
assigned per pixel in the image, and a link constructed between nodes representing neighbouring pixels. Two terminal
nodes are also defined, called the source and the sink, which
are labelled as s and t respectively. An st-min-cut partitions
the graph into two sub-graphs, such that one sub-graph contains s and the other t, by removing the set of edges which
partition the graph with minimal total cost.
For each pixel pi we assign a variable xi . Each xi
may take on either value from the set of labels L =
{l1 , l2 }. In our case, the labels are defined as L =
{Shadowed, Unshadowed}. The set of variables X =
{x1 . . . xn } is thus capable of representing every possible labelling of the set of all pixels P = {p1 . . . pn }.
If we define the neighbourhood Ni of a node ni to be the
set of nodes it is directly connected to then the cost associated with a labelling may be represented as the following
energy function
Γ(X) =

∑

φ(D|xi ) +

xi ∈X

∑

ψ1 (D|xi , x j ) + ψ2 (xi , x j )

.

j∈Ni

where θ represents the parameter vector describing the pose
of the figure. The extra unary term φ2 (xi |θ) expresses the
degree to which a particular labeling for node ni is supported
by the current value of the parameter vector θ. In PoseCut
this reflects the distance of node ni from the skeleton when
it is in the position prescribed by θ.
In order to estimate the direction of the predominant light
source in a scene we re-purpose the parameter vector θ to
instead represent the dominant light direction. To determine
φ2 (xi |θ) we need to identify which pixels would be shadowed if the light source were in the direction described by
θ. This requires modelling the parts of the scene that cast
or receive shadows. Typically this is not arduous; for the
scene depicting the Moore sculpture in Figure 1, for example, it was necessary only to model the plinth, and the ground
plane, which required less than a minute of modelling time
using VideoTrace [vdHDT∗ 07].
Once the modelling has been completed the scene can be
rendered and a shadow mask calculated on the basis of a
given light source direction. The shadow mask S determines
the value of φ2 (xi |θ) as follows:
φ2 (xi = Shadowed|θ) = 1 if S(pi ) = 0
= 0 if S(pi ) = 1.

(1)

Note also that φ2 (xi = Unshadowed|θ) = 1 − φ2 (xi =
Shadowed|θ).

• The unary potential φ(D|xi ) reflects the degree to which
an observed pixel value is predicted by the model associated with the label xi [BKT06]. We model both shadowed
and unshadowed pixels using full covariance Gaussian
Mixture Models (GMMs) defined in RGB space. Each
GMM contains 5 Gaussian components. Thus, φ(D|(xi =
Shadowed)) reflects the probability of the colour of pixel
pi under the GMM associated with shadowed pixels.
• The value of the first binary potential term ψ1 (D|xi , x j )
reflects the degree to which the pixel values associated
with ni and n j are explained by labels xi and x j . We use it
to favour label boundaries corresponding to strong image
gradients.
• The second binary potential term ψ2 (xi , x j ) is used to encourage continuity of segments by penalising labellings
which assign different labels to neighbouring nodes.

One intermediate shadow mask and the final shadow mask
rendered in the course of estimating the light source direction for the Moore sequence are shown in Figure 2 (left and
right respectively), along with the final mask for the loop sequence (bottom). The pixels identified as shadow in the CRF
corresponding to the final light source direction estimate for
the tennis ball sequence rendered (in red) over a frame of
input video is shown in Figure 3.

where

2.3. PoseCut for light source estimation
PoseCut extends this approach by introducing an additional
unary potential term into equation (1) which depends on the
parameters describing the pose of the object to segment. The
energy function is thus
Γ(X, θ) =

∑

φ1 (D|xi ) + φ2 (xi |θ) +

xi ∈X

∑
j∈Ni

ψ1 (D|xi , x j ) + ψ2 (xi , x j )

(2)

Figure 2: An intermediate and the final shadow mask estimates from the Moore sequence, and the final estimate from
the loop sequence.
Initialising the light source position to be directly overhead and applying Powell’s method [PTVF92] to estimate
θ using Equation 2 as a cost function typically results in an
accurate estimate of the light source direction. A graphcut is
calculated for each lighting direction sampled, and the cost
is returned to the Powell’s method implementation, which
then chooses further directions to sample. Note that each
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

A. van den Hengel D. Sale & A. R. Dick / SecondSkin: An interactive method for appearance transfer

sample requires a graphcut, which is computationally expensive. Powell’s Method was chosen for its ability to find
a solution using a small number of samples when an analytical Jacobian is not known. The cost varies smoothly for
small changes in lighting direction (due to gradually more
pixels being correctly or incorrectly labelled from the shape
model φ2 (xi |θ) described above), and so the gradient descent
method is able to find an optimum. There is no guarantee that
the optimisation is convex however, and Powell’s method is
not a global optimiser, so theoretically the result could be a
local minimum. This was not observed in practice, but could
be avoided by allowing the user to choose an initial direction
instead of using the default in such cases.
In order to ensure that the required result is achieved, we
allow the user to manually mark mis-classified pixels with
their correct label. Pixels identified as belonging to either
class are added to the GMM representing that class and the
corresponding nodes of the graph have their unary potentials
modified so that they match the selected model no matter
what the GMM predicts. This is done by setting the other
model’s prediction to 0 for the pixel. Thus it is not possible to
generate solutions which have incorrect labellings for these
pixels; they are hard constraints on the optimization.

1739

The rendering is dependent on the current estimate of the parameters to the BRDF associated with the surface, which are
varied to minimise this difference.
Numerous forms of BRDF have been developed
(see [Sch94] for a survey), and any of these capable of rendering the object on the basis of a set of material and lighting
parameters may be used within SecondSkin. The rendering
process does not have access to an environment map, or the
geometry of the entire scene, however, so only local shading
techniques are available.
In order to estimate the appearance of an object visible
in the scene we must first create a model of the corresponding surface geometry. As in Section 2, we use VideoTrace
for the purpose, but any image-based modelling tool would
be equally effective. Again, it is not necessary to model the
entire scene, but instead some part of it from which the appearance is to be captured. The fundamental requirement of
the 3D model is that the corresponding parts of the image
set exhibit all aspects of the required appearance model. The
model from which the light source direction was estimated
as described in Section 2 can be re-used for this purpose.
Through the light source direction estimation, object modelling and camera tracking processes we have estimates of
the incident (lighting), normal, and viewing vectors across
the object model. The remaining appearance parameters are
considered constant across the material of interest, and thus
may be estimated together.
3.1. Rendering and estimation

Figure 3: Left: Part of a frame from the tennis ball input
video. Right: The same frame with the final shadow segmentation marked in red.
To summarise, light source estimation proceeds as follows. After modelling the required scene geometry in VideoTrace the user identifies the image in which the shadow segmentation should take place and selects example regions of
shadow and non-shadowed pixels. Optimisation is then activated, and typically converges in less than a minute, at which
point the user can accept the parameter estimates or, if necessary, manually re-label mis-classified pixels.
The accuracy of the process has been tested by estimating
the light source direction from rendered images with known
lighting conditions. In every case the error in the estimate
was less than 5 degrees.

BRDFs such as those proposed by Phong [Pho75] and
Ward [War92] are typically expressed in terms of the surface normal, the incident light direction and the viewing angle measured in the scene coordinate frame. We label these
vectors n, l and v respectively. A general BRDF is parameterised by two vectors which represent the incident light direction and the viewing angle measured with respect to the
surface normal. These angles can be calculated as functions
of n, l and v.
Both types of BRDF also have a set of appearance parameters which we denote ω. These parameters may be altered
in order to change the appearance of the material, and might
include terms which control the width, shape and colour of
specular highlights for example.

3. Estimating surface appearance

Given that we are interested in colour images, each appearance function has three parts corresponding to the red,
green, and blue (RGB) channels which we label Ic (l, v, n, ω)
for c in {r, g, b} respectively. Let I(l, v, n, ω) represent the
combined (3 channel) appearance function.

We approach the estimation of an object’s appearance properties as an optimisation problem. The cost function to be
minimised reflects the difference between the appearance of
an object in the image set and a rendering of the same object.

Let Q = {p1 . . . pm } represent the set of pixels over which
we wish to estimate the appearance parameters. We express
the viewing angle v(p) as a function of the pixel value and
the surface normal n(p, µ) a function of the pixel value and

c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

1740

A. van den Hengel D. Sale & A. R. Dick / SecondSkin: An interactive method for appearance transfer

the 3D model µ. This does not reflect their dependence on the
camera position (which does not change), but simplifies the
notation. The energy function representing the difference in
appearance between the pixels of interest in the source image
and the corresponding pixels on the rendered object is
J(Q, l, I, ω) =

∑

∑

p∈Q

c∈{r,b,g}

(ϒc (p) − Ic (l, v(p), n(p, µ), ω))2

(3)
where ϒc (p) represents the intensity of the c colour channel of pixel p. The energy function J(Q, ω) is thus the sum
of the Euclidean distances between the rendered and image
colours. A sum of distances, rather than a sum of squares has
been used in order to improve the robustness of the resulting
fit to outliers. Outliers in this case are pixels with appearance
values which do not reflect the object material properties.

where the superscripts a, d and s indicate ambient, diffuse
and specular terms respectively, k indicates a material reflectance coefficient and i a lighting coefficient.
There are a pair of material and lighting coefficients for
each of the ambient, diffuse and specular terms in equation (4), and each pair is multiplied together. It is therefore
not possible to recover these parameters individually. It is
possible, however, to combine the effect of pairs of lighting
and material coefficients into an appearance coefficient. We
denote the combined material and lighting coefficients as j,
such that
jcl = ilc kcl , l ∈ {a, d, s}, c ∈ {r, g, b}.

(5)

The apparent point brightness in colour channel c predicted
by the Phong model for a single point source thus becomes
IcP (l p , v p , n p , ω) = jca + jcd (l p · n(p, µ)) +

3.1.1. Selecting pixels

jcs (r(l p , n(p, µ))· v p )α

Estimating the appearance of a material requires minimising
the energy function in equation (3) which is a sum over a
set of pixels Q. In order to generate an accurate appearance
estimate the set Q must exhibit a variety of material appearances, normals, and viewing angles. The ideal Q would thus
include pixels sampled from shadowed regions, regions in
diffuse light, and regions close to specular highlights.
Regardless of the particular BRDF used, the properties of
specular highlights are very sensitive to small changes in the
light source and viewing directions and the surface normals.
The intensity of the light directed at the camera and the small
area of typical specular highlights also mean that the corresponding CCD elements are often saturated. The saturation
of the CCD elements means that the measured pixel intensities no longer reflect the ’true’ BRDF and cannot be used
for appearance estimation. For this reason we identify saturated specular highlights, and label the corresponding pixels
such that they may be eliminated from the estimation process. Pixels on the boundaries of these regions are particularly valuable to the estimation process, however, as they are
the only source of information about the roughness of the
material. These pixels are not eliminated.

(6)

where ω is a vector of appearance parameters
ω = [ jra , jrd , jrs , jga , jgd , jgs , jba , jbd , jbs , α]T .
We also define I P as (IrP , IgP , IbP ).
The required estimate of the appearance parameters ω
may thus be defined as
ω = arg min J(Q, l, I P , ω),

(7)

ω

which can be recovered by numerical minimisation of
J(Q, N, I P , ω) over ω using a non-linear optimiser (specifically, the LBFGS-B [BLN95] optimiser).
3.3. General BRDFs

The set Q thus includes all pixels in all images corresponding to known (modelled) geometry, excluding those
specular pixels identified as having saturated the CCD.

As mentioned previously, general BRDFs take the l, n, and
v vectors in addition to their own set of parameters ω, and
output an intensity for each of the RGB channels (I c ). We
generalise our approach by substituting the Phong model’s ω
and I P in equation 7 with another BRDF’s parameters ω and
output I = (I r , I g , I b ). In addition to this, BRDFs are commonly expressed as a combination of models for ambient,
diffuse and specular reflectance terms, as is done in Blender
[Ble]. Therefore, we assume we can separate a composite
BRDF into these parts.

3.2. Phong example

3.4. Sequential appearance estimation

In order to illustrate the estimation process we now consider
the Phong reflection model [Pho75]. If we define r(l, n) to be
the reflection angle calculated on the basis of the lighting and
normal vectors l and n, then the apparent point brightness in
colour channel c predicted by the Phong model for a single
point source is

Numerical minimisation of J(Q, l, I, ω) produces a usable
estimate of the appearance properties of an object from a
video sequence. The unavoidable errors in estimated surface
normals, lighting directions, and to a lesser extent viewing
angle can however have a visible effect.

kca iac + kcd (l p · n(p, µ))idc + kcs (r(l p , n(p, µ))· v p )α isc

(4)

The modelling of specularities by any appearance function is particularly sensitive to errors in the estimates of the
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

A. van den Hengel D. Sale & A. R. Dick / SecondSkin: An interactive method for appearance transfer

vectors n, v and l. This sensitivity leads to errors in identifying the centre of the specularity and thus to an overestimate
of its width. This problem can be mitigated by estimating
the ambient, diffuse and specular factors in turn where the
BRDF allows these terms to be separated.
We begin by estimating the elements of ω associated with
ambient lighting, which we label ωa . Pixels which are in
shadow can be identified on the basis of the modelled geometry µ and are assumed to exhibit only ambient lighting.
We label the thus identified pixels Qa . The coefficients in ω
associated with the diffuse ωd and specular terms ωs are set
to 0, and numerical minimisation used to identify the ambient coefficients ωa such that
ωa = arg min J(Qa , l, I, ω)|ωd ,ωs =0 .

(8)

ωa

The set of diffuse pixels Qd includes Qa and those pixels
from Q identified as being unlikely to exhibit significant
specularity on the basis of the vectors n, l and v. That is,
where |acos(n· l) − acos(n· v)| > t for some adjustable constant t and none of the RGB channels are saturated. The estimate of the diffuse coefficients are thus determined by
ωd = arg min J(Qd , l, I, ω)|ωa =ωa ,ωs =0 .

1741

3.5. Texture recovery
Material estimation as described above returns a single estimate of the diffuse and specular colours for the object. In
general objects do not have a single diffuse colour, but rather
a texture which varies over the object surface. We model
this texture as a 3D non-uniform scaling of the base diffuse
colour at various points on the object.
We assume the texture of an object is largely a feature
of the diffuse component of the appearance model. In order
to provide a per-pixel model of this diffuse component we
modify the equation by which we estimate the channel c intensity ϒc (p) of pixel p in image ϒ as follows
ϒc (p) = λc (p)ϒdc (p) + ϒac (p) + ϒsc (p),

(11)

where ϒdc , ϒac and ϒsc represent the diffuse, ambient and
specular components of ϒc respectively. The λc (p) coefficient is a per pixel representation of the deviation of the
diffuse component from the estimate achieved through the
process described above. Note that equation (11) is linear in
λc (p) which means that we can recover a per pixel estimate
through a simple comparison of the estimated ϒdc , ϒac and ϒsc
and the real image information ϒc .

(9)

ωd

Finally we estimate the specular coefficients on the basis of
the whole pixel set Q. A lower energy value, and a more
convincing final result is achieved if we also refine the light
source direction estimate at this stage. The estimate of the
specular coefficients are thus determined by
[ωs , l] = arg min J(Q, l, I, ω)|ωa =ωa ,ωd =ωd .

(10)

ωs ,l

Figure 4 shows a rendering of the object model for the
Moore sequence with the calculated appearance model. The
appearance model of Cook and Torrance [CT82] was used to
generate the estimate. Note that only the light green (more

Figure 5: Two surface patches rendered with their respective
Λ textures.
Figure 5 shows the multiplicative texture estimate corresponding to two patches from the surface of the Moore
sculpture. Each rendered colour represents the estimate of
the value of λ which generates the ϒ(p) which best mimics
the corresponding original pixel colour ϒ(p).

Figure 4: A rendering of the estimated appearance properties back into the original sequence

corroded) material has been estimated here, and that the object exhibits two similar, but different materials. The rendered result shows that the most significant properties of
the material have been captured, but that the lack of texture
leaves the result unconvincing.
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

Figure 6: The spherical λ texture estimate from the Moore
sculpture (left) and loop sequence (right). Note the green areas reflecting the colour of the grass under the loop sculpture.

1742

A. van den Hengel D. Sale & A. R. Dick / SecondSkin: An interactive method for appearance transfer

By comparing the true and estimated appearances over the
set Q = {p1 . . . pm } we calculate at a corresponding set of
λ values Λ = {λ1 . . . λm } where λ1 = λ(p1 ). The user then
selects a centre point for a spherical coordinate system by
which the position of each pixel pi and thus each λi is measured. The two angular coordinates are used to map the λ
values onto a sphere. Texture synthesis [EL99] is then used
to generate texture for the remainder of the sphere. As the
back of objects is often not visible in an image set, we generate the texture over only half of the sphere and mirror the
result. The spherical λ texture estimated from the images in
Figure 5 is shown in Figure 6. Once this sphere map has
been extracted, it can be applied as a multiplier to the diffuse
color using sphere mapping, a common technique available
in many 3D packages. Figure 7 shows the synthesised tex-

ure 8, which has been rendered with known appearance parameters. The Phong model was used to render the image
with lighting coefficients all set to 1, and the material pa-

Figure 8: The rendered test image.
rameters set to the values given in Table 1. Note that, given
the form of equation (5), this is equivalent to setting the appearance parameters of the Phong model to those listed in
Table 1. Table 1 also shows the appearance parameters esParameter
Light direction
Diffuse colour
Specular colour
Ambient colour
Specular coeff.

Figure 7: A re-textured rendering of the Moore sculpture.
ture rendered back onto the object.
This process is capable of modelling a variety of appearance characteristics in addition to surface texture. The fact
that the texture is synthesised over a sphere and projected
onto the surface by spherical mapping means that the texturing process captures and propagates some of the larger scale
surface properties of the object, and its environment. This
can be seen by the results in Figure 1 (right). Only the light
green material has been modelled (as seen in Figure 4), but
the multiplicative spherical texture has captured some of the
variation between the appearance of the green and brown
materials. Note also that the darker multiplicative texture
values are concentrated around the bottom of the synthetic
object, as is the case for the real object. Limited environment
modelling effects may be achieved through the same mechanism, with the impact of the light reflected by other parts of
the scene recorded in the appropriate areas of the spherical
texture. This requires that affected parts of the source object
are modelled in order that the effect may be sampled. This
effect is particularly visible in the the multiplicative texture
for the loop sequence shown in Figure 7. The final result
for the loop sequence rendered with synthetic geometry is
shown in Figure 9.
4. Results
The accuracy of appearance estimates obtained by this
method is tested using the synthetic image shown in Fig-

Rendered
(-1.570, 0.785)
(0.8,0,0.4)
(1,1,1)
(0.1,0.1,0.1)
5

Estimate
(-1.602, 0.783)
(0.802,0.000,0.406)
(0.921,0.954,0.921)
(0.098,0.098,0.097)
4.86

Table 1: Rendered vs. estimated coefficients of the Phong
model used to render the synthetic test image shown in Figure 8. Note that each coefficient represents lighting and material properties combined, colours are given in RGB triplets
and directions in spherical coordinates.
timated through the process described above. All estimates
are within 8% of the true value, validating the method. Given
the purpose of the algorithm, however, the true test of performace is a visual comparison of the original and transfered
appearances. For this purpose a set of results on real image
sequences is presented.
Figures 1 and 9 show images of synthetic geometry rendered using an appearance model captured from an object
in the scene. The video accompanying the paper shows
the same results rendered back into the original image sequences. Note that the specular effects move as the camera
position changes, but that the effects of the environment provided by the multiplicative texture remain fixed to the object as desired. The rendered results show that the method is
capable of capturing and transferring an appearance model
which accurately represents many aspects of the appearance
of a real object. Figures 11 and 13 show two further examples and Figure 10 shows the corresponding multiplicative
textures.
SecondSkin performs appearance transfer using a technique which fundamentally depends on the use of appearance model such as those described above. The aspects of
appearance that may be transferred are thus limited to those
which may be represented by the model, and estimated from
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

A. van den Hengel D. Sale & A. R. Dick / SecondSkin: An interactive method for appearance transfer

1743

Figure 12: A frame from a real sequence with an added
sphere showing an overly smoothed appearance.
Figure 9: An image showing synthetic geometry rendered
with an appearance model captured from an object in the
scene.

Figure 10: The spherical multiplicative textures for the exercise and tennis balls respectively.

the image set. Figure 12 shows a sphere rendered with an
appearance which was captured from the top of the pillar on
which it sits. The material on the top of the pillar is smoother
than that on the sides, and thus only one face of the object
may be used to estimate a consistent set of appearance parameters. SecondSkin is able to estimate the required appearance parameters, but the results in Figure 12 illustrate the
dependence of the estimation process upon the availability
of suitable example patches.
One motivation for the appearance transfer process is that
of animating objects visible in a video sequence. In order to

achieve this end both the geometry and appearance of the existing object must be modelled in order to generate the synthetic replacement. Figure 13 shows a frame from a sequence
depicting an original object and its animated synthetic replacement. Here a ball is used in the original video to aid the
modelling process, though as described above, and can be
seen in the Moore example (Figure 1), that is not necessary.
The quality of results depends very heavily on the ability to
construct an accurate geometric model of the object we wish
to capture the material from, so that we have good shape
normals. Great care was taken to ensure very close models
for Figures 1 and 9, but using spheres provided an easy way
to produce high quality results. Results could be improved
further by using more accurate means to capture the surface
normals of objects.

Figure 13: A frame from a video sequence showing a real
object (right) and its animated synthetic dual (left).
Note that in the video supplement, there are occasional
jumps in the geometry’s scale and position as well as misalignments of the shadows. They are due to errors on those
frames from the camera tracking algorithm and could not be
improved upon. They do not relate to the results obtained
from this procedure, which only affect the material properties and lighting of the synthetic objects. An improved camera tracker would improve our results further.
5. Conclusion

Figure 11: A frame from the exercise ball sequence with
added synthetic geometry.

c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

SecondSkin provides a simple interactive process by which
the appearance properties of an object in a video sequence
may be estimated. This allows synthetic geometry to be rendered into a sequence so that its appearance matches that of

1744

A. van den Hengel D. Sale & A. R. Dick / SecondSkin: An interactive method for appearance transfer

real objects in the scene. It does not require camera calibration, or the insertion of specific objects into the scene. The
only other apparatus required is a software camera tracker
and a modelling package. No prior knowledge about the nature of the light source is required, but the algorithm does
assume that one distant light source predominates.
SecondSkin estimates a dominant light-source direction in
order to identify those parts of the modelled scene exhibiting specific reflectance properties (such as purely ambient
lighting). The nature of the appearance model also limits the
range of surface properties that may be modelled. In both
cases these are limitations of the models rather than the underlying method, and generalisation of the process should be
possible with further work.
References
[BG01] B OIVIN S., G AGALOWICZ A.: Image-based rendering
of diffuse, specular and glossy surfaces from a single image. In
ACM SIGGraph (2001), pp. 107–116. 3
[BKT06] B RAY M., KOHLI P., T ORR P.: Posecut: Simultaneous
segmentation and 3D pose estimation of humans using dynamic
graph-cuts. In Euro. Conf. Computer Vision (2006), pp. 642–655.
3, 4
[Ble] B LENDER F OUNDATION: Blender: A free, open source, 3d
content creation suite http://www.blender.org/. 2, 6
[Bli77] B LINN J. F.: Models of light reflection for computer synthesized pictures. In ACM SIGGraph (1977), pp. 192–198. 2
[BLN95] B YRD R. H., L U P., N OCEDAL J.: A limited memory
algorithm for bound constrained optimization. SIAM Journal on
Scientific and Statistical Computing 16, 5 (1995), 1190–1208. 6
[BVZ01] B OYKOV Y., V EKSLER O., Z ABIH R.: Fast approximate energy minimization via graph cuts. IEEE Trans. on Pattern
Analysis and Machine Intelligence 23, 11 (2001), 1222–1239. 3
[CF07] C AO X., F OROOSH H.: Camera calibration and light
source orientation from solar shadows. Computer Vision and Image Understanding 105, 1 (Jan. 2007), 60–72. 2
[CT82] C OOK R. L., T ORRANCE K. E.: A reflectance model for
computer graphics. ACM Trans. on Graphics 1 (1982), 7–24. 7
[DM97] D EBEVEC P. E., M ALIK J.: Recovering high dynamic
range radiance maps from photographs. In ACM SIGGraph
(1997), pp. 369–378. 2
[DRB97] D RETTAKIS G., ROBERT L., B OUGNOUX S.: Interactive common illumination for computer augmented reality. In
Proceedings of the Eurographics Workshop on Rendering Techniques ’97 (London, UK, 1997), Springer-Verlag, pp. 45–56. 2
[EL99] E FROS A. A., L EUNG T. K.: Texture synthesis by nonparametric sampling. In IEEE International Conference on Computer Vision (Corfu, Greece, September 1999), pp. 1033–1038. 8

[Lov97] L OVE R.: Surface Reflection Model Estimation from Naturally Illuminated Image Sequences. PhD thesis, University of
Leeds, 1997. 3
[mat] Matcap, part of the
http://www.pixologic.com. 3

ZBrush

modelling

package.

[MCAS90] M URRAY-C OLEMAN J. F., AM S MITH P.: The automated measurement of brdfs and their application to luminaire
modeling. Journal of the Illuminating Engineering Society WInter (1990). 2
[NHIN86] NAKAMAE E., H ARADA K., I SHIZAKI T., N ISHITA
T.: A montage method: the overlaying of the computer generated
images onto a background photograph. In ACM SIGGraph (New
York, NY, USA, 1986), ACM, pp. 207–214. 2
[PGV∗ 04] P OLLEFEYS M., G OOL L. V., V ERGAUWEN M.,
V ERBIEST F., T OPS K. C. J., KOCH R.: Visual modeling with a
hand-held camera. International Journal of Computer Vision 59,
3 (2004), 207–232. 2
[Pho75] P HONG B. T.: Illumination for computer generated pictures. Commun. ACM 18, 6 (1975), 311–317. 2, 5, 6
[PTVF92] P RESS W. H., T EUKOLSKY S. A., V ETTERLING
W. T., F LANNERY B. P.: Numerical recipes in C (2nd ed.): the
art of scientific computing. Cambridge University Press, New
York, NY, USA, 1992. 4
[Sch94] S CHLICK C.: A survey of shading and reflectance models. Computer Graphics Forum 13, 2 (1994), 121–131. 5
[SSI99] S ATO I., S ATO Y., I KEUCHI K.: Illumination distribution
from brightness in shadows: Adaptive estimation of illumination
distribution with unknown reflectance properties in shadow regions. In ICCV (2) (1999), pp. 875–882. 2
[TB] T HORMÄHLEN T., B ROSZIO H.: Voodoo camera tracker.
free download at www.digilab.uni-hannover.de. 2
PFTRACK: A commercial
[The] T HE P IXEL FARM:
camera tracking and image based modelling producthttp://www.thepixelfarm.co.uk. 2
[TSE∗ 04] T CHOU C., S TUMPFEL J., E INARSSON P., FAJARDO
M., D EBEVEC P.: Unlighting the parthenon. In ACM SIGGRAPH 2004 Sketches (2004), p. 80. 2
[vdHDT∗ 07] VAN DEN H ENGEL A., D ICK A., T HORMÄHLEN
T., WARD B., T ORR P. H. S.: Videotrace: rapid interactive scene
modelling from video. In ACM SIGGraph (2007). 3, 4
[War92] WARD G. J.: Measuring and modeling anisotropic reflection. In ACM SIGGraph (1992), pp. 265–272. 2, 5
[WC01] W EBER M., C IPOLLA R.: A practical method for estimation of point light-sources. In BMVC (2001). 2
[WSL04] W U E., S UN Q., L IU X.: Recovery of material under complex illumination conditions. In GRAPHITE ’04 (2004),
pp. 39–45. 2
[YDMH99] Y U Y., D EBEVEC P., M ALIK J., H AWKINS T.: Inverse global illumination: Recovering reflectance models of real
scenes from photographs. In ACM SIGGraph (1999). 2

[FRL∗ 98] FAUGERAS O., ROBERT L., L AVEAU S., C SURKA
G., Z ELLER C., G AUCLIN C., Z OGHLAMI I.: 3-d reconstruction of urban scenes from image sequences. Comput. Vis. Image
Underst. 69, 3 (1998), 292–309. 2
[KMG96] K ARNER K. F., M AYER H., G ERVAUTZ M.: An image
based measurement system for anisotropic reflection. Computer
Graphics Forum 15, 3 (1996), 119–128. 2
[LDR00] L OSCOS C., D RETTAKIS G., ROBERT L.: Interactive
virtual relighting of real scenes. IEEE Trans. on Visualization
and Computer Graphics 6, 4 (2000), 289–305. 3

c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

