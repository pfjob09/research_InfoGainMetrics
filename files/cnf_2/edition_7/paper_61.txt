Volume 28 (2009), Number 2

EUROGRAPHICS 2009 / P. Dutré and M. Stamminger
(Guest Editors)

Image Appearance Exploration by Model-Based Navigation
L. Shapira1, and A. Shamir2 and D. Cohen-Or1
1 Tel-Aviv

University, Israel
Center Herzliya, Israel

2 Interdisciplinary

Abstract
Changing the appearance of an image can be a complex and non-intuitive task. Many times the target image colors
and look are only known vaguely and many trials are needed to reach the desired results. Moreover, the effect of
a specific change on an image is difficult to envision, since one must take into account spatial image considerations along with the color constraints. Tools provided today by image processing applications can become highly
technical and non-intuitive including various gauges and knobs.
In this paper we introduce a method for changing image appearance by navigation, focusing on recoloring images. The user visually navigates a high dimensional space of possible color manipulations of an image. He can
either explore in it for inspiration or refine his choices by navigating into sub regions of this space to a specific
goal. This navigation is enabled by modeling the chroma channels of an image’s colors using a Gaussian Mixture
Model (GMM). The Gaussians model both color and spatial image coordinates, and provide a high dimensional
parameterization space of a rich variety of color manipulations. The user’s actions are translated into transformations of the parameters of the model, which recolor the image. This approach provides both inspiration and
intuitive navigation in the complex space of image color manipulations.
Categories and Subject Descriptors (according to ACM CCS): Computer Graphics [I.3.6]: Interaction Techniques—
Image Processing [I.4.3]: Enhancement—Image Processing [I.4.9]: Applications—Image Processing [I.4.10]: Image Representation - Multidimensional—

1. Introduction
The shift into digital form opened up vast possibilities for
image editing and manipulations. One of the more fundamental manipulation types is changing the appearance of the
image by applying color modifications. Such modifications
are needed to change the tone or mood of the image, to fit
a given style or design, or to re-select the color palette for
aesthetic or artistic reasons.
Nevertheless, such color manipulations are highly challenging. Color spaces are highly complex and non-intuitive.
Color modifications are difficult to describe and, typically,
numerous experiments are needed to reach the desired result. For instance, even when the target palette of colors is
given or taken from an example image, it is difficult to find
the correct modifications to apply these colors to a different image. Moreover, many times the artist does not have a
clear concept of the desired modification and inspiration or
exploration are needed to assist in reaching this goal. Lastly,
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

altering the chroma of an image is highly unstable. Modifications in color space are perceived in image space, and a
slight modifications in color may cause large artifacts in the
image. Hence, any modification of colors must take into account constraints in both color space, and the spatial neighborhoods on the image. In summary, both envisioning and
applying desired color modifications to an image are difficult tasks.
Tools available today for color manipulations, require
manual knobs-tuning in color space or histogram manipulations. There are several drawbacks in this approach. First,
there are many different colors in an image and it is tedious
to manually change them individually. Second, the results
of manipulations in color space are often unexpected, since
there is no visual link between the user’s actions and their
effect on the image. Third, working only in color space with
no spatial constraints may cause noticeable artifacts to appear.

630

L. Shapira & A. Shamir & D. Cohen-Or / Image Appearance Exploration by Model-Based Navigation

Figure 1: Exploring image appearance: Starting from the original image on the left, the user makes a series of selections in a
gallery interface. Each selection presents more refined variations, until finally, in the left, the user selects his final result.

In this paper we use a navigation metaphor to create and
inspire image color changes. We first model the distribution of the color channels of the image with a spatially constrained Gaussian Mixture Model (GMM). The parameters
of the GMM are combined to form the appearance searchspace defining as all possible color manipulations by simple
transformations of the model. Instead of tediously experimenting with different parameter values and guessing their
outcome, we use a WYSIWYG approach. We present the
user with a set of image variants and allow navigation in
space by direct user selections. The actual model is transparent to the user, while it is implicitly affected by his actions.
Navigation assists both in inspiring and in applying specific
color modifications to an image (Figure 1).
At all times, the user is provided with a visual representation of the appearance search-space and the navigation
path. The system allows the user to guide the navigation
in an iterative manner by choosing specific variants of images and synthesizing new image variations from them. We
use a Monte Carlo approach to create samples in regions of
this high dimensional space, and create image color variants
guided by the user choices. The user can traverse backwards
or forward in his path, jump to other regions of space, or use
advanced tools that guide the search by controlling the variations of images. These include modifying the model parameters on the image with a brush, modifying the GMM palette
in directly model space, or matching a target image palette.
Using these tools, the user’s actions are translated into transformations on the Gaussians of the model, focusing the navigation in a new region in the search-space. New images are
synthesized around this position and a new gallery is created,
providing direct depictions of his actions. This approach (as
demonstrated in Figure 1) is both inspirational, allowing to
explore new variants that might not have been considered,
and effective, providing means to narrow down towards a
desired image goal.

tions to the luminance channel for tone mapping or compressing high dynamic range images [RWPD05]. Nevertheless, such approaches are not easily extendible to multi
dimensional chroma channels. Dealing with each channel
separately is undesirable since the channels are highly correlated and manipulating them separately leads to artifacts
and limits the type of manipulations that can be performed.
In [AP08] changes to image appearance are seeded by user
brush strokes and propogated to similar areas in the image.
Colorization algorithms attempt to re-color a gray scaled
image [LLW04,LWCO∗ 07] using scribbles marked by hand.
Recently, several color manipulation works have used an example image as inspiration for recoloring an image. Given
the example target image, the input image’s colors can
match those of the example target image by either histogram
matching [GW01, PKD05], parametric matching [RAGS01,
TJT05] or by non-parametric sampling [ICOL05]. However,
finding the correct image to be used as an example is not
always easy. Moreover, there are times when the need for
color manipulation is not guided by an image, but more by
abstract aesthetic ideas or even by pure exploration.
There are other model based approaches for color manipulation in literature. In [OW04] the concept of color lines
is introduced for the RGB color space. Manipulating these
lines allows for simple yet believable color changes in the

2. Related Work
There are many effective tools and algorithms for image appearance editing which concentrate on the luminance channel of an image, or manipulate gray-level images. For instance, histogram equalization of the luminance channel
is one of the oldest color correction algorithms. More recently, works such as [BPD06, LFUS06] apply manipula-

Figure 2: (a) The original image, (b) The distribution of
pixels in the Hue and Saturation space, (c) The histogram
function of pixels in HS, (d) A GMM model of the HS histogram with 3 clusters, (e) A GMM model with 5 clusters.

c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

L. Shapira & A. Shamir & D. Cohen-Or / Image Appearance Exploration by Model-Based Navigation

image. In [COSG∗ 06] a psychological color model is used,
in which the hue and saturation of the pixels compose the
radial 1-dimensional color space. Their algorithm attempts
to ‘shift’ an image to match one of a set of predefined models, hence improving its harmony. In contrast, our work uses
higher dimensional models and implements more flexible
manipulations, which are influenced by user choices and not
a pre-established template.
In [MAB∗ 97] parameter-based representations of a highdimensional data are displayed in a Design Gallery, which
is a set of variation created for the user to choose from. A
distance metric is defined on the high dimensional space,
ensuring that options displayed in the gallery differ from
each other. Design galleries have been shown to be useful
for volume rendering transfer functions, 3D scene lighting
placement, and more [JKM01]. In [Ado07], a feature called
Brainstorm utilizes a design gallery based interface for animation variations. Brainstorm seems to operate on independent 1-D parameters, by displaying random combinations of
values for the user. Unfortunately, there is no technical publication to explain it. Our mechanism is visually similar in
creating a set of image variations. However, our focus is to
enable meaningful navigation of the parametric space representing a D-dimensional color space and not only display
variations.
3. Our Approach
The space of all color variations of a given image contains an
almost infinite number of possibilities and is extremely high
dimensional. Furthermore, there is no metric that can adequately express the perceptual distances among these variations. Our goal is to model the space by a structured appearance search-space with a relatively small number of dimensions and a given metric.
First, we calculate the color distribution of an image using one of two color spaces, namely the HSV or L*A*B*
color space. Next, we model this distribution using a Gaussian mixture (GMM) with a small number of Gaussians (See
Figure 2 for an example where an image is modeled by 3
and 5 Gaussians). We limit all possible color variations by
permitting only variations defined by the set of transformations applied to the parameters of the GMM color model.
The dimension of the appearance search space is therefore
the number of parameters times the number of Gaussians in
the model. In practice, we allow only translations, rotations
and scaling of the individual Gaussians arriving at d < 30.
Each point in the search space represents one color variation of the original image (see Figure 3). The actual image
variation can be synthesized by first accounting for all the
transformations applied to the model and then changing each
pixel’s color by transforming it in a similar manner as its associated Gaussian in the model.
Although the search space is of relatively low dimension
compared to the space of all color variations of a given
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

631

image, it is still difficult to grasp and explore. To assist
users in navigating towards their favorite color variation we
use a simple 2D interface. First, measuring the distance between two mixture variants [SCLG05] induces a metric on
the appearance search-space. Using this metric and multidimensional scaling we project samples from the searchspace onto 2D and display them to the user as a gallery of
images in the foreground. In Section 4 we describe the model
construction and the metric induced.
Initially, we fill the gallery with random samples from the
search space employing Monte Carlo sampling. Next, the
user navigates by defining regions in space which interest
him. This is done using one of several methods. The most
basic one involves simply selecting one or more favored images from the foreground. A new set of samples from the
relevant subspace are created and displayed for further exploration, while the old set of images are moved to the background. The user can go backward or forward in his search
path, or jump to other positions in space. This promotes a
top-down perceptual dominant order. At first strong variations are created by shifting the hues of the model, while
later more subtle transformations are applied. At all times
the user can control the degree of variation by setting the
amount of randomness in the sampling process.
The user can also initialize the navigation to a more specific region of the search space by using more direct manipulation tools. He can modify the Guassians directly in model
space using a palette editor. He can choose an example image and match the model to its colors. He can directly paint
on the image with a global brush tool that is used to modify
all pixels that are members in one Gaussian simultaneously.
This approach provides direct visual connections between
simple user actions and the resulting modified images, it is
inspiring and does not require an understanding of complex
notions and parameters. The navigation technique is illustrated in Figure 1 and explained in Section 5.
4. Modeling Color Manipulations
4.1. The Image Color Model
The key to effective navigation is to model it with a relatively
small and structural space that we call the appearance search
space. The search space definition is based on a Gaussian
mixture model of the selected color channels of the image.
We have chosen to use two possible color spaces: the HSV
color space, that describes perceptual color relationships in
a natural way to users, and the L*A*B* color space, that retains perceptual distance between colors. We use an accelerated Expectation Maximization (EM) algorithm [VNV06] to
create a model with k components. This algorithm iteratively
computes clusters C1 ...Ck of a given distribution (Figure 2).
In every iteration it splits the least probable cluster into two
clusters. Each cluster is represented by its mean and covariance matrix. Each pixel in the image is associated with a
probability vector p = (p1 , . . ., pk ) such that pi = P(x|Ci )

632

L. Shapira & A. Shamir & D. Cohen-Or / Image Appearance Exploration by Model-Based Navigation

E( f ) =

∑

e1 (p, f p ) + λ

p∈IMG

∑

e2 ( f p , fq )

{p,q}∈N

e1 (p, f p ) = − log(P(p| f p ) + ε)
e2 ( f p , fq ) =

1/(1 + max(edge(p), edge(q)))
0

f p = fq
f p = fq

where f p is the label assigned to pixel p, linked to the
highest probable Gaussian in the GMM model. edge(p) is
the value of pixel p in an image boundary map [MFM04]. N
is the set of pairs of 4-neighborhood pixels in the image, λ is
a parameter defining the degree of smoothness (we’ve used
λ = 2 for all images we’ve encountered, regardless of their
scale or composition).

Figure 3: An example of five variant images created from
five vectors sampled from the search space. The top left image is the original one. We first show three simple transformations, applied to a single Gaussian (translation, rotation
and scaling). The two bottom images represent more complex color manipulations.

and ∑i=1..k pi = 1. We have found that a low value of k (3 to
6) is sufficient for the GMM to provide a good approximation of the color distribution of an image, while still allowing
a rich space of variation.
4.2. Spatial Considerations
The basic GMM model is defined by considering only the
color distribution. To take into account image spatial considerations, we interleave EM iterations with a graph-cut
step which increases the probability of neighboring pixels
to belong to the same Gaussian. Every t iterations(typically
t = 3), during the expectation part of the EM algorithm, we
apply an alpha-expansion graph-cut algorithm [ZK04] on
the current Gaussians. The graph-cut uses a natural image
boundary map [MFM04] as a smoothness factor and results
in a labeling of each pixel, in which pixels in neighboring areas tend to be labeled in the same manner due to the smoothness term of the graph-cut. This labeling is used as input for
the next Maximization step. We have found that employing
this method reduces artifacts when dealing with noisy and
highly textured images. The graph-cut optimization minimizes the following energy functional, which is built from
e1 the data term, and e2 , the smoothness term:

Given a GMM model we can calculate for each pixel x ,
a probability vector px such that px (i) is the probability of
pixel x to belong to the i −th Gaussian. This in effect defines
a vector field over the image, creating a soft segmentation.
To further reduces artifacts in the synthesized images, we apply a median filter over this vector field. This constrains coherent areas in the image to undergo similar transformations.
In figure 4 you can see the effect of the spatial consideration
improvements on the generated color image variant.
4.3. Appearance Search Space
The actual appearance search space, containing all the possible color manipulations in our model, is the space of all
possible affine transformations on all k Gaussians of the mixture. For each Gaussian Ci we allow the following transformations:
• Translation - shift the mean µi of Ci . When dealing with a
radial dimension such as Hue (in HSV), the shift is cyclic.
• Rotation - apply a rotation on the covariance matrix Σi of
Ci .
• Scaling - Scale the principal axes of the covariance matrix
Σi , elongating or shrinking the Gaussian along its Eigenvectors.

Figure 4: (a) Original image (b) An image variant with no
spatial considerations (c) spatial considerations are applied,
a refinement of the GMM using a natural edge map, and a
median filter on the soft clustering probability vectors of the
pixels.

c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

L. Shapira & A. Shamir & D. Cohen-Or / Image Appearance Exploration by Model-Based Navigation

633

When changing only the chroma of an image (using only
2-channels), we use two translation values, rotation angle
and two scaling values (five parameters). To allow luminance
changes, we restrict the covariance such that the luminance
and color channels are orthogonal, and use three translation
and scaling values, and a rotation angle for the color channels only, in total seven parameters. Therefore the dimension
of the search space, d, is either 5k or 7k.
As described earlier, each d-dimensional vector v in the
search space represents a specific set of transformation for
the GMM, defining a specific color manipulation. Each
Gaussian Ci has an associated affine transformation Tv defined by the coordinates of v related to Ci . We synthesize the
actual image variant by accounting for all the transformations applied to the mixture of the original image. Let µi , Σi
be the original Gaussian Ci mean and covariance, and µ˜ i , Σ˜ i
be the transformed mean and covariance, A pixel’s color x
in the original image, is translated to new position xi in the
appropriate space by the Gaussian Ci using the following formula:
xi = Σ˜ i · Σ−1
˜ i,
i · (x − µi ) + µ

(1)

The final position xnew of x is a weighted blend of all xi
by the probabilities pi taken from the GMM:
k

xnew =

∑ pi xi

(2)

i=1

5. Navigating the Search Space
The appearance search space is still a relatively high dimensional space. Therefore, navigation it is not trivial. We provide a direct visual mechanism along with a Monte Carlo
sampling method to assist navigation. Each position in the
search space is represented by a gallery of possible image
variations sampled around that position to represent directions in which the user could follow. We begin with a sparse,
random sampling of the search space, and then gradually
converge to smaller and smaller sub spaces guided by the
user’s choices (Figure 1). In each iteration we create a small
number of samples (usually between 6 and 16) and display
them to the user. This creates a direct visual aid for navigation, which is effective for explorations, and helps find the
desired color variation.
This approach needs three components. First, a way to
sample the search space. Second, a way to project samples
from a d dimensional space position onto a 2D canvas for
display. Third, a layout mechanism of the actual images on
a 2D canvas that reduces cluttering and allows easy access.
Each scalar coordinate in the search space affects one
parameter of one Gaussian in the mixture. To sample the
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Figure 5: The process of generating the gallery of images:
(a) The different variant vectors sampled from the search
space represent variations of the GMM, (b) Using the distances between the models we build an affinity matrix between the samples, (c) We use MDS to project the vectors to
2D, (d) We build a kd-tree using a median-cut algorithm in
2D, (e-f) We arrange the images by mapping the kd-tree to a
regular grid to reduce cluttering and overlaps.

space, we can uniformly sample each coordinate within its
range and generate an image variation. However, uniform
sampling is impractical, and furthermore, not all coordinates
have the same perceptual importance. There is a need to
differentiate between the coordinates corresponding to the
means of each Gaussian, which represent that Gaussian’s
color, and the other coordinates which describe the covariance. Furthermore, in the HSV model, a greater importance
is given to the Hue channel, while in the L ∗ A ∗ B∗ model
each channel is of equal importance. Therefore, a biased
Monte Carlo search by random sampling is performed, giving the first coordinates (mean positions) the prominent role.
Later, as the user navigates into smaller sub-spaces, subtler
variations are created by fixing the mean and sampling the
other coordinates.
Let vt ∈ Rd be the base vector. Initially, when t = 0,
this vector is defined to be the GMM of the original image.
Later, at iteration t > 0, vt is defined by the user’s image
choices from the gallery. vt is separated into its “major coordinates” and “minor coordinates”: vt = (vtma j , vtmin ). Let
r = rand([−1, 1]d ), then a random sample vt+1 in our process is generated by:
t+1
t
t
v(t+1) = (vt+1
ma j , vmin ) = (vma j + α · rma j , vmin + β · rmin ),

where α and β control the level of variation. Initially, β =
0 and α > 0. Later, as the user converges towards a variation,
β is gradually enlarged in proportion to α , but the magnitude
of both become smaller.
During the search, a user can select more than one image variation. By choosing several images, we get a set of
base vectors {vti }, corresponding to all the selected image

634

L. Shapira & A. Shamir & D. Cohen-Or / Image Appearance Exploration by Model-Based Navigation

color variations. In this case, new samples are derived for
the next iteration as a random weighted affine combination
of the vectors vti . δi noise (user configurable parameter) is
added to the interpolated samples, linearly dependent on the
number of images selected:
v(t+1) = ∑ wi (vti + δi )
i

For each one of the generated vectors, a new image variation based on Equations 1 and 2 is synthesized.
To continue the search we need to display a gallery of
images to the user. Again, modeling the color variations
enables us to define a distance measure between the image variations, since each image is associated with a modified GMM. To properly display the images, we build an
affinity matrix from the mutual distances of these GMM’s,
and project the variations onto 2D using Multi-Dimensional
Scaling (MDS) [CCC00]. The matrix expresses the perceptual distance among all the images (Figure 5). The distance
metric between two GMM’s follows [SCLG05] and is defined in closed form as:





′
dist(p, p ) = − log 


 ∑ πi π j
i, j

′

|Vi j |
k
e i j |Σi | Σ′j

2 ∑ πi π′j
i, j



+ ∑ π′i π′j
i, j 

|Vi j |
k
e i j |Σi ||Σ j |

′

|Vi j |
k
e i j |Σ′i | Σ′j

(3)

Where, µ, Σ and µ , Σ are the mean and covariance matrices for the kernels of the Gaussian mixtures p(x) and
p′ (x) respectively, i and j are the indexes on the Gaussian
kernels, π{i, j} , π′{i, j} are the mixing weights of the respec′

′

′
T −1 ′
tive GMMs, ki j = µTi Σ−1
i (µi − µ j ) + µ j Σ j (µ j − µi ), and
′

−1 −1
Vi j = (Σ−1
i + Σ j ) . Note that this measure does not have
to be precise since its main objective is to arrange the images
in the gallery.

Figure 6: A screen capture of a navigation session showing images the user has already visited in the background.
The images are placed on a ring surrounding the gallery,
arranged by hue and saturation

thumbnails act as signposts, and the user can jump back and
visit them by selecting them or by backtracking his path.
Still, there are times when random exploration and selection
of favored images is not enough, and users require a finer degree of control over the navigation. We provide the user with
 three methods to initialize the navigation in a more explicit
 manner.




Initialization by Palette The palette selection tool (Fig

 ure 7) allows a user to move markers around on a 2
dimensional color map. Each marker is connected directly

to one Gaussian in the GMM, and moving it translates that
Gaussian in the color space. Each movement the user performs is reflected immediately in a preview window on the
image. Once the user approves of the result, navigation continues by creating a new gallery of image variations centered
around the user’s final selection.
Initialization by Color Brush The color brush allows direct interaction with the image (Figure 8). Upon activating
this tool, the user selects a preferred color, and brushes it

To reduce cluttering and image overlapping in the display
we relax the inter-sample distance constraints and place the
output images on a simpler, non-overlapping grid-like arrangement using a simple median cut algorithm. First, we
build a binary axis-aligned Kd-tree, where each leaf contains
only one sample. Each split is performed at the median of its
parent, where the axis is chosen to divide the longest dimension. Finally, a trivial mapping between the tree leaves and
a regular grid defines the layout of the samples (see Figure
5(e)-(f)).
As the user navigates through the galleries, a path of images is created. This path represents the color manipulations
the user has already visited. These images are kept in the
background as thumbnails, on a ring circling the current
gallery, arranged by hue and saturation (Figure 6). These

Figure 7: Initializing by directly editing the original palette:
(a) The user changes the original palette by dragging around
markers, which represent the prevalent colors in the original
image, once a satisfied result is achieved (b) a new set of
variations based on the result is created.

c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

L. Shapira & A. Shamir & D. Cohen-Or / Image Appearance Exploration by Model-Based Navigation

635

Figure 8: Initializing by brushing on colors: (a) The user
selects a color and paints it directly on the image (b) The underlying model is shifted towards the user’s selected palette
(c) A variety of results easily achieved using palette navigation.

over the image. Instead of coloring all the pixels under the
brush, we first choose the most probable Gaussian under the
brush, and color just pixels belonging to this Gaussian, but
globally on the image. Given an image I, a scribble S represented as a 1-bit mask over the image (defined by the pixels
under the tool), and a brush color cbrush , we calculate p,
ˆ the
vector of probabilities for the scribble to belong to a specific
Gaussian in the mixture as follows:
pˆ = ( pˆ1 , ..., pˆ k) =

∑

Figure 9: Initializing by example: Given a target image,
our algorithm finds a correlation between the current image model and the target image’s model. Transforming the
model creates a painting inspired by the target image. This
integrates smoothly in the creative process.

(p1 (i), ..., pk (i))/ |mask| ,

i∈mask

where i is a pixel in the image, (p1 (i), . . ., pk (i)) is a vector containing the probability for pixel i to belong to Gaussian j in the GMM. We select the Gaussian C j with the
highest probability j = arg max( pˆ1 , . . ., pˆk ), and translate its
mean towards cbrush .
Note that our color brush is somewhat similar to the appearance editing strokes presented in [AP08]. In their approach, the user’s changes are applied to areas under the
brush stroke and propagated. The propagation is done by
solving an optimization problem, using the fact that the similarity matrix of the image is low rank. In our work, we use
the coherency of the image while modeling its color distribution. Therefore, we are able to directly apply the changes
on the corresponding Gaussian.
Initialization by Example Image Selecting an example
image as an inspiration for navigation is a convenient way
to capture the color palette of that image in one operation,
rather than trying to manually recreate it [PKD05, RAGS01,
TJT05]).
Given a source image S and an example image T , we
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

first build a GMM for both images with the same number of
Gaussians, k. Next, we calculate a k by k affinity matrix, containing the distance between each pair of Gaussians in the
two models, calculated similar to equation 3, but restricted
to each pair of Gaussians. The distance is then proportional
to the probability of each Gaussian in the source image to be
assigned to each Gaussian in the target image. Starting with
the highest probability, each Gaussian in the source model is
matched with a Gaussian in the target model greedily. Once
there is a complete match, it is easy to calculate the transformation from each source Gaussian to its target, and create
a new base GMM for the image. Using this mapping, a set
of variations are presented to the user for further navigation.
Some results of using this tool can be seen in figure 9.
There are cases when navigation creates variations that
are unfit or wrong. For instance, when wrongly matching a
source image, or when artifacts are evident on the images.
Using our navigation tool the user can easily reject unwanted
variations navigating away from them and refining his selections toward better ones.

636

L. Shapira & A. Shamir & D. Cohen-Or / Image Appearance Exploration by Model-Based Navigation

6. Results
When first loading an image we construct the Gaussian Mixture Model of its color distribution. During the construction
we utilize a natural boundary map [MFM04] to add spatial
considerations, and preserve edges. Generating a new image
for the gallery consists of creating a vector within the painting search space, transforming the GMM, and synthesizing
the new image. Performance statistics on all steps of the algorithm can be found in Table 1. A wide variety of results
can be seen in Figure 12 and the supplementary material.
User Study To validate our results and test the effectiveness of our approach, we have conducted an informal user
study. Seventeen people participated in the study, most of
them experienced computer users, seven of them professional graphic designers. Although the target audience for
the tool is varied, the selection of subjects enabled a substantive comparison against a well established image processing
application, such as Adobe Photoshop.
The test subjects received a ten minute introduction to our
application, following which, they were given four tasks.
They were asked to perform each task both in Photoshop,
and in our application (see Figure 10 and supplemental material). Once completing the tasks, each subject filled out an
anonymous questionnaire. The analyzed results of the study
are presented below.
To measure the effectiveness of our approach, we compared the performance of users in the four tasks versus their performance in Photoshop (Questions 9,13,17,22
in supplemental material). In order to analyze the results
we employed a paired t-test. Each test requires pairs of
observations,(Ai , Bi ), which are independent across i. In our
case each pair consists of a single user’s answer for a question regarding our application (Ai ) and Photoshop (Bi ). A
paired t-test was defined check if µa is significantly larger
than µb using the following hypotheses
H0 : µa − µb ≤ 0
H1 : µa − µb > 0
H0 is the hypothesis that completing the tasks in PhotoImage
Dress (Fig. 8)
Tulips (Fig. 6)
Blue (Fig. 5)
Nature (Fig. 12)

Size
300x500
500x400
700x700
1000x760

GMM
0.766
1.7
2.38
2.57

Synthesize
0.23
0.26
1
1.45

Table 1: Performance statistics for our application. GMM is
the time (in seconds) required to model the image using EM.
Synthesize is the time (in seconds) it takes to synthesize one
color manipulated image and insert it into the gallery. All
tests were done on a 1.8Ghz Pentium 4 machine with 2GB
RAM.

Figure 10: In the user study, each user was asked to perform
four tasks in the painting images application, and again in
Adobe Photoshop: (a) Change the color of the leaves to red,
(b) Change the painting of the image to match the colors of
the website, (c) Starting from a modified version of an image,
return it to its original colors, and (d) Select an image of
your choice and freely create an alternative creative vision.

shop is more effective than in the navigation demo. A paired
t-test on our observations yielded a T value of 3.3788 and a
p-value of 0.0037, signifying that H0 is rejected with statistical significance. Hence, the tasks were accomplished more
effectively and quickly using our navigation application.
An important aspect of image processing and manipulation of colors is creativity. In order to test if our demo supports inspiration and creativity, we designed the tasks such
that each could be “solved” in various ways. Moreover, success in each test was measured by the users themselves. We
asked the users if they found the demo intuitive and inspiring (Questions 8,12,16,21 in supplemental material). For all
these tests, a paired t-test resulted in a p-value smaller than
0.05, meaning that users found the navigation application
more intuitive than Photoshop with statistical significance.
To further demonstrate the effectiveness of our approach, we
enclose a demonstration of our application in the accompanying material.
Limitations Our appearance search space represents a
wide variety of manipulations. However, not all color manipulations can be expressed by our model. For example, applying different manipulations to different regions in the image
as seen in Figure 11 (right). A simple way to overcome this
is to allow to model an image by parts or use masks on the
resulting image.
Changing colors in an image may still cause artifacts. Artifacts usually occur when Gaussians, whose mean values
are similar, are translated in opposite directions (Figure 11,
left). Using the L ∗ A ∗ B∗ color appearance model, designed
to approximate perceptual distance, can significantly reduce
the number of such artifacts, and sometimes using more
Gaussians in the mixture can also alleviate the problem.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

L. Shapira & A. Shamir & D. Cohen-Or / Image Appearance Exploration by Model-Based Navigation

637

7. Conclusion
In this paper we presented a method to modify the appearance of an image by navigation. Modeling an image’s color
distribution via a GMM, allowed us to define a space of rich
color variants on the image. Each point in this space represents a specific variant, and we provide tools and an interface for a user to navigate within it. Working directly on
the image, selecting favorites or brushing on colors, remains
simple and abstract, while giving the users a high degree of
control. It also promotes inspiration by providing access to a
vast space of options with simple exploration.
In the future we would like to improve our underlying
color model, for instance use a newer color appearance models such as CIECAM02 in order to better reflect perceptual
color differences on images. Moreover, we believe that modeling the color distribution dynamically, as the user navigates, could enhance the range of possible color manipulations, and model more accurately the user’s desires. Lastly,
providing a means to work on sub-parts of the image can
greatly enhance the local control of color changes.
References
[Ado07]
2007.

A DOBE S YSTEMS : After-effects cs3. Software, July

[AP08] A N X., P ELLACINI F.: Appprop: all-pairs appearancespace edit propagation. ACM Trans. Graph. 27, 3 (2008), 1–9.
[BPD06] BAE S., PARIS S., D URAND F.: Two-scale tone management for photographic look. In SIGGRAPH ’06: ACM SIGGRAPH 2006 Papers (New York, NY, USA, 2006), ACM Press,
pp. 637–645.
[CCC00] C OX T. F., C OX M. A. A., C OX T. F.: Multidimensional Scaling, Second Edition. Chapman & Hall/CRC, September 2000.
[COSG∗ 06]

C OHEN -O R D., S ORKINE O., G AL R., L EYVAND
T., X U Y.-Q.: Color harmonization. ACM Trans. Graph. 25, 3
(2006), 624–630.

[GW01] G ONZALEZ R. C., W OODS R. E.: Digital Image Processing. Addison-Wesley Longman Publishing Co., Inc., Boston,
MA, USA, 2001.
[ICOL05] I RONY R., C OHEN -O R D., L ISCHINSKI D.: Colorization by example. In Proceedings of Eurographics Symposium
on Rendering (EGSR’05) (Konstanz, Germany, June 29 - July 1
2005), pp. 201–210.
[JKM01] JANKUN -K ELLY T., M A K.-L.: Visualization exploration and encapsulation via a spreadsheet-like interface. IEEE
Transactions on Visualization and Computer Graphics 07, 3
(2001), 275–287.
[LFUS06] L ISCHINSKI D., FARBMAN Z., U YTTENDAELE M.,
S ZELISKI R.: Interactive local adjustment of tonal values. In
SIGGRAPH ’06: ACM SIGGRAPH 2006 Papers (New York, NY,
USA, 2006), ACM Press, pp. 646–653.
[LLW04] L EVIN A., L ISCHINSKI D., W EISS Y.: Colorization
using optimization. ACM Trans. Graph. 23, 3 (2004), 689–694.
[LWCO∗ 07] L UAN Q., W EN F., C OHEN -O R D., L IANG L., X U
Y.-Q., S HUM H.-Y.: Natural image colorization. In Rendering
Techniques 2007 (Proceedings Eurographics Symposium on Rendering) (June 2007), Kautz J., Pattanaik S., (Eds.), Eurographics.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Figure 11: Limitations: The left image exhibits color bleeding. These artifacts occur when Gaussians modeling pixels
similar in colors and close spatially, receive transformations
in opposite directions. The right image demonstrates a type
of manipulation which cannot be expressed by our model. In
this example a single Gaussian was split in three, each part
shifted towards a different color.

[MAB∗ 97] M ARKS J., A NDALMAN B., B EARDSLEY P. A.,
F REEMAN W., G IBSON S., H ODGINS J., K ANG T., M IRTICH
B., P FISTER H., RUML W., RYALL K., S EIMS J., S HIEBER
S.: Design galleries: a general approach to setting parameters
for computer graphics and animation. Computer Graphics 31,
Annual Conference Series (1997), 389–400.
[MFM04] M ARTIN D. R., F OWLKES C. C., M ALIK J.: Learning
to detect natural image boundaries using local brightness, color,
and texture cues. IEEE Transactions on Pattern Analysis and
Machine Intelligence 26, 5 (2004), 530–549.
[OW04] O MER I., W ERMAN M.: Color lines: Image specific
color representation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR 04) (June
2004), vol. II, IEEE, pp. 946–953.
[PKD05] P ITIÉ F., K OKARAM A., D AHYOT R.: N-Dimensional
Probability Density Function Transfer and its Application to
Colour Transfer. In International Conference on Computer Vision (ICCV’05) (Beijing, October 2005).
[RAGS01] R EINHARD E., A SHIKHMIN M., G OOCH B.,
S HIRLEY P.: Color transfer between images. IEEE Comput.
Graph. Appl. 21, 5 (2001), 34–41.
[RWPD05] R EINHARD E., WARD G., PATTANAIK S., D EBEVEC
P.: High Dynamic Range Imaging, 1 ed. The Morgan Kaufmann
Series in Computer Graphics. Morgan Kaufmann Publishers, August 2005.
[SCLG05] S FIKAS G., C ONSTANTINOPOULOS C., L IKAS A.,
G ALATSANOS N. P.: An analytic distance metric for gaussian
mixture models with application in image retrieval. In Artificial Neural Networks: Formal Models and Their Applications ICANN 2005 (2005), vol. 3697/2005, Springer Berlin / Heidelberg, pp. 835–840.
[TJT05] TAI Y.-W., J IA J., TANG C.-K.: Local color transfer via probabilistic segmentation by expectation-maximization.
In CVPR ’05: Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
(CVPR’05) - Volume 1 (Washington, DC, USA, 2005), IEEE
Computer Society, pp. 747–754.
[VNV06] V ERBEEK J. J., N UNNINK J. R., V LASSIS N.: Accelerated em-based clustering of large data sets. Data Min. Knowl.
Discov. 13, 3 (2006), 291–307.
[ZK04] Z ABIH R., K OLMOGOROV V.: Spatially coherent clustering using graph cuts. cvpr 02 (2004), 437–444.

638

L. Shapira & A. Shamir & D. Cohen-Or / Image Appearance Exploration by Model-Based Navigation

Figure 12: A gallery of explored image variations, each source image is displayed with several creative and easy to achieve
variations.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

