EUROGRAPHICS 2008 / G. Drettakis and R. Scopigno
(Guest Editors)

Volume 27 (2008), Number 2

Zippy: A Framework for Computation and Visualization on a
GPU Cluster
Zhe Fan, Feng Qiu, and Arie E. Kaufman
Computer Science Department, Stony Brook University, NY, USA

Abstract
Due to its high performance/cost ratio, a GPU cluster is an attractive platform for large scale general-purpose
computation and visualization applications. However, the programming model for high performance generalpurpose computation on GPU clusters remains a complex problem. In this paper, we introduce the Zippy framework, a general and scalable solution to this problem. It abstracts the GPU cluster programming with a two-level
parallelism hierarchy and a non-uniform memory access (NUMA) model. Zippy preserves the advantages of both
message passing and shared-memory models. It employs global arrays (GA) to simplify the communication, synchronization, and collaboration among multiple GPUs. Moreover, it exposes data locality to the programmer for
optimal performance and scalability. We present three example applications developed with Zippy: sort-last volume rendering, Marching Cubes isosurface extraction and rendering, and lattice Boltzmann flow simulation with
online visualization. They demonstrate that Zippy can ease the development and integration of parallel visualization, graphics, and computation modules on a GPU cluster.
Categories and Subject Descriptors (according to ACM CCS): I.3.2 [Computer Graphics]: Graphics Systems–
Distributed/network graphics; D.1.3 [Programming Techniques]: Concurrent Programming–Parallel Programming

1. Introduction
Using a GPU cluster for general-purpose computation and
visualization is becoming increasingly important. As researchers have accelerated a wide range of applications on
the GPU [OLG∗ 07], the need arises for using the GPU cluster to achieve further acceleration and to support an increase
in the problem size. Moreover, because of the high performance/cost ratio and the fast performance growth of GPUs,
GPU clusters seem very promising for future high performance computing (HPC). Recently, some major GPU vendors have targeted the HPC market. For example, NVIDIA
has announced the Tesla S870, a 1U rack-mount server with
4 GPUs dedicated to computation. AMD has announced the
FireStream 9170 which supports double-precision floating
point computation.
Programming a GPU cluster, however, is a complicated task. Researchers have explored GPU clusters for
several specific applications, such as volume compression
and rendering [SMW∗ 04], occlusion culling [GSYM03,
XPQS06], biological sequence search [HHH05], flow simc 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

ulation [FQKYS04], surface flow visualization [BSWE06],
N-body simulation, finite element computation [GSMY∗ 07],
and data clustering [TK06]. In these implementations,
the GPUs are programmed using the stream processing
model [BFH∗ 04, MDP∗ 04] and the communication among
the cluster nodes are programmed using the Message Passing Interface (MPI). With MPI, the programmer can explicitly control the data locality and communication, hence
application performance can be understood and optimized.
However, the programming is tedious. MPI, sometimes referred to as “the assembly language of parallel computing,”
is low-level and error prone. For example, for every data
transfer, the programmer needs to explicitly specify which
processor sends and which receives the data. Some algorithms are complex to express in this way. Moreover, because MPI only deals with system memories of cluster nodes
rather than GPU memories, integrating MPI and GPU computation is very inconvenient for the programmer.
Moerschell and Owens [MO06] have implemented a distributed shared-memory (DSM) abstraction for multi-GPU

342

Z. Fan, F. Qiu, and A. Kaufman / Zippy: A Framework for Computation and Visualization on a GPU Cluster

environments. This method virtualizes the distributed texture memories as a shared-memory and allows computation
kernels to directly read and write remote data, hence simplifies the programming. However, currently it has only been
implemented on a dual-GPU PC and the dual-GPU program
has not outperformed the single-GPU program. An important contribution of their work is identifying the bottlenecks
of the DSM and discussing possible solutions. One major bottleneck is the high latency overheads associated with
maintaining the data consistency. Due to current GPU hardware and driver limitations, each program using the DSM
must be separated into multiple passes that read data from or
write data to GPUs. A page-based method is used to maintain the data consistency; when a computation accesses a
part of a remote page, the whole page will be transferred.
Choosing a proper page size is important to reduce the performance loss caused by unneeded data transfers. Also, the
DSM hides data locality and communication. A side effect
is that it is unclear to the programmer how the application
performance is affected by the data access patterns or how
to improve the patterns.
In this paper, we describe the Zippy framework for GPU
cluster programming. It combines the best features of the
MPI+stream processing method and the DSM method. It abstracts a GPU cluster with two characteristics critical to high
performance, the non-uniform memory accessing (NUMA)
and the two-level parallelism hierarchy, and hides other details from the programmer. A GPU cluster is a NUMA environment: the bandwidth of remote GPU memory access
is several orders of magnitude lower than local GPU memory access while the latency is several orders of magnitude
higher. Therefore, Zippy allows the programmer to manage
data locality and communication. Convenient coarse-grained
communication operations are used to bring remote data to
local GPU memory and stream processing kernels only operate on the local data. In other words, Zippy provides the
programmer a clear view of two levels of parallelism, coarsegrained parallelism among all GPUs and fine-grained parallelism within each GPU, instead of mixing them.
To present the NUMA and the two-level parallelism hierarchy to the programmer, we adapt the Global Arrays (GA)
programming model [NPT∗ 06] to the GPU cluster and combine it with the stream processing model. Compared with the
MPI-based method, our method has two advantages: (1) its
high level data structures and API encapsulate the low-level
communication details using global spaces and simplify the
programming; (2) as the n-dimensional array is the common
data structure for both GA and stream processing, they are
seamlessly integrated into one framework. Compared with
the DSM, our method also provides a virtual shared memory
space but is simpler to implement. Moreover, it exposes data
locality and communication to the programmer. The programmer has precise control of when and which data region
is transferred, which is unavailable in the DSM. Therefore,
better performance and scalability can be obtained.

Previous software systems [HHN∗ 02, BRE05] for graphics clusters are specialized for parallel rendering and mainly
handle the transfer of image data and graphics commands.
With the programmability of modern GPUs, Zippy tries to
provide a general programming interface, to blur the boundaries between graphics, visualization, and computation, and
to expressly support the graphics and visualization community with its tasks on GPU clusters. We demonstrate three
example applications developed using Zippy: (1) sort-last
volume rendering, a traditional parallel graphics application;
(2) Marching Cubes isosurface extraction and rendering, a
more complex graphics algorithm that benefits from our general programming interface; and (3) lattice Boltzmann model
flow simulation and online visualization, an integration of
general-purpose computation and visualization.
There are other related research directions. Fatahalian et
al. [FKH∗ 06] have proposed the Sequoia programming language and implemented it on a Cell workstation and a PC
cluster. Unlike Sequoia that focuses on memory hierarchies
and abstracts programs as trees of memory modules to address vertical communication, Zippy focuses on the twolevel parallelism hierarchy and uses a regular GA data structure to simplify horizontal communication. However, both
explicitly expose data locality and communication and encourage block data movement. Yamagiwa and Sousa [YS07]
have proposed the Caravela system for programming GPUbased GRID environments, where the resources are loosely
coupled and the primary goal is to efficiently manage a
large amount of heterogeneous resources. In a GPU cluster,
however, the resources are tightly coupled and the primary
goal is to achieve the best possible application performance.
Manzke et al. [MBO∗ 06] are developing a hardware interconnection in a GPU cluster.
2. Zippy Overview
In a GPU cluster, each GPU can only access its onboard texture memory, and there is no direct connection between any
two GPUs. If a GPU needs data from the memory of another GPU, a two-layer communication is involved. The data
is uploaded from the GPU memory to the system memory
on the source node, transferred through the network, then
downloaded to the GPU memory on the destination node.
The two-layer communication would aggravate the difficulty
of programming with MPI. However, to provide a virtually
shared space to ease the communication among multiple
GPUs, we believe it is important to make the NUMA and
the two-level parallelism hierarchy explicit to programmers.
Table 1 lists the theoretical bandwidth and latency of the
GPU memory, PCI-Express bus, and the network. The actual
data transfer performances are considerably lower. The last
two rows show the theoretical bandwidth and latency for a
GPU to access data in the memory of a remote GPU. Compared with the GPU memory, the bandwidth is 2-3 orders
of magnitude lower and the latency is 2-4 orders of magc 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Z. Fan, F. Qiu, and A. Kaufman / Zippy: A Framework for Computation and Visualization on a GPU Cluster

nitude higher. In fact, due to hardware limitations, a GPU
cannot transfer data through the PCI-Express bus when it is
computing, which leads to much higher latency. Because of
this NUMA nature, Zippy differentiates between the shared
space and local space and exposes data locality to the programmer for optimal performance. To alleviate the effects
of the high remote GPU memory access latency, the sharedspace should be accessed only with coarse-grained operations such as block copy. Also, the stream processing should
only operate on the local space, so that the computation does
not wait long for an expensive communication.
Table 1: Theoretical bandwidth and latency in a GPU cluster, which demonstrates a NUMA characteristic.

GPU memory
PCI-Express
Infiniband
GigaE
Remote GPU memory
on Infiniband cluster
Remote GPU memory
on GigaE cluster

Bandwidth
100 GB/sec
4 GB/sec
2.5 GB/sec
0.1 GB/sec

Latency
0.05 µ sec
0.5 µ sec
1 µ sec
50 µ sec

2.5 GB/sec

2 µ sec

0.1 GB/sec

50 µ sec

Because of the large performance difference between
transferring data among GPUs and within a GPU, Zippy abstracts the GPU cluster with the two-level parallelism hierarchy. Unlike the MPI+streaming processing method, in Zippy
the two levels are tightly coupled and seamlessly integrated.
Unlike the DSM, in which inter-GPU communication can
be implicitly triggered by GPU computation, in Zippy the
boundary between two levels are clear to the programmer
and the communication operations are explicitly initiated.
At the coarse level, multiple GPUs collaborate with each
other. We employ the GA model [NPT∗ 06] for programming
this coarse-grained parallelism. GA has been developed in
the parallel computing community and has been employed
in applications, such as matrix multiplication, computational
chemistry and physics, and electronic structure. It employs
a global array data structure and a set of shared-memory
style functions for convenient data transfers in the global array space. In addition, GA acknowledges the NUMA characteristics of distributed-memory architectures. It exposes
data locality and the management of communication to the
programmer. The programmer can make sure that only the
needed data are transferred. The original GA toolkit was
proposed for PC clusters and supercomputers. To adapt the
GA model to a GPU cluster for programming the two-level
parallelism hierarchy, we have implemented a new objectoriented library that encapsulates the details of the GPU
computation and the two-layer communication.
At the fine level, existing GPGPU toolkits based
on streaming processing, such as Cg, GLSL, HLSL,
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

343

// 3D global array , size 8 x 6 x 6
ZDimension oDimGA( 3, 8, 6, 6 );
// Each element is a 32−bit float
ZGlobalArrayType oType( oDimGA, 1, FLOAT32 );
// Divide axis X into three parts , size 3, 2, and 3.
int anSizes [3] = { 3, 2, 3 }; oType. SetSplit ( 0, 3, anSizes );
// Divide both axes Y and Z evenly into two parts .
oType. SetSplit ( 1, 2, NULL );
oType. SetSplit ( 2, 2, NULL );
// Use global−array type to create global arrays
ZGlobalArray∗ GA0 = zCreateGlobalArray( ‘‘GA0’’, oType );
ZGlobalArray∗ GA1 = zCreateGlobalArray( ‘‘GA1’’, oType );

Listing 1: An example of creating global arrays.
Brook [BFH∗ 04], Sh [MDP∗ 04], NVIDIA CUDA, RapidMind, and PeakStream, have provided efficient ways to program computation kernels executed on a single GPU in a
SIMD fashion. Because arrays are the fundamental data
structures in GA and arrays are also natural for expressing
stream processing on GPUs, Zippy seamlessly integrates the
two levels of parallelism.
3. Zippy Framework
The Zippy programmer writes the C++ program using the
library classes and functions of Zippy. Multiple processes
that execute the program are launched in the cluster. Each
process with a unique ID runs on a cluster node. It controls
a local GPU and collaborates with other processes. There is
no centralized resource in the cluster.
3.1. Data Structures
The basic data structures are n-dimensional arrays, including local arrays and global arrays. The data of a local array
reside in the texture memory of the local GPU. The data of a
global array are distributed to the texture memories of multiple GPUs in the cluster. A global array is partitioned into
rectangular regions, called chunks, each owned by one GPU.
The global array provides a global space for easy data movement among GPUs. In addition, the local chunk of a global
array implicitly defines a local array. The programmer may
use a Zippy library function to obtain the pointer of this local array, so that the local chunk can be accessed in the same
way as other explicitly created local arrays.
Listing 1 shows an example of specifying a partition pattern and creating global arrays. The function of creating a
global array (zCreateGlobalArray) is a collective operation,
which means it is called by all processes. Each process allocates its local chunk in its GPU memory and stores a copy
of the partition pattern information in its system memory.
Zippy provides functions for the program to query the data
locality information at run time, such as which GPUs hold
a particular region of a global array, which region a GPU
owns, and the translation of a region between global space
and local space.

344

Z. Fan, F. Qiu, and A. Kaufman / Zippy: A Framework for Computation and Visualization on a GPU Cluster

3.2. Coarse Level Parallelism
Zippy provides a set of shared-memory style functions to
easily move data. The data can be copied within a global
array, from a global array to another, from a global array to
a local array, and vice versa. The programmer only needs to
specify the source and destination regions in the array space
and does not need to concern about which GPUs send or
receive the data. The low-level details are encapsulated in the
Zippy implementation. Moreover, because the data locality
is exposed to the programmer, he/she can assess the cost of
communication and use it to optimize the performance.
Data movement can be carried out by collective blocking functions, collective non-blocking functions, and onesided functions. The collective blocking/non-blocking functions should be called by all processes in order to get correct
results. A collective blocking function call will not return until the process completes its task locally. For any uninvolved
process the function call returns immediately. A collective
non-blocking function only reads out appropriate data from
the GPU if needed, starts the network transfers, and then
returns. A wait function must be called later to wait until
the network transfers are finished and to write the received
data to the GPU. The programmer can insert local computation before the wait function call, so that the network transfers can be overlapped with the computation. The one-sided
functions only need to be called by one process. In other processes, the responses are automatically provided by Zippy,
and the implementation is transparent to the programmer.
Four collective functions are provided: copy, composite,
ghost cell update, and chunk re-arrangement. The copy function moves a region of data from one global array to another.
The source and destination arrays can be the same or different, and the regions can be disjoint or overlapping. Listing 2
shows an example and the procedure is illustrated in Fig. 1.
The composite function is similar to copy, except that it has
an extra input parameter which is a pointer of a predefined or
user-written computation kernel specifying how the source
data are combined with the destination data.
The ghost cell support is for computations on regular grids
that sample the neighboring grid points, such as physical
simulations, and volume and image processing. When defining a global array type, the thickness of the ghost cell layer
in each dimension and either direction can be specified. With
a layer of ghost cells, each stored chunk is slightly larger
than the actual portion of data assigned to the process. In
the ghost cell update function, each process reads out its
boundary data from the GPU, sends the data to the neighbors, receives data from the neighbors, and updates the ghost
cells with the received data. The chunk re-arrangement function actually involves no data movement. Instead, the program simply modifies the mapping between process IDs and
chunk IDs, so that virtually all the chunks managed by different GPUs are re-arranged in the global array space.
Two one-sided functions are provided. The get function

// 3D Region: e . g ., [0, 4) x [0, 2) x [0, 3)
ZRegion oRegion0( oDimGA, 0, 4, 0, 2, 0, 3 );
ZRegion oRegion1(oDimGA, 4, 8, 2, 4, 0, 3 );
ZRegion oRegion2( oDimGA, 2,6, 4, 6, 2, 5 );
zCopy( GA0, oRegion0, GA1, oRegion1 );
zCopy( GA0, oRegion0, GA0, oRegion2 );

Listing 2: An example of copy operations.
Region 2

Region 1
Region 0
GA0

GA1

Figure 1: Data movement of the copy operations.
moves a region of data from a global array to a local array
and the put function moves data reversely. One-sided function is called by one process. Compared with the collective
functions, this kind of remote memory access is more convenient when the communication pattern is determined at
run time. Zippy synchronization primitives include barrier
and fence functions. Barrier synchronizes all the processes.
Fence is used to execute and complete one-sided operations.
The data consistency model of Zippy is easy to understand. If a program only uses local computation and collective data movement functions, the program behavior is
deterministic and no additional synchronization is needed.
To maintain data consistency, one just needs to understand
what is locally completed by each function call. The collective blocking function completes the whole task locally. The
collective non-blocking function completes the read of data
from the GPU. The wait function completes the remaining
tasks. All these operations are ordered in the program. Because Zippy uses a collective fence function call to execute
and complete the queued one-sided operations, the order between any one-sided operation and other operations is also
deterministic. The only indeterminable order is among multiple one-sided operations issued by different processes in
the same fence. They can be executed in arbitrary order. If
they conflict with each other, for example, put to overlapping
regions, the programmer should use multiple fence function
calls to separate them.
3.3. Fine Level Parallelism
On the GPU, the kernels operate on local arrays in SIMD
fashion. The programmer specifies the computation kernel,
source arrays, source regions, destination arrays, and a destination region. Listing 3 shows an example of adding the data
elements in Region1 of Array1 and Region1 of Array2 plus
a value of 0.5 and writing the results to Region0 of Array0.
The programmer writes kernels with existing toolkits
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Z. Fan, F. Qiu, and A. Kaufman / Zippy: A Framework for Computation and Visualization on a GPU Cluster

345

ZKernel∗ pKernel = zFindOrLoadCgFP( ‘‘Add.cg’’ );
pKernel−>Enable();
pKernel−>SetParameter1f( ‘‘ AdditionalNum’’, 0.5 );
zSetSourceArray( 0, pArray1 ); // the first source array
zSetSourceArray( 1, pArray2); // the second source array
zSetSourceRegion( 0, pRegion1 ); // the first source region
zComputeTo( pRegion0, pArray0 ); // Compute and write results to destination
pKernel−>Disable();

Listing 3: An example of executing a kernel on local arrays.
(e.g., Cg, GLSL). In Listing 3, because the mapping between the source regions and the destination region is simple, Zippy computes the texture coordinates that can be directly used in the kernel to access the source data. In more
complicated cases, the kernel may access arbitrary source
data elements with the n-dimensional coordinates in the local array. Zippy provides address translation functions. The
programmer needs to pass the packing information of the
source local arrays to the kernel, and in the kernel, call the
address translation to compute the texture coordinates.
3.4. Debugging Tool

Figure 2: The debugging window contains 4 panels. In the
top left panel, array data are displayed as a series of slices.
When the programmer moves mouse cursor here, the corresponding coordinates and value are displayed in the bottom
left panel. The programmer can click on the top right panel
and use keyboard to forward the computation steps. The bottom right panel lists all arrays, from which the programmer
can select one to view. This example shows the debugging of
Marching Cubes isosurface extraction of a chair data.

Debugging has been difficult in GPU cluster programming.
Zippy provides a debugging tool that manages all arrays and
allows the programmer to select any array to view the data in
a region. The debugging tool dynamically updates the data
display at run time. It also allows to read the n-dimensional
coordinates as well as the digital value of the mouse-picked
data element. To use this tool, a debugging barrier operation
needs to be added into the program. In the program, one process ID needs to be set as the control process. When the program is launched with debugging enabled, other processes
are executed in the usual way. The control process, however,
creates a debugging window (Fig. 2), allowing to interactively forward the steps of the module, select an array, and
view the data. For a local array, the data located in the GPU
can be directly displayed. For a global array, data in the region of interest are brought to local with a get function called
by the debugging barrier operation and then displayed.

existing GPGPU toolkits [BFH∗ 04, LKS∗ 06] to map an ndimensional local array to a 2D texture. For any n > 2, assuming that we know how to map the (n − 1)-dimensional
array to a 2D patch, the n-dimensional array can be viewed
as m arrays and mapped as m1 × m2 patches, where m is
the resolution in the n-th dimension, m1 = ceil(sqrt(m)) and
m2 = ceil(m/m1). By default, the i-th chunk of a global array is stored in the texture memory of the GPU controlled
by the i-th process. The local chunk defines a local array,
hence is stored in the same way as an explicitly created local
array. Every process stores in its system memory a copy of
the partition pattern information of the global array, and this
information is managed by Zippy.

4. Implementation

4.2. Data Movement

Zippy is implemented as a C++ library based on OpenGL,
Cg and GLSL, and the Message Passing Interface (MPI). To
run the program, the programmer needs to copy his/her executable program and related data files to all cluster nodes.
A simple script file can fulfill this task. Then multiple processes that execute the same program are launched in the
cluster through an MPI launcher command. The process ID
is simply set to be the MPI rank.
4.1. Data Storage
Currently, Zippy supports 1D-7D arrays. Data of a local array are usually stored in a 2D texture, unless the programmer
indicates to use a 3D texture. We apply a method similar to
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Because every process has the data locality information of
the global arrays, when calling a collective blocking function, every process has a global view of how data will be
transferred in the cluster. Each process executes the following tasks. If either the source or destination region overlaps
the region of the local chunk, the source and destination regions are decomposed into sub-regions based on data locality. Each pair of source and destination sub-regions represents a point-to-point data transfer in the cluster. If both subregions in a pair reside in the same GPU, the involved process copies the source data to a temporary Pixel Buffer Object managed by Zippy, so that the data can be later copied
to the destination without going off the GPU.
The other pairs represent network data transfers. For each

346

Z. Fan, F. Qiu, and A. Kaufman / Zippy: A Framework for Computation and Visualization on a GPU Cluster

of these pairs, the source process binds the texture to a
Framebuffer Object (FBO) and uses glReadPixels to read
out the data from the GPU. All these pairs are sorted as follows. A graph G = (V, E) is defined where V is the set of
process IDs and (i, j) ∈ E if and only if there is a data transfer between processes i and j. We find a maximum matching [Gal86] M in G. All edges in M are removed from G
and their corresponding pairs are appended to a list L. This
procedure is repeated until E is empty. Each process selects
from L only the pairs involving itself and executes the network transfers. The senders call the MPI_Send function and
the receivers call the MPI_Recv function. Because all processes sort the pairs in L in the same way, deadlock will
not happen. The senders and receivers reply on MPI_Send
and MPI_Recv to be pair-wisely synchronized and there is
no need for global synchronization. The above pair sorting
also tends to allow maximal number of concurrent point-topoint data transfers. After receiving the data from the network, each receiver binds the appropriate texture to an FBO
and uses glDrawPixels to write the data. For the composite
function, however, the destination and source data are copied
to temporary local arrays and combined into the destination.
The implementation of a collective non-blocking operation is similar. The function call reads out data from the GPU
if necessary, starts network transfers with the non-blocking
MPI_Isend and MPI_Irecv, and returns. The wait function
uses MPI_Wait to wait for the network transfers to finish,
and writes the received data to the GPU, if necessary.
For a one-sided function, only the process requesting the
data access needs to explicitly call the function. In other
processes, the service provided by Zippy has to take control somewhere. For simplicity, we implemented a method
similar to Thakur et al.’s [TGT05] fence implementation for
the MPICH2. When a get/put function is called, the program
does nothing but locally queuing up the command of this
operation. The queued operations are executed when a collective fence function is called. In the fence function, all processes send queued requests to others and execute services
for others. With the optimization proposed by Thakur et al.,
barrier synchronization can be avoided.

4.3. Local Computation
The local computation interface has two layers: the upper
provides the high-level abstraction while the lower encapsulates the GPU programming. Although in the latter only Cg
and GLSL are supported now, incorporating other toolkits is
feasible. In a computation, the shader program of the kernel is bound to fragment processors and textures of source
arrays are bound to texture units. The textures of the destination arrays are attached to an FBO. The n-dimensional
destination region is decomposed into a series of 2D slices,
each representing a rectangle region to be rendered.
As mentioned in Sec. 3.3, two methods are available for a

kernel to access the source data elements. In the first method,
Zippy automatically translates the local array address to the
texture coordinates. For each slice in the destination region,
the positions of the corresponding slices of the source regions are computed on the CPU. With hardware interpolation, the texture coordinates of the source data elements are
computed and can be directly used in the kernel. This is convenient for computation with fixed data access patterns. In
the second method, a set of optimized Cg and GLSL functions are provided for translating arbitrary n-dimensional coordinates to texture coordinates in the kernel. The packing
information of the source arrays is a parameter of these functions. The programmer needs to call a Zippy function to pass
this parameter to the kernel. This method is desired when arbitrary data access is needed, as in raycasting.
5. Example Applications
Three example applications have been implemented with
Zippy and tested on a Gigabit Ethernet cluster. Each cluster node has an NVIDIA Quadro 4500 GPU, dual Intel Xeon
3.6GHz CPUs, and a PCI-Express bus.
5.1. Sort-Last Volume Rendering
For the sort-last volume rendering, two global arrays are defined. One is a 3D global array, representing the volume
data. The other is a 2D global array that treats all partial
images as a whole large image. The rendering of a local volume chunk into a partial image is a local computation of
3D texture based volume rendering. Then, the partial images are composited for the final image with the binaryswap algorithm [MPHK94]. At each step, partial images are
transferred between pairs of GPUs. Using Zippy, the image compositing algorithm can be easily specified in the
global space. First, based on the order of the distances between subvolumes and the view point, all partial images are
virtually re-arranged using the chunk re-arrangement function. Then, the binary-swap is implemented with the composite and copy functions. The programmer only specifies
the source and destination regions without concerning about
how the GPU, system memory, and network are involved.
Fig. 3 shows snapshots of the rendering of a 1875 × 512 ×
512 visible human CT data. With 8 GPUs, this data was rendered at 11 FPS. The user can interactively change the transfer function and explore the volume.
In our scalability experiment, each GPU renders a 5123
subvolume, the image size is 10242 , and the sampling step
size is 1. The performance is reported in Fig. 4. With 16
GPUs, a 2GB volume can be rendered at 8 FPS, which gives
a 16 GVox/sec overall performance. In another experiment,
we disabled the local rendering and only tested the performance of the parallel image compositing, which is mainly
determined by the communication cost. The image compositing achieves 16 FPS. Our performance is comparable
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

347

Z. Fan, F. Qiu, and A. Kaufman / Zippy: A Framework for Computation and Visualization on a GPU Cluster
i:

Frames per second

Figure 3: Snapshots from the sort-last rendering of the
1875 × 512 × 512 visible human CT data.
20
16
12
8
4
0

1

2

3

4

5

a

b

c

d

e

f

{M i}: numbers of outputs

0

3

0

4

2

0

{P i}: prefix-sum of {M i}

0

0

3

3

7

9

{(i,j) k}: indices

1,0 1,1 1,2 3,0 3,1 3,2 3,3 4,0 4,1

{O k}: outputs

b0

b1

b2

d0

d1

d2

d3

e0

e1

Figure 5: Directly generating outputs in a densely packed
form for stream amplification.
number of outputs. While this paper was under review, we
learned of a concurrent work [DZTS07] that uses another
method, HistoPyramids, for stream amplification and MC.

1 GPU
2 GPUs
4 GPUs

0

{Ii}: inputs

8 GPUs

16 GPUs

0 1 2 3 4 5 6 7 8 1 9 10 11 12 13 14 15 162
Billions of Voxels

Figure 4: The performance of the sort-last volume rendering
on our GPU cluster. Each GPU renders a 5123 subvolume.
The problem size scales up when more GPUs are used.
with the fastest previous implementations. Houston [Hou04]
has implemented a system using Chromium. His implementation also exploited the GPUs for image compositing. On
a 16-node Gigabit Ethernet GPU cluster, the overall performance was 8 GVox/sec and the image compositing was executed at 17 FPS.
5.2. Marching Cubes
Marching Cubes (MC) used to be difficult to implement
on GPUs because each cube can generate 0 to 5 triangles
and previous GPU shading programs did not support variable size outputs. Researchers [KW05, KSE04] have used
the alternative method, Marching Tetrahedra, for GPU-based
isosurface extraction. Now, the new DirectX 10 compatible
GPUs have supported variable size outputs in the geometry shader. With this capability, Crassin [Cra07] has implemented Marching Cubes on an NVIDIA GeForce 8800 GTS.
We have used another method to implement MC, which
does not require the DirectX 10 compatible GPUs. The idea
is to simulate variable size outputs using the GPU-based
prefix-sum algorithms [Hor05, SLO06]. Horn [Hor05] has
implemented the first prefix-sum algorithm on the GPU.
Based on it, he has efficiently simulated stream compaction,
in which each input generates 0 or 1 output. Sengupta et
al. [SLO06, SHZO07] have proposed a faster work-efficient
GPU implementation of prefix-sum. Adopting Sengupta et
al.’s prefix-sum method and modifying Horn’s simulation of
variable size outputs, we efficiently simulate stream amplification for MC, in which each input generates a variable
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Our idea is to directly generate the outputs in a densely
packed array. The procedure is illustrated in Fig. 5. Given an
input array {Ii } of size n, we compute the numbers of outputs
and store them in array {Mi }. Then, the prefix-sum of {Mi }
is computed to array {Pi }. By the definition of prefix-sum,
Pi = ∑0≤l<i Ml . The array {Pi } gives two pieces of information: (1) the total number of outputs, denoted as N, equals
(Pn−1 + Mn−1 ); and (2) the outputs of Ii will start at position
Pi and end at position (Pi + Mi − 1) in the densely packed
output array. By enabling a fragment program and drawing
lines, array {(i, j)k } is computed. Each element of this array indicates that the corresponding output is the ( j + 1)th
output of Ii . With this information, the final outputs can be
computed. The last two arrays in Fig. 5 are virtually of size
N. Their physical size, L, is no less than N. L is initially estimated and dynamically updated as needed. The update of L
causes the reallocation of storage, but simple rules can make
this reallocation happen infrequently. For example, if N > L,
L = 2 × N; if N < L/10, L = max(N, 4096).
With Zippy, the MC algorithm is easily translated to the
kernels that operate on the local arrays. We have implemented a stream amplification function that encapsulates the
above computation of array {(i, j)k } from array {Mi }. The
local MC computation hence is fulfilled in three steps: (1)
compute {Mi }, the number of triangles generated in each
voxel; (2) call the stream amplification function to obtain array {(i, j)k }; and (3) compute the output array of triangles
based on array {(i, j)k }. The output array is then copied to
a Vertex Buffer Object and directly rendered with OpenGL.
The volume is also rendered together with the isosurface. We
have tested our program on a separate machine that has an
NVIDIA GeForce 8800 GTS and have compared the performance with Crassin’s. For a 643 volume, Crassin’s optimized
implementation achieved 44 to 77 FPS and ours achieved 92
to 240 FPS for various isovalues.
To parallelize the MC is simple. A global array is defined
for the whole volume. Each process independently computes
the local isosurface in local arrays and renders the local
data. The parallel image compositing module in Sec. 5.1 is

348

Z. Fan, F. Qiu, and A. Kaufman / Zippy: A Framework for Computation and Visualization on a GPU Cluster

(b)

Frames per Second

Figure 6: The Marching Cubes isosurfaces of (a) the engine
dataset, and (b) the lobster dataset.
80
60
40
20

Figure 8: A snapshot of the LBM flow simulation. The vorticity magnitude is volume rendered to show the turbulence.
8
Steps per second

(a)

6
4

1 GPU
2 GPUs

4 GPUs

8 GPUs

16 GPUs

2
0

0
1

2

3

4

5 6

7

8 9 10 11 12 13 14 15 16
GPUs

Engine, without rendering
Engine, with rendering

Lobster, without rendering
Lobster, with rendering

Figure 7: The performance of the Marching Cubes isosurface extraction on our GPU cluster for the 1283 engine data
and the 256 × 254 × 57 lobster data. The image size is 8002 .
Isovalue 106 was used in all tests.
reused here to get the final image. The system allows the
user to change the isovalue and view the result in real-time.
Fig. 6 shows snapshots of a 1283 engine CT data and a
256 × 254 × 57 lobster CT data. The original image size is
8002 . The performance on our GPU cluster is reported in
Fig. 7. For the pure MC computation, there is no communication cost. However, the frame rate is not proportional to
the number of GPUs because subproblems become smaller
as more GPUs are used and our MC implementation is less
efficient for smaller volumes.
5.3. LBM Flow Simulation and Visualization
The lattice Boltzmann model (LBM) is a computational fluid
dynamics method. It has been introduced to graphics applications by Wei et al. [WZF∗ 04]. On a regular grid, each grid
point has 19 particle distributions representing the probability of a particle moving in 19 directions. We have previously
implemented a hand-optimized LBM program [FQKYS04]
using MPI. Each GPU computes an LBM grid. In every computation step, the particle distributions at boundary sites may
move to adjacent grids, which requires data transfer among
GPUs. In the 3D case, each GPU exchanges data with 6 nearest neighbors axially and 12 secondary neighbors diagonally.
For each direction, a distinct subset of particle distributions
needs to be transferred. Thus, cumbersome tasks are needed

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Millions of grid points
Previous implementation (MPI), without rendering
New implementation (Zippy), without rendering
New implementation (Zippy), with rendering

Figure 9: Performance comparison between our previous
and new implementations on the same GPU cluster. In the
test, each GPU manages an 1003 sub-grid and the problem
size scales up when the number of GPUs increases.
for extracting data from the GPU, packing them, managing
network communication, and overlapping the communication with computation.
Using Zippy, we have implemented a new LBM program.
The entire simulation is defined in the global space. Each
GPU operates on its local chunk. With Zippy ghost cell support, a layer of ghost cells are defined around each local
chunk. The communication described in the previous paragraph is simply accomplished by non-blocking ghost cell update function calls. Based on a single-GPU version, which
contains 750 lines of C++ code and 550 lines of Cg code,
only 100 lines of C++ code are added to define the global
arrays and update the ghost cells for the GPU cluster implementation. The dynamic simulation results on the GPUs can
be directly visualized without going off the GPUs. Since the
simulation and visualization modules are both developed using Zippy, the simulation module allows the rendering module to directly access its data. Fig. 8 shows a snapshot of a
simulation of grid size 400 × 200 × 200. For an image size of
8002 , the overall speed is about 4.5 FPS on 16 GPUs. Compared with our previous MPI-based implementation on the
same GPU cluster, the new implementation achieves better
performance as shown in Fig. 9.
In Fig. 10, we divide the execution time of each example
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Z. Fan, F. Qiu, and A. Kaufman / Zippy: A Framework for Computation and Visualization on a GPU Cluster

Percentage of execution time

Overhead (barrier sync + runtime logic)
Waiting for remote data
GPU computation
100%
80%
60%
40%
20%

0%
Number of
GPUs

2

4
8 16
Volume
rendering

2
4
8 16
Marching Cubes
with rendering

2

4
8 16
LBM flow
computation

Figure 10: Percentages of time spent on different tasks.

application into three parts: overhead time spent on barriers and Zippy logic, time waiting for data transfers through
PCI-Express bus and the network, and time spent on local
GPU computation. In the sort-last volume rendering and the
LBM computation, the percentage of time spent on overhead operations is small. Both the percentages of time spent
on overhead operations and time waiting for remote data increase slowly as the number of GPUs increases. This shows
good scalability as the computational power of GPUs can
still be efficiently used when the number of GPUs increases.
In both applications, each GPU solves a relatively large subproblem and the total problem size is proportional to the
number of GPUs. In the MC isosurface extraction and rendering (benchmarked with lobster data), however, we have
fixed the total problem size. Accordingly, the percentages
of time spent on overhead operations and time waiting for
remote data increase faster. In the GPU cluster, the time
waiting for remote data transfers is the main bottleneck to
the performance. Using a higher bandwidth network, such
as Infiniband, will improve the performance. Our best implementation is the LBM computation. It shows the lowest
percentage of waiting time, because we have used the nonblocking ghost cell update function to partially overlap the
network communication with the computation. For other applications, this overlap is feasible for future implementation.

349

Currently, Zippy uses the CPUs only for communication
and GPU management. The CPU computational power has
not been fully exploited. Coordinating multiple GPUs and
CPUs in the same cluster to work together is an important
topic. A possible solution is to combine the vertical communication of Sequoia (see Sec. 1) and the horizontal communication of Zippy. This would help the programmer to exploit
multiple memory levels on the cluster, including GPU memories, system memories, and disks. Specifically, the global
arrays can be defined at any level so that the computational
power of both CPUs and GPUs can be exploited.
In the future, we will exploit the one-sided functions for
applications with dynamic communication patterns. Also,
we wish to enhance our implementation of the one-sided
functions. Currently, the program needs to call a collective
fence function to execute and complete queued one-sided
functions. Our design decision is due to a GPU hardware
limitation: we cannot read data from or write data to the
memory of a GPU when it is computing. The fence function makes it easy for Zippy to take control of the GPU and
provide services for one-sided functions. We hope that future
GPUs will address this limitation so that Zippy can remove
the fence function restriction.
Another interesting future direction is to build the local
computation interface (Sec. 4.3) of Zippy upon CUDA. This
will allow local computation to access a much larger and
more flexible linear memory space than the OpenGL textures. The programmer will write kernels more easily in C
than in shading languages. The new GPU features, such as
scatter and Parallel Data Cache, will be available to the programmer. Moreover, global irregular data structures would
be much simpler to implement with CUDA.
7. Acknowledgements
We wish to thank Jarek Nieplocha and Manojkumar Krishnan for discussing GA and Mike Houston for discussing parallel volume rendering. This work is supported by NSF grant
CCF-0702699. The engine and visible human data are courtesy of GE and National Library of Medicine, respectively.
References

6. Discussion and Future Work
Zippy provides virtually shared spaces to ease the programming of a GPU cluster and exposes to the programmer two
characteristics critical to high performance, two-level parallelism hierarchy and NUMA. We have demonstrated three
example applications developed with the Zippy global array
data structure. The concept could be extended to global irregular data structures. For example, each GPU owns a portion of a global data structure and uses well-defined coarsegrained communication functions to bring data from the
global data structure to the local GPU memory. In this way,
Zippy may support more applications on GPU clusters.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

[BFH∗ 04] B UCK I., F OLEY T., H ORN D., S UGERMAN
J., FATAHALIAN K., H OUSTON M., H ANRAHAN P.:
Brook for GPUs: Stream computing on graphics hardware. ACM Trans. on Graphics 23, 3 (2004), 777–786.
[BRE05] B HANIRAMKA P., ROBERT P., E ILEMANN S.:
OpenGL multipipe SDK: A toolkit for scalable parallel
rendering. IEEE Visualization (2005), 119–126.
S.,
S TRENGERT
M.,
[BSWE06] BACHTHALER
Parallel texture-based
W EISKOPF D., E RTL T.:
vector field visualization on curved surfaces using GPU
cluster computers. Eurographics Symp. on Parallel
Graphics and Visualization (2006), 75–82.

350

Z. Fan, F. Qiu, and A. Kaufman / Zippy: A Framework for Computation and Visualization on a GPU Cluster

[Cra07] C RASSIN C.: OpenGL geometry shader marching
cubes. http://www.icare3d.org/content/view/50/9 (2007).
[DZTS07] DYKEN C., Z IEGLER G., T HEOBALT C., S EI DEL H.-P.:
HistoPyramids in iso-surface extraction.
Tech. rep., Max Planck Inst. für Infor., 2007.
[FKH∗ 06] FATAHALIAN K., K NIGHT T. J., H OUSTON
M., E REZ M., H ORN D. R., L EEM L., PARK J. Y., R EN
M., A IKEN A., DALLY W. J., H ANRAHAN P.: Sequoia:
Programming the memory hierarchy. ACM/IEEE Supercomputing Conf. (2006), 4.
[FQKYS04] FAN Z., Q IU F., K AUFMAN A., YOAKUM S TOVER S.: GPU cluster for high performance computing. ACM/IEEE Supercomputing Conf. (2004), 47.
[Gal86] G ALIL Z.: Efficient algorithms for finding maximum matching in graphs. ACM Comput. Surv. 18, 1
(1986), 23–38.
[GSMY∗ 07] G ÖDDEKE D., S TRZODKA R., M OHD Y USOF J., M C C ORMICK P., B UIJSSEN S., G RAJEWSKI
M., T UREK S.: Exploring weak scalability for FEM calculations on a GPU-enhanced cluster. Parallel Computing
33, 10–11 (2007), 685–699.
[GSYM03] G OVINDARAJU N. K., S UD A., YOON S.-E.,
M ANOCHA D.: Interactive visibility culling in complex
environments using occlusion-switches. ACM Symp. on
Interactive 3D Graphics (2003), 103–112.
[HHH05] H ORN D., H OUSTON M., H ANRAHAN P.:
ClawHMMER: A streaming HMMer-search implementation. ACM/IEEE Supercomputing Conf. (2005), 11.
[HHN∗ 02] H UMPHREYS G., H OUSTON M., N G R.,
F RANK R., A HERN S., K IRCHNER P. D., K LOSOWSKI
J. T.: Chromium: a stream-processing framework for interactive rendering on clusters. ACM Trans. on Graphics
21, 3 (2002), 693–702.
[Hor05] H ORN D.: Stream reduction operations for
GPGPU applications. In GPU Gems 2, Pharr M., (Ed.).
Addison Wesley, 2005, ch. 36, pp. 573–589.
[Hou04] H OUSTON M.: Designing graphics clusters.
IEEE Visualization – Workshop on Parallel Visualization
Architectures and Chromium (2004).
[KSE04] K LEIN T., S TEGMAIER S., E RTL T.: Hardwareaccelerated reconstruction of polygonal isosurface representations on unstructured grids. Pacific Graphics (2004),
186–195.
[KW05] K IPFER P., W ESTERMANN R.: GPU construction and transparent rendering of iso-surfaces. Vision,
Modeling and Visualization (2005), 241–248.
[LKS∗ 06] L EFOHN A., K NISS J. M., S TRZODKA R.,
S ENGUPTA S., OWENS J. D.: Glift: Generic, efficient,
random-access GPU data structures. ACM Trans. on
Graphics 25, 1 (2006), 60–99.
[MBO∗ 06]

M ANZKE M., B RENNAN R., O’C ONOR K.,

D INGLIANA J., O’S ULLIVAN C.: A scalable and reconfigurable shared-memory graphics architecture. ACM
SIGGRAPH Sketches (2006), 182.
[MDP∗ 04] M C C OOL M., D U T OIT S., P OPA T., C HAN
B., M OULE K.: Shader algebra. ACM Trans. on Graphics
23, 3 (2004), 787–795.
[MO06] M OERSCHELL A., OWENS J. D.: Distributed
texture memory in a multi-GPU environment. Graphics
Hardware (2006), 31–38.
[MPHK94] M A K.-L., PAINTER J. S., H ANSEN C. D.,
K ROGH M. F.: Parallel volume rendering using binaryswap image composition. IEEE Comp. Graph. and App.
14, 4 (1994), 59–68.
[NPT∗ 06] N IEPLOCHA J., PALMER B., T IPPARAJU V.,
K RISHNAN M., T REASE H., A PRA E.: Advances, applications and performance of the Global Arrays shared
memory programming toolkit. Int. J. of High Perf. Comp.
App. 20, 2 (2006), 203–231.
[OLG∗ 07] OWENS J. D., L UEBKE D., G OVINDARAJU
N., H ARRIS M., K RÜGER J., L EFOHN A. E., P URCELL
T. J.: A survey of general-purpose computation on graphics hardware. Comp. Graph. Forum 26, 1 (2007), 80–113.
[SHZO07] S ENGUPTA S., H ARRIS M., Z HANG Y.,
OWENS J. D.: Scan primitives for GPU computing.
Graphics Hardware (2007), 97–106.
[SLO06] S ENGUPTA S., L EFOHN A. E., OWENS J. D.: A
work-efficient step-efficient prefix sum algorithm. Workshop on Edge Computing Using New Commodity Architectures (2006), D–26–27.
M.,
M AGALLON
M.,
[SMW∗ 04] S TRENGERT
W EISKOPF D., G UTHE S., E RTL T.: Hierarchical
visualization and compression of large volume datasets
using GPU clusters. Eurographics Symp. on Parallel
Graphics and Visualization (2004), 41–48.
[TGT05] T HAKUR R., G ROPP W., T OONEN B.: Optimizing the synchronization operations in message passing
interface one-sided communication. Int. J. of High Perf.
Comp. App. 19, 2 (2005), 119–128.
[TK06] TAKIZAWA H., KOBAYASHI H.: Hierarchical parallel processing of large scale data clustering on a PC cluster with GPU co-processing. J. of Supercomputing 36, 3
(2006), 219–234.
[WZF∗ 04] W EI X., Z HAO Y., FAN Z., L I W., Q IU F.,
YOAKUM -S TOVER S., K AUFMAN A.: Lattice-based
flow field modeling. IEEE Trans. on Visualization and
Computer Graphics 10, 6 (2004), 719–729.
[XPQS06] X IONG H., P ENG H., Q IN A., S HI J.: Parallel
occlusion culling on GPUs cluster. ACM Int. Conf. on
Virtual Reality Continuum and Its App. (2006), 19–26.
[YS07] YAMAGIWA S., S OUSA L.: Caravela: A novel
stream-based distributed computing environment. IEEE
Computer 40, 5 (2007), 70–77.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

