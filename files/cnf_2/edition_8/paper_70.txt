Volume 27 (2008), Number 2

EUROGRAPHICS 2008 / G. Drettakis and R. Scopigno
(Guest Editors)

Agile Spectrum Imaging: Programmable Wavelength
Modulation for Cameras and Projectors
Ankit Mohan1 , Ramesh Raskar2 and Jack Tumblin1
1 Northwestern
2 Mitsubishi

University, Evanston, IL
Electric Research Laboratories, Cambridge, MA

Abstract
We advocate the use of quickly-adjustable, computer-controlled color spectra in photography, lighting and displays. We present an optical relay system that allows mechanical or electronic color spectrum control and use
it to modify a conventional camera and projector. We use a diffraction grating to disperse the rays into different
colors, and introduce a mask (or LCD/DMD) in the optical path to modulate the spectrum. We analyze the tradeoffs and limitations of this design, and demonstrate its use in a camera, projector and light source. We propose
applications such as adaptive color primaries, metamer detection, scene contrast enhancement, photographing
fluorescent objects, and high dynamic range photography using spectrum modulation.
Categories and Subject Descriptors (according to ACM CCS): I.3.1 [Computer Graphics]: Hardware Architecture
I.4.1 [Image Processing and Computer Vision]: Digitization and Image Capture.

1. Introduction
At first glance, programmable, quickly-adjustable or ‘agile’ spectral selectivity for cameras, lights and projectors
might not seem to offer any significant advantages in computational photography. Existing photographic methods quite
sensibly rely on the well-established trichromatic response
of human vision, and any modest variation in the sensations of color caused by spectral variations in a scene can be
recreated without the need to adjust the spectrum of the device. Instead, we can simply adjust a weighed combination
of three or more distinct, but spectrally fixed color primaries,
such as red, green and blue (RGB) to achieve visually identical color changes.
Fixed-spectrum photography limits our ability to detect
or depict several kinds of visually useful spectral differences.
In the common phenomena of metamerism, the spectrum of
available lighting used to view or photograph objects can
cause materials with notably different reflectance spectra to
have the same apparent color because they evoke equal responses from the broad, fixed color primaries in our eyes or
the camera. With an ‘agile’ light source guided by our camera, we might change the illumination spectra enough to disrupt the metameric match. Similarly, we might interactively
adjust and adapt the illuminant spectrum to maximize conc 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

trasts of a scene, both for human viewing and for capture by
a camera.
Use of fixed-spectrum color primaries always imposes
limits on the gamut of colors we can capture and reproduce accurately. As demonstrated in the CIE chromaticity
map (Figure 11), each set of fixed color primaries in cameras
and displays defines a convex hull of perceived colors, and
only the colors inside it are reliably and accurately reproducible. In most digital cameras, the Bayer grid of fixed, passive R,G,B filters overlaid on pixel detectors set the color primaries. A similar passive pixel-by-pixel filter combines with
a fluorescent backlight to set the color primaries in LCD displays. Current DMD projectors use broad-band light sources
passed through a spinning wheel that holds passive R,G,B
filter segments. These filters compromise between narrow
spectra that provide a large color gamut, and broad spectra
that provide greatest on-screen brightness. However, if the
spectra of each color primary was ‘agile’, that is, changeable and computer specified for every picture, then we could
choose the best primaries on an image-by-image basis, for
the best capture and display of visual appearance.
Computer-controlled adjustments of the spectrum are
quite challenging in hardware or optics. Available spectral
adjustment mechanisms include tunable lasers, interference

710

A. Mohan, R. Raskar & J. Tumblin / Agile Spectrum Imaging

m(λ)
400nm

λ
550nm

700nm

m(λ)
400nm

λ
550nm

700nm

m(λ)
400nm

λ
550nm

700nm

Figure 1: Photos taken with our camera setup and corresponding wavelength masks. The scene is a simple rainbow projected
on a screen (using a diffraction grating and a white light source). m(λ) is the modulation mask value at different wavelengths.
(Left) We use a transparent mask, allowing all wavelengths to pass through. Resulting photograph has the complete rainbow.
(Center) We block a narrow wavelength range between red and green using an opaque occluder. Resulting photograph has
appropriate wavelengths missing from the rainbow. (Right) We block two separate ranges of wavelengths, and two gaps appear
in the photograph of the rainbow.
filters, and motorized diffraction gratings. They trade off
size, expense, noise, and optical efficiency for precise spectral selectivity. Specialized ‘multispectral’ or ‘hyperspectral’
cameras assess light intensities or reflectances in many spectrally narrow bands, far more than we need for most of the
visual experiences that we wish to capture. We advocate using just a few efficient color primaries chosen specifically
for each photo by means of an agile spectrum, rather than an
exhaustively detailed spectrum that is later reduced to just a
few channels for display.
To achieve any of these goals, we need a practical mechanism for quick, simple, fine-grained control of the spectrum
of visible light that leaves a projector or enters a camera,
without greatly disrupting the image-forming process itself.
We extend 4D ray-space analysis to include wavelength (λ).
A diffraction grating, and a carefully positioned lens system provides a very useful decomposition of the image into
three planes where rays converge according to three different
classifications. At one plane, rays merge according to incoming direction to form an image of the scene; at another, they
converge by incoming position to form an image of the lens
aperture, and at the third they converge according to wavelength to form the rainbow plane. Simply blocking a part of
the rainbow plane removes the corresponding wavelengths
from every point in the image. We essentially convert wavelengths in the scene to spatial positions in the rainbow plane,
and use grayscale masks to manipulate the spectrum of the
sensed image.
1.1. Contributions
As envisioned, improved agile spectrum imaging methods
may provide high-resolution control of light spectra at every

stage of computational photography, but this paper makes
only these initial contributions:
• A simple optical relay that permits direct wavelength manipulation by geometrically-patterned gray-scale masks.
The design applies 4D ray-space analysis to dispersive
elements within a multi-element lens system, rather than
conventional color filtering and use of wavelength selective filters. As far as we know, this is the only configuration to control wavelength spectrum using a purely mechanical mask for a perspective device with a finite sized
aperture.
• Our analysis determines the ‘rainbow plane’ where rays
converge so that wavelength determines ray position, x,
and image position determines ray direction, θ. The optical relay provides a graceful tradeoff between wavelength
selectivity and the entrance aperture size.
• We use the rainbow plane manipulation to create agile
spectra for light sources, cameras, and projectors. We
demonstrate the following new capabilities in this paper:
– A filter attachment for conventional cameras that can
place arbitrary notches in the spectral response of the
camera’s color primaries, enhancing its sensitivity for
special purpose color-discrimination tasks such as a
single shot spectral high dynamic range imaging.
– A prototype agile-spectrum projector with adaptive
color primaries that optimally uses the wavelength resolution for each projected image. Our design is similar to the hyperspectral image projectors of Rice et
al. [RBN06], but it is built entirely outside a traditional
projector, and is the exact inverse of our camera design.
– A smart light source that adapts its spectral emis-

c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

A. Mohan, R. Raskar & J. Tumblin / Agile Spectrum Imaging

sions to optimize scene contrasts and reveal spectral
reflectance differences that might not otherwise be detectable.
1.2. Related Work
The idea of dispersing light to modulate its spectral components is certainly not new. Spectroscopy has been an active
area of research ever since Newton discovered dispersion
of light [Jam07]. Spectroscopy has traditionally analyzed
the spectrum of point sources, and the concept of imaging spectroscopy or multi-spectral photography is relatively
new. Gat [Gat00] provides a nice review of liquid crystal tunable filters (LCTF), acousto-optical tunable filter (AOTF),
and interferometers and their applications in imaging spectroscopy. Placing one of these filters in front of a camera
allows a controllable wavelength of light to pass through,
and a series of photos can be merged to form one multispectral photo. Unfortunately these filters are rather expensive, and usually only allow a single wavelength of light to
pass through (notch pass). Li and Ma [LM91] used an imaging spectroscope which dispersed rays into their constituent
wavelengths, selected a single wavelength, and then recombined the rays using another diffraction grating. Our setup
is similar, but we do not require a second diffraction grating
and perform the recombination using a lens.
Several people have inverted the concept of a spectroscope to create a spectrally tunable light source using
a diffraction grating and a white light source [FWSS07,
MSL∗ 05, BRN∗ 06]. Rapidly expanding choices in illumination sources allowed the use of several narrow band
LEDs to illuminate an object and acquire multi-spectral
images [MH02]. Similarly, some new projectors use more
than three LEDs to get better color rendition [SKK∗ 06].
Rosenthal et al. [RSJ06] used a diffraction grating to disperse light, modulate it differently for each pixel in a scanline, then project one scanline at a time using a scanning mirror arrangement to form the image. Similarly Nelson [Nel06] described a system to project a linear (1D) image with controllable spectrum for each point along it. Rice
et al. [RBN06,RBNB07] propose a hyperspectral image projector design that is perhaps the most similar to ours. They
essentially use the tunable light source, obtained using dispersion, as the source for a DLP projector. By controlling the
wavelength emitted by the source before the spatial modulation of the DLP projector, they can control the color displayed at each pixel on the screen. This requires modifying
the light source inside a projector. Unlike their approach, we
modulate the image spectrum after the spatial modulation.
This greatly simplifies the projector design since we can easily retrofit a traditional RGB projector by placing lenses and
a diffraction grating outside it. In addition, this modification
allows us to use the same basic optical setup both as an image projector, and an image capture system.
Color has been an important part of computer graphics rec 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

711

search as for a while. Salesin and others [PWSS96, SOS98]
investigated the use of arbitrary ink pigments to reproduce
the right color in a printout; Wilkie et al. [WWLP06] proposed a BRDF model for diffuse fluorescent surfaces; Hersch et al. [HDC07] presented algorithms for printing images
with fluorescent inks that are visible only under ultraviolet
illumination; and Gooch et al. [GOTG05] proposed an algorithm for perceptual conversion of color images to grayscale. We propose applications of color modulation in the areas of metamer detection, glare removal, and high dynamic
range imaging, which have not been explored previously.
2. Optical Relay for Spectral Control
Here we describe the optical setup for the agile camera, but
the basic setup remains essentially unchanged for a projector. We introduce a diffraction grating and a series of lenses
in the optical path of a traditional camera to generate a plane
where all the rays from all the points in the scene of a particular wavelength meet at a point. This yields a one-to-one
mapping between the wavelength of a ray and the spatial position in this plane. We introduce a grayscale mask in this
plane to block or attenuate different wavelengths selectively.
The rays are then re-focused on the camera’s sensor to produce the final image with the spectrum of all points in the
image modulated by the mask function.
Figure 2 shows a simplified ray diagram for our optical
setup with a pinhole in place of the objective lens. The pinhole images a scene onto the plane P behind it. Points X and
Y in the scene are imaged to X p and Yp respectively. We place
a transmissive diffraction grating (or a prism) in the plane P.
A diffraction grating works by exploiting the wave nature of light [Hec01]. A ray incident on the diffraction grating effectively produces multiple diffracted outgoing rays
in different directions given by the grating equation, φm =
sin−1 ( mλ
d − sin(φi )), where d is the grating constant (distance between consecutive grooves), φi is the incident ray
angle, φm is the output diffraction ray angle for integer order m, and λ is the wavelength of the ray of light. Order 0
corresponds to the diffracted ray going straight through the
diffraction grating undeviated (direct transmission). As can
be seen from the grating equation, diffraction angles are a
function of the wavelength for all orders other than order
0. This causes spectral dispersion of the incident light ray.
Because higher orders have increasingly lower energies, we
only use order 1 in our setup. In Figure 2, everything after the
P plane is applied to order 1. Note that while order 1 is actually bent with respect to the incident rays, we show the green
component (λ = 550nm) going straight through the diffraction grating, and red (λ = 700nm) and blue (λ = 400nm) are
dispersed in opposite directions. This is done to keep the figure simple, and we discuss practical issues due to the bending of the light path in Section 3.
The lens L2 focuses the diffraction grating, P onto the sensor plane, S. In other words, plane S is conjugate to plane P.

712

A. Mohan, R. Raskar & J. Tumblin / Agile Spectrum Imaging
P
(Diffraction Grating)

Pinhole

Y
X
Z

L2

S
R
(Rainbow plane) (Sensor)

α

CB
Zp

CG

Xp
CR

Yp

Lens L1

Ys

Attenuating
Diffraction Grating
Mask
Lens L2

Sensor

C’R

α

Rλ α’

Xs

C’B

α

Zs
d

p

r

s

Figure 2: Ray diagram for the pinhole camera setup. The
pinhole images the scene onto the diffraction grating (P),
and the lens L2 images the grating onto the camera sensor (S). All rays of a given wavelength from all points in
the scene meet at a unique point in the R-plane. A mask in
the R-plane modulates the spectrum of the entire image.

All the spectrally dispersed rays coming out of point X p on
the diffraction grating converge at Xs on the sensor. Thus,
the image on the sensor is exactly the same as the image
formed on the diffraction through the pinhole, without any
chromatic blur. It is important to make sure the lens L2 does
not produce any vignetting. Traditional vignetting artifacts
usually result in the dark image corners, that can be calibrated and fixed to a large extent in post-processing. However, vignetting in our case may cause serious loss of information as some spectral components of corner image points
might not reach the sensor at all. Visually, vignetting would
result in undesirable chromatic artifacts on the sensor.
2.1. Pinhole case

Figure 3: Photo of the camera setup. We covered the entire
setup with black foam-core to prevent light from leaking into
the sensor, and remotely triggered the camera.

where s is the distance between the R-plane and the sensor,
and α is the cone angle made by rays converging on the
sensor at Xs . Also,
pα = (r + s)α ,
where p is the distance from lens L2 to the diffraction grating, and α is the dispersion angle of the grating. This gives
us,
sp
Rλ =
α.
r+s
From the lens equation, we have
1
1
1
+
= ,
p s+r
f2
and

Tracing back the dispersed color rays on the right side of
the grating (P) to the plane of the pinhole, we see that all
the red rays appear to come from a point, CR ; all green rays
from a point CG , and all blue rays from a point CB . The lens
L2 serves a second purpose - it focuses the pinhole plane to
the R-plane (the R-plane is conjugate to the pinhole plane
across lens L2 ). Placing a screen in the R-plane produces a
thin stripe of light with colors ranging from red to blue like
a rainbow (thus the name R- or rainbow-plane). All the dispersed rays of a particular wavelength from all the points in
the scene arrive at the same point on the R-plane, thus converting wavelength to a spatial position. By putting a blocker
corresponding to a certain wavelength in this plane, we can
completely remove that wavelength from the spectrum of the
entire image. By placing an arbitrary mask or an LCD in this
plane, we can simulate any arbitrary color filter in front of
the camera.
To make the analysis easier, we assume all rays are paraxial, which means all rays make small angles to the optical
axis, and remain close to it [Hec01]. Tracing the rays from
point X, we have
α =

Rλ
,
s

1
1
1
+ = .
p+d r
f2
Rearranging terms in these three equations, we get,
f2 (p + d)
,
p + d − f2

(1)

d f22
,
(p − f2 )(p + d − f2 )

(2)

r=

s=

Rλ = α

d f2
.
p + d − f2

(3)

2.2. Finite aperture case
While the pinhole case discussed in Section 2.1 is easy to
understand and analyze, it only admits an infinitesimally
small amount of light, and is not very practical. Figure 4
shows the optical setup with a finite sized lens (L1 ) instead
of the pinhole. The lens L1 exactly focuses the scene point X
on the diffraction grating plane, P. For each in-focus image
point (X p ), we have a cone (with cone-angle θ) of incoming
rays. The diffraction grating disperses each of these rays into
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

713

A. Mohan, R. Raskar & J. Tumblin / Agile Spectrum Imaging
P
(Diffraction Grating)

L1

L2

S
R
(Rainbow plane) (Sensor)

Lens L2
DLP Projector

Attenuating
Mask

Screen

Diffraction Grating
Lens L1

θ

X

Xp

θ

Rθ

d

p

r

Xs

s

Figure 4: Ray diagram for the finite-aperture case. Note that
the angle θ has been exaggerated here for ease of understanding. In a real system, and our prototype, θ α.

Figure 5: Photo of the projector setup. The screen is at a
distance of about two meters in the direction indicated. The
projected image is about 1m×1m in size.
P

their constituent wavelengths. For each ray in the incoming
cone of rays from each scene point, the grating creates a
cone of outgoing rays, each of a different wavelength. Like
the pinhole case, the dispersion angle is α. Because the sensor (S) is conjugate to the diffraction grating (P), the scene
point is imaged at the sensor location Xs . Not only is the
point in sharp focus, it is also the correct color, and the relay
does not cause any chromatic blur.
The R-plane, however, looks quite different from the pinhole case. Instead of a nice sharp stripe where each point
corresponds to one wavelength in the scene, lens L1 blurs
each wavelength of each scene-point to a size Rθ . Following
the same reasoning as Equation 3, we get
Rθ = θ

d f2
.
p + d − f2

The cone-angle θ is given by θ =
of lens L1 .

a1
d , where a1

is the aperture

θ
Rθ
a
= = 1.
Rλ
α
d
In the pinhole case, we have Rθ ≈ 0, and in the finite aperture
case we would like to have Rθ
Rα . If α is fixed (depends
on the diffraction grating used), we require
d.

L2

R

S (Sensor)

O

Figure 6: The diffraction grating plane (P), lens L2 plane,
sensor plane, and the R-plane, all pass through the point O
as shown to compensate for the bend in the optical axis.

(4)

From Equations 3 and 4, we get

a1

Optical Axis

(5)

We achieve this by using an objective lens (L1 ) with a large
focal length and small lens aperture. A large aperture admits more light but reduces the spectral selectivity of our
system by increasing the Rθ blur in the R-plane. The image
formed at the sensor plane remains in perfect focus irrespective of the aperture size. The optical relay imposes a tradeoff
between the aperture size (amount of light) and the desired
spectral selectivity in the R-plane. With a large aperture size,
the effective wavelength selectivity of the system is reduced.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

3. Our prototypes
Figure 3 shows a picture of our prototype multi-spectral
camera built based on the setup shown in Figure 4. We use a
Mamiya 645AFD medium format camera with the Mamiya
ZD digital back for the sensor, a Mamiya 80mm lens for
the lens L1 (detached from the body), a blazed transmissive
diffraction grating with 600 grooves/mm, and a lens L2 with
focal length 50mm. We print the mask on a transparency
sheet using a laser printer, and mount it on a little plastic
frame that slides back and forth using a computer-controlled
stepper motor. Our optical axis is effectively bent after the
diffraction grating because we work with the grating’s first
order output. Note the angled placement of the lens, mask,
and the camera instead of parallel to the grating as shown in
Figure 6.
We acquire a multi-spectral dataset by capturing multiple
photos with different positions of a tiny slit in the R-plane.
Each slit position allows a small subset of wavelengths to
pass through, thus throwing away a large portion of the light.
We can get better signal to noise ratio by using a Hadamard
coded mask instead of a single slit [SNB03].

714

A. Mohan, R. Raskar & J. Tumblin / Agile Spectrum Imaging

Closely related to the camera setup is a direct view device
that lets a viewer to look at a scene and mechanically modify
its color spectrum by moving a mask. This offers arbitrary
wavelength modulation and is more powerful than a LCTF
and AOTF, which usually only allow a single wavelength to
pass through. The camera viewfinder is an example of such
a setup, and a small hand-held device might be useful for applications such as metamer detection and prosthetic devices
that help with color blindness (Section 4.1).
So far we have discussed the optical design for an agilespectrum camera. But this design works just as well for a
projector, and unlike [RBN06], it is easy to retrofit a traditional projector because the spectral modulation takes place
after the spatial modulation. We place our spectral relay outside an existing projector. In Figure 4, lens L1 corresponds
to the exiting lens of a conventional projector. We focus the
projected image onto the diffraction grating, and place the
screen in the S plane. Projectors usually have a long folded
optical path inside them, so the condition in Equation 5 is
actually easier to achieve than in the case of a camera. Figure 5 shows our optical setup in front of a conventional DLP
projector. We place a convex lens in front of the projector’s
lens to shorten the focus distance and reduce the image size
to that of our diffraction grating. An agile projector can also
be used as an agile light-source by projecting a solid white
box covering the entire projected image.
4. Applications/Results
4.1. Spectrally controllable light source
A spectrally controllable light source allows us to view a
scene or object in different colored illumination by simply changing the mask placed in the R-plane. This enables
us to distinguish between metamers in a scene very easily. Metamers are colors that look identical to the human
eye (or a camera), but are spectrally dissimilar. Metamerism
is the combined result of illumination and reflectance spectra that evoke the same CIE tri-stimulus values. They are
commonplace for complex spectra because the cone cells
of the eye (or the Bayer filters on a camera sensor) have
a relatively broad spectral response. Figure 7 shows photos
captured with an ordinary RGB camera of a scene containing fluorescent and ordinary materials under illumination of
varying color. It demonstrates how materials with very different reflectance spectra can appear to have the same color
under certain illumination conditions.
Figure 8(a) shows another scene captured under white
light on the left, and on the right is a simulation of the visual
appearance for deuteranope color blindness [Vis], in which
red and green hues appear very similar. An agile spectrum
light source can compensate for this disability by adaptively
changing the color of the illumination. We block the green
wavelengths as shown in Figure 8(b), making the leaves dark
and clearly different from the red rose in both the original
photo and corresponding color blindness simulation.

Figure 7: A scene comprising of fluorescent envelopes, circular decals, colored Post-It Notes, and colored cloths under
different colored illumination. The top-left photo was taken
under white light, and all others were taken under different
monochromatic colors ranging from 700nm to 400nm using
our agile light setup. A simple scan of the different wavelengths allows us to distinguish between the metamers.

4.2. Spectral HDR capture and glare removal
The agile spectrum camera can also acquire high dynamic
range images if regions of high contrast differ in their spectra
as well. Instead of using spatially varying exposures [NB03],
we can use spectrally varying exposures by modulating the
colors appropriately. Figure 9(a) shows a photo of a scene
containing the text “EG”, and a bright green LED next to it.
Clearly, the LED is too bright; not only is it saturated, it also
causes a rather ugly glare that renders part of the text unreadable. Simply reducing the exposure does not help because it
makes the text darker as we darken the glaring light (Figure 9(b)). Instead, we block the green wavelength by using an appropriate mask in the R-plane (Figure 9(c)). This
leaves the red text unaffected, but greatly reduces the intensity of the LED and the glare. Unlike traditional HDR, only
the green color is attenuated uniformly throughout the image. As a result, the color of the circuit board turns pinkish.
With better spectral control we might be able to minimize
this artifact, but even without such refinements, the glare is
almost completely removed and the photo has much more
detail than before. We are even able to make out some detail
through the casing of the LED. Unlike previous approaches
for glare reduction [TAHL07], we do not change anything
outside the camera, and once we know the color of the offending highlight, we require only a single agile spectrum
photo to remove the glare. Also, since the wavelength modulation can be arbitrary, we can easily remove multiple glares
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

715

A. Mohan, R. Raskar & J. Tumblin / Agile Spectrum Imaging

m(λ)
400nm

λ
550nm

700nm

(a) Exposed to get detail in background text

m(λ)

λ

400nm

550nm

700nm

(b) Lower exposure

m(λ)

λ

400nm

550nm

700nm

(c) Green wavelength attenuated

Figure 9: Spectral High Dynamic Range photography and glare removal. (a) Photo of a green LED next to the red letters “EG”
with a traditional camera. (b) Photo of the same scene at a lower overall exposure. The wavelength modulation function m(λ)
is uniformly reduced. The text is too dark to read, and the glare still exists. (c) Our agile spectrum camera is used to attenuate
the green wavelength throughout the photo. We remove the glare and are able to see much more detail.

(a) Photo of a scene illuminated with white light (left), and a simulation of how it appears to a person with color blindness (right).
The rose and leaves are indistinguishable in the image on the right.

(a) Traditional projector

(b) Agile projector

Figure 10: Comparison of cyan color projected by a traditional and an agile spectrum projector. In the traditional
projector image, cyan is just a mixture of blue and green primaries, while in the agile case the cyan color is very different
from anything obtained by mixing blue and green.

4.3. Improved color rendition

(b) Photo of the same scene illuminated with light lacking green
color on the left, and its color blindness simulation on the right.
The leaves are now easily distinguishable from the rose.

Figure 8: Agile spectrum light sources can improve color
discriminability for people with color blindness.

of different colors, something not possible using a simple
colored filter. A closed-loop spectral HDR capture system
could be useful for tricky scenes like this one where traditional techniques fail to capture all the detail.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Most display devices have a very limited color gamut compared to the full CIE-xy color space chromaticity diagram (Figure 11), especially in the blue-green region. Reproducing a pure cyan hue is considered challenging for current
RGB projectors. Figure 10 compares color reproduction between a traditional and agile projector. The projected image
consists of three broad stripes. In the middle stripe a color
ramp extends from green (0,255,0) to blue (0,0,255). This is
surrounded by cyan color in the top and bottom stripes. The
cyan color appears to leak across the ramp for a traditional
projector, suggesting the projected cyan is indeed a mixture
of green and blue, and not a pure color. In the agile projector,
it is clear that the cyan is very different from colors obtained
by mixing blue and green. It is a saturated, pure cyan that is

716

A. Mohan, R. Raskar & J. Tumblin / Agile Spectrum Imaging

py

Figure 11: CIE xy chromaticity diagram. The sRGB color
space is extremely limited in the green/blue/cyan region.

not obtainable by simply mixing blue and green. The difference between the two cases is greatly reduced in Figure 10
because we captured the photos with an RGB camera.
4.4. Adaptive color primaries
Traditional cameras and projectors use standard RGB color
primaries. These color primaries are chosen to form a good
basis for the spectral response of the cone cells in the eye.
They work reasonably well for many scenes, but may cause
serious artifacts such as metamers and loss of contrast in
others. Recently projector manufacturers have started experimenting with six or more color primaries to get better color
reproduction [SKK∗ 06]. Even though ﬁxed multi-primary
projectors many cover the CIE color gamut as well or better than an agile spectrum projector, their ﬁxed spectral responses limit their usefulness and efﬁciency. Agile spectrum imaging allows the use of color primaries that adapt
to the projected or captured scene. We could use an LCD
or DMD in place of the mask in the projector setup. If this
LCD were synchronized to the spatial projection DMD, we
could get rid of the color wheel in the projector, and simulate an arbitrary color wheel using wavelength modulation.
Arbitrary adaptive color primaries result in better color rendition, fewer metamers, brighter images, and enhanced contrast. [RBN06] also discuss the use of arbitrary color primaries using pulse width modulation and a slightly different
optical design.
Figure 12 shows how a simulated agile spectrum projector
can offer greater brightness and color saturation than a traditional projector. A traditional RGB projector projects the
red component of the image for the fi rst third, blue a second
third, and green the last third of the frame time. Consider a
yellow pixel in a traditional projector. This pixel is turned
“on” when either the red or the green fi lter is in the optical
path. Assuming each of the red, green, and blue fi lters allow
a third of the visible light through, the intensity of a yellow
pixel is 13 × 13 + 13 × 13 + 13 × 0 = 29 the light intensity. A blue
pixel is only 19 the light intensity. With adaptive primaries,

py

time

(c) RGB color primaries

time

(d) Adaptive color primaries

Figure 12: Adaptive color primaries: the graphs show how
the intensity of a yellow pixel changes with time. In the traditional case, the pixel is turned on when the red and green
fi lters are active, and turned off when the blue fi lter is active.
In the adaptive case, the pixel is on for half the time when
the yellow fi lter is active. The overall intensity of the yellow
pixel is actually higher in the adaptive primaries case (see
text). The agile projector result was simulated by combining
two photos, each using a slot-like mask in the R-plane.

we need only two colors for this scene (yellow and blue),
and each can be displayed for half the time. If the yellow
fi lter is spectrally the same size as the red and green fi lters
combined, the intensity of the blue pixel increases to 16 , and
that of the yellow pixel increases to 13 the light intensity. We
also have the added fl exibility of making the yellow color
more saturated by narrowing the corresponding fi lter at the
expense of reduced light.
5. Limitations and Future Work
The optical design discussed for the agile projector and camera does have some limitations. The ﬁrst and most important is that we assume the aperture of the objective is much
smaller than the distance to the diffraction grating (Equation 5). A large aperture results in reduced spectral selectivity of the system. This may be a serious limitation depending
on the application. However, in our prototype, we get reasonable wavelength resolution with a fi nite sized aperture (f /16
or smaller), and we show several interesting applications
where this limitation is not a serious problem. Diffraction
artifacts due to the mask placed in the optical path present
another obstacle to high quality imaging. The mask may
contain sharp discontinuities and narrow notch pass or reject
conﬁgurations that may cause undesirable diffraction artifacts. This would limit the spectral and spatial resolution of
the camera/projector setup and requires further investigation
using Fourier optics. Finally, like a traditional projector, the
agile projector design also produces an in-focus image in a
particular plane. But unlike a traditional projector, any other
c 2008 The Author(s)

c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Journal compilation 

A. Mohan, R. Raskar & J. Tumblin / Agile Spectrum Imaging

plane would have chromatic artifacts in addition to the usual
spatial blur. This is not a problem in the camera case because
the positions of the grating, lens L2 and the sensor are fixed,
and the sensor and the grating are always conjugate to one
another. A point that is outside the plane of focus of the main
lens, L1 , behaves as expected - the point is de-focused on the
sensor without any chromatic artifacts, and the mask in the
R-plane modulates its color just like an in-focus point.
At present our agile spectrum optical relay prototype provides only coarse control over wavelength in comparison to
state of the art multi-spectral cameras, monochromators and
other traditional narrow-band spectrographic instruments.
This is primarily due to the use of crude optics and off-theshelf equipment. Professional lens design software such as
Zemax would surely improve the design and reduce aberrations. We would like to remove the color wheel of a DLP
projector and replace it with our optical relay equipped with
a fast LCD. Another important next step would be rigorous
color calibration of the system taking into account the nonlinear nature of the diffraction gratings and the bent optical axis. Finally, several interesting applications arise from a
combination of an agile spectrum projector and agile spectrum camera. For example, we could perform wavelength
multiplexing as opposed to time multiplexing for stereo display. Wavelength multiplexing is better because it is transparent to a traditional RGB camera, unlike time multiplexing that introduces artifacts in high speed cameras. A paired
setup would also be useful to get the complete BRDF of fluorescent materials.
References
[BRN∗ 06] B ROWN S. W., R ICE J. P., N EIRA J. E.,
J OHNSON B. C., JACKSON J. D.: Spectrally tunable
sources for advanced radiometric applications. Journal of
Research of the National Institute of Standards and Technology 111 (2006), 401–410.
[FWSS07] FARUP I., W OLDY J. H., S EIMY T., S ØN DROL T.: Generating lights with specified spectral power
distributions. Applied Optics 46 (2007), 2411–2422.
[Gat00] G AT N.: Imaging spectroscopy using tunable filters: A review. In SPIE Wavelet Applications VII (2000),
vol. 4056, pp. 50–64.
[GOTG05] G OOCH A. A., O LSEN S. C., T UMBLIN J.,
G OOCH B.: Color2gray: salience-preserving color removal. In SIGGRAPH (2005), vol. 24, ACM Press,
pp. 634–639.
[HDC07] H ERSCH R. D., D ONZÉ P., C HOSSON S.:
Color images visible under uv light. In SIGGRAPH
(2007), vol. 26.
[Hec01]

H ECHT E.: Optics. Addison Wesley, 2001.

[Jam07] JAMES J.: Spectrograph Design Fundamentals.
Cambridge University Press, 2007.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

717

[LM91] L I W., M A C.: Imaging spectroscope with an optical recombination system. In SPIE Three-Dimensional
Bioimaging Systems and Lasers in the Neurosciences
(1991), vol. 1428, pp. 242–248.
[MH02] M ILLER P. J., H OYT C. C.: Spectral imaging
system. US Patent 6373568, 2002.
[MSL∗ 05] M AC K INNON N., S TANGE U., L ANE P.,
M AC AULAY C., Q UATREVALET M.: Spectrally programmable light engine for in vitro or in vivo molecular imaging and spectroscopy. Applied Optics 44 (2005),
2033–2040.
[NB03] NAYAR S. K., B RANZOI V.: Adaptive dynamic
range imaging: Optical control of pixel exposures over
space and time. In IEEE ICCV (Oct 2003), vol. 2,
pp. 1168–1175.
[Nel06] N ELSON N. R.: Hyperspectral scene generator
and method of use. US Patent 7106435, 2006.
[PWSS96] P OWER J. L., W EST B. S., S TOLLNITZ E. J.,
S ALESIN D. H.: Reproducing color images as duotone.
In SIGGRAPH (1996), ACM Press, pp. 237–248.
[RBN06] R ICE J. P., B ROWN S. W., N EIRA J. E.: Development of hyperspectral image projectors. In Infrared Spaceborne Remote Sensing XIV (2006), vol. 6297,
p. 629701.
[RBNB07] R ICE J. P., B ROWN S. W., N EIRA J. E.,
B OUSQUET R. R.: Hyperspectral image projector for
hyperspectral imagers.
In SPIE (2007), vol. 6565,
p. 65650C.
[RSJ06] ROSENTHAL E., S OLOMON R. J., J OHNSON C.:
Full spectrum color projector. US Patent 6985294, 2006.
[SKK∗ 06] S UGIURA H., K ANEKO H., K AGAWA S.,
S OMEYA J., TANIZOE H.: Six-primary-color lcd monitor
using six-color leds with an accurate calibration system.
In SPIE Color Imaging XI: Processing, Hardcopy, and
Applications (2006), Reiner Eschbach G. G. M., (Ed.),
vol. 6058.
[SNB03]

S CHECHNER Y. Y., NAYAR S. K., B EL P. N.: A theory of multiplexed illumination.
In ICCV ’03 (2003), vol. 2, pp. 808–815.
HUMEUR

[SOS98] S TOLLNITZ E. J., O STROMOUKHOV V.,
S ALESIN D. H.: Reproducing color images using custom
inks. In SIGGRAPH (1998), ACM Press, pp. 267–274.
[TAHL07] TALVALA E.-V., A DAMS A., H OROWITZ M.,
L EVOY M.: Veiling glare in high dynamic range imaging.
In SIGGRAPH (2007), vol. 26.
[Vis] V ISCHECK C OLOR B LINDNESS S IMULATOR:
http://www.vischeck.com/daltonize/.
[WWLP06] W ILKIE A., W EIDLICH A., L ARBOULETTE
C., P URGATHOFER W.: A reflectance model for diffuse
fluorescent surfaces. In Graphite (11 2006), pp. 321–328.

