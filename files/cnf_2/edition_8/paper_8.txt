DOI: 10.1111/j.1467-8659.2007.01099.x

COMPUTER GRAPHICS

forum

Volume 27 (2008), number 1 pp. 91–101

Object-Space Visibility Ordering for
Point-Based and Volume Rendering
C. Hofsetz1 , N. Max2 and R. Bastos3
1 Microsoft,

Redmond, USA
of California, Davis, USA
3 NVIDIA Corporation, Santa Clara, USA
2 University

Abstract
This paper presents a method to accelerate algorithms that need a correct and complete visibility ordering of
their data for rendering. The technique works by pre-sorting primitives in object-space using three lists (one for
each axis: X, Y and Z), and then combining the lists using graphics hardware by rendering each list to a texture
and merging the textures in the end. We validate our algorithm by applying it to the splatting technique using
several types of rendering, including point-based rendering and volume rendering. We also detail our hardware
implementation for volume rendering using point sprites.
Keywords: I.3.3 Computer Graphics: Picture/Image Generation-Viewing Algorithms.
ACM CCS: I.3.3 [Computer Graphics]: Display algorithms

1. Introduction

volume rendering. Our technique traverses the data in objectspace in linear time during rendering, using three pre-sorted
lists. This approach can be used for any rendering algorithm
that needs visibility ordering. Since our method allows the
visualization of semi-transparent structure, it can be used for
volume rendering. We also present our hardware implementation of direct volume rendering using our proposed visibility
ordering technique.

One of the problems in point-based computer graphics is how
to spread the contributions of the two dimensional (2D) projection from a single three dimensional (3D) point. The projection of a three dimensional (3D) point in the image space
occupies only one pixel. The rendering algorithm somehow
has to fill in the gaps in between pixels to avoid holes in the
image. If the projected point cloud results in several points
per pixel (so all their projections do not cause any holes), the
high density of the points may cause the problem of aliasing.

In Section 2, we present previous work in point-based and
volume rendering techniques, specifically emphasizing the
visibility problems. Next, in Section 3 we summarize our
approach, first introduced in [HM05]. Our algorithm is explained in detail in section 4. In Section 5, we present a specific hardware implementation of our technique using point
sprites - screen-aligned squares [OPe03]. Finally, in Section
6, we present multiapplication results of our algorithm and
performance results of our hardware implementation.

The most common method of rendering point clouds is
to increase the size of each projected point to multiple pixels [LW85] [ZPvBG01] [Wes89]. The contribution of each
projected primitive is then blended with the contribution of
its neighbours. This technique is called splatting. To blend
the projected primitives correctly, the primitives must be processed in a specific order with respect to the camera viewpoint
(called visibility ordering).

2. Previous and Related Work

We present here a new artifact-free technique that maintains the visibility ordering needed for some applications
while being generic enough to be applied to point-based and
c 2007 The Authors
Journal compilation c 2007 The Eurographics Association and
Blackwell Publishing Ltd., 9600 Garsington Road, Oxford OX4
2DQ, UK and 350 Main Street, Malden, MA 02148, USA.

The splatting technique projects object primitives on the
screen. If the original object primitive is only a point or a

91

Submitted December 2005
Revised May 2006
Accepted February 2007

92

C. Hofsetz et al. / Object-Space Visibility Ordering

voxel, its projected contribution is spread over an area (i.e.,
the point is splatted) for hole-filling and anti-aliasing purposes. This area is called a kernel, a footprint or a splat, which
can be defined in object-space (for instance, by centering a
fixed-size disk on the 3D point) or in image-space. Each point
of the kernel (or its projection) on the screen has colour, and
blending information (usually in the form of opacity).
If more than one projected kernel overlaps the same pixel, a
compositing scheme is needed. Proper compositing depends
on the correct ordering of the splats with respect to the viewer.
Occlusion is dealt with by either (1) searching through the
objects in a specific order defined by the viewpoint location,
or by (2) identifying which objects are actually visible before blending. Approach (2) is usually faster because it only
selects the frontmost splats for rendering.
The surfel technique [PZvBG00] [RPZ02] uses a
hardware-assisted two-pass solution for this problem. First,
the visible pixels are determined by projecting opaque polygons for each point (or surfel) to the screen. In a second
pass, the z-buffer then selects the frontmost projected splats,
which are defined as object space elliptical weighted average
(EWA) resampling filters, and projected and rendered as textured quadrilaterals. The algorithm is implemented in graphics hardware using programmable vertex and pixel shaders.
Their algorithm does not work for semi-transparent surfels.

volume. Their implementation runs on the GPU, but since
the correct ordering is still calculated in the CPU, they have
to recreate their vertex arrays for every new viewpoint.
Higuera et al. [HHFG05] introduced a volume splatting algorithm that uses hardware-accelerated point sprites available
on current graphics hardware. Their implementation achieves
high-quality interactive frame rates. The entire volume is
stored in the GPU and they need depth-sorted lists of indices
for eight pre-defined viewing directions. Their approach is
specific for sparse volumes such as those from neurovascular
data. To avoid ‘popping’ artifacts, they need a proxy geometry, which is difficult to obtain for generic, non-sparse, data.
Unlike [CRZP04], our technique does not have ‘popping’
artifacts. Compared to [MC98], [NM05] and [LMK03], we
do not need to resample the volume. In comparison with
[HHFG05], out technique is more general, since it works for
any kind of point-based and volume data. Our graphics hardware implementation using point sprites is similar to theirs,
but we only need to access three lists of indices or replicated
data (instead of eight). Our technique can also handle larger,
non-sparse, volumes, and our approach does not need a proxy
geometry, which makes it faster and more general. Our pointsprite implementation achieved a speedup of 12 with respect
to what was presented in [HM05].
3. Our Approach

Splatting is also used in volume visualization algorithms.
In the first implementation of the splatting technique for
volume rendering [Wes89], a front-to-back or back-to-front
object-space order is used to solve occlusion. Voxels that are
closer to the image plane take precedence over farther voxels.
To avoid sorting, Westover [Wes89] chooses the object-space
axis that is closest to perpendicular to the image plane and
renders the slices of that axis in a front-to-back order. This is
called axis-aligned splatting. It produces ‘popping’ artifacts
when the selected axis changes. Chen et al. in [CRZP04] implemented Westover’s algorithm in graphics hardware, using
EWA volume splatting. Their algorithm attained interactive
frame rates by loading the whole visible volume (i.e., the part
with nonzero opacity) to the graphics memory. They were not
able to remove the ‘popping’ artifacts, though.
Mueller and Crawfis proposed a modification in the technique by slicing the volume with sheets parallel to the image
plane [MC98], thus avoiding the artifacts.
Li et al. [LMK03] use texture-based volume rendering
while skipping invisible voxels using a hierarchical structure to represent sub-volumes. They claim that ‘rendering
with empty space skipping is two to five times faster than
without it’. Their approach resamples the volume to obtain
the textures.
Recently, Nophytou and Mueller [NM05] presented a new
technique for image-aligned splatting in hardware. Similar
to [MC98], they use image-aligned planes to resample the

A simple way to guarantee a back-to-front order of compositing is to form three lists of points sorted along the X, Y and
Z axes. Then, at rendering time, we choose the axis that is
most closely aligned with the viewing ray (or its negative)
and use its list in sorted (or reverse sorted) order. There are
three methods to choose the axis:
1. If this decision is made per frame, as in [WES89], based
on the viewing ray at the image center, the whole image
may appear to pop at the frame that switches to a new
sorting order.
2. In a perspective view, the decision can also be made per
pixel ray, using clipping. This was first introduced in
[HNM∗ 04] and it is generalized in [HM05] for any pointbased and volume rendering data. However, in this case
there is a moving set of transition lines across which the
order changes with the changing view, creating smaller,
but still visible artifacts, as the switching region becomes
1D in image space but also 1D in time. For animations
using a moving viewpoint, the artifacts are more noticeable because they move coherently. These artifacts are
the result of the discrete nature of the algorithm. There is
a discontinuity at the borders of the regions.
3. In this paper, we review a third method (also presented
in [HM05]), which effectively blends the sorting order in
a region near these transition lines, so that the artifacts
are much less noticeable. We have extended the work
in [HM05] by implementing most of the algorithm in

c 2007 The Authors
Journal compilation c 2007 The Eurographics Association and Blackwell Publishing Ltd.

C. Hofsetz et al. / Object-Space Visibility Ordering

93

graphics hardware, taking advantage of current state-ofthe-art GPUs, and using point sprites. Also, recall that we
do not need to resample the volume.
4. Our Algorithm
The application of our visibility-ordering technique in any
point-based rendering framework is straightforward. First,
as preprocessing done on the CPU, we create three sorted
lists of all primitives, one for each axis X, Y and Z. Then, for
each list, we project all its points to a hardware pixel buffer
(pbuffer). Each pbuffer has four (R,G,B,A) 8-bit components
(if triangles or quadrilaterals are used, we need an extra 24-bit
depth channel; this is not needed for our graphics hardware
implementation using point sprites—see Section 5). In our
implementation, we represent each projected primitive as an
elliptical Gaussian (it can be any shape, depending on the application). The ellipses are rendered as textured triangles or
quadrilaterals. If the projected ellipses are always perfect circles, they may also be rendered as point sprites. As the splats
are added to each pbuffer, we accumulate their colour and
opacity contributions using standard alpha-blended composition. In this step, we also store a weight for each projected
pixel. The weight is defined per virtual ray, and it may be
calculated per vertex (Section 4.2) or per virtual ray for each
pbuffer (Section 5.5). If calculated per vertex, the weight is
interpolated linearly for every pixel of the projected polygon. Finally, the three pbuffers are merged using a weighted
average.
If our data is volumetric, each primitive is represented as an
ellipsoid in object-space, which is the support of our 3D Gaussian kernel. We project the ellipsoid onto the image plane. If
the ellipsoid is entirely on one side of the plane through the
viewpoint, parallel to the image plane, its projection is an
ellipse.
The pseudocode for this technique is:
1. Build three global sorted lists, one for each axis in objectspace (X, Y and Z). For some data the primitives are already sorted (e.g., volume data in a structured grid).
2. For each axis,
a. Clear pbuffer belonging to the axis
b. For each data point in the dataset, in either first-tolast order or last-to-first order in the list for the axis
(the order is defined by the viewing ray at the image
center, see Section 4.1):
i. Calculate 3D vertex positions and weights of the
primitive based on the data point and its kernel.
ii. If the primitive is not culled: (see Section 5.4),
A. Project all vertices to image plane
B. Rasterize the projected primitive (a triangle, a
quadrilateral or a point sprite).
iii. Calculate and composite the colour and opacity of
each fragment and blend the result in the corresponding pbuffer.

Figure 1: The rendering algorithm. Modules in V are executed by a vertex program. The pbuffer merging is executed
by a fragment program. Modules in the yellow region are
executed once for each axis list (X , Y , Z ).
3. Merge the pbuffers using a weighted average.
4. Display the image.
Figure 1 illustrates our rendering pipeline. After the primitives are loaded into a vertex buffer, all rendering is performed
in GPU hardware, with the help of vertex and fragment programs. The fragment colour and opacity are computed by
multiplying the colour from the projected primitive and the
gaussian texture. Notice that although the lists are searched
using the correct visibility order of the primitives in step 2.b,
which list we pick first does not matter, that is, results are invariant to the order in which the lists are picked (XYZ, XZY,
YXZ, . . . , etc.).
4.1. Pre-sorting
We maintain three lists of all ellipsoids, sorted by the objectspace X, Y and Z components of each data point. Each pixel
in the image may receive a contribution from any one of
those lists, although most of the time the full contribution

c 2007 The Authors
Journal compilation c 2007 The Eurographics Association and Blackwell Publishing Ltd.

94

C. Hofsetz et al. / Object-Space Visibility Ordering

will be from the axis list corresponding to the component
of the viewing ray’s direction vector with largest absolute
value. The direction of traversing each list is defined by the
sign of the respective component from the viewing ray to the
image center: first-to-last if it is negative and last-to-first if
it is positive. That is, for each coordinate axis, the traversing order of the corresponding list is determined by the sign
of the image center viewing ray in object space. After we
calculate the vertex positions and weights, we also identify
which projected primitives may be culled (see Section 5.4) in
the current list being searched. A few primitives may have to
be splatted two or three times. It is also possible to traverse
the list always in the same way, changing the compositing
equation accordingly (see Section 5.3).

4.2. Vertex Weights
In order to merge the pbuffers correctly, we have to weight
how much each projected pixel in every pbuffer contributes
to the final image. To achieve this, we render each vertex vi
of our quadrilateral or triangle with an extra parameter wi (a
weight). The weight w is different for each virtual ray ri . The
virtual ray ri at vi is calculated as
ri = vi − E,

Figure 2: Vertex weights. X and Y indicate the regions of
axes X and Y. If the list of axis X is being rendered, splats A
and C are projected, and splat B is discarded.

x

A

(1)

where E is the camera’s viewpoint. The weight of ri is
wi =

x,y,z

ri

riM AX − h /(1 − h)

B

(2)

Notice that the weight w is independent of the primitive and
its vertices. w varies according to the virtual ray r . In fact, it
is possible to calculate all weights w during the merging step.
However, if we calculate w early we may use that information
to cull non-visible primitives before they are rasterized. The
downside is that we may have to calculate w several times for
the same virtual ray. While traversing the lists, the algorithm
will only project the splats where at least one of the vertices
has w > 0. We do this to avoid projecting the whole dataset
for every list. For example, in Figure 2, if we are searching
through the list of axis X, splats A and C are considered for
rendering, and splat B is discarded.
Figure 3 illustrates how our weighting mechanism works.
All virtual rays r that lie in between rays rn and r q have the

y

rq

rn

C

x,y,z

where ri is the component x, y or z of the viewing ray r
for vertex i and rMAX
is the largest component of the viewing
i
ray for vertex i. The component to be chosen (x, y or z) is
the same component as the current list being searched. For
example, if we are processing the list of axis X , rix is chosen
for weighting. h indicates how far into the neighbour axis
region the algorithm is going to project splats for blending.
The allowed values for h are [0, 1). If h = 0, all splats are
projected three times, once for each list. The closer h is to 1,
the fewer splats are projected more than once. The value h =
1 is not allowed.

rp

H
Viewpoint

Figure 3: The H region: 2D cut of a 3D scene where the
viewpoint is positioned at the bottom right of the figure. Yellow
is a transition region H where w x and w y are both nonzero.

Y value as the largest absolute value of all components of
the ray. In this green region, the correct back-to-front order
of the objects in the scene is A, B and C. Likewise, the X
component is the largest component of all rays r that lie in
between rays rp and rn . In this red region, the correct back-tofront order of the objects is A, C and B. In the green region,
y
wi = 1, and at some point it decreases linearly in the yellow
transition region H until it reaches zero at the border of the
y
red region. wi = 0 at r p .
Figure 4 shows a plot of weight as a function of the camera
rays r from figure 3. The width of the transition region is
defined by the value of h. As h gets smaller, this region gets

c 2007 The Authors
Journal compilation c 2007 The Eurographics Association and Blackwell Publishing Ltd.

C. Hofsetz et al. / Object-Space Visibility Ordering

Figure 4: Weight as a function of camera rays from figure
3: h defines the size of the yellow transition region H. The
dotted and dashed lined show the weights for the sorting on
the r p and r q axes, respectively.

larger. If h approaches 0, the resulting region approaches the
whole quadrant between r p and r q . We use h as one of the
parameters of the vertex program.
If w is calculated per projected triangle or quadrilateral,
it must be interpolated for every pixel covered by the splat.
Here, wi is calculated at each vertex in hardware, and it is
set as one component of the secondary colour of the vertex
i. Then, the values of w are automatically interpolated inside
each triangle or quadrilateral. The fragment program stores
the interpolated value of wi as a depth value in the pbuffer.
For each RGBA pbuffer, the colour (RGB) and opacity (A)
are blended using traditional alpha-blended composition. The
weights w stored in the depth buffer do not need to be accumulated, since they are the same for every contribution to the
same pixel in the same pbuffer. No depth test is performed.
After processing the splats in all lists, the three resulting
pbuffers are merged using a fragment program that reads the
three corresponding pbuffer textures. This is the only part of
our technique that is performed in image-space. The colour
C p for each pixel p is
Cp =

2
2
n=0 C n · wn
,
2
2
n=0 wn

(3)

where C n is the colour, including the alpha component, and
w n is the weight of the fragment for pbuffer n, and n can be
0 (X axis), 1 (Y axis) or 2 (Z axis). For most of our tests,
we use h = 0.85 (as seen in Figure 5). The higher h is, the
faster the algorithm runs. However, high values of h lead to
an effect comparable to simply clipping the splats, causing
problems similar to those described in section 3, method 2
For instance, if h = 0.99 we may be able to see the transition
lines (see Figure 8, Section 6.2).

95

Figure 5: Merge map for h = 0.85: each X, Y and Z list
is rendered to a pbuffer texture. The pbuffers are merged in
the last stage of the rendering. Areas of solid colour indicate
where only one pbuffer contributes to the final image.

Notice that when using point sprites, we still have to calculate wi for culling purposes (see Section 5.4). However,
since we only have one vertex for each point sprite, w cannot
be interpolated during the rasterization of the point sprite. In
this case, the depth channel is not used. The weight has to be
calculated (again) during the merging step.
Our method is independent of the filtering mechanism.
Rendering depends on how the splatting is filtered for every
pbuffer. If high frequencies are handled there, they appear
correctly in the final, merged, image. The effect on high frequencies comes from the splatting itself, and is thus dependent on the size of the splatting kernel, which is basically a
reconstruction filter for the sampled voxel data. Its size can
be adjusted to give good reconstruction, with minimal loss
of high frequencies. The list merging is basically a cross dissolve between two or more splatting approximations to the
ideal ray-tracing integral of the reconstructed function (actually, an integral over a presampling filter kernel for the final
image pixel, of this ideal ray tracing integral, as explained
in [ZPvBG02]). Each such approximation has errors because
splatting the 2D kernels in a specific order is not the same
as integrating the color and opacity effect of the sum of the
3D kernels along rays. The purpose of the cross dissolve is
to remove the high frequency visible edge between two such
approximations, as this edge is not part of the original data.

5. Hardware Acceleration
Although the main contribution of the paper is intended for
point-based and volume graphics problems, in this section we
focus on volume rendering, where each data point (a voxel,
in this case) is reconstructed and filtered using a spherical
Gaussian kernel, and its projection is rendered as a point
sprite. Volume graphics is of special interest because of its
large datasets and complex transfer functions. We believe

c 2007 The Authors
Journal compilation c 2007 The Eurographics Association and Blackwell Publishing Ltd.

96

C. Hofsetz et al. / Object-Space Visibility Ordering

Table 1: Compositing Equations. C and α are the accumulated
colour and opacity, respectively, before the compositing step. C and
α are the accumulated colour and opacity, after the compositing
step. Ci and αi are the colour and opacity of the current fragment.
Front-to-Back

Back-to-Front

C = Ci α i + (1 − α i ) C
α is not used

C = Ci α i (1 − α) + C
α = α i (1 − α) + α

verse the arrays (the weight remains the same for each ray
in its pbuffer, independently of the voxel). When using vertex arrays, it is better to traverse the data always in the same
way. The reason is that if we do not use an indexed array we
can only search it in one direction; and if we do use indexed
arrays we would need two arrays for every axis list, one for
each direction. The best strategy is to traverse the data always in the same way and change the compositing equation
accordingly for every pbuffer.
The OpenGL code for chosing the correct blending equation is

it is relatively easy to adapt the volume rendering solution
presented here to a generic point-based rendering algorithm.
The use of point sprites has the advantage of saving both
memory and bandwidth. Only one vertex per data point has
to be sent to and stored in the GPU, instead of three or four
for triangles and quadrilaterals. If the volume fits the graphics
card memory, we may store our vertex arrays in vertex buffer
objects (VBOs) in the GPU. In this case, we may render
novel viewpoints without sending the data from the CPU to
the GPU again.
5.1. Vertex Arrays
The idea is to create three vertex arrays, one for each axis.
The vertices contain the 4D RGBA colour and opacity of
each data point, its 3D gradient and its 3D position v i (a
single vertex array and three indexed lists with scalar values
can also be used, but the performance is affected by the size
of the volume—see Section 6).
5.2. Point-sprite size
The size of each point sprite is defined in pixels and it can be
calculated for every vertex in a vertex program. Nevertheless,
for performance and filtering purposes we define a fixed size
for all splats in image-space, PSsize . In our implementation,
PSsize is defined as the size in pixels of the projected Gaussian
sphere located at the center of the volume v c :
P Ssi ze = W

b|vc − E|
|vc − E|

,

where b is the radius of the Gaussian sphere, E is the camera’s
viewpoint, v c is the projection of point v c on the image plane
and W is the width of the image in pixels. PSsize is calculated
once for every frame on the CPU. In all our tests, we noticed
that the visual difference between using a fixed-sized splat
and adaptive-size splat for every voxel is negligible.
5.3. Compositing
As shown in Table 1, we have different ways of compositing
the colours and opacities depending on which order we tra-

if(Gaze[CurrentAxisComponent] > 0)
//Front-to-back
glBlendFunc(GL ONE MINUS DST ALPHA, GL ONE);
else
//Back-to-front
glBlendFunc(GL ONE,
GL ONE MINUS SRC ALPHA);
where Gaze is the viewing ray at the image center, and CurrentAxisComponent is the component of the current axis list
being processed (X = 0, Y = 1 and Z = 2). Notice that the
colour C i must be pre-multiplied by α i before blending. This
compositing scheme allows us to traverse our data always in
the same direction. If indexed arrays are used, only one list
is needed for every axis.

5.4. Culling
Immediate-mode rendering allows early culling of primitives
in the GPU, which saves processing data points that do not
contribute to the final image. When using vertex arrays, we
cull primitives by moving their vertices outside the viewing
frustum. We do a quick test to check if w for any pixels of the
splat footprint is greater or equal than zero by calculating δ
w:
δw = wi − w p ,
where wi is the weight calculated for v i , the voxel position.
The projection of v i is at the center of the point sprite. w p is
the weight calculated from v p , and v p is any point located at
the limit of the Gaussian sphere at v i . In our implementation,
v p is
v p = vi +

bR
,
2

where R is the camera’s right vector normalized and b is the
radius of the Gaussian sphere (R and b are parameters of our
vertex program). Then, if wi ± δ w ≤ 0 we add a large value
to the resulting vertex position, so it is guaranteed to be out
of the viewing frustum.

c 2007 The Authors
Journal compilation c 2007 The Eurographics Association and Blackwell Publishing Ltd.

C. Hofsetz et al. / Object-Space Visibility Ordering

97

Figure 6: Example of the application of our technique to several different rendering techniques. We show the images in pairs
to emphasize the absence of artifacts where the boundaries of our axes lists meet. In the top row, Red = X axis, Blue = Z axis,
Green = Y axis. For all scenes, h = 0.85. (a) point-based rendering [HM05] (b) light-field rendering using coloured point clouds
- hyperline rendering (c) image-based rendering of range data with estimated depth uncertainty; (d) volume rendering.
5.5. Merging
To merge the X , Y and Z pbuffers we have to calculate the
correct weight for every pixel in all pbuffers. When we are
using point sprites, we have not stored the weights for all projected pixels in the point sprite. In the pbuffer-merging phase,
each pbuffer is bound as a full-screen texture. A fragment program calculates the weight for every pixel i, and blends the
pbuffers accordingly using Equation 2. r i is computed per
pixel using Equation 1.
6. Results
In this section, we describe our experimental results. We start
by presenting the results of applying our technique to several
types of point-based and volume rendering methods. Then,
we show how different values of our parameter h affect the
final rendered image. Finally, we present a performance analysis specifically for our volume rendering graphics hardware
implementation.
6.1. Multiapplication results
Figure 6 shows the result of our technique applied to
point-based rendering [Figure 6(a)], light-field rendering using coloured point clouds, also called hyperline rendering

[HNM∗ 04] [Figure 6(b)], to the ellipsoid projection method
of [ZPvBG02] [Figure 6(c)] using range data with estimated depth uncertainty [HNM∗ 04] and to volume rendering
[Figure 6(d)]. Notice how we cannot see the discontinuity of
the axis region borders.
In Figure 6(a) we used a 2D elliptical splat oriented perpendicular to the estimated normal of each data point. The
model used in this scene is The Frederick P. Brooks, Jr. Reading Room in Sitterson Hall at UNC-Chapel Hill. This model
has 362880 points.
Figure 6(b) shows our technique applied to light-field rendering using coloured point clouds, also called hyperline rendering [HCM∗ 04]. Each geometric point is represented in the
light-field rayspace, and it is called a geometry hyperline. The
resulting footprint of each geometry hyperline is blended in a
way similar to the splatting technique; therefore, it also needs
sorting. The ‘Pisa’ dataset is a 3D model obtained from six
500 × 500 input images of a synthetic model rendered using
OpenGL.
In Figure 6(c), we render the depth with uncertainty by
splatting 3D ellipsoidal Gaussian kernels. Here, instead of
only using the depth information per pixel, we compute a
depth uncertainty region around each data point [HNM∗ 04].
We render the depth with uncertainty by splatting 3D

c 2007 The Authors
Journal compilation c 2007 The Eurographics Association and Blackwell Publishing Ltd.

98

C. Hofsetz et al. / Object-Space Visibility Ordering

Figure 7: Foot dataset showing flesh and bone.

ellipsoidal Gaussian kernels (based on the EWA Volume
Splatting work in [ZPvBG02]). The size of each ellipsoid
is given by its original pixel size and how uncertain the depth
of the point is; the higher the uncertainty, the larger the major axis of the ellipsoid is. Usually, the uncertainty is high in
low-frequency regions (regions with homogeneous colours
where it is difficult to find an exact correspondence between
images), and it is low in high-frequency regions. The higher
the uncertainty, the less contribution the splat has to the final
blending. The contribution of the ellipsoid is also weighted
according to how close the angle of the current viewpoint is
to the original image’s viewpoint of this ellipsoid. For more
information refer to [HNM∗ 04]. The model used here has
four 300 × 300 input images.
Figure 6(d) shows the result of a direct volume rendering
algorithm from the foot dataset. The model is a 183 × 255 ×
125 rectangular solid with 1 byte of density information for
each voxel, using spherical sprite splatting as described in
section 2. Finally, Figure 7 shows the foot dataset with a
different opacity and colour map, with both inner bone and
outer semi-transparent flesh structures.

6.2. Parameter tweaking
High values of h cause more triangles to be culled and, thus,
the algorithm runs faster. However, as h approaches 1, we may
start seeing the limits of the axes regions. Figure 8 compares
two different values of h when using the ‘Pisa’ dataset and
hyperline rendering.

6.3. Performance analysis
Our tests consider the worst-case scenario for our algorithm,
that is, when we have to render all three lists. Nevertheless,

Figure 8: Comparison of different h values: the Pisa tower
model, hyperline rendering, h = 0.85. Notice the artifact
(diagonal line) that shows up with h = 0.99 and that is not
visible with h = 0.85.
Table 2: Datasets used for our performance comparison. A voxel
is considered ‘valid’ if its opacity is nonzero

Number of valid voxels
Data
Engine
Foot
Head
MRI

Volume

Axis aligned

PBuffers

256 × 256 × 110
183 × 255 × 125
208 × 256 × 225
180 × 256 × 213

287512
781301
1018958
3570525

862539
2343902
3056874
10711575

for several viewpoints only one or sometimes two lists have
to be rendered [e.g., in Figure 6(c)]. In those cases, we should
expect an increase in performance of about 30% to 60%.
Table 2 lists our datasets used for volume rendering. The
Engine, Foot and MRI Head datasets are shown in Figure 9.
The UNC CT Head is shown in Figure 10.
Performance comparison for the datasets in question with
respect to rendering to pbuffers and with Westover’s axisaligned implementation is shown in Table 3. Indexed is
where indexed arrays were used to access each list and Replicated is where each list has its own vertex array. P. Sprites
is our hardware implementation with point sprites and Triangles is a similar implementation using triangles. Align
projects only one aligned axis (Westover’s axis-aligned implementation) and PBuff is our rendering to pbuffers implementation. Values in the table are in frames-per-second (fps).
N/A indicates where there was not enough memory to load
the data into the vertex buffer objects (VBOs). Although we

c 2007 The Authors
Journal compilation c 2007 The Eurographics Association and Blackwell Publishing Ltd.

C. Hofsetz et al. / Object-Space Visibility Ordering

Figure 9: Direct Volume Rendering, h = 0.85.

did not test rendering without VBOs in this system, usually
the performance drops around 10% when VBOs are not used.
The resolution of the rendered images is 512 × 512 pixels.
We did not include points where opacity is equal to zero (i.e.,
the CPU does not send a data point to the GPU if its opacity is zero), which implies that if the opacity map changes,
the lists have to be recreated. Our algorithms were implemented in OpenGL 2.0 using Windows XP running on a
3.0 Ghz Pentium 4 with 1 Gb memory, and an NVIDIA
7900 GT graphics card with 512 Mb memory. In this
system, our technique projects up to 50 million splats
per second.
Recall that Westover’s axis-aligned implementation does
not solve the ‘popping’ artifacts. We have included it here as a
reference to demonstrate how our hardware implementation
also improves the axis-aligned rendering technique. Also, as

99

Figure 10: The UNC Head. In the top image, Red = X axis,
Blue = Z axis, Green = Y axis. h = 0.85.

stated before, our graphics hardware implementation is at
least one order of magnitude faster than the work presented
in [HM05].
Not surprisingly, the implementation where indexed arrays were used, even though economic in terms of storage,
is almost always slower than replicating the voxels three
times. We believe this is due to the additional bandwidth
needed to send the indices. On the other hand, pre-processing
time is much smaller for large volumes (as seen in Table 4).
Since our target system had only 1Gb of system memory,
this is probably overhead of swapping virtual memory pages
between memory and disk. Besides reading the entire volume to the system memory, when using pbuffers, triangles
and replicated vertex arrays, our memory needs are larger
than our available space in our test system. For example, if
we consider the Foot dataset we need to create more than

c 2007 The Authors
Journal compilation c 2007 The Eurographics Association and Blackwell Publishing Ltd.

100

C. Hofsetz et al. / Object-Space Visibility Ordering
Table 3: Performance comparison—values are in frames-per-second (fps). N/A indicates
where there was not enough memory to load the data into the vertex buffer objects (VBOs) in
the graphics card

Replicated
P. Sprites

Indexed
Triangles

P. Sprites

Triangles

Data

Align

PBuff

Align

PBuff

Align

PBuff

Align

PBuff

Eng
Foot
Head
MRI

61.16
32.79
22.45
6.07

61.16
23.27
18.02
3.03

58.31
29.81
20.00
N/A

31.25
14.40
5.40
N/A

58.48
30.53
20.39
2.94

47.51
15.29
15.09
1.53

61.16
5.49
4.32
N/A

29.85
2.49
2.00
N/A

Table 4: Pre-processing time. Values in the table are in seconds. N/A indicates where there
was not enough memory to load the data into the vertex buffer objects (VBOs)

Replicated
P. Sprites

Indexed
Triangles

P. Sprites

Triangles

Data

Align

PBuff

Align

PBuff

Align

PBuff

Align

PBuff

Eng
Foot
Head
MRI

3.91
2.82
4.67
6.33

3.26
3.35
5.64
55.51

4.32
3.64
5.86
N/A

3.57
4.15
73.55
N/A

4.22
3.38
5.69
5.92

3.27
3.19
5.47
5.50

3.10
3.16
5.19
N/A

3.46
3.52
5.89
N/A

21 million vertices positions, colours, gradients and texture
locations.
Finally, we can achieve interactive frame rates even without using VBOs and vertex arrays. With the advent of
extremely high bandwith interfaces such as PCIExpress, we
can render large volumes that do not fit the available memory by constantly reading voxels (and pre-calculated gradients) from disk and immediately sending them to the graphics pipeline. Alternatively, parts of the volume may be loaded
from disk and quickly organized into vertex arrays and stored
in VBOs. The initialization time showed in Table 4 is an upper bound. It includes starting the program, the initialization
of OpenGL and window subsystems, calculating gradients,
loading the volume to memory and loading the vertex buffer
objects. Gradients may be pre-computed and stored in a file,
and OpenGL/window initializations have to be done only
once per execution of the program.

the market. For instance, in [NM05] the authors use a depth
test to perform an early-z rejection test and a technique called
early splat elimination, both of them performed in hardware.
A similar approach based on early ray termination (but applied for rendering slices) is used in [LMK03].
Besides improving performance, we intend to add new
features, such as varying the light position and perhaps interactive transfer-function modification.
Acknowledgments
The authors would like to thank Lars Nielsen, Anselmo Lastra, Nick England and David McAllister of the University
of Carolina at Chapel Hill for the reading room model, and
George Chen from STMicroelectronics for the camera calibration of the dataset in Figure 6c.
References

7. Conclusions
We have presented a new, general, hardware-assisted technique for rendering point-based and volume data while maintaining correct visibility ordering. In addition, we explained
our hardware implementation of this technique for volume
rendering.
In the future, we plan to further exploit our rendering system using the capabilities of the newest graphics hardware in

[CRZP04] CHEN W., REN L., ZWICKER M., PFISTER H.:
Hardware-accelerated adaptive ewa volume splatting. In
Proceedings of IEEE Visualization 2004 (Washington,
DC, USA, 2004), IEEE Computer Society, pp. 67–74.
[HCM∗ 04] HOFSETZ C., CHEN G., MAX N., NG K. C., LIU
Y., HONG L., MCGUINNESS P.: Light-field rendering using
colored point clouds - a dual-space approach. Presence 13,
6 (2004), 726–741.

c 2007 The Authors
Journal compilation c 2007 The Eurographics Association and Blackwell Publishing Ltd.

C. Hofsetz et al. / Object-Space Visibility Ordering

101

[HHFG05] HIGUERA F. V., HASTREITER P., FAHLBUSCH R.,
GREINER G.: High performance volume splatting for visualization of neurovascular data. In IEEE Visualization
(2005), p. 35.

[Ope03] OPENGL: Opengl architectural review board,
arb point sprite opengl extension. In OpenGL Extension Registry (2003), http://oss.sgi.com/projects/oglsample/registry/ARB/point sprite.txt.

[HM05] HOFSETZ C., MAX N.: Hardware-assisted visibility ordering for point-based and volume rendering. In
SIBGRAPI (2005), IEEE Computer Society Press.

[PZvBG00] PFISTER H., ZWICKER M., VAN BAAR J.,
GROSS M.: Surfels: surface elements as rendering primitives. In SIGGRAPH ’00: Proceedings of the 27th annual
conference on Computer graphics and interactive techniques (New York, NY, USA, 2000), ACM Press/AddisonWesley Publishing Co., pp. 335–342.

[HNM∗ 04] HOFSETZ C., NG K., MAX N., CHEN G.,
MCGUINNESS P., LIU Y.: Image-based rendering of range
data with estimated depth uncertainty. IEEE Comput.
Graph. Appl. 24, 4 (2004), 34–42.
[LMK03] LI W., MUELLER K., KAUFMAN A.: Empty space
skipping and occlusion clipping for texture-based volume rendering. In Proceedings of IEEE Visualization 2003
(Washington, DC, USA, 2003), IEEE Computer Society,
p. 42.
[LW85] LEVOY M., WHITTED T.: The use of points as a
display primitive. Technical Report TR 85-022, University
of North Carolina at Chapel Hill, 1985.

[RPZ02] REN L., PFISTER H., ZWICKER M.: Object space
ewa surface splatting: a hardware accelerated approach to
high quality point rendering. Computer Graphics Forum
21, 3 (2002).
[Wes89] WESTOVER L.: Interactive volume rendering. In
VVS ’89: Proceedings of the 1989 Chapel Hill workshop
on Volume visualization (New York, NY, USA, 1989),
ACM Press, pp. 9–16.

[MC98] MUELLER K., CRAWFIS R.: Eliminating popping
artifacts in sheet buffer-based splatting. In Proceedings of
IEEE Visualization 1998 (Los Alamitos, CA, USA, 1998),
IEEE Computer Society Press, pp. 239–245.

[ZPvBG01] ZWICKER M., PFISTER H., VAN BAAR J.,
GROSS M.: Surface splatting. In SIGGRAPH ’01: Proceedings of the 28th annual conference on Computer graphics
and interactive techniques (New York, NY, USA, 2001),
ACM Press, pp. 371–378.

[NM05] NEOPHYTOU N., MUELLER K.: Gpu accelerated
image aligned splatting. In Volume Graphics (2005), pp.
197–205.

[ZPvBG02] ZWICKER M., PFISTER H., VAN BAAR J.,
GROSS M. H.: Ewa splatting. IEEE Trans. Vis. Comput.
Graph. 8, 3 (2002), 223–238.

c 2007 The Authors
Journal compilation c 2007 The Eurographics Association and Blackwell Publishing Ltd.

