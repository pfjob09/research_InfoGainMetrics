DOI: 10.1111/j.1467-8659.2008.01179.x

COMPUTER GRAPHICS

forum

Volume 27 (2008), number 6 pp. 1687–1709

Efficient CPU-based Volume Ray Tracing Techniques
Gerd Marmitt, Heiko Friedrich and Philipp Slusallek
Computer Graphics Group, Saarland University, Saarbr¨ucken, Saarland, Germany

Abstract
Recent research on high-performance ray tracing has achieved real-time performance even for highly complex
surface models already on a single PC. In this report, we provide an overview of techniques for extending real-time
ray tracing also to interactive volume rendering. We review fast rendering techniques for different volume representations and rendering modes in a variety of computing environments. The physically-based rendering approach
of ray tracing enables high image quality and allows for easily mixing surface, volume and other primitives in
a scene, while fully accounting for all of their optical interactions. We present optimized implementations and
discuss the use of upcoming high-performance processors for volume ray tracing.
Keywords: interactive volume rendering, scientific visualization, ray tracing
ACM CCS: I.3.3 Computer Graphics: Raytracing

1. Introduction
This report presents the current state in interactive volume
rendering based on ray tracing extending surveys pre∗
sented by Meissner et al. [MHB 00] and Marmitt et al.
[MFS05], respectively. The following sections provide a
brief overview about ray tracing, volume rendering and
their correlations, and introduce the terminology used in this
report.

contributions along R(t) need to be sampled and accumulated
while secondary rays may be used to include global lightning
effects such as shadows, or multiple scattering of light (see
Figure 1). Secondary rays are only considered for iso-surface
rendering in the following.
We define the ray tracing algorithm as an algorithm that
given a ray – or set of rays – enumerates or accumulates the
contributions along the ray(s):
for all pixels do
enumerate primitives along ray
accumulate contribution

1.1. Ray Tracing
Ray tracing is a physically-based image synthesis technique
[PH04] which is well-known for its excellent image quality. In the past, this rendering algorithm was considered too
slow for interactive applications, but recently even real-time
frame rates on commodity PCs were achieved for polygonal
∗
scenes [RSH05, WBS02, WBS07, WIK 06] and for volume
data sets (e.g. [GBKG04]). Its core concept is the ray for
computing visibility and simulating the distribution of light
in a virtual environment.
A camera model is used to generate primary rays that are
cast through the pixels in the image plane to determine the
objects visible along a ray R(t) = O + D ∗ t with origin O
and direction D. For computing the light received at O, all
c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and
Blackwell Publishing Ltd. Published by Blackwell Publishing,
9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main
Street, Malden, MA 02148, USA.

This is exactly the inverted rasterization algorithm, which
iterates over primitives, determines the rays affected, and
then accumulates contributions to each of them:
for all primitives do
determine covered rays
accumulate contribution
Ray tracing corresponds to the common classification of
image-order rendering, while rasterization corresponds to
the classification of object-order rendering. Hybrid methods
exist as well. Note that the necessary sorting is implicit in
ray tracing but not in rasterization. In this report, we discuss
volume rendering algorithms that follow the presented ray

1687

Submitted October 2007
Revised January 2008
Accepted February 2008

1688

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

is versatile. Principle categories are whether the data set is
structured or unstructured (aka. irregular). Structured data
sets have an inherent organization that allows for simple addressing of the voxels given a position in the data set. On
the other hand, unstructured data sets require additional information in the form of an adjacency list for addressing operations, i.e. neighbourhood computations. Figure 2 depicts
some widely used volumetric grid types and their terminology. Note that the unstructured meshes result directly from
scattered data points by adding, e.g. a Delaunay tetrahedralization [Cho02].
Figure 1: A simple 2D ray tracing example: a ray is cast
from a camera through a pixel in the image plane into the
scene. After hitting the sphere, a colour is computed at the
hit point using a secondary ray (shadow ray) to calculate if
the hit-point on the sphere is lit by the light source or blocked
by an occluder.

tracing definition and only briefly summarize other volume
rendering approaches for reference.
1.2. Volume Rendering
Volume rendering is a process to generate 2D images from 3D
volumetric data. Primary acquisition sources for volumetric
data sets are (among others) computed tomography (CT)
and magnetic resonance imaging (MRI) scanners as well as
computational simulations like field and fluid simulations.
An excellent introduction to this area can be found in Engel
∗
et al. [EHK 06].
1.2.1. Volumetric data and interpolation
Volumetric data sets consist of (multiple) scalar, vector or
tensor entities that are given at discrete locations in space.
However, here we focus solely on single scalar values called
voxels (volumetric elements). They are the smallest element
of computation in volume rendering and represent physical
quantities like pressure or density. The data type of the voxel
values can be arbitrary ranging from binary up to floating
point numbers. The structure of these scalar fields of voxels

For regular and rectilinear data sets, we can define cells in
the grid as a box defined by eight grid points. For curvilinear
data sets, a transformation from physical space to compu∗
tational space [WCA 90] maps the distorted values into a
regular grid which allows for using the box cell structure. As
volume data is defined only at discrete locations in space, an
interpolation must be performed to reconstruct in-between
values. In the case of regular, rectilinear and curvilinear data
sets, a piecewise trilinear interpolation is usually applied but
higher order interpolations are also possible [The01] (see
Figure 3).
Unstructured data sets have an irregular cell structure that
is typically obtained by partitioning the data set into a tetrahedral mesh such that a piecewise linear (or higher order)
interpolation can be applied within each tetrahedron. The
tetrahedral partitioning yields also useful adjacency information that can be used for rendering.
In general, a value at point p i can be reconstructed as
weighted sum of the scalar values at the vertices of the cells.
The weights are obtained by computing the local coordinates
of p i relative to the cell using barycentric coordinates (see
Figure 3).

1.2.2. Ray tracing
The surface ray tracing has to be changed slightly in order
to render images of volumetric data sets. As we do not only
have empty space together with an opaque surface definition,
it is necessary to step along the ray through the volume to
accumulate the contribution from semi-transparent voxels

Figure 2: 2D examples of volumetric grid types (from left): regular, anisotropic regular, rectilinear, curvilinear and unstructured.
c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

1689

Figure 3: Left: a cell defined by eight voxel locations. Middle: within a cell at each location p i , a value can be calculated by trilinear interpolation. Right: within a tetrahedron
a piecewise linear interpolation can be used to interpolate
in-between values.

(see Section 1.3). The stepping distance is not necessarily
equidistant and can be set adaptively, i.e. for importance
sampling.
Ray tracing based volume rendering has many benefits
compared to other techniques like superior image quality,
many acceleration techniques to speed up the rendering process, and finally is highly parallelizable because all rays can
be traversed independently which can be exploited in various ways, i.e. via software single instruction multiple data
(SIMD) computations, multiple CPUs, multiple hardware
pipelines, and compute clusters. Most approaches consider
primary rays only which are usually referred as ray casting.

1.3. Volume Rendering Techniques
In order to obtain a meaningful visualization, a mapping is
used to transform voxel values to optical properties, i.e. absorption and emission coefficients (aka. colour and opacity),
and other entities that convey visual information.
In general, the literature differentiates among five volume
rendering techniques that operate in image order, each with
a special field of application [HJ04, LCN98, SM00]. See
Figure 4 for typical images and Figure 5 for an overview.

Figure 5: A one-dimensional example for ray tracing based
volume rendering: MIP seeks along the ray the maximum
value. X-ray computes the absorption along the ray. Isosurface rendering ends at the first hit-point of the ray with a
user specified iso-value. The semi-transparent method accumulates emission and absorption until the ray is saturated.

Decomposition: Decomposition methods convert the data set
into geometric primitives, e.g. spheres or slices. The primitives are placed in space and are scaled and coloured based
on the mapping. Slices are commonly rendered as pseudocoloured textures, height maps, or are used for further processing like segmentation.
Iso-surface Rendering: In some applications, it is important
to examine the distribution of a certain single value (the isovalue) within the data set. The visualization of all points
p within the data set with the same iso-value (the so called
level set) yields a surface, and therefore this method is widely
called iso-surface rendering. We can define iso-surfaces formally as f (x; y; z) = const.
Semi-transparent Rendering: Semi-transparent rendering
considers the volume as a transparent medium. If a light
ray pass through a volume, its radiative energy may be

Figure 4: Various volume visualizations used to render the engine data set (from left): decomposition (one slice rendered as
hight map), iso-surface, semi-transparent, maximum intensity projection (MIP) and X-ray.
c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1690

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

absorbed, emitted and scattered. To render a volume as a
transparent medium, a transfer-function [DHS01] maps the
scalar voxel values to absorption, scattering and emission
parameters. Then, the contribution along a ray can be computed by solving the general volume rendering integral. The
volume rendering integral in its low albedo form (scattering
should be ignored) [HJ04]
L

Iλ (x, y, D) =

cλ (p)μ(p) exp−

p
0

μ(t)dt

dp

(1)

0

computes for every pixel x, y with ray direction D, ray length
L and the incoming light I. The term cλ denotes the colour
(i.e. radiant energy) contribution with respect to the wavelength λ while μ denotes the absorption factor [HJ04].
For the sake of rendering performance, the general integral is usually greatly simplified and scattering is completely
neglected. This simplified integral becomes after some transformations (i.e. expansion to Riemann sum) two recursive
front-to-back compositing formulae, one for the colour c
(emission coefficient) and one for the opacity α (absorption
coefficient). This is well suited for ray casting:
ci+1 = c(pi+1 )α(pi+1 )(1 − αi ) + ci
αi+1 = α(pi+1 )(1 − αi ) + αi .

(2)

At each sampling point p i along the ray, these equations
are computed. Whereby c( p i ) denotes the colour of the interpolated voxel value (possibly with an additional transfer
function), and α( p i ) is the opacity. A nice property of this solution is that an early ray termination can be performed when
α is above some threshold (1 − ζ ). This means that light behind that sample point, which would have little effect on the
pixel of the image plane, is ignored. A back-to-front approximation of the volume rendering integral exists as well, but
because no early ray termination can be performed [SM00],
this method is hardly used in volume ray casting applications.
Maximum intensity projection (MIP): For each view and pixel
to be rendered, the maximum-intensity-projection method
computes the maximum value encountered along the ray.
This method is often used for magnetic resonance angiograms where thin structures, e.g. blood vessels, have to
be rendered accurately.
X-Ray Rendering: X-ray rendering approximates also the
volume rendering integral but ignores the emission and only
accumulates the absorption along a ray. This results in pictures which look like typical X-ray images. No mapping is
used except for the final pixel value.

1.4. Report Overview
In this report, we present efficient software implementations
for MIP, iso-surface and semi-transparent volume rending in
the context of regular/rectilinear data sets (see Section 3) as

well as curvilinear and unstructured grids (Section 4). We
discuss efficient traversal of the data structures and fast rendering computations within traversed cells. We also focus
on optimized algorithms and efficient data layouts. For reference, we briefly summarize in Section 2 non-ray tracing
based volume rendering algorithms.

2. Alternative Rendering Approaches
In this section, we briefly cover alternatives to software
volume ray tracing. We begin with projection, because this
is one of the most often used techniques. Projection methods can be further categorized in cell-projection, vertexprojection (splatting) and texture-mapping. Software as well
as hardware implementations exist for this approach. However, as graphics hardware became more and more powerful,
all approaches have been adapted to GPUs since 1990. The
section will be closed with brief discussions of the shear-warp
algorithm and several custom hardware implementations.

2.1. Cell Projection
An early software implementation based on projection was
proposed by Lucas [Luc92]. The projected faces of an irregular grid are sorted using the painter’s algorithm known
from polygonal rendering. Lucas uses only the centroids of
each face as sorting criteria which may lead to an incorrect
sorting. Wilhelms et al. [WGTG96] use a software scan conversion with optimized sorting by using coherence between
adjacent pixels as well as scan-lines. Cell faces are always
decomposed into triangles. A kd-tree culls invisible regions
from the current viewpoint.
Rendering unstructured data consisting of a tetrahedral
mesh was also early adapted to rasterization hardware.
Shirley and Tuchman [ST90] decompose each tetrahedron
in up to four triangles depending upon the viewpoint and
project each triangle onto the image plane. Accurate colours
are only computed for the triangle vertices. All other colour
values are generated using linear interpolation to compute
intersection points in-between a triangle.
Using 3D texture mapping, R¨ottger et al. [RKE00] were
able to extend the projected tetrahedra algorithm [ST90] for
hardware-accelerated rendering of volumetric data. Parts of
the volume integral depending upon the length of the segment are linearly approximated by a modulation of the vertex colours. The remaining parts depend upon the textural
coordinates and can be tabulated in a 2D texture map.
It is also possible to project irregular meshes onto screen
space and then process them in image-order using conventional ray tracing. It is a hybrid approach since the order
of covered cells is determined by ray shooting before accumulating. Bunyk et al. [BKS97] decompose the entire tetrahedral mesh into triangles. Before applying ray casting, the

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

1691

complete mesh is projected onto the screen, which simplifies
the intersection test from a 3D to a 2D problem. Adjacency
information computed during pre-processing is used to accumulate in correct order.
Hong and Kaufman [HK98] use the same approach for
curvilinear volumes. Each hexahedral face is decomposed
into two triangles, resulting in 12 triangles per hexahedron.
In a first version, each triangle was tested independently. In
a follow-up paper, Hong and Kaufman [HK99] suggested to
group all 12 triangles together and apply a ray crossing technique. The number of intersections determines the triangle
where the ray exits the hexahedron.
Projection-based ray-casting for tetrahedral meshes were
also adapted for graphics boards. Weiler et al. [RKE00]
use the ray–plane intersection [Gar90] to determine the face
where a ray exits the tetrahedron. The ray integration relies
on pre-integration, as described in [EKE01]. These computations are performed in the fragment program. Due to the
limited flexibility of GPUs at that time, ray casting is only
applied within a single cell. Subsequent cells along a ray
have to be treated by the following independent rendering
passes. Interactive rendering of mid-sized models is possible
with 2–5 fps.
Mroz et al. [MHG00, MKG00] suggested a rendering algorithm for orthographic cell projection but it is optimized
for maximum-intensity projection only. In a pre-processing
step, all cells completely surrounded by cells with higher
maximum values are removed. The remaining cells are sorted
into a list according to their maximum value together with
their position in space. Orthographic projection allows furthermore to store entry and exit coordinates per ray for each
pixel because the shape and size are identical for all cells.
Six arrays (each image-space dimension has three associated
object-space dimensions), therefore suffice to reduce the rendering almost to enable look-ups. Another approach, which
could also be implemented in image order was presented by
Mora et al. [ME05].
A fast iso-surface ray casting algorithm also based on
object-order ray casting was proposed by Neubauer et al.
[NMHW02] (see Figure 6 for two example images).
The complete data set is subdivided into macro-cells of
size m3 where m is usually between 4 and 10. This macrocells are then used to build a min-max octree (similar to
∗
[WG92] and [WFM 05]).
For each pixel on the image plane that has not yet been
processed, the octree is traversed and at each traversal step
the minimum and maximum values are checked whether cells
containing the iso-surface are in the next sub-tree or not. At
a leaf node, the boundaries of the macro-cell are orthogonally projected/rasterized onto the image plane. This yields
a hexagonal footprint. For each pixel in this hexagon, local
rays are used to traverse the macro-cell grid. This reduces

Figure 6: Two example images of the chest data set rendered with two iso-surfaces (iso-values 440 and 1100) and
Neubauers [NMHW02] approach. An average frame rate of
0:8 is reported with an image resolution of 512 × 512 pixel
on a single Pentium IV 1.9 GHz.

the number of traversal steps for the octree structure because
all pixels that are covered by the same hexagon would lead
to the same traversal steps. For the macro-cell traversal, the
method of Amanatides and Woo [AW87] is used. If a boundary cell, i.e. a cell possibly containing an iso-surface intersection, is encountered, an intersection test is performed with
the iso-surface and eventually the normal and shading are
computed.
Another adaption of a CPU based renderer was suggested
by Mora et al. [MJC02] and Hong et al. [HQK05]. Both
demonstrated an object-order ray casting approach for the
CPU, where rasterization hardware allows for fast projection
of the volume cells. However, these approaches are restricted
to regular volumes and orthogonal projection. A min-max
octree is first used for an efficient classification of cells.
Sub-volumes of N × N × N voxels are then projected onto
the image plane. Fragments are generated corresponding to
the rays intersecting with this sub-volume. The correct order
of sub-volumes is implicitly given by the min-max octree.
Dividing each sub-volume into pre-computed layers further
reduces the visibility ordering. Later, Mora and Ebert [ME05]
adapted the same data structures for MIP.
2.2. Vertex Projection (splatting)
Splatting is a forward mapping algorithm where the reconstruction of the original signal is achieved by spreading each
data sample’s energy across the cell’s footprint in image
space. This is often approximated by some 2D basis function. The volume is represented as an array of overlapping
basis functions where their amplitudes are given by the voxel
values. Common kernels are Gaussian, which are stored in
a footprint table. It was first described by Westover [Wes90]
rendering regular grids on a CPU.
Typically, two approaches can be distinguished: either
compositing all splats back-to-front or using the so-called

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1692

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

sheet-buffer method [MC98]. Here, the splats are organized
in cache-sheets, which are subsequently added back-to-front.
These cache-sheets are aligned with the volume face most
parallel to the image plane. This results in popping artifacts,
if the orientation of the compositing sheets suddenly changes.
Arranging the sheet-buffer parallel to the image plane overcomes this problem [MC98]. To this end it is necessary to
add slabs of partial kernels within the sheet.
However, splatting is not well suited for perspective projection. M¨uller and Yagel [MY96] therefore suggest a hybrid
method. The voxel contributions are partly pre-computed by
splatting in object space. Pixel accumulations happens using
ray-casting, i.e. by shooting rays intersecting the splats in
space.
Aliasing effects caused by the discrete evaluation of the
splatting equation can be reduced by adapting Heckbert’s elliptical weighted average (EWA) resampling filter [Hec89]
for volume splatting [ZPvBG01]. The footprint function is replaced with a resampling filter. Each footprint function is now
separately band-limited and is hence respecting the Nyquist
frequency of the rasterized image. Chen et al. [CRZP04]
proposed an adaptive EWA filter to get rid of further aliasing
artefacts.
2.3. Texture Mapping
Cabral et al. [CCF94] was one of the first showing that texture capabilities of graphics boards can be used directly for
rendering volumetric data sets. It can be seen as a hybrid
approach of backward and forward projecting. In a first step,
slices are generated parallel to the image plane by trilinearly
interpolating the sample values on each slice (i.e. backward
projection). After a slice has been processed, the result is
blended into the frame-buffer (i.e. forward projection). A final attenuation handles the case of off-centre pixels, because
the path lengths differ.
Engel et al. [EKE01] improve the rendering quality by
proposing pre-integrated volume rendering. The ideas presented in [RKE00] are extended and improved for regular
grids. Basically, all slices are converted to slabs, i.e. they
are enriched with a thickness so that interpolated values inbetween cannot be missed by a transfer function leading to
high frequencies. The numerical integration is split into two
parts: one for the continuous scalar field and another for the
transfer functions. The lookup tables containing the evaluated
transfer function for each possible input need modification
only when the transfer function is changed. R¨ottger et al.
also combined this approach later with volumetric clipping
∗
and advanced lighting [RGW 03].
2.4. Shear-Warp Algorithm
Shear-warp [LL94] is still one of the fastest software implementations for volume rendering. The basic idea is to
factorize the projection matrix into a 3D shear and a 2D

warp. Using a shearing operation the data set is transformed
into sheared object space. The 2D slices are aligned and
resampled such that they are perpendicular to the viewing
directing. In this space, all viewing rays are parallel to one
coordinate axis and the volume is considered as a stack of 2D
slices. Then an intermediate image can be composed using
the sheared object space values before warping this image to
the image plane. For a perspective transformation, each slide
needs an individual scaling during resampling.
Rendered images are prone to show stair-casing artifacts
near a 45◦ viewing angle. Intermediate slices lying halfway
between two adjacent volume slices overcome this problem.
Images may furthermore blur during a zoom-in, because the
resampling of the warp matrix is not adaptive. An enhanced
version solving these problems can be found in [SM02] but it
increases the computational cost. Although the warping step
significantly limits the produced image quality, especially for
perspective rendering, it is still used today, e.g. in VolumePro
∗
[PHK 99].

2.5. Custom Hardware
The need for custom graphics hardware arose with the
demand for real-time volume rendering systems. Neither
GPUs nor CPUs were fast enough at that time to achieve
this goal. Most systems have been developed for rendering regular data, e.g. Cube [KK99], Vizard [KS97] and
∗
VolumePro [PHK 99]. Due to the highly regular computation, all of them achieved real-time frame-rates allowing for
interactive rendering. However, changing or extending custom hardware is tedious and costly. Another disadvantage
which they share with GPUs is the limited memory. Out-ofcore solutions are in general not an alternative due to the high
bandwidth needed.
Cube [KK99] uses a hybrid-order algorithm based upon
the shear-warp algorithm. It was planed to compute only
the shear-step on-board and let the graphics board warp and
render the image. Eight identical rendering pipelines are able
to render a 2563 volume at 30 fps.
Never commercially realized, Cube [KK99] was the pre∗
decessor of the well-known VolumePro board [PHK 99]. Although the scalability was enhanced, perspective rendering
was only made available in the latest generation [WBLS03].
∗

Vizard [KS97], and Vizard II [MKW 02] were based on
an image-order processing offering full ray casting including early-ray termination. Phong shading was implemented
using look-up tables. The performance is not comparable to
VolumePro due to the field-programmable gate array (FPGA)
implementation making the system more flexible at the cost
of the achieved frames-rates.
The methods discussed so far provide fast and reliable
volume rendering. Parallelization is in almost all approaches

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1693

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

possible which works, e.g. in favour of modern GPUs. However, as we will see in Section 5.3, the flexibility of modern
graphics boards allows for the implementation of ray casting
directly. This significantly improves the image quality while
preserving the speed. Texture-mapping and shear-warp are
also fast and memory-efficient but lack rendering quality.
Custom hardware is fast and delivers high-quality but offers
so far only a limited flexibility.
Scientific visualization demands high-quality and flexibility. Ray tracing naturally offers both, but was until recently
considered as too slow. In the following sections, we will
present algorithms and data structures to accelerate volume
rendering and techniques to improve image quality.

3. Ray Tracing Based Rendering of Rectilinear Data
Regular and rectilinear data sets are the most common grid
types in volume rendering. Almost all scanner devices and
many simulation applications output these grids. In the following three sub-sections, we will discuss efficient algorithms for iso-surface rendering, semi-transparent volume
rendering, and MIP in addition to the necessary data structures to speed up the rendering process or to increase the
image quality.

3.1. Ray Tracing Based Iso-surface Rendering
The performance of iso-surface ray tracing depends heavily
on two algorithms: the intersection test of the ray with the
implicitly defined surface function within a cell, and the
identification of cells which contain a piece of the iso-surface
in front-to-back order.

3.1.1. Ray iso-surface intersection tests in a cell
Various methods have been proposed to calculate the intersection point of a ray with the implicit iso-surface function
in a cell.
Approximative methods. Such methods are cannot always
detect valid intersections points although they exist. One
representative of this is the method of Neubauer et al.
[NMHW02]. They suggested to use repeated linear interpolations. At the intersection points p in and p out of the ray
with the cell the values v in and v out are calculated. Then, it is
assumed that the function along p in and p out is linear. Solving
the linear interpolation formula for t i , an intersection point
t i can be calculated. By applying a trilinear interpolation at
t i and looking at v in and v out , a ray segment can be identified where a more accurate intersection point can be found.
This process is repeated recursively. Although this method is
computationally very cheap, it is error prone because it only
linearly approximates the trilinear function.

0

Analytic methods with C -continuity. The iso-surface
within a cell can be reconstructed using a trilinear interpolation. To do so, the ray equation R(t) is substituted into the
trilinear interpolation equation. Solving the resulting equa∗
tion for t results in a cubic polynomial (see [PPL 99] for a
complete derivation and discussion).
This cubic polynomial can be solved analytically by applying, i.e. Cardano’s formula. Nonetheless, an efficient and
numerical stable implementation is nontrivial [Sch98], even
when using double precision computations. Furthermore,
computational expensive calculations like [arcus] cosine are
∗
involved. Marmitt et al. [MKW 04] proposed a faster and
numerically more stable method, although only single precision computations are used. The key to their fast algorithm is
the observation that actually only one root of the cubic polynomial is needed. They isolate the roots by computing the
extrema of the polynomial. These extrema split the ray segment into at most three parts. Then, they step through these
segments from front-to-back, computing the data values at
its start and end points from the cubic polynomial. Once an
interval containing zero is found, it can be guaranteed that
it contains exactly one root, that the root lies in the interval,
and that it is the first relevant root.
The root within the interval can then be calculated by
a simple recursive bi-section until the desired accuracy is
reached or a small number of bi-section steps is performed.
Iterated bi-secting is more efficient and after three to four
iterations commonly no visual differences can be observed.
The calculation of an intersection point with this method
∗
[MKW 04] is sufficiently exact, numerically more stable,
and approximately three times faster compared to Schwarze’s
algorithm [Sch98] to solve Cardano’s formula.
Analytic methods with C1 -continuity. The accurate methods discussed before operate on one cell only for the sake
of computational performance allowing for interactive applications. This implies, however, that there is no higher-order
continuity, i.e. C1 is not guaranteed at cell boundaries. In the
following, an approach proposed by R¨ossl et al. [RZNS04]
is briefly covered. Neighbouring cells are not taken directly
into account but the cell is subdivided into 24 congruent tetrahedra to achieve smoother iso-surface contours. To compute
the intersection of the incoming ray with the iso-surface, 65
Bernstein-B´ezier coefficients are calculated, resulting in 10
coefficients per tetrahedron.
The ray-iso-surface intersection test is then computed by
solving quadratic equations along the ray for each tetrahedron. This quasi-interpolating spline results in an univariate
piecewise quadratic polynomial. Given the two intersection
points of the ray with a tetrahedron q 0 and q 1 as well as the
arithmetic average q = (q1 + q2 )/2 together with 10 B´ezier
co-efficients of the tetrahedron, the values w 1 , w 2 , and w are
computed by applying de Casteljau’s algorithm at all three
intersection points. The following equation then specifies the

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1694

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

Figure 7: A simple iso-surface (see Section 1.3) rendering
example: a ray steps through the volume until it hits the
iso-surface.
intersection point of the ray with the iso-surface:
ατ 2 + δβτ + δ 2 w1 = 0,

τ ∈ [0, δ],

(3)

where δ denotes the length of the ray segment spanning the
tetrahedron, α is set to α = 2(w 1 + w 2 − 2w), and β to β =
4w − 3w 1 − w 2 .
This intersection test is quite efficient in terms of computational requirements and allows for rendering high quality
iso-surfaces which satisfy a pseudo C1 continuity along cell
boundaries of the iso-surface. Additionally, shading gradients can be directly computed from the polynomial pieces
of the splines. Nevertheless, the memory requirements for
the needed coefficients is high and a careful tradeoff must
be made between pre-processing of all values and on-the-fly
computation.
3.1.2. Fast boundary cell traversal
A particular iso-surface defined by the data set is only located
within a small subset of all cells. These cells, called boundary
cells, are in general irregularly distributed, sparse, and often
enclosed by large regions of empty space.
A na¨ıve ray casting algorithm would step along a ray and
test all pierced cells whether a piece of the iso-surface can
be found within the cell. This is only applicable for small
data sets due to the high number of memory requests for
the voxels which would cause cache thrashing. Additionally,
it is computationally expensive to determine if a cell is a
boundary cell without any acceleration structure. Efficient
data structures enumerate the boundary cells in a front-toback manner to allow early ray termination (see Figure 7).
Such acceleration structures can be classified into surfacebased, range-based and space-based methods. Surfacebased methods use so-called seed-sets, i.e. a subset of all
cells with the property that every iso-surface has to intersect
with at least one cell in the seed-set. During visualization,
adjacency information is used to find other intersecting cells

once a located cell is given. Itoh et al. [IK95] proposed
extrema graphs and boundary cell lists for acceleration. Extrema graphs contain extrema points, i.e. points whose scalar
values are higher or lower than the values of all surrounding points, connected by arcs. Boundary cell lists are sorted
in ascending order with respect to their scalar value. Each
iso-value is tested against each arc for intersection, and the
intersecting cell for each possibly intersecting arc is found
and used as a seed for propagation. The boundary cell list is
intended for open iso-surfaces because they have no intersection with the arcs of the extrema graphs. Bajaj et al. [BPS96]
suggested that the interval trees build upon set of arcs.
An alternative are range-based methods, which build the
data structures based on the scalar values associated with each
cell. Gallagher [Gal91] divided the range of data values into
sub-ranges (buckets). For each cell, the starting bucket and
its span are computed so that cells with the same span can be
grouped together. Traversal is hence reduced to span groups.
A different approach uses min-max graphs, where each cell
is represented by a point in 2D. The x-coordinate corresponds
to the minimum scalar while the y-coordinate corresponds to
the maximum scalar of a cell. A 2D kd-tree is built over these
points to identify all cells containing the searched iso-value.
∗
Cignoni et al. [CMM 97] suggested interval trees to answer
the iso-surface query.
The last group performs the decomposition based on the
spatial information and hence allowing for querying the volume directly. Such approaches will be discussed in more
detail in the following sections.
Implicit min-max kd-trees. An approach by Wald et al.
∗
[WFM 05] utilizes min-max kd-trees for coherent isosurface ray tracing. In coherent ray tracing, SIMD extensions
can be successfully exploited for traversing packets of rays in
parallel. This can be realized efficiently with kd-trees, which
require only a single binary decision per ray in each traversal step. Although kd-trees seem ill-suited for data-parallel
packet traversal, as all of the cells in a regular data set are
small and thus rays in a packet could diverge and may traverse/intersect different cells, this argument is less relevant
for iso-surface rendering.
Due to the sparse distribution of the boundary cells, a
volume usually contains large regions of empty space. These
property is ideal for kd-trees [Hav01] (see Figure 8).
Building such an implicit kd-tree is quite easy. First, a kdtree over all the voxels of the entire data set is built. This is
done in a way that a kd-tree split plane always coincides with
the cell boundaries of the volume’s cells, yielding a one-toone mapping between the volume’s cells and the voxels of
the kd-tree. A simple way is to split the volume at the cell
boundary that is closest to the centre in the largest dimension.
Second, the minimum and maximum values are computed
recursively for all kd-tree nodes. Each leaf node stores the

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

1695

All that remains to be stored are the minimum and maximum values in the kd-tree nodes. A further memory saving
can be accomplished if the minimum and maximum values
at the leaf nodes are not stored, and instead are computed on
the fly from the cell’s voxels. This is tolerable because these
computations have to be performed only at leaf nodes.

Figure 8: Rather than traversing all cells along the rays
until an iso-surface is hit (left), a top down traversal in a minmax hierarchy can be used to quickly skip regions without
boundary cells (right). As the traversal of neighbouring rays
bears high coherence, at least on the higher levels of the
hierarchy, SIMD packet traversal can be efficiently applied.
minimum and maximum values of its associated cell, and
each inner node stores the minimum and maximum values of
its children. Note that this acceleration structure is similar to
the one used by Wilhelms and van Gelder [WG92].
In a na¨ıve implementation, one would store the entire tree
simply using the same node layout as for surface ray tracing
[WSB01]. By adding two additional values for minimum
and maximum to each node, it is possible to cull a sub-tree
early if no parts of the iso-surface can be found in both
sub-trees. Assuming default 16-bit data values, this na¨ıve
approach, however, requires 12 bytes for each node: 8 bytes
for specifying the plane and pointers, plus 4 bytes for the
minimum and maximum values. As a kd-tree of N leaves
has an additional N − 1 inner nodes, for N 16-bit voxels it
requires (2N − 1) × 12 bytes for the kd-tree. At 2 bytes per
input data value, the size of the acceleration structure would
then be 12 times the size of the input data. Obviously, this
overhead is too high. For 8-bit voxels, the relative overhead
would be even worse.
Fortunately, the memory overhead can be significantly reduced. If we assume for a moment that the number of cells
in each dimension is a power of two (the number of voxels
is then 2N + 1), the resulting kd-tree would be a balanced
binary tree, i.e. all its leaves are in the same level. In a balanced binary tree however it is easy to show that all the nodes
in the same level l will use the same splitting dimension d l .
Therefore, we only have to store this value once per level and
not in all nodes of level l. Furthermore, in a balanced kd-tree
no pointers are needed for address computations.
The remaining split plane positions in the nodes can also
be avoided. Let R d,l be the number of cells to be split in
dimension d and level l, i.e. level l splits in R x,l × R y,l × R z,l
cells. It follows immediately that there are at most R d,l − 1
possible split locations. As split-in-the-middle is a necessary
pre-requisite for balanced trees, it therefore suffices to just
save all possible split locations per level in a small array.

As mentioned before, this implicit kd-tree construction
works only if the data set has 2N cells in each dimension. One
simple method of making arbitrary data sets comply to this
constraint would be to pad them to a suitable size. Instead, a
better solution is to assume that all nodes are embedded in a
larger virtual grid that exceeds the scene’s original bounding
box and to build the kd-tree over that virtual grid.
By properly assigning the splitting plane positions, we can
make sure that all virtual nodes lie outside the real scene’s
bounding box. As the kd-tree traversal code always clips the
ray to that bounding box, we know that rays will never be
traversed outside the box, and thus can guarantee that no ray
will ever touch any of these virtual nodes. As such, we do not
∗
have to store them, either. For more details see [WFM 05].
Implicit kd-tree traversal. The described data structure is
– except for the minimum and maximum values stored per
node – very similar to the polygonal case where kd-trees
are typically used. Thus, the already existing traversal code
requires only a minor modification: During each traversal
step we first test whether the current iso-value lies within the
minimum and maximum range specified by the current node.
If this is not the case, we immediately cull this sub-tree, and
jump to the next node in the traversal stack. Otherwise, we
perform exactly the same operations as in the polygonal case
(see [Wal04] for a thorough discussion).
Results. Table 1 provides some performance numbers of
Wald’s approach. Their testing system was equipped with
five 1.8 GHz Dual-Opteron 246 processors connected via
Gigabit Ethernet using a view port size of 5122 for all test
scenes.
The data sets used for their experiments are: the bonsai tree
(2563 ), the aneurysm (2563 ), the synthetic Marschner-Lobb
(ML) data set (5123 ), the Visible Female (5122 × 1734),
and the Lawrence-Livermore (LLNL) Richtmyer-Meshkov
simulation (20482 × 1920). These data span a wide range
of different data, from low (ML) to high surface frequency
(bonsai, LLNL), from medical (aneurysm and female) to
scientific data (LLNL), and from very small (aneurysm) to
extremely large data sets. The table shows the performance
gain achieved by using packet ray (SIMD) instead of single ray (pure C) and the speed up of clustering processors,
respectively. The ratio between single and packet ray varies
between 1:5 and 1:8 for total views and raises close to the
optimum of 4:0 for close-up views. In such views, each
cell covers several screen pixels allowing for a complete

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1696

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

Table 1: Overall rendering performance in fps when running Wald’s
∗
[WFM 05] framework in various scenes including diffuse shading,
for both a single (dual-CPU) PC, as well as with a 5-node dualOpteron cluster.

Single PC

5-Node cluster

Scene

C

SIMD

Ratio

C

SIMD

Ratio

Bonsai
Aneurysm
ML 5123
Female
(zoom)
LLNL
(zoom)

3.4
3.0
1.2
2.7
2.3
0.9
1.6

5.2
6.2
2.3
4.2
7.9
1.3
5.4

1.5
2.0
1.8
1.5
3.5
1.5
3.9

16.2
14.6
6.1
13.6
11.2
–
7.6

24.6
29.8
11.3
20.7
39.1
–
28.7

1.5
2.0
1.8
1.5
3.5
–
3.8

The overview of the LLNL data set could not be rendered, because
the memory footprint at this view was larger than the 2-GB RAM
per client in the cluster setup.

Figure 9: Two time steps (45, 270) of the LLNL data set
∗
rendered with DeMarle et al.’s approach [DPH 03] between 6:1 (left) and 2:1 (right) fps. DeMarle et al.’s used
a PC cluster with 31 machines equipped with two Pentium
IV 1.7 GHz each. The resolution for each time step is 2048 ×
2048 × 1920 voxels.
traversal of four rays in parallel. Interactive frame rates are
in all test cases possible. An almost perfect performance increase can be achieved by running the framework on multiple
PCs in parallel. As can be seen, this setup allows up to 39 fps,
even including the most complex data sets. Finally, because
the kd-tree structure is stored in the implicit form, rectilinear
data sets can be supported as well.
3.1.3. Interactive large iso-surface ray tracing
An interactive out-of-core iso-surface ray tracing engine was
∗
presented by DeMarle et al. [DPH 03]. They exploit a PC
cluster to render volume data sets that are too large to fit
into the main memory of a single PC (see Figure 9). The
core rendering engine is basically a port of Parker et al.’s
∗
∗
[PMS 99, PPL 99] ∗ -Ray engine. The ∗ -Ray ray tracer is
considered as one of the first interactive ray tracing systems

and runs on a parallel shared memory SGI Onyx 2000 with
typically 128–256 processors.
For volumes, the ray tracer uses a hierarchical grid acceleration structure and a three level bricking (i.e. 3D tiling)
approach with a very low memory overhead for the acceleration structure. In some sense, this is similar to storing
minimum and maximum values only every N level in a tree
structure.
For the time-varying Lawrence Livermore National Laboratory (LLNL) data set of a Richtmyer–Meshkov instability
simulation (270 time steps) with a voxel resolution of 2048
× 2048 × 1920 and 8 bit voxel values (7.5 GB per time step)
only 8:5 MBs of memory are needed per time step for the
hierarchical grid data structure.
Similar to Wald’s distributed OpenRT rendering sys∗
tem [Wal04], DeMarle et al. [DPH 03] use also a typical
client/server architecture. A master system divides the image
into rectangular tiles and distributes the render tasks to the
render nodes on a per tile basis. If a node is a multi-processor
system the render task is broken down into sub-tasks, whereas
each task handles a single scan-line. An additional abstraction layer for data management is shared between all processors. If a ray touches a volume brick, it must request that
brick from the data management, which loads the data from
the network if the data is not present.
The hierarchical grid is similar to Neubauer’s approach
(see Section 2). At the leaves, a macro-cell size of approximately 83 voxels is used and the minimum and maximum
values are propagated up in the hierarchy. Rays start traversal
at the top hierarchy level using an incremental grid walking
algorithm. If an iso-surface is contained within a macro-cell,
the traversal starts again at the lower level grid. This process is executed recursively through the hierarchy until the
ray hits the iso-surface or the ray exits the volume. Ray isosurface intersection tests at the lowest level are performed
using an analytic approach solving Cardano’s formula (see
Section 3.1.1).
Results. DeMarle presents performance numbers for two
data sets: the torso part of the visible female (428 MB) and the
above mentioned LLNL data set (7:5 GB). Their test system
consists of 32 Linux PCs each equipped with 2 Pentium IV
1.7 GHz processors and 1 GB of RAM. All renderings are
done with a screen resolution of 5122 pixels.
The first benchmark, performed with the visible female
data set, is a scaling test. As expected, the performance increases linearly with the number of render nodes. With one
render node, approximately 1 fps can be achieved and with
32 nodes up to 22 fps. All in between values lie on an almost
perfect line. The LLNL data set can be rendered, dependent
on the complexity of the iso-surface, with 2.2 up to 6.7 fps
using the complete cluster system.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

3.1.4. Comparisons
In comparison to Wald’s kd-tree approach, the system of
DeMarle consumes much less additional memory for its acceleration structures. However, the reported performance is
almost five times slower for the visible female data. Wald
reports a performance of approximately 8.1 fps on a single
PC with two AMD Opteron 1.8 GHz CPUs. Furthermore,
an out-of-core extension from [WDS04] allows to render the
LLNL data set interactively on a single PC. Reasons for this
are mainly the optimized intersection test and the kd-tree acceleration structure which in general compares favourably to
the multi-level grid structure used by DeMarle.
3.1.5. Discussion
In this section, we have presented several algorithms to intersect rays with implicit iso-surfaces, acceleration structures,
and further optimization techniques as well as out-of-core
techniques for the rendering of extremely large data sets.
For iso-surface rendering the intersection tests of Marmitt
∗
[MKW 04] and R¨ossl [RZNS04] have both their strengths
and weaknesses. If memory overhead and computational
costs are not an issue and high accuracy, i.e. C1 -continuity,
is required, the approach of [RZNS04] is probably the algorithm of choice. In all other cases, the approach of Marmitt et
∗
al. [MKW 04] should be considered because multiple intersections within a cell are correctly treated, the computational
costs are very low, and no additional memory is needed.
Similar, as for polygonal scenes, kd-trees have proven to
be an efficient acceleration structure for iso-surface rendering
and the memory footprint is also within practical limits. Furthermore, just a single technique is necessary and no ‘magic’
parameters have to be selected to speed up the rendering
performance.
∗

In recent work of Wald et al. [WBS07, WIK 06], Reshetov
et al. [RSH05] and Lauterbach et al. [LYTM06], new traversal algorithms for grids, kd-trees and bounding volume hierarchies have been proposed to speed up the ray tracing
performance for polygonal scenes up to a factor of 10. It
would be interesting to exploit these new techniques also in
the domain of volume rendering.

1697

and thus are not visible. Efficient space leaping structures and
techniques are required to skip cells and volume regions that
do not contribute to the final image (Section 3.2.1).
Many results of computations, i.e. normal estimations for
shading, are (re)computed whenever they are needed. As
neighbouring rays tend to perform very much the same computations, it is worthwhile to cache values which can be
reused for the next ray, or packet of rays (Section 3.2.2).
Volume rendering requires a very high memory bandwidth
and may cause cache thrashing by displacing frequently used
data values from the caches. In order to achieve high rendering performance, this cache thrashing should be reduced as
much as possible because memory requests can be extremely
expensive in terms of clock cycles (Section 3.2.3).
3.2.1. Space leaping/empty space skipping
Space leaping is one of the most effective acceleration technique for semi-transparent volume rendering as it reduces the
number of reconstruction samples. In general, we can differentiate between techniques which use a spatial data structure
to skip empty regions and methods that exploit coherence
without pre-computing any data structures.
Space leaping with spatial data structures. In
Section 3.1.2, we have described two acceleration
structures namely multi-level grids and kd-trees to speed up
iso-surface ray tracing. However, there exists a large variety
of other approaches. One of the first complete visualization
systems based on ray casting was presented by Avila et
al. [ASK92]. The introduced polygon assisted ray casting
(PARC) algorithm works basically as an acceleration for the
grid traverser. A polygonal visual hull is constructed around
the invisible voxels within the regular grid. The graphics
boards’ z-buffers are then employed to determine for each
ray a near and far position for the grid traverser. This allows
a ray to traverse non-empty cells only, and hence to skip the
unavoidable empty surrounding of each data set.

The key for fast semi-transparent volume rendering is the
optimization of the following three elements (beside the early
ray termination previously described in Section 1.3).

A concept very similar to this are shell structures introduced by Udupa and Odhner [UO93]. Shells are defined as a
set of voxels in the neighbourhood of the structure boundary
with a number of attributes associated with each voxels. All
voxels within this set share the same range of opacity values where the ranges are application-specific defined. Voxels
completely surrounded by high-opacity voxels are not stored
in this set to save memory. The additional attributes contain
neighbourhood information, gradient, and transfer function.
Udupa and Odhner reported a performance boost of three orders of magnitude compared to a straight-forward (possibly a
brute-force grid traverser) implementation. The concept was
later extended by Yagel [Yag94].

About 60–80% of all voxel values are classified to be
transparent by the transfer function in real-world applications

Another obvious space leaping method is to store the
volume at different resolutions using a pyramidal structure

3.2. Semi-transparent Rendering
The most versatile visualization method for volumetric data
sets is the semi-transparent rendering model. Many interactive solutions have been proposed during recent years, each
trading rendering speed for quality or flexibility.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1698

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

[DH92]. Thus, homogeneous regions can be identified on
higher levels of the pyramid allowing to skip such regions
efficiently.
Cohen and Sheffner [CS94] suggested proximity clouds to
skip empty regions within a volume using a grid traverser.
Each cell contains the minimum distance to the next nonempty cell, which is computed in a pre-processing step. This
distance might either be based on the City-Block metric (i.e.
rectangular shapes around each cell) or on the Euclidean
metric (i.e. circular shapes around each cell). If a ray encounters a cell, the following n cells can be skipped, where n
is the encoded minimum distance. Freund and Sloan [FS97]
combined this idea with transfer functions, i.e. they encoded
the transfer function evaluation directly into the proximity
clouds to skip non-interesting regions.
The chessboard distance (CD) algorithm introduced by
Sramek [Sra94] works very similar to proximity clouds. In
this case, macro-cells are defined within the regular grid
by the chessboard distance (i.e. the maximum component
of a calculated distance vector). A ray is hence defined by a
sequence of non-uniform samples at its intersections with the
macro-cell faces caused by the varying sizes of the macrocells. Macro-cells maintain in contrast to proximity clouds
the condition of six-connectivity, because in the vicinity of
an object, the macro-regions are identical to a single cell.
During traversal, empty macro-cells can be skipped leading
to a speed up of more than three times [Sra94]. This approach
was later extended for rectilinear grids by creating a second
regular grid with the same dimensions as the original data set
[SK00, Sra96].
Other structures like octrees [WG92] and bounding
volume hierarchies can be exploited as well. The important point is that minimum and maximum values are available at every granularity level of the acceleration structure in order to know the range of values within the substructures.
To avoid a false transparent/opaque classification for
volume bricks, Grimm et al. [GBKG04] proposed for preclassification applications the use of quantized binary histograms. Basically these histograms h are lists of size s,
where s is the range of voxel values, i.e. 0–255 if 8 bit voxel
values are given in a data set. During the pre-processing
phase, for each brick, such a histogram is built. By iterating over the brick’s voxels, a flag is set to every h[v],
where v is the voxel value, and the rest is left to zero. To
save memory and increase the lookup efficiency, this list is
quantized into, e.g. 32 buckets. A similar histogram can be
built for the opacity values of the transfer function. To determine if a brick is fully transparent, one has to iterate over
all values in both lists and check if all voxels are classified as transparent. The binary histograms are conservative
such that a transparent region can be classified as opaque
but not vice versa, and thus no non-transparent regions are
skipped.

Space leaping without spatial data structures. Lakare
et al. [LK04] proposed a space leaping technique that exploits
coherence in brute force ray casting applications. Their idea
is driven by the fact that a group of rays very likely traverses
the same amount of empty space until a semi-transparent
region is reached.
To exploit this observation, two different kinds of rays
are used in two render passes: detector rays, shot in the
first pass, and leap rays used in the second pass. In the first
render pass, one detector ray is shot for a group of four pixels
(see Figure 11 for the sampling pattern) on the image plane.
All detector rays are shot and behave like in a traditional
ray casting system performing ray marching, sampling, etc.
Additionally, they keep track of the first non-empty region
they reach and store this information in a leap buffer.
For every pixel, the leap buffer has in the image plane
a corresponding entry that encodes a distance. For detector
rays this distance is the number of transparent samples until
a semi-transparent cell is found. The entries for the leap rays
are filled with the minimum number of samples during which
the rays encounter empty space by spreading the values from
the detector rays to adjacent pixels. The space-leaping rays
are then shot in the second pass starting at the minimum
position encoded in the leap buffer.
This algorithm is only correct if the volumetric object is
not too far away from the camera. If this distance exceeds a
certain value, rays diverge too much in perspective rendering
and sampling artefacts can occur due to missed features.
However, Lakare et al. [LK04] report that with this algorithm
brute force ray casting can be accelerated up to 165%.
3.2.2. Caching and pre-computations
As previously mentioned, it can be worthwhile to cache some
values during ray casting that are likely to be reused in the
near future. Grimm et al. [GBKG04] proposed to use a gradient cache on a per brick basis (see Section 3.2.1), for gradients (i.e. the first derivative of the function) [MMMY97].
See Figure 10 for an example image.
They use two data structures for efficient caching. The gradient cache itself and a bit list which indicates if a particular
gradient has already been computed and is in the cache or not.
The number of entries in the cache is equal to the number of
voxels in the brick. Every time a gradient has to be computed,
the corresponding presence bit is checked if the gradient has
already computed. If not, the gradient is calculated. After
processing the brick, the cache is flushed for the next brick.
The gradients on the border of a brick have to be calculated
up to eight times, which is rarely the case and thus no significant performance is lost. In average, a speed up factor
of approximately 2.2 can be expected by using the gradient
cache. Nevertheless, this high speed up can only be expected
if orthogonal projection is used. In perspective rendering, the
rays can diverge for bricks that are far away from the image

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

1699

Figure 10: A close up of the visible male data set (hip region) rendered with the approach of Grimm et al. [GBKG04].

Figure 11: Left: a part of the image plane showing the location of the detector rays (orange dots). For the rest of the
pixels leap rays are used. Right: the distances of the detector
rays are spread to the remaining pixels by using a minimum
operator.

Figure 12: Two example images of the UNC MRI-head, with
256 × 256 × 110 voxel resolution, rendered with the UltraVis
system on a PIII 500 MHz system and 256 × 256 image
resolution. The images are rendered with between 1.7 and
7.0 fps.

plane. As a result, the distance between neighbouring rays is
large, which will lower the reuse ratio for caches.

(the so called set) where a particular data from the memory
can be mapped to. Considering that the main memory is
internally subdivided into equal sized pages whereby the
offset of a particular data in this page determines the set in
the cache, four times more memory for the volume data set
is allocated as actually needed. The voxel data are then only
placed in the first quarter of each page. Data that are placed in
the remaining parts of the pages will then never be replaced
by voxel data – within the cache.

3.2.3. Memory and cache optimizations
Knittel [Kni00] presented with UltraVis a highly optimized
ray casting approach for semi-transparent volume rendering
which is especially targeted for Intel’s Pentium-III processor
(see Figure 12). His approach depends heavily on concrete
details of the cache replacement strategies of the P-III processor. He observed that a major obstacle for high performance volume rendering is the limited memory bandwidth.
Ray tracing based volume rendering has a memory access
pattern that does not fit today’s cache architectures and thus
many data have to be reloaded numerous times due to cache
thrashing.
Cache optimizations and spread memory layout. In
Knittel’s UltraVis system, three different data structures are
accessed: the volume data itself, the transfer function and
additional parameters such as for thresholds and shading.
In order to avoid, the replacement of frequently used data in
the cache, i.e. the transfer function values, a spread memory
layout is used to virtually lock the data in the level one cache
once they are loaded. The Pentium III processor uses a 4-way
associative cache which means that there are four cache lines

In order to reduce the cache thrashing of the voxel data,
the order of the voxel values is altered via a cubic interleaved
address function. Using such an interleaved memory storage
a cube of n3 voxels occupies n × n × n cache locations
(assuming n is a power of two, and n3 is smaller than the
cache size). This reduces cache thrashing and thus increases
the cache hit ratio because neighbouring rays very likely
access some of the previously cached voxel values.
3.2.4. Discussion
Many techniques have been proposed to speed up semitransparent volume rendering and only a fraction could be
discussed here. However, today there is no software implementation that is able to render high quality images of midsized data sets on a single PC without any limitation either in
terms of quality or flexibility, e.g. orthographic rendering is

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1700

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

used to simplify the rendering process. Furthermore, acceleration techniques like object-order ray casting [MJC02] are
only useful for primary rays. If secondary rays are needed,
there is no single basis like the image plane where these
techniques can be appropriately used.
However, new processors like IBM’s Cell are on the way
which can avoid some of the problems that are inherent in
semi-transparent volume rendering. For example, Knittel’s
approach to avoid cache thrashing is not necessary on the
Cell architecture because there is simply no cache on the
Cell’s SPEs. The new challenges are then mainly how to keep
the SPEs busy using, e.g. virtual hypertheading techniques
in software as well as ‘hand tuned’ caching for data which
are not frequently used and sometimes altered. Additionally,
today’s solutions which rely on massive computational power
using compute clusters can soon be shrunk to a single PC
using the Cell processor (see Section 5.2).
3.3. Maximum Intensity Projection
MIP is an important technique mainly used in real-world
medical imaging applications to visualize thin structures like
blood vessels or contrast-enhanced tissues. A ray-tracing algorithm was first proposed by Sakas et al. [SGS95] by comparing the current cell maximum with the traced maximum of
∗
the ray value. Parker et al. [PHK 03] improved this method
by adding bricks, i.e. blocks of constant size. Similar to Sakas
et al.’s method, bricks can be skipped if the maximum ray
value is higher than the highest value within the brick.
∗

Parker et al. [PPL 99] suggest the use of a priority queue in
conjunction with a spatial data structure, i.e. min-max multilevel grids. The priority queue is used to keep track of cells or
macro-cells in the grid with the maximum values. Although
they use the priority queue in combination with a min-max
multi-level grid, all other common min-max hierarchical data
structures could be exploited as well.
First, the priority queue is initialized with the root node
and its maximum value. Iteratively, the node with the highest
maximum value is taken from the list and its children are
inserted instead. At a leaf node, a macro-cell traversal is
started and at each pierced cell-face a bilinear interpolation
is performed because they assume a linear function along
the ray, and thus no extremal value can be found within a
cell. The maximum value encountered along the ray segment
through a macro-cell is again stored in the priority queue.
The algorithm terminates if one of these leaf cells appears at
the head of the priority queue. This approach was recently
adapted for the implicit kd-trees [GLBH07].
4. Ray Tracing Based Rendering of Irregular Data Sets
Handling curvilinear or even unstructured data is more demanding compared to regular grid structures. However, software ray-casting systems were proposed as early as 1990

∗

[Gar90, Lev90, WCA 90]. Different methods have been developed in the following years. Usually, two steps need to be
taken for rendering irregular data. In a first step, the initial
cell (tetrahedron or hexahedron) is determined using some
acceleration structure containing the volume boundary faces.
The following step iteratively traverses all subsequent cells
along a ray until the last cell is reached. Incremental traverser require adjacency information and hence do not allow
for sliding interfaces, i.e. complete faces must be shared between adjacent cells.
4.1. Locating the Initial Cell
One obvious way to find the initial cell is the extraction of
boundary faces from the data set for tetrahedral and hexahedral meshes. Such faces are easily identified as they are
the only faces not shared between two cells. A spatial index
structure over these faces of the cells then allows to quickly
locate the entry point and cell using standard ray tracing.
Several researchers [BKS97, HK98, HK99, KN91] proposed to project the boundary faces onto the image plane and
fetch the associated cell from the data set. A disadvantage of
these approaches is that they require an update each time the
viewpoint changes. This is avoided by using a spatial index
∗
as acceleration structure. So far grids [Gar90, PPL 99] and
kd-trees [MS06] were proposed.
4.2. Ray-primitive Traversal
Image order ray casting was not only one of the first methods proposed for rendering irregular grids but also reached
∗
interactive frame rates as early as 1999 [PPL 99]. We will
therefore cover recent implementations in this area in more
detail.
∗

Garrity [Gar90] and Wilhelms et al. [WCA 90] were the
first who considered software ray casting for irregular grids.
Wilhelms decomposes the surface of each hexahedral cell
into 12 triangles and uses the barycentric coordinates for
interpolation of data values. Garrity decomposes each hexahedral cell into five tetrahedra (see Figure 13) and performs
ray-plane intersections for his incremental traversal.
4.2.1. Computational space traversal
The computational space is an abstract representation of the
logical organization of a curvilinear (or even rectilinear) grid.
It can be seen as a mapping from a warped (i.e. curvilinear)
grid to a regular grid with orthographic coordinate system
and unit length of cells for each dimension. This term originates from numerical simulations, because all computations
are performed in this space. All results are then mapped
back to the physical space, i.e. the non-regular representation [Fr¨u94].
Fr¨uhauf [Fr¨u94] traverses a curvilinear grid in computational space. This greatly reduces the difficulties of

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

1701

Figure 13: A convex hexahedron can be decomposed into
five tetrahedra if the renderer supports tetrahedral meshes
only.
∗

resampling along the ray because the computational space
is regular. Each vertex or vector can be transformed from
computational into physical space and backwards using the
Jacobian matrix. This Jacobian is approximated using the
central differences of the adjacent vertices in computational
and physical space. Each vertex can now be translated from
computational into physical space by multiplication with the
corresponding Jacobian matrix element. The computed vector for this conversion is attached to each grid vertex.

Figure 14: Parker [PPL 99] sort the tetrahedral mesh into
a grid. Once a grid-cell possible containing the iso-surface
is determined, all tetrahedra intersecting the grid-cell are
tested sequentially.

An incremental grid-traversal for regular grids is employed
to resample the scalar values. At each face intersection of
a grid cell, the ray is bent by bilinearly interpolating the
previously computed vectors. To reduce the computational
costs, these interpolated ray paths are stored and updated
only when changing the viewpoint. Hence, if the mapping
parameters, e.g. transfer function, change, the pre-computed
ray paths can be used directly. Unfortunately no performance
measurements were reported.

As data structure, the multi-level grid for regular data sets
is reused again in this case to determine the subset of tetrahedral cells containing the iso-surface or the maximum intensity. To this end, each tetrahedron is sorted into its corresponding grid cell on the lowest level (see Figure 14). Again,
the grid cells on all levels contain minimum and maximum
values, in this case resulting from the list of tetrahedra attached to each cell on the lowest level. Each ray first traverses
the multi-level grid until it finds a cell on the lowest level containing tetrahedra with possible iso or maximum value. This
restricts the search to tetrahedra lying in the found grid cells
and efficiently culls large regions of the volume data set.

4.2.2. Ray-plane-based traversal
Ma [Ma95] was one of the first exploiting the parallelism of
ray tracing for rendering tetrahedral data. The data set as well
as the rendering is distributed among processing nodes. In a
pre-processing step, the volume is partitioned such that each
processor handles only a sub-volume. Boundary faces are
projected orthographically onto the screen to determine the
first cell along a ray. Exit faces are determined as described in
[Gar90]. All processors accumulate the interpolated values
along a ray independently. These contributions must be sorted
with respect to the visibility order before the final image can
be composed. Ma showed that the frame rate scales linearly
with the number of processors.
4.2.3. Grid-based traversal
∗

To our knowledge Parker et al. [PPL 99] showed one of the
first methods that was able to render irregular volume data

at interactive rates on a supercomputer, but was restricted to
iso-surface rendering and MIP. The later one is realized using
a priority queue tracking the current maximum value within
the hierarchical data structure (see Section 3.3).

As no connectivity information is used, all tetrahedra assigned to grid leafs need to be checked, like in a traditional
surface ray tracer. First, the barycentric coordinates from the
intersected faces, i.e. triangles, are determined for interpolating the scalar value. The intersection with the iso-surface
can then be computed using linear interpolation.
Results. The rendered bioelectric field consists of over one
million tetrahedra. Using a 512 × 512 view port, interactive rendering was possible with 16 processors or more (see
Figure 15). Curvilinear volumes are not supported by this
system although possible when decomposing each hexahedron into five tetrahedra as demonstrated in Figure 13.
Parker et al. demonstrated high-quality volume rendering of large data sets on a super-computer. The iso-surface

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1702

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

was not extracted but computed on the fly and could hence
be changed during rendering. The measured performances
showed again that a ray tracing system scales well with the
number of processors.
¨
4.2.4. Plucker
space traversal
An optimized traversal algorithm suitable for unstructured
and curvilinear data sets was presented by Marmitt and
Slusallek [MS06]. The volume is traversed using so-called
Pl¨ucker tests after finding the initial cell using a kd-tree.
Basically, the Pl¨ucker coordinates allow for easy determination whether an oriented line passes clockwise or counterclockwise around another oriented line in space [Eri97].
The exiting face of a cell can be determined by testing
all edges with the traversing ray in Pl¨ucker coordinates. Using connectivity information, the next cell can be processed
in the same way until the last cell along a ray is reached.
As this test is line based, it can be used for arbitrary polyhedra and hence especially for triangles and quadrilaterals,
as the faces of tetrahedra or hexahedra (with planar faces),
respectively.
Tetrahedral traversal. The tetrahedral traversal is basically
an extension of the ray-tetrahedron intersection demonstrated
by Platis and Theoharis [PT03]. Additional advantages are
gained by fact, that all inner faces are shared by two tetrahedra. Pl¨ucker coordinates for these three edges hence do
not have to be computed either. These known edges from the
previous exiting face are given by the vertices v 0 , v 1 , and v 2
in Figure 16.
To determine the exit face, lines v 0 → v 3 and v 1 → v 3 are
tested against the ray which already provides a final result
for one third of all cases on average. Otherwise, an additional test of the ray against v 2 → v 3 finally determines the
exit face. With these optimizations the number of tests is
reduced to 2.66 in average [MS06]. Figure 16b shows the
theoretical optimum with 2.33 tests. This however, complicates the control flow, because more conditionals are now
needed. For this reason, a better performance can be barely
expected.
To move along the tetrahedra, additional 16 bytes of connectivity information are required. All Pl¨ucker coordinates
are computed on-the-fly and consume therefore no memory.
For the final rendering, the interpolated scalar value at the
intersection point is needed for visualization.
To visualize iso-surfaces, it is sufficient to check whether
the user-defined value is within the interpolated scalar values
computed at the entry and exit face with the barycentric
coordinates.
Hexahedral traversal. For this purpose Marmitt and
Slusallek [MS06] use again the properties of the Pl¨ucker

Figure 15: Ray-tracing of over 1 million tetrahedra from
abioelectric field simulation. Heart and lungs are represented
∗
as polygonal mesh for orientation [PPL 99].

Figure 16: (a) Optimized approach: the solid lines’ tests
are given from the exit face of the previous tetrahedron and
the dotted line needs only to be computed if the test with
the dashed lines failed to provide an unambiguous result.
(b) Theoretical optimum: applying one test (dashed line) followed by either one of the dotted lines for exit face decision.
An additional test is in one-third of all cases necessary for
computing barycentric coordinates.

space and basically apply the same op-timizations as in the
tetrahedral case. When optimized for planar hexahedral faces
at most four tests per hexahedron are necessary. This algorithm is illustrated in Figure 17. In this figure, the entry face
is determined by v 0 , v 1 , v 2 and v 3 . The opposite face is
determined by v 4 , v 5 , v 6 and v 7 .
In a first step, the ray is tested against the edges v 4 → v 5
and v 6 → v 7 (bold horizontal lines in Figure 17). Note that
each area contains at most three faces of the hexahedron, i.e.
there are only two edges left to check. These are different for
each area, e.g. v 2 → v 6 and v 3 → v 7 for A 0 , v 4 → v 6 and
v 5 → v 7 for A 1 , and v 0 → v 4 and v 1 → v 5 for A 2 .

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

1703

Figure 17: (a) To determine the exiting face, the (planar)
hexahedron is subdivided into three areas (A 0 − A 2 ). (b) The
next step is to check which face is intersected by applying two
additional Pl¨ucker tests (A 0 : dotted edges, A 1 : solid edges
and A 2 : dashed edges).
Table 2: While the performance (in fps) of the hexahedral Pl¨ucker
traverser (hex-P) is for maximimum-intensity-projection (MIP) and
semi-transparent rendering (semi) slightly lower than the tetrahedra Pl¨ucker (tetra) for the Blunt-fin, the hybrid approach (hex-H)
outperforms the other two algorithms in every rendering task.

Data set

Initial

Iso

Mip

Semi

Blunt-fin (tetra)
Bucky-ball (tetra)
Blunt-fin (hex-P)
Blunt-fin (hex-H)
Comb (hex-P)
Comb (hex-H)

27.35
21.68
27.35
27.35
18.43
18.43

1.67
0.92
1.86
2.00
2.48
2.43

1.99
0.95
1.35
2.16
0.88
1.25

1.74
0.84
1.55
1.77
3.14
3.55

Testing the second edge is only necessary, if the first test
does not lead to a decision. Again simple sign comparisons
are sufficient to determine the correct exit face.
Instead of using additional Pl¨ucker tests within the areas
A 0 − A 2 , it is preferable to apply three ray-bilinear patch
intersections [RPH04]. The correct exiting face is found if
the intersection (u, v) ∈ [0, 1]2 . As the bilinear patches deliver
appropriate parametric coordinates, they can be used directly
for interpolation.
Results. The rendering performance of a 512 × 512 view
port on a dual-core Opteron with 2 GHz are reported in
Table 2 for iso-surface rendering (iso), maximum-intensity
projection (MIP) and semi-transparent (semi) rendering.
Finding the initial face using a kd-tree requires 6–18% of
the total rendering time (initial). More importantly, the hybrid approach (hex-H) delivers not only a better quality, but
is in most cases faster compared to the pure Pl¨ucker approach (hex-P) when rendering hexahedral meshes. Marmitt
and Slusallek [MS06] therefore suggest to use this hybrid
approach for traversing curvilinear grids on a ray tracing

Figure 18: Unstructured (Bucky-ball) and curvilinear
(Combustion Chamber) data sets cannot only be rendered
into one scene. Even more important is that all primitives
interact with each other [MS06].
basis. With OpenRT [DWBS03] as ray tracing framework,
it is also easy to combine different primitives in one scene
and account for all their optical interactions (see Figure 18).
OpenRT also supports efficient ray tracing on PC clusters
and shared memory systems [MS05] providing almost linear
scalability in both cases.
4.3. Discussion
Despite the more complicated handling of irregular data sets,
the previous discussed algorithms provide interactive rendering of unstructured and curvilinear meshes. Even Parker’s
∗
[PPL 99] brute-force approach was able to render a large
volume with up to 16 fps on a larger shared memory system.
Marmitt’s and Slusallek’s [MS06] traversal in Pl¨ucker space
was optimized in many ways allowing for rendering midsized models at near-interactive rates on a single dual-core
CPU. Both methods benefit from the easy parallelization of
ray tracing. Due to the linear scaling with the number of
processors, future chip generations with four and eight cores
or more will probably enable interactive frame rates on a
consumer PC.
Ray tracing is also well known for its easy implementation
of advanced rendering effects. The demand for such features
is rising steadily, especially for visual artists, game developers and animators. There can be no doubt, that such advanced
rendering effects together with parallelization will increase
research activities in the near future.
5. Hardware Support
The discussion of previous ray tracing approaches for volume
rendering leads naturally to the conclusion, that parallelism

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1704

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

will be a key component of future ray tracing systems. Using
∗
shared-memory systems [PPL 99] or a cluster of consumer
∗
PCs [WFM 05], it is already possible to achieve interactive frame rates. In contrast to consumer graphics boards,
their flexibility allow for a far easier implementation even
for sophisticated rendering effects like global illumination
∗
[WFM 05].

In general, multi-core processors will enable a series of applications for future scientific visualization. One important
issue is the large amount of data produced by scientific simulations or devices. An example is the Richtmyer-Meshkov
instability in Figure 9 consisting of 270 time steps with 7.5
GB of data per time step.

The gap between processor speed and memory latency
is steadily increasing which is bad for applications like
volume rendering because they require an enormous memory
bandwidth. Even worse, the size of volumetric data sets is
rapidly increasing too. One possibility to reduce the memory
bandwidth requirements is parallelism. This parallelism can
be exploited on three different levels.

5.3. GPU Fragment Programs

5.1. Single Instruction Multiple Data
SIMD instructions allow to operate on several rays in parallel. Today, SIMD can operate on four 32 bit floating point or
integer data at the same time and will probably be extended
in the future. The implementation depends on the specific
family of processors, e.g. SSE for the ×86 family or AltiVec
for the PowerPC. If data coherency can be exploited, SIMD
significantly reduces the instruction count and thus improves
performance. Furthermore, SIMD instruction sets allow often to avoid costly conditionals using masking operations
[Kni00].
5.2. Multi-core CPUs
All major chip manufacturers are currently developing multicore processors which will certainly work in favour of ray
tracing. Preliminary measurements show that the scaling is
close to linear. Volume visualization usually requires a high
memory bandwidth to conquer the large amount of data. Such
processor architectures need to maintain or even improve the
memory connection bandwidth. Recently, IBM realized a
processor aiming for both high parallelism and high memory
bandwidth.
The Cell [IBM05] processor architecture consists of a single simplified PowerPC that is in charge of controlling eight
synergistic processor elements (SPEs) by a fast interconnection bus. Using this bus more than 20 GB/sec of data can be
transferred. Each of the SPEs acts as an independent vector
processor equipped with 256 KB local store and 128 SIMD
registers.
As SPEs are vector processors, they also support a SIMD
instruction set similar to AltiVec or SSE. As previously
demonstrated, ray traversal and computations within a cell,
e.g. ray iso-surface intersection tests can be significantly
∗
∗
speeded up [MKW 04, WFM 05]. An implementation was
already presented by Benthin et al. [BWSF06]. At least Wald
et al. [MS06] can be directly ported to the Cell, due to very
similar data structures used.

Finally, the latest graphic boards offer enough flexibility to
implement ray casting on the GPU. Especially fragment programs are suitable for such implementations which fully exploit the built-in parallelism of modern GPUs.
One of the first implementations of volume ray casting
using programmable graphics hardware was proposed by
Kr¨uger and Westermann [KW03] using fragment programs
for ray casting operations. The expensive invocation of the
fragment shader is avoided by using the early z-test, i.e. the
value in the z-buffer contains the accumulated opacity along
the ray.
In 2005, Stegmaier et al. [SSKE05] presented an implementation of a rectilinear volume ray caster on modern consumer graphics boards. A fragment program simulates ray
tracing of individual pixels through the volume. The volume
rendering integral is approximated by sampling at a finite
number of positions. After the initial intersection with the
bounding box is found, subsequent voxels are fetched from
∗
a 3D texture map. Hadwiger et al.’s [HSS 05] approach uses
∗
a multi-level grid introduced by Parker et al. [PPL 99] and
is therefore restricted to iso-surface rendering.
Even more parallelism can be exploited by clustering
several graphic boards together [MSE06]. The quality of
Stegmaier et al.’s approach can be enhanced as demonstrated
∗
by Strengert et al.’s [SKB 06] by implementing the KubelkaMunk approach for tracking reflectance and transmittance.
Figure 19 shows the achieved quality.
However, the programming model as well as the application interface of GPUs is still tedious to use. For example,
the number of loops in a fragment program is restricted to
256 at the current state. A ray caster has to use nested loops
for traversing the volume. Even then, secondary rays for
advanced shading can hardly be implemented, because no
recursion is available. First steps in porting ray tracing to
GPUs have already been taken [PBMH02], but it depends
upon future flexibility of graphic boards whether this will be
a sufficient basis for allowing for full-featured ray tracing.
6. Summary and Conclusions
In this report, we presented an overview of ray tracing techniques suitable for volumetric rendering. After briefly recapping, the main visualization tasks for volume visualization,
we grouped all major contributions in two categories, i.e.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

1705

health, and the simulation of physical processes are just a few
key words. Such tasks require flexible rendering approaches
that are readily provided using ray tracing.
Acknowledgements
We would like to thank all the people who contributed to this
report, either by comments and suggestions or by making
images available. In particular, we thank Holger Theisel for
discussion and Thomas Ertl, Stephen Parker, S¨oren Grimm
and G¨unther Knittel for providing images.

Figure 19: Simulation of the convection flow in the earths
crust including refractive effects [SKB∗ 06].

object-order and image-order approaches. Object-order algorithms and hybrids project the voxel data onto the image plane
and thus follow the basic rasterization principle. They can be
further classified into cell projection, splatting and texture
mapping. Important methods were summarized as alternative approaches including shear-warp and custom hardware.
With their increasing flexibility, high memory bandwidth,
and excellent floating point performance, graphic boards establish themselves as a serious competitor.
Ray tracing itself has also several advantages. Volumes,
whether they are regular, curvilinear or even unstructured,
are just another primitive for the ray tracer and hence
can be easily combined even with polygonal surfaces in
a simple plug ‘n’ play fashion. This was demonstrated by
∗
∗
Parker et al. [PPL 99], Wald [WFM 05] and Marmitt and
Slusallek [MS06]. Additionally, shadows, reflection, refraction or translucency interact with each other without additional effort. Even advanced shaders, e.g. for global illumination must be implemented once and subsequently work for
all primitives. This is not only useful for computer games and
artists, but also for simulation practices, e.g. virtual surgery
applications. The flexibility which is necessary for supporting all of these effects is today only available with software
ray tracing systems. Nevertheless, recent developments in
custom ray tracing hardware for surface models [WSS05]
have shown that the programmable ray tracing hardware will
soon become available and can be then extended for volume
rendering.
The lack of real-time rendering performance especially for
large data sets can be reduced by exploiting coherence and
parallelism. Right now, it is not clear which hardware architecture will be best suited in the future for volume rendering
because GPUs and custom hardware become more flexible,
and CPUs more parallel.
Scientific visualization becomes more and more important in major research activities today. Natural disasters, understanding weather and climate changes, improving human

References
[ASK92] AVILA R. S., SOBIERAJSKI L. M., KAUFMAN A. E.:
Towards a comprehensive volume visualization system. In
VIS’92: Proceedings of the 3th IEEE Visualization (1992),
pp. 13–20.
[AW87] AMANATIDES J., WOO A.: A fast voxel traversal algorithm for ray tracing. In EG’87: Proceedings of Eurographics (1987), pp. 3–10.
[BKS97] BUNYK P., KAUFMAN A. E., SILVA C. T.: Simple, fast, and robust ray casting of irregular grids. In
Dagstuhl’97: Scientific Visualization (1997), pp. 30–36.
[BPS96] BAJAJ C. L., PASCUCCI V., SCHIKORE D. R.: Fast
Isocontouring for Improved Interactivity. In VVS’96: Proceedings of the 1996 Symposium on Volume visualization
(1996), pp. 39–47.
[BWSF06] BENTHIN C., WALD I., SCHERBAUM M., FRIEDRICH
H.: Ray tracing on the CELL processor. In RT’06: Proceedings of the 2006 IEEE Symposium on Interactive Ray
Tracing (2006), pp. 15–23.
[CCF94] CABRAL B., CAM N., FORAN J.: Accelerated volume rendering and tomographic reconstruction using texture mapping hardware. In VVS’94: Proceedings of the
IEEE Symposium on Volume Visualization (1994), pp. 91–
98.
[Cho02] CHOI S.: The delaunay tetrahedralization from delaunay trangulated surfaces. In SCG’02: Proceedings of
the 8th Symposium on Computational Geometry (2002),
pp. 145–150.
[CMM∗ 97] CIGNONI P., MARINO P., MONTANI C., PUPPO E.,
SCOPIGNO R.: Speeding up isosurface extraction using interval trees. IEEE Transactions on Visualization and Computer Graphics 3, 2 (1997), 158–170.
[CRZP04] CHEN W., REN L., ZWICKER M., PFISTER H.:
Hardware-accelerated adaptive EWA volume splatting.
In VIS’04: Proceedings of the 15th IEEE Visualization
(2004), pp. 67–74.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1706

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

[CS94] COHEN D., SHEFFNER Z.: Promity clouds – an acceleration technique for 3D grid traversal. The Visual Computer, 11 (1994), 27–38.
[DH92] DANSKIN J., HANRAHAN P.: Fast algorithms for volume ray tracing. In VVS’92: Proceedings of the Workshop
on Volume Visualization (1992), pp. 91–98.

[GLBH07] GROS¨ M., LOJEWSKI C., BERTRAM M., HaGen H.: Fast implicit kd-trees: Accelerated isosurface ray tracing and maximum intensity projection
for large scalar fields. In CGIM’07: Proceedings of
Computer Graphics and Imaging (2007), pp. 67–
74.

[DHS01] DUDA R. O., HART P. E., STORK D. G.: Pattern
Classification. Wiley-Interscience, 2001.

[Hav01] HAVRAN V.: Heuristic Ray Shooting Algorithms.
PhD thesis, Faculty of Electrical Engineering, Czech
Technical University in Prague, 2001.

[DPH∗03] DEMARLE D. E., PARKER S., HARTNER M., GRIBBLE
C., HANSEN C.: Distributed interactive ray tracing for
large volume visualization. In PVG’03: Proceedings of
the IEEE Symposium on Parallel and Large-Data Visualization and Graphics (2003), pp. 87–94.

[Hec89] HECKBERT P.: Fundamentals of Texture Mapping
and Image Warping. Master’s thesis, Department of Electrical Engineering and Computer Science, University of
California, 1989.

[DWBS03] DIETRICH A., WALD I., BENTHIN C., SLUSALLEK
P.: The OpenRT Application programming interface –
towards a common API for interactive ray tracing. In
OpenSG’03: Proceedings of the OpenSG Symposium
(2003), pp. 23–31.
[EHK∗06] ENGEL K., HADWIGER M., KNISS J. M., RezkSalama C., WEISKOPF D.: Real-Time Volume Graphics. A.
K. Peters, Ltd., 2006.
[EKE01] ENGEL K., KRAUS M., ERTL T.: High-quality preintegrated volume rendering using hardware-accelerated
pixel shading. In HWWS’01: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Workshop on Graphics hardware (2001), pp. 9–16.
[Eri97] ERICKSON J.: Pluecker coordinates. Ray Tracing
News 10, 3 (1997).
[Fr"u94] FRU¨ HAUF T.: Raycasting of nonregularly structured volume data. In EG’94: Proceedings of Eurographics (1994), pp. 295–303.
[FS97] FREUND J., SLOAN K.: Accelated volume rendering
using homogeneous regions encoding. In VIS’97: Proceedings of the 8th IEEE Visualization (1997), pp. 191–
196.
[Gal91] GALLAGHER R. S.: Span filtering: An optimization
scheme for volume visualization of large finite element
models. In VIS’91: Proceedings of the 2nd Conference on
Visualization’91 (1991), pp. 68–75.
[Gar90] GARRITY M. P.: Raytracing irregular volume data.
In VVS’90: Proceedings of the Workshop on Volume Visualization (1990), pp. 35–40.
[GBKG04] GRIMM S., BRUCKNER S., KANITSAR A., GRO¨ LLER
M. E.: Memory efficient acceleration structures and techniques for CPU-based volume ray-casting of large data. In
VOLVIS’04: Proceedings IEEE/SIGGRAPH Symposium
on Volume Visualization and Graphics (2004), pp. 1–8.

[HJ04] HANSEN C., JOHNSON C. R.: The Visualization Handbook. Elsevier, 2004.
[HK98] HONG L., KAUFMAN A.: Accelerated ray-casting for
curvilinear volumes. In VIS’98: Proceedings of the 9th
IEEE Visualization (1998), pp. 247–253.
[HK99] HONG L., KAUFMAN A. E.: Fast projection-based
ray-casting algorithm for rendering curvilinear volumes.
IEEE Transactions on Visualization and Computer Graphics 5, 4 (1999), 322–332.
[HQK05] HONG W., QIU F., KAUFMAN A.: GPU-based
object-order ray-casting for large datasets. In VG’05: Proceedings of the International Workshop on Volume Graphics (2005), pp. 177–186.
[HSS∗ 05] HADWIGER M., SIGG C., SCHARSACH H., B¨UHLER
K., GROSS M.: Real-timeray-casting and advanced shading
of discrete isosurfaces. In EG’05: Proceedings of Eurographics (2005), pp. 303–312.
[IBM05] The cell project at IBM research. http:/www.research.ibm.com/cell/, 2005.
[IK95] ITOH T., KOYAMADA K.: Automatic isosurface propagation using an extrema graph and sorted boundary cell
lists. IEEE Transactions on Visualization and Computer
Graphics 1, 4 (1995), 319–327.
[KK99] KREEGER K., KAUFMAN A.: Hybrid volume and
polygon rendering with cube hardware. In HWWS’99:
Proceedings of the ACM SIG-GRAPH/EUROGRAPHICS
Workshop on Graphics hardware (1999), pp. 15–
24.
[KN91] KOYAMADA K., NISHIO T.: Volume visualization of
3D finite element method results. IBM Journal of Research
and Development 35, 1–2 (1991), 12–25.
[Kni00] KNITTEL G.: The ULTRAVIS system. In VVS’00:
Proceedings of the IEEE Symposium on Volume Visualization (2000), pp. 71–79.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

[KS97] KNITTEL G., STRASSER W.: VIZARD – visualization accelerator for realtime display. In HWWS’97:
Proceedings of the ACM SIGGRAPH/EUROGRAPHICS
Conference on Graphics Hardware (1997), pp. 139–
146.

1707

IEEE Symposium on Volume Visualization (2000), pp. 81–
90.
[MHG00] MROZ L., HAUSER H., GRO¨ LLER E.: Interactive
high-quality maximum intensity projection. Computer
Graphipcs Forum 19, 3 (2000), 341–350.

[KW03] KRU¨ GER J., WESTERMANN R.: Acceleration techniques for GPU-based volume rendering. In VIS’03: Proceedings of the 14th IEEE Visualization (2003), pp. 287–
292.

[MJC02] MORA B., JESSEL J. P., CAUBET R.: A new objectorder ray-casting algorithm. In VIS’02: Proceedings of the
13th IEEE Visualization (2002), pp. 203–210.

[LCN98] LICHTENBELT B., CRANE R., NAQVI S.: Introduction
to Volume Rendering. Person Education, 1998.

[MKG00] MROZ L., K¨ONIG A., GRO¨ LLER E.: Maximum intensity projection at warp speed. Computers and Graphics
24, 3 (2000), 343–352.

[Lev90] LEVOY M.: Efficient ray tracing for volume data.
ACM Transactions on Graphics 9, 3 (1990), 245–261.
[LK04] LAKARE S., KAUFMAN A.: Light weight space leaping using ray coherence. In VIS’04: Proceedings of the
15th IEEE Visualization (2004), pp. 19–26.
[LL94] LACROUTE P., LEVOY M.: Fast volume rendering using a shear-warp factorization of the viewing transformation. In SIGGRAPH’94: Proceedings of the 21st annual
Conference on Computer Graphics and Interactive Techniques (1994), pp. 451–458.
[Luc92] LUCAS B.: A scientific visualization renderer. In
VIS’92: Proceedings of the 3rd IEEE Visualization (1992),
pp. 227–234.
[LYTM06] LAUTERBACH C., YOON S.-E., TUFT D., MANOCHA
D.: Interactive ray tracing of dynamic scenes using BVHs.
In RT’06: Proceedings of the 2006 IEEE Symposium on
Interactive Ray Tracing (2006), pp. 39–46.
[Ma95] MA K.-L.: Parallel volume ray-casting for
unstructured-grid data on distributed-memory architectures. In PRS’95: Proceedings of the IEEE Symposium
on Parallel rendering (1995), pp. 23–30.
[MC98] M¨ULLER K., CRAWFIS R.: Eliminating popping artifacts in sheet buffer-based splatting. In VIS’98: Proceedings of the 9th IEEE Visualization (1998), pp. 239–245.
[ME05] MORA B., EBERT D. S.: Low-complexity maximum
intensity projection. ACM Transactions on Graphics 24,
4 (2005), 1392–1416.

[MKW∗ 02] MEISSNER M., KANUS U., WETEKAM G., HIRCHE
J., EHLERT A., STRASSER W., DOGGETT M., FORTHMANN
P., PROKSA R.: VIZARD II: A reconfigurable interactive
volume rendering system. In HWWS’02: Proceedings of
the ACM SIGGRAPH/EUROGRAPHICS Conference on
Graphics Hardware (2002), pp. 137–146.
[MKW∗ 04] MARMITT G., KLEER A., WALD I., FRIEDRICH
H., SLUSALLEK P.: Fast and accurate ray-voxel intersection techniques for iso-surface ray tracing. In VMV’04:
Proceedings of 9th International Fall Workshop – Vision,
Modeling, and Visualization (2004), pp. 429–435.
[MMMY97] M¨OLLER T., MACHIRAJU R., MUELLER K., YAGEL
R.: A comparison of normal estimation schemes. In
VIS’97: Proceedings of the 7th IEEE Visualization (1997),
pp. 19–26.
[MS05] MARMITT G., SLUSALLEK P.: Fast Ray Traversal of
Unstructured Volume Data using Plucker Tests. Tech. rep.,
CG Lab, Saarland University, 2005.
[MS06] MARMITT G., SLUSALLEK P.: Fast ray traversal of
tetrahedral and hexahedral meshes for direct volume rendering. In EUROVIS’06: Proceedings of the EG/IEEE
Symposium on Data Visualisation (2006), pp. 131–138.
[MSE06] M¨ULLER C., STRENGERT M., ERTL T.: Optimized volume raycasting for graphics-hardwarebased cluster systems. In EGPGVV’06: Proceedings of Euro-graphics Symposium on Parallel Graphics and Visualization (2006), pp. 59–
66.

[MFS05] MARMITT G., FRIEDRICH H., SLUSALLEK P.: Recent
advancements in ray-tracing based volume rendering techniques. In VMV’05: Proceedings of 10th International Fall
Workshop – Vision, Modeling, and Visualization (2005),
pp. 131–138.

[MY96] M¨ULLER K., YAGEL R.: Fast perspective volume
rendering with splatting by utilizing a ray-driven approach. In VIS’96: Proceedings of the 7th conference on
Visualization (1996), pp. 65–72.

[MHB∗ 00] MEISSNER M., HUANG J., BARTZ D., MUELLER
K., CRAWFIS R.: A practical evaluation of popular volume rendering algorithms. In VVS’00: Proceedings of the

[NMHW02] NEUBAUER A., MROZ L., HAUSER H.,
WEGENKITTL R.: Cell-based first-hit ray casting. In EUROVIS’02: Proceedings of the EG/IEEE Symposium on
Data Visualisation (2002), pp. 77–86.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1708

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

[PBMH02] PURCELL T. J., BUCK I., MARK W. R.,
HANRAHAN P.: Ray tracing on programmable graphics hardware. In SIGGRAPH’02: Proceedings of
the 29th annual Conference on Computer Graphics and Interactive Techniques (2002), pp. 703–
712.
[PH04] PHARR M., HUMPHREYS G.: Physically Based Rendering : From Theory to Implementation. Morgan Kaufman, 2004.
[PHK∗ 99] PFISTER H., HARDENBERGH J., KNITTEL J., LAUER
H., SEILER L.: The VolumePro real-time ray-casting system. In SIGGRAPH’99: Proceedings of the 26th annual
Conference on Computer Graphics and Interactive Techniques (1999), pp. 251–260.
∗

[PHK 03] PEKAR V., HEMPEL D. J., KIEFER G., BUSCH M.,
WEESE J.: Efficient visualization of large medical image
datasets on standard PC hardware. In VISSYM’03: Proceedings of the Symposium on Data Vi-sualisation 2003
(2003), pp. 135–140.
[PMS∗ 99] PARKER S., MARTIN W., SLOAN P.-P. J., SHIRLEY P.,
SMITS B., HANSEN C.: Interactive ray tracing. In SI3D’99:
Proceedings of the ACM Symposium on Interactive 3D
Graphics (1999), pp. 119–126.
[PPL∗ 99] PARKER S., PARKER M., LIVNAT Y., SLOAN P.-P.,
HANSEN C., SHIRLEY P.: Interactive ray tracing for volume visualization. IEEE Transactions on Visualization
and Computer Graphics 5, 3 (1999), 223–250.
[PT03] PLATIS N., THEOHARIS T.: Fast ray-tetrahedron intersection using pl¨ucker coordinates. Journal of Graphics
Tools 8, 4 (2003), 37–48.
[RGW∗ 03] R¨OTTGER S., GUTHE S., WEISKOPF D., ERTL T.,
STRASSER W.: Smart hardware-accelerated volume rendering. In VISSYM’03: Proceedings of EG/IEEE TCVG Symposium on Visualization (2003), pp. 231–238.
[RKE00] R¨OTTGER S., KRAUS M., ERTL T.: Hardwareaccelerated volume and isosurface rendering based on
cell-projection. In VIS’00: Proceedings of the 11th IEEE
Visualization (2000), pp. 109–116.
[RPH04] RAMSEY S., POTTER K., HANSEN C.: Ray bilinear
patch intersections. Journal of Graphics Tools 9, 3 (2004),
41–47.
[RSH05] RESHETOV A., SOUPIKOV A., HURLEY J.: Multi-level
ray tracing algorithm. In SIGGRAPH’05: Proceedings of
the 32th Annual Conference on Computer Graphics and
Interactive Techniques (2005), pp. 1176–1185.
[RZNS04] R¨OSSL C., ZEILFELDER F., N¨URNBERGER G., SEIDEL
H.-P.: Reconstruction of volume data with quadratic super

splines. IEEE Transactions on Visualization and Computer Graphics 4, 10 (2004), 397–409.
[Sch98] SCHWARZE J.: Cubic and quartic roots. In Graphics
Gems. Academic Press, 1998, pp. 404–407.
[SGS95] SAKAS G., GRIMM M., SAVOPOULOS A.: Optimized
maximum intensity projection. In EGRW’95: Proceedings
of the 5th Eurographics Workshop on Rendering (1995),
pp. 55–63.
[SK00] SRAMEK M., KAUFMAN A.: Fast ray-tracing of rectilinear volume data using distance transforms. IEEE Transactions on Visualization and Computer Graphics 6, 3
(2000), 236–252.
M.,
KLEIN
T.,
BOTCHEN
[SKB∗ 06] STRENGERT
R., STEGMAIER S., CHEN M., ERTL T.: Spectral volume rendering using GPU-based raycasting. The Visual Computer 22, 8 (2006), 550–
561.
[SM00] SCHUMANN H., M¨ULLER W.: VisualisierungGrundlagen und Allgemeine Methoden. Springer,
2000.
[SM02] SWEENEY J., M¨ULLER K.: Shear-warp deluxe: The
shear-warp algorithm revisited. In VISSYM’02: Proceedings of EG/IEEE TCVG Symposium on Visualization
(2002), pp. 95–104.
[Sra94] SRAMEK M.: Fast surface rendering from raster data
by voxel traversal using chessboard distance. In VIS’94:
Proceedings of the conference on Visualization’94 (1994),
pp. 188–195.
[Sra96] SRAMEK M.: Fast ray-tracing of rectilinear volume
data. In VESV’94: Virtual Environments and Scientific Visualization’96 (1996), pp. 201–210.
[SSKE05] STEGMAIER S., STRENGERT M., KLEIN T., ERTL T.:
A simple and flexible volume rendering framework for
graphics-hardware-based raycasting. In VG’05: Proceedings of the International Workshop on Volume Graphics
(2005), pp. 187–195.
[ST90] SHIRLEY P., TUCHMAN A.: A polygonal approximation to direct scalar volume rendering. In VVS’90: Proceedings of the 1990 Workshop on Volume Visualization
(1990), pp. 63–70.
[The01] THEISEL H.: CAGD and Scientific Visualization.
PhD thesis, Faculty of Electrical Engineering, Rostock
University, 2001, Habilitionsschrift.
[UO93] UDUPA J. K., ODHNER D.: Shell rendering. IEEE
Computer Graphics and Applications 13, 6 (1993), 58–
67.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

G. Marmitt et al. / Efficient CPU-based Volume Ray Tracing Techniques

[Wal04] WALD I.: Realtime Ray Tracing and Interactive
Global Illumination. PhD thesis, Computer Graphics
Group, Saarland University, 2004.
[WBLS03] WU Y., BHATIA V., LAUER H., SEILER L.: Shearimage order ray casting volume rendering. In SI3D’03:
Proceedings of the Symposium on Interactive 3D graphics
(2003), pp. 152–162.
[WBS02] WALD I., BENTHIN C., SLUSALLEK P.: OpenRT –
A Flexible and Scalable Rendering Engine for Interactive
3D Graphics. Tech. rep., Computer Graphics Lab, Saarland University, 2002.

1709

plicit KD-Trees. IEEE Transactions on Visualization and
Computer Graphics 11, 5 (2005), 562–573.
[WG92] WILHELMS J., GELDER A. V.: Octrees for faster isosurface generation. ACM Transactions on Graphics 11, 3
(1992), 201–227.
[WGTG96] WILHELMS J., GELDER A. V., TARANTINO P.,
GIBBS J.: Hierarchical and parallelizable direct volume
rendering for irregular and multiple grids. In VIS’96: Proceedings of the 7th IEEE Visualization (1996), pp. 57–
64.

[WBS07] WALD I., BOULOS S., SHIRLEY P.: Ray tracing deformable scenes using dynamic bounding volume hierarchies. ACM Transactions on Graphics 26, 1 (2007), 485–
493.

[WIK∗ 06] WALD I., IZE T., KENSLER A., KNOLL A., PARKER
S. G.: Ray tracing animated scenes using coherent grid
traversal. In SIGGRAPH’06: Proceedings of the 33th Annual Conference on Computer Graphics and Interactive
Techniques (2006), pp. 485–493.

[WCA∗ 90] WILHELMS J., CHALLINGER J., ALPER N.,
RAMAMOORTHY S., VAZIRI A.: Direct volume rendering of
curvilinearvolumes. In VVS’90: Proceedings of the Workshop on Volume Visualization (1990), pp. 41–47.

[WSB01] WALD I., SLUSALLEK P., BENTHIN C.: Interactive
distributed ray tracing of highly complex models. In
EGRW’01: Proceedings of the 12th Eurographics Workshop on Rendering (2001), pp. 274–285.

[WDS04] WALD I., DIETRICH A., SLUSALLEK P.: An interactive out-of-core rendering framework for visualizing massively complex models. In EGRW’04: Proceedings of the
15th Eurographics Workshop on Rendering (2004), pp.
81–92.

[WSS05] WOOP S., SCHMITTLER J., SLUSALLEK P.: RPU: A
programmable ray processing unit for realtime ray tracing.
In SIGGRAPH’05: Proceedings of the 32th Annual Conference on Computer Graphics and Interactive Techniques
(2005), pp. 434–444.

[Wes90] WESTOVER L.: Footprint evaluation for volume
rendering. In SIGGRAPH’90: Proceedings of the 17th annual Conference on Computer Graphics and Interactive
Techniques (1990), pp. 367–376.

[Yag94] YAGEL R.: Shell accelerated volume rendering of
transparent regions. The Visual Computer 10, 1 (1994),
53–61.

[WFM∗ 05] WALD I., FRIEDRICH H., MARMITT G., SLUSALLEK
P., SEIDEL H.-P.: Faster isosurface ray tracing using im-

[ZPvBG01] ZWICKER M., PFISTER H., VAN BAAR J., GROSS
M.: EWA volume splatting. In VIS’01: Proceedings of the
12th IEEE Visualization (2001), pp. 29–36.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

