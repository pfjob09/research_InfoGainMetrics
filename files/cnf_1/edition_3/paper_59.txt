How Interactive Visualization Can Assist Investigative Analysis:
Views and Perspectives from Domain Experts
John Stasko1
Georgia Tech
(Panel Organizer)

Sarah Cohen2
Duke University

ABSTRACT
Interactive visualization could become an essential tool in the
work of investigative analysts. Visualization could help analysts
to explore large collections of data and documents, supporting the
analysts’ investigative sense-making processes. This panel
gathers recognized leaders from three important domains,
investigative reporting, biosciences (genomics), and intelligence
analysis, that all include a fundamental investigative analysis
component. The panelists will provide a glimpse into their
worlds, describing and illustrating the data they examine, the
goals and methods of their analysts, and the culture of their
respective professions. In particular, the panelists will explore
how visualization could potentially benefit investigators from
their domain and they will provide guidance for visualization
researchers seeking to collaborate with their colleagues.
KEYWORDS: investigative analysis, interactive visualization,
visual analytics, investigative reporting, genomics, intelligence
analysis.
INDEX TERMS: H.5.2 [Information Systems]: Information
Interfaces and Presentation—User Interfaces
1

INTRODUCTION

One of the chief applications of visualization has been to browse
data and to perform exploratory analysis. Effective visualizations
present data in a form that allows people to rapidly inspect it
while seeking to identify items of interest. In some cases, the
person examining the data may be looking for particular items or
patterns. Frequently, the person examining the data is proceeding
without any explicit intent – they simply hope to find something
“interesting.”
Because of this utility, visualization systems seem like they
would be effective tools in the arsenal of investigative analysts
who seek to discover unknown truths and to identify new
knowledge. Investigative analysts typically work with large
collections of data; they must both gather the data in some useful
form and conduct an analysis of it. The data involved in
investigative analysis is typically quite varied, and it often
includes collections of unstructured and semi-structured text
documents.
While visualization has begun to be effectively applied in
investigative analysis (for instance, an entire session of the 2007
Visual Analytics Symposium was devoted to that topic [1,2,3]),
this application is still in its infancy and much remains to be done.
1

stasko@cc.gatech.edu
sarah.cohen@duke.edu
3
Larry.Hunter@ucdenver.edu
4
JoeP@i2.co.uk
2

IEEE Symposium on Visual Analytics Science and Technology
October 12 - 13, Atlantic City, New Jersey, USA
978-1-4244-5283-5/09/$25.00 ©2009 IEEE

Lawrence Hunter3
Univ. of Colorado School of Medicine

Joe Parry4
i2 Limited

Visualization researchers are now increasingly drawn to
investigative analysis as an area that is full of interesting problems
and is ripe for new work.
Unfortunately, however, approaching a new investigative
domain can be a daunting challenge for a visualization researcher.
Clearly, a researcher will be better able to make a positive impact
in a new domain if he or she has a strong understanding of the
goals, tasks, methods, data, and culture of that domain. The
importance of user-centered design of software systems is a
fundamental principle that one learns in any human-computer
interaction course and it applies here as well.
While visualization researchers may seek a strong level of user
and task understanding, many factors often hinder them from
doing so. First, a domain simply may contain a substantial
amount of specialized knowledge in an area in which the
visualization researcher has no training.
Additionally,
investigative analysts are often extremely busy people themselves.
It may be difficult to gain adequate interaction time in order to
understand the person’s challenges and data. Furthermore, data
from the domain may be proprietary, sensitive, or simply may be
difficult to acquire.
For these reasons and many others, it is often difficult for
visualization researchers to engage a new investigative domain
and its analysts at a deep enough level to make an effective
contribution. While the limited time of this panel clearly will not
provide an in-depth understanding of the domains of its members,
we hope that it at least provides a good introduction to the
objectives and challenges faced in these domains, which we feel
are some of the most compelling and that could tremendously
benefit from visualization. We also hope that the panel provides a
sense of the culture of each field represented, highlighting what is
important to investigators there and perhaps even debunking some
myths that visualization researchers may harbor about analysts.
The panel includes three distinguished leaders in their fields
who have a tremendous wealth of experience conducting
investigations and supporting others who are conducting
investigations. Sarah Cohen is a Professor of Journalism and
Public Policy at Duke University after having been a Pulitzer
Prize winning investigative reporter and editor at the Washington
Post. Lawrence Hunter is the Director of the Computational
Bioscience Program and of the Center for Computational
Pharmacology at the University of Colorado School of Medicine,
after having spent 10 years at the National Institute of Health and
the National Cancer Institute. Joe Parry is Research Director at i2
Limited which provides the industry-leading Analyst’s Notebook
software system for law enforcement, intelligence, military, and
commercial organizations.
The panelists will address many different aspects of their own
investigative work and the work of their colleagues and clients.
These aspects will include:
Purpose – What are the goals of investigative analysts in their
area? What types of information do analysts seek to uncover or
identify? What is considered a successful investigation?
Data – What types of data do they work with? Where does
their data come from and how does one access it? In what

279

formats does the data occur? Are there domain-specific tools that
help with data acquisition and analysis?
Methodology – How are investigations conducted in their
field? What tools do analysts presently use? What the specific
challenges analysts in their domain face? What processes could
most benefit from interactive visualization tools?
Collaboration – Do analysts in their area mostly work alone or
do they work together in larger groups? What are the barriers to
collaboration? Do analysts work together synchronously or
asynchronously and are they collocated or at different locations?
Output – How do analysts communicate the results of their
investigations to others? How do they connect conclusions to the
data influencing those conclusions? Do they presently use
visualization to communicate their findings?
2

POSITION STATEMENTS

2.1
Sarah Cohen
Few people think of March 19, 2008 as a date they will remember.
But it was a milestone for those of us covering the Democratic
primary campaign for president. In response to a court ruling,
Hillary Clinton released her daily schedule as First Lady in a
collection of 11,000 pages of heavily censored page images.
News organizations rushed to convert them into searchable text.
Reporters scrambled to split up duties – some seeking hints of
Clinton’s reaction to the Monica Lewinsky episode and the
subsequent impeachment of her husband and others looking for
records on the secretive health care reform effort she led five
years earlier. They wrote stories for their websites and newspapers
and posted the documents for their readers.
Then they stopped. Journalists lacked a tool to extract and map
the names, places, organizations and dates buried in the
documents, so there was little more to do.
Without a simple tool to make a drill-down chronology of her
time in the White House that could be searched, sorted, filtered
and color-coded, we couldn’t see whether there were holes we
should investigate. Without a tool to draw connections among her
visitors and identify the various networks among them, we
couldn’t see whether the networks were reappearing on the
campaign or among donors to Bill Clinton’s charitable
foundation.
That story, at least, had one source – the documents. Most indepth reporting relies on information culled from dozens of
sources: censored documents and databases obtained through
Freedom of Information Act requests, court proceedings and
congressional hearings, regular inspections and audits,
government invoices and contracts, emails and calendars (usually
redacted and provided on paper, not electronically). Other sources
include interviews, documents from confidential sources and less
opaque data feeds and Internet searches. Little of the information
comes neatly packaged. Much of it remains available only on
paper or, at best, in PDF form. It is sometimes provided in a form
unabashedly designed to thwart serious analysis.
Visualization for these kinds of stories is equally frustrating.
For Obama’s first 100 days, no timeline tool was adequate to help
us analyze a hand-entered database of about 400 activities. For his
appointees, no network tool helped us identify the common
threads until we searched unrelated databases, coded connections
and standardized affiliations. And for a story on farm disaster
payments, small-multiples and overlays of drought map images
took more than a week to create, often copying and pasting by
hand.
Journalists’ goals don’t change with technology. For many, it
remains quite simple: tell the stories that powerful institutions and
people would prefer to keep hidden. Tools to more efficiently and
effectively combine and understand the sources – including

280

visualization tools – would reduce the barriers to in-depth
reporting and help mitigate the effects of grave financial pressures
on public service journalism.
Bio: Sarah Cohen became the Knight Professor of the Practice of
Journalism and Public Policy at Duke University in July. Prior to
the appointment, she worked for 11 years as an editor and reporter
for The Washington Post, primarily on national and local
investigative projects. She shared in most national investigative
reporting prizes for her work on a variety of stories, including the
Pulitzer Prize, the Goldsmith Award, the Robert F. Kennedy
journalism prize, the Selden Ring Award and the Investigative
Reporters and Editors gold medal. Before the Post, Cohen worked
as the training director for Investigative Reporters and Editors, a
grass-roots training organization that works to improve in-depth
journalism, and as a reporter at the St. Petersburg Times and The
Tampa Tribune. She earned an MA from the University of
Maryland in public affairs reporting and a BA from the University
of North Carolina-Chapel Hill in economics.
2.2
Lawrence Hunter
The era of genomic medicine has begun. Ongoing exponential
decreases in the price of genotyping have opened the floodgates of
genome-scale information relevant to treating patients with
common diseases, such as diabetes, heart disease and cancer.
However, the torrential pace of data generation, coupled with
substantial growth in significant discoveries and the breakdown of
traditional disciplinary boundaries has made it increasingly
difficult for biomedical scientists to take advantage of all the
knowledge relevant to their research. Most phenomena of interest
to biomedical research involve the concerted activity of hundreds
of genes and their products, and contemporary biotechnology
makes it increasingly possible to identify these entities and
characterize their interactions. Exploiting these results requires
putting them in the context of relevant existing knowledge of
biology, which is embodied in the peer-reviewed scientific
literature and in the gene-centric databases maintained by the
National Institutes of Health and similar entities.
A typical analysis problem begins with the use of a genomescale technology that identifies dozens to hundreds (or, in the case
of cancer or development, sometimes thousands) of genes and
gene products relevant to a disease or other phenomenon of
interest. Many genomic instruments can quantify the levels of
each gene (e.g. over time or in response to dosage of a drug), and
therefore the correlational structure among them. Generating
explanations of the roles of all of these genes and relationships in
light of what is already known about them is a critical task. There
are more than 18 million peer reviewed biomedical journal
articles indexed in the National Library of Medicine’s PubMed
database, with more than 800,000 new articles added in the last
year. Databases provide extensive additional information; one
particularly important resource is the systematic annotation of
genes to the extensive Gene Ontology, which provides controlled
vocabulary and formal relations among tens of thousands of
descriptors of molecular function, biological processes, and
cellular locations.
Not all information about genes is completely reliable, and
scientists trying to navigate this space need the ability to both
skim brief summaries and to find the details necessary to verify
and validate summary statements in the context of their particular
experimental data. Spatiotemporal contexts, such as
developmental stage or anatomical locations are important, as are
more specifically biological dimensions, such as evolutionary or
metabolic relationships. Ultimately, the goal of this user

community is not only to understand the data arising from their
experiments, but also to generate novel, significant hypotheses
based on inferences from data that was not previously known.
A variety of approaches to addressing these challenges are in
use in the biomedical research community, ranging from text
mining and information retrieval to statistical inference and visual
exploration. However, the rapid growth of genomic information,
and the pressing needs for improvements in medical care suggest
that further improvements could be of great importance.
Bio: Dr. Lawrence Hunter is the Director of the Computational
Bioscience Program and of the Center for Computational
Pharmacology at the University of Colorado School of Medicine,
and a Professor in the departments of Pharmacology, Computer
Science (Boulder), and Preventive Medicine and Biometrics. He
received his Ph.D. in computer science from Yale University in
1989, and then spent more than 10 years at the National Institutes
of Health, ending as the Chief of the Molecular Statistics and
Bioinformatics Section at the National Cancer Institute. He
inaugurated two of the most important academic bioinformatics
conferences, ISMB and PSB, and was the founding President of
the International Society for Computational Biology. Dr. Hunter's
research interests span a wide range of areas, from cognitive
science to rational drug design. His primary focus recently has
been the integration of natural language processing, knowledge
representation and machine learning techniques and their
application to interpreting data generated by high throughput
molecular biology.
2.3
Joe Parry
i2 has 20 years of experience in producing visual analysis
software for analysts in several domains: law enforcement (state
and local level), intelligence analysts at a federal level, military
intelligence, and commercial – typically fraud. Across these
domains there is considerable diversity. Military analysts have the
strongest time pressures; law enforcement analysts have strong
requirements in terms of evidential reporting; and intelligence
analysts typically have more data readily to hand. However, they
also have much in common. For example, each domain has their
own grading system for assessing the reliability, classification and
disclosure rules of pieces of information.
A common theme across many cases is the discovery of
identifiers of interest: names, addresses, phone numbers, email
addresses, bank account numbers, amongst others. Patterns of
activity are deduced, connections between individuals, the timing
and the location of key events like sightings, phone calls, etc., can
lead to the generation of hypotheses/lines of inquiry which help
drive the direction of the investigation as a whole. Relationship
link diagrams, timelines and maps are the three most commonly
expressed visualization needs.
Collaboration has been emphasized in recent years. In terms of
the typology presented in Illuminating the Path [4], we find that
typically collaboration is asynchronous remote (different time,
different place), or synchronous local (same place, same time). In
some shift patterns one sees continuous work done by a revolving
team (same place, different time), but that is relatively
uncommon. Asynchronous remote collaboration is typically
achieved by emailing files. This “baton-passing” approach shares
a lot with the way that documents are authored in many
professions. The key advantage of this approach is that
information can be exchanged freely across organizational
firewalls: disadvantages are that there is no definitive version of
the information and multiple copies of the document can cause
confusion.

In the case of (same place, same time) collaboration, this is
done using a shared screen at a desk, within meeting rooms
equipped with projectors and/or interactive whiteboards, or often
done away from the computer entirely in a relatively informal
context. In the latter case, printouts of visualizations are often
pointed at and scribbled on. Printing is much more important than
may be first realized. As cases get complex, it is common to print
out the current known state of the case and pin it up on a wall for
the investigation team to see and draw on. Evidential and other
procedural requirements, especially within the law enforcement
domain, mean that visualizations must fit with a “paper trail” of
documents.
Analysts have a very strong sense of ownership over the
products they produce, and visualizations are no exception.
Analysts raise concerns that their visualizations may be
misinterpreted when viewed outside of the context of the task at
hand. To ameliorate this, and also to facilitate basic reporting
needs, visualizations are very commonly embedded as pictures
within textual reports. In this state, they lose their interactivity and
the consumer cannot “drill-down” on the information represented.
Such images are often produced in a separate “production” stage
after the analysis has been done. At the reporting stage, it is very
common for a visualization to have to fit onto an A4/Letter size
piece of paper! Visual Analytic tools in general tend to neglect
the reporting aspects of the job.
For the future, many of the general challenges facing are
practical ones. Tool support for versioning, auditing data access,
document searching and collaboration could be better. Tools need
to be easily deployable by IT staff if they have any hope of
adoption. The amount of available data is growing, but perhaps
more importantly there are now more and more data sources that
need to be checked during an investigation. Any help in getting
data saves the analyst valuable time. Lastly, improved
summarization/aggregation techniques for large data sets would
be very welcome.
Bio: Joe Parry is Research Director at i2 Limited in Cambridge
UK. i2 is the leading worldwide provider of visual investigative
analysis software for law enforcement, government, military,
intelligence and commercial organizations. i2's products are used
by thousands of analysts on a daily basis in the course of their
investigative duties. Over Joe's ten years at i2 he has had a pivotal
architectural role in several major developments, including the
flagship Analyst's Notebook product. He now runs the futures
team at i2, which provides many of the advanced algorithms and
techniques that lie behind i2's products. His most recent projects
pioneer the use of rich internet application (RIA) and Web 2.0
style technology for intelligence analysts.
REFERENCES
[1]

[2]

[3]

[4]

R. Chang, M. Ghoniem, R. Kosara, W. Ribarsky, J. Yang, E. Suma,
C. Ziemkiewicz, D. Kern, and A. Sudjianto. WireVis: Visualization
of Categorical, Time-Varying Data from Financial Transactions.
Proceedings of IEEE VAST 2007 (Sacramento, CA, October 2007),
pages 155-162.
H. Lam, D. Russell, D. Tang, and T. Munzner. Session Viewer:
Visual Exploratory Analysis of Web Session Logs. Proceedings of
IEEE VAST 2007 (Sacramento, CA, October 2007), pages 147-154.
J. Stasko C. Görg,, Z. Liu, and K. Singhal. Jigsaw: Supporting
Investigative
Analysis
through
Interactive
Visualization.
Proceedings of IEEE VAST 2007 (Sacramento, CA, October 2007),
pages 131-138.
J.J. Thomas and K. A. Cook. Illuminating the Path. IEEE Computer
Society, 2005.

281

