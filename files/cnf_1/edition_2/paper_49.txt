VAST 2010 Challenge: Arms Dealings and Pandemics
1

Georges Grinstein and Shawn Konecni
University of Massachusetts Lowell

Jean Scholtz and Mark Whiting
Pacific Northwest National Laboratory

Catherine Plaisant
University of Maryland
1
ABSTRACT
The 5th VAST Challenge consisted of three mini-challenges that
involved both intelligence analysis and bioinformatics. Teams
could solve one, two or all three mini-challenges and assess the
overall situation to enter the Grand Challenge. Mini-challenge
one involved text reports about people and events giving
information about arms dealers, situations in various countries
and linkages between different countries. Mini-challenge two
involved hospital admission and death records from various
countries providing information about the spread of a world
wide pandemic. Mini-challenge three involved genetic data to
be used to identify the origin of the pandemic and the most
dangerous viral mutations. The Grand Challenge was to
determine how these various mini-challenges were connected.
As always the goal was to analyze the data and provide novel
interactive visualizations useful in the analytic process. We
received 58 submissions in total and gave 15 awards.
Keywords: visual analytics, human information interaction,
sense making, evaluation, metrics, contest.
Index Terms: H.5.2 [Information Interfaces & Presentations]:
User Interfaces – Evaluation/methodology
1 BACKGROUND
As in previous years the objective of the VAST 2010 Challenge
was to provide researchers with realistic tasks and datasets for
evaluating their research or products [1, 3]. While the data we
provided resembles realistic data in format, it is much smaller
and cleaner than actual data [4]. Nonetheless, the VAST
Challenge helps researchers understand how their software
would be used in a novel analytic task and determine if their
visualizations and interactions would be beneficial in the
analysis. The datasets we provide have been repeatedly used by
researchers and software providers throughout the years after the
Challenge is over to demonstrate the capabilities of their
systems. The ground truth that is embedded in the datasets helps
researchers evaluate the utility of their visualizations and how
these should evolve.
2 VAST 2010 CHALLENGE

1
grinstein@cs.uml.edu; skonecni@cs.uml.edu;
plaisant@cs.umd.edu; jean.scholtz@pnl.gov;
mark.a.whiting@pnl.gov

IEEE Symposium on Visual Analytics Science and Technology
October 24 - 29, Salt Lake City, Utah, USA
978-1-4244-9487-3/10/$26.00 ©2010 IEEE

The VAST 2010 Challenge consisted of three mini-challenges
(MC) and one Grand Challenge (GC). Each MC had a data set,
instructions and a number of questions to be answered. The GC
required participants to pull together information from all three
data sets and write a debrief summarizing the situation. Most
teams are not able to analyze all three of the mini-challenges and
enter only mini-challenges. The number of entries this year was
on par with those of the past years (Figure 1). This was
especially rewarding as one mini-challenge required text
processing capabilities and another required analysis of genetic
information, and we were not sure how many researchers in the
field had these capabilities. As in previous years, we had no set
awards but gave out awards based on the submissions we
received. Awards fell into three categories: analytic process,
analysis completeness, and novel and useful visualizations. All
teams receiving an award could contribute two page summaries
for the proceedings. All submitted materials are available in the
Visual Analytics benchmark repository [5]. All teams received
certifications of participation and were invited to the VAST
Challenge Participants’ workshop where they demonstrated their
tools.
The VAST 2010 scenario featured a number of arms
dealings in various countries, a world-wide pandemic, and some
genetic data helping identify the origin of the pandemic. MC 1
consisted of text documents which participants needed to
process to identify the key characters and events and/or
situations in each of 12 main countries. After summarizing the
situation in each country, participants were asked to produce a
social network showing the relationships between the key
players. MC 2 required participants to analyze over 1 million
hospital admission records (with chief complaints) and death
records, and characterize the pandemic.
MC 3 asked
participants to analyze a number of genetic samples and use this
information to locate the origin of the disease as well as identify
the most dangerous viral mutations.
The National Visualization and Analytics Center (NVAC)
Threat Stream Generator project team at Pacific Northwest
National Laboratory developed the data sets for MC1 and MC2.
The data for MC3 was developed at University of Massachusetts
Lowell. Each data set was embedded with non-trivially
discoverable ground truth [4].
Judges had no access to the systems so teams were asked to
provide a video and a concise process description of how their
system was used to arrive at their conclusions, and how the
various visualizations helped in the analysis.
We received 5 GC entries and 53 MC entries (Table 1).
2.1 Judging
We recruited judges from the visualization and the analytic
communities. While visualization researchers were allowed to
judge even if they entered the challenge, they were not allowed
to judge any of their own entry categories (mini challenge or
grand challenge). Each judge was given access to the solutions
and reviewed 3-4 entries online and were asked to rate the

263

VAST Contest 2006: 6 entries – Grand Challenge only
VAST Contest 2007: 7 entries- Grand Challenge only
VAST Challenge 2008: 6 Grand Challenge entries and
67 mini-challenge entries:
• 22 cell phone network
• 13 migrant boats
• 12 wiki edit
• 20 evacuation trace entries
28 different organizations from 13 countries submitted
entries. 13 were student teams.
VAST Challenge 2009: 5 Grand Challenge entries and
44 mini-challenge entries:
• 22 badge and network traffic
• 17 social network
• 5 video
28 different organizations from thirteen countries
submitted entries. 18 were student teams.
VAST Challenge 2010: 5 Grand Challenge entries and
58 mini-challenge entries:
• 14 text
• 22 pandemics
• 17 genetics
31 organizations (18 universities) from 7 countries for a
total of 155 participants. 14 teams were student teams.
Figure 1. Statistics from VAST Challenges 2006-2010
analytic process, the visualizations and interactions, and the
novelty of the submission. As in previous years, we also gave
an accuracy measure for each team’s solution. However, as the
task and data sets get more realistically complex, the accuracy
has become more difficult to measure. For example, the main
characters and events in each country required mentioning
dozens of different entities. The accuracy ratings were assigned
by the VAST Challenge co-chairs and the dataset providers.. All
teams managed to find non-trivial information in the data sets.
Each co-chair was responsible for reading the submitted
reviews and summarizing those results at a VAST Challenge
Review meeting. Additional analysts attended the meeting and
were responsible for reviewing the GC entries. All entries were
discussed and 15 awards to 13 teams determined.
3 Summary of VAST Challenge 2010 Awards
This year we gave an effective visualization award in each minichallenge: for effective interactive visualization of document
contents (IBM Research – China), for effective visualization of
symptoms (Periscopic), and for innovative visualization of the
genetic data (BFS Innovation). We also gave an award for
Outstanding Interaction Model (Simon Fraser University) in
MC1. We were surprised that there were few innovations in
social network visualization for MC1 given the current interest
in social network analytics.
There were a number of good analysis awards: the team from
Noblis, Centrifuge and Future Point Systems (GC), Simon
Fraser University (GC), Oculus Info Inc. (MC1), and City
University London (MC2 and MC3). Three teams received
awards for their analytic process and/or explanation of their
process: Georgia Institute of Technology (MC3), Bangor
University (MC2), and two teams from the University of

264

Konstanz (MC2). Palantir Technologies received an award for
the quality of the video demonstration (GC).
Other teams received awards for additional functionality:
Georgia Institute of Technology for their support of data
ingestion (MC1); the team from Purdue University and the
University of Houston for their support for future detection of
pandemics (MC2); and the team from the Universidad
Autonóma de Madrid and University of Maryland for cleverly
adapting a tool from the plagiarism detection domain to look at
genetic samples (MC3).
Overall, the quality of the submissions definitely improved
making it more difficult to single out teams for awards. We
were encouraged that teams voiced the usefulness of the
Challenge data sets and tasks and that they spend much of their
time enhancing their tools in determining what makes their
visualizations more useful and their analysis more efficient.
4 PARTICIPANT DISCUSSION SESSION
We held participant workshops, in 2008 for several hours one
evening prior to the conference and in 2009 as an all-day
session, with several speakers and time for the teams to talk and
demonstrate their work. Teams appreciated the opportunity to
interact with one another and some formed new partnerships.
We repeated this in 2010 with an all-day participant session
prior to the conference. NSF provided student travel support.
5 THE PATH FORWARD
This VAST Challenge marks the 5th year of the event. While we
have been extremely successful in attracting participants, we
encourage the community to join us in setting the direction for
future VAST Challenges. Each year we face numerous
decisions including what type of scenario to use, what type of
data sets to provide, what types of analysis tasks to define, and
what domains and visuals should be emphasized. While we
attract many teams to the MCs, the responses to the GC have not
increased, although the entrants have changed. Do contact any
of the VAST Challenge co-chairs if you would be willing to
help us develop a road map for the next five years.
ACKNOWLEDGMENTS
This work was supported in part by the National Visualization
and Analytics CenterTM (NVACTM) located at the Pacific
Northwest National Laboratory in Richland, WA. The Pacific
Northwest National Laboratory is managed for the U.S.
Department of Energy by Battelle Memorial Institute under
Contract DE-AC05-76RL01830. Members of the committee
were also supported in part by the National Science Foundation
(0947343 and 0947358).
We also wish to thank Jereme Haack, Carrie Varley, and
Cindy Henderson of Pacific Northwest National Laboratory; the
students of the University of Massachusetts Lowell especially
Loura Costello and Heather Byrne and Swetha Reddy of the
University of Maryland.
REFERENCES
[1]
[2]
[3]
[4]
[5]

VAST Challenge 2010:www.cs.umd.edu/hcil/VASTchallenge2010
Other years: www.cs.umd.edu/hcil/VASTcontest06 and 07 , and
www.cs.umd.edu/hcil/VASTchallenge08 and 09
Costello, L., Grinstein, G., Plaisant, C. and Scholtz, J., Advancing
User-Centered Evaluation of Visual Analytic Environments
through Contests, Information Visualization 8 (2009) 230–238
Whiting, M., Haack, J., and Varley, C. Creating realistic, scenariobased synthetic data for test and evaluation of information
analytics software. Proc. of BELIV’08, ACM (2008)
Visual Analytics benchmark repository:
http://hcil.cs.umd.edu/localphp/hcil/vast/archive/

