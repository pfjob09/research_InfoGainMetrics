Optimizing Parallel Performance of Streamline Visualization for Large
Distributed Flow Datasets
Li Chen*
Tsinghua University

Issei Fujishiro†
Tohoku University

ABSTRACT
Parallel performance has been a challenging topic in streamline
visualization for large unstructured flow datasets on parallel
distributed-memory computers. It depends strongly on domain
partitions. Unsuitable partitions often lead to severe load
imbalance and high frequent communications among the domain
partitions. To address the problem, we present an approach to
flow data partitioning taking account of flow directions and
features. Multilevel spectral graph bisection method is employed
to reduce communication and synchronization overhead among
distributed domains. Edge weights in the corresponding adjacent
matrix is defined based on an anisotropic local diffusion operator
which assigns strong coupling along flow direction and weak
coupling orthogonal to flow. Meanwhile, the distributions of seed
points and flow features such as vortex structure are also
considered in partitioning so as to obtain good load balance. The
experimental results are given to show the feasibility and
effectiveness of our method.
KEYWORDS: Flow visualization, parallel streamline, graph
partition, load balance, flow clustering.
INDEX TERMS: I.3.3 [Computer Graphics]: Picture/Image
Generation; I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism; C.2.4 [Computer-Communication
Networks]: Distributed Systems—Client/Server, Distributed
Applications; J.2 [Physical Sciences and Engineering].
1

INTRODUCTION

With the rapid development of computer hardware and software,
the computational grid is becoming extremely complicated, and
the amount of dataset produced by routine computations is
becoming extremely large. To address this challenge, many
advances have been made for large-scale data visualization such
as out-of-core techniques and distributed parallel algorithms.
Out-of-core techniques are commonly used where data sets are
larger than main memory. Ueng et al. [1] presented an approach to
compute streamlines for large unstructured grids. However, for
extremely large data size, it is still time-consuming to access data
stored on disk by I/O operations although the algorithms were
well designed to reduce it. Meanwhile, a single PC cannot satisfy
the speed requirements from users for extremely large data size.
In order to handle large-scale datasets, distributed memory
*

Institute of Computer Graphics and Computer Aided Design,
School of Software, Tsinghua University, Beijing 100084, P.R.
China, chen@mail.tsinghua.edu.cn.
†
Institute of Fluid Science, Tohoku University, 2-1-1
Katahira,, Sendai 980-8577, Japan, fuji@vis.ifs.tohokua.ac.jp.
IEEE Pacific Visualisation Symposium 2008
4 - 7 March, Kyoto, Japan
978-1-4244-1966-1/08/$25.00 ©2008 IEEE

(a) A bad partition

(c) A bad partition

(b) A good partition

(d) A good partition

Figure 1. Different partitions for flow fields

architecture is very successful to overcome both the memory and
speed limitation and scalability. Much work has been done on
developing parallel visualization algorithms for distributed
memory parallel machines. Many effective methods have been
presented for parallel isosurface generation and volume rendering
[3,4]. However, almost no good parallel algorithm for streamline
visualization was presented on distributed-memory platforms
although the high parallel performance can be achieved on shared
memory machines. The communication and synchronization
overhead makes the parallel performance low down dramatically.
Advection based flow visualization such as streamlines,
streamtubes, particles or flow ribbons [5,6] is still one of the most
important flow visualization methods because it can provide
intuitive and precise local insight, but it is very difficult to obtain
high speedup performance on distributed-memory machines. In
the streamline generation method, the next position on the tracing
line is strongly dependent on the current position, so the parallel
performance depends strongly on domain partitions. Because the
flow data is located in distributed memory, each processor only
can generate the part which the streamline passes through in the
data domain on the processor. When the streamline exits from the
domain of the processor and enters another one, the processor
need send the corresponding information to next processor so as
to continue streamline tracing. Therefore, during parallel
streamline generation on distributed memory machines, one
streamline may consist of many streamline segments which are
generated by different processors. And one streamline may enter
the same processor several times to generate different segments of
the streamline. Before visualization, it is very difficult to know

87

how a streamline runs over the whole field, thus load imbalance is
a very critical problem in streamline generation. Unsuitable
partitions may lead to severe load imbalance and high frequent
communications among the processors. It is very important to
develop effective repartition methods to address this problem for
large unstructured datasets.
For most parallel scientific simulations, the domain partitions
of visualization are same with computational process. The
distributed domains are partitioned based on load balance in
computational process, but it may be extremely imbalanced in
visualization cases. Fig.1 shows different partitions for two flow
fields. (a) and (c) are bad partition examples in speedup
performance. Because the partition does not consider any flow
features, more processors need be crossed in order to generate
even one streamline. It will not only cause extra communication
overhead but also difficult to estimate load amount on each
processor. Meanwhile, in (a), all seed points of streamlines are
concentrated in several processors, so all other processors will be
idle during the initial period in streamline generation. After
taking both flow direction and seed positions into account, better
partition design is shown in (b). Each streamline is as long as
possible within one processor and seed points are evenly
distributed in all processors. In (c), even for generating just one
streamline, the tracing need be done among all four processors
frequently and communicate among four processors many times,
but after repartition based on flow features, totally only three
times’ communication and synchronization wait is required.
In order to improve parallel streamline visualization
performance, this paper presents an approach to flow data
repartition taking account of flow directions and features for large
unstructured flow data on distributed-memory platforms. It is
based on the multilevel spectral graph bisection method. Edge
weights in the corresponding adjacent matrix is defined based on
an anisotropic local diffusion operator which emphasizes flow
directions. Meanwhile, seed points and some flow features such as
vortex structure are considered in partitioning to get good load
balance.
This paper is structured as follows. Section 2 reviews related
work on this topic including flow clustering methods and graph
partition methods. Section 3 briefly describes our parallel
streamline algorithm for distributed memory machines. Section 4
introduces the construction of eigen matrix to reflect flow
directions. Section 5 gives some implementation details of our fast
dynamic spectral mesh partition. In section 6, some other factors
such as seed point positions and important flow features are
combined in the repartition process. Finally experimental results
and conclusions are given in section 7 and 8, respectively.
2

RELATED WORK

2.1
Flow Clustering Methods
Many methods have been presented for scalar data clustering such
as wavelet or Fourier analysis. However, flow field clustering has
been a challenging topic. Some multiscale clustering approaches
have been proposed in recent years. Heckel et al. [7] presented a
good method for the hierarchical representation of vector fields. It
is based on iterative refinement using clustering and principal
component analysis. The algorithm generates a top-down
segmentation of the discrete field by splitting clusters of points.
The error of the various approximation levels is measured by the
discrepancy between streamlines generated by the original
discrete field and its approximations based on much smaller
discrete data sets. This method need not require any topological
connectivity information. However, due to the top-down
segmentation, an accurate representation may require a large
cluster count.

88

At the same time, Telea and Van Wijk [8] also presented a
visualization method that produces simplified but suggestive
images of the vector field automatically, based on a hierarchical
clustering of the input data. It is based on bottom-up strategy.
Each data point is placed in a cluster. Then similar clusters are
merged bottom-up, using a metric of the difference in position and
orientation of the vectors that represent the clusters. The cluster
shapes are indirectly constrained by adapting the weights of the
metric terms. This method only needs a few parameters with
which users can produce various simplified vector field
visualizations that communicate different insights on the vector
data. However, this method may be sensitive to the weight tuning,
leading to clusters that are not aligned with the flow.
Another class of clustering method is physics-based clustering
approaches. These methods simulate a physical process, such as
anisotropic diffusion to construct a multiscale decomposition [911]. In [9,10], an anisotropic operator is applied to an initial finegrained, noise-like signal. The initial clusters, represented
implicitly by the fine-grained noise, are coarsened and aligned to
the flow, which determines the anisotropy of the operator. The
diffusion time serves as a multiscale parameter. In [11], Griebel,
Preusser, et al. construct the cluster hierarchy in a strictly
algebraic fashion. A local alignment tensor is defined that encodes
a measure for alignment to the direction of a given flow field.
This tensor induces an anisotropic differential operator on the
flow domain, which is discretized with a standard finite element
technique. The entries of the corresponding stiffness matrix
represent the anisotropically weighted couplings of adjacent nodes
of the domain mesh. Then an algebraic multigrid algorithm is
used to generate a hierarchy of fine to coarse descriptions for the
above coupling data. This hierarchy comprises a set of coarse grid
nodes, a multiscale of basis functions and their corresponding
supports. A multilevel decomposition of the flow structure can be
well obtained.
We employ a similar idea with [11]. However, our target is
different from it. In order to handle large data size, Garcke et al.
used multiscale representation so as to keep maximum precision
by some simplified level decompositions. The simplified field
only can be an approximation of the original field with some loss
of details. In our case, we hope to keep all the details of the
original flow field. Parallel algorithms and distributed-memory
platforms are used to overcome speed and memory bottleneck for
extremely large data size. Multilevel simplification is not
considered in our clustering algorithm, instead, load balance and
communication minimization are focused to improve speedup
performance. In order to make each streamline as long as possible
within one processor to reduce communication overhead, a similar
local alignment tensor with [9] based on flow direction is defined.
Discretizing the corresponding anisotropic differential operator
using finite elements defines strong couplings along flow
direction and weak couplings orthogonal to flow direction
between mesh neighboring points [11]. An adjacent graph matrix
encoding the flow direction is obtained. Based on the matrix, we
can get optimum partition result according to the spectral graph
partition theory.
Yu et al. [12] presented the design of a scalable parallel
pathline construction method for visualizing large time-varying
3D vector fields. It is based on a new 4D representation of the
time-dependent vector fields and a hierarchical clustering of the
corresponding 4D space. The method in [8] was used for
clustering. It allows for interactive navigation of a large timevarying flow field using a parallel computer while reducing the
communication overhead. Compared with this method, our
approach used a different clustering method which can combine
other feature affections and fit for very complicated unstructured
grids. The well-balanced load is easy to get by the multilevel

spectral graph bisection clustering method.
Feature-based clustering method should be the most promising
way because of their potential capability to dramatically reduce
the complexity of the flow field. Some work has been done on
field simplification methods by detecting features such as saddles,
sources, sinks, and vortices. In [13], the authors presented a good
way to match feature patterns using Clifford algebra convolution
operations. Vortex features are found by a particular vortex mask
matching against the flow field. [14,15] also did some good
research on feature-based clustering. However, these methods
cannot work well in our case. Feature-based methods are usually
strongly application-dependent. The flow features are difficult to
be specified. Currently, there are two kinds of definitions for flow
features. One is based on topological analysis such as critical
points, separatrices, etc. The other is based on physical
characteristics such as vortices, shock waves, etc. Flow topology
theory [16] works well for 2D case, but it still remains some
problems for 3D case. Meanwhile, if a flow field has many critical
points, the topology analysis may be unstable. And if a flow field
has no critical points, it will be useless during the repartitioning
process in our case. As for vortex, it has no clear definition yet
until now. Therefore, feature-based method is still very difficult to
apply.
In order to make use of the advantages of features in flow
fields, we introduce the feature detection in our clustering method.
To consider about its limitation, we just take it as a supplement
means for getting better partitions. The partitioned domains are
mainly decided by flow directions with some consideration of
detected flow features. By taking account of both flow direction
and feature distribution, our flow partitioning method becomes
more effective and scalable in improving the parallel performance
of streamline generation.
2.2
Spectral Graph Partitioning Methods
Accumulated evidence shows that spectral methods can find good
and acceptable clustering results as judged by human experts on a
variety of real datasets and also on artificial cases, so we adopted
it in the flow mesh repartitioning. Although the spectral approach
has been widely used in clustering and partitioning [21] and
shown to be effective, one of the most difficult problems in our
case is that it need be coupled to the parallel computational
analysis process and require repeated load balancing. Dynamic
mesh partition is very important to get high parallel performance.
The most widely-used technique of spectral mesh partitioning is
Recursive Spectral Bisection (RSB) [17] that is derived from a
graph bisection strategy based on a specific eigenvector of the
Laplacian matrix of the graph. In particular, the eigenvector
corresponding to the second smallest eigenvalue gives some
directional information about the graph. The special properties of
this eigenvector have been extensively investigated by Fiedler
[18]; hence, called the Fiedler vector. The computational
challenge of the RSB algorithm is the efficient calculation of the
Fiedler vector. RSB is regarded as one of the best partitioners due
to its generality and high quality; however, the method is very
expensive since it requires computing the Fiedler vector at each
recursive step. The multidimensional spectral partitioning (MSP)
[19] algorithm improves RSB by considering several cuts at each
recursive step. For example, it can perform spectral octasection to
partition a graph into eight sets using three eigenvectors. MSP
requires less computation than RSB to generate the same
partitions. These algorithms are often combined with local
refinement strategies such as the Kernighan-Lin (KL) heuristic
[20] to improve the fine details of the partition boundaries.
However, they are still too slow to apply in many applications.
In order to reduce the partitioning time for large meshes,
multilevel algorithm is a good way to contract the graph. It can

reduce the size of the mesh by collapsing edges, partitioning the
smaller graph, and then uncoarsening it back to obtain a partition
for the original mesh. The most sophisticated schemes use a
sequence of successively smaller contracted meshes, and smooth
the partitions using KL during the uncoarsening phase. The
multilevel implementation of RSB, called MRSB [21], calculates
the Fiedler vector for the coarsest graph, and then prolongates it
for the original mesh. For example, in [19,22], graph contraction
strategies are performed and spectral methods just on the coarsest
mesh. The good mesh partitioning tool MeTiS [23] also adopted
multilevel scheme. It uses heavy edge matching during the
coarsening phase, a greedy graph growing algorithm for
partitioning the coarsest mesh, and a combination of boundary
greedy and KL refinement during the uncoarsening phase.
We employ the similar way to repartition distributed mesh
domains so that it can have less communication during streamline
tracing process by taking into account flow directions. Moreover,
the multilevel idea is adopted to accelerate the partitioning
process. Furthermore, load balance is fully considered according
to mesh element number, flow direction, flow features and seed
point positions.
3

BRIEF INTRODUCTION TO
STREAMLINE ALGORITHM

OUR

PARALLEL

In order to describe clearly the presented dynamic flow data
repartitioning method for improving parallel streamline
visualization performance, we first give a brief introduction to our
parallel streamline visualization algorithm for distributed datasets.
Because the flow data is located in distributed memory, each
processor only can generate a part of results which the streamline
passes through inside the data domain on the processor. When the
streamline exits from the domain of the processor and enters
another one, the processor need send the corresponding
information to next processor so as to continue the streamline
tracing. Therefore, during parallel streamline generation on
distributed memory machines, one streamline may consist of
many streamline segments which are generated by different
processors. Meanwhile, one streamline may enter the same
processor several times to generate different segments of the
streamline. To handle the complicated case and reduce the
communication time, the following three linked lists are used in
our implementation:
(1) active_tracing_st_seg_list: This link data structure stores all
the points which will be traced in the current processor and
its traced results (line segments). The members include the
streamline ID, the current segment ID, the traced line
segment results.
(2)
next_tracing_list: This link stores all the points which
stopped at the processor domain boundary and will enter
next processor to continue tracing. Its members contain the
streamline ID, the current segment ID, next processor ID,
the current point information (coordinates, element ID, data
attributes, etc). This link information will be used to
generate communication table with other processors.
(3)
streamline_seg_list: This link is used to store all the
streamline segments generated inside the current processor.
The members contain the streamline ID, streamline segment
ID, traced line segment results. This link information is used
to generate final streamline results.
The key steps of our parallel streamline algorithm are as follows:
(1) Seeding and finding all seed points each processor contains;
(2) For each processor, inserting the seed points to
active_tracing_st_ seg_list；
(3) While the total number in active_tracing_st_seg_list on all
processors is greater than 0, tracing in each processor:

89

a. Generate the streamline segments for all the points in
active_tracing_st_seg_list in the current processor;
b. If meeting the whole flow field boundary, the streamline is
over;
c. If running into another processor (meeting the processor
domain boundary), add it to next_tracing_list (streamline
segment ID plus 1);
d. After all the tracing finishes in current processor:
― Copy the tracing results into streamline segment linked
list with its streamline ID and streamline segment ID;
― If the number in next_tracing_list > 0, generate
communication table among processors;
― Send unfinished streamlines to each corresponding
processor and receive the points from other processors;
― Insert received streamlines into active_tracing
_st_seg_list.
(Repeat a-d until all the streamlines are over)
(4) Communicating among processors to collect all the streamline
segments in all processors;
(5) Combining all the segments generated in different processors
for the same streamline in the order of segment ID.
According to the above algorithm, we can see the following three
factors are very important for obtaining high parallel performance:
(1) Partition along flow directions to keep each streamline as long
as possible so as to pass through less processors. This can
reduce communication times greatly. The overhead of
synchronization time for communication in parallel streamline
algorithm is very heavy.
(2) The total number of the streamline segments generated on
each processor is balanced.
(3) The traced line segment number on each processor between
two consecutive communications is as balanced as possible.
Our dynamic flow mesh repartition algorithm is designed to fit
to the three factors.
4

ADJACENT MATRIX BASED ON FLOW DIRECTION

Our target is to partition a flow field along the flow directions so
that each streamline can stay in one processor or pass through the
minimum processor number. Therefore, a local coupling operator
is used in our method on the flow domain to have a strong
coupling weight in the direction of the flow and a weak coupling
weight orthogonal to the flow.
To implement this, we adopted the same flow aligned
anisotropic diffusion operator used in [9] and [11]. Different from
[11] for generating multiscale segmentation, we only use it to
build a local coupling tensor encoding the flow direction.
Suppose that v ≠ 0 on a flow domain Ω. The linear diffusion
in the direction of the vector field and a Perona Malik type
diffusion orthogonal to the field is considered. Then, at each point
x∈Ω, we can find a family of continuous orthogonal mappings
such that B(v)v=∥v∥e0 ,where {ei} i=0,・・・,d-1 is the standard basis
in Rd. Taking into account this coordinate transform, we define a
local coupling tensor
⎛α ( v )
⎞
⎟ B (v )
a (v) = B (v)T ⎜⎜
⎟
Id
ε
d −1 ⎠
⎝

(1)

where α is a positive real which represents the strong coupling
in the flow direction, and Idd-1 is the identity matrix in dimension
d-1. In any direction orthogonal to v we have a coupling with
weight ε which is supposed to be small.
In order to apply the local differential operator on unstructured
finite element mesh, the following discretization is performed. For

90

a triangulation mesh with nodes (Xi)i=1,・・・,n, the piecewise linear
basis {φi}i=1, ・ ・ ・ n of hat functions is uniquely defined by the
property φi(Xj) = δi j for all vertices Xj , where δi j is the Kronecker
symbol. Then the n×n finite element adjacent weight matrix A =
(Ai j) which describes the weighted coupling of adjacent nodes is as
follows:
(2)
Aij := a (v)∇φ i ∇φ j dx

∫

Ω

Different from [11], in order to apply spectral graph partitioning
algorithm on it, we define

Aii :=

n

∑A

j =1, j ≠i

ij

and Aij := A ji

(3)

It is worthy to mentioning that this process is basically
localized. Only some communication is necessary on adjacent
boundary elements among distributed domains, so it can be highly
parallelized.
5

PARALLEL MESH REPARTITION

Spectral mesh partitioning has been one of the most important
algorithms on mesh partitioning. The eigenvector corresponding
to the second smallest eigenvalue of mesh adjacent matrix gives a
very good balanced layout of graph bisection. A high quality
distributed mesh requires good balance rate to make full use of
each processor and minimum edgecut number to reduce
communication overhead. Meanwhile, it can provide good results
in clustering problems which satisfies our requirements well.
Compared with other partitioning method, it provides us with an
exact mathematic solution to the both balance and clustering
problem. The biggest challenge of spectral-based methods is time
cost. Therefore, a fast and high parallel eigenvalue solver is the
key in our dynamic mesh repartition for flow field visualization.
HPC-MW parallel eigen solver was adopted in our approach
which was also developed by our group [24]. In this solver, many
kinds of eigenvalue algorithms are parallelized and optimized for
various types of computers including vector/RISC processors,
SMP parallel architectures and PC clusters. For example, the
Lanczos method is one of effective methods. It generates a three term recursion that results in the tridiagonalization of the problem
matrix, which retains the eigenvalues of the original matrix. The
advantage of the method is that the tridiagonal matrix can be
made as small as desired, and eigenvalues in a desired spectral
range can be obtained without computing for the entire problem
matrix. This is very suitable for spectral bisection method because
only the second smallest eigenvalue and eigenvector is needed.
The method is also highly parallelized due to predominantly
matrix-vector operations, and is especially suited for problems
using geometric domain decomposition. It has been tested by
many applications to exhibit a good parallel efficiency.
Although the eigenvalue solver has very high speed for large
data size, it is still not fast enough in dynamic load repartition.
Therefore, we adopted the similar idea with ParMeTis, developed
by George Karypis et al [25]. The partitioning process consists of
three phases: graph coarsening, initial partitioning, and
uncoarsening / refinement. In the graph coarsening phase, a series
of graphs is constructed by collapsing together adjacent vertices
of the input graph in order to form a related coarser graph.
Computation of the initial partitioning is performed on the
coarsest (and hence smallest) of these graphs, and so is very fast.
Usually only hundreds of nodes are used in the eigen solver
computation. Finally, partition refinement is performed on each
level graph, from the coarsest to the finest (i.e., original graph)
using a KL/FM-type [20] refinement algorithm. The heavy weight
edge matching is performed during the coarsening phase. The
weight of each edge is computed by the method described in
section 3.

Because our mesh repartition for flow field visualization is a
dynamic process, it should fit to both the parallel finite element
computation process and parallel visualization process. An
effective dynamic mesh repartitioning tool which can easily
couple both the FEM computational process and visualization
process was developed. It is based on an MxN communication
model, by which the large-scale computation modules run on M
processors and high performance parallel visualization modules
run on another N processors, concurrently. This can allow
computation and visualization to select suitable parallel hardware
environments respectively. Meanwhile, 'real-time' steering can be
achieved during computation so that the users can check and
adjust the computation process in real time. Furthermore, different
numbers of processors can achieve better configuration between
computation and visualization. The key issues include fast data
repartition, ensuring the security so as to forbid the direct access
to internal of the cluster from external network, executing reliable
synchronization strategies and smart communication strategies
based on Globus Toolkit + MPICH-G2, creating mixed
environment of different types of clusters.
6

OTHER
CONSIDERATIONS
BALANCING

FOR

LOAD

By the above spectral mesh partition, the node number can be
balanced in each processor and the partitioned domain can adapt
to flow directions. However, in the streamline flow visualization
method, the load is not mainly decided by the node number in
each processor but the number of streamlines the processor need
generate and the tracing length each streamline run inside the
processor. Before visualization, it is very difficult to know how a
streamline runs over the whole field, thus load imbalance is a very
critical problem in streamline generation. Thus, some other factors
such as seeding point distribution and the possible trajectory of
each streamline should also be considered in mesh repartitioning.
6.1
Seeding Point Distribution
In our parallel streamline visualization module, the following
three styles are provided for determining seed points: (1) Userspecified manually: The coordinates of all seed points are input by
users; (2) Semi-automatic: Users specify the number of seed
points and the plane equation which all seed points located on.
Then our module selects good positions of all seed points on the
plane; (3) Automatic: Some existing good seed placing methods
are implemented in our module to determine seed points
automatically according to flow features so as to reveal the flow
more thoroughly.
The seed distribution among processors is not considered in
all the three styles. They are specified just based on the users’
interest and the features of the whole field. Therefore, in the
dynamic repartition process, the even distribution of seeds among
processors should be carefully considered. Multi-conditional
balance algorithm was implemented using the similar way with
ParMetis. The priority of evenly seeding is higher than the node
number balance in our implementation.
6.2
Flow Features
Except seed points, flow features also have very important
influence on the performance. For example, around the vortex
areas, the traced streamline is usually longer than straight
direction flow. In complicated flow areas, the streamline segment
number is larger than flat flow areas. Thus, we combined flow
feature detection into the flow repartition process.
The vector field topology analysis technique developed by
Chong, et al [27] was employed in our flow feature detection. It is
based on the critical point theory. The topology of a vector field

consists of critical points (where the velocity vector is zero), and
integral curves and surfaces connecting these critical points. From
this, we can infer the shape of other tangent curves, and hence to
some extent the overall structure of a given vector field.
The positions of the critical points occur only in cells where all
the three components of the vector pass through zero. The exact
position of a critical point can be calculated by interpolation in
case of a rectangular grid. The position of a critical point in case
of a curvilinear grid can be calculated by recursively subdividing
the cells or by a numerical method such as the Newton iteration.
Once the critical points have been found, they can be classified by
approximating the velocity field in their neighborhood with the
first order Taylor expansion [16]. According different type of
critical points, we can assign different weights for the nearby
points.
However, the flow topology analysis sometimes cannot locate
all flow features in 3D data sets. Vortex core is known as a very
important feature in flow fields, but up to date, it has no formal
definition. Some researchers attempted to define it as the center of
swirling flows. Much work has been done on finding vortex lines
[28-30]. We adopted the method presented by Roth and Peikert
[30] to find vortex. The larger node weights are assigned for the
nodes near the vortex. The weight value is inversely proportional
to the distance to the vortex center. If there are more than one
critical point or feature structures near a node, the node weight
value is the sum of all the computed weights to each critical point.
By this way, the less node number can be assigned in the
processor which contains complicated flow areas so as to keep the
total node weights balanced among processors.
According to our parallel streamline algorithm, after the first
time communication, the tracing points in each processor will
become imbalanced. Although the load can be balanced by
repartitioning again, this will bring much extra time cost. In fact,
because we partition flow fields based on flow directions, most
streamline segments can be generated before the first
communication. Therefore, the later load imbalance has very little
affection to the parallel performance. Thus, we just do mesh
repartitioning once in the parallel streamline generation at each
time step.
Meanwhile, among different timesteps, a tradeoff between the
high mesh partition quality and less data migration amount among
processors should be considered. Because the flow direction is
changed continuously, the coherence of adjacent timesteps was
made use of to reduce the moving data as possible as we could.
Therefore, the dynamic reparation and data migration time cost is
usually much smaller than that at the first timestep.
7

EXPERIMENTAL RESULTS

We have tested the presented flow data repartition algorithm by
some flow datasets.
Test 1: A flow simulation passing through an ellipsoidal cylinder
We have applied the method to a 3D irregular flow field, which
simulates a flow passing through an ellipsoidal cylinder. Two
sizes of data are used for the tests: a smaller one with 2,531,790
grid elements and a larger one with 162,034,560 elements. Fig.2
shows the streamline sampling results. Some vortices occur after
the flow passes by the cylinder.
Fig. 3 is the results of eight partitions for the small dataset using
our partition method. We can see the repartitioned mesh can
reflect flow direction. Meanwhile, the vortex areas (located in red
and deep blue respectively in Fig. 3(b)) are specially treated to
contain less node number compared with other domains.
We tested it on a PC-cluster (24 nodes, 48 processors, Intel Xeon
2.8 GHz processors with a Myrinet network, 2 processors per

91

Parallel Performance T est for Small Dataset
9
8
7
Elapsed Time

6
5
4
3
2
1
0
0

10

20

30

40

PE#
Orig Partition(80)

New Partition (80)

Orig Partition (700)

New Partition (700)

Parallel Performance Tests for Small Data

Figure 2. A flow simulation passing through an ellipsoidal cylinder

18
16
14

Speedup

12
10
8
6
4
2
0
0

(a) Original mesh partition
(b) Repartitioned mesh
Figure 3. Mesh repartition for the flow simulation passing through an
ellipsoidal cylinder.

node). For the mesh partitioning process, it took 1.83 seconds to
generate 8 partitions for smaller data and 2.57 seconds for larger
dataset, respectively. Due to the memory limitation, the larger
data set cannot be partitioned by 1 PE (Processing Element)，so
the partitioned results were obtained by the parallel mesh
partitioning module with 8 PEs.
The parallel performance test results are shown as Fig. 4.
Using 1 PE to 32 PEs, the elapsed time (secs) for generating 80
streamlines and 700 streamilnes running on the original domain
partitions and new partitions are given (the mesh partition time
cost is regarded as preprocessing and not counted in the elapsed
time). We can see the parallel streamline on the new partitioned
mesh has much better speedup scalability than that on the original
mesh, especially with the increase of streamline number. This is
mainly because the visualization load is extremely imbalanced in
original mesh case. Because the seed points are all located
upstream at the beginning in this dataset, during the whole process,
only a part of processors are busy and all others are idle. For eight
PEs case shown as Fig. 3, using the original mesh partition to

92

5

10

15

20

25

30

35

PE#
Orig Partition (80)

New Partition (80)

Orig Partition (700)

New Partition (700)

Figure 4. Parallel streamline performance tests for the small data
set(2,531,790 grid cells). The elapsed time using 1PE to 32 PEs for
generating 80 streamlines and 700 streamlines with the original mesh
partition and the new one respectively is shown as (top), and the
speedup performance is shown as (bottom)

generate 700 streamlines, the idle time cost for totally all PEs is
4.137 secs, which is almost 50% of the total elapsed time and the
communication time cost is 0.117secs. Using our new
repartitioned mesh, the idle time cost for all PEs is only 0.525 secs,
which is much smaller than that on the original mesh. The
communication time cost is 0.027, also much faster.
Test 2: An air flow simulation in a room (data courtesy of H.
Okuda)
Fig. 5 shows a 3D air flow simulation in a room. There are many
barriers in the room which make the air flow complicated. The
mesh has 1,140,813 unstructured elements. The original mesh

Figure 7. A tornado flow dataset

Figure 5. A 3D air flow simulation in a room

(a) Original mesh partition
(b) Repartitioned mesh
Figure 8. Flow mesh repartition for a tornado flow dataset
.

(a) Original mesh partition
(b) Repartitioned mesh
Figure 6. Flow mesh repartition for a 3D air flow dataset

and new partitioned mesh are shown in Fig. 6. We can see the
new partition fits flow directions much better than original one.
Test 3: A 3D tornado dataset (data courtesy of Roger Crawfis at
Ohio State University).
This is a structured dataset with the size 192*192*192. We treated
it as the unstructured mesh format for testing our algorithm. Fig. 7
shows the tornado flow. The original mesh and new repartitioned
mesh are displayed in Fig. 8. Both the flow direction and seed
point distribution are considered in the example.
8

CONCLUSIONS

This paper describes an efficient parallel mesh repartitioning
algorithm for large unstructured flow streamline visualization on
distributed memory platforms. Flow directions, seed point number
and flow features are considered into flow mesh repartition to
reduce communication overhead and obtain accurate load balance.
The experimental results demonstrated the feasibility and
effectiveness of our method.
Future work will focus on combining more flow features into
flow repartition to obtain optimum partition results and further
improving repartition speed.
ACKNOWLEDGEMENTS
This study is a part of the "Frontier Simulation Software for
Industrial Science" project in the "IT Program" funded by the
Ministry of Education, Culture, Sports, Science and Technology,
Japan (MEXT). It is also supported in part by the NSFC through
grants No. 60773143 and No. 90715043. We wish to thank the
anonymous reviewers for their constructive comments.

REFERENCES
[1]

S.-K. Ueng, C. Sikorski and K.-L. Ma. Out-of-Core streamline
visualization on large unstructured meshes. IEEE TVCG, 3(4):370–
380, 1997.
[2] C.D. Hansen and P. Hinker. Massively parallel isosurface extraction.
Proc. Visualization, pages 77-83, 1992.
[3] C. M. Wittenbrink. Survey of parallel volume rendering algorithms.
Proceedings of International Conference on Parallel distributed
Processing Techniques and Applications, Las Vegas, Nevada, pages
1329-1336, 1998.
[4] K. L. Ma, J. Painter, C. D. Hansen and M. F. Krogh. Parallel Volume
Rendering Using Binary-swap Compositing. IEEE CG&A, Vol.14,
No.4, pages 59-67, 1994.
[5] W. J. Schroeder and C. R. Volpe, W. E. Lorensen. The stream
polygon: a technique for 3d vector field visualization. Proc.
Visualization 91, IEEE Computer Society Press, pages 126-132,
1991.
[6] J. J. van Wijk. Flow visualization with surface particles. IEEE
Computer Graphics and Applications, 13(4): 18-24, 1993.
[7] B. Heckel, G. Weber, B. Hamann and K. I. Joy. Construction of
vector field hierarchies. Proc. Visualization ’99, IEEE Computer
Society Press, pages 19-25, 1999.
[8] A. C. Telea and J. J. van Wijk. Simplified representation of vector
fields. Proc. Visualization ’99, IEEE CS Press, pages 35-42, 1999.
[9] U. Diewald, T. Preußer and M. Rumpf. Anisotropic diffusion in
vector field visualization on Euclidean domains and surfaces. IEEE
TVCG, 6(2):139–149, 2000.
[10] H. Garcke, T. Preusser, M. Rumpf, A. Telea, U. Weikard and J. J.
van Wijk. A phase field model for continuous clustering on vector
fields. IEEE TVCG, 7(3):230–241, 2000.
[11] M. Griebel, T. Preusser, M. Rumpf, M. A. Schweitzer and A. Telea.
Flow field clustering via algebraic multigrid. Proc. Visualization ’04,
IEEE Computer Society Press, 2004.

93

[12] H.F. Yu, C.L. Wang and K-L. Ma. Parallel hierarchical visualization
of large time-varying 3D vector fields. Proceedings of ACM/IEEE
Supercomputing 2007 Conference, Reno, NV November, 2007.
[13] J. Ebling and G. Scheuermann. Clifford convolution and pattern
matching on vector fields”. Proc. Visualization’03, pages 193-200,
2003.
[14] H. Theisel and H. P. Seidel. Feature flow fields. Proc. EG/IEEE VisSym’03, pages 141-148, 2003.
[15] D. Bauer and R. Peikert. Vortex tracking in scale space. Proc.
Visualization’02, pages 233-241, 2002.
[16] J. Helman and L. Hesselink. Visualizing vector field topology in
fluid flows. IEEE Computer Graphics and Applications, 11(3): 3646, 1993.
[17] H. D. Simon. Partitioning of unstructured problems for parallel
processing, Computing Systems in Engineering, Vol. 2, pages135148, 1991.
[18] M. Fiedler. A property of eigenvectors of nonnegative symmetric
matrices and its application to graph theory. Czechoslovak
Mathematics Journal 25, pages 619-633, 1975.
[19] B. Hendrickson and R. Leland. A multilevel algorithm for
partitioning graphs. Report SAND93-1301, Sandia National
Laboratories, Albuquerque, NM, 1993.
[20] B.W. Kernighan and S. Lin. An efficient heuristic procedure for
partitioning graphs. The Bell System Technical Journal, Vol. 49,
pages 291-308, 1970.
[21] S. T. Barnard and H. D. Simon. Fast multilevel implementation of
recursive spectral bisection for partitioning unstructured problems.
Concurrency: Practice and Experience, Vol. 6, pages 101-117, 1994.
[22] R. Van Driessche and D. Roose. A graph contraction algorithm for
the fast calculation of the Fiedler vector of a graph. In Proc. 7th
SIAM Conference on Parallel Processing for Scientific Computing,
pages 621-626, 1995.
[23] G. Karypis and V. Kumar. A fast and high quality multilevel scheme
for partitioning irregular graphs. Report 95-035, University of
Minnesota, Minneapolis, MN, 1995.
[24] http://www.fsis.iis.u-tokyo.ac.jp/
[25] http://www-users.cs.umn.edu/~karypis/metis/parmetis/
[26] Earth Simulator Research and Development Center web site: http: //
www.es.jamstec. go.jp/
[27] M.S. Chong, A..E. Perry, B.J. Cantwell. A general classification of
3D flow fields. Physics of Fluids Ann., Vol. 2, No.5, pages 765-777,
1990.
[28] D. Sujudi and R. Haimes Identification of swirling flow in 3-D
vector fields. AIAA-95-1715, 1995.
[29] D. Kenwright. Automatic vortex core detection. IEEE Computer
Graphics and Applications, Vol. 18, No. 4, pages 70-74, 1998.
[30] M. Roth and R. Peikert. A higher-order method for finding vortex
core lines. In: Proceedings of IEEE Visualization ’98. ACM Press,
New York, pages 143-150, 1998.

94

