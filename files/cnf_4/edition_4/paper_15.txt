A Novel Visualization System for Expressive Facial Motion Data
Exploration
Tanasai Sucontphunt∗
Computer Graphics and
Interactive Media Lab
Department of CS
University of Houston
http://graphics.cs.uh.edu

Xiaoru Yuan†
Laboratory of Machine
Intelligence
School of EECS
Peking University

A BSTRACT
Facial emotions and expressive facial motions have become an intrinsic part of many graphics systems and human computer interaction applications. The dynamics and high dimensionality of facial motion data make its exploration and processing challenging.
In this paper, we propose a novel visualization system for expressive facial motion data exploration. Based on Principal Component Analysis (PCA) dimensionality reduction on anatomical facial
sub regions, high dimensional facial motion data is mapped to 3D
spaces. We further rendered it as colored 3D trajectories and color
represents different emotion. We design an intuitive interface to
allow users effectively explore and analyze high dimensional facial motion spaces. The applications of our visualization system on
novel facial motion synthesis and emotion recognition are demonstrated.
Keywords: Motion Visualization, Emotion, Facial Expression,
Facial Motion Capture, Principal Component Analysis, High Dimensional Data Visualization.
Index Terms: H.5.2 [Information Interfaces and Presentation]:
User Interfaces—Graphical user interfaces
1

Qing Li‡
Computer Graphics and
Interactive Media Lab
Department of CS
University of Houston
http://graphics.cs.uh.edu

Zhigang Deng§
Computer Graphics and
Interactive Media Lab
Department of CS
University of Houston
http://graphics.cs.uh.edu

In this work, we propose a novel visualization system for expressive facial motion data exploration. First we collected high-fidelity
3D facial motions of an actress by placing hundreds of facial markers on her face using an optical motion capture system. The captured subject was directed to speak a designed corpus with various
facial emotions. We divide the whole face into six anatomical facial
sub regions (each facial marker belongs to one sub region). Then
for the facial markers in each facial sub region, we apply the Principal Component Analysis (PCA) and 3D rendering technique to
create a trail of 3D expressive facial motion space. In the 3D expressive facial motion trail, different emotions are represented as
different colors. A recorded facial motion sequence is visualized
as an intuitive 3D trajectory by our visualization system, as shown
in Figure 1. Figure 1 shows a snapshot of this running visualization system. In this work, we also demonstrate several versatile
applications of our novel visualization system, including visually
exploring facial expression space, novel facial motion synthesis by
visual interaction, and emotion recognition from facial motion sequences. To our knowledge, our proposed visualization system for
dynamic and high dimensional expressive facial motions is the first
attempt in the field. It opens up a new way to visually explore and
understand the sophisticated nature of dynamic facial expressions.

I NTRODUCTION

Facial expressions and realistic facial motions are not only challenging topics in the computer graphics and human computer interaction community, but also important to other fields, including
artificial intelligence, cognitive science, psychology, communication, etc. As facial expressions and digital humans become more
and more prevalent in many applications, understanding the visual space of expressive facial motions is clearly a priority. However, despite this growing area of research in the past several
decades [23, 21, 10, 4, 8, 7], there currently is not a systematic
methodology to understand and visualize expressive facial motion
space, which is intrinsically high dimensional and complex.
A human face is arguably the most complex muscular region of
the human body, and human facial motion is the consequence of
subtle skin deformation supported by the relaxation/stress of tens of
hidden facial muscles. The high dimensionality of facial motions
and the unresolved interplay between facial emotions and mouth
movement impose grand challenges to intuitive visualization of expressive facial motions. Meanwhile, in recent years how to intuitively visualize high dimensional spatial-temporal data has caught
a great deal of attention from the visualization community.
∗ e-mail:

tanasai@cs.uh.edu

† e-mail:xiaoru.yuan@gmail.com
‡ e-mail:qingli12@cs.uh.edu
§ e-mail:zdeng@cs.uh.edu

IEEE Pacific Visualisation Symposium 2008
4 - 7 March, Kyoto, Japan
978-1-4244-1966-1/08/$25.00 ©2008 IEEE

Figure 1: A snapshot of the running 3D visualization system for expressive facial motion data exploration. The six windows (from top
to bottom, from left to right) are for forehead region, left eye region,
right eye region, left cheek region, right cheek region, and mouth/jaw
region. Here, red for anger, green for happiness, and blue for sadness.

The remainder of this paper is organized as follows: Section 2
reviews related work on facial animation and temporal-spatial data
visualization. Section 3 briefly introduces how expressive facial
motion data are acquired in this work. Section 4 describes the details of our approach, including region-based data reduction and the
3D visualization generation. Section 5 demonstrates several applications of our visualization system, including visually exploring
facial expression space, novel facial motion synthesis by visual interaction, and emotion recognition from facial motion sequences.
Section 6 discusses advantages and limitations of our system and
concludes this paper.

103

2

R ELATED W ORK

Since the computer facial animation was introduced in 1972 [22],
various facial animation and modeling techniques have been developed. In particular, in recent years data-driven facial animation has drawn a lot of interests from the community. The datadriven facial animation generally makes use of a pre-collected facial motion database for novel animation synthesis and editing applications, including learning statistical models from facial motion
dataset [10, 4, 8] or recombining recorded frames to satisfy novel
acoustic speech input [2, 19, 7]. For example, the Facial Action
Coding Systems (FACS) proposed by Ekman and Friesen [9] is
a widely used system to represent various human expressions by
combining basic facial action units. Wang et al. [24] have proposed
a framework for the facial expression embedding and the style separation across different persons. The embedding in their work is
based on the LLE techniques.
Since each frame of the captured facial motion data contains
hundreds of 3D markers, such data can also be regarded as timevarying, high dimensional or multivariate data. Many techniques,
such as glyphs [1, 18], scatterplot matrices [5], parallel coordinates [12, 13, 25], and pixel-oriented methods [15], have been developed to visualize and explore high dimensional data sets. In such
techniques, dimensions are positioned in one- or two-dimensional
arrangements on the screen.
Multi-resolution approaches [28, 11, 30] are used to group the
data into hierarchies and display them at a desired level of detail.
These approaches do not retain all the information in the original
data, since many details will be filtered out at low resolutions.
High dimensionality is another source of clutter. Many approaches currently exist for dimension reduction. Principal Component Analysis [14], Multi-dimensional Scaling [17, 26], and
Self Organizing Maps [16] are popular dimensionality reduction
techniques used in data and information visualization. Yang et
al. [31, 32] proposed a visual hierarchical dimension reduction technique that creates meaningful lower dimensional spaces with representative dimensions from the original data space instead of generating new dimensions. These techniques generate a lower dimensional subspace to reduce clutter but some information in the original data space is also lost.
Visualizing time varying data sets is common for volumetric
data [20]. Recently researchers have developed methods for visualizing time varying volume data without relying on animation [29].
Raniel and Chen [6] use volume rendering techniques to visualize videos. In our work, we first convert high dimensional data to
multiple 3D points with PCA. The resulting points form a 3D trail
which can be rendered for a variety of applications.

Figure 2: The left shows a facial motion capture system, the middle
is a snapshot of the captured actress. In the right panel, blue and red
points represent the 102 captured markers, where the red points are
the 90 markers used for this work.

to calculate head motion. A neutral pose with closed mouth was
chosen as a reference frame and was packed into a 90 × 3 matrix,
y. For each motion capture frame, a matrix xi was created using the
same marker order as the reference. After that, the Singular Value
Decomposition (SVD), UDV T , of matrix yT xi was calculated.
yT xi = UDV T

(1)

Finally, the product of VU T gave the rotation matrix, R.
R = VU T
4

(2)

O UR A PPROACH

In this section, we describe how to transform the original 270
(90 × 3) dimensional facial motion data to a 3D interactive visualization system. This approach consists of two major steps: regionbased dimension reduction and 3D visualization. In the regionbased dimension reduction step, we first partition the whole face
into six anatomical facial sub regions. The 3D positions (xyz) of
the markers in each facial sub region is concatenated to form a single vector whose dimensionality is further reduced using the principal component analysis. In this work, we experimentally set the
reduced dimensionality to three, as such, each concatenated vector
is transformed to a 3D point at the space spanned by the largest
three eigen-vectors. In the 3D visualization stage, given the transformed 3D points (each point corresponds to a facial motion on a
specific facial region), we use efficient visualization techniques to
characterize different expressive facial motions in colors (red for
anger, green for happiness, and blue for sadness respectively). Finally, users can navigate through this 3D visualization system interactively and perform dynamic data explorations (Section 5).
4.1 Region-based Dimension Reduction

3

E XPRESSIVE FACIAL M OTION DATA ACQUISITION

To acquire high-fidelity expressive facial motion data, we captured
facial motions of an actress using a VICON motion capture system
(the left panel of Fig. 2). The actress with 102 markers on her face
was directed to speak a designed corpus several times, and each
repetition was spoken with a different facial emotion. In this work,
totally three basic emotions are considered (happiness, anger and
sadness).
The facial motion data was recorded with a 120 frames/second
rate. Because of tracking errors caused by rapid, large head movement and the removal of unnecessary facial markers, we used only
90 of 102 markers for this work. The 90 markers were fully tracked.
Figure 2 shows a facial motion capture system, the 102 captured
markers and the 90 kept markers. The motion frames for each corpus repetition are labeled with the intended emotion.
After data capture, we normalized the facial motion data (removed head movement from the data). All the markers were translated so that a specific marker was at the local coordinate center of
each frame. Then a statistical shape analysis method [3] was used

104

Figure 3: Illustration of region-based dimension reduction for expressive facial motion data. The left panel shows how the face is divided
into six regions, the middle panels shows the 3D position of markers
in one region is packed into a vector, and the right panel shows the
concatenated vector is reduced to a 3D point in the reduced PCA
space.

Figure 3 shows the process of region-based dimension reduction for the recorded expressive facial motion data. In this step, we

divide all the markers into six groups - each corresponds a facial
region (refer to the left panel of Fig. 3). These six facial regions
are forehead region, left/right eye region, left/right cheek region,
and mouth region. Some markers are not positioned in symmetry.
Then, we concatenate 3D positions of markers in one facial region
into a vector. These vectors {Xi } are put as a matrix D, and Xi is a
column of D. Singular Value Decomposition to generate three new
matrices of U, S, and V , as shown in Eq. 3 to Eq. 5 is then used. The
result of this process is that one high dimensional vector Xi (motion
for a facial sub region) is transformed into a 3D point, Ci (refer to
the right panel of Figure. 3).
⎛
⎜
⎝

X1
X2
...
Xn

⎞T

⎛

⎟
⎜
T
⎠ = U.S.V = ⎝

U1T
U2T
.
UmT

EigMX =

⎞T

⎛

⎟
⎜
⎠ ×⎝

U1

U2

s1
0
.
0

0
s2
.
.

.
0
.
0

0
.
.
sm

⎞
⎟
T
⎠ ×V

U3

(4)

C = EigMX T .(Xi − µ )

(5)

(3)

Here, µ represents the mean vector of {Xi }, each column of
EigMX is a retained eigen matrix, and C is the reduced vector (PCA
coefficient). In this work, we experimentally set the reduced dimensionality to three due to the following reasons:
• Because we apply PCA to an anatomical facial sub region (not
the whole face), and hence reducing dimensionality to three
still can keep more than 90% of the motion variation.
• It is much more intuitive to interactively navigate and explore
a 3D visualization space than higher dimensional spaces.
4.2 3D Visualization
In this section, we describe how to use efficient 3D rendering techniques to create an intuitive 3D space for expressive facial exploration and interaction. Now we have transformed a facial motion
frame to six 3D points and each of them represents its motion at a
specific facial sub region. If these 3D points are simply plotted as
dots in 3D spaces, their results will be like the right panel of Fig. 3.
To provide intuitive interfaces for data exploration, we convert point
clouds to more appealing representations.
We are aware the emotion category of any recorded motion capture sequence (the captured subject was directed to intentionally
speak with specified emotions), therefore, we know the emotion
label for any recorded facial motion capture frame. In this 3D visualization step, we assign a different color for each emotion (red for
anger, green for happiness, and blue for sadness). We conducted experiments on three different visualization techniques: simple line,
volume rendering, and 3D tube rendering.
For simple line, we only connect points of the same motion sequence together as a line and color them based on their emotion
categories (Figure 4 (a)). We will discuss in more details on 3D
Volume rendering and 3D Tube rendering in the follow-up subsections.
4.2.1 3D Volume Rendering
In the volume rendering technique (Figure 4 (b)), first, we divide
the 3D data into a 3D volume grid. In the implementation of this
work, we use 2003 = 8,000,000 grids. From assigned colors (red for
anger, green for happiness, and blue for sadness), we also assume
that each 3D point (located in a specific grid) will affect (or propagate to) its neighboring grids with an attenuation function. This
attenuation function is modeled as a Gaussian function. As such,
the intensity of any grid is the summation of propagation from its
neighboring 3D points. Each grid is colored by blending of the

Figure 4: A comparison result of three visualization techniques of (a)
simple line, (b) volume rendering, (c) 3D tube rendering with alpha
values, and (d) solid 3D tube rendering.

105

colors of all points located in the grid and the propagations from
neighboring points. For color blending, we use the alpha blending
technique. Each time when a point identified to be in a voxel it
will color the grid with its emotion color and also the alpha value.
Hence, in the final volume visualization results, color of each voxel
represents the meaning of emotion, and its intensity is proportional
to the probability of the occurrence of this voxel.
4.2.2 3D Tube Rendering
In the 3D tube rendering technique (Figure 4 (c) and (d)), we shield
each simple line connection with a cylinder and shade it with the
Phong shading to make it more visually intuitive.
We use the cylinder with constant radius along all trajectories.
Each cylinder is placed on each line segment by rotating the cylinder with the same angle as the line segment aligned on a 3D space.
For shading style, we use simple Phong shading. Each cylinder is
rendered by color material assigned by emotion colors with proper
ambient, diffusion and specular parameters. In our experiment, we
use one light source located on the top with 0.2 for ambient parameter (Ka ), 0.7 for diffusion parameter (Kd ), and 0.7 for specular parameter (Ks ). All the parameters are the same for each RGB
channel.
For this 3D tube rendering, we did two different experiments:
one uses alpha values and the other uses solid material. Both semitransparent and opaque alpha values for 3D tube rendering have
been test, as shown in Figure 4(c) and (d) respectively. In Figure 4
(c), we use 3D tube with alpha values. Also, in Figure 4 (d), we
apply solid material instead of using alpha values. As we can see
in the figure, the solid material-based rendering tube yields more
tangibility on the colored trail trajectory.
4.2.3 Result comparison
Through experiment comparisons, we found that the solid 3D tube
rendering generates the most appealing visualization as shown in
Figure 4. The 3D trails with shading generated by the 3D tube rendering technique also clearly illustrate the trajectory trend of each
data set. Figure 5 shows final visualized results for one facial region. The tubes color coded with red, green and blue correspond to
the emotion of angry, happy and sad respectively. The white dots
are selected frames shown at the bottom row.
5 A PPLICATIONS OF O UR V ISUALIZATION S YSTEM
In this section, we describe how to explore expressive facial motion
space based on this developed 3D visualization system. Specifically, we will describe the following applications: visually exploring facial expression space, novel facial motion synthesis by visual
interaction, and emotion recognition from facial motion sequences.
5.1 Visually Exploring Facial Expression Space
As shown in Figure 1 and 5, recorded facial motions of each facial
region are visualized in a separate 3D window. Hence, users can
conveniently employ standard 3D navigation interfaces to interactively explore the visualized facial expression spaces. For example,
users can rotate and zoom the 3D trajectories in any of the above
visualization windows. Given a new facial motion sequence, the
users can intuitively check its corresponding 3D trajectories in the
six visualization windows. Additionally, users can visually see how
different facial expressions are differentiated in these spaces, and
visually tell which emotion pairs are often intersected each other. If
a pair of emotions is always easily intersected, it means that facial
motions (of these two emotions) are visually similar and may cause
visual confusions.
5.2 Novel Facial Motion Synthesis by Visual Interaction
Generating novel expressive facial motions has been one of challenging topics in the animation community for decades, and most

106

Figure 5: Visualization results for one facial region (right cheek) using
the solid 3D tube technique. Red, green and blue correspond to the
emotion of angry, happy and sad respectively.

of the proposed approaches heavily rely on some optimized search
or machine learning models; however, little work can synthesize
novel expressive facial motions via intuitive and simple visual operations. This novel visualization system can be employed for generating novel expressive facial motions by intuitive visual operations.
Users can pick any 3D motion trajectory from the six visualization windows. These six selected motion trajectories (each point
of these trajectories represents three retained PCA coefficients) can
be transformed back to the motion marker space, and finally motion
markers are put together to generate a novel facial animation frame.
Conceptually, this mapping can be regarded as the inverse of the
process illustrated in Figure 2. Taking into consideration that the
frame numbers of the selected trajectories are different, we use a
motion time-warping technique [27] to normalize all selected trajectories to have the same number of frames. In this work, a facial
animation preview window shows its synthesized facial animation
when the six 3D trajectories are picked (Figures 6 and 7). It should
be noted that users not only can pick any of existing 3D trajectories
in the six visualization windows, but also can use 3D drawing tools
to create novel 3D trajectories in each of the visualization windows.
5.3 Emotion Recognition via Visual Inspection
With our visualization system, users can recognize emotion category of novel captured facial motions via intuitive visual inspection. In motion capture practice, typically hundreds of thousands
of motion frames are recorded. However, not all of motion data
have clear emotion labels or the emotion labels of some facial motion sequences are lost. In this case, various pattern recognition and
machine learning methods certainly can be tried. Our visualization
system provide an alternative to recognize emotion category of any
facial motion sequence by intuitive visual inspection. After loading
a new facial motion sequence and visualizing it as a 3D trajectory
(with a different color), users can visually inspect which emotion

Figure 6: The first example of novel facial motion synthesis by visually picking 3D trajectories in visualization windows. Here heavy white
trajectories represent selected ones. The right shows the synthesized face.

Figure 7: The second example of novel facial motion synthesis by visually picking 3D trajectories in visualization windows. Here heavy white
trajectories represent selected ones. The right shows the synthesized face.

107

(colored) space this trajectory pass through in the six visualization
windows (Figure 1). Finally, the users can easily judge its emotion label. This conceptually intuitive visualization offers a novel
way for emotion perception and recognition applications. Figure 8
shows a snapshot of the running visualization system for emotion
recognition, where the white trajectories are the ones corresponding to a test facial motion sequence. To illustrate the consistency, as
shown in figure 9, we use 80% of the original data set as the training
set and the remaining 20% as the testing facial motion sequences.
The testing facial motion sequences used in the experiment are the
happy emotion. The experiment shows that the locations of the testing facial motion trajectories are highly overlapped with the happy
emotion of the training facial motion trajectories.
6

D ISCUSSION AND C ONCLUSIONS

In this paper, we propose a region-based 3D visualization system
for expressive facial motion data exploration. We collected highfidelity 3D facial motions of a selected human subject using a motion capture system. We divide the data into six anatomical sub regions, and project data into 3D spaces, which are further rendered
as 3D tubes. In this visualization system, different emotions are
represented as different colors.
Based on this visualization system, we demonstrated several important applications including navigating the 3D facial expression
spaces, novel facial motion synthesis by visual interactions, and
emotion recognition from facial motion sequences via visual inspection. The visual cue provided by our system can greatly facilitate the users to handle highly difficult tasks.
We are aware that the three basic emotions (anger, sadness, and
happiness) studied in this work are not enough to cover all types
of emotions [9], e.g. fear, surprise, and disgust are not covered in
this work. In future we plan to examine a wider array of emotions.
Furthermore, a major limitation of the current work is that we only
collected expressive facial motion data from a single subject. In
order to fully generalize our findings, we plan to extend this work
with a number of male and female subjects. However, the purpose
of this work is to validate our methodologies, and we found novel
and intuitive applications for our approach.
A number of questions remain open. For example, if expressive
facial motion data of multiple subjects are captured and rendered
into one space, how different would these visualizations be? And
does our visualization system reflect the personalities of captured
subjects somehow? In the future, we plan to try other type of visualization techniques, such as parallel coordinate [12, 13, 25] for
visualizing captured expressive facial motion data. In this work, the
motion embedding is based on a linear PCA technique. As the facial skin deformations are non-rigid, we plan to consider non-linear
techniques in the future research.
R EFERENCES
[1] D. F. Andrews. Plots of high dimensional data. Biometrics, 28:125–
136, 1972.
[2] C. Bregler, M. Covell, and M. Slaney. Video rewrite: Driving visual
speech with audio. Proc. of ACM SIGGRAPH’97, pages 353–360,
1997.
[3] C. Busso, Z. Deng, U. Neumann, and S. Narayanan. Natural head
motion synthesis driven by acoustic prosody features. Computer Animation and Virtual Worlds, 16(3-4):283–290, July 2005.
[4] E. Chuang and C. Bregler. Moodswings: Expressive speech animation. ACM Trans. on Graph., 24(2):331–347, 2005.
[5] W. S. Cleveland and M. E. McGill. Dynamic Graphics for Statistics.
Wadsworth, Inc., 1988.
[6] G. Daniel and M. Chen. Video visualization. In IEEE Visualization
2003, page 54, Washington, DC, USA, 2003. IEEE Computer Society.
[7] Z. Deng and U. Neumann. efase: Expressive facial animation synthesis and editing with phoneme-level controls. In SCA’06, pages 251–
259, Vienna, Austria, 2006. Eurographics Association.

108

[8] Z. Deng, U. Neumann, J. P. Lewis, T. Y. Kim, M. Bulut, and
S. Narayanan. Expressive facial animation synthesis by learning
speech coarticulation and expression spaces. IEEE Transactions on
Visualization and Computer Graphics, 12(6):1523–1534, 2006.
[9] P. Ekman and W. Friesen. Unmasking the Face: A Guide to Recognizing Emotions from Facial Clues. The Prentice-Hall Press, 1974.
[10] T. Ezzat, G. Geiger, and T. Poggio. Trainable videorealistic speech
animation. ACM Trans. Graph., pages 388–398, 2002.
[11] Y. Fua, M. Ward, and E. Rundensteiner. Hierarchical parallel coordinates for exploration of large datasets. In Proc. IEEE Visualization’99,
pages 43–50, 1999.
[12] A. Inselberg. The plane with parallel coordinates. The Visual Computer, 1:69–91, 1985.
[13] A. Inselberg and B. Dimsdale. Parallel coordinates: a tool for visualizing multi-dimensional geometry. In IEEE Visualization ’90, pages
361–378, 1990.
[14] J. Jolliffe. Principal Component Analysis. Springer-Verlag, 1986.
[15] D. A. Keim. Pixel-oriented visualization techniques for exploring very
large databases. Journal of Computational and Graphical Statistics,
5(1):58–77, 1996.
[16] T. Kohonen. The self-organizing map. In Proc. of IEEE, pages 1464–
1480, 1978.
[17] J. Kruskal and M. Wish. Multidimensional Scaling. Number 07-011.
Beverly Hills and London: Sage Publications, 1978.
[18] R. Littlefield. Using the glyph concept to create user-definable display
formats. In Proc. NCGA, pages 697–706, 1983.
[19] J. Ma, R. Cole, B. Pellom, W. Ward, and B. Wise. Accurate visible
speech synthesis based on concatenating variable length motion capture data. IEEE Transactions on Visualization and Computer Graphics, 12(2):266–276, 2006.
[20] K. L. Ma. Visualizing time-varying volume data. IEEE Computer in
Science and Engineering, 5(2):34–42, March/April 2003.
[21] C. Nass, E. Kim, and E. Lee. When my face is the interface: An experimental comparison of interacting with one’s own face or someone
else’s face. In Proc. of CHI’98, pages 148–154, 1998.
[22] F. Parke. Computer generated animation of faces. In Proc. ACM National Conference, volume 1, pages 451–457, 1972.
[23] J. Walker, L. Sproull, and R. Subramani. Using a human face in an
interface. In Proc. of CHI’94, pages 85–91, 1994.
[24] Y. Wang, X. Huang, C. S. Lee, Z. L. S. Zhang, D. M. D. Samaras,
A. Elgammal, , and P. Huang. High resolution acquisition, learning
and transfer of dynamic 3-d facial expressions. In Proc. of the European Association for Computer Graphics, 23(3), 2004.
[25] E. J. Wegman. Hyperdimensional data analysis using parallel coordinates. Journal of the American Statistical Association, 411(85):664–
675, 1990.
[26] S. J. Weinberg. An introduction to multidimensional scaling. Measurement and evaluation in counseling and development, (24):12–36,
1991.
[27] A. Witkin and Z. Popovic. Motion warping. In Proc. of Siggraph’95,
1995.
[28] P. Wong and R. Bergeron. Multiresolution multidimensional wavelet
brushing. In Proc. IEEE Visualization, pages 141–148, 1996.
[29] J. Woodring and H.-W. Shen. Chronovolumes: a direct rendering technique for visualizing time-varying data. In VG ’03, pages 27–34, New
York, NY, USA, 2003. ACM Press.
[30] A. Woodruff, J. Landay, and M. Stonebraker. Constant information
density in zoomable interfaces. In Proc. of the working conference on
Advanced visual interfaces, pages 57–65, 1998.
[31] J. Yang, W. Peng, M. Ward, and E. Rundensteiner. Interactive hierarchical dimension ordering, spacing and filtering for exploration of
high dimensional datasets. In Proc. of IEEE InfoVis, pages 105–112,
2003.
[32] J. Yang, M. Ward, E. Rundensteiner, and S. Huang. Visual hierarchical
dimension reduction for exploration of high dimensional datasets. In
Eurographics/IEEE TCVG Symposium on Visualization, pages 19–28,
2003.

Figure 8: A snapshot of the running visualization system for emotion recognition, where the white trajectory is the one corresponding to a novel
test facial motion sequence (happy).

Figure 9: A snapshot of the running visualization system for emotion recognition, where the white trajectories are the another 20% of the data
set corresponding to a novel test facial motion sequence (happy).

109

