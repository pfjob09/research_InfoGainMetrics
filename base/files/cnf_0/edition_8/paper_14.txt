MULTIDIMENSIONAL DETECTIVE
Alfred Inselberg, Multidimensional Graphs Ltdy
&
Computer Science Department
Tel Aviv University, Israel
aiisreal@math.tau.ac.il

Abstract

fast, was the inspiration in 1959 for ”Parallel” Coordinates. The systematic development began in
1977 [4]. The goals of the program were and still
are (see [6] and [5] for short reviews) the visualization of multivariate/multidimensional problems
without loss of information and having the properties:

T he display of multivariate datasets in parallel

coordinates, transforms the search for relations
among the variables into a 2-D pattern recognition
problem. This is the basis for the application to
Visual Data Mining. The Knowledge Discovery
process together with some general guidelines are
illustrated on a dataset from the production of a
VLSI chip. The special strength of parallel coordinates is in modeling relations. As an example,
a simplified Economic Model is constructed with
data from various economic sectors of a real country. The visual model shows the interelationship
and dependencies between the sectors, circumstances where there is competition for the same
resource, and feasible economic policies. Interactively, the model can be used to do trade-off analyses, discover sensitivities, do approximate optimization, monitor (as in a Process) and Decision
Support.

1. Low representational complexity.
Since
the number of axes, N equals the number
of dimensions (variables) the complexity is
ON ,
2. Works for any N,
3. Every variable is treated uniformly (unlike ”Chernoff Faces” and various types of
”glyphs”),
4. The displayed object can be recognized under projective transformations (i.e. rotation,
translation, scaling, perspective),
5. The display easily/intuitively conveys information on the properties of the Ndimensional object it represents,

Introduction
I n Geometry parallelism, which does not require

6. The methodology is based on rigorous mathematical and algorithmic results.

a notion of angle, rather than orthogonality is the
more fundamental concept. This, together with the
fact that orthogonality ”uses-up” the plane very

Parallel coordinates (abbr.k-coords) transform
multivariate relations into 2-D patterns, a property that is well suited for Visual Data Mining.

 Senior Fellow San Diego SuperComputing Center
y 36A Yehuda Halevy Street, Raanana 43556, Israel

1

Y

X

X1

X2

X3

X4

X5

X6

X7

X8

X9

X10

X11

X12

X13

X14

X15

X16

473 Points

Figure 1: The full dataset consisting of 473 batches
Several Data Mining tools EDA (Chomut [2]),
Finsterwalder[3], VisuLab(Hinterberger[10]), ExplorN(Carr et al), Influence Explorer(Spence &
Tweedie
[11]),
WinViZ(Eickemeyer),
VisDB(Keim [7]), Xmdv(Ward[8]), XGobi(Buja),
Strata(k-coords by Gleason), Diamond(Rogowitz
et al [9]), PVE(Inselberg, Adams, Hurwitz, Chatterjee, Austel) etc. include k-coords. Here we focus on the Data Mining application and describe:

result, software usually mimic the experience derived from the standard and more familiar displays
(i.e. not the dual), rather than exploit the special
strengths of the methodology and avoid its weaknesses.
Without the proper geometrical understanding
and queries, the effective use of k-coords becomes
limited to small datasets. By contrast, skillful application of the methodology’s strengths enables
the analysis of datasets consisting of thousands of
A scenario for the discovery process,
points and hundreds of variables. The intent here,
Guidelines for using k-coords in Data Min- is not to elaborate on the design and implementation criteria but rather to provide some insights
ing, and
on the ”discovery process”. The paradigm is that
The construction and use of visual models for of a detective, and since many parameters (equivalently dimensions) are involved we really mean a
multivariate relations.
”multidimensional detective”.
There are certain basics (see references) which
have important ramifications. For example, due
to the Point ! Line duality, some actions are
best performed in the dual and their opposite
in the original representation. Another important matter is the design of queries. The task
is akin to accurately cutting complicated portions
of an N-dimensional ”watermelon” (i.e. the Ndimensional representation of the dataset). The
”cutting tools” are the queries which must also operate in the dual (i.e. the k-coords display). They
need to be few, exquisitely well chosen and intuitive. This requires an efficient and convenient
way of combining the ”atomic” queries to form
complex queries, corresponding to more intricate
”cuts” of the dataset; and there are other issues. Figure 2: The batches high in Yield, 1, and
These points are not often appreciated and, as a Quality, 2.
Y

X

X1

X2

X3

X4

X5

X6

X7

X8

X9

X10

X11

X12

X13

X14

X15

18 Points

X

2

X

X16

Y

X

X1

X2

X3

X4

X5

X6

X7

X8

X9

X10

X11

X12

X13

X14

X15

the variables’ scales of X 3 through X 12 are inverted so that 0 (zero) amount appears at the top.
The remaining, X 13 through X 16, denote some
physical parameters. We emphasize that this is a
real dataset and in order to protect the innocent,
as well as confuse the competition, it is not possible to give a more explicit description or show
numerical values; for our purposes it is also not
necessary. Prior to embarking on our exploration
it is essential to
understand the objectives and use them to
obtain ”visual cues”.

X16

9 Points

Figure 3: The batches with zero in 9 out of the ten Here the objective is to raise the yield, X 1, and
maintain high quality, X 2, a multiobjective opdefect types.
timization (since more than one objective is involved). Production experts believed that it was
the presence of defects which hindered high yields
The Problem
and qualities. So the goal was to achieve zero defects.
Aside from starting the exploration without bias,
together with some healthy scepticism about the
”convictions” of the domain experts, the first ad- Discovery Process - How to be a Multimonition is:
dimensional Detective

T he keen observer can ascertain from Fig. 1 the
distributions, with X 2 being somewhat bipolar
(having higher concentrations at the extremes),
and X 1 having something like a normal distribution about it’s median value. This brings us to the
next admonition. Namely, no matter how messy it
looks,

do not let the picture intimidate you,

as can easily happen by taking an uninformed look
at Fig. 1 where our subject’s dataset is displayed.
It pertains to the production data of 473 batches
of a VLSI chip with measurements of 16 process
parameters denoted by 1 X 2 : : : X 16. The
yield, as the % of useful chips produced in the
carefully scrutinize the picture
batch, is denoted by X 1, and X 2 is a measure
of the quality (in terms of speed performance).
and you are likely to find some patterns, let’s
Ten different types of defects are monitored and
call them visual cues, which hint at the relations
among the variables.
We embark on our quest and the result of our
first query is shown in Fig. 2 where the batches
having the highest X 1 and X 2 have been isolated. This in an attempt to obtain clues; and
two real good ones came forth (the visual cues
we spoke of). Notice X 15 where there is a separation into two clusters. As it turns out, this
gap yielded important (and undiscloseable) insight
into the physics of the problem.
The other clue is almost hidden. A careful comparison – and here interactivity of the software is
essential – between Fig. 1 and Fig. 2 shows that
some batches which were high in X 3 (i.e. and due
to the inverted scale low in that defect) were not
Figure 4: The batches with zero in 8 out of the ten
included in the selected subset. That casts doubt
defect types.
into the belief that zero defects are the panacea,

X

Y

X

X1

X2

X3

X4

X5

X6

X7

X8

X9

X10

X11

X12

X13

X14

X15

X16

21 Points

3

Y

Y

X

X1

X2

X3

X4

X5

X6

X7

X8

X9

X10

X11

X12

X13

X14

X15

1 Points

Figure 5: The best batch. Highest in Yield,
and very high in Quality, 2.

X

X16

X

X1

X 1,

X2

X3

X4

X5

X6

X7

X8

X9

X10

X11

X12

7 Points

X13

Figure 7: Upper range of split in

X14

X 15

X15

X16

Y

Y

X
X

X1
X1

X2

X3

X4

X5

X6

X7

X8

X9

X10

X11

X12

X13

X14

X15

X

X

X5

X6

X7

X8

X9

X10

X11

X12

X13

X14

X15

X16

X 15

finding, but perhaps this is due to measurement errors in that one data item. We return to the full
dataset and isolate the cluster of batches with the
top yields (note the gap in 1 between them and
the remaining batches). These are shown in Fig. 6
and they confirm that small amounts (the ranges
can be clearly delimited) of 3 and 6 type defects are essential for high yields and quality. The
moral of the story is

and motivates the next query where we search for
batches having zero defects in at least 9 (excluding
3 where we saw that there are problems) out of
the 10 categories. The result is shown in Fig. 3 and
is a shocker. There are 9 such batches and all of
them have poor yields and for the most part also
low quality! What is one to do? Refer again to
the previous admonition and scrutinize the original picture Fig. 1 for visual cues relevant to our objectives and our findings so far. And ... there is one
staring us in the face – the visual difference between 6 and the other defects. It shows that the
process is much more sensitive to variations in 6
than the other defects. We chose to treat 6 differently and remove its zero defect constraint. The
result is seen in Fig. 4 and, remarkably, the very
best batch (i.e. highest yield with very high quality) is included. This is an opportunity to learn,
so when that batch is highlighted with a different
color (can’t be seen in black and white) it does
not have zeros (or the lowest values) for 3 and
6 as shown separately in Fig. 5. A ”heretical”

X

X4

Figure 8: Batches with the lower range of

X

X

X3

11 Points

Figure 6: Batches with the highest Yields do not
have the lowest defects in 3 and 6.

X

X2

X16

5 Points

X

X

X

test the assumptions and especially the ”I
am really sure of ...”s.

X

Let us return to the subset of data which best satisfied the objectives, Fig. 2, to explore the gap in
the range of 15. In Fig. 7 we see that the cluster with the high range of 15 gives the lowest
(of the high) yields 1, and worse it does not give
consistently high quality 2. Whereas the cluster corresponding to the lower range, Fig. 8, has
the higher qualities and the full range of the high
yield. It is evident that the small ranges of 3,
6 close to (but not equal to) zero, together with

X

X

X

X

4

X
X

X

Y

Y

X

X1

X2

X3

X4

X5

X6

X7

X8

X9

X10

X11

X12

X13

X14

X15

X

X16

X1

22 Points

X2

X3

X4

X5

X6

X7

X8

X9

X10

X11

X12

X13

X14

X15

X16

7 Points

Figure 9: Top Yields produce split in

Figure 11: Top Yields, note two batches with
lower Quality

X 15

X

the short (lower) range of 15 provide necessary
amounts of 3 and 6 and lower range of 15.
conditions for obtaining high yields and quality.
This bit of serendipity provides an instance of my
Using a characterization algorithm it can be shown
favorite ”stochastic” theorem :
that these conditions are also sufficient. Given any
subset of the data the algorithm finds:
you can’t be unlucky all the time!

X

1. the smallest subset of variables which describe the data without loss of information,
and

X

X

Some Deeper Insights

W

X

hy the gap in 15? It was obtained by impos2. orders the variables in terms of their predic- ing simultaneously the constraints for top yields
tive power.
and quality. Fig. 9 shows the result of constraining
only 1 and the resulting gap in 15, whereas the
In our case these are the 3 parameters 3, 6 high 2 by itself does not yield a gap as shown in
and 15 which are needed for the characteriza- Fig. 10. And this, I am told, provided further intion of a very good batch. By a stroke of good sights into the physics of the problem.
luck these 3 can be checked early in the process
Just for fun we look next into the very top yields
avoiding the need of ”throwing good money after
Fig. 11 and see that except for two batches the othbad”. Looking again at Fig. 1 we notice a gap in
ers also have very high 2. Isolating the lower
1 between the top 5 batches and the rest. The
quality batches turns out to be very informative.
high cluster consists of only those having the small
The picture, Fig. 12, suggests that high yields and

X
X

X X

X

X

X

X
Y

Y

X
X

X1

X2

X3

X4

X5

X6

X7

X8

X9

X10

X11

X12

X13

X14

X15

X16
X1

186 Points

Figure 10: High

X 2 does not cause split in X 15

X2

X3

X4

X5

X6

X7

X8

X9

X10

X11

X12

X13

2 Points

Figure 12: The two batches with high
lower 2

X

5

X14

X15

X16

X 1 and

Y
547

52.25

572.8

1284

302.1

358.8

1977

4859

Y
547

52.25

572.8

1284

302.1

358.8

1977

4859

X

291.4
Ag

-3.274
Fsh

264.1
Min

452
Man

127
Conc

216.2
Gov

961.8
Oth

2676
Gnp

291.4
Ag

X

-3.274
Fsh

264.1
Min

Figure 13: Model of a country’s economy

452
Man

127
Conc

216.2
Gov

961.8
Oth

2676
Gnp

Figure 14: Competition for labor between the
Fishing & Mining sectors – compare with previlower quality may be due to the different ranges of ous figure
6, whose influence we have already seen elsewhere, and specific ranges of 13, 14, 15,
16. This observation suggests that it may be among the variables in multivariate datasets. The
possible to partition this multivariate problem into real strength of the methodology is the ability to
sub-problems pertaining to the individual objec- construct and display such relations in terms of
tives. The wide variation of 9 for these two hypersurfaces – just as we model a relation bebatches, as seen in Fig. 12, lead to further testing tween two variables by a planar region. Then
and the conclusion that 9 is a ”junk variable” by using an interior point algorithm with the
with respect to the objectives. The observations model we can do trade-off analyses, discover senand conclusions here involved a relatively small sitivities, understand the impact of constraints,
number of variables and should be cross-checked and in some cases do optimization. We just
with larger datasets. In general, a dataset with P want to indicate how this works. For this purpoints has 2P subsets anyone one of which can be pose we use a dataset consisting of the outputs
the ”interesting” one (with respect to the objec- of various economic sectors and other expenditives). Our approach can provide a powerful tool tures of a particular (and real) country. It confor coping with this combinatorial explosion. The sists of the monetary values over several years of
visual cues obtained can help to rapidly focus on the Agricultural output, outputs of the Fishing,
the interesting portions of the data. In this sense, it Mining, Manufacturing and Construction induscan serve as a preprocessor to other methods, in tries, together with Government, Miscellaneous
spending and resulting GNP; eight variables altoaddition to providing unique insights on its own.
After this analysis, it was revealed that this gether.
We will not take up the full ramifications of conwas a well studied problem and dataset, and our
findings differed markedly from those found with structing a model from data. Rather, we want to
other methods for process control [1]. This is not illustrate how -coords may be used as a visual
an isolated case and there have been other success- modeling tool. Using a Least Squares technique
ful application of this approach in the manufacture we ”fit” a function to this dataset; for our purposes
of printed card boards, PVC and manganese pro- here we are not concerned whether the choice of
duction, retailing, finance, trading, insurance, sea- function is ”good” or not. The specific function
sonal weather forecasts, risk analysis, determina- we obtained bounds a region in R8 and is repretion of skill profiles” (i.e. as in drivers, pilots etc) sented by the upper and lower curves(envelopes)
and elsewhere. The results frequently surprised shown in Fig. 13(the interested reader may want
to refer to the previously cited references).
the domain experts.
The picture is in effect a visual model of the
Visual & Computational Models country’s economy, incorporating it’s capabilities
and limitations, interelationships among the sece have outlined a process for discovering in- tors etc. A point interior to the region, satisfies all
teresting, with respect to the objective, relations the constraints simultaneously, and therefore rep-

X
X

X

X

X

X

X

k

W

6

Figure 15: A Convex Hypersurface in 20-D and Interior Point Algorithm.
resents (i.e. the 8-tuple of values) a feasible economic policy for that country. Using an interior
point algorithm (see previously cited references)
we can construct such points. It is done interactively by sequentially choosing values of the variables and we see the result of one such choice in
Fig. 13. Once a value of the first variable is chosen (in this case the agricultural output) within it’s
range, the dimensionality of the region is reduced
by one. The upper and lower curves between the
2nd and 3rd axes correspond to the resulting 7dimensional hypersurface and show the available
range of the second variable (Fishing) reduced by
the constraint (i.e. fixing the value of the first variable). In fact, this can be seen (but not shown here)
for the rest of the variables. That is, due to the relationship between the 8 variables, a constraint on
one of them impacts all the remaining ones and restricts their range. The display allows us to experiment and actually see the impact of such decisions
”downstream”. By interactively varying the chosen value for the first variable we found, from this
model, that low values for Agriculture correspond
to low ranges of values for Fishing, and similarly
corresponding to high values occurring together.
So it is not possible to have a policy that favors
Agriculture without also favoring Fishing and vice
versa. The algorithm fails where at any stage the
polygonal line crosses an intermediate curve, and

that is very informative.
Proceeding, a very high value from the available
range of Fishing is chosen next. It corresponds to
very low values of the Mining sector. By contrast
in Fig. 13 we see that a low value in Fishing yields
high values for the Mining sector. This inverse relation was investigated and it was found that the
country in question has a large number of migrant
workers. When the fishing industry is doing well
most of them are attracted to it leaving few available to work in the mines and vice versa. The comparison between the two figures shows the competition for the same resource between Mining and
Fishing. It is especially instructive to discover this
interactively. The construction of the interior point
proceeds in the same way.
Let us move over to Fig. 15 where the same
construction is shown but for a more complex
20-dimensional hypersurface (”model”). The intermediate curves (upper and lower) also provide
valuable information and ”previews of coming attractions”. They indicate a neighborhood of the
point (represented by the polygonal line) and provide a feel for the local curvature. Note the narrow
strips between 13, 14 and 15 (as compared
to the surrounding ones), indicating that for this
choice of values these 3 are the critical variables
where the point is ”bumping the boundary”. A theorem guarantees that a polygonal line which is in-

X X

7

X

between all the intermediate curves/envelopes rep- [5] A. Inselberg. Parallel Coordinates : A Guide
resents an interior point of the hypersurface and
for the Perplexed, in Hot Topics Proc. of
all interior points can be found in this way. If
IEEE Conf. on Visualization, 35-38. IEEE
the polygonal line is tangent to anyone of the inComp. Soc., Los Alamitos, CA, 1996.
termediate curves then it represents a boundary
point, while if it crosses anyone of the interme- [6] A. Inselberg and B. Dimsdale. Parallel Coordinates: A Tool For Visualizing Multididiate curves it represents an exterior point. The
mensional Geometry, in Proc. of IEEE Conf.
latter enables us to see, in an application, the first
on Vis. ’90, 361-378. IEEE Comp. Soc., Los
variable for which the construction failed and what
Alamitos, CA, 1990.
is needed to make corrections. By varying the
choice of value over the available range of the vari- [7] D. A. Keim and H. P. Kriegel. Visualization
able interactively, sensitive regions (where small
techniques for mining large databases: A
changes produce large changes downstream) and
comparison. Trans. Knowl. and Data Engr.,
other properties of the model can be easily dis8-6:923–938, 1996.
covered. Once the construction of a point is completed it is possible to vary the values of each vari- [8] Ward M. O. XmdvTool: integrating multiple
able and see how this effects the remaining varimethods for visualizing multivariate data,
ables. So one can do trade-off analysis in this way
Proc. IEEE Conf. on Visualization, San Jose,
and provide a powerful tool for, Decision Support,
CA, 326-333. IEEE Comp. Soc., Los AlamiProcess Control and other applications.
tos, CA, 1994.
It should be self-evident that the efficacy of a vi[9] M. Schall. Diamond and ice : Visual exsual data mining tool needs to be judged by applyploratory data analysis tools. Perspective, J.
ing it to real and necessarily challenging datasets.
of OAC at UCLA, 18(2):15–24, 1994.
Flashy demos based on artificial or small datasets
can be very impressive but misleading. Each mul- [10] C. Schmid and H. Hinterberger. Comtivariate dataset and problem has its own ”personparative Multivariate Visualization Across
ality” requiring substantial variations in the disConceptually Different Graphic Displays, in
covery scenarios and calls for considerable ingeProc. of 7th SSDBM. IEEE Comp. Soc., Los
nuity – a characteristic of good detectives. It is not
Alamitos, CA, 1994.
surprising then that the most frequent requests are
for tools to, at least partially, automate the explo- [11] L.A. Tweedie, R. Spence, H. Dawkes, and
Su H. Externalizing Abstract Mathematical
ration process. Such a development is under way
Models, Proc. CHI, Vancouver, Can., 406and will include a number of new features, includ412. ACM Press, 1996.
ing intelligent agents, gleaned from the accumulated experience.

References
[1] E.W. Bassett. Ibm’s ibm fix. Industrial Computing, 14(41):23–25, 1995.
[2] T. Chomut. Exploratory Data Analysis in
Parallel Coordinates. M.Sc. Thesis, UCLA
Comp. Sc. Dept., 1987.
[3] R. Finsterwalder. A Parallel Coordinate
Editor as a Visual Decision Aid in MultiObjective Concurrent Control Engineering
Environment 119-122. IFAC CAD Contr.
Sys., Swansea, UK, 1991.
[4] A. Inselberg. N-Dimensional Graphics, Part
I – Lines and Hyperplanes, in IBM LASC
Tech. Rep. G320-2711, 140 pages. IBM LA
Scientific Center, 1981.
8

