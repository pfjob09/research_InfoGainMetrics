Building a Visual Database for Example-based Graphics Generation
Michelle X. Zhou

Min Chen

IBM. T. J. Watson Research Center
19 Skyline Dr.
Hawthorne, NY 10532
{mzhou, minchen}@us.ibm.com
Abstract
Example-based graphics generation systems automatically create new information visualizations by learning
from existing graphic examples. As part of the effort on
developing a general-purpose example-based generation
system, we are building a visual database of graphic examples. In this paper, we address two main issues involved in
constructing such a database: example selection and example modeling. As a result, our work offers three unique contributions: First, we build a visual database that contains a
diverse collection of well-designed examples. Second, we
develop a feature-based scheme to model all examples uniformly and accurately. Third, our visual database brings
several important implications to the area of information
visualization.

1. Introduction
Automated graphics generation systems promise to
simplify developers' tasks by automatically designing visualizations based on data characteristics and user tasks [11,
15, 20]. To automatically create proper visualizations, most
existing systems use a rule-based approach [11, 15, 20].
Nonetheless, hand-crafting design rules and managing an
increasingly larger rule base could be very challenging.
Alternatively, we are exploring an example-based
approach to graphics generation. Our approach uses a casebased machine learning method [7] to create new graphics
from a database of existing graphics (examples). Upon a
user’s request (e.g., comparing two data sets), our approach
first uses a quantitative similarity measuring model to
retrieve the top-matched examples from the database [23].
The top-matched examples are then directly reused for or
be adapted to the new situation (e.g., new data).
As the success of case-based learning largely depends
on the design of the case database [7], part of our effort is
building a visual database of graphic examples. To the best
of our knowledge, the challenges and implications of constructing such a database have not been addressed before.
In this paper we address two main issues involved: example
selection and example modeling.
Example selection is concerned with the quality and
coverage of the database, which directly impact the capability of the generation system and the quality of the results.
For example, if our database included only examples

Proceedings of the IEEE Symposium on Information Visualization 2002 (InfoVis’02)
1522-404X/02 $17.00 © 2002 IEEE

Ying Feng
Dept. of Computer Science
Indiana University
Bloomington, IN 47405
yingfeng@cs.indiana.edu
designed for a specific application/domain, the system
would then not be able to synthesize effective graphics for
other applications/domains. Therefore, we have formulated
a set of criteria to guide our example selection.
Example modeling addresses how we express a diverse
collection of examples accurately and uniformly to facilitate case-based learning. Specifically, we must capture various characteristics of graphic examples for case retrieval
[23]. We thus develop a feature-based XML annotation
scheme to describe each example. This step is necessary,
since it remains a challenge for today’s technologies to
automatically obtain higher-level image properties (e.g.,
what data is encoded or what visual technique is used).
By addressing the two issues mentioned above, our
work presents three unique contributions:
1) We build the first visualization database that contains over 300 graphic examples from a wide variety of
sources. These examples range from 2D business diagrams
to 3D molecular visualization.
2) We develop a comprehensive model to express a
graphic as a collection of integrated, hierarchical data and
visual features. Our model enables us to capture richer and
finer characteristics of graphics and to perform quantitative
visual analysis using machine learning.
3) Our database also fills a long-standing need for a
benchmark to test and compare different visualization techniques, and the need for a knowledge repertoire to learn
visual design techniques and principles.
We organize the rest of our paper as follows. After a
brief review of related work, we describe our visual database construction and discuss its potential applications.

2. Related Work
In the area of multimedia retrieval, researchers have
built various visual databases of images/videos [10, 1, 16].
These images/videos come with minimal semantic annotations. In contrast, we construct a database of information
visualizations with fine-grained annotations to facilitate
semantics-based visual analysis and retrieval.
To characterize information graphics, researchers have
worked on data characterization [21] and different visual
design taxonomies [9, 11, 22, 12]. While our annotation
scheme is based in part on these previous efforts, we have
extended them to express a wider variety of properties of a
graphic, and at multiple levels of abstraction. As described
in Section 3.2, our current model also offers several significant improvements over our initial version [23].

(a) Napoleon’s long march

(b) A teapot image

(c) Statistics Map (d) Hockey penalty rules

Figure 1. Graphics samples: (a) qualified example (b–d) unqualified examples.
(a) Generated by Sage, reprinted by permission from S. Roth. (b) Copyright 1993 IEEE, reprinted by permission [6]. (c) reprinted by permission [5].
(d) Copyright 1998 TUBE Graphics, reprinted by permission [18].

There are also related efforts on example-based graphics generation. A notable system, Sage/SageBook [2], reuses
examples created by its own rule engine. Thus it does not
touch upon the issue of building a general visual database.
In contrast, we are building a typical example-based generation system that must exploit examples from different
sources. Moreover, SageBook describes the data and visual
features of its examples separately. We, on the other hand,
use an integrated scheme to express data features, visual
features, and the corresponding data-visual mappings.

3. Constructing Visual database
In this section, we describe how to choose proper
examples using a set of criteria and how to model a graphic
example as a set of data and visual features.

3.1 Example Selection
As a knowledge source for designing new graphics, our
examples directly impact the capability/quality of the generation system. Thus, we formulate a set of quality and coverage criteria to aid our example selection.

Quality Criteria
We measure the quality of examples from three aspects.
Suitability. Almost every graphic is created with a purpose
(e.g., illustrating information vs. decoration [8]). It would
not be effective to create new graphics by learning from
examples that were designed for a different purpose.
Accordingly, suitability measures how closely candidate
examples are related to our overall system goal. As our current goal is to create information visualizations, we choose
only graphics that serve the purpose of communicating
information and helping users achieve their informationseeking goals. By this criterion, for example, Figure 1(a) is
suitable, but Figure 1(b) is not.
Modelability. To allow computers to perform accurate
analysis (e.g., similarity measuring), we must model the
characteristics of each example comprehensively, including
both its data and visual properties. Modelability judges
whether a candidate graphic can be accurately described by
a defined model. For the purpose of graphics generation,
currently we select only examples that can be expressed

Proceedings of the IEEE Symposium on Information Visualization 2002 (InfoVis’02)
1522-404X/02 $17.00 © 2002 IEEE

using our feature-based model described later.
It is worth mentioning that not every graphic can be
accurately modeled due to missing information or the complexity of information. For example, we cannot model Figure 1(c) accurately, since its source [5] did not explain what
exact data the graphic intends to convey. Although all the
information needed to model Figure 1(d) is available, it is
difficult to describe each penalty pose1 precisely, which
happens to be the key characteristic of this example.

Design quality. To create useful new graphics, we use
design quality to assess how effectively candidate examples
can convey the intended information. This seems the most
obvious criterion but the most difficult one to follow due to
the subjectivity involved in measuring the effectiveness of a
visualization [11]. Currently, we qualitatively evaluate the
design quality of an example by judging whether we can
digest the intended information and how quickly we can.
Coverage Criteria
To handle heterogeneous data using an assortment of
visual techniques, our system relies on having a diverse set
of examples. Moreover, the diversity prevents our system
from learning biased designs (e.g., learning a visual technique only meant for a particular type of data/domain). In
our work, we use data and visual coverage to systematically
evaluate the diversity of examples.
Data coverage. Data coverage measures the number of
different visualization situations covered by our examples.
Here a visualization situation is characterized by the data to
be conveyed, user’s information-seeking goals, and the
environment settings (e.g., display devices used). Accordingly, we measure data coverage by evaluating the heterogeneity of these three factors. For example, Figure 1(a) and
Figure 2 differ greatly in their data and information-seeking
goals. Specifically, Figure 1(a) illustrates the Napoleon’s
long march, including the army’s locations (longitudes and
latitudes) and sizes, to help users learn the overall trend of
the event. Figure 2 depicts IBM revenues before and after
Gerstner took office to help users draw a comparison.
Visual coverage. Visual coverage evaluates the number of
1 Here we stress the difficulty of expressing graphic compositions as a set
of symbols/numbers for the purpose of machine learning.

different visual forms and styles exhibited by our examples.
In particular, the variety of visual forms measures the number of high-level visual organization formalisms. For example, Figure 1(a) and Figure 2 present two different visual
forms: a line graph and a bar chart. The variety of visual
styles, on the other hand, counts the number of different
visual techniques (e.g., highlighting an important data by
changing its color vs. by drawing a bounding box).

Compared to our previous model [23], our current one
presents two significant improvements. 1) We have modified and extended our previous feature sets to allow a more
accurate and comprehensive description of an example. 2)
We have augmented the previous one-to-one data-visual
mapping structure to capture m-to-n mappings (see below).
Since we have explained some of our features and their
rationale in [23], here we present only the enhancements.

Results
Using the criteria defined above, we have collected
over 300 examples from a wide variety of sources, including newspapers (e.g., NY Times), design books (e.g., [6, 19,
18]), and automated graphics generation systems (e.g., [11,
15, 20]). Not only does our collection cover a rich assortment of data, but it also represents a wide range of visual
forms and styles.
We have also learned a couple of valuable lessons during the selection process. First, we find newspapers to be
one of the better sources for finding desired examples, since
the accompanying articles give us many clues on what the
illustrations aim to convey. Another excellent source is
automated graphics generation systems, which produce
effective graphics using design rules. Second, we have difficulty in finding examples designated for different environment settings (e.g., devices). This may be attributed to
the current domination of the desktop computing.
Although our selection criteria are established based on
our graphics generation needs, they can still be used as general guidelines for constructing a general graphic database.
We can easily modify and augment these criteria to suit
other application purposes. Furthermore, some of our criteria may become obsolete with the advance of relevant computer technologies. For example, the modelability criterion,
would be unnecessary if future technologies can capture all
characteristics of a graphic example automatically.

Visual Elements and Visual Features. The most basic
building blocks of a visual depiction are visual elements
[11, 15, 20]. A visual element is a meaningful visual pattern
with its unique syntax and semantics. It may be recursively
made up of other visual elements [20]. For example, the
top-level visual element of Figure 2 is composed of several
lower-level elements, including stacked bars and the labels
connecting the bars. Each stacked bar consists of two subelements, the vertical position of the bar and rectangular
stacks. Finally, a stack is made up of two primitive elements
(elements with no sub-components): a length and a color.
To express the characteristics of each visual element,
we use 7 features (Table 1), including three newly added
syntactic features: Technique, Parameter, and Dimension. Technique captures the particular visual style used in a visual element, and Parameter describes specific details of a Technique
or a visual form (expressed by Category). For example,
Parameter may record the actual color used in highlighting
an object (a Technique) or a layout constraint used in composing a diagram (a visual form). Capturing visual details is
useful, since a generation system may directly reuse these
Parameters when realizing a new graphic (see Section 4.1).
Dimension indicates the dimensionality of the rendering
space (e.g., 2D vs. 3D).
All three new features allow our generation system to
better cope with different user visual preferences. Unlike
our previous model, which let users express only their preferred visual formalism (e.g., the Category is scatter plot or
table), users can now specify their favored visual style
(Technique), visual details (Parameter), and rendering space

3.2 Example Modeling
Each collected example by itself is just a set of pixels,
which carry little meanings. We must model the examples
before using them to design new graphics. Our model is
based on the theory that a visualization expresses a complex mapping between a set of data elements and visual elements [11, 20]. To express such a data-visual mapping, we
describe the data elements, visual elements, and their correspondences. In particular, we characterize data/visual elements using a set of symbolic/numeric features.
'01

$85.9
35.0

33.4

SERVICES

15.0

HARDWARE

33.8

13.0

SOFTWARE

11.1

OTHER

$4.6

Total Revenue
In billions

$4.7

'92

$64.5

pr

Table 1. Visual features (* new or modified).

Feature

Proceedings of the IEEE Symposium on Information Visualization 2002 (InfoVis’02)
1522-404X/02 $17.00 © 2002 IEEE

Definition

Category

Path

Syntactic category of a visual element, a path in
our visual hierarchy.

Technique*

String

One of the 21 visual techniques for visualizing
data.

Parameter*

Pair

Parameters of visual techniques, in the form of
attribute-value pairs.

Dimension*

Integer

Dimension of Euclidean space (1, 2, or 3),
where the visual element is rendered.

Intention

Path

The purpose of a visual object, a path in our
visual task ontology.

Role

String

One of the 10 roles which a visual element can
play in a visual composition.

Graph

Represents a visual composition, including the
sub-components and the relations among them.

Semantic

Composition
Structure*

Figure 2. Comparing IBM revenues.

Value

Syntactic

a binary value (original vs. derived) to indicate whether a data
element is originally in the database or is derived from others. This feature benefits us from the following two aspects.
First, Source enables us to define different data similarity metrics (e.g., comparing only data with the same Source
values). Second, it helps us learn the data needed to complete a graphic design. Usually a user request provides only
the data to be visualized but not the data for generating
visual references, such as coordinate axes and legends. To
create those visual references, our system must derive the
needed data. Suppose that we need to design a graph like
Figure 1(a). To create the coordinate axes, our system must
derive the ranges of the longitudes and latitudes from the
original location data. Such a data derivation can be learned
based on the Source feature specified in the examples.
Similar to our modification made to the visual Structure,
we also extend our data Structure to capture complex data
compositions. Using a tree structure, for example, our previous model simply cannot express the data relations
among elements D3, D4, and D6 shown in Figure 3(a), since
both D3 and D4 are the parents of D6.

(Dimension). By taking into account different user preferences, our system could then synthesize customized graphics from the desired examples. We are not claiming that we
can accommodate every possible user preference using our
current visual features, since building a complete model of
user visual preferences is a challenging task itself. Nevertheless, we have made the first attempt to enable users to
specify their preferences flexibly at multiple levels.
To describe complex visual compositions, we extend
our previous tree-based Structure feature to be a graph-based
feature. Unlike our previous Structure, which only expresses
the visual relations between a visual element and its children, the current one also captures the relations among the
children. Figure 3(a) (right) depicts the overall visual composition of Figure 2. For example, the top-level element V0
is made up of V1, V2, and V3; V1 and V2 are juxtaposed
together, and V3 connects V2. Our current Structure can easily describe these relations as a graph of four elements V0–
V3. In contrast, such complex relationships could not be
specified by our previous tree-based Structure.

Data Elements and Data Features. Similarly, we use data
elements and their compositions to express the data
encoded in a visualization. Specifically, a data element is a
meaningful information unit that can be comprehended by
users through its visual encodings. In Figure 2, for example,
the revenue produced by each division is a data element.
Figure 3(a) (left) shows the key data elements and their
compositions encoded by Figure 2. Here three types of data
relations appear in this composition: index/identify (one element is used to index or identify the other), and sum (one
element is the summation of others).
Our model uses 16 features to characterize a data element (Table 2). Building on our previous model [23], we
modify the definitions of two features, Type and Form, to
describe the make of a data element more accurately. For
example, we can now easily distinguish between a set of car
prices (Type: entity; Form: array) and a complex patient record
(Type: compound; Form: singleton). It is however impossible
for us to differentiate such data sets previously.
In a visualization process, a data element may directly
come from a database or may be derived from other data
elements. As shown in Figure 2, for example, the total revenue per year may be computed from the division revenues
stored in a database. We add a new feature Source that uses
D0

[D0, V0]

V2 VBars

V4 VBarVPos

D3 TotalRev

sum

identify

Name

D5

D6

V3 VConnectors

compose

index index
D2 Year

Rev$

compose

VBarLength
V8

......

[D1, V2]

VBarColor
V9

[V1, D3]

V6 VText

[D3, V1]

[D5, V5]

(a) Data-visual mappings

[D6, (V6, V8)]

(b) PDGraph

[V5, D5]

[V7, D4]

sum

identify

compose

[V4, D2]

[D2, V4]

[D4, V7]

[V3, /]

[V2, D1]
compose

index index

compose

V5 VLabel

V7 VBarStack

[V0, D0]
juxtapose connect

juxtapose connect

V1 VCaption

D1 DBars

D4 DivisionRev

Graph-based Indexing
Using our feature sets defined above, we can now
describe the data and visual characteristics of a graphic
example. To efficiently access/retrieve relevant examples,
we must index our cases by its unique features or a unique
structure of the features [7]. Due to the complex make of a
graphic, it is difficult to find a single feature or a subset of
features to uniquely identify a case. Resorting to the most
intrinsic property of a visualization, we decide to use a
data-visual mapping structure to index our examples, since
each example expresses a unique data-visual mapping [21].
To capture such a mapping, we link a visual element to
its corresponding data element and vice versa (Figure 3a).
For the sake of simplicity, our previous model assumed that

V0 VChart

DChart

......

In summary, we use a total of 23 features to describe
the characteristics of data and visual elements. By no means
this feature set is complete for capturing all properties of a
visualization. Nevertheless, we find that the current feature
set is adequate for us to capture various subtle semantic and
syntactic differences between examples. In addition, our
feature-based scheme is flexible, and we can easily add/
remove/modify a feature to suit different analysis needs.

compose
[V8, D6]

[V9, /]

(c) PVGraph

Figure 3. Data-visual mappings, PDGraph, and PVGraph for the graphic shown in Figure 2.

Proceedings of the IEEE Symposium on Information Visualization 2002 (InfoVis’02)
1522-404X/02 $17.00 © 2002 IEEE

[V6, D6]

such a data-visual binding is always one-to-one. However,
the simplified modeling is incapable of describing a complex example. Thus our current model supports the most
general form of m-to-n data-visual mappings. For example
in Figure 3(a), data element D6 (a division’s revenue
amount) is mapped to two visual elements, V6 (the text used
to label the amount) and V8 (the length of the bar).
The m-to-n data-visual mappings imply that within one
example, data elements and visual elements could form different hierarchical structures of their own. For example,
Figure 3(a) shows the two different data and visual hierarchies encoded by Figure 2. To create a unique structural
index, we must combine the two hierarchies while preserving all the features. To handle this problem, we introduce a
two-way graph-based indexing mechanism: one by data
and the other by visual.
In particular, a PDGraph links together mapping pairs
(e.g., D0 and V0 in Figure 3a) by the structure of the data
elements (e.g., D0). Figure 3(b) shows a PDGraph, built on
the data composition structure in Figure 3(a). Likewise, a
PVGraph organizes data-visual mapping pairs following the
structure of the visual elements. Figure 3(c) depicts a
PVGraph, formed based on the visual structure in Figure
3(a). Here symbol “/” represents a non-existing element.
Note that a mapping pair may contain more than one visual
or data elements (e.g., the shaded pair in Figure 3b), when
Table 2. Data features (* new or modified).
Feature

Value

Definition (see attached for a full definition)

Semantic
Domain

Path

The data application/domain. It is a path in our
application/domain ontology.

Category

Path

The semantic category to which a data element
belongs. It is a path in our semantic ontology.

Type*

String

One of the 4 meta data types:
{entity, relation, ellipses, compound}.

Form*

String

One of the 2 meta data formations:
{singleton, array}

Scale

String

One of the 4 ways a data element is ordered and
measured: {nominal, ordinal, interval, ratio}

Unit

String

The unit of measure used.

Continuity

String

Whether data are continuous or discrete.

Resolution

Integer

The number of distinct values of a data set.

Volume

Integer

The total number of children in a data element

Cardinality

Integer

The number of elements in a data set.

Arity

Integer

The number of elements in a data relation.

Source*

String

Whether the data is originally in the database or
derived from others.

Role

String

One of 10 presentation-related roles that a data
element plays in a data composition.

Intention

Path

The presentation goal of visualizing a data element. It is a path in our user task ontology.

Importance

String

One of 4 presentation priorities of data: {primary, secondary, extraordinary, background}

Graph

A graph expresses a data composition.

Meta

Pres. related

Composition
Structure*

Proceedings of the IEEE Symposium on Information Visualization 2002 (InfoVis’02)
1522-404X/02 $17.00 © 2002 IEEE

<?xml version="1.0" encoding="UTF-8"?>
<pictureAnnotation xmlns:xsi="..."
xsi:noNamespaceSchemaLocation="ourSchema.xml">
<visualRoot ...>...</visualRoot>
<dataRoot ...>...</dataRoot>
<visualElement name="VBars" dataEncoded="DBars">
<category>VisualUnity</category>
<dimension>2</dimension>
<technique>depict</technique>
<parameter />
<intention> Inform Compare </intention>
<role>element</role>
<structure xsi:type="ComplexVisType">
<operator>compose</operator>
<operand>VBarVPos</operand>
<operand>VBarStack</operand>
</structure>
</visualElement>
......
<dataElement name="DBars" visualEncoding="VBars">
...... </dataElement>
......
</pictureAnnotation>
Figure 4. An XML annotation fragment for Figure 2.

the data-visual mapping is not one-to-one.
Using PVGraph and PDGraph together, we can now
uniquely index all examples in the database. Consequently,
we can access and manage these graphic examples by their
PDGraphs and PVGraphs. For example, when retrieving an
example according to a user request, we compare their
PDGraphs and PVGraphs (see Section 4).

XML-based Annotation Schema
To encode all the features for an example, we develop
an XML annotation schema [17]. Using this schema, we create an XML document for each collected example, which
now has two parts: an annotation in the form of an XML document, and the image itself in JPEG format.
Figure 4 presents a fragment of the XML annotation created for Figure 2. As shown here, our XML annotation
defines a visual root (the top-level visual element), a data
root (the top-level data element), a set of visual elements,
and a set of data elements. Each visual/data element
(including the root) is uniformly described using our visual/
data features defined in Table 1 and 2.
Currently, each XML document is created by hand, since
existing image understanding technology cannot automatically extract key characteristics that we need (e.g., data
encoded) except certain low-level visual features (e.g.,
color or texture histograms).

4. Use of Visual Database
Using our visual database as a knowledge base, our
generation system can create new graphics by learning
from existing examples. In addition to graphics generation,
we have also explored several other uses of the database.

4.1 Example-based graphics generation
Our example-based graphics generation consists of two
main phases: graphic sketch generation and sketch realization. Here a sketch is an intermediate representation that
outlines the basic structure and elements but omits the lowlevel visual details. For example, a sketch of a bar chart
specifies the number of axes and bar elements to be created
but not the specifics of the axes and bars, such as their exact
scales and positions. Since describing the details of the
entire generation process is out of the scope of this paper,
here we highlight the key ideas.
The input to our system is a user request, represented
by an XML document similar to the XML annotations made
for our examples. However, a user request is often partially
specified, since users may not know every aspect of the
data or the desired visual encodings. As a preprocessing,
our system parses each XML document (including the
request and examples in the database) and automatically
builds the corresponding PDGraphs and PVGraphs.

Sketch Generation
Figure 5 outlines our three-step sketch generation algorithm. Given a user request, our algorithm first retrieves the
top-K (e.g., K = 3) matched examples based on the distances
between the request and existing graphics (lines 1–7). It
then evaluates whether top-matched examples are adequate
for creating a new graphic (lines 8–12). If the evaluation
fails, the current request is decomposed into a set of request
Sketch GenerateSketch (Request req, DB db, integer k)
1. Example retrieval
1
2
3
4
5
6
7

Examples candidateSet ← empty set
for each example e ∈ db do
d ← distance (req, e)
insert e to candidateSet in ascending order of d
if sizeOf (candidateSet) > k then
remove the last element from candidateSet endif
endfor
2. Example evaluation

8
9
10
11
12

boolean success ← false
for each candidate c ∈ candidateSet do
success ← evaluate (req, c)
if success break
endfor
3. Query decomposition and sketch composition

13
14
15
16
17
18
19
20
21
22

if success then sketch ← compose (req, e)
else
Requests requestSet ← decompose (req)
Sketches sketchSet ← empty set
for each sub-request p ∈ requestSet do
newSketch ← GenerateSketch (p, db, k)
add newSketch to sketchSet
endfor
sketch ← compose (req, sketchSet)
endif

23

return sketch

Figure 5. Outline of example-based sketch generation

Proceedings of the IEEE Symposium on Information Visualization 2002 (InfoVis’02)
1522-404X/02 $17.00 © 2002 IEEE

fragments (line 15), and the sketch generation is recursively
called to create sketches for the fragments (lines 17–20).
Next we briefly discuss the three main routines involved.

Distance. Our distance function calculates the distance
between a user request and an example by computing a
weighted sum of distances of their corresponding PDGraphs
and PVGraphs. The distance between two graphs is a
weighted sum of distances of their elements. Finally, the
distance between two data/visual elements equals to a
weighted sum of the distances of their features using our
quantitative similarity model defined in [23].
Evaluate. Our evaluation function uses two heuristics to
determine whether we can use a matched example to synthesize a new graphic. 1) The matching score (similarity
distance) for every data element of the request should be
below a certain threshold. 2) Every primitive data element
of the request acquires a visual mapping. Note that we do
not require to find direct mappings for intermediate data
elements, since their mappings can be composed from those
of their children.
Compose. A sketch can be directly constructed based on a
matched example (line 13), or it can be composed from a
set of sketches (line 21). Since a sketch may be made up of
sketches created from different examples, our system needs
to ensure that such a composition is valid. Currently we
compare the proposed composition with existing visual
compositions in the examples. If a similar composition
could not be found, our composition function returns null,
and the overall sketch generation fails.
Sketch Realization
At this stage, our system transforms the sketch into the
actual graphic on the screen. To fill in all visual details
(e.g., the exact scale of a visual element), our system may
learn different settings and constraints from the Parameter
features of the matched elements. We then determine the
final layout using a numerical constraint solver [4].

4.2 Implications to Information Visualization
Although our original motivation of building a visual
database is to support example-based graphics generation,
our further exploration shows that our database also brings
important implications to information visualization.

Benchmark
Offering a diverse collection of graphic examples, our
database fills the long-standing need for a benchmark to
test and compare different visual techniques. Like other
benchmarks (e.g., document or video databases used by
TREC (http://trec.nist.gov)), our database can be used in two
ways: graphics retrieval and graphics organization.
Graphics retrieval. Given a new visualization, graphics
retrieval attempts to find all similar visualizations in the
database. Researchers can use our database as a testbed to
experiment different retrieval algorithms using different
quantitative similarity metrics. In particular, we have investigated using different similarity metrics (e.g., by assigning
different feature weights) for graphics retrieval. This pro-

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(a)

Figure 6. Existing examples (1–7) and their computed similarity clusters (a).
(2) & (5) Copyright 1994 ACM, reprinted by permission from B. Myers [14]. (3) Copyright 1986 ACM, reprinted by permission from J. Mackinlay [11].
(4) Copyright 1997 AAAI, reprinted by permission from S. Roth [2].

cess is very similar to the example retrieval step described
in Figure 5 (lines 1–7) except that now a user request is a
fully specified graphic example. Graphics retrieval is especially useful when evaluating a new visualization, since we
can accurately assess how this new visualization is related
to existing visualizations (e.g., how new it is) [2, 23].
Assume that Figure 2 is a new example. We would like
to learn to which examples in the database it is closely
related. As a result, our algorithm retrieves examples 2 and
1 in Figure 6 as the top two matched examples, with the
respective distances of 0.2647 and 0.2891. By examining
the matching results, we can explain that Figure 2 is similar
to Figure 6(2) because both are variations of bar charts.
Despite their visual differences, Figure 2 is also close to
Figure 6(1) since both depict a whole-part data relation.

Graphics organization. As the number of examples
grows, a certain type of organization is necessary for us to
easily identify the overall trend among all examples and the
local patterns within a subset. Moreover, when a new example is encountered, we would like to classify it to dynamically update the existing organization. Graphics
organization studies the general structure and patterns of
graphic examples in a database. From this standpoint, our
database also provides researchers a testbed to apply different pattern-finding methods, including classification and
clustering, to study the organization of the examples.
In one of our experiments, we have used a hierarchical
clustering algorithm [3] to arrange 7 examples (Figure 6)
based on their pair-wise distances computed by our similarity model. By setting a distance threshold, our algorithm
produces several clusters. For example, if the threshold is
0.2, we obtain 3 clusters (Figure 6a): (6, 7), (1), and (2, 3, 5,
4). Here, example 1 is collected from a newspaper to depict
the sales made by a business. Examples 2 and 5 are from
[14] to illustrate the relations of various car attributes (e.g.,
prices and mileages). Examples 3 and 4 are generated by

Proceedings of the IEEE Symposium on Information Visualization 2002 (InfoVis’02)
1522-404X/02 $17.00 © 2002 IEEE

APT [11] and SAGE [15], respectively. The last two examples
6 and 7 are created by a design professional for a real-estate
application. Using the same clustering algorithm, we can
also classify Figure 2, which belongs to the same cluster
where examples 1–5 reside (Figure 7).
Properly organizing examples not only helps us understand the intrinsic relations among different examples, but
also facilitates graphics generation. For example, our example retrieval process (Figure 5) may use the organization
(clusters) of the examples to optimize its search, i.e., first
checking all the examples within the same cluster of a
promising example that has been identified.

In both graphics retrieval and graphics organization,
our fine-grained model of graphic examples plays a critical
role. Unlike other work on visual retrieval [10, 16], which
uses limited semantic information, our model allows
researchers to define more accurate similarity metrics by
taking into account both syntactic and semantic properties
of graphics. An accurate similarity metric in turn helps produce better retrieval results and visual analysis.

new

Figure 7. Clusters after adding a new example.

Learning Visual Design Principles
Besides using our database as a benchmark for visual
analysis, we could also use our database as a knowledge
repertoire to learn different visual design principles. Recall
the process of composing a new sketch. To verify whether a
new visual composition is valid, we can take another
approach. From our examples, we may use decision-treebased machine learning technique [13] to induce a set of
classification rules for visual composition. We can then use
the derived rules to cross-validate whether the proposed
new composition is appropriate.
Likewise, we can use machine learning methods to
study or generalize a number of visual design principles or
techniques from our examples. For example, we may study
the correlations between specific domain data and corresponding visual techniques used, or discover the rules of
using a particular visual technique (e.g., Highlighting) with
specific parameter settings (e.g., style of Highlighting).

tents and wordnet in image retrieval. In Proc. ACM SIGIR ’97, pages
286–295, 1997.
[2]

M. Chuah, S. Roth, and S. Kerpedjiev. Sketching, searching, and
customizing visualizations: A content-based approach to design retrieval. In M. Maybury, editor, Intelligent Multimedia Information
Retrieval, pages 83–111. AAAI Press/The MIT Press, 1997.

[3]

R. Duda and P. Hart. Pattern Classification and Scene Analysis.
Wiley, 1973.

[4]

M. Gleicher. A Differential Approach to Graphical Interaction. PhD
thesis, School of Computer Science, Carnegie Mellon University,
Pittsburgh, PA 15213-3891, 1994.

[5]

R. Harris. Information Graphics: A Comprehensive Illustrated Reference. Management Graphics, 1996.

[6]

P. R. Keller and M. M. Keller. Visual Cues: Practical Data Visualization. IEEE Computer Society Press and IEEE Press, 1993.

[7]

J. Kolodner. Case-based Reasoning. Morgan Kaufmann, 1993.

[8]

J. Levin, G. Anglin, and R. Carney. On empirically validating functions of pictures in prose. In D. Willows and H. Houghton, editors,
The Psychology of Illustration: Basic Research, volume 1, chapter 2,
pages 51–86. Springer-Verlag, New York, 1987.

[9]

G. Lohse, K. Biolsi, and H. Rueter. A classification of visual representations. Communications of the ACM, 37(12):36–49, 1994.

5. Conclusions And Future Work
We build a database of graphic examples to support an
example-based graphics generation system, which aims to
create new visualizations directly from existing visualizations (examples). In the process of constructing our database, we have addressed two issues involved: example
selection and example modeling. First, we have defined a
set of example selection criteria to assure the coverage and
quality of our examples. Second, we have presented a feature-based annotation model to describe each graphic comprehensively and accurately. Furthermore, we have
discussed the important implications that our database may
bring to the area of information visualization.
As we continue collecting graphic examples to enrich
our database, we are developing a user interface to ease the
example annotation. To annotate each example, currently
we use XmlSpy IDE (http://www.xmlspy.com) to create an
XML document by hand. Our new interface should be able to
take a graph structure directly as an input (e.g, the data or
visual hierarchy shown in Figure 3a), and then automatically generates an XML document. To further simplify the
laborious hand annotation process, we are also exploring
how to automatically extract salient features using
advanced computer vision and graphics techniques (e.g.,
extracting 3D shape features).
So far we have only collected positive (good) examples
in our database, we would also like to augment the database
by adding negative examples to help the example adaptation process. In other words, we can better judge whether a
proposed adaptation (e.g., a new visual composition) is
acceptable using both positive and negative examples.

[10] W. Ma and B. Manjunath. A texture thesaurus for browsing large
aerial photographs". J. of American Society for Information Science,
49(7):633–648, May 1998.
[11] J. Mackinlay. Automating the design of graphical presentations of
relational information. ACM Trans. on Graphics, 5(2):110–141,
1986.
[12] J. Marks. A formal specification scheme for network diagrams that
facilitates automated design. J. of Visual Languages and Computing,
2(4):395–414, 1991.
[13] T. Mitchell. Machine Learning. McGraw-Hill, 1997.
[14] B. Myers, J. Goldstein, and M. Goldberg. Creating charts by demonstration. In CHI ’94, pages 106–111, 1994.
[15] S. F. Roth and J. Mattis. Automating the presentation of information.
In Proc. IEEE Conf. on AI Applications, pages 90–97, 1991.
[16] S. Tong and E. Chang. Support vector machine active learning for
image retrieval. In ACM MM ’01, pages 107–118, 2001.
[17] W3C, www.w3.org/TR/2001. XML Schema, March 2001.
[18] P. Wildbur and M. Burke. Information Graphics: Innovative Solutions in Contemporary Design. Thames and Hudson, 1998.
[19] R. Wurman. Information Architects. Graphics Press, New York,
1996.
[20] M. Zhou. Visual planning: A practical approach to automated visual
presentation. In Proc. IJCAI ’99, pages 634–641, 1999.
[21] M. Zhou and S. Feiner. Data characterization for automatically visualizing heterogeneous information. In Proc. IEEE InfoVis ’96, pages
13–20, 1996.

Acknowledgment

[22] M. Zhou and S. Feiner. Visual task characterization for automated
visual discourse synthesis. In Proc. ACM CHI ’98, pages 292–299,
1998.

We would like to thank Joonhwan Lee from CMU for
designing examples 6 and 7 in Figure 6. We also thank all
parties noted in the paper for letting us use their figures.

[23] M. Zhou and S. Ma. Representing and retrieving visual presentations
for example-based graphics generation. In Proc. 1st Intl. Syp. on
Smart Graphics 2001, pages 87–94, 2001.

References
[1]

Y. Aslandogan, C. Their, C. Yu, and N. Rishe. Using semantic con-

Proceedings of the IEEE Symposium on Information Visualization 2002 (InfoVis’02)
1522-404X/02 $17.00 © 2002 IEEE

