An Optimization-based Approach to Dynamic Visual Context Management
Zhen Wen

Michelle X. Zhou

Vikram Aggarwal

IBM T. J. Watson Research Center
Hawthorne, NY 10532
{zhenwen, mzhou, aggarwal}@us.ibm.com
U1 Text: Find homes between $500k and $600k with U2 Text: Which is the cheapest?
at least 0.3 acre in central Westchester.

U2' Text: Just the cheapest

R1 Speech: I found 8 houses meeting your criteria. R2 Speech: Here is the cheapest one.

R2' Speech: Here it is.

Figure 1. A recorded user-RIA conversation fragment: U1–U2 are user inputs, R1–R2 are corresponding RIA-generated responses.

ABSTRACT
We are building an intelligent multimodal conversation system
to aid users in exploring large and complex data sets. To tailor to
diverse user queries introduced during a conversation, we automate the generation of system responses, including both spoken
and visual outputs. In this paper, we focus on the problem of visual
context management, a process that dynamically updates an existing visual display to effectively incorporate new information
requested by subsequent user queries. Specifically, we develop an
optimization-based approach to visual context management. Compared to existing approaches, which normally handle predictable
visual context updates, our work offers two unique contributions.
First, we provide a general computational framework that can
effectively manage a visual context for diverse, unanticipated situations encountered in a user-system conversation. Moreover, we
optimize the satisfaction of both semantic and visual constraints,
which otherwise are difficult to balance using simple heuristics.
Second, we present an extensible representation model that uses
feature-based metrics to uniformly define all constraints. We have
applied our work to two different applications and our evaluation
has shown the promise of this work.
Keywords: intelligent multimodal interfaces, visual context management, automated generation of visualization, visual momentum
1. INTRODUCTION
To support context-sensitive information access and exploration,
we are building an intelligent multimodal conversation system,
called Responsive Information Architect (RIA). Specifically, RIA

IEEE Symposium on Information Visualization 2005
October 23-25, Minneapolis, MN, USA
0-7803-9464-X/05/$20.00 ©2005 IEEE.

allows users to express their information requests in context using
multiple modalities, including natural language, GUI, and gesture.
Moreover, RIA dynamically creates a tailored response, including
visual and spoken outputs (Figure 1)1. RIA is embodied in two
applications: a real-estate application that helps users to search for
residential properties and a hospitality application that aids users in
finding hotels and relevant amenities (e.g., restaurants).
Based on the understanding of a user request [5], RIA automatically creates its response in three steps. First, RIA decides the type
of the response. In the case of U1 (Figure 1), RIA decides to
present the requested houses directly. Depending on the context,
RIA may formulate different types of responses. For example, if
the retrieved data set is very large it may ask the user to supply
additional constraints. Second, RIA determines the data content of
the response [21]. In responding to U1, RIA selects a subset of
house attributes such as the price and style. Third, RIA designs the
form of the response using suitable media and presentation techniques [22]. For example, in R1 (in Figure 1) RIA uses graphics to
display house data on a map and speech to summarize the number
of retrieved houses.
To handle a follow-up request like U2 (Figure 1), RIA creates a
response R2, which emphasizes the cheapest house while preserving most of the existing visual context provided by R1. In contrast,
for request U2', RIA zooms in on the cheapest house and simplifies
the display of other information (R2'). Here we use the term visual
context to refer to a visual scene that a user perceives when issuing
a query. Without integrating new information into an existing
scene, a user may have difficulty in comparing and combining
information. For example, if R2 displays only the cheapest house,
the user may not easily relate this house to others retrieved earlier.
As demonstrated by this example, RIA tailors its response to a
user request at run time. Our focus here is on visual context man1 All examples used in this paper are working examples created by RIA.

187

agement, a process that dynamically determines how to incorporate newly requested information into an existing visual context so
that users can comprehend all relevant information as a coherent
whole. More precisely, visual context management is a process
that derives a set of visual transformations, which updates an existing scene to incorporate new information. To obtain R2 in Figure
1, RIA updates R1 by adding details to the cheapest house and simplifying other retrieved houses.
Since it is very difficult to predict how a user-RIA conversation would unfold, it is impractical to plan all possible visual context transformations a priori. To incrementally present diverse,
unanticipated information introduced during a conversation, we
model visual context management as an optimization problem. The
objective is to find a set of visual transformations that maximizes
the satisfaction of all visual context management constraints (e.g.,
ensuring semantic continuity and minimizing visual clutter). To the
best of our knowledge, our work is the first to address visual context management using an optimization-based approach. As a
result, it offers two unique contributions:
1. It can derive a set of near optimal visual transformations by
simultaneously balancing a broad set of constraints for
diverse interaction situations.
2. It is easily extensible, since it uses feature-based metrics to
uniformly model all visual context management constraints.
In the rest of the paper, we first discuss the related work. We
then describe our optimization-based approach to visual context
management, highlighting the aspects mentioned above. Finally,
we present our evaluation results.
2. RELATED WORK
Our work is built upon the studies of understanding and analyzing
the characteristics and cognitive functions of visual context (e.g.,
[3, 19, 7, 14]). For example, studies show that a semantically
coherent visual context can facilitate the detection and identification of objects in complex scenes [3]. Moreover, a coherent visual
context provides strong cues to guide effective distribution of
user’s visual attention, which is critical in processing visual outputs [14]. Moreover, the concept of visual momentum is introduced to measure how visual context transitions impact the user’s
ability in extracting and integrating information across multiple
displays [19]. While these works provide us with a theoretical
foundation to draw upon, we offer a computational model that realizes these theories.
Researchers in the area of human-computer interaction and
information visualization have exploited visual context in creating
better visual interfaces. To provide users with desired visual support for their working memory, focus+context interfaces present
important information within context [4]. However, existing
approaches focus on manipulating the views of a fixed data set (for
example, a hyperbolic browser [13] and a radial space-filling display [18] load the entire data set at once). In contrast, we must
dynamically update visual context to incorporate new, unanticipated information introduced during a user-RIA conversation.
To handle more dynamic situations where a visual context may
be continuously updated through a head-tracked display, Bell et al.
have used a greedy algorithm to maintain a set of visual constraints, such as preventing object occlusion and ensuring visual
continuity [2]. Compared to their approach, our work considers a
more comprehensive set of constraints (including their visual constraints) to ensure both semantic and visual coherence of a display.
Moreover, our optimization-based approach can simultaneously
balance all constraints, including conflicting ones (e.g., maintaining semantic continuity vs. minimizing visual clutter).
Another piece of related work is using an AI planner to automatically derive a set of visual transformations for creating a

188

Data to be visualized
House

c Sketch
generation

Cortland
MLS-1234567
Bedrm 4
Price: $485000

City
...

d Visual layout

Img
MLS

Boundary

MLS-1234567

Loc
Price

Price: $485000
Bedrm: 3
...

Cortland

Name

MLS-7845167
Price: $550000
Bedrm: 4
...

...

S'

e Visual context
management
Existing context (St)

Cortlan
d

New context (St+1)

Figure 2. Visual output generation.
coherent visual presentation [20]. However such planning-based
approaches are not scalable, since they are designed to handle
small data sets (e.g., one patient at a time) in predictable situations.
In contrast, our optimization-based method is designed to handle
large data sets in a highly interactive environment.
3. VISUAL OUTPUT GENERATION
Figure 2 provides an overview of RIA’s visual designer, which
automatically creates a visualization for a given user query in three
steps. First, the sketch generator creates a visual sketch, describing
the visual encodings of the data to be visualized [22]. Second, the
visual layout manager determines the geometry (e.g., size and
location) of the visual encodings. Specifically, it dynamically
derives a set of spatial layout constraints, such as avoiding object
occlusion and ensuring visual balance. Third, the context manager
updates an existing visual context to incorporate the new sketch.
Although we have implemented the three-step pipeline, these
three steps may be inter-twined. For example, the layout manager
may need to relocate the objects after the context manager decide
what to keep/delete. To avoid going back and forth between the
two steps, our layout manager now computes a range of candidate
geometric parameters for each visual object in the scene.
4. EXAMPLES
We use a set of examples to illustrate how visual context management is subject to a number of factors, such as user intention and
various visualization constraints. First, a user’s data navigation
intention, which is often implied by a user query expression,
impacts visual context management. For example, query U2 in Figure 1 may imply that the user is browsing the data, while U2' may
suggest that the user is filtering the data2. Accordingly, for U2 RIA
preserves most of the visual context to facilitate further data
U3: Tell me about Hartsdale.
RIA: Hartsdale is located in central Westchester county, 22 miles
from Manhattan.

Figure 3. A follow-up query to U1 to show semantic continuity.
2 Currently RIA infers the user navigation intention using keywords such
as “just” or “which one”.

browsing (R2), while zooming in on the cheapest house for U2' to
satisfy data filtering (R2'). Moreover, conversation flow affects
visual context management. By default, RIA assumes that a user
conducts a continuous conversation and interprets a user query in
the context of previous queries. Suppose that U3 in Figure 3 is
issued after U1 in Figure 1. As a continuation, RIA presents the
requested city data while preserving the relevant houses. During a
conversation, however a user may want to start a new context3.
Assume that the user issues query U4 (Figure 4) after U1 to switch
to a new context. In this case, RIA creates a visual output that only
shows the requested school data in the new context.
In addition to user intentions, various visualization constraints,
such as maintaining continuity across displays [19] and minimizing visual clutter [12], influence visual context management. To
maintain semantic continuity, for example, in Figure 3 RIA incorporates the requested city data in the context of relevant houses.
Moreover, to maintain visual continuity, in Figure 5(b) RIA displays the requested restaurants along with the hotels retrieved earlier, although the restaurants and the hotels are only remotely
related according to our data ontology.
Over the course of a user-RIA conversation, visual objects
may be accumulated in a visual context. Since complex visual displays may overload a user’s working memory and impair information comprehension [12], RIA tries to minimize visual clutter
while maintaining continuity. To reduce clutter, RIA simplifies the
display of less important information like the houses in Figure 3
and the hotels in Figure 5(b), or simply removes irrelevant data.
To help users to integrate information across multiple scenes,
RIA must also maintain important perceptual landmarks [19] and
provide smooth transitions [14]. In Figure 1, RIA preserves various geographical landmarks, such as the Hudson river and major
highways, which help to anchor a visual transition. Moreover, RIA
ensures smooth visual transitions to allow users to keep track of
changes. For example, RIA animates camera changes and visual
object updates (see accompanying video).
In summary, RIA must consider a wide variety of visual context management constraints, including accommodating user intentions, ensuring visual continuity, and minimizing visual clutter.
These constraints often exhibit inter-dependencies and may even
conflict with one another. For example, preserving semantic continuity may violate a visual clutter reduction constraint. It thus
would be very difficult to maintain a coherent visual context using
simple heuristics, which may not be able to balance all constraints.
5. OPTIMIZATION-BASED VISUAL CONTEXT MANAGEMENT
To balance all relevant constraints simultaneously, we develop an
optimization-based approach to visual context management. We
explain our approach in three steps. First, we present our featurebased representation that characterizes a visual context and visual
U4: Tell me about Scarsdale schools.
RIA: Here is the information about Scarsdale school district.

Figure 4. A subsequent query issued in a new context.
3 Currently in RIA, a user can click on the “new context” button to indicate
the starting of new context.

U5 Show hotels near IBM Hawthorne
R5 I found 4 hotels near IBM Hawthorne. <Display (a)>
U6 What about Chinese restuarants?
R6 There are two chinese restuarants near IBM. <Display (b)>

(a)

(b)

Figure 5. Maintaining visual continuity.

operators. Here a visual operator defines a visual transformation
that updates the properties of one or more visual objects. For
example, a Highlight operator updates the color of a selected visual
object. Second, we use feature-based metrics to uniformly model
various visual context management constraints. In particular, each
metric assesses the desirability of applying one or more visual
operators to update a visual context. Third, we present a simulatedannealing algorithm that dynamically derives a set of visual operators by maximizing the satisfaction of all relevant constraints.
5.1 Feature-based Representation
Since a visual context is continuously updated during a user-RIA
conversation, we describe the state of the context at the beginning
or end of each user turn. Formally, we use the following notations.
Given user turn t+1, St and St+1 denote the visual context at the
beginning and end of the turn, respectively. We use a set of features
to describe semantic and syntactic properties of St and St+1. Similarly, we use a set of features to characterize each visual operator.
5.1.1 Bi-Level Visual Context Representation
Visual context St consists of a set of visual objects. For example, at
the beginning of user turn U2 (Figure 1), the visual context contains objects such as retrieved houses and cities (R1). To characterize the overall scene at St and each visual object involved, we
develop a bi-level descriptor, which describes the overall properties of a scene and the details of each visual object.
Specifically we describe a scene using aggregated features,
such as the total number of visual objects (volume) and the colors
used (colorVariety) in the scene. As our goal is to incorporate a new
scene into an existing visual context, we use the same set of features to describe the new scene (e.g., the details of the cheapest
house in U2 in Figure 1). To facilitate the integration, we add a foci
feature. Feature foci describes the current visual focus. For U8 in
Figure 6, foci is the requested train stations. Given a user query,
RIA’s content selector dynamically decides visual foci [21]. Now
foci is used to set camera parameters and compute the semantic relevance between a new scene and an existing visual context.
Besides describing an overall scene, we characterize the properties of each visual object. A visual object is an encoding of a data
object and has basic visual properties, such as color, size, and location [4]. To facilitate visual context management, here we focus on
the data and visual semantic features. We use data features like category to describe the semantic category of the encoded data, and
visual features such as prominence to specify how a visual encoding may be perceived. Table 1 lists all semantic features that RIA
uses. Here we focus on explaining three complex features: data
importance (dImportance), visual proximity (vProximity), and
visual prominence (prominence).

189

U7: Show colonials under $300k in Yonkers.
RIA: There are 4 houses meeting your criteria. <Display (a)>
U8: Show train stations within 1 mile of this one
<click on the house with mls 2421595>

Rs(d, D') = Max [Rs(d, d'j), ∀ j], where d is a data object, and D'
is the current query foci, d'j∈D', and Rs(d, d'j) computes the relevance between two data objects:
⎧ 1, if two data objects are the same
(a) R s ( d, d' j ) = ⎨
.
⎩ β ( r ( d, d' j ) ), otherwise

RIA: I found two train stations. <Display (b)>

Here β is a function indicating the relevance value of the relation r
between d and d'j in a database. RIA uses a data ontology to look
up the data relation r between d and d'j. It then uses the database to
verify the relation. Let d be a house and d'j be a city in a database.
By our ontology, a house is located-in a city. Using this relation,
RIA verifies whether house d is in fact located in city d'j in the
database. If the relation holds, then β=1, otherwise β=0. Now the
value of r() for each type of data relation is defined in our data
ontology.
In summary, we define overall data importance for data object
d at a given user turn t+1 as follows:
(a)

(b)

Figure 6. Maintaining a proper visual ordering.

Data importance. Feature data importance indicates how important a data object is to a given user turn. All data objects start with
the same importance value (0.0). RIA dynamically updates data
importance in two ways. First, RIA uses its content selector to
decide data importance [21]. If the content selector chooses a data
object to present at a given user turn, it updates the importance of
this object accordingly. For query U8 in Figure 6, the content selector assigns the requested train station data with the highest importance and the house constraining the train stations with a lower
importance. If a data object is not selected, its importance is
reduced using a time decay function. For example, to respond to
U2 in Figure 1, RIA does not select the houses shown earlier
except the cheapest one. Accordingly, their importance values are
reduced at turn U2. Whenever a user starts a new context, RIA
resets the importance to 0.0 for non-selected data.
Second, RIA uses data relationships to decide data importance.
This is very useful when the content selector assigns the same
importance to multiple data objects. In R1 (Figure 1), the content
selector assigns the same importance to all retrieved houses. Since
these houses are not mentioned in U3 (Figure 3), their importance
is reduced. However houses located in Hartsdale are more semantically relevant to U3 than others are. As a result, RIA better preserves these houses to provide the user a useful context to
comprehend the retrieved city data (Figure 3). Thus, we define a
semantic relevance metric:
Table 1. Features for a visual context (*dynamically computed).
Feature

Definition

Visual object-level features
category

semantic category of data defined in a data ontology

landmark

whether an object is a landmark specified by ontology

shapeComplexity

how complex a shape it is—a pre-assigned score

dImportance*

how semantically important it is to the current context

vProximity*

how visually close it is to the current visual foci

prominence*

how visually prominent it appears

Visual scene-level features

190

volume*

number of visual objects in a scene

colorVariety*

number of different colors in a scene

shapeVariety*

number of different geometric shapes in a scene

⎧ val, if d is selected at turn t+1
⎪
(1) I d ( d, t + 1 ) = ⎨ 0.0, if new context starts, otherwise .
⎪ Max [ I ( d, t ) × exp ( – α ), R ( d, D' ) ]
d
s
⎩
Here val is the importance computed by the content selector and α
is the decay factor. Current we use α = 1.5 for a rapid decay.
Visual proximity. While data importance assesses how a visual
object is related to a user query semantically, visual proximity measures how a visual object is related to the current visual foci spatially. When presenting user-requested information, it is desirable
to show items that are located nearby, since such items may help to
establish a useful context for users to comprehend the intended
information [1]. For example, in Figure 5(b) RIA shows the
requested restaurants and the nearby hotels to give users an overall
sense of where everything is. By this notion, feature visual proximity computes the spatial distance of visual object v to visual foci V':
(2) Iv(v) = 1 - Min[dist((v, v'j), ∀j], where v'j∈V', dist() computes
the Euclidean distance of two visual objects in a normalized screen
coordinate.
Visual prominence. Visual prominence measures how easily a
visual object can be perceived by a user. It is now modeled using
three basic visual variables: color, size, and location. Given a
visual object v, we define its color prominence P1(v), size prominence P2(v), and location prominence P3(v).
Color prominence states that the more contrast a visual object
produces against its background, the more prominent it can be perceived. For example, a red object is more prominent than a yellow
object against a white background. Color contrast can be computed
based on color theory [8]. We use function contrast() to compute
the contrast value of object v against its background:
P1(v) = contrast(v).
Size prominence asserts that the bigger a visual object is, the
more prominent it appears:
P2(v) = v.bbx.width × v.bbx.height, where the bounding box is
computed in a normalized screen coordinate.
Location prominence states that objects placed near the center
of a display are more prominent than those located elsewhere:
P3(v) = 1 - dist(v, c), where c denotes the center of a display,
function dist() computes the normalized screen Euclidian distance
between the center of v and the center of the display.
Combining three formulas above, we model the overall visual
prominence of visual object v:

(3) P ( v ) =

∑ ui × Pi ( v ) , where i=1..3, and weight ui=0.33.
i

5.1.2 Visual Operator Representation
We use visual operators to model visual transformations that
update a visual context and incorporate new information [20].
Visual operators can be categorized based on their effects. So far
we have identified four groups of operators: camera operators that
modify the parameters of a camera, appearance operators that
update visual appearance (e.g., Highlight), geometry operators that
change geometric properties (e.g., Move and Scale), and structural
operators that modify a scene structure (e.g., Add). Table 2 lists all
operators that RIA uses. Depending on the actual implementation,
an operator may exhibit different visual effects. For example, we
may implement Delete by making objects transparent gradually or
simply hiding them. Moreover, we can easily define new visual
operators based on application requirements.
To represent all visual operators uniformly, we associate each
operator with seven features. Feature operand denotes visual
objects that an operator manipulates, and feature parameter holds
the specific information that is required to perform the intended
transformation. As shown below, operator Scale has one parameter
scaleFactor. Feature effect is a function measuring what properties
of the operands will be modified after the operation. For example,
Scale changes the size of an object. On the other hand, feature cost
estimates the cost of performing the intended visual transformation. Currently, it measures the perceptual cost needed for a user to
perceive the transformation. For example, it is more costly for a
user to perceive object movements than highlighting effects [14].
Finally, features temporal-priority, startTime and endTime control
the timing of applying an operator. The fragment below outlines
the definition of operator Scale:
Scale extends Operator {
List operand
Vector3f scaleFactor // parameter stating how much to scale
float effect() // this function modifies the size of operands
float cost = medium
float temporal-priority = medium
float startTime, endTime
}

5.2 Feature-based Desirability Metrics
As described in Section 3, a number of constraints influence visual
context management, including user intentions and visualization
constraints. To uniformly model all constraints, we define a set of
metrics based on our representation of a visual context and visual
operators. These metrics assess the desirability of applying one or
more visual operators to an existing visual context to incorporate
new information. By their purpose, we divide metrics into two
groups: visual momentum metrics and visual structuring metrics.
Table 2. A catalog of visual operators in RIA
Operator

Definition

Camera Operators
Camera

Update camera parameters (e.g., zoom in)

Appearance Operators
Highlight

Highlight an existing visual object (e.g., change color)

Geometry Operators
Move

Modify the location of a visual object

Scale

Modify the size of a visual object

Structural Operators
Simplify

Simplify the representation of a visual object

Add

Add a visual object to a scene

Delete

Delete a visual object from a scene

Visual momentum metrics assess the coherence of a visual context
across displays [19]. Visual structuring metrics evaluate the structural coherence of a visual context after the new information is
integrated [12]. Our purpose here is not to enumerate a complete
set of visual context management constraints, instead we show
how to formulate key constraints quantitatively. For simplicity, all
feature/metric values are normalized to lie between [0, 1].
5.2.1 Visual Momentum Metrics
Visual momentum measures a user’s ability to extract and integrate
information across multiple displays. Since the amount of visual
momentum is proportional to a user’s ability to comprehend information across displays [19], RIA tries to maximize the visual
momentum when updating a visual context. Specifically, we adopt
three key techniques from [19] that are most applicable to our
visual context management task: 1). maximizing both semantic
and visual overlaps of consecutive displays, 2). preserving perceptual landmarks, and 3). ensuring smooth visual transitions.
Maximizing display overlap. Proper display overlap helps users
to piece together information across successive displays [19].
Using two key display overlap techniques suggested by Woods
[19], we define two metrics: visual overlap and semantic overlap
metrics. A visual overlap metric computes the invariance between
two displays, specifically the average invariance of each visual
object in St and its new state in St+1:
1
O v ( S t, S t + 1 ) = ---- ∑ inv ( v i, t, v i, t + 1 ) .
N

i

Here visual object vi, t∈St, vi, t+1 ∈St+1, and vi, t+1= opi(vi, t), opi is
a visual operator; N is the total number of visual objects in St; inv()
computes the invariance between two visual objects. If vi, t+1 is
invisible, inv() =0.0; otherwise it is the average invariance of locations, sizes, and colors:
inv(vi, t , vi, t+1) = Avg[inv_loc(vi, t , vi, t+1), inv_size(vi, t , vi, t+1),
inv_color(vi, t , vi, t+1)].
Similarly, we define a semantic overlap metric that assesses
whether semantically related items remain together across displays. It computes the semantic relevance of St and St+1:
1
O s ( S t, S t + 1 ) = -----2
N i

∑ ∑ ( Rs ( di, dj ) ) ,

where data objects di

j

and dj are encoded by vi, t and vj, t+1, respectively, Rs() computes
their semantic relevance using Formula (a) on page 4.
Using the visual and semantic overlap metrics defined above,
we model an overall display overlap metric regulated by the user
navigation intention, which allows more display overlap for data
browsing but less overlap for data filtering [9]:
(4) O(St, St+1)=ε[w1×Ov+ w2×Os]. where weights w1=w2=0.5,
and ε is a constant, ε = 1.0 for data browsing, otherwise ε = 0.5.
Preserving perceptual landmarks. Perceptual landmarks are distinguishable features that anchor a visual context transition, which
in turn helps users to relate information in successive scenes [19].
For example, the Westchester county map serves as a common
background for all displays (Figures 1–4) and key geographical
landmarks such as major rivers and highways persist in scenes
whenever possible (e.g., Hudson river in Figure 1). To preserve the
maximal number of perceptual landmarks in a visual context, we
count the normalized number of landmarks in the context:
(5) L(St+1) = Lt+1/N, where Lt+1 is the number of landmarks
existing in visual context St+1, and N is the total number of landmarks existing in an entire application.

191

Ensuring smooth transition. Sudden changes in a scene prevents
users from visually tracking the changes. As a result, the causal
connection between an existing scene and a new scene may be lost
[19, 6]. To ensure smooth transitions between successive displays,
animation is often used to provide users with a powerful cue to
interpret the changes [6]. We define a metric to compute the average smoothness of applying a set of visual operators:
(6) T(Op) = Avg[smoothness(opi), ∀ i], where visual operator
opi∈Op, smoothness() is defined by operator cost (Sec 5.1.2):
smoothness(opi) = 1 - cost(opi).
The above metric states that the less mental cost that an operator
incurs, the smoother the transition that the user perceives.
Combining formulas 4–6, we define an overall visual momentum metric to ensure the maximal across-display continuity:
(7) φ(Op, St, S') = Avg[O, L, T], where visual operators Op transform visual context St to incorporate the new scene S'.
5.2.2 Visual Structuring Metrics
In addition to maximizing visual momentum during a visual context transition, we ensure that the structure of the context be coherent after the transition. Since our sketch generation takes care of
the structuring issues regarding visual encoding (Figure 2), here
we focus on the structuring issues concerning visual integration
from two aspects. One is to ensure a proper visual ordering so that
users can easily attend to the new content [12]. The other is to minimize the visual clutter after the integration of new information.
Establishing a proper visual ordering. To establish a proper
visual ordering, we constrain that data items important to the current user query be expressed prominently. For example, in Figure
6(b) RIA highlights the newly requested train station information,
while simplifying the house representations based on their relevance to the train stations. Here the house constraining the train
stations is slightly simplified, while the others are reduced to their
minimum (Figure 6a–b). To capture such desired correlation
between data importance and visual prominence, we define a
visual ordering metric:
(8) ζ ( S t + 1 ) =

∑∑1 –
i

I ( d i, v i ) × P ( v j ) – I ( d j, v j ) × P ( v i ) .

j

Here di and dj are data objects, and vi and vj are their corresponding
encodings at turn t+1. Function I() is the overall importance of a
data objects di and its visual encoding vi using Formulas 1–2:
I(di, vi) = µ1×Id(di) + µ2×Iv(di), where weights µ1=0.7, µ2=0.3
to favor the semantic importance. Moreover, Pt+1() computes the
visual prominence by Formula 3.
Minimizing Visual Clutter. A visually cluttered presentation may
create confusion and make the scene impossible to scan [12]. To
provide an informative but uncluttered visual context, we measure
the overall complexity of a display. Oliva et al. have discussed two
key sets of factors that affect visual complexity [15]. One set of
factors includes the quantity of objects and the variations of their
properties, such as the number of different colors and shapes
appearing in a scene. Another set of the factors is concerned with
the spatial layout of a scene, such as symmetry and openness.
Since our layout manager maintains spatial layout constraints
including symmetry (Figure 2), here we measure visual complexity
using the first set of factors:
χ(St+1) = λ1 × colorVariety(St+1)/Nc + λ2 × areaUsage(St+1)
+ λ3 × shapeComplexity(St+1).
Here weights λ1=λ2=λ3=0.33, Nc is the total number of colors
allowed in one display (now Nc = 7), and colorVariety() obtains the

192

total number of colors used in St+1.
Metric areaUsage() computes the normalized screen space
occupied by St+1:
areaUsage ( S t + 1 ) =

∑ boundingArea ( vi ) ,

where visual

i

object vi∈St+1, boundingArea() returns the screen space occupied
by vi in a normalized screen coordinate.
Metric shapeComplexity() computes the total number of distinct shapes in St+1 and the average complexity of all shapes4:
shapeComplexity(St+1) = shapeVariety(St+1)/N ×
Avg[shapeComplexity(vi)].
Here N is the total of visual objects in St+1, shapeVariety() and
shapeComplexity() are two features defined in Table 1.
To minimize the visual complexity of a scene, we maximize:
(9) ψ(St+1) = γ [1 - χ(St+1)], where γ is a constant, γ = 1.0 for data
browsing to allow more visual objects; otherwise γ = 0.5.
5.3 Simulated-Annealing Algorithm
Combining Formulas 7–9, we define an overall objective function:
reward ( Op, S t, S' ) = w 1 × φ + w 2 × ζ + w 3 × ψ .
(10)
Here Op is a set of visual operators for transforming visual context
St to incorporate new scene S', and weights w1=w2=w3=0.33.
Our goal now is to find a set of visual operators that maximizes
the objective function. This task is to solve a typical quadratic
assignment problem, which is NP-hard [17]. Since a simple greedy
algorithm may suffer from being trapped at local maxima, we
adopt simulated annealing, which has proven to be effective for
solving this class of problems [11].
Figure 7 outlines our algorithm. The input to our algorithm is
visual context S at the beginning of user turn t+1, and a new scene
S' to be integrated. The algorithm uses a “temperature” parameter
T to populate the desired result list iteratively (line 2–15). In our
experiments, T is initialized to be T0=2.0, the minimal temperature
Tmin=0.05 and reduction rate ∆t=0.1, which together control the
number of iterations.
During each iteration, the algorithm samples a set of operators
(now MAX_SAMPLE_COUNT=40) (lines 4–13). In each sampling,
List simulatedAnnealing(Scene S, Scene S').
1
2

List result ← empty
float T ← T0

3
4
5
6
7
8
9
10
11
12
13
14
15

while (T > Tmin) do
for each sample count ∈ [1, MAX_SAMPLE_COUNT]
Operator op ← find_operator(S, S', result)
if (op == null) then return result endif
List currOpList ← result + op
float diff ← reward(currOpList) - reward(result)
if diff > 0 then add op to result
else if probability exp(-diff/T) > rand(0, 1)
then add op to result endif
endif
endfor
T ← T - ∆t
endwhile

16

return result
Figure 7. The outline of our simulated annealing algorithm.

4 Different shapes are assigned different complexity values. For example a
text is considered more complex than a simple geometric shape like circle.

routine find_operator() uses a greedy strategy to find a top candidate (line 5). Specifically, it computes a reward() for applying
already selected operators and an operator op to a visual object that
has not been updated by the same operator (Formula 10). It then
ranks all candidates by their reward values and returns the top one.
Using the top candidate and the existing result set, the algorithm
tests whether the reward be greater than that of using the existing
result set alone (lines 7–8). If it is better, the candidate is then
added to the result set (line 9). Otherwise, it tests whether the current control probability is greater than a random number generated
by rand() between [0, 1] (line 10). If it is true, the candidate is then
added (line 11).
In each iteration, parameter T controls the probability of
accepting sub-optimal operators. It is then gradually reduced so
that the algorithm is less likely to accept sub-optimal operators
(line 14). When the algorithm eventually converges (i.e., T reaches
a target minimum temperature), it returns a set of visual operators
that maximizes our objective function in Formula 10 (line 16). The
complexity of find_operator() is O(n2×m2), where n is the total
number of visual objects in St and S', and m is the number of available operators. Since the number of steps in temperature decrease
and the total number of samples evaluated at each temperature are
constants, the total complexity of our algorithm is O(n2×m2).
5.4 Grouping and Ordering Visual Operators
After finding a set of desired visual operators, RIA groups the
operators by their type and by their operands. For example, RIA
groups together all Highlight operators that have the same type of
operands. RIA then determines the order of applying these operators. Currently, operators within a group are applied at the same
time. Such application guides users to recognize perceptual groupings during visual transition [14]. For example, highlighting a
group of houses simultaneously allows users to perceive them as a
group. Moreover, operators in different groups are ordered by their
temporal priority. For example, Delete normally occurs before Add
to prevent the obsolete data from clobbering the new data. Now the
temporal priority for each type of operator is pre-defined.
6. EVALUATION
We have implemented RIA using Java and C++, with the
graphics renderer running on Linux and all other components on
Windows. RIA has been applied to two applications running on
realistic, sizable data sets. For example, our real-estate data is subscribed from a multiple listing service, containing 2000+ houses
and each with 128 attributes (e.g., price and style). In our experiments, we assign equal weights for metrics initially and tune them
based on application needs (e.g. increasing the weight for visual
momentum to better help a user in visual transitions).
6.1 Experiments
We have tested RIA extensively as a whole by letting users run a
wide variety of queries in a number of experiments. From these
tests, we have collected a set of common queries for both applications. Using these queries, we designed a set of experiments to
evaluate our work on visual context management. Our experiments
consisted of two parts.
First, we randomly chose 50 query sequences from 26 users
with each sequence containing about 10 queries on average. We
ran RIA on these queries using three different visual context management methods: a). without visual context management, b). a
greedy approach, and c). simulated annealing described in this
paper. Similar to the algorithm used in [2], our greedy approach
uses two features, data importance (Sec 5.1.1) and data relevance
(Sec 5.2.1), to guide operator selection. For each query, we
recorded the results produced by each of the three methods: the
operator selected and the order of applying these operators.

100%
75%
50%
25%
0%

No VCM

Greedy

Simulated Annealing

against Designer1

59%

76%

92%

against Desinger2

56%

74%

90%

Figure 8. Experiment result summary for 10 query sequences.

Second, we recruited two graphic designers, whom we asked
to perform visual context management for 10 query sub-sequences
randomly selected from the 50 sequences used above. To minimize
accumulation errors, we used shorter query sequences, each of
which contains 2–4 queries. For each query, we provided the
designers with the data content selected by RIA, the existing visual
context, and a set of available visual operators5. We then asked
them to mark the visual operators that they would use and the order
of applying these operators. The rationale of involving designers in
our evaluation is to test whether our approach performs as adequately as professional designers would do.
6.2 Result Analysis
We compared the results produced by RIA with those made by the
human designers. Specifically, we count the total number of queries where RIA and each designer produced the same results: the
operator set6 and the operator ordering. Figure 8 shows the comparison results. When compared to designer 1, for example, the
simulated annealing produced the same results for 92% of the queries. Unsurprisingly, our optimization-based approach did the best,
the greedy approach came in second, and the results from no visual
context management (No VCM) were the last. Note that even without visual context management, RIA produced results similar to
those of designers for over 50% of the queries. This is because the
same Add and Camera operators were always used for opening
queries in a sequence or for context-switch queries (e.g., Figure 4).
We also carefully examined the differences, which mainly fell
in two categories. One is that RIA chose to zoom in on the new
content (R2' in Figure 1), while the designers added the new content without change the field of view. The designers did explain
their rationale—they assumed that the user was browsing the data.
For validation purpose, we adjusted the user navigation intention
in Formula 9, RIA did produce the same results in these cases. The
other case is that the designers did not seem to order the operators
consistently. For most of the cases, the designers chose to apply
operator Simplify before Add. In several cases, however the order
was reversed without obvious reasons.
Due to the time and effort required, it is impractical to ask a
human designer to perform visual context management for a large
number of queries. For validation purpose, we compared the
results produced by the greedy approach and by the simulated
annealing for all 50 query sequences (a total of 500 queries). These
two approaches produced different results in 25% of the queries.
This is consistent with their results for the 10 query sequences,
where there are discrepancies in 20% of the queries. Most of these
discrepancies rise from the fact that the greedy approach favors the
semantic continuity, while the simulated annealing tries to balance
both semantic and visual continuity. As a result, the greedy
approach tended to delete more content from the existing context.
5 We also let the designers add other visual operators if needed.
6 Two operators are considered the same if the parameters of the computer-derived operator match the qualitative description of the designerderived operator.

193

During our study, we also observed that our designers spent a
considerable amount of time to finish the task for all 10 query
sequences (over an hour). Especially in our experiments, each
query sequence is relatively short so that the designer would not
need to integrate information across many displays. When asked,
they attributed their time cost to weighing all the possible ways of
managing the visual context under various situations (e.g., user
intention and visual continuity). Since our method automatically
balances various constraints, it is very valuable especially for handling continuous and often unanticipated user interaction patterns.
Although the study demonstrated the promise of our work, it
also exposed some of its limitations. First, we would need a finergrained model to determine the temporal order of the operators.
For example, RIA now always applies the same type of operators
simultaneously. As our designers have shown, the same type of
visual operators (e.g., Add) may be applied at different times to
ensure better visual effects. To create a display like Figure 4, our
designer would add the school district boundary first, then the
school icons and labels, and last the details shown in the green
rectangle. Moreover, our current work takes little advantage of the
overall user interaction pattern, although it exploits individual
query expressions (e.g., U2 vs. U2' in Figure 1). In fact, both
designers commented that they would make better decisions if they
knew the user interaction pattern. For example, in one tested
sequence, the same query was repeated a few times. Once our
designer detected this pattern, he kept as much relevant information as possible on the screen.
7. SET UP AND APPLICATION
It takes three steps to set up to use our visual context management
method. First, we define the static features such as assigning data
semantic categories (Table 1). Building a simple data ontology
helps to define these features. Second, we build a catalog of visual
operators (Table 2). Third, we formulate feature-based metrics to
model various constraints important to an application (Sec 5.2).
For example, in a mobile application, we may model devicedependent visual context management constraints [10].
To bootstrap the process and avoid tuning fallacy, we recommend starting with simple settings. So far we have used a simple
data ontology, a set of basic visual operators, and equally weighted
metrics (Formula 10) to adequately handle diverse interaction situations in two different applications. When necessary, it is easy to
extend what we have. First, we can introduce new visual operators
easily (e.g., adding a fisheye view operator for visual morphing).
Moreover, we can easily incorporate new features/metrics in our
objective function for new factors of context management(e.g., a
new metric for modeling user cognitive states).
In addition to supporting multimodal conversation systems
like RIA, our approach to visual context management is applicable
to the broader problem of creating better visualization. For example, it can be used in a GUI-driven interactive visualization system,
where a more coherent visualization can be produced to integrate
information obtained across multiple turns of user interaction.
8. CONCLUSIONS
When creating a visualization in an interactive environment, we
must dynamically decide how to incrementally integrate new
information into existing displays to ensure the coherence of the
overall context. Here we present an optimization-based approach
to visual context management. Given an existing visual context
and the new information to be presented, our goal is to find a set of
visual operators that can best update the existing visual context and
incorporate the new information. To achieve this goal, we formulate a set of metrics to model various context management constraints, such as preserving visual ordering and maintaining visual

194

momentum. Using these metrics, we define an objective function
to assess the overall desirability of applying a set of visual operators. Finally, we use a simulated-annealing algorithm to maximize
the objective function and find the desired operators.
Unlike existing approaches, which often consider a subset of
our constraints in a more deterministic context, our optimizationbased approach dynamically balances a variety of constraints for
diverse interaction situations. It is also easily extensible, since we
can easily incorporate new features/constraints. We have applied
our work to two different applications, and our study shows that
RIA performs adequately against human designers.
REFERENCES
[1]

[2]
[3]
[4]
[5]

[6]
[7]

[8]
[9]

[10]

[11]
[12]
[13]
[14]

[15]

[17]
[18]

[19]

[20]
[21]

[22]

P. Baudisch, N. Good, and P. Stewart. Focus plus context screens:
Combining display technology with visualization techniques. In Proc.
UIST ’01, pages 31–40, 2001.
B. Bell, S. Feiner, and T. Hoellerer. View management for virtual and
augmented reality. In Proc. UIST ’01, pages 101–110, 2001.
I. Biederman. Perceiving real-world scenes. Science, 177(7):77–80,
July 1972.
S. Card, J. Mackinlay, and B. Schneiderman, editors. Information Visualization: Using Vision to Think. Morgan Kaufmann, 1999.
J. Chai, S. Pan, M. Zhou, and K. Houck. Context-based multimodal
input understanding in conversation systems. In Proc. ICMI ’02, pages
87–92, 2002.
B. Chang and D. Ungar. Animation: From cartoons to the user interface. In UIST 93, pages 45–55. ACM, 1993.
M. Chun and Y. Jiang. Contextual cueing: Implicit learning and memory of visual context guides spatial attention. Cognitive Psychology,
36:28–71, 1998.
D. Forsyth and J. Ponce. Computer Vision: A Modern Approach. Prentice-Hall, 2002
I. Fujishiro and R. Furuhata. Gadget/iv: A taxonomic approach to
semi-automatic design of InfoVis applications. In Proc. IEEE InfoVis’00, pages 77–83, 2000.
A. Karlson, B. Bederson, and J. SanGiovanni. AppLens and
LaunchTile: Two designs for one-handed thumb use on small devices.
In Proc. CHI ’05, 2005 pages 201–210, 2005.
S. Kirkpatrick, D. Gelatt, and M. Vecchi. Optimization by simulated
annealing. Science, 220(4598):671–680, 1983.
K. Mullet and D. Sano. Designing Visual Interfaces. SunSoft, 1995.
T. Munzner. H3: Laying out large directed graphs in 3d hyperbolic
space. In Proc. IEEE InfoVis ’97, pages 2–10, 1997.
K. Norman, L. Weldon, and B. Shneiderman. Cognitive layouts of
windows and multiple screens for user interfaces. International Journal of Man-Machine Studies, 25:229–248, 1986.
A. Oliva, M. L. Mack, M. Shrestha, and A. Peeper. Identifying the
perceptual dimensions of visual complexity of scenes. In Proc. the
26th Cognitive Science Society Annual Meeting, 2004.
P. Pardalos and H. Wolkowicz, editors. Quadratic Assignment and
Related Problems, volume 16. American Mathematical Society, 1994.
J. Stasko and E. Zhang. Focus+context display and navigation techniques for enhancing radial, space-filling hierarchy visualizations. In
IEEE InfoVis ’00, pages 57–65.
D. Woods. Visual momentum: A concept to improve the cognitive
coupling of person and computer. International Journal of Man-Machine Studies, 21:229–244, 1984.
M. Zhou. Visual planning: A practical approach to automated presentation design. In IJCAI ’99, pages 634–641, 1999.
M. Zhou and V. Aggarwal. An optimization-based approach to dynamic data content selection in intelligent multimedia interfaces. In
Proc. UIST ’04, pages 227–236. ACM, 2004.
M. Zhou and M. Chen. Automated generation of graphic sketches by
examples. In IJCAI ’03, pages 65–71, 2003.

