12th International Conference Information Visualisation

The Impact of Multiple Coordinated Views
on the Visual Data Exploration and Analysis
1

Michele de Paula da Silva Maciel, 1Bianchi Serique Meiguins, 1Rodrigo Augusto de Moraes
Lourenço, 2Aruanda Simões Gonçalves Meiguins, 1Paulo Igor Alves Godinho
1
Programa de Pós-Graduação em Ciência da Computação – Universidade Federal do Pará (UFPA)
2
Área de Ciências e Tecnologia – Centro Universitário do Pará (CESUPA)
Belém-Pará-Brasil
{chelepsmaciel, piagodinho}@gmail.com;{bianchi, rodrigo, aruanda}@redeinformatica.com.br
The purpose of this paper is to contribute to the
discussion on the impact of multiple views in data
analysis, the contribution of coordination between data
analysis views and the influence of multiple coordinated
views on understanding and task-solving efficiency.
The analysis was based on a tool named PRISMA
that supports multiple coordinated views for three
techniques[2]: treemap, scatterplot and parallel
coordinates. Five smaller tools were extracted from
PRISMA: three tools that implemented each individual
technique, one that implement exclusively the multiple
views on the same screen and one with three techniques
each one in a different screen. All tools supported
coordination between data table, charts and filters.
In order to make a fair analysis, the proposed
taxonomy classified tasks by complexity. All the tools
were used to solve tasks of the same level of complexity.
Complexity was determined by the number of attributes
involved and the type of required answer. Tests were
performed with 10 users and 2 datasets
This paper is organized as follows: an overview on
Information Visualization and Multiple Coordinated
Views; discussion of related work; description of
usability tests and results analysis; final remarks and
future work.

Abstract
The use of information visualization tools that
support multiple coordinated views for data exploration
and analysis has become more frequent. However, few
usability studies have focused this group of tools. The
goal of this paper is to perform usability experiments
with tools supporting multiple coordinated views on the
same screen, in separate screens and tools that support
each technique individually. To ensure a fair evaluation,
a complexity-based task taxonomy was developed. The
tools were therefore used to answer questions of the
same level of complexity.

Keywords--- Information Visualization, Multiple
Coordinated Views, Usability

1. Introduction
Information Visualization (IV) is used to help
people perform tasks such as exploration and analysis of
large amounts of data. There are still great challenges in
this area, regarding ease of understanding, the tasksolving process and usability issues.
According to Chen [1], usability problems in IV is a
reflection of the considerable lack of studies, researches
and further analysis about information visualization
techniques. However, new approaches for the application
of information visualization emerge. An interesting
example of prominent research area in IV is the use of
multiple coordinated views of data. In tools that provide
this mechanism, each view may be represented by a
different visualization technique and each change or
conFig.tion held in one view is reflected in all of the
others. Meanwhile, many concerns arise. Why to use it?
Why not to use it? Is it more difficult to use? Is it more
productive? Which user profile should use it?

1550-6037/08 $25.00 © 2008 IEEE
DOI 10.1109/IV.2008.67

2. Information Visualization
Information Visualization tools are exploratory –
they allow the user to investigate, search for information
and experience scenarios and visual spaces to identify
options and better understand the presented dataset [2]
[1].
To develop visualization systems, it is necessary to
understand better how humans interact with information
and what tasks they intend to perform with the support of
this tool. According to Carr [3], a good visualization tool
must have characteristics defined by the tasks performed
by the user. Among these characteristics, these are
highlighted:

113

•
•
•

•

Overview: the user must win concept about all
the data that will be analyzed.
Zoom: allows focusing on certain subset of data
for analysis.
Filter: allows the reduction of the size of
displayed dataset by eliminating items based on
their attributes.
Details on demand: allows the user to see
details related to a single or set of data.

•

•

There are Information Visualization systems that
offer only one data view. However, there are systems
that have more than one view of data through the usage
of several techniques, to provide different aspects of the
data in order to help in the process of investigation.

3. Task Taxonomy and Visualization
Pillat [7] developed taxonomy of tasks for
multidimensional information visualization techniques in
order to specify the tasks users perform while using a
visualization technique. The proposed taxonomy is
composed of seven tasks: identifying, determining,
viewing, comparing, inferring, configting and locating.
Such tasks can be regarded as user goals or necessities
using a multidimensional information visualization
technique. To evaluate the proposed taxonomy, the
authors performed interaction tests with users composed
of four questions/tasks.
Wiss and Carr [8] made a comparative study of three
3D views to identify relevant factors on the support of
tasks. In this study the views Cam Tree, Information and
Information Landscape Cube were used for the domain
of hierarchical file system. The three selected tasks were
based on the seven Information Visualization tasks
described by Shneiderman: overview, zoom, filter,
details-on-demand, relate, history and extract.
For Laskowski and Plaisant [9], the evaluation of
Information Visualization systems may be analyzed in
three levels: component, system and work environment.
The component level includes individual algorithms,
visual representations, interactive techniques and
interface designs. The system level focuses on interfaces
that combine and integrate multiple components and
need to be evaluated by comparison to the typical
technology used by target users. These evaluations
usually take place in controlled experiments performed
in labs, using scenarios and simple tasks. The work
environment level is related to the description of actual
user tasks in their natural environment.
The taxonomy of tasks presented by Henry and
Fekete [10] evaluates the way Information Visualization
supports the exploration of visual data tables. The
authors developed a controlled experiment to measure
the influence of the table layout influences the
understanding and the exploration process.
The presented works contributed as theoretic
basement for this research, because they fomented the
creation of taxonomy of tasks according to levels of
complexity, which was used in tests with users to verify
the impact of the multiple views in the analysis of data.

2.1. Multiple View Coordination
Multiple views systems use two or more distinct
views to help the process of investigating a single
conceptual entity [4].
The main features of coordination in Information
Visualization are flexibility with respect to data –
possible use of more than one dataset; view – selection
of a type of visual representation for a determined
dataset; and coordination - definition of the
characteristics of coordination between each pair of
views [5].
For the development of Information Visualization
systems with multiple coordinated views, the following
recommendations apply [4]:
Use multiple views when there is a variety of
attributes, models, user profiles, levels of
abstraction or gender;
• Use multiple views when different views
highlight correlations or disparities;
• Use multiple views when the data are complex.
It is recommended to partition data in multiple
views to create manageable data views and
provide ideas for the interaction between the
different dimensions;
• Use multiple views minimally. Justify the use of
multiple views versus user learning cost.
The main possibilities for coordination between the
views are presented below [6]:
•

•
•
•

•

Overlapping data items indicate the relationship
between them.
Label: determines how and which content will
be displayed for each data item. It should be
specified by the user.
Attribute Manipulation: ability for the user to
add or remove attributes from the visual
representation. Users may also change the order
in which such attributes are presented.

Selection: data items selected in a view are
highlighted in other views.
Filter: data items with values within a specific
range will not be presented in any view.
Color, Transparency and Size: items used to
represent the variation of values of a given
attribute. The same criteria is used to represent
the same information in multiple views.
Ordering: the order in which data items are
drawn in the view can be determined by the
values of an attribute in the dataset.

114

origin of the most frequently sold cars?”
Require the analysis of three or four attributes.

4. Experiment
In order to evaluate and design more usable
coordinated views tools it is necessary to understand the
tasks performed by those using the techniques and to
apply this understanding in the development of the
application [11].
Several tools or techniques for multidimensional
information visualization have been recently developed.
However, evaluation issues are seldom approached [7].
In order to measure the influence of multiple
coordinated views in data analysis a taxonomy of tasks
based on complexity level was created. The used tool
was PRISMA, a multiple coordinated views tool that
supports three visualization techniques (Scatterplot,
Treemap, Parallel Coordinates) [12]. Five “new” tools
were extracted from PRISMA: three tools each
implementing one of the techniques individually; one
with three coordinated views presented in the same
screen (Fig. 1); and one with three coordinated views
each one in a different screen (Fig. 2).

•

High-complexity tasks:
Require a more complex answer – an analysis
involving comparison, for exam
example. Require the
analysis of various attributes.

Two datasets were used for the user test – a FIFA
World Cup dataset and an electronic product sales
dataset. Datasets and tasks area available at
http://paginas.terra.com.br/informatica/bianchiserique/ba
ses/datasets.rar and tasks.rar.
normally, in tests with
According to Preece [11],, nor
users, from
m 5 (five) to 12 (twelve) users participate.
participate The
author also tells the tests of usability of a virtual
community called HutchWolrd,, in which seven users had
participated.
Fifteen tasks were specified for each dataset - five
from each complexity level. The tests involved 10 users
with good interaction skills but no experience on
information visualization. During the test, users
answered a questionnaire on task execution and
evaluation
on and afterwards they filled in a questionnaire
on training evaluation.
In the development of the research, each user
should choose one of the questions of a determinated
level of complexity, being that such task could be done
through anyone of the tools. And thus, the research
would continue until the end of the 15 tasks. So, any task
could be done through any tool, what can be proven with
the small number of errors shown in the analysis of the
results.
Experiments lasted three consecutive days. First day
was focused on user training. Users performed medium
mediumcomplexity tasks. On the second day, users answered all
questions related to the FIFA dataset. On the third day,
users answered the questions on the sales dataset.

Fig. 1: Coordinated views on the same screen.

4.1. Analysis Results
There were two approaches for analyzing the results.
The first approach focused on the user-assigned
complexity
lexity level of the tasks and the second considered
con
the evaluator-assigned
assigned complexity levels.
After completing a group of task, the users were
asked to classify
sify the tasks from 1 to 5 in terms of
complexity. 51% of tasks were classified as level 1 or
level 2 (Fig. 3). 40% of these tasks were solved with
w
some use of the coordination resources. 82% of the
answers were correct, 15% partially correct and 3%
incorrect (Fig. 4). In tasks on levels 3 and 4 the correct
answers rate was 68%, 19% were partially correct and
13% incorrect (Fig. 5). For level 5 ta
tasks, 84% were
correct, 8% partially correct and 8% incorrect (Fig. 6).
The exploration of these five tools was the used
strategy to make a comparative analysis (between the
Multiple Coordinated Views in a screen, Multiple

Fig. 2: Coordinated techniques in separate
screens
The degree of complexity is divided into high,
medium and low. This classification refers to the kind of
answer and to the number of attributes involved:

•

Low-complexity tasks:
Require a simple, "yes" or "no" answer. Require
the analysis of only one or two attributes.

•

Medium-complexity tasks:
Require a simple answer, but concrete, not just
positive or negative. For example, “what is the

115

Fig. 5 : Correctness rate for tasks of complexitycomplexity
level 3 and 4

Coordinated Views in separated screen and individual
techniques) which could base the objective of the
research that is to verify the impact of the MV in the
analysis of data.

1

T

a

s

k

C

U

o

s

e

m

r

p

P

l

e

e

r

x

s

i

p

t

y

e

-

c

L

t

e

i

v

v

e

0

0

8

0

%

6

0

%

0

%

0

%

0

%

l

e

4

L

e

v

e

l

1

a

n

d

2

l

e

v

e

l

3

a

n

d

4

L

e

v

e

l

5

2

9

%

%

P

C

o

r

r

e

c

t

a

r

t

i

a

l

l

y

P

r

c

r

c

C

o

o

r

d

i

n

a

t

e

0

%

5

1

e

I

c

n

h

d

i

i

v

i

n

i

d

q

u

0

0

%

9

0

%

8

0

%

0

%

0

%

0

%

0

%

7

complexity-level

-

e

5

4

r

e

t

o

c

n

4

%

8

%

8

%

c

h

n

u

i

q

a

u

l

e

6

2

0

%

1

3

%

7

Fig. 6: Correctness rate for tasks of complexitycomplexity
level 5

User

When analyzing the tasks in terms of the complexity
level assigned by evaluators, the rate of correct answers
reduced according to thee raise of complexity (Fig. 7).
1

6

I

e

%

T

1

t

c

%

T

Fig. 3: Tasks
perspective.

e

d

8

4

r

o

0

0

%

8

0

%

0

%

0

%

0

%

0

%

6

3

0

%

4

2

0

%

2

1

0

%

0

%

P

C

o

r

r

e

c

a

r

t

i

a

l

l

y

t

r

c
r

t

r

c

C

o

r

r

e

c

t

P

a

c

a

o

r

r

e

l

l

c

y

I

n

c

o

r

r

e

c

o

o

r

d

i

n

t

e

r

e

o

t

I

r

e

t

c

n

c

t

t

H

C

o

i

i

g

h

5

8

%

3

0

%

1

1

%

4

%

8

%

%

9

%

9

%

d

a

3

%

M

e

d

i

u

m

8

T

e

c

h

i

n

i

q

u

e

8

2

%

1

5

L

I

n

d

i

v

i

d

c

h

n

i

q

u

e

7

9

%

1

1

0

0

%

9

0

%

8

0

%

0

%

0

%

0

%

0

%

3

0

%

2

0

%

1

0

%

0

%

7

6

5

4

r

o

r

r

e

c

t

P

o

o

r

d

i

n

t

e

t

0

Fig. 7: Correctness rate per task complexity evaluators perspective.

a

o

r

r

e

l

c

l

y

I

n

c

o

r

r

e

c

t

t

d

a

1

I

e

n

c

h

d

i

i

v

i

n

i

d

q

u

e

6

8

%

1

9

%

%

1

8

%

3

l

8

e

c

h

n

%

u

a

T

%

%

i

a

c

T

0

complexity tasks the correctness rate was
For low-complexity
similar for the use of individual techniques and of
coordination (Fig. 8).

C

C

9

%

Fig. 4 : Correctness rate for tasks of complexitycomplexity
level 1 and 2

1

w

l

1

e

o

u

a

T

1

%

i

q

u

e

7

4

%

116

1

0

0

%

9

0

%

8

0

%

0

%

0

%

0

%

0

%

0

%

2

0

%

1

0

%

0

%

Fig. 10: Correctness rate per technique on highcomplexity tasks

7

In average, it is possible to realize that for tasks of
higher complexity the coordinated techniques are more
useful. For simpler tasks the results are similar for
individual or coordinated techniques.

6

5

4

3

1

C

C

o

T

o

e

r

c

d

i

h

i

n

a

n

i

t

e

q

u

o

r

r

e

c

t

I

n

c

o

r

r

e

c

0

0

%

8

0

%

6

0

%

0

%

0

%

0

%

t

d

e

9

0

%

9

2

%

1

0

%

4

I

n

T

d

e

i

c

v

i

h

d

u

n

i

a

q

l

u

e

8

%

2

Fig. 8: Correctness rate per technique on lowcomplexity tasks

a

C

o

r

r

e

c

I

F

A

C

u

o

5

For medium-complexity tasks, the correctness rate
was 85% for the use of coordination against 76% for the
use of individual techniques (Fig. 9). In high-complexity
tasks the correctness rate was 58% for coordinated views
and 60% for individual views (Fig. 10).
1

0

0

%

9

0

%

8

0

%

0

%

0

%

0

%

0

%

0

%

2

0

%

1

0

%

0

%

d

u

c

t

a

C

o

r

r

e

c

t

r

t

i

a

l

l

I

o

r

r

e

c

n

c

o

r

r

e

c

2

%

1

e

i

n

i

q

3

5

%

3

0

%

5

n

d

u

e

i

c

v

i

h

d

u

n

i

a

q

%

9

1

%

%

2

8

%

%

5

8

%

1

0

%

2

5

%

2

0

%

u

1

5

%

1

0

%

1

%

%

3

%

l

1

7

e

t

%

1

I

T

c

%

2

h

e

d

2

c

1

%

2

t

5

e

r

7

%

3

T

r

t

2

a

o

t

2

n

c

y

P

c

i

n

Analyzing the time for solving the tasks, on the
interval between 0.5 and 4 minutes there was 52% of
low-complexity tasks, between 4 and 10 minutes there
were 67% of medium complexity tasks and among tasks
that took more than 10 minutes to be solved, 66% were
classified as high-complexity tasks (Fig. 12). For tasks
solved with individual techniques the average duration
was 7.07 minutes and for tasks supported by coordinated
views the average duration was 7.76 minutes.

3

d

y

t

Fig. 11: Correctness rate per technique on for
medium and high-complexity tasks

4

r

l

6

8

5

o

l

c

P

6

o

a

e

s

7

7

C

i

r

4

%

o

r

p

6

r

t

I

c

F

r

P

t

1

%

8

6

e

%

1

2

%

1

2

%

%

5

%

5

2

Fig. 9: Correctness rate per technique on
medium-complexity tasks.

5

%

0

%

0

|

-

-

2

2

|

T

1

0

0

%

9

0

%

8

0

%

0

%

0

%

0

%

0

%

0

%

2

0

%

1

0

%

0

%

e

-

-

c

4

.

4

I

n

d

i

v

i

d

u

|

a

-

l

-

6

6

T

e

c

.

|

C

-

o

-

1

o

0

r

1

d

e

n

a

d

0

|

-

-

1

5

%

%

1

5

|

-

-

a

Fig. 12: Distribution of task-solving method:
individual or coordinated techniques

7

6

5

4

The use of tools and comprehension of the
techniques evolved from the first day (world cup dataset)
to the second (product sales dataset). The rate of correct
answers increased accordingly.(Fig. 13)

3

a

C

o

r

r

e

c

t

o

o

r

d

i

n

a

t

e

I

e

n

c

h

d

i

i

v

i

n

i

d

q

u

u

e

a

e

c

h

n

i

q

u

i

r

3

8

%

a

r

e

l

c

l

y

n

c

o

r

r

e

c

t

t

0

%

6

6

%

%

l

6

T

o

d

5

T

t

I

c

C

r

P

e

3

2

%

8

%

117

1

0

0

%

8

0

%

0

%

0

%

0

%

0

%

The auxiliary charts were more frequently used with
the individual techniques approach (11%) than with the
coordinated ones (6%).
According to 90% of the users, the experience with
the visualization tool was considered easy, practical, fast
and playful. The involvement of the user in the usage of
the tool was perceivable. However, the users had argued
that a tool like that requires graphical interpretation, and
perhaps it could confuse the development of the task.
This also was proven by the evaluator during the tests,
because the users made the correct procedure in the tool
during the development of a task, found the answer, but
they could not "see it", because it was in the graphical
form. But besides, the users had supported the premise
that the graphical form had great importance because of
the amount of data. In relation to the user’s profile, it was
possible to observe that any person is able to use a tool
of visualization of information well, however it also is
notable that people who had certain knowledge in
relation to the content of the database, did the tasks with
more ability because they could interpret the data with
more easiness.

6

4

2

a

C

o

r

r

e

c

r

I

F

A

C

u

o

5

d

u

a

r

e

l

c

l

y

n

c

o

r

r

e

c

t

t

4

%

o

r

p

6

r

i

I

c

F

t

P

t

c

t

2

%

1

1

%

s

7

P

6

8

%

7

%

%

Fig. 13: Correctness rate per dataset
Fig. 14 presents the correctness rate for tasks using
coordinated views on both datasets.
1

0

0

%

8

0

%

0

%

0

%

0

%

0

%

6

4

2

a

C

o

r

r

e

c

t

i

f

f

e

r

e

n

t

s

c

r

e

e

m

e

s

c

r

e

e

i

r

5

a

r

e

l

c

l

Conclusions

y

n

c

o

r

r

e

c

t

t

The goal of this paper is to contribute to the
discussion about the usability of information
visualization tools, particularly the ones supporting
multiple coordinated views. The paper presented a study
that used one of these tools. The tool PRISMA was
divided into five tools, three of them supporting one
technique individually and two supporting coordinated
techniques. Additionally, tasks were elaborated based on
complexity taxonomy.
In general, the analysis of the correctness rate shows
that the coordinated features are better explored for tasks
of higher complexity. When comparing the two
coordinated approaches, the correctness rate was better
when the views were presented in the same screen.
Therefore the ability to associate the represented
information in different techniques and separate screens
may be a difficult task for the users. However, after the
tests many users emphasized the initial impact of
multiple views on the same screen and 40% affirmed
they would prefer only two views on the same screen
instead of three. Although it was possible to resize the
area of the view and make only one of the techniques
visible, this feature was never used. When questioned
about the two coordinated approaches 70% of the users
preferred the coordinated visualization techniques on the
same screen.
Another point for discussion is about the diversity of
answers provided by the users. The analysis of the
answers became very complex because of the lack of
standardization. The evaluation would certainly be easier
if the elaborated questionnaire had provided instructions
or models to help the users formulate the answers.

4

%

a

o

n

7

s

t

I

c

d

r

P

1

%

1

1

%

n

7

3

8

%

1

9

%

%

Fig. 14: Correctness rate for coordinate views
When using coordinated techniques on the same
screen the average time to complete the tasks was longer
(8,38 minutes) than when using coordinated views on
separate screens (7,08 minutes).
According to Carr [3], managing the space of a
monitor is a complex task in the field of the VI, the
standard alternative is the usage of windows and the
relationship established between them is relevant, since
visualization of information is much more than graphical
operations. Thus, the analysis of the data becomes more
practical through the comparative analysis with all the
elements in the same screen, in a reach of a movement of
the eyes and if the user feels the necessity to expand the
data, he/she can make this too.
The average number of times filters and other
conFig.tion settings were used with coordinate views on
the same screen was 1.32 and on different screens 3.17.
Thus the users apply filters and other settings more
frequently when using the coordinated views on separate
screens.
Around 30% of the users interacted with the data
table to compare data (23%), learn about data (5%) and
to select items by brushing (2%). Details on demand
were used by 62% of the users. This behavior was
similar when using any of the tools.

118

There are some interesting directions for future
work:
•
•
•
•

Re-evaluation
of
the
questions
and
standardization of the answers
Use of different datasets in a larger study
Use of different visualization techniques and
tools.
Use of different user profiles.

References
[1]

Chen, C. and Cserwinski,M. (2000) “Empirical
Evaluation
of
Information
Visualization:
An
introduction”, Int’l J. Human-Computer Studies,
vol.53.pag 631-635.
[2] Spence, R. Information Visualization. Addison Wesley ACM Press, 2001. 459 p.
[3] Carr, D. A. (1999) “Guidelines for Designing
Information Visualization Applications”. Proceedings of
ECUE’99. Stockholm, Sweden. December. 1999.
[4] Baldonado, M. Q. W.; Woodruff, A.; Kuchinsky, A.
Guidelines for using multiple views in information
visualization. Proceedings of the working conference on
Advanced Visual Interfaces, pp. 110 – 119. Palermo.
Italy. 2000.
[5] North, C. and Sheneiderman, B. (2000) “Snap-Together
Visualization: A User Interface for Coordinating
Visualizations via Relational Schemata”. ADVANCED
VISUAL
INTERFACES
International
Working
Conference May 23-26.
[6] Pillat, R. M.; Freitas, C. D. S. Coordinating Views in the
InfoVis Toolkit. Proceedings of Advanced Visual
Interface. pp. 496-499. Venezia, Italy. 2006.
[7] Pillat, R. M., Valiati, E. R. and Freitas, C. M. D. (2005)
“Experimental Study on Evaluation of Multidimensional
Information Visualization Techniques”. In: CLIHC'05,
Cuernavaca - Mexico. p. 20 - 30.
[8] Wiss U., Carr D.: An empirical study of task support in
3d information visualizations. In Proceedings of the
International Conference on Information Visualisation
(IV) (1999), IEEE Computer Society, pp. 392—399.
[9] Laskowski, S., Plaisant, C., Evaluation Methodologies
for Visual Analytics section 6.1, in Thomas, J., Cook, K.
(Eds.) Illuminating the Path, the Research and
Development Agenda for Visual Analytics, IEEE Press
(2005) 150-157.
[10] Henry, Nathalie, Fekete, Jean-Daniel, Evaluating Visual
Data Understanding. Proceedings of the 2006 AVI
workshop on BEyond time and errors: novel evaluation
methods for information visualization; p. 1-5. Venice,
Italy. 2006.
[11] Preece, J., Rogers, Y. and Sharp, H. Interaction Design:
beyond human-computer interaction. Wiley. 2005
[12] Godinho, I; Meiguins, B.;Gonçalves, A.; Carmo, C.;
Garcia, M.;Almeida, L.; Lourenço, R. PRISMA – A
Multidimensional Information Visualization Tool Using
Multiple Coordinated Views. Proceedings of the 11th
International Conference on Information Visualization,
pp. 23-32. Zurich, 2007.

119

