Sonification of Remote Sensing Data: Initial Experiment
Frances L. Van Scoy
Virtual Environments Laboratory, West Virginia University, Morgantown, WV, USA
Virtual Systems Laboratory, Gifu University, Gifu, Japan
fvanscoy@ wvu.edu

based on the intensity of the corresponding pixel.
Sounds generated for some images, such as concentric
circles, are fairly easy to recognize with only a small
amount of practice, but the sounds are not musical.
The NCSA Biological Imaging Group [ I ] observed
that many early attempts at sonification were difficult to
listen to for long periods of time, because the tones
generated were repetitious and the chords typically
dissonant. Their goal in the CRUMBS project was to
build a system that would generate sound which would
be both melodic and unobtrusive. We share that goal.
Our previous and current work [4] differs from theirs
in that we can express many more distinct values than
does the CRUMBS project and our music is intended to
be pleasant to listen to, unlike the sounds produced by
the vOICe project.

Abstract
We are generating music from a particular view of a
multi-dimensional geographic information system (GIS)
data set to alert a viewer to the existence of hidden
clusters of data points. This paper describes some
results from an early experiment with generating music
from a 2-d slice of the data and testing whether these
representations are easily understood by beginning
users.

1. Basic Definitions
Recently, the term "visualization" has taken on the
extended meaning of the "presentation of information to
the ears, or the fingers, or even the nose "[3]. This paper
describes work in sonification.
Sonification is the conversion of relations in a data
set, generally numeric, into relations in an auditory
range, or simply, the generation of sound from data.

3. Our Previous Work
Our previous work has emphasized the generation of
listenable tonal music.
We initially explored describing mathematical
functions by mapping equal size intervals of numbers in
the domain of the function to a note in a chromatic scale
of one or more octaves, where the larger the numeric
value of the number, the higher the pitch of the note. This
resulted in music that was somewhat dissonant, such as
the representation of a circle, shown in Figure 1.

2. Related Work
Our work is related to the CRUMBS project by
Rachel Brady and others [I] and the vOICe project by
Meijer [2].
The CRUMBS project divides a range of data values
into five classes and then plays between one and five
parts (piano, guitar, flute, and so forth) of musical
composition to indicate the class of the current value of
interest. The music generated varies somewhat over time
to reduce listener fatigue.
The vOICe project generates sound from a 64 x 64
pixel image in grey scale. The image is scanned from left
to right, with a sound generated based on the values in
each column. Each row represents a different frequency.
The sound generated for a column is a combination of
these frequencies, with the volume of each frequency

I

A

453
0-7695-0743-3100
$10.000 2000 IEEE

I

I

I

I

I

I

I

I

I

I

I

I

I

I

I

I

.

I .

game of 40 measures. Each player was represented by a
different pitch in the chromatic scale. Each measure
contained one beat of music using the pitch assigned to
each player on the court during that minute. (We
arbitrarily used the pitch for the five players playing the
most seconds in a given minute.) If WVU was ahead by
one or two points, each beat contained one quarter note.
As WVU increased its scoring lead, each beat contained
two eighth notes, four sixteenth notes, or eight thirtysecond notes. When WVU fell behind in points, the
music changed to a minor key.
We observed three problems in this approach:
deciding how to assign which pitch to which player;
generating music with cadence, a sense of completeness
at the end; and retaining player information when in a
minor key
(1) The basketball playing positions are one center,
two forwards, and two guards. Because we wanted to
maintain a well-defined pitch center we assigned to the
first-string center the dominant pitch and to a back-up
center the note one octave higher. In this way we were
assured that there would always be a dominant in each
measure. We assigned to the first-string forwards and
guards the notes re, mi, fa, and sol, to regular substitutes
la and ti, and to other players the remaining notes of the
chromatic scale. This assignment of pitch, however,
required a priori knowledge of position played and
approximate relative amount of playing time for each of
the twelve team members.
(2) We wanted to avoid listener fatigue caused by
playing the same notes in the same order in adjacent
measures (which would happen, especially in the first
five to seven minutes of the game when only the firststring players were playing). So, we decided to use a
cycle of permutations with period 40, since there are
forty minutes in a game. As a player entered the game,
his note replaced in the permutation cycle that of the
player he was replacing. We chose a particular cycle
from English change-bell ringing because such patterns
are generally accepted as pleasing to listen to by British
and American listeners and because they end with a
sense of completion (i.e. have cadence).
(3) In the initial work described in the paper cited
here, we never resolved the problem of retaining player
identify when in a minor key. We wanted to evoke a
sense of sadness when WVU was losing by flatting the
notes for one or two players, thus putting those measures
into a minor key. However, some combinations of notes
have no value which when flatted changes the music into
a minor key. So, we arbitrarily went to a minor key and
lost all identification of notes with players. More
recently we are combining the individual notes
representing players with a chord in each measure, where
the chord indicates which team is leading, for example C

However, a medical student with a masters degree in
artificial intelligence and an avocational interest in jazz
music observed that he liked the resulting music, because
it sounded like jazz. We ignored his comment for
several months, and worked at finding ways of
generating tonal music from complex data sets.
In the past year we have focussed our attention on
data from two sources: basketball games and remote
sensing geographical information system (GIS) data.

4. Basketball Data
The basketball data we have sonified consists of
minute-by-minute data from West Virginia University
basketball games for the 1997-1998 and 1998-1999
seasons. For each minute, the data shows which WVU
players played during that minute (with game clock time
of each substitution of player) and for each player the
number and kind of each score made and the number of
fouls charged. The data also shows the score for WVU
and the opposing team at the end of that minute.
Our goals in sonifying this data have been two:
(1) to assist the listener in recognizing which
combinations of players perform well together (in
meeting the general goal of increasing the number of
points scored by WVU relative to those scored by the
opposing team)
(2) to determine whether a listener can distinguish
between games in which the coach substituted players
according to a rotation which emphasized five strong
players and a small number of supporting players
brought in to give rest breaks to the others and those in
which the coach tried various combinations of players
throughout the game.
These goals derived from the different nature of the
two seasons for which we had data. In the 1997-1998
season there were seven strong seniors on the teams.
Five of these, the "first-string," generally played most of
the game, with the other two playing for a few minutes at
a time to provide rest breaks for the five starters. This
team was quite successful, and played in the Sweet
Sixteen round of the NCAA conference at the end of the
season. These seven students completed their college
eligibility, however, and in the 1998-1999 season the
team was comprised of freshmen, sophomores, junior
college transfer students playing their first WVU season,
and upper classmen who had not played much during the
previous season. This team had a very difficult season,
and lost more games than they won.
We generated music in the following manner. We
produced one measure of music for each minute of
playing time, giving us a musical composition for one

454

means our team is winning and Am means the other team
is winning.

5. Remote Sensing GIs Data
The GIS data we are sonifying is remote sensing data.
The simplest data consists of near infrared, red, and
green intensity readings at each point in a plot. This
data is a list of 5-tuples: x-coordinate, y-coordinate, near
infrared intensity, red intensity, and green intensity),
where the intensity values are integers in the range 0
through 255. GIS users then compute a second list of 4tuples: near infrared intensity, red intensity, green
intensity, and frequency of occurrence of this triple of
intensity values in the plot. They then plot the 4-tuples
in 3-d space (along the NIR, R, and G axes), using other
methods such as brightness of the point in NIR-R-G
space to indicate frequency of occurrence. By studying
clusters of points in NIR-R-G space, GIS users can
determine attributes of the plot under study such as the
presence of deciduous or coniferous trees. However
even in the 3-d case described here one cluster of points
can obscure from view another significant cluster of
points. This is illustrated in Figure 2, in which the
picture on the left could be the projection onto the x-y
plane of either of the 3-d sets of points shown on the
right. We are generating music that will alert a GIS user
to the existence of obscured clusters.

Figure 3. Music Generated from Three Triples
When generating music from basketball game data
there is a natural linear progression from the data
representing one minute to the data representing the next
minute. For our 3-d data there are many ways to choose
subsequent (x,y) pairs. We might scan by rows or by
diagonals. Figure 4 shows two possible ways to scan
data in the x-y plane, by rows or by lines at a 45" angle.

Figure 4. Two Ways to Order the Selection of
(x,y) Coordinates
As we visit each (x,y) pair according to one of these
scanning patterns, we generate a measure of music based
on the z coordinates of data points with these (x, y)
values.
When we build the full system, we intend to allow a
user to choose two x values and two y values, and then
the system will scan the enclosed rectangle by rows,
generating music at each point.

Y

X

Figure 2. Two Dimensional Projection of Either
of Two Three-Dimensional Data Sets
In general we begin by studying the data with respect
to the x-y plane. We choose a range of notes and then
map the potential z values between 0 and 255 to those
notes. For each cell in the x-y plane we generate a
measure of music containing the notes. For example, if
we had the (x,y,z) triples (10, 10, 1), (10, 10, 4 3 , (10,
10, 105) and (10, 10, 147) and mapped the integers 0 to
255 to a one octave chromatic scale, with 13 notes, we
might generate the notes C, D, F, and G, based on the
various z values. That is, we map z values between 0
and 20 to C, between 21 and 42 to C#, between 43 and
63 to D, and so forth.

Figure 5. Result of allowing user to choose two
x and two y values to delimit area to be scanned
The result will be music which for each cell in the
selected region of the (x,y) plane plays notes

455

positions for a particular x-interval contain points. Note
that it is easy to define a data set which generates music
lacking cadence.

corresponding to the z values at which the (x,y,z) triple
is present in the data set.

6. The 2-Dimensional Data Experiment

7. Data Sets Implemented
A major problem is: can we generate music which will
enable a user to recognize the hidden structure of the
data? We have conducted a Preliminary experiment
using two-dimensional data only, uncomplicated by
choice of scanning algorithm described previously.
For example let the following three sketches in Figure
6 indicate clusters of points in (x,y)-space.

We constructed music for the 12 data sets pictured in
the Appendix. The music generated from each data set is
also given in the Appendix.

8. The Design of the Experiment
We explained to each experimental subject the
concepts behind the experiment and then demonstrated
two examples of music generated from simple data sets
by our scheme. We then presented pictures of the twelve
2-d data sets shown in Figure 1 (but arranged in a
different order, and labeled with letters) and asked the
subject to study them. We next gave the subject access to
a web page with links to 12 MIDI files, one for each data
set. We asked the subject to play a MIDI file and then
select the data set which it represented. When working
with an individual subject we allowed the subject to replay the MIDI file as often as desired, and we maintained
a count of the number of times each subject played each
file. (We also did the experiment in a class. The
students requested that the second and third melodies be
replayed, but did not ask for others to be replayed.) We
did not give the user any feedback on correctness of
answers until the end of the testing session.

Figure 6. Three sets of points in two dimensions
When projected onto the x-axis, each set of points
appears the same, one large cluster along the x-axis. The
music we generate from each is very different. Two
parallel clouds generate:

Figure 7. Music from parallel horizontal lines

9. Observations from the Experiment

The "rotated T" generates:
This initial experiment is not a scientific study. We
tested a total of 32 subjects, faculty and students at West
Virginia University and Gifu Uniiersity and friends and
colleagues. The number of people, classified by country
of birth and student or nonstudent (faculty, staff,
alumnus), correctly identifying each numbered image
from music is shown in Table 1.

Figure 8. Music from rotated T
And, the "rotated and skewed Y" -generates:

Table 1. Number of subjects identifying each
music-image pair correctly
Figure 9. Music from rotated and skewed Y

melody

We construct the music by dividing Cartesian space
into a grid. In our simple examples here the grid is 8 x 8.
Each interval along the x-axis represents two beats, and
each interval along the y-axis represents a note in the C
major scale. Notes are chosen based on which grid

non US

Total

student

student

4

456

12

4
5

1
1

2
4

1
1

1
2

1
1

4
5

1
1

4
5

I

I

11
16

Table 2 shows the number of individuals in each
category receiving each possible score in the exercise.

Table 2. Number of subjects with each score
number
Correct

I
0

I

non US
I non

student

1

I student
I o

I
I student
I
I 0

us

I

I non
I student I
l o I

Toti'

1

1

Since music 1 was one of the sample ones played for
all subjects before the exercise, it is not surprising that it
is the one most frequently correctly identified.
However, music 4 is only a slight variation of the other
sample played for all subjects before the exercise, and it
was correctly identified only 11 times, tied for lowest. In
fact, music 4 was missed by both people receiving scores
of 10. (One of them also missed number 8, and the other
missed number 11.)
Again, due to the small sample size, it is hard to make
conclusions about what made a particular melody easier
or harder to identify. Three of the four pictures
consisting only of horizontal lines were correctly
identified by more than half of the subjects, but the
fourth such picture was recognized by only I 1 of 32
subjects, tied for last. Two of the three pictures
recognized by the fewest subjects consist of two parallel
lines at a 450 angle. For one of these, picture 3, 12
people correctly recognized it from the music while 6
misidentified its music as belonging to picture 4.
However, no one misidentified the music belonging to
picture 4 as belonging to picture 3; rather this music was
most frequently incorrectly scored as belonging to
pictures 8, 11, and 7, other pictures with a line at a 450
angle.

10. Conclusions and Future Work

12
total

This initial experiment used a very small population
and did not control for variables. We have begun
collaborating with a researcher in testing and evaluation
to design a proper formal experiment. We expect to
complete the design of this experiment during summer
2000 and then conduct the experiment in fall 2000 with a
large population so that we can obtain meaningful
results.
The initial experiment described in this paper involves
just one slice of the 3-d data being observed, not a 3-d
set as is our simplest GIS data. We need to expand the
experiment to music generated from these 3-d sets.
We also intend to extend this work to allow the user
to select any viewing perspective, not just those parallel
to the axes.
This work has the potential to become much more
complicated, as GIS users often sample intensity at more
than 3 positions on the electromagnetic spectrum. In
future work we hope to address the problem of
generating music for 4-and higher-dimensional systems.

4
17

3

7

32

We are somewhat discouraged by the relative poor
performance of our subjects. While only 25% received a
perfect score, over half had one third or fewer correct
answers. We observe that all of those with perfect scores
were born in the US but don't know whether that is due
to the western cultural bias in our music (almost all of the
non US respondents are from China, India, or Japan;
only two are Europeans) or the greater emphasis on
music education in US elementary schools. We intend
to design a formal experiment this summer to test for
factors such as age, place of birth, and musical
experience and preference. We also recognize the need
to develop a more formal "training" of subjects before
they do the exercise and hope that the institution of such
will improve performance.
Table 1 above shows the obvious: some melodies
were easier to recognize than others.

11. References

457

[ l ] Brady, Rachel, Robin Bargar, h o o k Choi, and Joseph
Reitzer, Auditory Bread Crumbs for Navigating Volumetric
Data, ZEEE Visualization ‘96,San Francisco, October 1996.

WV EPSCoR and the WVU Department of Computer
Science and Electrical Engineering. Some of the work
was done at the Virtual Systems Language, Gifu
University, Gifu, Japan. The question addressed in this
paper came fiom a discussion with Joe Sewash then of
the West Virginia State GIS Technical Center, located at
West Virginia University.

[2] Meijer, Peter, The VOICE - Seeing with Sound,
httD://ounvorld.comDuserve.com/homeDages/Peter
Meiier/voice
.htm

[3) National Science Foundation, Large Scientific and Software
Data Set Visualization Program Announcement, document NSF
99-105, April 7, 1999.

[4]Van Scoy, Frances L., Sonification of Complex Data Sets:
An Example from Basketball, Proceedings of VSMM’99,
(Dundee, Scotland, September 1-3, 1999), 203-216.
htt~:Nwww.csee.wvu.edu/-vanscov/vsnun99/VSO~

.HTM

Acknowledgments
This work is supported by the EPSCoR programs of
the National Science Foundation and the State of West
Virginia and uses the facilities of the West Virginia
Virtual Environments Laboratory which is supported by

458

-

Appendix Music Generated from Images
Image

.

Music

II
0
0
I

459

I

I

I

I

I

-

19

460

l

l

I

. .

.

.

.

