Interactive Art for Zen: “Unconscious Flow”
Naoko Tosa, Ryohei Nakatsu
ATR Media Integration & Communication Research Laboratories
2-2, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288 Japan
Phone: +81-774-95-1401
{tosa,nakatsu} @mic.atr.Co.j p

Abstract
In face-to-face communications, the occasional need
for intentional lies is something with which everyone
can identify. For example, when we get mad,
circumstancesmayforce us to put on a big smile instead
of expressing our anger; when we feel miserable,good
manners may dictate that we greet others warmly. In
short, to abide by social norms, we may consciously lie.
On the other hand, if we consider the signs that our
bodies express as communications (body language),we
can say that the body does not lie even when the mind
does. Considering this phenomenon, we propose a
means of “touchingthe heart” in a somewhat Japanese
way by measuring the heartbeat of the “honest”biody
and using other technologies to develop a new code of
non-verbal communicationsfrom a hidden dimension
in society. We call this “Meditation art” or “Zen
Art.” “Zen” is Buddism style meditation.

The authors are interested in how to recognize unconscious
feelings by using computer-based interaction (Figure.1). Art
is a natural way to portray human unconscious emotions. We
have been trying to achieve this in interactive art by using the
technologies and techniques of art.

Figure 1. Mask of the human face

2. Concept of Unconscious Flow

Keywords: HCI (Human-ComputerInterface),
Character Behavior;Mixed Reality, Real-time
Animation. InteractiveArt

Two computer-generated mermaids have been
function as individual agents for two viewers. Each
mermaid agent moves in sync with the heart rate
detected by a BVP (blood volume pulse) sensor attached
to a finger of its viewer. Then, using a synchronization
interaction model that calculates the mutual heart rates
on a personal computer, the two mermaids express the
hidden non-verbal communications of the viewers. The
relax-strain data calculated from the heart rates and the
level of interest data calculated from the distance
between the two viewers are mapped on the model. The
synchronization interaction model reveals
communication codes in hidden dimensions that do not
appear in our superficial communications.

1. Introduction
The author believes that interactive art is one type of
component that provides sympathy in communications.
Interactive art can be thought of as an emotions and sympathy
interface. It is familiar to us and forms agents or characters
that can handle sensitive communications. In addition, such
agentslcharacters work on our mental states and emotional
expressions, and on our character and intelligence.This means
that a person can also self-create his or her own personality.
On the other hand, emotion recognition technology recognizes
only the surface emotions of people [1][2][3].’

Then, using a camera to pick up hand gestures and a
personal computer t o analyze the images, the

535
0-7695-0743-3100$10.00 0 2000 IEEE

3. Synchronization Interaction Model

synchronization interaction model is applied to
determine each mermaid’s behaviors. For a high degree
of synchronism, the agents mimic the hand gestures of
their subjects, but for a low degree of synchronism, the
agents run away. As for the background sound, the heart
sounds of the subjects are picked up by heart rate sensors
and processed for output on a personal computer for
bio-feedback.

The relax-strain data calculated from the heart rates
and the level of interest data calculated from the heart
rate variations are mapped o n the model. T h e
synchronization interaction model reveals
communication codes in hidden dimensions that do not
appear in our superficial communications (Figure 3).

+

For the installation, a space of four meters wide, four
meters deep and three meters high is required. A dark
and quiet space is preferable. Interactive actions are
displayed on one main screen and two Japanese “shoji”
screens. A Japanese “hinoki” wooden bucket with a
diameter of one meter that is filled with water is placed
in the center of the installation. Two persons, each fitted
with a stethoscope, experience non-verbal
communications by touching their Computer Graphics
embodiments in the bucket. This was shown at the
SIGGRAPH’99 Art Show. People of many nationalities
interacted with the [Unconscious Flow] (Figures 2-1.
and 2-2).

Depending on the levels of the users’ feelings as
evaluated by the interestealess interested and strained
relaxed axes, the following four types of animations are
generated.

InI.n..Id

Femele’s data

Relaxed

Stmind

Male’s data

Less interested

Figure 3. Synchronization interaction model

(1) When both people are in a situation where they are
highly relaxed and interested, they are considered
synchronized. An animation is generated in which, for
example, their Computer Graphics-reactive
embodiments join hands in companionship or enjoy
friendly actions (Figures 4-1 and 4-2).

Figure 2-1. [Unconscious Flow] exhibited at
the SIGGRAPH’99Art Show

Less interested

Figure 2-2. [Unconscious Flow] exhibited at
the SIGGRAPH’99 Art Show

Figure 4-1. Highly relaxed and interested

536

(3) When both people are in a situation where they are
highly relaxed and less interested, they are considered
indifferent and “going their own ways”. An animation
is generated in which, for example, their Computer
Grahpics embodiments do not interfere with each other
(Figures 6-1 and 6-2).
1nt.ru.t.d

I

Figure 4-2. Highly relaxed and interested

(2)When both people are in a situation where they are
highly strained and less interested, unfriendly
communications is generated. An animation is
generated in which, for example, their Computer
Grahpics embodiments quarrel with each other (Figures
5-1 and 5-2).
Inter

Lo.

I\

1nt.nrt.d

Figure 6-1. Highly relaxed and less interested

set&

1

Figure 6-2. Highly relaxed and less interested

Less interested

(4) When both people are in a situation where they are
highly strained and highly interested, they are assumed
to have stress and feelings of shyness. An animation is
generated in which, for example, their Computer
Graphics-reactive embodiments behave shyly, in the
way shown below (Figures 7-1 and 7-2).

Figure 5-1. Highly strained and less interested

In?-

Figure 5-2. Highly strained and less interested

Figure 7-1. Highly strained and highly interested

537

Macintosh and some commands to the Computer
Graphics Generator if some Computer Graphics need
to be changed depending on the synchronization
interaction model. The Computer Graphics Generator
creates Computer Graphics based on these commands
and outputs the Computer Graphics. The MAX/MSP
program processes the sound data and the heart rate
sound as required and then outputs the result. The Image
Recognition analyzes the image data fed from the CCD
camera and the relational information of the hands, and
the Computer Graphics displayed is sent to the Event
Control and the Computer Graphics Generator. The Event
Control sends some commands to the Computer Graphics
Generator if some Computer Graphics need to be changed
depending on the data (Figure 9).

Figure 7-2. Highly strained and highly interested

As described above, new codes of non-verbal
communications that cannot be seen in face-to-face
communication are found through Computer Graphics
embodiments.

4. System

4.1 Hardware
The synchronicity based on the heart rates from the
electrodes of an electrocardiograph is calculated by a
PC, and the PC generates an arbitrary feeling in
Computer Graphics form. The hand movements of the
two persons are captured by an installed camera, and
an image analysis of the data is performed. In
accordance with the synchronicity interaction model,
a Compurter Graphics embodiment either follows the
movement of the hand of the partner with high
synchronicity or goes away from the hand of the partner
with low synchronicity. When one touches the
Computer Graphics embodiment of the partner, a
vibrator gives him or her a simulated feeling of touch.
The heart rate sensor measures the timing of the heart,
which is processed by the PC and outputted (Figures
8-1 and 8-2).

Figure 8-1. Setup of [Unconscious Flow]

Ss".O"

4.2 Software Configuration

CCD Camen

Sampler

U

A Heart Rate Analyzer is used to analyze the input
data and to send the data to the Event Control as event
data. The Event Control sends the heart rate data as
MIDI commands to a MAXIMSP program on a

MIXER

Figure 8-2. Hardware configuration

538

MAWMSP CI

Event
Control

I
Figure 10. Real-time animation system

4.3 Computer GraphicsAnimation
This system uses the Real-Time Animation
Synthesizer (RTAS)API for the mermaids’ animations.
RTAS is an API for creating interactive content with
3D Computer Graphcs animation. RTAS is composed
of two parts: Animation Synthesizer and Behavior
Interpreter (Figure 10).
The animation Synthesizer has two functions to
synthesize animations. One function is “Transition,”
and the other one is “Blending.” This system mainly
uses the ‘Transition” function. This function creates a
“Transition” animation to smoothly connect the current
animation with the next animation when an event
occurs. the Animation Synthesizer can accept events
at any time because the “Transition” animation is
generated automatically. The Behavior Interpreter
interprets scripts that describe behaviors representing
high-level actions using animations (Figure 11).

Figure 11. Real-time animation synthesizer

4.4 Hand Recognition
A marker held in each person’s hand is recognized
by using the CCD camera. The CCD camera recognizes
the positions of two markers (Figure 12). The related
program (Image Recognition) processes the distance

In this system, the Event Control sends Some scripts
to the Computer Graphics Generator as events if some
animations need to be changed.

between the
and
hand touches a mermaid or not.

539

whether a

rate of oneself. Other areas related to this work are sounc
healing and healing psychology. Future study wil
integrate Unconscious Flow with these areas.
Meditation and bio feedback with Interactive art wil
become very important in the future for human-tohuman and even human-to-computer interaction. In
the present day, people are exposed to various kinds of
stresses in daily life. In human-to-computer interaction
as well as in human-to-computer communications,
people will want to have relaxed and less stressful
communications.
Figure 12. Image processing by hand recognition

6. Acknowledgments

4.5 Heart Rate Sensor

This work was camed out in collaboration with SonyKihara Research Center, Inc. In particular, much thanks
is given to Mr. Ueda, manager, Mr. Asukai, senior
researcher, Mr. Sakamoto, Mr. Oto, Mr. Nozaki, Mr.
Serita,researchers, and Mr. Komatsu, president. Also,
the authors are grateful for the ambient heartbeat music
provided by Mr. Nagahara and the Real-time Computer
Graphics technology by Mr. Ozaki of Sony Creative
Center.

Each person’s heart rate is measured by placing a
BVP sensor on his or her finger. The heart rate is sent
to a PC connected to the heart rate sensor (ProComp+)
via RS232C and mapped on the synchronicity model
depending on the heart rate (Figure 13).

7. References
[ I ] Tosa, N.and Nakatsu, R. “ Life-like Communication
Agent - Emotion Sensing Character MIC and Feeling
Session Character MUSE-.” in Proceedings of the
International Conference on Multi-media Computing
and Systems, 1996, pp. 12-19.

Figure 13. BVP sensor worn on a finger

5. Conclusion
[2] Maes, P. et al. “The ALIVE system: Full-body
Interaction with Autonomous A gents.” in Proceedings
of the Computer Animation 95 Conference, 1995.

This work was exhibited at SIGGRAPH’99 held in
Los Angeles. Many people visited the exhibition site
and enjoyed interaction with this Unconscious Flow.
On the west coast, the idea of healing and meditation
is quite familiar. That is the reason why this work
accepted by so many people. To date, this work has
been using a biofeedback function based on the heart

[3] Reilly, S. “Building Emotional Characters for
Interactive Drama,” in Proceedings of the Twelfth
National Conference on Artificial Intelligence, Seattle,
WA, 1994.

540

