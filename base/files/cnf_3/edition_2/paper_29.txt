2010 14th International
Information
Conference
Visualisation
Information Visualisation

Visualization of Multi-sensory Meeting Information to Support Awareness
Tomi Heimonen, Saila Ovaska, Markku Turunen, Jaakko Hakulinen,
Juha-Pekka Rajaniemi, Kari-Jouko Räihä
Tampere Unit for Computer-Human Interaction, Department of Computer Sciences,
University of Tampere
{firstname.lastname}@cs.uta.fi
that provides both synchronous interactions during live
meetings and asynchronous browsing of past meetings.
The catalyst for the design of PrimaVista stemmed from
the sabbatical of two members of an academic humancomputer interaction research unit on the other side of
the globe. They wanted to prevent physical distance and
time difference from keeping them aware of the topics
talked about in the meetings. Instead of detailed meeting
notes or audio feeds, they were more interested in
acquiring an informal, overall “feeling” of what is going
on in the unit while they are away. This kind of
awareness should not require extra effort from the local
participants, nor should it require much time or resources
from the remote colleagues.
Throughout the development of PrimaVista, our
primary challenge has been the design of effective
visualization methods for the captured multi-sensory
information. The goal was to provide the participants of
the meetings, and in general all members of the research
unit, a tool that provides increased awareness of meeting
activity while respecting the attendees’ potential privacy
concerns. In the following, we will situate our work
through a discussion of previous research, present the
design process of the visualization interface, and briefly
introduce initial findings from the field study conducted
to evaluate the system in a real meeting environment.

Abstract
Awareness of shared work activities is important for
fostering professional relationships. This paper
introduces visualization techniques for a meeting support
system based on automatic speech recognition and multisensory meeting information capture, including voice
levels and participant comments. The system has been
deployed in live meetings over the past 5 months and we
report meeting participants’ expectations and findings
from a field study. Initial findings suggest that the
proposed approach can facilitate awareness without the
need for detailed meeting transcriptions and associated
audio recordings, however the quality of speech
recognition results and their effective visualization
remain critical factors for increased adoption.
Keywords: Electronic meeting systems, Automated
speech recognition, Awareness visualization, Multisensory input

1. Introduction
Modern telecommunication technologies have
greatly facilitated the work of both co-located and
remotely located organizations. Methods such as video
and voice over IP conferencing and Web-based meeting
systems provide increased fidelity over traditional
teleconferencing. These meetings are often recorded in
digital format for later perusal by members of the
organization that could not attend the live meeting.
However, while participating in such activities, people
have to make an explicit, conscious tradeoff between
personal privacy and group awareness – or how much
they are willing to reveal about themselves to others.
Adequately managing this privacy-awareness tradeoff at
the interface level is a key design issue in modern
ubiquitous collaboration environments, where people’s
activities are constantly tracked through different
environmental sensors and software they use.
This paper describes the development of
visualization techniques for a meeting support system
called PrimaVista. It combines automatic speech
recognition with multi-sensory information captured
from the meeting into a unified visualization interface
1550-6037/10 $26.00 © 2010 IEEE
DOI 10.1109/IV.2010.37

2. Background and related work
The design and development of PrimaVista is
situated in the tradition of research on ubiquitous media
spaces and shared workspaces, electronic meeting
systems and the visualization of awareness data within
these contexts. This study draws on the body of work in
from the early 90’s on the development of media spaces
integrating audio and video communications. The
definition of a media space encompasses computer-based
communication systems that support synchronous
collaboration between distributed groups [1]. One of the
motivations for media space development was to
facilitate interactions between co-workers beyond the
task-specific support offered by CSCW (computersupported collaborative work) systems, including support
for informal interactions and general awareness of people
and their activities across distributed locations [4].
194

3. PrimaVista

Media
spaces
provide
possibilities
for
communication beyond the specific work tasks and help
sustain the working relationships of coworkers. An
important aspect of media space design, as a form of
mediated communication system, is the understanding
that they are not replacing face-to-face communication.
Similarly, when considering the use of PrimaVista as a
support system for meetings, we do not envision that it
can replicate the experience of attending a face-to-face
meeting. Indeed, the key long term research question is
to which extent we can support the sustainment of
working relationships through the awareness information
provided about meetings, and how explicit does that
information need to be in order to be perceived useful.
Electronic meeting systems, on the other hand, are
specifically geared towards capturing the information
content of meetings beyond ad hoc interactions [12].
While there are many different kinds of meeting support
systems [11], as well as different types of meetings,
many meetings can be augmented with automated tools
for capturing the process, materials, and participant
interactions. These tools support Interaction Capture and
Retrieval (ICR) [19] to give easier access to the meeting
logs, either as textual content, visualizations of the
process and its results, or actual recordings of audio and
video from the meeting. Example systems include
European parliament meeting annotation systems [14]
and the AMIDA project [7]. Commonly, these ICR
systems capture the meetings with several cameras and
microphones and apply automatic speech recognition
(ASR) to produce a textual transcript of the meeting (see
e.g., [13]). Some even add semantically meaningful tag
words or annotations to particular content to ease the
browsing of the recordings [3]. However, these systems
usually restrict the number of participants and require
each to have a microphone. Some systems [10, 17] also
process the ASR results further in order to extract the
“gist” of the meeting. Gisting is of particular interest for
PrimaVista, given that these highly compressed
summaries are quick to review and do not require the
users to browse through the full meeting content to get
an overview of the discussion topics.
In addition to information capture, a key aspect of
promoting awareness is how the data are presented to the
users. Awareness visualizations are of particular interest
here since we are aiming for an easy-to-use system that
attempts to minimize impact on the meetings themselves.
They are designed to convey a sense of the dynamics of
the meeting or activity in visual form. Examples of such
systems include a visualization of classroom dynamics
such as gesticulating and body movement [5], historyoriented visualization of activity in an exhibition space
based on recorded audio levels [15], and visualizations of
interpersonal interactions in face-to-face meetings,
including level of participation, speech overlap and
visual attention in the meeting [6, 16]. Especially the
timeline visualizations in these systems inspired the
design of those portions of the visualization interface that
integrate data from multiple sources.

The design of PrimaVista had to fulfill several
design goals that were formulated on the basis of existing
design frameworks and observations of current meeting
practices, including: supporting awareness through
visualization, unobtrusive data collection, preventing
potential privacy issues, and allowing input from
meeting participants. Due to the difficulties with
arranging live meetings across time zones, one key goal
was to provide a meeting record visualization that is easy
to use asynchronously to form an overview of what was
talked about. Another consideration that shaped the
design was the role of data collection. Any overt display
of technology related to the system would undoubtedly
change the meeting dynamics and affect attendee
satisfaction. Thus, techniques like automated speech
recognition and audio level measurement were an ideal
starting point since they require no extra effort from the
participants. As supporting awareness was a key design
goal, and any records compiled by the system would be
available for all members of the research unit, alleviation
of privacy concerns were identified as a related design
challenge. Recording full audio from meetings would not
have been a feasible approach due to data protection
issues. Instead, we aimed at reaching a compromise that
retains the usefulness of the captured awareness
information while providing a comfortable level of
perceived privacy, i.e. awareness of and comfort with
what kind of information is recorded and stored.

3.1. System description
The PrimaVista system is composed of several
software components for inputs and outputs as well as
dedicated computing hardware. Audio from the meeting
room is captured using wired and wireless microphones
hooked into an audio mixer, which in turn feeds the
audio streams to the server. On the server side, the audio
input component processes the incoming streams,
analyzing audio energy levels for both microphones and
segmenting the signal from the speaker microphone. The
3-7 second segments are fed into the automatic speech
recognition component, which utilizes several parallel
recognition processes to turn them into text. While the
ASR engine [8] has language models for both English
and Finnish, the system currently only supports Finnish,
since the recognition accuracy for non-native English
speakers is currently too low for practical purposes.
Additionally, since the ASR component is optimized
for recognition accuracy instead of speed, there is latency
of one to two minutes, which creates challenges for the
visualization of the ASR results. The ASR manager
prioritizes the processes in such a way that the most
recent audio segments are processed first. This means
that if new audio is fed to the system continuously, some
of the segments will be processed only after the meeting.
To increase recognition rates, we used acoustic
adaptation for individual speakers. Adaptation models
were built to those people who speak most often in the

195

meetings. Any user can use the graphical interface to
inform the system of the identity of the current speaker.
The recognition results are stored in the server database
along with the speaker identity, the audio energy levels
and timestamps to indicate the time the audio segments
were recorded and processed.
The PrimaVista Web service aggregates the data
from the database using PHP scripts and generates the
visualization interface for client computers. Interactivity
and animation are provided by the jQuery JavaScript
library (http://jquery.com). The client Web application
refreshes data from the HTTP server utilizing
asynchronous JavaScript calls to provide a responsive
user interface. A non-interactive version of the
visualization interface is displayed in the meeting room
on a 1920 x 1200 pixel resolution high definition
television display (see Figure 4 for the room layout).
In addition to the Web-based clients, the system
supports the generation of alternative visualizations of
the meeting data. We have experimented with utilizing
small (10”) WiFi enabled picture frames to provide
concise tag cloud visualizations of the ASR results. The
visualizations are generated on the server computer and
transferred to the picture frame’s internal memory.
Finally, the system also supports additional data
inputs. We are currently using xuuk eyebox2TM devices
(http://www.xuuk.com) to track the number of
participants in the meeting room based on their gazes.
The eyebox2TM is a video-based, infrared eye-tracking
device that detects how many people are looking at it
from up to 10 meters away. Thus, when placed in the
vicinity of the presenter, it offers a non-intrusive method
for approximating the number of meeting attendees.

visualized in a histogram fashion. Each vertical bar
represents a one-minute timeslot and the height of each
bar indicates the relative quantity of keywords spoken by
meeting attendees during the timeslot.

Figure 1. PrimaVista interface Version 1.
In addition to the ASR results, this version of the
system added new features in the form of a text-based
feedback channel (to the extreme right), whereby users
could add comments after logging into the system.
Additionally, to facilitate gisting and perusal of past
meetings, we added a replay feature that allows the user
to play back past meetings in faster than real time. After
implementing these features, we discovered that for the
users the distinction between browsing old meetings and
viewing the live meeting data was somewhat blurred,
with all this functionality being contained in a single
view. Additionally, as we began to add data from other
inputs besides ASR, the timeline view was becoming too
complex for easy scanning. The changes we made to
solve these challenges led to the current design iteration.

3.2. Design process of the visualization interface
The PrimaVista visualization interface has gone
through two major iterations from the initial designs
constructed in the fall of 2008. The initial design was
based on a set of brainstormed scenarios and use cases.
At that time it the plan was to deploy the system in
several meeting rooms, which prompted the use of floor
map layout with a Google Maps inspired pan-and-zoom
interface for viewing the ASR data in the form of tagcloud visualizations within each room.
The initial design evolved into Version 1 of the
visualization interface (Figure 1). The interface is a
simplified rendition of the layout of the meeting room
where the system was installed. It was decided during the
planning phase of the project to deploy PrimaVista in a
single location, hence there was no need to include the
interactive floor layout functionality. Tag clouds for the
recognized words appear in the physical locations of the
microphones embedded into the environment.
Figure 1 shows the tag cloud for the speaker, located
towards the front of the room. Interactive controls are
located at the top of the view, with a date-time control to
select the active date and a timeline control for viewing
the activity visualization. The timeline displays a threehour timeslot, within which the meeting activity is

3.3. Current visualization interface
In the current iteration (Version 2) of the PrimaVista
design, the visualization interface is set up as a multiple
views dashboard like display, divided into two discrete
modes: the Live View and the Past View. The Live View
display (Figure 2, left) is continuously updated to reflect
the current state of the system. The basic layout is
divided into three major sections: the Words pane (1),
containing the ASR results, the Comments pane (2),
containing participant comments and an input box (4),
and the Users pane (3), that lists currently logged on
users. Additionally, there is a drop-down menu (5) for
selecting the active ASR speaker model and an indicator
for system status (6). The speaker model selection can be
used during meetings to instruct the system to use a
personalized ASR speaker model for a given presenter.
The ASR results are visualized as a temporally
sorted linear list of time-stamped keywords. Instead of
the precise time we chose to display an approximation
rounded to the level of minutes (e.g., “9 minutes ago”).

196

Figure 2. PrimaVista interface Version 2: Live View (left) and Past View (right).
The rationale was that it would be confusing to see the
exact timestamp, given that the keywords are shown
immediately when the ASR process completes,
regardless of their order in the original continuous
speech. Furthermore, we made a decision not to display
more than the last 10 minutes of the recognition results.
If the ASR processing takes longer, the text will be
available only in the Past View, where it can be
positioned in relation to the actual time when it was said.
The Past View allows for browsing meeting data in
more detail (Figure 2, right). The Words and Comments
panes retain the same functionality as in the Live View,
however an additional tag cloud visualization option is
included for the Words pane (Figure 3), similar in design
to the one in Version 1 of the visualization interface.

situated at the top of the Past View. It is composed of
four separate timelines that display (top to bottom) ASR
results, user generated comments, audio levels and eye
gaze data (i.e., number of eyes following the presenter)
at a given time. Each vertical line maps to a duration of 1
minute and the height of the bar represents the quantity
of data in units per pixel. In the example in Figure 3, the
audio level was high throughout the meeting, speech was
recognized continuously after initial hassle when the
speaker had not really started, very few comments were
made, and eyebox2TM was not in use. An interactive
slider positioned over the timelines is linked to the
content of Words and Comments panes, allowing the
user to browse through the data using direct
manipulation. The size of this time window can be
dynamically changed from 10 minutes to three hours.
The design draws inspiration from earlier work by
Begole et al. [2] on rhythmic pattern visualization for
work group activities. Their evaluation showed this kind
of ‘percent-active’ graph to be favored by users, due to
its ability to clearly show transitions and ease of
comparing activity levels throughout the day.
While the tag cloud visualization creates awareness
of the key topics of discussion, the timeline is intended to
provide a concise overview of the rhythm of the meeting
in terms of presenter activity and attendee interaction –
through activity bars for audience voice levels, textual
commenting and visually attending to the speaker. The
interactive slider allows the user to skim through the
meeting much more effectively than the playback feature
of Version 1. The focus can be adjusted to cover a
shorter period to go through various stages of the
meeting, or have it encompass the whole meeting to gain
an overview of the most important themes.

Figure 3. Tag cloud visualization in Past View.
The tag cloud visualization aggregates the keywords
over the specified time interval, whereby the font size of
the keywords varies as a function of their frequency.
Although there are several challenges with respect to
accuracy of the data-to-visualization mapping strategies
when it comes to tag clouds (e.g., frequent words of
equal interest occupy differing amount of screen space
depending on their length), tag clouds are nevertheless
intuitively familiar to many users. We avoided
overloading the tag cloud with graphical properties such
as orientation and color, since end users can easily
misunderstand their meaning [18].
The key difference to the previous iteration is the
inclusion of an interactive compound timeline display

4. Field study
In the following we report initial findings of the ongoing field study, including the early expectations our
colleagues had towards the system, and observations of
in situ use of the system in meetings. Much of the
evaluation planned for the system is still to come, since
the technical quality – especially the automated speech
197

In a separate questionnaire, we asked opinions about
the most important aspects in PrimaVista. Most
respondents (16 out of 18 responses, 89%) agreed that
PrimaVista should not disturb the meeting. Nearly as
many (15/18, 83%) emphasized that PrimaVista needs to
give enough information about the meeting, and the
information should reliably depict what took place
(14/18, 78%). Some other aspects of PrimaVista received
mixed reviews. For instance, 9 persons consider identity
protection during speech recognition (i.e. the system
does not explicitly identify the speaker publicly)
important, while 7 persons claimed it is not important.

recognition component – is under constant development.
The findings thus far do not relate to the final system but
to the current and early versions of it. Different versions
of PrimaVista have been used in several group and unit
level meetings to date. The Web interface has been used
by 17 users outside of the development team.
The PrimaVista system is developed for internal use
within an academic research unit of approximately 50
researchers, with 35 Finnish-speaking staff members.
Two thirds of them are male, and their age range is
between 23 and 59 years. In general, they do not use
speech recognition technologies; only every sixth person
uses speech recognition regularly, but all have at some
time tried an application that is based on speech
recognition input.
The system is permanently installed in the unit’s
meeting room (Figure 4). The high definition TV is
situated in front of the audience, showing the Live View
of the system to meeting attendees. The presenter wears
a lapel microphone, and audience microphone is located
in the middle of the room on a separate stool. Some
participants access the web interface on their own
laptops in the meeting room or remotely.

5. Discussion
The most important user concerns during the field
study related to the degree to which PrimaVista disturbs
the meeting process, and what information is presented
of the meeting, and in which manner. Observations of
system use during meetings suggest that after an initial
adjustment period, attendees do not overly pay attention
to the microphones or the displays. While this indicates
we have managed to integrate the system into the
meeting room successfully, the issue of providing
enough accurate information using the appropriate
representations still remain.
PrimaVista shows meeting activity by visual means;
similar functionality but with audio gisting is offered by
Catchup [17]. For both systems, reaching the correct
level of abstraction is important. Unimportant parts of
the speech need to be extracted so that the important
things stand out better. We have done rudimentary
filtering by removing short words and stop words,
however we envision that a much more aggressive
filtering is needed (e.g., the term frequency based
approach used in Catchup) to remove extraneous data.
Filtering the ASR results also has direct implications for
the visualization design. In its current form, the tag cloud
visualization becomes saturated even when quite short
time spans (i.e., less than 5 minutes) are used, simply due
to the amount of words present in discussions.
Another challenge for effective visualization lies in
the way the ASR back-end processes the audio streams.
Based on our observations of the system’s Live View use
in meetings, attendees tend to assume the ASR results
are composed of complete sentences, whereas this is not
the case in reality. This can lead to miscomprehensions,
especially in the case of the linear keyword list, given its
visual resemblance to typical instant messaging chat
window interfaces. One potential solution is to alter the
visual representation of the keywords to resemble tag
like elements, instead of linear structured text.
Ultimately, the quality of the ASR summaries is
likely to be the key issue for this kind of an awareness
system, in terms of overall usefulness. A recent study by
Hsueh and Moore [9] showed that it is possible to
provide a quick overview of meeting decisions more
effectively by using a decision-specific summarization
technique that provides compression ratio of 98-99%,
than relying on general summaries. Given our goal of

Figure 4. PrimaVista setup in the meeting room.
The PrimaVista concept was presented to the
members of the research unit prior deployment. At that
point no demonstration of the system was available. In
addition to talking about the plans, we collected a short
paper survey that was filled in by 35 of our colleagues.
Two respondents (out of 35) explicitly brought up
privacy-related concerns by emphasizing the disclosure
of confidential information about ongoing and planned
projects. However, another respondent commented that
since the meeting room is most often used for public
presentations, privacy appears not to be a huge issue, in
the sense that people outside of the live meeting are able
to access the records. Some other privacy related
concerns were raised; for example, how to indicate to the
attendees that the system is currently recording. Also the
question of speech recognition quality was raised, though
only by a handful of colleagues. Ease of use was a more
commonly emphasized concern.
Some of our colleagues expressed concerns about
the acceptability of PrimaVista as a meeting capture tool
since introducing the system affects meeting dynamics in
several ways. It offers another channel for selfexpression in the form of live commenting. Seeing the
recognition results appear on the display could make
some shy people even more silent.

198

[5]

providing a system that requires very little effort to
operate and gain insights into the topics of meetings, this
kind of highly extractive approach could be beneficial.
Most ICR systems are designed to fill pragmatic
goals and to provide factual information for their users
[19]. Ideally, the ASR results should be as error-free as
possible to provide a reliable basis for the keyword
visualization, however the recognition rate will never in
practice reach 100%, even with personalized speaker
models. According to initial survey feedback, to our
colleagues the ability to view useful meeting summaries
seems to be the main attraction. Whether there is enough
information in the tag clouds and activity visualizations
for our colleagues to actually gain a reliable overview of
the meeting remains a topic of further study. Based on
our observations during the field trial, the current
PrimaVista implementation does not sufficiently fulfill
the pragmatic needs in all situations.

[6]

[7]

[8]

[9]

[10]

Conclusions
This paper discussed the design process of
awareness visualizations in the context of PrimaVista, a
Web based meeting support system based on automatic
speech recognition and multi-sensory meeting
information capture. Our goal was to promote awareness
to meeting content; both during live meetings for remote
participants, as well as provide records of past meetings
that can be browsed after the fact. The system has been
deployed in meetings since November 2009 and a field
study to refine its design and functionality is currently
ongoing. Initial results from the field study underline the
importance of robust enough speech recognition and
effective visualization of the ASR summaries to the
overall usefulness of the system.

[11]

[12]
[13]

Acknowledgements

[14]

We thank our colleagues at the University of
Tampere for participating in the field study. This
research was supported by funding from NORDUnet3
through the Privacy in the Making project.

[15]

References

[16]

[1]

[2]

[3]

[4]

Baecker, R., Harrison, S., Buxton, B., Poltrock, S., and
Churchill, E. Media spaces: past visions, current realities,
future promise. In CHI '08 Extended Abstracts. ACM,
New York, NY, 2245-2248, 2008.
Begole, J., Tang, J. C., and Hill, R. Rhythm modeling,
visualizations and applications. In Proc. ACM
Symposium on User interface Software and Technology.
ACM, New York, NY, 11-20, 2003.
Behera, A., Lalanne, D., and Ingold, R. DocMIR: An
automatic document-based indexing system for meeting
retrieval. Multimedia Tools Appl. 37, 2 (Apr. 2008), 135167.
Bly, S. A., Harrison, S. R., and Irwin, S. Media spaces:
bringing people together in a video, audio, and
computing environment. Commun. ACM 36, 1 (Jan.
1993), 28-46.

[17]

[18]

[19]

199

Chen, M. Visualizing the pulse of a classroom. In Proc.
ACM International Conference on Multimedia. ACM,
New York, NY, 555-561, 2003.
DiMicco, J. M., Hollenbach, K. J., and Bender, W. Using
visualizations to review a group's interaction dynamics.
In CHI '06 Extended Abstracts. ACM, New York, NY,
706-711, 2006.
Garner, P. N., Dines, J., Hain, T., El Hannani, A.,
Karafiát, M., Korchagin, D., Lincoln, M., Wan, V., and
Zhang, L. Real-Time ASR from Meetings. In Proc.
Interspeech 2009.
Hirsimäki, T., Creutz, M., Siivola, V., Kurimo, M.,
Virpioja, S., and Pylkkönen, J. Unlimited vocabulary
speech recognition with morph language models applied
to Finnish. Computer Speech & Language 20:4, 515–
541.
Hsueh, P.-Y. and Moore, J. Improving meeting
summarization by focusing on user needs: A taskoriented evaluation. In Proc. Int. Conf. on Intelligent
User Interfaces. ACM, New York, NY, 17-26, 2009.
Kristjansson, T., Huang, T. S., Ramesh, P., and Juang, B.
H. A unified structure-based framework for indexing and
gisting of meetings. In Proc. 1999 IEEE international
Conference on Multimedia Computing and Systems Volume 02. IEEE Computer Society, Washington, DC,
572.
Nijholt, A., Rienks, R., Zwiers, J., and Reidsma, D.
Online and off-line visualization of meeting information
and meeting support. The Visual Computer 22: 12, 965–
976, December 2006.
Nunamaker, J.F., Dennis, A.R., Valacich, J.S., Vogel, D.,
and George, J.F. Electronic meeting systems. Commun.
ACM 34, 7 (1991), 40-61.
Otsuka, K., Araki, S., Ishizuka, K., Fujimoto, M.,
Heinrich, M., and Yamato, J. A realtime multimodal
system for analyzing group meetings by combining face
pose tracking and speaker diarization. In Proc.
International Conference on Multimodal interfaces.
ACM, New York, NY, 257-264, 2008.
Ramabhadran, B. Siohan, O. Mangu, L. Zweig, G.
Westphal, M. Schulz, H., and Soneiro, A. The IBM 2006
speech transcription system for European Parliamentary
speeches. In Proc. ICSLP, 1225–1228, 2006.
Skog, T. Activity wallpaper: ambient visualization of
activity information. In Proc. Conference on Designing
Interactive Systems. ACM, New York, NY, 325-328,
2004.
Sturm, J., Herwijnen, O. H., Eyck, A., and Terken, J.
Influencing social dynamics in meetings through a
peripheral display. In Proc. International Conference on
Multimodal interfaces. ACM, 263-270, 2007.
Tucker, S., Bergman, O., Ramamoorthy, A. and
Whittaker, S. Catchup: A useful application of timetravel in meetings. In Proc. ACM Conference on
Computer Supported Cooperative Work. ACM, New
York, NY, 99- 102, 2010.
Viegas, F. B., Wattenberg, M., and Feinberg, J.
Participatory visualization with Wordle. IEEE
Transactions on Visualization and Computer Graphics
15, 6 (Nov. 2009), 1137-1144.
Whittaker, S., Tucker, S., Swampillai, K., and Laban, R.
Design and evaluation of systems to support interaction
capture and retrieval. Personal Ubiquitous Comput. 12, 3
(Jan. 2008), 197-221.

