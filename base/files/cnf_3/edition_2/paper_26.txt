2010 14th International
Information
Conference
Visualisation
Information Visualisation

Supporting the analytical reasoning process in maritime anomaly detection:
evaluation and experimental design
Maria Riveiro, G¨oran Falkman
Informatics Research Centre, University of Sk¨ovde, Sweden
{maria.riveiro, goran.falkman}@his.se
resulting in high false alarm rates
2. The lack of adequate support for the analytical reasoning process
Anomaly detection is a complex and not well-defined
problem, and therefore, autonomous anomaly detection
systems typically produce high number of false alarms.
Hence, user involvement in the detection process is crucial.
Human expert knowledge can be used to guide the anomaly
detection process, for example, reducing the search space,
updating expert rules or refining normal models derived
from data. The first challenge involves, thus, the study of
adequate ways of interacting and visualizing the underlying computational anomaly detection process, facilitating
user involvement. The second problem concerns the necessity of providing adequate user support during the whole
detection and identification of anomalous behavior process, allowing a true discourse with the information. This
issue includes deepen our understanding of the human analytical and decision making processes, in order to find how
computational methods can support them.
Data mining techniques have been proved to be very
valuable in the detection of anomalies. Data-driven
anomaly detection approaches calculate deviations from
models built from data that represent normal behavior. In
order to involve the user in the detection process and support operator’s analytical reasoning process, we suggest the
use of interactive visual representations of normal models
built from data. Visualization and interaction techniques
are widely recognized to be very powerful in data mining
[4] and may close the gap between the domain experts and
the data mining engines. In addition, visualization methods take advantage of human abilities to perceive patterns
and to interpret them [5].
The lack of appropriate evaluation methodologies that
reflect the relation between interaction, visualization and
human analytical reasoning processes sets high demands
on the design of the empirical tests. We argue that the connection between analytical reasoning processes and visual
representation/interaction techniques must be clear traced,
from a user point of view. However, little effort has been
devoted to drawing such connections [9]. Based on this

Abstract
Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies include investigations on the analytical reasoning process that needs to be supported. In this paper, we
present an approach to evaluate the ability of certain visual
representations from an integrated visual-computational
environment to support the completion of representative
tasks. The problem area studied is the detection and identification of anomalous vessels and situations while monitoring maritime traffic data.
This paper presents: (1) a brief review of current evaluation methodologies within information visualization and
visual analytics, (2) an analysis of operator’s analytical
reasoning process (derived from field work in maritime
control centers and a literature review on analytical reasoning theories), (3) a list of representative tasks for usability evaluation and (4) an approach to evaluate the use
of normal behavioral models representations during the
detection process.
Keywords— evaluation, visual analytics, visualization,
anomaly detection, maritime domain awareness

1

Introduction

The surveillance of large sea areas with multiple and
diverse sensors (radars, cameras, self-reporting systems,
etc.) normally involves the analysis of vast amounts of
heterogeneous data. Exploring the data and monitoring
surveillance systems may become a demanding activity for
operators, not only due to the complexity of such data but
also to other factors like uncertainty or time constraints. In
order to support the operator while monitoring large sea
areas, timely detection of anomalous vessels or situations
that need further investigation may reduce operators’ cognitive load.
The detection of anomalous behavior using data mining methods has received a lot of attention during the last
years. Nevertheless, surveillance systems used in real environments rarely include anomaly detection capabilities.
We argue that there are two main reasons:
1. The lack of user involvement in the detection process,
1550-6037/10 $26.00 © 2010 IEEE
DOI 10.1109/IV.2010.34

170

assessment, we present the design of usability tests that
reflect the analytical reasoning process that needs to be
supported (in this study, the detection and identification
of anomalous behavior). Furthermore, Stasko claims in
a recently published overview of challenges in information visualization evaluation [26] that successful evaluations should include representative tasks for participants
to perform. Along those lines, we suggest a set of characteristic tasks performed by operators in maritime control
centers. The ultimate goal is to investigate if visual representations of normal behavioral models built from data
(1) assist operators to find anomalous behavior while monitoring traffic data and (2) facilitate user involvement in the
underlying data mining process.
The main contributions of this paper are: (1) an analysis of operator’s analytical reasoning process (derived from
field work in maritime control centers and a literature review on analytical reasoning theories), (2) a list of representative tasks performed while analyzing maritime traffic
and (3) an approach to evaluate the use of normal behavioral models representations during the detection process
that includes operator’s analytical reasoning process. In
general, this study can be used in future information visualization and visual analytics evaluations and in particular,
within the maritime domain scenario.
The remainder of the paper is organized as follows: section 2 contains necessary background information to properly frame this study. Section 3 summarizes our filed work
in maritime control centers and presents an analysis of operator’s reasoning process. Section 4 discusses tasks that
need to be supported in connection to the analytical reasoning process and describes the evaluation approach taken. In
addition, section 4 provides a brief summary of the results
obtained from the empirical evaluation (a detailed description of the results’ analysis process can be found in [20]).
Conclusions and future work are presented in section 5.

2

detected and identified. A classification and examples of
maritime anomalies from operator’s and practitioners point
of view is provided in [22].
Algorithms used for detecting anomalies can be classified in three main groups: data driven -based, signature/rule -based or hybrid-based (combination of the previous two). Signature-based approaches look for predefined patterns, specified using rules. Systems based on
data driven approaches match the data to normal behavioral models (deviations from these models are considered
anomalies). Such normal models are built from data that
represents standard behavior.
In the maritime scenario, few anomaly detection approaches can be found in the literature. Exceptions are
the proposals presented in [12, 19]. Both methods can be
classified as statistical data driven approaches (solutions
based on statistical approaches are frequently applied to the
anomaly detection problem). These methods built a statistical model of recorded vessel traffic using a probability
distribution.
In this project, we have implemented a statistical data
driven approach, based on Kraiman et al.’s proposal [12].
The detection approach is a combination of Self Organizing Maps (SOM) and Gaussian Mixture Models (GMM).
Here, the normal behavioral model is a multivariate probability density function of the feature space. The features considered are: longitude, latitude, speed, course and
heading of vessels traveling along Swedish coast. Data is
based on readings from an Automatic Identification System (AIS) (see section 4.2 for more details).

2.2

Evaluation methodologies and metrics

The proliferation of information visualization techniques has also highlighted the need for principles and
methodologies for empirical evaluation [3]. Research in
InfoVis has largely focused on the development of innovative computing techniques, but their evaluation has often
been relegated to a secondary role. During the past ten
years, the community has been stressing the need of improved methods in areas such as task analysis and usability
evaluation. Even if the progress has been slow, recent conference proceedings start to tackle the evaluation challenge
within information visualization and visual analytics. An
example in this direction was the workshop held during
CHI 2008, namely BELIV’08: BEyond time and errors,
novel evaLuation methods for Information Visualization.
Two significant reviews on the evaluation of information visualization are [2, 18]. From a theoretical point of
view and inspired by empirical research within Human
Computer Interaction (HCI), Carpendale [2] presents an
extensive analysis of different types of evaluations, from
quantitative to qualitative approaches, discussing their advantages and disadvantages. Furthermore, the paper elaborates on how to choose suitable evaluation methodologies

Background

This section introduces first, the anomaly detection
problem in the maritime domain (section 2.1); secondly,
it presents a brief review on evaluation methodologies and
metrics within information visualization and visual analytics (section 2.2) and finally, it provides an overview of analytical reasoning theories and frameworks (section 2.3).

2.1 Anomaly detection
“Anomaly” is a many-sided concept. It is normally associated with terms like abnormal, unusual, irregular, rare,
deviation, strange, illegal, threat, atypical, inconsistent,
etc. Many data mining techniques analyze data in order to
find behavioral anomalies. Behavioral anomalies are defined as deviations from the normal behavior. In this paper,
an anomaly is defined from a user (operator or organization) point of view, as events or situations that need to be

171

and current challenges facing empirical research in information visualization. Among these challenges, the author
emphasizes practical aspects common to all empirical research, like the difficulty of obtaining an appropriate sample of participants, selecting the right methodology or analyzing the results. From a more practical point of view,
Plaisant [18] presents a brief summary of current evaluation practices in information visualization, identifying
challenges and proposing possible first steps to improve the
evaluation phase. The paper uses a classification of user
studies in four thematic areas made by Komlodi (University of Maryland) after surveying fifty user studies of information visualization: (1) controlled experiments comparing design elements, (2) usability evaluation of a tool, (3)
controlled experiments comparing two or more tools and
(4) case studies of tools in realistic settings. The main challenges highlighted in [18] are divided in three main blocks:
utility needs to be demonstrated in real settings, specific aspects of empirical studies must be improved (e.g. the use
of very simplistic tasks) and the need to address universal
usability (making visualization tools accessible to diverse
users regardless of their backgrounds or technical disadvantages). Possible first steps towards the improvement
of information visualization evaluation are, according to
Plaisant: the creation of repositories of data and tasks, the
use of case studies on users in their natural environment
doing real tasks and the implementation of the developed
techniques in toolkits. Finally, the paper presents lessons
learned from three examples of transformations from prototypes to real-world products (e.g. Spotfire).
The difficulty of finding appropriate evaluation metrics
has been addressed by the community lately in several
meetings. For example, a recent workshop, Metrics for the
Evaluation of Visual Analytics held at IEEE VAST 2007,
focused on discussing which measures are most helpful
to the analytic and research community (articles presented
during the workshop are for example [13, 30]). The selection of metrics has been a recurrent topic in Scholtz’s studies (see [6, 23–25]). Scholtz encourages the use of usercenter evaluation in general [23], and in particular within
the visual analytics community [6, 24, 25]. From her point
of view, evaluation needs to go beyond technical performance and usability metrics, measuring the utility and impact of new software to facilitate information interaction.
In [24, 25], Scholtz presents an extensive list of relevant
metrics regarding the following areas: situation awareness,
collaboration, interaction, creativity, and utility. The metrics described are specifically to support information analysts using visual analytic environments. Relevant for our
work is the description of existing metrics for (1) situation
awareness (e.g. SAGAT1 method, verbalization methods
1 Situation

Sensemaking loop

Reality/Policy Loop

15. Reevaluate
12. Search
for Support

Structure

9. Search for
Foraging loop Evidence
6. Search for
Relations
3. Search for
Information

1. External
Data Sources

4. Shoebox

13. Hypotheses

10. Schema

7. Evidence
File

16. Presentation

14. Tell Story

11. Build Case

8. Schematize

5. Read &
Extract

2. Search &
Filter

Effort

Figure 1: “Think Loop Model” for the analytical process (adapted
from [1, 17]). The overall process is organized into two major
loops: foraging and sense-making loop.

like think and talk aloud, or subjective measures that ask
users to assign numerical values to their situation awareness), (2) interaction (suggestions regarding the capabilities and tasks that interaction should provide are given:
capabilities such as the ability to view occluded information or the ability to change level of abstraction, and tasks
like locate, identify, distinguish, categorize, cluster, distribute, etc.) and (3) utility (reduction of time or better
performance, reduction of cognitive load or increase analyst’s confidence in system recommendations).

2.3

Analytical reasoning theories

Analysis is cyclic and iterative. Reaching judgment
about a single question is normally an iterative process
that will produce several more questions about a larger issue [27]. Many forms of intelligence analysis are “sensemaking” tasks [17]. Such tasks consists of some kind of
information gathering, representation of the information in
a schema that aids the analysis, the development of insight
through the manipulation of the schema and the creation
of some knowledge product or direct action based on the
aid (information → schema → insight → product). Sensemaking provides a theoretical framework for understanding the analytical reasoning process that an analyst performs. From a psychological perspective, sense-making
has been defined as “how people make sense out of their
experience in the world” [8].
In [1, 17], the authors describe intelligence analysis as
an example of sense-making. The overall process is organized into two major loops [1, 17]: (1) a foraging loop that
involves processes aimed at seeking information, searching
and filtering it, and reading and extracting information possibly into some schema and (2) a sense making loop that
involves iterative development of a mental model (a conceptualization) from the schema that best fits the evidence.
Figure 1 represents a notional model of the analyst’s pro-

Awareness Global Assessment Technique

172

cess derived from cognitive task analysis (the “think loop
model” [1]). The boxes represent an approximate data flow
and the arrows represent the process flow. The processes
and data are arranged by degree of effort and degree of information structure. The data flow shows the transformation from raw information to reportable results. External
data sources are the raw evidence, the shoebox represents
a subset of the external data that is relevant for processing,
the evidence file refers to snippets extracted from the shoebox, schema is the representation of the information so that
it can be used more easily to draw conclusions, hypothesis
are the tentative representation of these conclusions with
supporting arguments and finally, there is a presentation or
work product (that can be used for communication).
The foraging loop is essentially a trade off between
three kinds of processes [17]: (1) exploring or monitoring
more of the space (increasing the span of new information
items into the analysis process), (2) enriching (or narrowing) the set of items that has been collected for analysis
and (3) exploiting the items in the set (thorough reading of
documents, extraction of information, generation of inferences, noticing of patterns, etc.). A detailed description of
the information foraging theory, models, empirical investigations and applications of the theory to the design of user
interfaces can be found in [16].
The analyst integrates [1]: (1) a bottom-up approach
that builds a theory based on a hypothesis by assembling
evidence assumed relevant to a question, and (2) a top
down approach that searches for evidence for an assumed
hypothesis. It is important to notice that sense-making does
not always follow the progression: data → information →
knowledge → understanding. For instance, sense-making
can have many loops or does not always have a clear beginning or end points (this has been highlighted by Klein,
Moon and Hoffman in [10]).

3

Figure 2: Monitoring station at VTS West Coast control room,
Gothenburg, Sweden.

Stockholm’s harbor, Sweden2 (hereafter “harbor center”),
(2) Vessel Traffic Service (VTS) Center, Swedish Maritime Administration, Gothenburg, Sweden (hereafter VTS
Gothenburg)3 and (3) VTS Center, Maritime Search and
Rescue Center in Finisterre, Spain (hereafter VTS Finisterre)4 . The in-depth interviews were done with (1) Peter
Baltzer (Consultant, Security & Quality, Stockholm), (2)
Lars Sundberg (Sea Captain and Harbour Master, Stockholm) and Anders Br¨odje (VTS West Coast Manager,
Swedish Maritime Administration, Gothenburg).
Besides slightly differences among the three visited
centers, the actual process of finding anomalous behavior
can be summarized as follows: (1) overview: continuous
control of the traffic in real-time using AIS, radar, marine
currents and weather information; (2) filter: if something is
unusual or unfamiliar (operators normally base their judgment on their experience), detailed information must be
obtained, e.g. zooming into the area; (3) waiting time: operators usually wait a reasonable period of time, observing
how the situation evolves; (4) more detail: if the situation
has not become normal, they start a dialog with the vessel or try to get more data using, for example, additional
information stored in databases; (5) taking action: if they
believe that an incident is imminent or has occurred, they
take action reporting the situation or alerting other organizations.
These steps resemble the foraging and the sense-making
loop of the think loop model described in section 2.3. The
trade off between exploring, enriching/narrowing and exploiting is clearly exemplified in the process of finding
maritime anomalous behavior. The sense-making loop involves the generation of hypothesis (regarding the abnormality of certain behavior) and the presentation phase relates to the construction of reportable results.
The first step in the process describes the basic task that

Supporting analytical reasoning

In order to deepen our understanding of the human analytical reasoning process in this particular domain and set
a foundation for the design of usability tests, this section
briefly describes how experts detect and identify anomalous behavior while analyzing maritime traffic data. Early
visits to maritime control centers are presented in [21]. We
have completed the field work previously described with
more visits to two of the centers described below. In addition, we present a general description of operator’s analytical reasoning process (crucial for the evaluation approach).

3.1 Field work in maritime control centers
In order to characterize how experts detect and identify
anomalous behavior, we have conducted empirical work
through two primary qualitative methods: participant observation and in-depth interviewing. The participatory
observations were made in three different locations: (1)

2 Stockholms

Hamn AB, Magasin 2, Frihamnen, 102 54 Stockholm
West Coast, Sj¨ofartsverket, Sydatlanten 15, 418 34 G¨oteborg
4 Centro de Coordinaci´
on de Salvamento Finisterre, Monte Enxa 22,
Porto do Son, 15971 A Coru˜na, Galicia, Spain
3 VTS

173

operators constantly carry out: the establishment and update of the normal picture or situation of the supervised
zone. Step 1 clearly coincides with one of the main processes of the foraging loop described in [17], exploring
or monitoring the space. Operators build a normal operational picture, based not only on current data available,
but also on their experience and knowledge. There is, thus,
an implicit phase that has not been captured by the process
described above, that we refer as “understanding the normal vessel behavior”. The second step, filter, corresponds
to the narrowing the set of items described in [17]. Step 2
may be improved using automatic support. Here, data mining techniques may be of great assistance, since they can
filter out the most probable anomalous vessels and highlight them to the operator. Step 4 corresponds to the exploiting the items in the set described in [15]. Steps 2 and
3 correspond to the iterative generation and confirmation
of hypothesis, as reflected in the think loop model. This
phase will be generally denoted as detection/identification
of anomalous behavior. During phase 5, reports explaining the incidents are created. Step 5 coincides with the final
presentation phase described by Pirolli and Card [17].
In conclusion, we have identified four critical phases in
the anomaly detection reasoning process:
1. Understanding normal vessel behavior and continuous establishment of normal situation picture
2. Matching incoming data to normal (mental) models
(generation of hypothesis). Detection/identification
of anomalous behavior
3. Confirmation
4. Explanation (presentation)

4

enhances

Analytical reasoning process
(human)

visualization
interaction

Anomaly detection process
(machine)

supports

Figure 3: Analytical reasoning and anomaly detection processes
integration. User is involved in the detection process, computational methods support user’s analytical process.

models will be possible, reducing thus, false alarm rates.
In the following sections we present simplified representative tasks carried out in maritime control centers that
are used in the evaluation process (we take a similar approach to those presented in [7, 11, 28]). Additionally,
the evaluation procedure, metrics, the environment and the
data are described.

4.1

Defining tasks for evaluation

Evaluating whether visual analytical principles or a certain visual and interactive environment are effective, involves answering the question, as Koua et al. [11] formulate: effective for what? Hence, we begin discussing goals
and tasks in maritime surveillance centers.
Maritime surveillance centers provide services that improve vessel traffic safety and efficiency and protect the environment. Human operators need to timely identify traffic situations emerging in an area and respond appropriately. Examples of such situations are vessel collisions and
groundings in the port and entrance areas. Since one of
the main goals of maritime operators is to timely identify
such situations, the monitoring and analysis process can
be viewed as a set of goals that include: (1) understanding normal vessel behavior and continuous establishment
of normal situation picture (2) matching incoming data to
normal models (detection/identification anomalous behavior) (3) confirmation and (4) explanation or reporting of the
detected situations.
To be effective in the task of recognizing vessels that
need extra attention or detecting conflict situations, operators need to be armed with vast amounts of background
information. This background information is utilized to
build a baseline that can be used for comparison when a
conflict is suspected. Understanding normal vessel behavior and update continuous normal situation picture have
been described above as essential tasks for monitoring maritime traffic effectively. The first part (understanding normal vessel behavior) involves recognizing how the traffic is
distributed for a given time and area. During update continuous normal situation picture, operators must be constantly aware of the actual label/name, position, course,
speed, communication agreements, destination, etc. of vessels within the actual traffic situation picture.

Evaluation approach

The goal of visual analytical systems is to support the
analytical reasoning process, maximizing human perceptual, understanding and reasoning capabilities in complex
and dynamic situations. Our ultimate goal is to find the
optimal combination of human analytical reasoning and
computational methods for the maritime anomaly detection problem. We need two support both human and computational processes (see figure 3), and we suggest the use
of interactive visualizations. On one hand, computational
anomaly detection methods should support the human reasoning process. On the other hand, we need user involvement to improve the performance of the detector. In order
to achieve both goals, we suggest the use of visual representations of normal behavioral models built from data.
The evaluation approach taken assesses both the ability
of such representations to support the first two phases of
the analytical reasoning process previously described, and
the comprehensibility of the visual representations provided. If users comprehend the normal behavioral models built from data, updating, validating and enhancing the

174

Table 1: List of visual operations and operational tasks
V ISUAL OPERATION
Categorize

Distribution

Distinguish

Compare

Identify

Locate

Rank
Cluster
Correlate
Associate

O PERATIONAL VISUALIZATION TASK
Categorize vessels that report incorrect or do not report necessary information (e.g. AIS
turned off, incorrect IMO reported, change of name/type)
Describe how navigational lanes (for cargo, passenger, fishing, etc.) are distributed over the
map and time (different patterns during the day, day/night, month, etc.)
Distribution of speed, dimensions, course for different types and areas
Recognize outliers (high/low values speed, danger of grounding, non-typical areas)
Distinguish vessels outside normal routes
Recognize non-cooperative behavior
Compare behavior among vessels (e.g. cargo with fishing, passenger, tanker)
Compare behavior particular vessel to behavior vessels same type
Identify vessels that need extra attention
Identify conflict situations (proximity, collision, grounding, situations more than one vessel)
Identify type of conflict
Identify relations among vessels/entities
Locate navigational lanes and not navigational areas
Locate fishing areas, closed fishing areas
Locate areas most heavy traffic and slow movers
Locate vessels in conflict situation
Locate vessels need extra attention
Locate areas possible conflict
Locate dangerous cargo
Indicate max, min speed values for different types of vessel
Indicate max, min dimensions for different areas in harbor
Group vessels that share similar characteristics
Discern which type vessels and navigational lanes share similar attributes
Associate sea lines with origin-destination
Associate vessel dimensions with navigational areas, grounding zones

Understanding normal vessel behavior may be considered, thus, as a “high-level” goal. In the process of acquiring this knowledge, the operator generates more specific,
“low-level” queries. For example, a novice operator analyzes and explores the data in order to locate fairways,
describe different types of vessels, associate vessel tracks
with destinations, locate non navigational and dangerous
zones, etc. These low-level queries or user tasks may require one or more visualization operations [14], and a specific visualization/interaction technique may support one
or more low-level queries or user tasks (see table 1).
Many of these low-level queries match categories of visualization operations, presented in well-known visualization operations taxonomies. Two of such taxonomies can
be found in [29, 31]. Zhou and Feiner present in [31] a
classification of visual tasks regarding their visual accomplishments and implications. The purpose of Zhou and
Feiner’s study is to formulate general rules that can directly
relate high level presentation intents (such as inform) to
low-level visual techniques (such as highlight). The taxonomies made by Wehrend and Lewis [29] and Zhou and
Feiner [31] regarding visualization operations consider the

following domain independent categories: locate, identify,
distinguish, categorize, cluster, distribution, rank, compare, associate, cluster, background, emphasize, generalize, rank, switch, encode, and correlate. Regarding their
relevance and functionality in temporal cartographic animations, Ogao and Kraak [14] condensed these categories
in four: identify, locate, compare, and associate.
Neither low-level nor high-level goals or tasks have
been defined previously in this domain (maritime) and for
this problem (anomaly detection), and neither have tasks or
goals that are accomplished analyzing data been related to
visualization operations. We do not expect to cover all lowlevel queries or tasks, but a small set that can be considered
representative, since they are instantiations of different visual operations identified in earlier taxonomies. Regarding
their relevance for maritime anomaly detection, we consider: categorize, cluster, distribute, distinguish, compare,
identify, locate, rank, correlate and associate. Considering
[11,14,29], visualization operations are defined as follows:
• Categorize: to place in specifically defined groups or
classes
• Cluster: to join into groups of similar or related type

175

Table 2: List of visual operations, operational tasks and specific tasks for evaluation
A NALYTICAL

V ISUAL

REASONING

OPERATION

Categorize
Compare
Understanding
normal vessel
behavior &
acquisition
normal
picture

Distribution

Locate
Correlate
Associate
Distinguish

Detection
identification
anomalous
behavior

Compare
Identify

O PERATIONAL TASK

S PECIFIC TASK EXPLORED DURING THE
EXPERIMENTS

Categorize vessels that have no type assigned
Compare behavior cargo with fishing, passenger, tanker
Describe how different navigational lanes
(cargo, passenger, fishing areas, etc.) are
distributed over the map and time

Locate areas most heavy traffic and slow
movers
Discern which type vessels and navigational lanes share similar attributes
Associate sea lanes with origindestination
Recognize outliers (high/low values
speed, danger of grounding)
Compare behavior particular vessel to behavior vessels same type
Identify conflict situations (proximity,
collision, grounding)

• Distribute: to describe the overall pattern. Distribute
is related to cluster. The cluster operation involves
detection of groups, whereas distribute requires a description of the overall clustering
• Distinguish: to recognize as different or distinct (relevant for anomaly detection and identification)
• Compare: to examine in order to find similarities and
differences (comparing in order to find differences is
relevant for anomaly detection and identification)
• Identify: to establish the collective characteristics by
which an object is distinctly recognizable or to describe an object which was previously unknown (relevant for anomaly detection and identification)
• Locate: to specify position (of known objects/entities)
• Rank: to order with respect to other objects/entities of
same type
• Correlate: to establish relations and connections
• Associate: to link or join in a relationship
Table 1 presents a set of visual operations that embrace,
from our point of view, the set of low-level queries or user
tasks related to the high-level goals of “understanding normal vessel behavior” and “detecting/identifying anomalous
behavior”. While examples of all visual operations can be
found for both “high-level” goals, the flow distinguish,
compare and identify are typical operations of “detecting/identifying anomalous behavior”. Table 1 maps such
visual operations to example operational tasks. Table 2 includes specific exercises for evaluation purposes.

Categorize vessels that do not report type
Compare fairways of different types of vessels
and point out main similarities
Describe navigational and no navigational zones
for cargo, tanker, fishing, pilot and passenger
Describe normal behavior of cargo, passenger,
tanker, pilot and fishing vessels
Locate zones with high traffic density
Examine which type vessels share similar attributes
Name typical passenger routes and describe their
frequencies
Are there any vessels that do not behave normally?
Compare selected (by system) not normal vessels
to background traffic and describe differences
Identify truly anomalous vessels/situations from
ones highlighted by system. Provide justification

During the evaluation, we would like to answer the following question: does the representation of normal behavioral models built from representative data support the “understanding of normal vessel behavior”? “Matching incoming data to normal (mental) models” is as well an iterative process, that looks for deviations from normal behavioral (mental) models. In this case, does the joint representation of normal models and the incoming data (or data
selected as suspicious) support the detection and identification of anomalous behavior?

4.2

Method

The evaluation approach assesses both the ability of normal behavioral vessel representations to support the first
two phases of the analytical reasoning process previously
described, and the comprehensibility of the visual representations provided. To do so, we have divided the tasks
presented in representative of understanding the vessel behavior and tasks representative of finding anomalous behavior (e.g. see tasks marked with “anomaly” in table 2).
Participants are divided into two groups. One group is
considered the baseline group, thus, not representation of
the normal behavioral model is provided as aid to performing the tasks. Both groups are then compared taking into
account the metrics described below.
Metrics The proposed assessment methodology includes three criteria: correctness of response, time to complete each task and user reactions. The first two criteria
measure the ability of the visual representations of normal
176

models to facilitate the comprehension of vessel behavior
and the detection of hidden anomalies. Therefore, both
groups (participants with and without support from normal
behavioral models representations) are compared regarding the correctness of their responses and the time spent
on completing tasks. The third metric, user reactions, assesses the appropriateness of the visual representations of
normal models provided and the extent to which participants view such representations supportive for their goals
and tasks. It also includes the interpretation and participant’s level of understanding of such representations. User
reactions are collected through a questionnaire with rating
and open-ended questions. In addition, user actions and
steps are logged in order to study the strategy that the participants use to solve the tasks.
Evaluation environment In order to carry out the representative tasks, the prototype (VISAD) has been adjusted
by building an interactive platform (using C#) that integrates Google Earth and Matlab (see figure 4). The vessel
traffic data is shown over the geographical map and a integrated module displays representations of normal behavioral models. The biggest challenge we have faced while
representing normal behavioral models was the fact that
we would need a six-dimensional space to represent such
probability density functions. Therefore, we have projected the probability values over the geographical map (as
overlays and 3D views). The interested reader is referred
to [20] for more details on the representation of normal behavioral models.
Data The data set used for investigative analysis is a
collection of real AIS messages broadcasted by vessels
traveling along the west part of the Swedish coast, including Gothenburg port area and parts of the coast of Denmark, Germany and Norway. The data corresponds to 17
days during winter. Ten days of the data set are used as
training data, i.e. they are considered to model the normal
vessel behavior. Normal behavioral models are constructed
for each type of vessel and navigational status (see an example in figure 4). The normal models capture the dynamic
behavior of the different types of vessels (cargo, tanker,
passenger, pilot and fishing boat). Four days of data are
explored by the participants during the experiments.

Figure 4: Prototype, VISAD. Real AIS traffic data is displayed
using Google Earth. For each vessel and observation, dynamic
data (position, speed, course, etc.) and static information (type
of vessel, dimensions, name, MMSI, etc.) are displayed. In addition, the probability value of having such observation is presented. On the right, a visual representation of the normal behavioral model for cargo vessels is provided.

obtained the best rating.

5

Future work and conclusions

Following the described quantitative evaluation, a qualitative approach will be carried out, where operators from
maritime control centers will be interviewed.
This article has briefly described the process of designing empirical tests to assess the usability of normal behavioral models representations. The contribution of this work
is threefold: a description of the analytical reasoning process that needs to be supported, a characterization of representative tasks for evaluation and the application of such
findings to the design of empirical tests. These aspects can
be used not only in future evaluations (as guidelines) but
also during the design of future anomaly detector systems
when fully automatic approaches are not viable and human
participation is needed. Hence, both visual analytics (analytical reasoning and evaluation) and anomaly detection
may benefit from this work.

Acknowledgements

4.3 Summary of results

This work was supported by the Information Fusion Research Program (University of Sk¨ovde, Sweden) in partnership with the Swedish Knowledge Foundation under grant
2003/0104 and carried out in collaboration with SAAB
Electronic Defence Systems (Gothenburg, Sweden).

The analysis of the results obtained from the experiments can be found in [20]. The results show that the group
of participants that have normal model visualizations perform better, both carrying out the tasks related to understanding the normal behavioral picture and the tasks related to detecting anomalous behavior. However, no significance difference regarding the time to complete each
task was noted. Users using normal models representations
rate the visualizations useful and among the three visualizations provided, the overlay representation of the model

References
[1] J.W. Bodnar. Making sense of massive data by hypothesis
testing. In International Conference on Intelligence Analysis, 2005.
[2] S. Carpendale. Evaluating information visualizations. In
A. Kerren, J.T. Stasko, J.-D. Fekete, and C. North, edi-

177

[3]

[4]

[5]
[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]
[17]

tors, Information Visualization, Lecture Notes in Computer
Science 4950, pages 19–45. Springer-Verlag Berlin Heidelberg, 2008. 10.1007/978-3-540-70956-5 2.
C. Chen and M.P. Czerwinski. Empirical evaluation of
information visualizations: an introduction. International
Journal of Human-Computer Studies, 53(5):631–635, 2000.
P. Compieta, S. Di Martino, M. Bertolotto, F. Ferrucci, and
T. Kechadi. Exploratory spatio-temporal data mining and
visualization. Journal of Visual Languages & Computing,
18(3):255–279, 2007.
M.-F. Costabile and D. Malerba. Editor’s foreword. Journal
of Visual Languages & Computing, 14(6):499–501, 2003.
L. Costello, G. Grinstein, C. Plaisant, and J. Scholtz. Advancing user-centered evaluation of visual analytic environments through contests. Inf Visualization, 8(3):230–238,
2009.
U. Demˇsar. Combining formal and exploratory methods for
evaluation of an exploratory geovisualization application in
a low-cost usability experiment. Cartography and Geographic Information Science, 34(1):29–45, January 2007.
M. Duffy. Sensemaking in classroom conversations. In
I. Maso, P. A. Atkinson, S. Delamont, and J. C. Verhoeven,
editors, Openness in research: The tension between self and
other, pages 119–132. Assen, The Netherlands: Van Gorcum, 1995.
A. Ebert, A. Dix, N. Gershon, and M. Pohl. Human aspects
of visualization. In Human-Computer Interaction – INTERACT 2009, Lecture Notes in Computer Science, pages 965–
966. Springer Berlin / Heidelberg, 2009.
G. Klein, B. Moon, and R. R. Hoffman. Making sense of
sensemaking 1: Alternative perspectives. IEEE Intelligent
Systems, 21(4):70–73, 2006.
E.L. Koua, A. MacEachren, and M.J. Kraak. Evaluating the
usability of visualization methods in an exploratory geovisualization environment. International Journal of Geographical Information Science, 20(4):425–448, 2006.
J.B. Kraiman, S.L. Arouh, and M.L. Webb. Automated
anomaly detection processor. In A.F. Sisti and D.A. Trevisani, editors, Proceedings of SPIE: Enabling Technologies for Simulation Science VI, pages 128–137, Jul 2002.
S. Laskowski, T. O’Connell, and Yee-Yin Choong. A framework for user-centered evaluation for a visual analytics contest. In Position Paper for the IEEE VAST Metrics for the
Evaluation of Visual Analytics workshop, 2007.
P. J. Ogao and M. J. Kraak. Defining visualization operations for temporal cartographic animation design. International Journal of Applied Earth Observation and Geoinformation, 4(1):23– 31, 2002.
E. S. Patterson, E. M. Roth, and D. D. Woods. Predicting vulnerabilities in computer-supported inferential analysis under data overload. Cognition, Technology & Work,
3:224–237, 2001.
P. Pirolli. Information Foraging Theory: Adaptive Interaction with Information. Oxford Unive. Press, 2007.
Peter Pirolli and Stuart Card. The sensemaking process
and leverage points for analyst technology. In International
Conference on Intelligence Analysis (IA’05), 2005.

[18] C. Plaisant. The challenge of information visualization evaluation. In AVI ’04: Proceedings of the working conference
on Advanced visual interfaces, pages 109–116, New York,
NY, USA, 2004. ACM.
[19] B. Ristic, B. La Scala, M. Morelande, and N. Gordon. Statistical analysis of motion patterns in AIS data: Anomaly
detection and motion prediction. In 11th Int. Conf. of Information Fusion, July 2008.
[20] M. Riveiro and G. Falkman. (Submitted) Evaluating the
usability of normal behavioral models visualizations for analytical reasoning . In 7th International Conference Computer Graphics, Imaging and Visualization. IEEE, 2010.
[21] M. Riveiro, G. Falkman, T. Ziemke, and T. Kronhamn. Reasoning about anomalies: a study of the analytical process
of detecting and identifying anomalous behavior in maritime traffic data. In W.J. Tolone and W. Ribarsky, editors,
SPIE Defense, Security, and Sensing. Visual Analytics for
Homeland Defense and Security, volume 7346, Orlando,
FL, USA, 2009.
[22] J. Roy. Anomaly detection in the maritime domain. volume
6945, pages 69450W1–14. SPIE, 2008. Optics and Photonics in Global Homeland Security IV.
[23] J. Scholtz. Beyond usability: Evaluation aspects of visual
analytic environments. In 2006 IEEE Symposium On Visual
Analytics Science And Technology, pages 145–150, Baltimore, MD, 2006.
[24] J. Scholtz. Metrics for evaluating human information interaction systems. Interacting with Computers, 18(4):507–
527, 2006.
[25] J. Scholtz. Workshop on evaluation: Possible metrics to
consider for evaluation. In Position Paper for the IEEE
VAST Metrics for the Evaluation of Visual Analytics Workshop, 2007.
[26] J. Stasko. Evaluating information visualizations issues and
oportunities. In Proc. BELIV’06 - Beyond Time and Errors:
Novel Evaluation Methods for Information VisualizationWorkshop of AVI’06, pages 5–8, 2006.
[27] J.J. Thomas and K.A Cook, editors. Illuminating the Path:
The Research and Development Agenda for Visual Analytics. IEEE Computer Society, Los Alametos, CA, 2005.
[28] Caroline Tobon. Usability testing for improving interactive
geovisualization techniques. Technical Report ISSN:14671298, Centre for Advanced Spatial Analysis, University
College London, 2002.
[29] S. Wehrend and C. Lewis. A problem-oriented classification of visualization techniques. In VIS ’90: Proceedings of
the 1st conference on Visualization ’90, pages 139–143, Los
Alamitos, CA, USA, 1990. IEEE Computer Society Press.
[30] M. A. Whiting, C. Varley, and J. Haack. Introducing additional domain-specific measures in evaluating visual analytic tools. In Position Paper for the IEEE VAST Metrics for
the Evaluation of Visual Analytics workshop, 2007.
[31] M. X. Zhou and S. K. Feiner. Visual task characterization
for automated visual discourse synthesis. In CHI’98: Proceedings of the SIGCHI conference on Human factors in
computing systems, pages 392–399, NY, USA, 1998. ACM
Press/Addison-Wesley Publishing Co.

178

