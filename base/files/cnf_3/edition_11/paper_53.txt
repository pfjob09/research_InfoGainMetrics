A Mixed Reality System for Building Form and Data Representation
Matthew Pilgrim’, Din0 B o ~ c h l a g h e m Dennis
~,
Loveday2, Michael Holmes3
1 Centre f i x Innovative Construction Engineering, Loughborough University
2 Loughborough University, Department of Civil and Building Engineering
3 Arup Research & Development
ABSTRACT: The development of a prototype mixed
reality system for three-dimensional building form and
data visualisation is presented.. The requirements are:

1.1

VR is a medium that provides participative 3D
visualisation and simulation of virtual or computergenerated-worlds. Unlike animation (where previously
created images are simply replayed in sequence) VR
environments can be freely viewed and examined in any
way from an infinite number of perspectives without
noticeable delay - that is, in real time.

Allow user interaction with an external view of one
or more buildings simultaneously.
Maintain user contact with his or her normal
working environment.
Support multi-user inpui’ for collaborative design
discussions.
Facilitate intuitive navigation of the internal spaces.
Present data relating to multiple design solutions.

1.2

KEYWORDS: Virtual Environment, Augmented Reality,
Mixed Reality, Visualisation, I3uilt Environment, Thermal
Analysis, Building Performance.

1.3

Augmented virtuality

This consists primarily of a completely computergenerated graphical display that has been augmented by
This involves
the use of video ‘reality’ [ l l ] .
superimposing a real scene on an aspect of a virtual
reality model. An example could be integrating a video
display of an outdoor scene with the view through a
window in the VR model of a building. The key
difference between this and AR being that, what is being
augmented is primarily ‘virtual’ rather than ‘real’. In the
future, however, it may be more difficult to make this
distinction. The prototype described does not make use of
this mode at present.

Introduction

The system presented in this paper is designed to
facilitate the validation, optimisation, review and
presentation of building related engineering data using
Mixed Reality (MR). MR seeks to combine the best
features of real environments with those of virtual
environments. Figure 1 illustrates the relative position of
these in what Milgram et a1 [ I l l term ‘The Virtuality
Continuum’. Each mode in the continuum is briefly
described below.

369
0-7695-1195-3/01 $10.00 0 2001 IEEE

Augmented reality

Augmented Reality (AR) is a technology in which
the user’s view of the real world is augmented with
additional information generated by a computer [9]. It is
complementary to Virtual Reality (VR) and enables users
to interact with an integrated virtual and real world with
ease. AR involves superimposing information in the form
of a 3D computer-generated image on top of a ‘real-life’
visual scene. The scene may consist of still photographs
and/or video images. It is possible, for example, to
superimpose a 3D CAD model of a building onto a
picture of its proposed site to show what the completed
building will look like. In the case of video images, real
time processing is essential to ensure currency of the
information being relayed.

The paper proposes that only by combining the
relative advantages of the Augmented and Virtual Reality
are these achievable. The overall goal is to successfully
distribute amongst the worJ$orce of an engineering
consultancy an application i h t is accessible through
inexpensive hardware whilst supporting more advanced
input/output mechanisms. The guiding aim is to support
the design process and facilitcite communication amongst
the project team and the conrtruction client. The paper
presents an overview of the system and highlights key
issues raised during the development process.

1

Virtual environment

2

The Immersive mode requires a Head Mounted
Display (HMD), a three-degree of freedom tracking
device and a miniature video camera, Figures 4 & 5.

Virtual reality in design and construction

There has been considerable interest in the use of
Virtual Reality in construction for the last decade
although the uptake was slow at the outset [4] it is now
gaining momentum. As a design tool VR has many
advantages for the architect. By allowing architects to
immerse themselves in their design, VR allows a much
clearer understanding of both a qualitative and
quantitative nature of the space they are designing. VR
allows designers to evaluate proportion and scale using
intuitive interactive modelling environments [ 101 and so
simulate the effects of lighting, ventilation and acoustics
in internal environments [12, 161. As a visualisation tool
VR is also used to communicate ideas from designers to
clients by generating walkthrough models to test the
design with the clients in a more direct manner [ 141. VR
can also be used to model the construction sequence in
order to simulate and monitor site progress. This is done
using a pre-prepared library of 3D graphical images of
building components, facilities etc. and their related
activities, and generate VR models representing views of
the construction sequence at any given time of the process
[I].

The Augmented Reality mode is based on
recognising pre-registered patterns in real time from a
video source. The patterns position and orientation is
used to scale and rotate the 3D (virtual) object before it is
rendered over the video source and displayed to either a
standard SVGA monitor or a HMD, Figures 2 & 4.
The Virtual Environment mode uses the same
rendering routines as the AR mode. Navigation of the
virtual environment is either mouse driven or by a
combination of mouse commands and head tracking. The
images are displayed on either a monitor or HMD,
Figures 3 & 5.
In the following sections the software and hardware
are discussed with respect to each of these modes.

4

The pattern recognition software is available as a
library of routines and' example material called the
ARToolkit developed at HIT Laboratory [3]. The
rendering routines presently used within this toolkit rely
on the GLUT libraries and as such are restricted to
running under DOS. This restricts the development of,
applications running on Microsoft Windows operating
systems, therefore the ARToolkit was converted to use
OpenGL calls for the scene rendering and transformation.
These converted files were then compiled into a basic
application for displaying the captured video with a
primitive shape rendered in 3D over the recognised
pattern. This application emulates the example provided
with the original ARToolkit and demonstrates no more
than an improved method of implementation.

Mixed reality environments can be useful for site
ixploration, the visualisation of proposed buildings within
the context of their locations, and the planning, and
monitoring of construction and refurbishment projects
[2,13]. Data visualisation in this and other fields has
undergone major developments in the last decade
resulting in systems and tools which have become mature
for producing visualisation applications in areas such as
Computational Fluid Dynamics, Medicine, Social
Sciences, and the Environment [7, 6, 8 & 151. A
comprehensive review of these systems is presented in the
AGOCG technical report 9 [SI.

3

Augmented reality software

System overview
Using OpenGL functions the frame rate of the video
layer was significantly improved by rendering each frame
as a texture (on a single polygon) thus maximising the use
of the graphics cards dedicated texture memory. Again
this technique has been observed elsewhere and is not
novel. This improvement does, however, allow a full
screen mode to be implemented; this is important, as it is
not presently possible (without significant development)
to render a single application window directly to a video
output port. Instead the entire screen is replicated on the
s-video port. Thus, to supply the appropriate images to
the HMD it is necessary to have a full screen mode.

The prototype described here is capable of importing
geometry from a proprietary building analysis tool or a
common 3D file format and displaying it and associated
data in one of four ways:
Desktop Virtual Environment (D-VE)
Immersive Virtual Environment (I-VE)
Desktop Augmented Reality (D-AR)
lmmersive Augmented Reality (I-AR)
The desktop mode requires a MS Windows driven
PC and a web camera, Figures 2 & 3.

370

The ability to import thrze-dimensional models from
two different sources has been implemented:

6

Desktop hardware

This is the most basic mode of implementation and is
designed to allow the system to be used on relatively
‘low-end’ workstations with a minimum hardware
requirement. The basic requirements are:

ASE Files, this is a common file format that is
available as an export from proprietary packages such
as 3D Studio max.
Analysis geometry, written within an in-house
analysis tool containing additional data about the
surface types (such as their properties).

A web camera (preferably USB)
A standard personal computer

To allow multiple models (of either type) to be
presented simultaneously, ,a project file type was
implemented. This contains information about the source
of each model required within the project and the pattern
with which it is to be associa.ted. Further improvements
of the AR code were:

In this configuration the web camera is used to
capture relatively low-resolution images in real-time. The
software then augments these with the scaled and rotated
rendering of the model on the monitor. In the Virtual
Environment mode the camera is made redundant, the
cursor is used to drive the onscreen rendering of the
model. Due to the low resolution image captured from
the camera, the AR Mode is limited to a sizeable window
instead of full screen where the picture quality will be
poor due to the amount of scaling required.

The ability to have a single model highlighted in the
AR mode, thus allowing ithe transition into VR mode
with the same model displayed.
Models were
selected from those available within the project file
by looping through the list using the arrow keys. The
selected model was identified using a red line around
the base (Figure 4).
The addition of an Auto-Scale feature to size the
models to the pattern’s base area allowing models of
varying sizes to be displayed side by side. This can
be disabled if relative scales are required.
A rotate mode was implemented to spin the selected
model continuously thus allowing closer inspection.

7

Immersive hardware

The Immersive Mode requires significantly more
hardware and as such will not be appropriate for all users.
The hardware used on the test system was:
A graphics card with S-Video input and output, see
below for further requirements.
A miniature 12 volt colour S-Video quality camera.
The Sony Glasstron HMD (S-Video or VGA input)
The Intersense InterTraxZ USB Tracking device
designed to fit the Glasstron HMD

Virtual reality software
The ARToolkit does not contain code for
implementing desktop VR (referred to here as a Virtual
Environment, VE). The HIT Laboratory has however
demonstrated applications that combine AR with smooth
Here, we use the routines
transitions into a VE.
developed to render the AR content to display the model
in either a head-tracked or mouse-driven VE.

7.1

Graphics Cards

The system captures high-resolution images from the
miniature camera through the S-Video input on the
graphics card and supplies the AR or VR rendering back
to the HMD through the output port. Several graphics
cards were tested:

The head-tracked Immersive Mode uses an
Intersense InterTraxZ tracking device to provide the
user’s viewpoint orientation with regard to the model.
This is combined with left and right mouse button
commands to provide forward and backward flight
along the line of sight.
The Desktop Mode removes the necessity of owning
a tracking device by alhwing a standard mousedriven cursor to govern the direction of movement.

Hercules 3D Prophet I1 GTS 64mb
ASUS AGP-V7700
AT1 Radeon 64 VIVO
Of the three cards tested, no single card offered all
the desired attributes, i.e. have both an input and output svideo port, be windows 2000 compliant and have video
capture drivers compatible with the ARToolKit. One
reason for this is that the newer cards, which are
compliant with MS Windows 2000 (a more stable
development environment), ship with video capture
drivers based on Windows Driver Model (WDM). WDM

Once in the VE mode it is possible to view any of the
models within the project file from any viewpoint.

371

Output: 380TV lines via a BNC connector
Interchangeable lenses (3.6 through t o 12”).

is an attempt to make drivers more compatible but the
ARToolKit was developed using Microsoft Vision SDK,
which is based on the older Video For Windows (VFW)
driver model. Therefore the project required a graphics
card supplied with a VFW driver.

This was fixed to the front of the HMD using Velcro
to allow its removal or repositioning. The camera output
was converted to a standard s-video cable using an
adapter and connected to the input port of the graphics
card. Once the VFW drivers are correctly installed it is
possible to test the cameras signal with a variety of video
capture programs including Adobe Premier. The lens
chosen for the camera greatly impacts the effectiveness of
the AR experience. It is important that when the user
looks through the HMD at the surrounding environment
and for example his or her own hands that they see
minimum distortion. For example the scale of their hands
is correct, they are in focus and not distorted. The
optimum lens for the hardware detailed here was found to
have a focal length of 3.6”.
An amount of distortion
was always present within the system because of the use
of a mono HMD, using two cameras and a stereo HMD
was seen as prohibitively expensive.

Another graphic card related problem is the control
of the output to either the SVGA monitor or the s-video
port. Ideally the image would be displayed on both
simultaneously, thus allowing bystanders to see the
immersive environment whilst the user if wearing the
HMD. Not all graphics cards tested supported this mode
of operation and none of them supplied any
documentation relating to it. A software fix called
TVTool was located which, depending on the architecture
of the graphics board, allowed.the output to be switched
between the monitor and or the s-video port.

7.2

Head tracking

Head tracking is not required for the AR modes as
video based pattern recognition is used; instead the head
tracking is used in conjunction with a HMD to implement
a more immersive VE experience. The first tracking
device used was the Intertrax serial device designed to
strap to the rear of the HMD. This device was bulky and
required an external power supply. This was therefore
changed for the newer Intertrax2 device. The Intertrax2
tracker, which is a lot smaller than its predecessor, is
connected to the host computer via the USB port. The
USB port supplies power to the device, thus removing the
necessity for an external power source and it’s associated
cable. The tracker data was captured using the API
supplied with the device and used to orientate the users
view of the VE.

7.3

8

The system described so far is capable of displaying
the contents of either an ASE or proprietary file as threedimensional rendering. This is adequate to communicate
the form of the object described by the file (e.g. a
building) but not any additional data related to that object.
Thus, several enhancements to facilitate data
representation were made to the code. The data used
relates to the input required for a proprietary thermal
analysis tool. That is, in addition t o the buildings’
geometry, it is necessary to supply each surface’s thermal
properties and boundary conditions. Although the system
described is limited to input data the same concept may
be applied to the analysis results. The ability to display
data in an intuitive way encourages communication
between members of the design team leading to a greater
understanding of the analysis. Several examples of
analysis input representations are given.

Head mounted display

The HMD used within the prototype had the
following specification; low cost, high resolution (1.55
million pixels), lightweight and s-video/SVGA input.
The image quality of the HMD was visibly greater when
the images were supplied via a SVGA cable. This wasn’t
possible because affordable graphics cards with dual
monitor support (and a s-video input port) are not
available and therefore a s-video supply was used.

7.4

8.1

A mini CCD camera (33C-B36) with the following
characteristics was selected:
(H)

* 15”

Surface type

The analytical tools used in this study require the
engineer to categorise the boundary conditions for each
surface as Internal, External, Adiabatic or Isothermal.
This data was mapped by applying colour to the surfaces
(e.g. blue internal surfaces) to allow rapid identification of
miss-classified surfaces. For large models containing
many surfaces, a facility was added to allow surfaces of
each category to be hidden from view. This allows the
rapid identification of clusters of similarly classified
surfaces.

Video camera

Size: 33”
(W) x 33”
Weight: 280g
Power source: 12v

Display capabilities

(D)

372

8.2

three-dimensional model and the input/output of the
analysis tool.

Surface display

With all surfaces of thl: same category opaquely
rendered in one colour, it became difficult to distinguish
the edge of one surface from another in the same
category. A wire frame mode was therefore implemented
(figure 4). Both the wire frame and opaque mode could
be independently turned on or off, thus allowing several
different display combination:;. With both modes turned
on the surfaces are framed with a black outline allowing
their edges to be distinguished. With just the wire frame
mode turned on, the user is able to see through the model
thus allowing them to apprecia:te its complexity.

8.3

9.2

In many scenarios it is necessary for more than one
engineer to be involved in the validation and optimisation
of the design. This collaboration is best supported by the
prototype operating in the desktop mode where the
camera or pattern can be easily moved by any en,’mineer to
emphasise his or her particular point of view on a design
decision. The results of this movement are displayed in
real-time, preferably on a large desktop monitor, figures
1-4. Likewise, it is possible for the engineers to work
together around the monitor to navigate the design in the
desktop VE mode. Remote collaboration may also be
possible but has not been considered within this project.

Surface normals

Both the ASE and Proprietary files contained data
relating to the normals of each surface. As it is common
to check the direction a surface is facing it was considered
important to represent this data within the application.
For example, the proprietary analysis file requires all
surfaces to face inwards. ‘The surface normals were
therefore rendered as a red line of fixed length from the
centre of each surface. This representation could be
extended to encode information such as the surfaces’ area,
heat transfer coefficients etc by scaling the line’s length.

9

9.3

Design review & presentation

Here, the prototype is used to facilitate ‘round-table’
discussions of the building’s form and associated analysis
data. In this mode the miniature camera may be replaced
by a tripod mounted digital video camera for improved
image quality. This may also offer the ability to record
the meeting (note the computer generated content would
be missing but could be recorded from the system video
output). The SVGA monitor used in the collaborative
mode could also be replaced with a projection system,
again allowing a higher level of detail to be seen by the
participants of the meeting. In this mode of operation, the
engineers could easily present and contrast several design
options to the client. In addition to this, the intuitive AR
interface will allow the client to manipulate the view of
the model for themselves (previously not feasible with for
unskilled operators). It will also be possible for a single
engineer to walk through the interior of the model using
the immersive V E mode whilst the other participants
watch the projection screen.

Application in engineering design

This project focuses on lechnological developments
that may impact the way engineers design and collaborate
within the context of large construction projects.
Although the prototype is still in the early stages of
development, the following scenarios have been
identified, through discussions with the sponsoring
companies, as possible modes of operation.

9.1

Collaborative design

Design validation 8i optimisation

An engineer may use the system to inspect and
validate the design parameters associated with each
surface of a building, both internally and externally, by
switching seamlessly between the AR and VE modes. As
the system’s capabilities mature and it becomes capable
of displaying analytical results e.g. the building’s thermal
response, optimisation of the design will be possible. For
example, by assigning different input parameters and the
associated analysis results to individual patterns within a
single project file, multiple datasets/objects may be
represented side-by-side in the immersive AR display
mode (figure 4). By physically manipulating the patterns
the engineer will be able to perform multiple comparisons
of the data, thus facilitating the decisions required to
optimise the design. Here, the immersive VE mode
(Figure 5) is ideal for the in-depth exploration of both the

10 Future work
The display of analysis results within the system
could be implemented by reading separate result files and
configuring a mapping variable e.g. mapping results
column one to surface normal length.
In addition,
animated representations of data become feasible.
At present, the pattern matching routines require the
entire pattern to be visible to the camera before the
computer-generated image can be rendered. This, in
practice, means the user must be careful not to obscure
the camera view or to place anything (like a figure) on the
pattern. Oblique angles can also reduce the routine’s
ability to identify the pattern; this may be resolved by

373

using several patterns with a known relationship mapped
onto a solid (for example the numbers one to six on the
sides of a cube). The system would augment the primary
pattern (e.g. side one) even if obscured, by calculating its
position from any of the secondary patterns (sides two to
five) thus making the system more robust.

6
7

11 Conclusion

8

An augmented and virtual reality prototype for the
presentation of a building’s form and its associated data
have been presented. Several scenarios for its use have
also been proposed.
Both the development and
application of the prototype have been driven by cost
restraints common to the construction industry.
At
present this constraint may still restrict the use of HMDs,
which remain equivalent in cost to a well specified
personal computer. It is therefore proposed that an
optimum system will
support many different
configurations of hardware, thus allowing companies to
select the appropriate level of expenditure.
The
combination of AR and VR with immersive and desktop
modes results in a flexible system suitable for many
different engineering applications.

9

IO

11

12

12 Acknowledgements
The work detailed within this paper was carried out
in partial fulfilment of the EPSRC Engineering Doctorate
scheme at The Centre for Innovative Construction
Engineering (CICE), Loughborough University.
The
Research Engineer, Matthew Pilgrim, is sponsored by
Amp Research and Development where the work forms
part of the Desktop of the Future Project.

13

14

13 References
1

2

3

4

5

15

Adjei-Kumi T. & Retik A., (1997): ‘A Library Based 4D
Visualisation of Construction Processes’, Proceedings of
the International Conference on Information Visualisation
IV’97, London, August 27-29, pp 315-321.
Anumba C. J. & Duke A. (1997): ‘Structural Engineering
in Cyberspace: Enabling Information and Communications
Technologies’, The Structural Engineer, Vol. 75, No. 15, 5
August, pp 259-263.
Billinghurst, M. and Kato, H . (1999). Collaborative Mixed
Reality. In Proceedings of International Symposium on
Mixed Reality (ISMR ‘99). Mixed Reality-Merging Real
and Virtual Worlds, pp. 261-284
Bouchlaghem, N., M., and Liyanage, I., G., Virtual Reality
Applications in the UKs Construction Industry,
Construction on the Information Highway, CIB W78
Working Commission on Information Technology in
Construction, Bled (Slovenia), Turk, Z. (Ed), University of
Ljublajana 1996.
Brodlie K. W., Gallop J. R., Haswell J., Hewit W. T.,
Larkin S., Lilley C. C., Morphet H., Townend A., Wood J.,

16

Wright H., “Review of Visualisation Ssystems”, AGOCG
Technical Report, no. 9. February 1995.
Chen P. C., “A Climate Simulation Case Study”,
Proceedings of IEEE Visualisation 93, IEEE Computer
Society Press, 25-29 October 1993, San Jose, USA.
Cox J . P., “The Visualisation of 3D Device Simulation:
Using AVS with an Existing Simulator”, Proceedings of
AVS ‘93, 24-26 May 1993, Lake Buena Vista, Florida,
USA.
Kim J. J. H., Dogan N., McShane D.L., Kessler M. L., “An
AVS-Based System for Optimisation of Conformal
Radiotherapy Treatment Plans”, .Proceedings of AVS ‘95,
19-21 April 1995, Boston, USA.
Klinker G. J., Ahlers K. H., Breen D. E., Chevalier P. Y.,
Crampton C., Greer D. S., Koller D., Kramer A., Rose E.,
Tuceryan M. & Whitaker R. (1997): ‘Confluence of
Computer Vision and Interactive Graphics for Augmented
Reality’, Presence:
Teleoperations
and
Virtual
Environments: Special Issue on Augmented Reality, MIT
Press, Spring, 1997.
Kurmann D. (1995): ‘Sculptor - A Tool for Intuitive
Architectural Design’, CAAD Futures ’95 - The Global
Design Studio, Tan M. & Teh R. (Eds), Singapore, pp 323330.
Milgram P. & Kishino F. (1994): ‘A Taxonomy of Mixed
Reality Visual Displays’, IEICE Transactions on
Information Systems, E77-D, No. 12, December.
Nimeroff J. S., Simoncelli E., Badler N. I. & Dorsey J.
(1995):
‘Rendering
Spaces
for
Architectural
Environments’, Presence: Teleoperators and Virtual
Environments, Vol. 4, No. 3, pp 286-297.
O’Connor N. & Retik A. (1998): Augmented Reality in
Construction, Proceedings of the International Conference
on Information Visualisation IV’98, London, July 29-3 1, p
320.
Ormerod M and Aouad G. (1997): ‘The Need for Matching
Visualisation Techniques to Client Understanding in the
UK Construction Industry’, Proceedings of the International
Conference on Information Visualisation ” 9 7 , London,
August 27-29, pp 322-328.
Post F. J . , Van Walsum T., Post F. H., “Iconic Techniques
for Feature Visualisation”, Proceedings of IEEE
Visualisation 95, IEEE Computer Society Press, 29 October
3 November 1995, Atlanta, USA.
Shinomiya, Y. et al. (1994): ‘Soundproof Simulation in the
Living Environment Using Virtual Reality’, Proceedings of
the International Conference Virtual Reality Environments
in Architecture and Design, Leeds, November 2-3

14 Appendix

374

Fig 1 - The Virtuality Contiriuum by Milgram et a/ [ 1 I ]
~~~

Real
Fnvirnnment

Augmented
Reallitv(AR)

Augmented
Virtualitv (AV)

Virtual
Environment

Virtuality Continuum (VC)
Fig 2 - AR desktop mode: Wire frame view

Fig 3 - VR desktop mode: View of buildings interior

Fig 4 - AR immersive mode: Multiple objects

Fig 5 - VR immersive mode (hardware)

375

