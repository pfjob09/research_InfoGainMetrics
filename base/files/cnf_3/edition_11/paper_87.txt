Geometry Compression Based on Mantissa Chunking of Vertices
Deok-Soo Kim'*, Jaeyeol Chung',
Youngsong Cho', Taeboom Jang', and Hyun Kim2

' Department of Industrial Engineering, Hanyang University
17 Haengdang-Dong, Sungdong-Ku,Seoul, 133-791, Korea
Phone: ( 8 2 ) 2 - 2 2 9 0 - 0472
Fax: ( 8 2 ) 2 - 2 2 9 2 - 0472
*Email : dskim@hanyang.ac.kr

* Concurrent Engineering Team, ETRl
Taej eon, Korea

model, represented in the desired data structure with the
desired level of redundancy for the particular application,
can be recovered from the more rapidly transmitted file.
Three most important criteria of compression are the
shape fidelity, the compression ratio, and the
decompression time. Usually the compression time is a
secondary concern since the compression can be done as
an off-line process.

Abstract
Transmission of 3D shape model through Internet has
become one of the hottest issues in these days. Presented
in this paper is a new approach for the rapid transmission
of the geometry data of the shape model. By analyzing the
important three factors, the shape fidelity, the file size,
and the decompression time, for the compression, we
point out the potential problems of previous approaches of
using the deltas between consecutive vertices and propose
an alternative of directly using the position values of
vertices of the model. It turns out that the proposed
approach has smaller file size, has lesser distortion in the
model, and the decompression is faster.

2. Literature
In this paper, we will present an approach to compress
the geometry of a shape model. The issues regarding on
the compression of the geometry and topology will be
provided while the issues on the normal vectors, colors,
and textures are left for another paper. Since the size of
topology data can be twice as larger as the size of
geometry in a mesh model, several researchers have been
working on the compression of topology and achieved
significant compression ratios so that the current
techniques approach to the potential theoretical limit of
topology compression. It is guaranteed by now that only
two bits per triangle is needed to represent the whole
topology of the model [Rossignac and Szymczak 19991,
and further reduction is possible considering the
characteristics of the triangle patterns in the model.
On the other hand, the compression of geometry has
not yet been investigated as much as it deserves, and
hence we believe that improvements in the geometry
compression have to, and can, be made. Even though the
main issue of this paper is geometry, the way geometric
data is compressed is closely related to the topology and
hence we will discuss both issues in parallel.

1. Introduction
Presented in this paper is an approach to compress and
progressively transmit the geometry of a model placing
different priorities on the bit chunks of the mantissa.
However, the compression issue regarding on the
topology will be also discussed since the compression of
geometry is closely related to its counterpart of topology.
Even a carefully designed representation of a model,
such as the winged-edge data structure for a boundary
representation of a 3D shape model, usually contains a
certain level of redundancy by intention. The carefully
designed redundancy is incorporated into the model to
facilitate the speed-up to answer to queries or
manipulations for various kinds of potential applications.
However, the redundancy results in an increase over
minimum possible file size to store the model and hence
causes the transmission delay. Compression refers to a
process of removing the redundancy in the representation
of the model to minimize the file size so that the original

2.1. Topology compression
The compression techniques for the topological data

606
0-7695-1195-3/01
$10.000 2001 IEEE

are in general'concerned with the lossless compression of
the topology of' a:: model- and,.usually provide the exact
bound of the compression ratio.with respect to the number
of either triangles or vertices. .)
In 1995, Deering proposed'ii pioneering scheme using
the generalized triangle mesh which extends the
generalized triangle strip, which has been used in graphics
libraries,.such as OpenGL,"using ,a fifo queue of size 16
[Deering 19951. .geering's algorithm takes roughly 11 bits
per -vertex. In the"topological surgery scheme,,presented
by Taubin and Rossignac, a vertex spanning tree and a
triangle spanning tree are constructed to represent the
topology [Taubin 19981. In 1997, Chow presented a
meshfying algorithm which- transforms an arbitrary mesh
into Deering's generalized triangular mesh structure
[Chow 19971. Touma, and Gots:man reported an algorithm
using a vertex cycle, a cyclic 5:equence of vertices along
triangle edges in the mesh [Touma 19981, and Gumhold
and Strasser an algorithm using cut-border data structure
[Gumhold 19981. Rossignac reported Edgebreaker which
transforms a mesh structure into a string of C, L, E, R,
and S symbols, and the symbols are entropy encoded to
reduce the file size [Rossignac 19991. On the other hand,
Kim ct al. presented an a1gorit;hm which is a completely
different approach from the previous researches [Kim
'

Rossignac proposed a generalization of Deering's work so
that the current position is predicted by the relative
position of k previous vertices [Taubin 19961. On the
otherhand, Touma and Gotsman proposed an algorithm
using a parallelogram rule [Touma 19981. In this approach,
a new vertex is predicted as the fourth corner of a
parallelogram where the other three corners are the
vertices of previous triangle. The error between the fourth
corner of the parallelogram and the real vertex is encoded.
Lee and KO proposed an approach similar to Touma and
Gotsman with a difference of another mapping to model
space [Lee 20001.

'

3. Bit-error trade-off in deltas
To compress the geometry of a model, Deering
calculated-the deltas between consecutive vertices where
the vertex sequence is determined by the generalized
triangle mesh [Deering 19951. Then, each coordinate
value of a delta was pushed into a 16-bit representation
that plays the role of mantissa, and then those 16-bit
symbols, which are in fact numbers with a certain level of
redundancy, are Huffman coded. It seems, however, that
only one exponent value is used for all of the 16-bit
fraction number.
The deltas altogether constitute a distribution and a
distribution inevitably has a variation.. The consequence
of the variation among real numbers is the variations of
exponents as well as mantissa in the machine number. To
transmit a set offloat numbers, the exponents as well as
the mantissas should be transmitted in a synchronized
fashion. To make the representation of the set more
compact, it would be better to get rid of either of exponent
or mantissa, if possible. Hence, only one value can be
applied for all data in the set. It can be easily found that
exponent has usually smaller variation that mantissa has.
To make the exponents of float numbers identical, the bits
in the mantissa of some numbers should be shifted and
this bit-shift results in a lost accuracy. Obviously, this lost
accuracy can become disastrous if the number of bits to
use for the deltas is close to the number of bits shifted. In
particular, the content of 16-bit memory, in Deering's
method, becomes zero if the number of shift is greater
than or equal to 16, regardless what the real value of delta
was. Note that this happens if the maximum variation in
the exponents of deltas is greater than or equal to 16.
Provided that the lengths of the edges in a mesh model
do not vary much, the approach for geometric
compression taken by Deering may work fine since the
generalized triangular mesh structure always provides the
nearby vertex as the next one. If this approach is, however,
taken by other topology compression algorithms,
potentially any algorithm, that may produce a point
sequence with a larger variation among the distances

19991.

2.2. Geometry compression
In the graphics community, real numbers are usually
stored in a float dcfincd by four consecutive bytes, and
therefore 32-bits, of memory, and so are the X, Y, and Z
components of vertices of a model. The 8 bits
immediatcly after the first sign bit constitutes an exponcnt
and the rest 23 bits define a mantissa [Cheney 19941.
While the topology compression algorithms are
lossless, its counterpart for geometry is usually lossy. By
discarding a portion of the 32.-bits, which perturbs the
model a little bit without changing the visual appearance
significantly, redundancies among the coordinates of the
vertices, or the difference vectors between the vertices,
result and these redundancies yield the room for
compression. Once the redundaincies occur, the redundant
information may be coded in either fixed or variable
length coding scheme. It is a usual practice to use
Huffinan coding to take the frequencies of the
redundancies into account so that better compression ratio
can be obtained. Since the absolute values of coordinates
of the deltas are usually much less than those of vertices
themselves, most of the approaches use deltas. Based on
this observation, Deering propoijed a scheme to compress
the coordinate values of the deltas using 16 bits from
mantissa and Huffman coding i n 1995 [Deering 19951. In
the work of topological surgery in 1996, Taubin and

607

between the points in the sequence, the result may be
disastrous. If further bit reduction is tried (for example,
using 8 bits for the deltas), the problem may get more
serious. This problem can occur even with Deeringâ€™s
generalized triangle mesh if the sizes of the triangles in
the mesh model vary significantly. Note that a mesh
model is usually simplified before transmitted in a
network and the simplification process intentionally
transforms the meshes to one with irregular sizes to
minimize the file size.
The consequence of the larger errors in the
representation of deltas is the serious distortion in mesh
models. The example shown in Fig. 1, a cactus model,
clearly illustrates this problem. In our experiment, we
employed Deeringâ€™s approach for the compression of
deltas with the vertex sequence produced by Edgebreaker.
Fig. l(e) is the original model of a cactus, and (a), (b), (c)
and (d) are models compressed, transmitted and
decompressed using deltas represented with 4, 8, 12 and
16-bit precisions, respectively, along with one exponent
value. As shown in (d), 16-bit precision delta conveys the
original model relatively faithfully, while 4,8 and 12-bit
precision models are unacceptably distorted. All other
examples we have experimented have shown similar
phenomenon. Another example, a cat model, is shown in
Fig. 2.

C!

Fig. 2. The distortion of cat model by
compressing deltas with one exponent value.
(a) 4-bit model, (b) 8-bit precision, (c) 12-bit
precision, (d) 16-bit precision, (e) the original
model
Fig. 3 illustrates the distribution of exponents of deltas
for the cactus model, and clearly shows this fact. What is
shown in Fig. 3(a) is the distribution of all coordinates of
the deltas, while (b) shows its counterparts for each X, Y,
and Z coordinates separately. As shown in both figures,
the differences between the maximum and the minimum
values of the exponents are larger than 16. Similar
analysis for the cat model shows same pattern. Note that
the sequences of vertices are all computed from
Edgebreaker.
Suppose that we were lucky so that all the deltas have
identical exponent values and hence the 16-bit precision
deltas are exact up to the 16â€˜h bits. Even in this case, the
rest 7 bits are lost for all deltas, and the errors accumulate
as the number of points increases. In the case of 16-bit
precision of deltas, the worst-case error can be as big as
( V - I ) / 2 l 6 where V is the number of vertices in the
model.

L:

Fig.1. The distortion of cactus model by
compressing deltas with one exponent value.
(a) 4-bit model, (b) 8-bit precision, (c) 12-bit
precision, (d) 16-bit precision, (e) the original
model

4. Mantissa chopping of vertices
In our experiment, using Deeringâ€™s delta compression
with 16-bit precision along with Edgebreaker, the overall
compression ratio turned out approximately 10% of the
original model. We believe that the topology compression
via Edgebreaker has more contributed to the compression.
Deeringâ€™s algorithm uses Huffman code, which takes
advantage of the redundancies among data, and therefore
it is necessary to store the tree information as well in the
file. In the case that there is few redundant data items, the
file size of compressed representation may get even larger
than its uncompressed counterpart.

In our experiment, the distortion is mainly due to the
fact that there are several S nodes in the Edgebreaker
representation of the model and the delta between the
previous point to new point corresponding to S node is
usually much larger than the ordinary deltas between
consecutive triangles coming from C node. Hence,
forcing the exponent of all the deltas identical to the
largest one inevitably cause a significant error. Once an
error occurs, the error propagates to the positions of all
the vertices following in the rest part of the model.

608

W ,

(b)
Fig. 4. The distribution of exponents of vertices for
the cactus model.
(a) all coordinates together, (b) X, Y, and Z
coordinates separated

(b)

Fig. 3. The distribution (of the values of the
exponents of deltas for the cactus model.
(a) all coordinates together, (b) X, Y, and
Zcoordinates separated

Fig. 5 and Fig. 6 illustrate our experiments. Fig. (a), (b),
and (c) are models of 4, 8, and 12-bit precisions,
respectively, for all vertices. Note that the overall
structure of the cactus is already obtained with the 4-bit
precision model. For many applications, the 8-bit model
would be more than sufficient. (Of course, the visual
appearance is model dependent.) In this experiment, there
are four distinct exponents, and we forced the exponents
identical so that the exponent can be transmitted only
once.

An important issue is that thl:re is no guarantee for the
acceptable level of redundancies among deltas. The only
way to4get more redundancies iri deltas is to use fewer bits
for the precision. However, having fewer bits for deltas
increases the possibility of the distortion of the model.
Hence, these two conflicting issues result in a dilemma.
One claim we can make immediately and clearly is the
following. If we transmit the approximated positional data
directly, the errors do not accumulate but distributed over
the whole model. In addition, the control of shape error
can be more manageable in terms of bits of the data to
transmit. Shown in Fig. 4 is the distributions of exponent
values of the vertex positions of the previous cactus
model. Cactus and cat models consist of 620 and 3,545
vertices, respectively. However, there are only four
distinct exponent values for both models, and the
differences between the maximum and the minimum
exponent values are only 3 in both models. Only the
cactus model is illustrated in this paper.

5. Progressive transmission
Refer to Fig, 5 and Fig. 6. After transmitting the initial
model using the first 4-bits in the mantissa, the rest 19 bits
can be further transmitted as a number of stages which
depends on the requirements in the applications. As i t can
be imagined via the explanations up to this point, the
decompression algorithm for mantissa chopping is
extremely simple and therefore fast.

609

W

iri

per triangle. The subcolumn H in the column F shows the
file sizes corresponding to the geometry of model with mbit precision, where m is the number of bit-precision for
deltas in Deeringâ€™s approach. The meaning of the
numbers in the subcolumn in I are a little bit different
from those of subcolumn H. The numbers are the file
sizes of geometry for incremental 4 bits after the previous
level of precision.
For example, the number 930 corresponding to 8 bit
shows the file size of 5 , 6, 7, and 8â€œ bits in the shifted
mantissa for each coordinates of vertices in the model. As
is expected, each incremental file size of the proposed
algorithm is roughly constant. Note that the first chunk
contains a small amount of header. The total file size
including topology file are shown in the column J. It
appears that the proposed algorithm is worse than
Deeringâ€™s in the low precision. However, it should be
noted that the numbers in H are Huffman coded file while
the numbers in I are not at this implementation. We expect
a significant bit saving will be further achieved by
employing Huffman coding in the future. In the proposed
algorithm, user will see the model immediately after
he/she receives the first 1,255 bytes. Then, the display
will be updated when the files in the subcolumn I are
received.
The decompression time is shown in Table 2. While
the decompression time of Deeringâ€™s algorithm increase
as the precision gets higher as shown in the column P, its
counterpart of the proposed algorithm in the column Q is
constant for each increment. The total time consumed in
the decompression for the proposed algorithm does not
exceed that of Deeringâ€™s. However, users see the first
display of the model on the screen only after 31.4
millisecond even for 16 bit model, where Deeringâ€™s
algorithm requires to wait until 74.3 milliseconds. Note
that the time needed for the transmission is not include in
the table in this statistics.

w

Fig. 5. Cactus model via mantissa chopping.
(a) 4-bit precision, (b) 8-bit precision,
(c) 12-bit precision, (d) 16-bit precision,
(e) original model with full 23-bit precision

Fig. 6. Cat model via mantissa chopping.
(a) 4-bit precision, (b) 8-bit precision,
(c) 12-bit precision, (d) 16-bit precision,
(e) original model with full 23-bit precision
Shown in Tables 1 and 2 are some statistics from the
transmission and the decompression of cactus and cat
models. The codes, which are not optimized yet, are
written in Java and the platform is Celeron 333 with 128
MB of memory. In the case of the cactus model, for
example, the compressed topology file, shown in the
column E, takes 309 bytes which calculates exactly 2 bits

Model

Cactus

1,236

620

Table 1 Iile size comparison between Deeringâ€™s and the proposed algorithms

610

time unit millisecond

Table 2. Decompression time comparison between Deering's and the proposed algorithms
Progressive Meshes, In Proc. ACM SIGGRAPH '97, August
1997.
[Huffman 19951 Huffman, D. A., A method for the construction
of minimum redundancy codes, In Proc. Syrnposium on
Theory of Cotnputirig, pp. 703-712, 1995.
[Kim 19991 Kim, Y.-S., Park, D.-G, Jung, H.-Y., and Cho, H.-G,
An Improved TIN Compression Using Delaunay
Triangulation, In Proc. Pacific Graphics '99, pp. 1 18-125,
1999.
[Lee 20001 Lee, E.-S., and KO, H.-S., Vertex Data Compression
For Triangle Meshes, Eurographics 2000, Vol. 19, No. 3, pp.
1 - 10, 2000.
[Rossignac 19991 Rossignac, J., Edgebreaker: Compressing the
incidence graph of triangle meshes, IEEE Trailsactioris on
Visualizarion arid Coinputer Graphics, Vol. 5 ( I ) , pp. 47-61,
January-March, 1999.
[Rossignac and Syzmczak 19991 Rossignac, J., Szymczak, A.,
Wrap&Zip decompression of the connectivity of triangle
meshes compressed with Edgebreaker, Cornpirtational
Geotnetn, Vol. 14, pp. 119-135, 1999.
[Sayood 20001 Sayood, K., Introduction to data compression,
Secorid Edition,Morgan Kaufmann Publishers, San Francisco,
California, 2000.
[Taubin 19981 Taubin, G , Rossignac, J., Geometric
Compression Through Topological Surgery, ACM
Trailsactioris on Graphics, Vol. 17, No. 2, pp. 84-1 15, April
1998.
[Touma 19981 Touma, C., and Gotsman, C., Triangle Mesh
Compression, 111 Proc. Grapliics lriterfnce '98, pp. 26-34,
1998.

6. Conclusions
In this paper, we have presented an approach to
compress and progressively transmit the geometry of a
triangular mesh model. Through an error analysis of the
previous approaches, we claim that transmitting the
position vertices is better than transmitting the deltas
between consecutive vertices. Further bit saving will be
achieved by entropy encoding; the data in the proposed
algorithm.

7. Acknowledgement
The first author was supported by Ministry of
Information & Communication of Korea (Support Project
of Uitiversih Foundation R e w a r c h 2000, supervised by
IITA), and the last author was supported by the Dual
Technology Project by the Kormn Government.

References
[Cheney 19941 Cheney, W., Kincaid, D., Niltrierical
Mutheinntics and Coriiptirig, B rooks/Cole Publishing

Company, 1994.
[Chow 19971 Chow, M. M., Optirnized Geometry Compression
for Real-time Rendering. in Proc. IEEE Visualizatiori '97, pp.

347-354, October, 1997.
[Deering 19951 Deering, M., Geometry Compression, 112 Proc.
SiCCRAPH'95, pp. 13-20, August 1995.
[Forst 20001 Forst, G, Thorup, A., Minimal Huffman Trees,
Actu Itlfortnatica Vol. 36, pp. 72 1-734, 2000.
[Goodrich 20011 Goodrich, &I. T., Tamassia, R., Datu
Structures and Algoritlirns in JAVA, Secorid Edition, John

Wiley & Sons, Inc., 1998
[Gumhold 19981 Gumhold, S., Strasser, W., Real Time
Compression of Triangle Mesh Connectivity, 111 Proc.
SICCRAPH '98, pp. 133-140, 1998.
[Hoppe 19961 Hoppe, H., Prcgressive Meshes, Itz Proc.
SIGGRAPH '96, pp. 99-108, August 1996.
[Hoppe 19971 Hoppe, H., View-Dependent Refinement of

611

