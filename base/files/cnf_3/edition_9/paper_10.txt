A More Realistic Action-Reaction Model for Virtual Environments
Stanislav Pugach, Gordon J. Clapworthy
Department of Computer & Information Sciences, De Montfort University,
Milton Keynes MK7 6HP, United Kingdom
{spugach, gc} @dmu.ac.uk

Abstract
In this paper, a method of increasing the realism of
non-verbal interaction in 3D is proposed. The method is
aimed at creating believable truly autonomous actors in
virtual environments. A simple action-reaction interaction
model with a predefined set of actions is used. The actions
are recognised using weighted-directed graphs (Markov
Models), using a set of control points as features to
reduce the dimensionality of the recognition task, thus
enabling fast and effective recognition. The method
naturally implements characteristics such as reaction
delay and gradual recognition with possible mistakes. The
approach is tested on a simplified boxing scenario.

1. Introduction
One of the most promising applications of computer
animation in general, and character animation in
particular, is the creation of virtual environments.
“Interactiveness” is an important feature of a virtual
environment, but virtual interaction techniques have had
little attention paid to them in the past few years.
A typical virtual environment can contain, apart from
inanimate objects, a number of characters, or actors.
These can be human- or computer-controlled. An actor
normally has a "body" (for example, a human-like
model), which it controls, and through which it interacts
with the environment and other actors. Each actor has a
"brain", in the form of a user interface for humancontrolled actors, or a behaviour algorithm for computercontrolled actors.
The interaction between the inhabitants of a virtual
environment and inanimate objects is straightforward;
modern hardware allows for believable real-time dynamic
simulation. Interaction between actors, on the other hand,
has not progressed far beyond the exchange of verbal
information. Speech recognition was the last major
qualitative advance in interaction techniques, and
removed the need to type commands and statements.

Figure 1. Interacting actors: external view
Non-verbal information, while an important part of reallife interaction, is seldom included in virtual interaction.
To some extent, non-verbal information is regarded as
part of the Human Computer Interaction (HCI) field,
where sometimes the user's gestures are monitored,
resulting in appropriate responses from the computer.
Intricacies of interaction, verbal or not, are usually only
present when some of the participants are immediately
human-controlled, or when the process is observed by the
human user. Detailed interaction between computercontrolled actors (especially "off-screen") is usually
sacrificed for speed; the information is directly exchanged
in the internal format, without translation into words or
actions. At the same time, behaviour-modelling
algorithms are becoming increasingly complex and
intricate, and incorporating more psychological concepts.
It is possible to create a truly independent and selfcontained virtual actor, without the need for "virtual
telepathy".
A more realistic model of interaction, which naturally
allows for lifelike behaviour of participants, is proposed
in this paper. The model is based on recognition, which is
done in near real time and concurrently with the action.
This allows for concepts such as reaction delay and
correction of mistakes to be implemented. The approach

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

is tested on a restricted boxing scenario with two virtual
humans, one of which is charged with the task of reacting
to (that is, dodging) the other's actions (punches).
The method has been designed to be immediately
expandable to more complex scenarios involving
interacting figures, such as ballroom dancing or any other
scenario that involves a predefined limited choice of set
actions.
The algorithm uses weighted-directed state graphs to
store and match the patterns. A method of reducing the
dimensionality, based on control points, is also proposed.
The control points are attached to the links of the model
and serve as features for the recognition task.

2. Related Work
There are few publications addressing general nonverbal interaction (not limited to recognition of the user’s
gestures) in both the Human Computer Interaction and
Character Animation fields.

Character Animation
In the field of Character Animation, the closest thing to
interaction is pre-scripted (or keyframed) sequences, as
the stress is put on creating single-actor motions of higher
quality. Some examples of interaction can be found in
research related to motion warping.
In Arikan & Forsyth [1], a multi-character scenario
includes figures tackling each other, which is
implemented using hard constraints for a method using
motion examples to create new motion sequences. In this
case, the hard constraints force the characters to perform
particular actions and are similar to keyframes in effect.
Gleicher [3] used two interacting figures, a dancing
couple, to test a motion-retargeting algorithm. One of the
figures is gradually resized, while the other needs to
maintain the outlook of the motion by maintaining
contact. While not immediately relevant, this example
addresses the issue of contact during interaction.

Human Computer Interaction
Normally, however, non-verbal interaction is addressed
as part of HCI research, and only in scenarios with direct
human involvement, as part of different user interfaces.
One of the main goals of HCI research is making
interfaces more natural. In pursuit of this, the control
means progressed from a keyboard to mice and light pens
to VR helmets and gloves, and speech and gesture
recognition. Two of the major development directions in
HCI are intelligent environments (IE) and virtual reality
(VR).
Intelligent environments are systems providing
multimodal interfaces for control over some appliances;
for example, a robot arm controlled by hand gestures in

Wachs et al. [12]. One of the largest projects in intelligent
environments is the Intelligent Room project at MIT
Artificial Intelligence Labs (publications by Coen [2],
Kulkarni [6]), which allows voice and pointer control
over a variety of devices, such as projectors, and a
multitude of computer functions.
Virtual Reality puts its main emphasis on placing the
user in an artificial environment, in which he is
represented by some character, an avatar. An immersive
VR system typically includes a head-mounted display
and, optionally, a glove. One of the main problems
particular to VR is avatar control. Various techniques are
used to map the available control devices on to the
avatar's behaviour range (Mine [7], Su & Furuta [11]).
One of the prospective applications of VR is providing
immersive interfaces, similar in function to websites, with
the system designed by Nijholt & Hustlijn [9] being a
good example.
In both cases (IE and VR), the interaction is controloriented, and most recognised actions by the user are
either instructions, or serve for object manipulation. For
that purpose, speech (written or spoken) can be used, but
natural language is often ambiguous, as is shown in
Karlgren et al. [5]. More often, gestures are used as a
means of interaction and part of multimodal interfaces. In
most cases, such interaction is limited to hand gestures
(for example, in Huang & Pavlovic [4]). The gesture
analysis may involve complex hand posture
reconstruction algorithms, or be image-based. Examples
of the latter include Wu & Huang [13], which describes a
semi-supervised learning system using an expectationmaximisation approach to partition a base of unlabeled
examples, and Shamaie & Sutherland [10], which uses
eigenvector representation of the images and a bipartite
graph-matching algorithm for recognition.

3. System Overview
The scenario for the project has been chosen to be
realistic while being as simple as possible. The interaction
model includes two human figures. One of the figures is
the leader, carrying out actions (different types of
punches). The other is the follower, reacting to these
actions (by dodging or blocking the punches).
The leader performs a sequence of moves, which is
specified by the user. The moves are taken from the move
library, where they are stored in the form of fixed-form
animations (in BVH motion-capture format). The
follower has no knowledge of the sequence, and needs to
react to the leader's moves correctly. For that, the move
library contains response motions for each of the leader's
moves. It is also assumed that the follower knows the
lengths of the leader's moves.

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

The recognition is based on the posture data, a first step
to true vision-based interaction. The follower does not
have to reconstruct the leader's pose from a video feed,
and is provided with the data about the positions of the
leader's body parts. This approach is a compromise
between the usual omniscience of computer-controlled
virtual actors and computer vision, and allows the project
to concentrate on interaction. This layout is closely
related to marked motion-capture input (available, for
example, from electro-magnetic motion-capture systems).
If the leader were to execute the moves exactly as they
are stored in the move library, the recognition task would
be trivial. However, the complete movement is composed
of a succession of movement segments from the library,
and for the resulting motion to be smooth, the moves
would need to be interpolated at the boundaries. This
results in distortions at the start and the end of each
elementary motion. Such distorted motion cannot be
reliably recognised by direct comparison with the library
patterns. In a real-life scenario the motion itself would be
slightly different each time.
One of the traditional approaches to recognition of
variable data is by using statistical methods. The most
widespread examples of such are the Hidden Markov
Models, used in speech and handwriting recognition.
For this project, a simpler system was used. It is still a
Markov Model, but not a "hidden" one. This means that
each state has only one value assigned to it, making the
model a first-order stochastic system (Hidden Markov
Models are second-order stochastic systems). The main
advantages are simplicity and a considerably lower
computational cost, both in recognition and in training
(which is an expensive operation for an HMM).
The data structure used is a weighted-directed graph, a
state transition graph with the nodes representing states of
the system (and containing state-description information),
and the probabilities of state transitions assigned as
weights to the corresponding arcs.
Of course, using the complete set of "perceived" limb
positions as the input vector would make the recognition a
challenging task. For the model used, the complete set
would consist of 126 float values (21 links with 6 values
per link). A better way is to take the DOF vector of the
model, which is 51 elements in size, but this is still too
long (even with non-articulated hands). Some sort of a
feature extraction scheme is required to reduce the length
of the input vector to a manageable size.
The algorithm employs a distinct property of the chosen
scenario, which is the critical importance of the arm
motions over the rest of the body. In most cases,
particular body parts will be of greater significance for the
interaction than the others, though generally the lower
limbs are more critical. In a dancing scenario, for
example, the positions of the feet would carry the most
information.

1)
2)

3)

Figure 2. Graph evolution during training.
(Cycles are shown expanded)
To isolate the vital data, control points are used. A
control point is a "mark", attached to one of the leader's
links. Each control point is defined by the following :
 the link of the leader to which the control point is
attached;
 the relative position of the control point on the link
(in the form of a transformation matrix).
The positions of all control points form the input
vector. In the current scenario, two control points are
used, placed on the leader's hands. Control-point positions
are used, so the resulting input vector consists of only six
float values.
The graph patterns are trained on a number of marked
sequences (the moves are known - a form of supervised
learning). The graphs for each move are filled with
various ways of performing the move. At the end of the
training, each graph contains all variations encountered in
the training data. In the chosen scenario, since only the
start and the end of each move are altered, the graphs tend
to have a structure similar to Figure 2.
At the start of each of the leader's moves, the follower
enters the recognition mode. On each frame of
recognition, the distances between the actual control-point
trajectory and each pattern graph are calculated:
n

d

¦ e in 2  Pi d i ,
i 0

where n is the length of the length of the sequence
(reaction delay), Pi is the probability of transition from
the previous state, and
2
d i ¦ a ij  s ij 
j

is the Euclidean distance from the i-th element of the
sequence to the closest node in the graph (aij is the j-th
element of the input vector, sij is the j-th element of the
i-n
best matching node from the pattern graph). The e term
gives greater weight to the more recent values.
The closest match is additionally tested to see if the
distance is within the (arbitrarily chosen) threshold. This
gives room for "reasonable doubt", if the alteration by the
interpolation builder is strong enough. If even the closest
result is "too far", the process is repeated on the next
frame.

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

1

2

3

4

5

6

…

10

Figure 3. Reaction delay example: the top row shows the original motion, with no delay; the bottom row shows
the effect of a reaction delay of 4 frames – the reaction starts later, and catches up by frame 10
If the match is close enough, a decision is made; the
recognition is paused to be restarted for the next move in
the sequence. No provision is made for incorrect
decisions, although this is unreasonable in a fight
scenario, where an incorrect recognition is a valid
outcome (a missed punch) and even desirable for one of
the figures involved.
This organisation of recognition process (gradual
advancement into the recognised move) corresponds to
the real-life situation, where the follower tries to establish
the next move as the leader performs the start of it,
invariably lagging slightly behind because of the time
required for the recognition.
The time taken to recognise the move (the number of
times the trajectory was extended) is stored and used later
in building the follower's part of the animation. The
follower's responses are chained to coincide with the
leader's actions, but the start of each response is timewarped, so that the motion starts only after the recognition
delay. In effect, the follower waits for the move to be
recognised, then hurries into the response, then catches up
with the leader, so that the action and the reaction end at
the same time. After that, the cycle is repeated for the
next move.

4. Results
The method was tested using a human figure plug-in
for 3D Studio Max. The library contained six different
punches, out of which a sequence of any length could be
assembled. The junctions between consequent moves in
the sequence were interpolated (over 10 frames in both
directions) using a style-preserving algorithm. In addition,
the recognition delay was capped at 10 frames (for
external purposes), after that, the correct decision was to
be forced. This in no way restricted the method, since the
recognition is almost guaranteed after the interpolated
start of the move, if the library moves are sufficiently

different from each other at that moment, as the distance
from the correct pattern graph will rapidly approach zero.
The method proved to be effective and reliable. Over
many experiments, the delay cap has never actually been
reached, and the recognition typically got to the correct
result after only 3 or 4 frames. The process (with 6 library
patterns) took negligible amounts of time (less than 1 ms).
This would allow the use of the method at interactive
rates, or even applying it continuously. The recognition
time is, of course, proportional to the size of the pattern
library, but that is true for the absolute majority of the
recognition methods now.

5. Evaluation
Not all intricacies of the nonverbal virtual interaction,
of course, could be considered at once. Instead, an attempt
has been made to test the suggested interaction
mechanism in a favourable case of leader and follower
going through a sequence of actions (out of a predefined
set).
In this simplified interaction model, the main problem
is recognition, which needs to be effective. This project
employs a widely used recognition method through
weighted-directed graphs, and also introduces a novel
concept of control points. This approach allows the
dimensionality of the recognition task to be reduced
considerably. Another advantage is that it ensures that
only important information is considered.
The sets of control points would, of course, be different
for every application. A careful choice of control points
may aid considerably in recognition. As a control point
can be positioned anywhere relative to the link to which it
is attached, some important motion properties can be
accentuated by placing a control point at some distance,
where it would perform greatly exaggerated motions, thus
making small motions of some links more distinctive.

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

The idea of control points allows the system to
"concentrate" on essential motion components, which
would aid in virtually any area involving recognition. The
control points come at a very low computational cost (one
additional matrix multiplication), which would, in most
cases, pay off. However, human intervention would
probably be required to select the control points for a
particular application.
Another shortcoming of the control-points approach is the
fact that the position of the link is either taken into
account (via a control point) or ignored completely,
allowing no intermediate choice. Some generalisation of
the approach (for example, by assigning importance
weights to the control points) might help in that.

6. Conclusion
The interaction model used in the project is very basic
and could be enhanced in a number of ways. The obvious
steps would be to make the interaction continuous and bidirectional. Further research could explore interaction
models with more than two participants. The method
suggested in the project is efficient and relatively cheap
computationally, allowing such enhancements. However,
the action-reaction model would not be sufficient for a bidirectional case, as it does not provide for concurrent or
interrupting actions. To deal with these, some protocol
needs to be developed.

[8] Myers, B.A., "A Brief History of Human Computer
Interaction Technology", ACM Interactions, 1998, 5(2), pp.
44-54
[9] Nijholt, A., Hulstijn, J., "Multimodal Interactions with
Agents in Virtual Worlds", In: Kasabov, N. (ed.), “Future
Directions for Intelligent Information Systems and
Information Science”, Physica-Verlag: Studies in Fuzziness
and Soft Computing, 2000, retrieved from:
www.cs.vu.nl/~joris/Articles/dunedin.ps
[10] Shamaie, A., Sutherland, A., "Graph-based Matching of
Occluded Hand Gestures", IEEE Applied Imagery Pattern
Recognition Workshop, 2001, retrieved from:
http://www.computing.dcu.ie/~ashamaie/shamaiea_graph.ps

[11] Su, S.A., Furuta, R., “A Specification of 3D Manipulations
in Virtual Environments”, Proceedings of the 4th
International Symposium on Measurement and Control in
Robotics, 1994, NASA Conference Publication 10163, pp.
64-68
[12] Wachs, J., Kartoun, U., Stern, H., Edan, Y., “Real-Time
Hand Gesture Telerobotic System”, Proceedings of WAC
2002, retrieved from:
http://www.ie.bgu.ac.il/juantesa/tele_gest/real_time_gesture.pdf

[13] Wu, Y., Huang, T.S., “View-independent recognition of
hand postures”, In CVPR, volume 2, 2000, pp. 88-94

References
[1] Arikan, O., Forsyth, D.A., "Interactive Motion Generation
from Examples", Proceedings of SIGGRAPH'2002, pp.
483-490
[2] Coen, M.H., “Design Principles for Intelligent
Environments”, In Intelligent Environments, Papers from
the 1998 AAAI Spring Symposium, Technical Report SS-9892, AAAI Press, 1998, pp. 37-43
[3] Gleicher, M., “Retargetting Motion to New Characters”,
Proceedings of SIGGRAPH ’98, pp. 33-42
[4] Huang, T.S., Pavlovic, V. I., "Hand gesture modeling,
analysis, and synthesis", Proceedings of International
Workshop on Automatic Face- and Gesture- Recognition,
1995, pp. 73-79
[5] Karlgren, J., Bretan, I., Frost, N., Jonsson, L., "Interaction
Models, Reference, and Interactivity in Speech Interfaces
to Virtual Environments", Proceedings of Eurographics'95,
retrieved from:
http://www.sics.se/~jussi/imagina.ps
[6] Kulkarni, A., “A Reactive Behavioral System for the
Intelligent Room”, Master's thesis, Massachusetts Institute
of Technology, Cambridge, MA, 2002, retrieved from:
http://www.ai.mit.edu/people/kulkarni/
[7] Mine, M., "Virtual Environment Interaction Techniques",
UNC Chapel Hill Computer Science Technical Report
TR95-018, 1995, retrieved from:
ftp.cs.unc.edu/pub/technical-reports/95-018.ps.Z

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

