Semi-Automatic Image Annotation Using Frequent Keyword Mining
A. Dorado and E. Izquierdo
Queen Mary, University of London
Electronic Engineering Department
{andres.dorado,ebroul.izquierdo}.elec.qmul.ac.uk
Abstract
Research in Content-Based Image Retrieval is an
expanding discipline with an accelerated growing in the
last ten years. Advances in telecommunications and the
huge demand of visual information on Internet and
mobile devices is occupying the attention of the
researchers in developing efficient systems to ease the
task of useful visual information retrieval by the users.
This work presents a semi-automatic image annotation
process using the low-level image descriptor Fuzzy Color
Signature to extract the most similar images from an
annotated database and frequent pattern mining to select
the candidates keywords for annotating the new image.
The idea is aimed at establishing a bridge between visual
data and their interpretation using a weak semantic
approach.

1. Introduction
In response to the huge demand of applications for
managing visual information, several content based image
and video retrieval systems have been developed.
Typically, these systems provide automatic extraction of
most of perceptual information such as color, texture,
shape, structure, and spatial relationship. The support for
low-level information processing has the advantage that
the applications can be domain independent. On the other
hand, some systems managing high-level information
such as semantic primitives and related semantic
information have been developed [1]. These systems have
some limitations such as domain-specific application and
end user's dependency.
CBIR systems combine query by visual example and
text to overcome the difficult task of describing
perceptual features. The use of keywords is a first step
towards high-level concepts management. Keywords are
useful for describing semantic image content. Although,
keywords are content-dependent, they are appropriate to
express descriptive metadata.
In [2] an image classifier for propagating keywordbased annotations is used. Each image is classified
against a set of predefined categories. In contrast, the
presented approach does not require a pre-defined lexicon
allowing addition of new labels whether a new frequent

keyword is found. In [3][4] image search, relevance
feedback, and image annotation are combined. A semiautomatic annotation strategy links images to keywords
based on relevant feedback provided by the user. The
strategy presented in this paper propagates annotations
without user feedback. However, the user can improve
the results adjusting accuracies and either add or remove
labels. Two similar approaches to identify the underlying
links between the low-level features and the high-level
concepts associated with images are introduced in [5],
where the method utilizes classification trees and k-means
clustering algorithms. In the presented approach, the
clustering and decision tree techniques are not required
and the accuracy attached to each keyword is introduced.
In this work, a semi-automatic image annotation
process is presented. The first set of images is manually
labelled. Candidate keywords for annotating a new image
are extracted from its most similar images after a frequent
pattern mining process. Similarity is based on a low-level
image descriptor called Fuzzy Color Signature (FCS)
using Earth Mover’s Distance as Metric. The organization
of this paper is as follows. In Section 2 the general image
annotation process is introduced. In Section 3 the FCS
image descriptor is presented. In Section 4 the proposed
supervised image annotation approach is described.
Section 5 present some experimental results and Section 6
concludes the paper.

2. Image Annotation Process
The image annotation is an interactive process to
assign a set of labels to an image. In content-based image
retrieval, interaction is a complex interplay among the
user, the images, and their semantic interpretations [6]. In
order to facilitate the description of the processes, the
notation proposed by Smeulders et al [6] is used.
The image annotation process denoted by L can be
summarized as
^U , ix ` L ^ia x `
where U is a non annotated image space defined as a
tuple ^I , Z ` , where I is a set of images, Z is a set of
labels, and i x   I is a selected image to be annotated.
The process L begins with the creation of an instance of
the abstract non annotated image space U ^I , Z ` .

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

Afterwards, the annotation operator L  L maps U into
the annotated images space A in an interactive
annotation session, which is a sequence of annotation
spaces U 0 ,U 1, ,U n 1,U n with U n ix  L§¨U n ·¸ .
¹
©
The result is an annotated image
ia x  : i x 
ki : ai 0m1
where k  Z are keywords labeling the image i x  and
a  0,100@ are the accuracies assigned to each keyword
by the annotator.

^

`

^

`

3. Fuzzy Color Signature Descriptor
The Fuzzy Color Signature (FCS) is a compact
descriptor with the most representative colors of the
image [7]. The process can be summarized as
^ix ` FCS F j x  where ix   I is an image and F j x 
is the FCS of i x  .
The FCS generation process maps the original image
colors into a small number of representative colors using
a peak detection function derived from the color
distribution. The following is a detailed description of this
process.
The Red, Green, and Blue color values (RGB) are
extracted from image i x  using function f x  g ix  ,
where f x  Rx , G x , Bx  .
RGB color values of i x  are mapped from RGB color
space into HSL color space by an operator t .
t
f x  
o f cx   t  T : f cx  t f x  ,
where f cx  H x , S x , Lx  .
HSL color values of i x  are quantized applying an
operator t c
tc
f cx  o f ccx   t c  T : f ccx  t c f cx 
where f ccx  H cx , S cx , Lcx  .
Finally, using a composition of functions h , the FCS
image descriptor F j x  ¦ R h f ccx  is generated.
j

^

¦Rj

The

average cumulative distribution function
i
h1i  ¦ jW
i W h0  j  2W  1 were W is the size of the
window to select the range of values for the average
calculation.
The peak detection function h2 i  h0 i   h1i  ,
which uses the zero-crossing points to detect the peak
values.
Once the peak positions are detected, the fuzzy
mapping function applies wi h3 i  as a clustering
method to accumulate the frequencies in the range of the
peak to generate the FCS, where wi ¦i PP j  f  j  ,

PP  j  e

 m i  j 'Pi

, and m t 1 is the membership
function. In Figure 1 the changes of the sample image
#139050 throughout the image descriptor generation are
shown.

`

indicates the aggregation operation on the whole

image, F j is a FCS of the form ^xi , wi `0n 1 , xi is a
dominant color of i x  , and wi is the rough frequency of
pixels with xi color. h consists of the following four
functions:
The
cumulative
distribution
i
function h0 i  ¦ j 1 f  j 
where f  j  is the frequency of the j -th bin of H cx  .

(a) RGB values

(b) Quantized

(c) Using FCS

FCS={ (127, 818) (140, 590) (180, 540) (239, 12) (280, 3265) }

Figure 1. Color descriptor for image #139050

4. A Supervised Image Annotation Approach
In Section 2 a manual process for image annotation
was presented. This process has several disadvantages
such as time consuming, user dependency, and difficult to
realize on large image databases. Hence, an automatic
process is desired. However, the semantic gap makes
necessary to keep the user supervision to adjust the results
and improve the accuracy of the process. This work is
addressed towards a supervised or semi-automatic process
for image annotation.
The semi-automatic image annotation process denoted
by L can be summarized as ^Q, i x ` L ^ia x ` , where Q
is a query space defined as a 4-tuple IQ , FQ , SQ , ZQ ,

^

`

IQ is a selection of images from the large image archive
I , FQ  F is a selection of the features derived from
the images in IQ , SQ is the similarity function selected,
and ZQ  Z is a set of keywords to capture goal-

dependent semantics, i x   I is a selected image to be

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

annotated, and ia x  represents the image i x  with
annotations.
The process L belongs to the approximate query
category. Therefore, the result is a ranking Ri x  with
the k most similar images in IQ with respect to i x 
(hereinafter, q ), based on SQ applied on the FCS values.
s§¨ F q , F d ·¸ , is
¹
©
q
calculated using Earth Mover's Distance [8]. F is the
FCS of the query image q and F d is the FCS of d

The similarity between FCS, Sq,d

representing an element of the data set of images. The
visualization operator V maps the query space Q into
the display space D . The role of V is limited to
bounding the number of image display. Rq  V q  .
­
½
§ q d·
®di qRdi  s¨ F , F ¸ d H ; i : 1 k ¾
©
¹
¯
¿
Using the similarity metric, V selects K images
satisfying Sq,d d H , where K and H are user-specified

> @

P  min Z Q ; that is, is no proper subset of P is in the

same equivalence class.
The frequent keywords identified by L are key
patterns for annotating image q . In the next step, L
calculates the accuracy for each keyword. These
accuracies are calculated by using a weighted average
aij ¦ a j S q, j ¦ S q, j where aij is the value of
accuracy of the i -th frequent keyword or the minsup
value whether the first iteration of the annotation process,
0 d j d K represents the j -th ranked image in Rq  and
S q, j is the distance between the j -th image and the
query image q .
Finally, the result of L is a set of tuples of the form
k : a  annotating image q . The annotator can adjust the
accuracies and either eliminate or add keywords.

V q 

constraints. The images in Rq  are organized in
ascending order by distance.
Once the set of most similar images is selected, L
identifies the most frequent keywords associated to these
images. To do this, a Frequent Pattern Mining task is
used. This task consists in determining all frequent
patterns together with their supports for a given threshold
minsup. It can be formally described as follows [9]:
Let ZQ be a finite set of labels, I Q a finite set of
images and R  I Q u Z Q a binary relation, where

i, z   R

may be read as “label z annotates image i ”.
Each subset P of ZQ is called a pattern. It is said that a
pattern P annotates an image i  IQ if i, k   R k  P .
Let f P 

^i  I Q

i

5. Experimental Results
A set of 2K non annotated images were taken from the
CorelDraw image CDs. A set of 371 manually annotated
images were downloaded from the www.freefoto.com
site. Theses annotations correspond to the headings
grouping photographs by category. A domain for the
minsup constraint was defined using a ' 0.01 within
the interval >0,1@ .
The set of keywords Z Q extracted from these images
is /Clouds/, /Clouds-abstract/, /Mountains/, /Sky/, /Sunset/
Figure 2 presents a ranked set of similar images to the
non annotated image #139050 (Figure 1). The image ID
along with the resulting EMD is shown.

`

P be a function which assigns to

each pattern P  Z Q the set of all images that annotates

#15-63-78
EMD=3.27

#15-63-20
EMD=4.75

#15-61-3
EMD=6.74

this pattern. The support of a pattern P is given by:
supP  cardP  card I Q . A pattern P is called frequent

 

if supP  t minsup , where minsup  >0,1@ .
A D -pattern P is a subset of Z such that
cardP  D . A candidate D -pattern is a D -pattern where
all its proper sub-patterns are frequent. Given two
patterns P, O  Z Q , let PT O if and only if
f P  f O  . The set of patterns which are equivalent to
a pattern P is given by >P @ O  Z Q | PT O . In the

^

`

case of patterns P and O with PT O , both patterns have
the same support. A pattern P is a key pattern if

#15-63-34
EMD=7.88

#15-63-2
EMD=8.13

#15-47-55
EMD=8.35

#15-63-25
EMD=8.42

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

#15-63-40
EMD=8.22

#15-63-26
EMD=8.42

Figure 2. Similar images to #139050

Table 1 lists the frequent keywords found for image
#139050 using the method described in Section 4.
Table 1. Frequent Patterns V(139050)
Pattern
/Clouds/
/Clouds-abstract/
/Mountains/
/Sunset/
/Clouds/, /Clouds-abstract/
/Mountains/, /Sunset/

Support
0.11
0.11
0.88
0.77
0.11
0.77

After the frequent pattern mining process, the
following list of keywords and their corresponding
accuracies for annotating the image #139050 was found:
{(/Mountains/:0.88), (/Sunset/:0.77)}
Figure 3 and Figure 4 show the query image #15033
and its ranked set of similar images respectively.

Figure 3. Query image #15033

#15-47-18
EMD=4.45

#15-03-16
EMD=5.01

#15-62-65
EMD=5.44

#15-63-81
EMD=5.77

#15-03-54
EMD=5.89

#15-45-24
EMD=6.12

#15-03-29
EMD=6.46

#15-62-13
EMD=6.73

#15-62-17
EMD=7.37

Figure 4. Similar images to #15033
Table 2 lists the frequent keywords found for image
#15033 using the method described in Section 4.
Table 2. Frequent Patterns V(15033)
Pattern
/Clouds/
/Mountains/
/Sky/
/Sunset/
/Clouds/, /Sky/
/Mountains/, /Sunset/

Support
0.44
0.55
0.44
0.11
0.44
0.11

The result of the annotation process for image #15033
is: {(/Clouds/:0.44), (/Mountains/:0.55), (/Sky/:0.44)}

6. Conclusions and Further Work
The presented approach takes advantages of the
frequent keyword mining task for propagating labels on
large images databases. The proposed semi-automatic
image annotation process is useful for improving the
precision of the query results in CBIR Systems. In
addition, it facilitates the interaction between the user and
the system. The accuracy attached to each keyword can
be used for further work on user feedback.

7. References
[1] A. Del Bimbo, Visual Information Retrieval,
Academic Press, Morgan Kaufmann Publishers, 1999.
[2] G. Sychay, E. Chang, and K. Goh, “Effective image
annotation via active learning”, In Proc. IEEE
International Conference on Multimedia and Expo ICME,
Piscataway, NJ, USA, 2002, vol. 1 pp. 209-12.
[3] Z. Zhu, L. Wenyin, H. Zhang, and L. Wu, “An image
retrieval and semi-automatic annotation scheme for large
image databases on the Web”, In Proc. International
Society for Optical Engineering SPIE, USA, 2001, vol.
4311, pp. 168-77.
[4] L. Wenyin, S. Dumais, Y. Sun, H. Zhang, M.
Czerwinski,
B.
Field,
“Semi-automatic
image
annotation”, In Proc. International Conference on
Human-Computer Interaction INTERACT’01, IFIP TC.
13, IOS Press, Amsterdam, Netherlands, 2001, pp. 32633..
[5] I.K. Sethi, I.L. Coman, and D.Stan, “Mining
association rules between low-level features and highlevel concepts”, In Proc. International Society for Optical
Engineering, SPIE, USA, 2001, vol. 4384, pp. 279-90.
[6] A. W. M. Smeulders, M. Worring, S. Santini, A.
Gupta, and R. Jain, “Content-Based Image Retrieval at
the End of the Early Years”, IEEE Trans. on PAMI, Dec.
2000, 22(12): 1349-80.
[7] E. Izquierdo and A. Dorado. “Fuzzy Color
Signatures”. In Proc. of the IEEE Int. Conf. on Image
Processing, ICIP, Rochester, NY. USA, 2002, vol. 1, pp.
433-36.
[8] Y. Rubner, C. Tomasi, and L.J. Guibas. “A metric for
distributions with applications to image databases”. In
Proc. IEEE Int. Conf. on Computer Vision, Bombay,
India, Jan. 1998, pp. 59-66.
[9] Y. Bastide. R. Taouil. N. Pasquier, G. Stumme. and L.
Lakhal, “Mining Frequent Patterns with Counting
Inference”, ACM SIGKDD Explorations, Dec. 2000, vol.
2 issue 2 pp. 66 – 75.

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

