A New Data Distribution Method for Parallel Ray Tracing
Mohamed Amri
University of Science and Technology Houari Boumedienne-Algiers
BP- SONARCO- Information System Department-Hassi Messaoud -ALGERIA
E-mails: amri_m_dz@yahoo.fr; amrim@algreb.bp.com
Mohamed Amri, magister degree in computer graphics
science, computer graphics researcher and software
developer.

Abstract
The Ray Tracing algorithm produces realistic high
quality images, but it requires a long time of calculation
on a single processor machine, which limits its practical
use. With the development of distributed objects
architectures, such as CORBA (Common Object Request
Broker), the most promising way to improve ray traced
pictures productions seems to be parallelisation which
offers both increased CPU power and memory facilities.
A natural way of parallelisation is to distribute pixels
over the CORBA objects system. However, since we want
to deal with large scenes and making our algorithm
functional over any parallel architecture system, scene
objects have also to be distributed among processors, so
a modified parallel algorithm is necessary. We propose a
new method, which distributes the scene objects among
processors (CORBA objects) according to their speed
frequencies, which uses the object coherency property.
Our approach of exploiting the bus CORBA gives very
encouraging results.

regardless of the increased CPU and memories. In this
paper, we proposed a new method of data distribution of a
CSG (Constructive Solid Geometry) modelled scenes (see
[3] for more information about CSG) and also, our
algorithm of dataflow exchanging during the load
balancing process.

2. Property of the object coherency
Between two neighbour pixels, primary ray directions
passing by these pixels are a little different [8].
Consequently, secondary rays will have therefore
tendency to take the same paths. Since the coherence is
the fact that two rays passing by neighbouring pixels have
a great probability of intersecting the same objects, the
time processing of a pixel is then a good estimator of the
processing time of neighbouring pixels.
Secondary Rays
Luminous Source
Object

Primary rays

1. Introduction
The ray-tracing algorithm is the most powerful
technique for realistic image computation. The main
interest of this method lies in the fact that a sophisticated
illumination model can take into account specular
reflection transparency and shadow effects (see [7]). Ray
tracing simulates the operation of a camera, following
light rays in a reverse order. In fact, the essential work
consists in evaluating ray object intersections. A major
drawback of this model is its high computational cost
which limits its practical use, although numerous
acceleration schemes (see [9]). An efficient solution is to
profit from the power of several computers connected by
a local area network; the most promising way to decrease
the network traffic is to use architectures of distributed
objects, which offers messages exchange facilities,

Shadow rays

Screen

Observer
Figure1. Property of the object coherency.

3. CORBA
CORBA (Common Object Request Broker) is network
architecture of distributed objects (see [4]). Interactions
between applications are materialized by remote
invocation of methods of objects. The notion client/server
intervenes solely during the use of an object.

1
Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

Client
Application
Bus CORBA

Implementation
Code

Representation) and a totality of messages of request
transportation to objects (Request, Reply). However,
GIOP is only a generic protocol, IIOP provides an
implantation of GIOP to the over TCP/IP and therefore
Internet.

4. Overview of our approach

Object
reference

Request

Object
CORBA

Object
State
Activation

Object
Interface

Server
Application

Figure 2. The Object model Client/Server.
The above figure presents the different notions
intervening in this object model client/server:
-The bus CORBA is the intermediary/negotiator through
which objects are likely to converse.
-The application client is a program that invokes methods
of objects through the bus CORBA.
-The reference of object is a structure designating the
object CORBA and containing the necessary information
to localize it on the bus.
-The request is the mechanism of invocation of an
operation or access to an attribute of the object.
-The implantation of the object is the entity coding the
object CORBA to a given instant and managing a state of
the object.
-The bus CORBA forwards requests of the client
application to the object CORBA by masking all
problems of heterogeneity (languages, operating systems,
equipment, and network). It offers an oriented object and
client/server environment. It provides a Naming Service,
which allows the client to possess references of
SERVERS according to symbolic names associated to
them.
-Requests to objects seem always to be local; the bus
CORBA forwards them by using the most appreciated
canal of communication!
-Objects are in memory solely if clients use them.
All buses to the norm CORBA have to provide
protocols GIOP (General Inter ORB Protocol) and IIOP
(Internet Inter ORB Protocol). The protocol GIOP defines
a common representation of data (CDR or Common Data

Our distributed ray tracer system on the bus CORBA
consists in three object types: the modeller object, the
tracer object and the displayer object:
The modeller object: overlaps methods of CSG
modelling scenes, divides the 3D scene objects into N
disjoints groups, and assign each group to a different PC
for rendering (see section 5), that is the CSG tree is
restricted on each group (see [9]).
The tracer object: Contains the rendering methods
operating upon a given ray and a CSG object, the tracers
objects are running over the servers network.
The displayer object: running upon the displayer PC and
overlapping the image printer methods.
Our system consists in a modeller object running over a
master PC (highest memory size) and N tracers objects
running upon N servers PCs; Proceeds in three pipeline
phases as shown in figure 4:
Pre-processing phase: In this phase the client (modeller
object) models the scene, divides the 3D scene objects
into N disjoints groups, and assign each group to a
different server PC (tracer object) for rendering. The
whole CSG scene is restricted on each group.
Local rendering phase: In the second phase, every
server (tracer object) renders its scene region. Unlike
Scott and al system’s [6], there is no peer-to-peer
redistribution of pixels among servers.
Composition phase: In the third phase, the displayer PC,
receives sub images from all servers and composites
them. The final task of the displayer PC is to output the
final image to the frame buffer or disk.

5. Data Subdivision
A natural way of the ray tracing parallelisation is to
distribute pixels over the CORBA machines. We actually
want to deal with very large scenes, and make our
algorithm functional over any parallel system
architectures. Hence, scene objects have also to be
distributed among processors. So, a modified parallel
algorithm is necessary.
Unlike Bouatouch and al data distribution method’s [8],
that uses a uniform subdivision of the CSG modelled
scene, our algorithm uses an irregular one: it makes at a
first step an irregular subdivision of the scene similar to
Priol and al method’s [9]: The minimal bounding boxes
of the CSG primitives (see [9]), are projected on the

2
Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

SCREEN. This projection generates a set of rectangles.
Each rectangle is then decomposed into its four segments,
used to perform a binary space partitioning. The way to
carry out the BSP (Binary Space Partitioning) is to choose
one segment among the available segments list. The line
containing this segment separates the plane into two
regions. According to the segments location, the segments
are broken into two other lists, each one is associated to a
region. The line extension of the chosen segment may
intersect other segments. In this case, they are split and
put in the two new lists. The same process is applied
recursively to these regions, until the associated lists
become empty. Terminal boxes, containing or not a
subset of primitives, are then created (figure3).
Original scene

Our approach of data distribution, allows each processor
to be assigned an amount of load depending on its speed
frequency. It works according to the following scheme:
1- Estimating the execution time for each non-empty
terminal box projection: for each pixels sub group of the
non-empty terminal box projection on the screen, that the
dimension is less than a given size (8u8 in our
implementation), the central pixel synthesis time is
evaluated, which is therefore affected to the whole sub
group of pixels: this is due in fact, to the object coherency
(see section 2). Hence, we obtain the approximate
execution time for the whole non-empty terminal box
projection (figure 3). What follows of this paper, each
non-empty terminal box projection is called sub-region.
2- The sub regions thus obtained, are ordered by their
estimated execution time: B1, B2…Bn
3- Each processor Pl, receives a macro-region Rk, which
is the association of the set of the ordered sub-regions Bj,
such as the sum of their execution time (¦TBj) verifies
the following inequality

n

¦TBj d ¦ TBi u
i=1

Speedl
p

¦Speedm
m=1

Where
TBi is the estimated execution time for the sub region Bi
n is the number of the sub-regions resulting from the
subdivision process.

B1

n

¦ TBi Represents the estimated execution time for the

i=1

B2

whole scene.
Speedl : the speed of the lth processor.
P: the available processors number over the distributed
system.
As an example the scene of figure3, the sub regions
estimated execution time are mentioned in Table1; On the
three processors system, having the respective speed
frequencies 100 MHZ, 200 MHZ, 300 MHZ; each
processor should be assigned the loads mentioned in
Table2 and illustrated in figure4.

B5
B3

B4

B10
B6

B11
B14

B7
B8

B9

B12

B13

Figure 3. Example of an irregular subdivision
of the scene into sub-regions.

3
Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

Table1. Example of Estimated execution time for
Figure3 sub regions
Sub region
B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14

Estimated execution time
3
15
4
3
2
6
8
7
4
9
2
10
6
9

6. Load balancing

Table2. Example of loads distribution over three
processors system having different speed based
upon the sub regions estimated execution time
mentioned in Table1
Processors
Associated subregions

P1
P2
P3
B5, B11, B6, B13, B10, B14,
B1, B4, B8, B7
B12, B2
B3

Estimated
execution time for
the processors’
loads
Client
(Modeler abject)

Servers
(Tracer objects)

Displayer PC
(Displayer object)

14

Our data distribution strategy is based upon an irregular
scene subdivision method, which makes it optimal,
because it fits very well to the scene objects form.
Furthermore, our approach exploits efficiently the object
coherency to distribute loads depending on the machines
speed; hence, we minimise the trade network during the
load balancing process.
Unlike Bouatouch distribution system’s which uses a
BSP based upon a uniform subdivision of the scene that
needs more complicated algorithm (see [8]), and
dedicated to a system of processors having the same
speed, our distribution strategy deals with speed varieties
by using a simple non consuming algorithm.

27

43

Pre-processing

Local
rendering

Composition

Nebel [10] proposed a concurrent data flow method,
which combines both objects and rays data flow, by using
information on each processor load, but actually, the load
here, represents the remaining rays number waiting for
evaluation, which doesn’t reflect at all, the correct load
which is the time that should be consumed to achieve the
pixels computation. Furthermore, the processors loads
should be communicated during all the synthesis process
that leads to a network trade. Our load-balancing
algorithm is too close to our data distribution method,
where, each processor is assigned a load depending on its
speed frequency.
In order to avoid communication bottlenecks, each time
an object is missing from a given tracer object (PC), the
latter localises on the bus CORBA, the PC owning the
corresponding cell, to activate its method returning the
overlapped CSG object. If ever one of the PCs finishes its
work before the others, and in order to make our load
balancing algorithm more efficient, the afore-mentioned
PC, evaluates the whole remaining load (in number of
pixels) over the distributed system, if it is superior than a
given threshold (global threshold), for each PC, where the
remaining load is superior than a local threshold, a sub
group of pixels with its restricted CSG tree is fetched into
a temporarily buffer in the local memory. Hence, our load
balancing algorithm, uses objects data flow at first step,
and pixels data flow at second step. Actually the pixels
dataflow has not to occur, if all the sub-regions execution
times have been estimated correctly at 100%, which still
difficult to be achieved.
Unlike many previous distributed ray-tracing systems
[1][2][6][8][10], PCs don’t need to be scrutinized
periodically to be informed about their changes. This
scrutinizing generates useless messages and therefore
increasing the network trade. In our system, during the
entire load balancing process, all the PCs inform each
other by simple methods remote invocation.

Figure 4. Example of data distribution.

4
Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

Other important characteristics of our system are as
follows:
-The messages are gathered within a method that
realises the treatment as a whole, which means less net
traffic.
-The treatments innovation is made in two ways (clientserver and sever-client), which therefore¸ permits the
speeding up of the synthesis process.
-The reuse of the CORBA tracer objects for the
synthesis of new scenes is very simple. This is due, in
fact, to CORBA objects persistence

Table4. Image synthesis time for scene1 and
scene2
Number of
processors

Time to
render
scene1

Speed up

Time to
render
scene2

Speed up

1

1025

1036

2

531

1.93

536

1.93

3

364

2.81

358

2.89

4

270

3.79

269

3.85

5

217

4.72

215

4.81

7. Experimental results
Now that the principles of our approach have been
explained, we present our experimental results. We have
implemented our parallel 3D Ray tracing in JAVA.
Results are given for a system of five 600 MHZ Pentium
III (no graphics accelerators were used). The PCs
communicate over 10 Base-T Ethernet Hub. The number
of available processors was increased by one each time
the experiment was run.
Our algorithm has been tested on scene images
including from 75 to 1815 primitives including spheres,
cylinders and parallelepiped; the table below (Table3)
describes theses images features
Table3. Tested Scenes features
Scene1

Scene2

Resolution

390u390

390u390

Luminous
sources

2

2

Photometric
tree depth

2

2

Primitives

-30 cylinders.
-15 parallelepiped.
-30 Spheres.

-900 cylinders.
-15 pparallelepiped
-900 Spheres

Our algorithm conserves approximately a constant
execution time for images containing between 75 and
1815 primitives. The table below gives the total time to
ray trace scene1 and scene2. The times given include
communication overhead. All times are given in seconds.
The total time for one picture to be traced is the time from
generation of the first ray to the time at which the last
return pixel color is added into the frame buffer.

Figure 5. Example of image test

5
Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

8. Conclusion and future work
In this paper, we have presented a new kind of data
distribution algorithm for parallel ray tracing which fits
on PCs speed varieties. The use of this method of data
distribution gives encouraging results. This algorithm
balances the rendering load using a mixed objects and
pixels dataflow among the servers and reduces the image
synthesis time. We have exposed some ideas to exploits
efficiently all the distributed resources available to us to
construct a fast and low cost parallel ray tracing over the
bus CORBA. Our current interest is to improve our data
distribution strategy to make the algorithm working
without needing to pixels data flow during the load
balancing process.

9. References
[1] D.Badouel, K.Bouatouch, Z.Lahjomri and T.Priol. Versatile
tool for parallelzing realistic rendering algorithms. Intern
Publication n°598, July 1991.
[2] T.W.Crockett and T.Orloff. A MIMD Rendering Algorithm
for distributed Memory Architectures. Proc. Parallel Rendering
Symp., ACM Press, New York, pp. 35-42, Oct. 1993
[3] A. VAN DAM, J.F.HUGHES, J.D.FOLEY, S.K.FEINER.
Computer Graphics: Principles and practise. ADDISONWESLEY PUBLISHING COMPANY. November 1991.
[4] J.Geib, C.Gransart, P.Merle. «CORBA: des concepts a la
pratique” Collection InterEditions, editions MASSON, Paris,
France, 1997. ISBN: 2-225-83046-0.
[5] S.ROTH. Ray casting for modelling solids. Computer
Graphics and Image Processing, vol. 18, 1982, pp. 109-144
[6] T.D.Scott, V.F.Fuscp and R.S. Ferguson. 3D Ray-Tracing on
PC Networks using TCP/IP. IEEE International Conference on
Information Visualisation. London England July1999.
[7] T.Whitted. An improved illumination model for shaded
display. Communications of the ACM, vol. 23,n°6, juin 1980, p.
85-92.
[8] K.BOUATOUCH, T.PRIOL. Experimenting with a parallel
ray-tracing algorithm on a hypercube machine. Intern
Publication n°405, Avril1988.
[9] T.PRIOL, B.ARNALDI, K.BOUATOUCH, A New Space
Subdivision Method for ray tracing CSG modelled scenes.
Iintern Publication n°613, February 1987.
[10] J.-C. Nebel. A concurrent dataflow algorithm for ray
tracing on the CRAY T3E. Third European SGI/Gray MPP
Workshop (1997), Paris, September 11-12,

6
Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

