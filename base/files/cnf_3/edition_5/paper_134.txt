An Immersive Game for K-5 Math and Science Education
Nicoletta Adamo-Villani* & Ronnie Wilbur♦
*Department of Computer Graphics Technology, Purdue University, USA
♦ Department of Speech, Language and Hearing Sciences, Purdue University, USA
nadamovi@purdue.edu, wilbur@purdue.edu
Abstract
We present SMILE™ (Science and Math in an
Immersive Learning Environment) an immersive game
in which deaf and hearing children ages 5-10 learn
math and science concepts and ASL (American Sign
Language) terminology through interaction with
animated 3D characters and objects. The application
can be displayed in stationary, four-wall immersive
devices (i.e., the FLEX), Fish Tank VR systems, and
non-immersive standard computer systems. Users
interact with the program choosing from a variety of
input devices including a 6DOF wand, a pinch glove, a
data glove, a dance platform, or standard mouse and
keyboard.

1. Introduction
Research in VR and education is a relatively young
field but in recent years it has shown considerable
growth. Youngblut reports over forty VR-based
learning applications [1] and Roussou describes about
ten VLE (Virtual Learning Environment) designed for
informal education [2].
Although the benefits of VR experiences need to be
defined in a more comprehensive way, recent studies
show that VR can provide a more effective learning
tool than traditional classroom practices, students enjoy
working with virtual worlds, and the experience is
highly motivating [1].
In regard to disabilities education, literature
findings suggest that VR has advantages over other
teaching technologies because it can fulfill the majority
of the learning requirements of students with
disabilities [3]. Some of the most commonly
encountered needs of people with learning impairments
include: access to safe and barrier-free scenarios for
daily living tasks; self-pacing; repetition; control over
environment; ability to see or feel items and processes
in concrete terms (difficulty with abstract concepts);
and motivation.
As far as children’s conceptual learning, Roussou
suggests that there are many compelling reasons for
believing that VLE for children provide effective
teaching tools [4]. However, due to the use of highend expensive equipment and non-standard ways in
which

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

applications are developed, the majority of existing
VLE for children are limited to academic and research
environments and institutions of informal education,
such as museums. One noticeable example of VLE for
children is NICE [4], an immersive, multi-user VLE in
which students learn basic biological concepts while
constructing, cultivating, and tending a virtual garden.
Another example is the VREAL (Virtual Reality
Education for Assisted Living) project [5], the only
existing VLE for deaf children. Funded by the U.S.
Department of Education, VREAL is an immersive
virtual environment in which deaf students learn basic
life skills, language arts, and mathematics. Five US
schools of the Deaf used the program in 2004 and
assessment studies showed an improvement of
students’ test scores by an average of 35%.
SMILE™ [6] follows in the trail pioneered by
projects such as VREAL and NICE, but makes unique
contributions to this area of research. (1) To our
knowledge, SMILE is the first bilingual VLE in which
animated 3D characters communicate with the users in
both spoken and written English, and ASL. (2) It is the
first immersive game designed for math/science formal
education, with activities based on standard elementary
school curriculum. (3) According to feedback provided
by numerous ASL signers who have tested the
application, SMILE improves significantly on existing
examples of VLE for the Deaf in terms of realism and
fluidity of the 3D characters’ signing motion. (4) It is
one of the very few examples of VLE for children
combining strategies used in commercial computer
games with lessons from educational research on
learning and motivation, with the primary goal of
making learning fun. (5) It will be available for use by
elementary schools and deaf education programs
throughout the United States. Students will interact
with the application using a relatively inexpensive
portable immersive system that eliminates the need for
a cumbersome HMD unit, while maintaining the
feeling of immersiveness.
In section 2 of the paper we discuss the design of
SMILE™; in section 3 we present its technical
implementation and we describe different systems used
to display the application. Conclusive remarks and
future work are included in section 4.

2. Design
SMIILE™ is an interactive virtual world comprised
of an imaginary town populated by fantasy 3D avatars
that communicate with the participant in written and
spoken English, and ASL. The user can explore the
town, enter buildings, select and manipulate objects,
construct new objects, and interact with the characters.
In each building the participant learns specific
math/science concepts by performing hands-on
activities developed in collaboration with elementary
school educators (including deaf educators), and in
alignment with standard math/science curriculum.
SMILE has an overall story which is introduced
through a cutout-style 2D animation at the beginning
of the game. The story includes an overarching goal
(restore the lost willingness to smile in the city of
‘Smileville’) which creates a boundary condition that
unites all the individual game tasks. Each activity is in
the form of a ‘good deed’ whose objective is to make
one of the ‘Smileville’ characters smile again by giving
him/her a meaningful new object. The ability to
construct the object is dependent on the acquisition of
math/science skills, and relative ASL signs.
All game activities are carried out in a cartoon-like
virtual world designed to be appealing to the target age
group. Key design features of the environments include
basic geometric shapes with round edges, vibrant and
varied colors, and a bright lighting setup with limited
shading and soft shadows. The choice of the color and
lighting schemes was based on research studies on the
impact of color and light on learning [7] [8], and on the
association between colors and children’s emotions [9].
One study shows that de-saturated colors have a
negative impact on stimulation while highly saturated
colors increase alpha waves in the brain which are
directly linked to awareness. Another study reports
that younger children (5 to 6 ½-years-old) are
especially attracted to vibrant colors and most positive
emotional responses are associated with warm colors.
Research on the relationship between light and learning
suggests that a bright lighting setup, with the presence
of daylight, is associated with improved students’
performance. Fig. 1 shows part of the city of
‘Smileville’ and two 3D characters.

Figure 1. Screenshot of the city of ‘Smileville’;
two characters in the foreground

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

The design of the characters is very stylized and
consistent with the visual style of the environment. All
characters are modeled as continuous polygon meshes
with a poly-count that does not exceed 6000 polygons
per avatar. A low polygon count is necessary in order
to maintain high frame rate and real-time interaction.
To realize high visual quality with a limited number of
polygons, the 3D surfaces have been optimized by
concentrating the polygons in areas where detail is
needed the most: the hands, the face, and the parts that
bend and twist (i.e., elbows, shoulders, wrists, and
waist). With such distribution of detail it is possible to
represent realistic hand/face configurations and organic
deformations of the skin during motion. The body of
each character is set up for animation with a skeletal
structure that closely resembles a real human skeleton.
The face is rigged with 20 to 30 joint deformers
positioned so that they deform the digital face along
the same lines pulled and stretched by the muscles of a
real face.
In order to represent the signing motion with
fluidity and realism, the virtual signers are animated
with a library of signing clips recorded directly from
an ASL signer wearing a Metamotion 19-markers
optical motion capture suit [10] and a pair of
Immersion 18-sensors cybergloves [11]. The individual
signs are captured and stored as separate animation
segments and interactivity and smoothness of motion is
realized via a new method of real-time blending of the
animation clips [12]. Keyframe animation is used to
animate various facial expressions such as eye blinks,
eyebrow deformations, and mouth movements;
directional constraints are used to control gaze
direction.

3. Technical implementation
SMILE™ can be displayed on different systems:
(1) stationary 4-wall projection devices (i.e., the FLEX
[1]); (2) single screen portable projection systems; (3)
Fish Tank VR systems, and (4) standard desktop
computers. The application could also be modified to
be viewed through a head mounted display unit.

3.1. Application Development
Several software packages and libraries have been
used to convert the data into a format compatible with
the specialized hardware. Graphics are rendered in the
FLEX using OpenSceneGraph [14], an open source
graphics development toolkit which works on top of
OpenGL.
Communication
between
the
OpenSceneGraph libraries, the FLEX display system,
and the input devices is implemented with the

VRJuggler toolkit [15] and Gadgeteer, VRJuggler’s
device handler. Sound is configured to work using
OpenAL and VRJuggler’s Sonix plug-in. OsgCal, an
adaptor for the CAL3D Character Animation Library
[16] allows the application to use Cal3D’s functions to
control skinned character animation within the
OpenSceneGraph driven virtual environment
All models were created as Maya 3D files and then
exported into the four components necessary for use
with Cal3D functions: .cmf (mesh file), .csf (skeleton
file), .caf (animation file), and .crf (texture file). Once
exported, the separate files are reassembled as a model
node within the scene graph of the osg program using
the osgCal libraries. OsgCal functions are used to
control the playback of the animation clips. When the
program receives key input signals from the user, the
osgCal startLoop and stopLoop functions cue the
appropriate animations.

3.2 SMILE™ in the FLEX
The student views the application through a pair of
light-weight LCD active stereoscopic glasses as it is
projected onto the immersive, four screen display (see
fig. 2). This display provides the user with images of
the virtual environment projected to the front, side, and
floor screens. The participant wears an InterSense head
tracker [17] which enables the application to determine
the position and orientation of his/her eyes; this
information is used to re-draw the environment based
on the user’s perspective, as the direction of the gaze
changes. The student can travel through the
environment using an Intersense 6 DOF wand or a
Cobult Flux dance platform [18].

Figure 2. Student in the FLEX
Objects can be selected and manipulated with the
wand, or a simple gesture control system comprised of
a pair of Fakespace Lab’s Pinch Gloves [19] coupled
with an Intersense wrist tracker. In addition to grasping
and releasing objects, the gesture control system allows
for input of a limited number of ASL math signs
(numbers 0-20), and for travel through the
environment.

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

3.3 SMILE™ on portable systems
Because SMILE™ is designed primarily for display
in a four-wall projection system, certain objects in the
scene exist in the user’s peripheral vision and on the
floor of the scene. However, we have developed a
portable version of the application which eliminates
unnecessary information from the sides of the
environment and moves the important features to the
front of the user’s view. The transition from a 4-wall
display system to a single monitor has been
accomplished by editing the VRJuggler configuration
files. Adding additional devices, such as LCD shutter
glasses for CRT monitors or desktop tracking systems,
required nothing more than the installation of new
device drivers and the creation of new configuration
files.
So far SMILE™ has been tested on the following
portable systems:
• A projection-based
immersive system
consisting of a screen and frame, a high-end
laptop, two commodity projectors, a pair of
polarizing filters, and inexpensive polarized
glasses .
• A Fish Tank VR system consisting of a
DellE520 desktop PC, a CRT monitor, an
Essential Reality P5 glove with 6 degrees of
tracking and bend sensors for each finger
[20], a pair of eDimensional wireless 3D
glasses [21] and an Intersense 3DOF PC head
tracker [17]. The system is shown in fig.3
• A standard, non immersive desktop computer
system

Figure 3. The Fish Tank VR system used for
SMILE™
The application is designed mainly for use with the
Intersense IS-900 system and takes advantage of 6
DOF tracking for both a view node, mounted on the
user’s forehead, and a wrist or wand node tracking the
movement of the user’s hand. However, when a
tracking system is not available, input can be
accomplished via mouse and keyboard. A mouse or
joystick can be used to move an icon corresponding to

the location of the wand in space; keyboard presses can
be programmed to represent gestures or actions that
might be performed by a tracked user.
Portable demos of SMILE are available for download
at:
http://www2.tech.purdue.edu/cg/i3/smile/demos.html

[5]

[6]

4. Conclusions and future work
This paper presents the design and implementation
of the SMILE™ project. Though still at an early stage
of development, SMILE™ can be considered the first
bilingual immersive learning game for deaf and
hearing children that can be delivered on a variety of
immersive, non-immersive, stationary, and portable
systems.
SMILE™ has been evaluated throughout its
development by a panel of experts in VR, animation,
and ASL, and by groups of target users. The learning
outcomes of using SMILE have not been assessed yet.
Future work involves production of additional content
(for grades 4-5) and evaluation of learning and
knowledge acquisition with elementary school aged
deaf and hearing children. The evaluation will be
carried out in collaboration with ISD (the Indiana
School for the Deaf) and two elementary schools in
West Lafayette, IN; students will interact with the
application using the Fish Tank VR system described
in the previous section.

[7]

[8]

[9]
[10]
[11]
[12]

5. Acknowledgments
This research is supported by NSF-RDE
grant#0622900, by PHS-NIH grant#5R01DC00524104, by the College of Technology at Purdue University
(grant#00006585), and by the Envision Center for Data
Perceptualization at Purdue University.

[13]
[14]

10. References

[15]
[16]

[1]

[17]

[2]

[3]

[4]

Youngblut, C. Educational Uses of Virtual Reality
Technology. VR in the Schools - coe.ecu.edu, 1997, 3,
1.
Roussou, M., Johnson, A., Moher, T., Leigh, J.,
Vasilakis, C., & Barnes, C. Learning and Building
Together in an Immersive Virtual World, Presence,
1999, 8, 3, pp. 247-263.
Darrow, M.S. Virtual Reality's Increasing Potential for
Meeting Needs of Persons With Disabilities: What
About Cognitive Impairments? Proc. of the Annual
International Conference "Virtual Reality and
Disabilities". Northridge, CA: California State Center
on Disabilities, 1995.
Roussou, M. Learning by doing and learning through
play: an exploration of interactivity in virtual

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

[18]

[19]
[20]
[21]

environments for children. ACM Computers in
Entertainment, 2004, 2, 1, pp. 1-23.
Edge, R. VREAL: Virtual Reality Education for
Assisted Learning. Proc. of Instructional Technology
and Education of the Deaf: an International
Symposium, NTID-RIT, Rochester, NY, 2001.
Adamo-Villani, N. Carpenter, E., & Arns, L. An
immersive virtual environment for learning sign
language mathematics. Proceedings of Siggraph 2006
– 33rd International Conference on Computer
Graphics and Interactive Techniques, Boston, 2006.
(The
ACM
Digital
Library
url
http://portal.acm.org/citation.cfm?id=1179316&jmp=c
it&coll=GUIDE&dl=ACM&CFID=7878894&CFTOK
EN=20130255#CIT )
Engelbrecht, K. The impact of color on learning.
NeoCON2003,
2003.
url
http://www.coe.uga.edu/sdpl/articleoftheweek/colorP
W.pdf
Grangaard, E.M. Color and Light Effects on Learning.
US Department of Education – Office of Educational
Research and Improvement. Technical Report,
ED382381, 1995.
Boyatzis, C.J., & Varghese, R. Children’s emotional
associations with colors.
Journal of Genetic
Psychology, 1994, 155, 1, pp. 77-85.
Metamotion.
Motion
Captor,
2006
url
http://www.metamotion.com/captor/motion-captor.htm
Immersion.
Cybergloves,
2006
url
http://www.immersion.com/3d/products/cyber_glove.p
hp
Adamo-Villani, N. Beni, G. Wilbur, R. & Nadolske,
M. Interactive animation system for sign language. US
patent pending # 20060134585, 2005 url
http://www.freshpatents.com/Interactive-animationsystem-for-sign-languagedt20060622ptan20060134585.php
Fakespace Systems. FLEX and reflex, 2006 url
http://www.fakespace.com/flexReflex.htm
OpenSceneGraph,
2004
url
http://www.openscenegraph.org
VRJuggler, 2005 url http://www.vrjuggler.org
CAL3D Character Animation Library, 2006 url
http://home.gna.org/cal3d/
InterSense. IS-900 Precision Motion Tracking, 2006
url http://www.intersense.com/products.aspx?id=45
Cobalt Flux. Cobalt Flux Dance Platform, 2006 url
https://host156.ipowerweb.com/~cobaltfl/sunshop/inde
x.php?action=item&id=1&prevaction=category&previ
d=1&prevstart=0
Fakespace Labs. The Pinch Gloves, 2004 url
http://www.fakespacelabs.com/tools.html
Mindflux-essential
reality
P5
glove
url
http://www.mindflux.com.au/products/essentialreality/
p5glove.html
eDimensional url http://www.edimensional.com/

