Cognitive Issues in Information Visualisation Design for Interaction for the
Visually Impaired
D. Graham, I. Benest, P. Nicholl
University of Greenwich, University of York, University of Ulster
{D.Graham@gre.ac.uk, ian.benest@cs.york.ac.uk, p.nicholl@ulster.ac.uk}
Abstract
This paper reports on the findings of a study on
improving interaction design for visually impaired
students, focusing upon the cognitive criteria for
information visualisation.
Keywords---Information Visualisation, Cognition,
Interaction Design, Visual impairment.

1. Introduction
“Information visualization can be defined as the use
of interactive visual representations of abstract data to
amplify cognition” [1], [4], [13], [17]. The characteristic
of the data that distinguishes information visualisation
from scientific visualisation is that the data are abstract.
Visualisation data types and the tasks that need to be
supported range from 1D Linear for tasks such as
TextArc, to 3D World (Desktops for example), and
Multidimensional, e.g. matrices [9]. The elements of the
task taxonomy, described by Shneiderman [9], are:
overview, zoom, filter, details-on-demand, relate, history,
and extract. Several challenges identified for information
visualisation and tool development for information
visualisation are with respect to: import data; combining
visual representations with textual labels; seeing related
information; viewing large volumes of data; integrating
data mining; collaboration with others, and; achieving
universal usability [9].
Spence [12] defines visualisation: “visualize: to
form a mental model or mental image of something”.
There is a natural dominance towards the use of images,
as for sighted people, vision represents around sixty per
cent of their “input”. Information visualisation for the
visually impaired provides a major challenge. Current
interface design for teaching visually impaired students
whilst SENDA (Special Educational Needs and
Disabilities Act in mainland UK) or SENDO (Special
Educational Needs and Disabilities Order in Northern
Ireland) compliant, has often neglected the direct
involvement of target users in determining the
requirements specific to their needs. In particular, there

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

is a lack of awareness of the cognitive issues for the
spectrum of users deemed to be visually impaired. A
research project funded by the Higher Education
Academy aimed to determine and produce criteria for the
design of interfaces through the participation of target
users from the outset, implementing these criteria in
teaching exemplars in Computer Science at Ulster, and in
Electronics at York. An important constraint was that
these criteria would be inclusive, i.e. usable by both
sighted and partially sighted students. This posed a
considerable problem particularly for the information
visualisation of exemplars at York for conveying
electronic circuit diagrams.

2. Method
The first activity required is knowledge acquisition.
Different authors present methodologies with varying
stages of knowledge acquisition, but fundamentally
they all involve; the identification and conceptualisation
of requirements and problem characteristics, formalising
these into some mediating representation scheme,
implementation, and final testing and validation [5].
Knowledge acquisition can be machine-aided or humanlabour oriented.
Johnson and Johnson’s methodology [7], enhanced
by Graham [6], proposes a three stage knowledge
acquisition process based around semi-structured
interviews. The first phase is to perform a broad, but
shallow survey of the domain. This allows the elicitor to
become oriented onto the domain, so a more flexible
approach can be taken. This type of horizon broadening
is standard approach in social science research. Once this
shallow trawl of the domain has been done, the second
phase requires that a more detailed task analysis is
performed by the elicitor, focusing on the area of
interest. The structure of the interview uses a teachback
technique to traverse the domain and validate elicitor
understanding with result that the elicitor progressively
refines the model of the expert’s competence. This model
is qualitatively drawn up and uses a mediating
representation, Systemic Grammar Networks (SGNs) [3].
These are a context free, qualitative representation,

which can be used as a tool for the systems design, but
their use does not imply the final application of any
particular knowledge engineering software or
methodology. SGNs have been used in many domains
including oncology, pcb (printed circuit board) design,
and fault diagnosis. The third phase of this approach is to
validate the models drawn up from the expert with the
wider expert community. The theoretical predictions of
the model are presented to the initial community used in
the first phase, and then to a further independent
population, to check the appropriateness and validity of
the model which has been created.
This knowledge acquisition methodology was
adopted and tailored to the needs of the project. The first
phase, broad and shallow survey, was achieved by
arranging local interviews with clients from the Royal
National Institute for the Blind (RNIB) in London and in
Ulster, and at TechDis in York using questionnaires
specifically tailored to suit visually impaired
interviewees. The second phase, a more detailed task
analysis, was achieved through the design of semistructured interviews with a visually impaired student
expert at Ulster. Knowledge synthesis and analysis of
interview findings led to design criteria for information
visualisation, rather than the employment of SGNs which
were not considered practical for visually impaired
experts. The majority of these criteria were cognitive and
it is these criteria that will be described.
Validation (and verification) was achieved by the
evaluation of implemented criteria in exemplars at Ulster
and York, for teaching Computer Science and
Electronics respectively.

Notetakers (portable devices for taking notes, etc, some
with Braille keyboards) [15].
Whilst extremely useful information was gleaned,
the solutions offered were not sufficiently inclusive. A
prime example was the use of tactile diagrams and
graphs aimed at blind and partially sighted people [10],
for example “Tactile and large print maps of 3 London
Underground (LU) Stations” using raised lines. These
diagrams were in principle very pertinent, because the
concept of the London Underground map was based on
an electronics circuit and therefore relevant to the York
exemplars. It was initially difficult to see how these
diagrams could be made computer tractable. T3, prima
facie, appeared to be a solution. The T3 [14] is a touch
sensitive, multi sensory device which provides instant
audio feedback from tactile images. It enables visually
impaired people to access graphical information. The T3
is connected to a standard PC or laptop computer via a
USB connection and the self-installing programme
compact disc is inserted. To activate the system, a T3
tactile diagram overlay is placed on the surface of the
device and touched by the operator’s finger. The T3 is
the European version of the Talking Tactile Tablet (TTT)
owned by Touch Graphics, New York. It requires tactile
diagrams (such as the LU maps), so it would be
necessary to create every combination and permutation
of these for teaching Electronics, too numerous to be
practical or inclusive. The NCTD [10] states that Tactile
Diagrams are useful when:
• The user is print-impaired and has some tactual
ability.
•

Novel concept not easily described in words.

•

Real object is unavailable for touching.

3. Results

•

The shape/form/pattern is important.

The broad and shallow survey conducted at
Greenwich with the RNIB resulted in a great deal of
relevant publications, materials and links to appropriate
sites.
Guidance
on
“Designing
forms
and
Questionnaires” [14] was sent from the RNIB. It
confirmed that the best option for knowledge acquisition
was through semi-structured interviews or questionnaires
completed either face-to-face or over the telephone. This
accommodated the resolution of any ambiguities by
allowing the interviewer to clarify points, and to
overcome on-line and on mass questionnaire fatigue. It
also facilitated the knowledge acquisition to take place
without the need for additional tools, such as those
described below being required (Braille printers or text
to speech output, etc.)
Current interface design for partially sighted and
blind users predominantly includes Tactile User
Interfaces (TUIs) or Audio User Interfaces (AUIs) [2],
[10]. The RNIB sent information on “Using a computer
without vision” and “Notetaking” [15]. The former
highlighted products that enable a blind person to use a
computer by hearing in electronic speech what is
displayed on the screen (mainly screen readers) or read
as Braille on a Braille display, which included

•

Needed to illustrate scale and relationship:
biology, maps, technology.

•

As a reference: once, or as reminder.

•

Necessary to enhance educational experience –
variety.

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

Tactile Diagrams are not:
• Good for fine detail.
•

Good when extremely large.

•

Good without training.

•

Good without support materials.

These factors meant that they were not a suitable
solution. The most difficult hurdle was designing an
interface that was both computer tractable and inclusive.
Coping with the number of combinations and
permutations for the electronics exemplars also meant
that any solution needed to be dynamic.
Guidelines were suggested by Tiresias [15] for
several aspects of computing for varying disabilities, but
were highly specific, for web accessibility for instance.
The most generic advice was the “User Needs Summary”
dealing with each disability in turn. Specific to

applications software were “Guidelines for Application
Software Accessibility” [8]. These guidelines (2
priorities) covered application software running under
any operating system or runtime environment. Priority 1
ensures that the application can be used by most people
with impaired mobility, vision, hearing, cognition and
language
understanding,
using their
assistive
technologies. Priority 2 makes it easier to use and will
include more people with cognitive impairments or
multiple disabilities. These guidelines were certainly
inclusive.
The task analysis conducted with the student expert
at Ulster proved to be most insightful. The student had
had a period of being sighted and therefore was able to
offer viewpoints with and without a visual and/or haptic
memory of things. For example, the student had a visual
memory of a grid, but only a haptic memory of resistors
and capacitors. The student was therefore able to
discriminate between what was meaningful to a visually
impaired student that had a visual and/or haptic memory.
This proved to be highly significant in terms of
metaphors used. Terms such as “door to” would be
meaningful irrespective of the degree of visual
impairment, whilst terms such as “points to” would be
meaningless even with a visual memory. For using
computer interfaces, predictably, the main sense used
was hearing, however sight was still above smell and
taste. In relation to everyday activities for the student,
hearing and touch can be interchangeable. The student,
probably due to the possession of visual memory, still
thought in terms of images. The student was able to
touch-type (learnt whilst sighted) so used standard
Querty keyboards for input and, speakers and GUIs with
screen readers such as Dream, for output. The student
was unable to read Braille which was considered a great
disadvantage as there were major advantages to being
able to use Braille displays and printouts for checking
computer programs for example. The recommendations
from the student for interaction design were that: colour
contrast can be of great immediate benefit for many
partially sighted people; explanations using terms like
“door, room, lego” were meaningful to all; the best input
and output devices were “anything tangible”, i.e. audio
or tactile, with touch for orientation, keyboard for input;
“hearing is serial, vision is parallel”. The student had
used examples of raised maps for aircraft flight safety
procedures, but as no reference point was given as to
where the student was located on the aircraft or map, the
map was meaningless. The student had no visual
memory of AND/OR/NOT gates or their schematics. The
student had some visual memory of programming,
namely Visual Basic, prior to losing sight.
With respect to teaching and learning electronics,
particularly schematics, three descriptions of a circuit
diagram were presented at the task analysis: “Description
by
Components
Top-to-Bottom,
Left-to-Right”;
“Description by Location and Node”, and; “Description
– Human Orientated”. Due to the possession of visual
memory, the second description was “more everyday
language”, the third “more hierarchical”. The

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

hierarchical structure was deemed to be an aid to
cognition, however, the student chose the second
description which was also the easier to implement. The
student said that the following information was needed
for navigating the web (or schematics, to some extent):
• “Where are you?
•

What can you do?

•

When do you know you’re there?

•

How can you get back (not all pages have a
home or back button)?

•

Best naming convention for Lecture 15, slide 2
say, would be 15.2.

•

Brief reminders of where you are at all times –
just when moving on (not FROM WHERE, just
TO WHERE), re. naming convention.

•

Reference to Daisy navigation system for
sectioning of books from RNIB.

•

Textual description preferable to sound (audio
icons).

•

Speed of voice should be controllable, such as
Talking books (RNIB), using something like
Cont+Alt. Could be an issue with platforms.
Could use arrow keys for navigation.

•

Best to keep tone constant”.

The interview on a Computer Science on-line tutorial
revealed that the student:
• Found pauses included in the present audioaided visual presentation were necessary, else
too fast.
•

Presentation could be improved by the use of
male and female voices, male for the tutorial
facts, female for the details say.

For sighted people, diagrams reduce the cognitive
burden and allow externalizing to reduce memory load
[9]. Given that visually impaired people still only have a
short term memory of seven plus or minus two items [2]
with which to capture and appreciate an idea – the
semantics, any cognitive burden needs to be reduced as
much as possible. Any additional syntax will therefore
get in the way.

4. Discussion and Conclusions
The distributed cognition of the student expert was
mainly acoustic. Understanding diagrams appears to be
the crux of the problem. Learning something like Unified
Modelling Language (UML) for a computer science
student would pose a major problem as it involves
diagrams and programming code.
Information Visualisation criteria identified for
interface design for visually impaired students:
• Solutions should be inclusive (suitable for
sighted, partially sighted and blind users).
•

Solutions should be computer tractable.

These criteria may be diametrically opposite.
• Solutions should be dynamic.

[2]

•

Metaphors should be meaningful to all (“doors”,
“rooms”, “lego” not “points to”).

•

Touch is best for orientation.

[3]

•

Sound is best for input and output, unlike
Braille it is inclusive.

[4]

•

Colour contrast can help a large range of (but
not all) people.

•

Inclusion can be aided by multi-modal and
multi-media interfaces.

•

High-level names which are well understood by
all should be adopted, so that an individual’s
seven items are not overly compromised.

•

An emphasis on naming items followed by their
use should also help consolidate sighted
people’s learning.

•

Superfluous information or detail needs to be
suppressed.

[5]
[6]
[7]

[8]
[9]

The most poignant statement: “hearing is serial,
vision is parallel”. Substitutions for visual information
tend to be audio. Touch provides greater parallel input
and output, but is not as accessible to all, or as widely
used or inclusive.

[11]

Acknowledgements

[12]

This research was funded by the Higher Education
Academy subject network for Information and Computer
Sciences Development Fund.
The assistance and information provided by Mr.
James Bird at the RNIB is gratefully acknowledged. We
reserve our greatest thanks for the student expert at the
University of Ulster, for his tolerance, considerable
insight, and in making this project possible.

References
[1]

Bederson, B., and Shneiderman, B., The Craft of
Information Visualization: Readings and Reflections,

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

[10]

[13]
[14]
[15]
[16]
[17]

Morgan Kaufmann Publishers, San Francisco, CA,
(2003). In: [11].
Benyon D., Turner P. and Turner S, Designing for
Interactive Systems. Addison Wesley 104; 404-417
(2005).
Bliss J., Monk M. and Ogborn J. Qualitative Data
Analysis for Educational Research. Croon Helm (1993).
Card, S., Mackinlay, J., and Shneiderman, B., Readings
in Information Visualization: Using Vision to Think,
Morgan Kaufmann Publishers, San Francisco, CA,
(1999). In: [11].
Graham D. and Barrett A., Knowledge-Based Image
Processing Systems. Springer-Verlag (1997).
Graham D., Knowledge Acquisition: A Case Study in
Computer Fault Diagnosis and Repair. PhD thesis,
Brunel Unversity (1990).
Johnson L. and Johnson N., Knowledge elicitation
involving teachback interviewing. In: Knowledge
acquisition for expert systems: a practical handbook.
Kidd A. L. (ed.), New York, NY Plennum, 91-108
(1987a).
Guidelines for Application Software Accessibility. Irish
National Disability Authority Guidelines Web Site.
http://www.acessit.nda.ie
Preece, J., Rogers, Y., and Sharp, H., Interaction Design
Beyond Human-Computer Interaction, Wiley, (2002), p.
98.
RNIB National Centre for Tactile Diagrams (NCTD)
Web Site. www.nctd.org.uk
Shneiderman, B., and Plaisant, C., Designing the User
Interface Forth Edition, Addison Wesley, (2005), pp.
580-601.
Spence, R. Information Visualization: Design for
Interaction Second Edition, Pearson Education, (2007),
p. 5.
Spence, R., Information Visualization, Addison-Wesley,
Reading, MA, (2001). In: [11].
T3, RNIB Web Site. www.rncb.ac.uk/t3/index.html
“User Needs Summary”. Tiresias Web Site.
www.tiresias.org
“Using a computer without vision” and “Notetaking”.
RNIB Web Site. www.rnib.org.uk
Ware, C., Information Visualization: Perception for
Design: Second Edition, Morgan Kaufmann Publishers,
San Francisco, CA, (2004). In: [11].

