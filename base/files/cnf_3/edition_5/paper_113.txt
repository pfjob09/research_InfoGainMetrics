Occlusion culling for stereoscopic display of complex mechanical sets

1

I. Mansa1, A. Amundarain1, A.García-Alonso2, L. Matey3
CEIT, Euskal Herriko Unibertsitatea, 3 CEIT and Tecnun (University of Navarra)
{imansa,aamundarain@ceit.es, alex.galonso@ehu.es, lmatey@tecnun.es}
2

Abstract
This paper explores the benefits that can be achieved
for stereoscopic visualization when using occlusion culling
strategies. The virtual environments analysed in this paper
are complex mechanical sets that are not densely occluded.
Those which are built up by dense geometric sets like
aerospace engines composed of thousands of components
and millions of polygons. The algorithm uses occlusion
queries and succeeds in improving the frame rate of the
stereoscopic visualization with an effective scheduling of
the queries. Different coherences, including stereo
coherence, are used to achieve this effective scheduling. An
aggressive approach has been also validated for inclusion
in the proposal. The occlusion culling algorithm can be
easily implemented and provides a significant boost in
performances.

provide users with more realistic experience and a better
understanding of the three-dimensional world. The
influence of sound, tactile and visual stimuli performing
accessibility tasks simulations is shown in [3]. The use of
stereo improves both accuracy and time reaction in tasks
such as navigation, manipulation or training.
Figure 1 presents accessibility simulations of a
complex aircraft engine with stereoscopic visualization and
a large haptic device. Stereo display is required in order to
correctly reach the elements and avoid collisions with
mock-up’s parts.

1. Introduction
Interactive visualization of large data sets makes use
of different strategies and algorithms: occlusion culling is
one of them. Large data sets may model quite different
environments, urban walkthroughs, flight simulators,
Geographic Information System (GIS) data, factories,
vehicles, etc. Other culling techniques, like frustum
culling, will not be discussed here and the results analyzed
try to be independent of their use.
This research studies the benefits that can be achieved
using occlusion culling when visualizing interactively large
engineering data sets like aircraft engines. The research has
been integrated within a stereo VR system used for
maintenance simulation: man and tool accessibility
analysis in order to analyze assembly-disassembly
sequences and times [1, 2].
Some applications also need a better understanding
and perception of the different elements that compound the
virtual environment. Performing accessibility tasks
requires an accurate and natural presentation of spatial
information, so stereoscopic visualization is required to

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

Figure 1 Virtual simulation with a complex
mechanical set.
In these pages the expression direct drawing means
that the image is generated without using occlusion culling
techniques. They seek to improve the display frame rate at
which images are generated.
The occlusion scheme analyzed in this work extends,
to stereo images, results gathered in a previous monoscopic
occlusion-culling scheme [4]. That research justified that
scenarios need to improve the frame rate provided by direct
drawing. It also explained the characteristics of dense
geometric scenarios where occlusion culling methods
provide very high frame rate improvements. Densely

occluded scenarios have enough margin to clearly benefit
from occlusion culling. However, it was not clear whether
non-densely occluded scenarios could improve frame rate
using occlusion culling. That previous research
demonstrated that non-densely occluded scenarios can
obtain significant benefits when using adequate occlusion
culling algorithms.
Stereo visualization, in principle, consumes twice the
graphic power required to generate monoscopic images.
Our goal has been to mix different occlusion culling
strategies in order to reduce that “double cost” that has the
undesired consequence of halving the monoscopic frame
rate. Our previous work analyzed GPU and geometric
models evolution and concluded that the struggle to
achieve interactive frame rates is not solved by GPU
evolution. In first place, geometric models increase their
complexity and secondly, users ask for rendering quality
improvements. Our research had two objectives. The first
one was to show that occlusion culling is useful even for
non-densely occluded scenarios, as was shown in [4]. This
paper addresses the second one, to explore occlusion
culling architectures and to analyze which are the actual
benefits that can be achieved for stereo visualization (see
end of Section 2).
Next section compiles previous work in the field of
occlusion culling and stereoscopic visualization. At the end
of the section two open fields of research are described.
Section 3 analyses a method that improves the frame rate
of not densely occluded models using stereoscopic
visualization while Section 4 presents an analysis of the
results. Finally the main conclusions of this research will
be drawn.

2. Related work
The problem of determining the visibility of
geometric-sets in virtual environments has been widely
studied. Cohen-Or et al. [5] and Bittner and Wonka [6]
provide in their survey several techniques to compute the
visibility of scenes.
Urban and architectural scenes were the first ones in
being analyzed. The initial studies were made by Teller
and Sequin [7], and the research is still in progress. Wonka
et al. [8] have obtained very interesting advances the last
years for this type of scenarios.
Another field of research has consisted in determining
the visibility of the objects in the image space. Zhang et al.
[9] computed the visibility of an object through two tests:
an overlap test and a depth test. Several image space
algorithms have been included by the manufacturers in the
graphic systems to accelerate the visibility computations.
ATI presented the HyperZ technology [10], where the

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

depth operations were optimized. Another feature
implemented was the HP occlusion flag, which allows
feedback of occlusion information for drawing objects.
Klosowski and Silva [11] developed an occlusion
algorithm employing this feature.
The last generation of graphic hardware has presented
a new feature: the occlusion query. Besides determining
the visibility of a geometry set, it also informs about how
many pixels are visible. In addition it offers mechanisms to
use it without breaking the synchronism between the CPU
and the GPU. A bad scheduling in the use of the occlusion
queries can break that synchronization and that will
suppose a slowdown in the performances. Several GPUs
solve this problem in the “occlusion-switch” proposed by
Govindaraju et al. [12].
Frame to frame coherence is an important resource
that can improve the response of occlusion algorithms
because visibility information is similar in continuous
frames. This feature allows an effective scheduling of the
occlusion queries, where some queries can be launched in
one frame and requested in the following one. This
technique provides an efficient synchronization between
GPU and CPU. Bittner et al. [13] and Sekulic [14] verified
this technique in densely occluded scenarios. Our previous
work [4] showed the achievements that can be obtained on
non-densely occluded scenarios and the limitations that
appear when occlusion density drops significantly.
Wang et al. [15] reduced the time needed to generate
the stereo pair image deriving from the left image the
pixels values of the right image. The existent gaps are
filled by ray casting. Their algorithm was designed for
multi-processors. They reported a speed-up of 50% and
80% in the scenes analyzed with a single processor
configuration. A Simultaneous Generation of Triangles
algorithm was proposed by Güdükbay and Yilmaz [16].
The list of visible objects of the left eye image is used in
the second eye image. They operate with the objects inside
an enlarged view frustum. The criterion to enlargement is
half the projection of the interocular distance for each eye.
They reported 42% of performance gain.
Although a lot of work has been performed in these
two fields, occlusion culling and stereo rendering, there are
open research topics. Occlusion culling stereo algorithms
adapted to the new generation of GPUs that support
occlusion queries, and the benefits provided by stereo
occlusion culling for non-densely occluded environments
remain unexplored. The most related work is presented by
Gobbetti and Marton [17]. They present an out-of-core
view dependent simplification based on voxels with
stereoscopic visualization but they do not explore the
benefits provided by stereo coherence.

3. Stereo coherence OCF algorithm (scOCF)
This section analyses the behaviour of an occlusion
culling algorithm for stereo visualization that combines
frame to frame coherence and stereo coherence. Visibility
analysis is assisted by current hardware occlusion queries.
Even though it is possible to use smarter data
structures to manage the information of the scene, the list
based structure we are using does not penalize efficiency.
The algorithms here considered do not require a complex
data structure or a pre-process that sorts spatial
information. So, there is not a need for using complex data
structures. The algorithm optimizes the frame rate in
runtime using a simple algorithm that consumes as less as
possible computing resources.
Left eye rendering makes use of previous frame left
eye information (frame to frame coherence) as shown in
Figure 2. Due to the similarity of the images generated by
both eyes, visible objects on the left eye can be considered
also visible on the right eye. This is the principle of stereo
coherence. Right eye rendering exploits the information
obtained in the left eye (Figure 2).
This supposition should reduce the number of
occlusion queries performed in the right eye and, as a
consequence, it should improve the frame rate.

Figure 2 Coherence exploited in scOCF method.

3.1. Left eye rendering
Left image is generated by Occlusion Culling Fraction
(OCF) algorithm [4]. When a new frame is generated, the
algorithm inherits some queries from the previous frame.
There are two sets of lists that discriminate objects as:
occluded and visible. Each list has two versions (last and
next) which refer to data from the last frame and data that
will be used in the next frame. These lists (last_visible,
next_visible,
last_occluded,
next_occluded)
swap
information from frame to frame.
Before describing the algorithm steps, an important
detail must be considered. The last_visible list contains all
the objects that were drawn in the left eye of the previous
frame. When the right image of the previous frame was

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

rendered, the visibility queries for these objects were
collected and stored as both eyes need them. That way,
extra data flow from GPU to CPU is avoided. In fact, some
of the objects that were drawn in the previous frame were
occluded as the collected queries report. We use the same
criterion that was experimented in [4].
The first step initializes the next_visible list and
traverses the last_visible list.
• Objects that were visible in one or more pixels are
added to the next_visible list (starts the list).
• Objects that are totally occluded (0 pixels drawn)
are added to the last_occluded list (updates the
list).
The second step determines if the camera has moved
more than a threshold. And if so, objects in the next_visible
list are sorted by a depth priority criterion. Depth is
measured between the viewpoint and the object’s AABB
nearest corner. The purpose of using a sorted list is to
improve occlusion query efficiency. On the other hand,
sorting adds some computational load, so the sorting
process is avoided if camera position has not changed too
much.
In the third step, the objects in the next_visible list are
drawn. A query is also posted for each one of them. The
result will be requested in the next eye. This way, the
“shadow” left in the depth buffer will make more efficient
the occlusion queries in next steps when testing possible
occluded objects (those in the last_occluded list).
The fourth step places occlusion queries for a fraction
of the objects in the last_occluded list. Hardware occlusion
queries are sent to the GPU, with lighting and texturing
disabled to speed up the result. The queries do not update
depth and image buffers. When a fraction of last_occluded
objects are not analyzed, they are directly inserted into the
next_occluded list (starts the list).
Last step recollects, for the fraction of objects
analyzed, the result of the occlusion queries. Depending on
the number of visible pixels, objects are inserted into one
of the lists.
• Objects that are visible in one or more pixels are
added to the next_visible list (updates the list).
• Objects that are totally occluded (0 pixels drawn)
are added to the next_occluded list (updates the
last list).
Lighting, texturing, depth and image buffers are
enabled. Visible objects are rendered with a query.
Figure 3 summarizes the left eye rendering process.

Figure 4 Right image generation process.

4. Results

Figure 3 Left image generation process.

3.2. Right eye rendering
Right eye rendering follows the same policy of the
OCF scheme but neither sorting (step 2) nor fraction
analysis is required (step 4).
At the beginning of the right eye rendering, the results
of the queries placed in the GPU by the left eye, have to be
requested and stored. If those results are not collected, the
rendering of the right eye will overwrite them.
The second step analyzes the result of the queries.
Visible objects are drawn, and occluded objects are added
to the occluded_tmp list. To guarantee a conservative
method, objects with an “occluded” result for the left eye
have to be tested again due to binocular disparity.
So, in the third step, visibility queries for the objects in
next_occluded and occluded_tmp lists are placed.
The next step recollects the result of these occlusion
queries. Objects with one or more pixels visible are
rendered.
As a final step, the last and next lists swap for the next
frame.
Figure 4 summarizes the right eye rendering process.

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

The experiments have been carried out using a single
PC configured with one Pentium 4 at 3.2 Ghz, 2 Gb of
RAM and running under Windows XP. The frames were
rendered in stereo at a resolution of 2x1280x1024 on an
NVIDIA GeForce 7800 GTX 256 RAM and PCI-express
bus.
The test scenes are mechanical assemblies composed
of thousands of elements and a few millions of polygons.
In this paper, we study two aircraft engine models built up
by 1.6 and 2.9M polygons respectively. The rendering
features simulate realistic lighting inside a workshop with
four ambient lights and environment mapping reflections
(see Figure 5).

Figure 5 Example of a test scene: Turbine 2.
The procedure followed allows us studying the
behavior of the algorithms with different models and
camera paths. As the camera moves automatically along

30
25

Frame rate (fps)

the desired path several data can be measured in a
controlled way. In every frame the whole model is located
within the viewing frustum, therefore view frustum culling
itself does not remove geometry and does not affect the
experiments. This approach permits assessing the quality
of each individual image and “its frame rate”. The frame
rate is not an averaged value; instead it is “discretely”
measured for each image. Quality is evaluated by
measuring the percentage of wrong pixels in the image.
The reference value used to calculate the speed-up is
the frame rate when rendering the model using direct
drawing. The speed-up is the percentage gain obtained by
each algorithm compared to the reference one. That is:

scOCF 50%
scOCF 100%
direct drawing

10

0
0

100

200

300

400

500

600

700

Frame
800

Figure 7 Frame rate using OCF fractions.
Method

Mean (%)

Min (%)

Max (%)

scOCF 100%

230

47

374

scOCF 50%

258

47

426

Table1 Speed-up reached with Turbine2.
We also studied the loss of quality of the images
generated with different fractions and they were negligible.
Table 2 presents the percentage of wrong pixels generated
in each eye when using a 50% fraction. As 100% is
conservative it does not produce errors.

30

Eye

Mean (%)

Min (%)

Max (%)

StdDev (%)

25

Left

0,034

0

2,224

0,132

Right

0,001

0

0,013

0,002

20

Table 2 scOCF 50% statistics of the % of wrong
pixels generated with Turbine 2.

15
scOCF 100%
OCF 100% x2
direct drawing

10
5
0
0

100

200

300

400

500

600

700

Frame
800

Figure 6 Frame rate achieved with Turbine 2
In most of the viewpoints, the frame rate achieved by
scOCF allows interactive simulations (more than 15 fps)
with stereoscopic visualization. The average speed-up
obtained (230%) shows the benefits of the algorithm
proposed.
Next experiment analyzes the use of different
occlusion culling fractions (see Figure 7). It analyzes a
conservative approach (100%) and a more aggressive
choice (50%).
Table 1 shows the main results that have been
compiled. They show that a slight aggressive strategy
increases the frame rate an additional 30%.

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

These values seem negligible, and a selection of 50%
fraction brings good frame rate with minimal loss of
quality. These results are quite similar to those reported for
monoscopic rendering [4].
Figure 8 presents results for another turbine built up by
1.6 M polygons (Turbine 1).
scOCF 50%
scOCF 100%

50

Frame rate (fps)

Frame rate (fps)

15

5

speed-up = ( ( occlusion - direct_drawing ) / occlusion ) *100

This information is very intuitive to be aware of the
algorithm performances.
Figure 6 presents the frame rate achieved with the
scOCF algorithm. We have also rendered the same frames
with the monoscopic algorithm (OCF). A balanced
comparison of both methods requires drawing twice each
frame for the OCF algorithm (OCF x2). Figure 6 contains
these data.
In this experiment, a fraction of 100% (query all
occluded objects) is used. The use of stereo coherence
clearly improves the frame rate compared to a plain
rendering with OCF algorithm: about 50% average speedup.

20

direct drawing

40
30
20
10
0
0

100

200

300

400

500

600

700

Figure 8 Frame rate using Turbine 1.

Frame
800

The algorithm proposed boosts the performances as to
achieve real time visualization with Turbine1.
Table 3 presents the speed-up obtained with the
models studied.
Method

Mean Turbine 1 (%)

Mean Turbine 2 (%)

scOCF 100%

98

230

scOCF 50%

132

258

Table 3 Speed up obtained with turbines 1&2.

[3]

[4]

[5]

Conclusions and future work
The experiments presented show the interest of
occlusion culling for the interactive stereo visualization of
large data sets in compact environments that are not
densely occluded. The method proposed can be easily
implemented. It provides speed ups between 100% and
250% (mean values).
Some techniques have been adequately scheduled to
achieve these improvements in the frame rate. The scheme
is based on two frame to frame coherence techniques:
camera deviation threshold and occlusion culling fraction.
It also exploits eye to eye coherence and aggressive
occlusion strategies.
Aggressive strategies provide speed ups between a
50% and 425%. On the other hand the artifacts generated
by this choice are nearly negligible.
Many avenues for future work lie ahead. The
algorithm can be improved by combining several ways of
exploiting frame to frame coherence and eye to eye
coherence. We would like to explore other complex and
compact data sets to test our achievements.

[6]
[7]
[8]
[9]

[10]
[11]

[12]

[13]

Acknowledgements
The research work presented in this paper is supported
by the European Commission, under the FP6 IST-2002002114 Enactive NoE (http://www.enactivenetwork.org).
Dr. Garcia-Alonso research was partially supported by the
Spanish Ministry of Educational and Science MEC and the
Gipuzkoako Foru Aldundia with FEDER.

[14]
[15]

[16]

References
[1]

[2]

Borro, D., Hernantes, J., Mansa, I., Amundarain, A.,
García-Alonso, A., and Matey, L., "Virtual Maintenance
for Dense Environments," proceedings of the Laval Virtual
2005 (VRIC 2005), Pp. 141-148. Laval, France, 2005.
Borro, D., García-Alonso, A., and Matey, L.,
"Approximation of Optimal Voxel Size for Collision
Detection in Maintainability Simulations within Massive

11th International Conference Information Visualization (IV'07)
0-7695-2900-3/07 $20.00 © 2007

[17]

Virtual Environments," Computer Graphics Forum, vol.
23, Pp. 13-23, 2004.
Díaz, I., Hernantes, J., Mansa, I., Lozano, A., Borro, D.,
Gil, J. J., and Sánchez, E., "Influence of Multisensory
Feedback on Haptic Accesibility Tasks," Virtual Reality,
Special Issue on Multisensory Interaction in Virtual
Environments, 2006.
Mansa, I., Amundarain, A., Elizalde, E., Garcia-Alonso, A.,
and Matey, L., "Towards Adaptive Occlusion Culling
Using Camera Coherence," proc. of Information
Visualization, Pp. 591-596, 2006.
Cohen-Or, D., Chrysanthou, Y. L., Silva, C. T., and
Durand, F., "A Survey of Visibility for Walkthrough
Applications," Visualization and Computer Graphics, IEEE
Transactions on, vol. 9, Pp. 412-431, 2003.
Bittner, J. and Wonka, P., "Visibility in Computer
Graphics," Environment and Planning B: Planning and
Design, vol. 30, Pp. 729-756, 2003.
Teller, S. J. and Sequin, C. H., "Visibility Preprocessing for
Interactive Walkthroughs," Computer Graphics (Proc. of
SIGGRAPH '91), vol. 25, Pp. 61-69, 1991.
Wonka, P., Wimmer, M., Zhou, K., Maierhofer, S., Hesina,
G., and Reshetov, A., "Guided Visibility Sampling," ACM
Trans. Graph., vol. 25, Pp. 494-502, 2006.
Zhang, H., Manocha, D., Hudson, T., and Hoff III, K. E.,
"Visibility Culling using Hierarchical Occlusion Maps,"
Computer Graphics (Proc. of SIGGRAPH '97), Pp. 77-88,
1997.
Morein, S., "Ati Radeon Hyper-Z Technology," proc. of the
Hot3D- Graphics Hardware Workshop, 2000.
Klosowski, J. and Silva, C. T., "Efficient Conservative
Visibility Culling Using The Prioritized-Layered Projection
Algorithm," IEEE Transactions on Visualization and
Computer Graphics, vol. 7, Pp. 365-379, 2001.
Govindaraju, N.K., Sud, A., Yoon, S.-E., and Manocha, D.,
"Interactive Visibility Culling in Complex Environments
using Occlusion-Switches," proceedings of the Symposium
on Interactive 3D graphics, Pp. 103-112, 2003.
Bittner, J., Wimmer, M., Piringer, H., and Purgathofer, W.,
"Coherent Hierarchical Culling: Hardware Occlusion
Queries Made Useful," Computer Graphics Forum, vol. 23,
Pp. 615-624, 2004.
Sekulic, D., "Efficient Occlusion Culling," in GPU Gems,
vol. Chapter 29, R. Fernando, Ed.: Addison-Wesley, 2004,
pp. 487-503.
Wang, M., Zhang, N., Qu, H., and Kaufman, A. E.,
"Interactive Stereoscopic Rendering of Volumetric
Environments," IEEE Transactions on Visualization and
Computer Graphics, vol. 10, Pp. 15-27, 2004.
Güdükbay, U. and Yilmaz, T., "Stereoscopic ViewDependent Visualization of Terrain Height Fields," IEEE
Transactions on Visualization and Computer Graphics,
vol. 8, Pp. 330-345, 2002.
Gobbetti, E. and Marton, F., "Far Voxels: A Multiresolution Framework for Interactive Rendering of Huge
Complex 3D Models on Commodity Graphics Platforms",
ACM Transactions on Graphics, 24(3): Pp. 878-885, 2005.

