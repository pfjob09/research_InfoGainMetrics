PC-based Volume Rendering for Medical Visualisation and Augmented Reality
based Surgical Navigation
R.J. Lapeer

R.S. Rowland

Min Si Chen

School Of Computing Sciences
University of East Anglia
Norwich, UK
E-mail: rjal@cmp.uea.ac.uk,r.rowland.uea.ac.uk,msc@cmp.uea.ac.uk
• Stereo visualisation using anaglyph or dedicated devices such as shutter glasses or autostereoscopic displays. See Figure 1(b).

Abstract
We describe a generic software architecture for stereoscopic augmented reality microsurgery, built upon a general framework for volume rendering based 3D visualisation. The software is called ARView and allows the user
to perform the standard procedures for stereoscopic augmented reality surgical navigation: calibration of a visualisation device; registration of pre-operatively and intraoperatively acquired patient image data; overlaying with
either volume or surface rendered (segmented) data using
different methodologies; and tracking of moving objects in
the surgical scene.

1. Basic Functionality
ARView is designed to work on a PC with an off the
shelf graphics adapter. To use the basic functions of the software, a graphics card with at least 32MB of texture memory
is preferred. ARView has been developed from the generic
volume rendering software, 3DView, which provides the
following standard utilities:
• Volume, sub-volume and MIP (maximum intensity
projection) rendering using 2D textures (default) or 3D
textures, if supported1 .
• Spline-based intensity and alpha-blending transfer functions: arbitrary (non-linear) mapping can be
performed. See Figure 1(a).
• Multi-plane reformatting (MPR) and arbitrary cutplanes.
1

The most recent off the shelf graphics cards are optimised for 3D
games and many already support 3D textures, even in laptop computers.

• Segmentation using 3D watersheds and active contours
[8]. See Figure 1(c).
• Mesh generation using the Marching Cubes algorithm
[9] and mesh decimation and simplification [6]. See
Figure 1(d).
Currently, the software is developed using MFC, however its architecture allows easy conversion to UNIX or
LINUX platforms. The strength of 3DView/ARView as
compared to other non-commercial, public domain sofware,
e.g. VTK, VolVis, etc., is that is solely developed for medical visualisation and its applications in surgical navigation
with or without augmented realilty (AR). This implies that
the software is concisely written (from scratch!) whilst its
architecture is easy extendible for different brands of graphics accelerators and for additional functionality related to
medical visualisation. For the same reason, image file formats supported are raw (.BMP, .RAW) and DICOM fornat
(i.e. the standard radiological image format as generated by
commercial scanners and other imaging devices).
The version of 3DView/ARView with basic functionality can be freely downloaded from
http://www2.cmp.uea.ac.uk/˜rjal/.

2. General architecture
ARView is an object-oriented application, written in C++
and utilises the well-known Factory design pattern [5] in
many parts of the system . Rendering classes, image data,
hardware devices and algorithm implementations are contained within discrete class hierarchies. ARView itself communicates with these classes at the topmost (abstract) level.
This makes it easier to implement support for additional
hardware, image formats, or alternative algorithms as future needs dictate. For example, image data - whether mesh

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

(d)
Figure
1.
The
main
window
of
3DView/ARView. Note the ortho-plane image sections (sagittal, coronal and transverse sections) of the image data set on the
left hand side of the window. The volume rendered 3D image is shown in the middle subwindow with (a) showing the ARView volume
rendered VHP female with non-linearly altered alpha blending using a spline-based
transfer function; (b) showing a stereo window suitable for anaglyph (red/green)
glasses or autostereoscopic displays; (c)
Segmented brain using watershed segmentation; (d) polygonised segmented brain using Marching Cubes and subsequent mesh
decimation.

(a)

(b)

(c)

or volume - is encapsulated in the CModel class hierarchy
as shown in Figure 2.
A factory class - CModelFactory - is employed to instantiate the correct concrete class depending on the format of
the image data. To support this, a second class structure is
used to provide a “reader” for each type of image data - see
Figure 3.
ARView uses a static method in CModelFactory, which
takes a filename as an argument and returns an instance of a
CModel sub-class. CModelFactory instantiates the correct
CFileReader derived class and uses this to return a concrete
class derived from CModel. So all of the information necessary to distinguish between different file formats is encapsulated in this factory class. The same pattern is used throughout ARView, in particular where differences are hardware
related. The structure shown in Figure 4 is used to instantiate a “renderer” class at run time, which depends mainly
on the graphics card and driver version of the user’s com-

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

puter. For selections of a particular file format or rendering
option, the factory class encapsulates an optional dialog display which can be used to override the default mechanism.

Figure 2. ARView’s CModel class hierarchy
for image data. The volume class (CVolume)
currently supports the customised .3dv format (C3DVVolume) , the DICOM format (CDcmVolume) and the raw file format (CRawVolume). Polygonal models are supported for
the customised format, endoscope and standard surface meshes.

Figure 4. ARView’s rendering classes. The
default is 2D texture-based rendering which
can be switched to 3D texture-based rendering if the hardware permits. In the above
example, different classes are available for
different manufacturers (e.g. Nvidia, ATI) or
different options, e.g. shading, pixelmap or
palette.

3.4 for further explanation of these devices, and contains no
hardware-dependent code itself. This same design has been
adapted for the major algorithmic elements of ARView. Algorithms used for segmentation, surface extraction, mesh
decimation, contour scanning, edge detection, camera calibration and image registration are abstracted into “pluggable” classes, which can be easily changed at design time
or, in many cases, at run time. This is a major benefit when
assessing alternative techniques during development.

Figure 3. ARView’s CModelFactory class - the
secondary class structure provides a ’reader’
for different image data formats.

A discrete factory class as described previously is not
needed for more straightforward cases such as those related
to the selection of AR specific hardware. Instead a static factory method is implemented in the base class. For example,
the structures for classes encapsulating frame grabbers and
tracking devices are much simpler as is illustrated in Figure 5. By adopting this architecture, ARView itself is not
only very robust, but has a simplified interaction with the
major components of the systems, i.e. the model, the renderer, the tracking device and a framegrabber (see section

Figure 5. Simplified class structure for
straightforward cases such as AR hardware
selection.

3. Augmented Reality microsurgery
3.1. General issues
Stereoscopic augmented reality (AR) surgery has been
researched for almost ten years and some systems have
been already successfully tested in the Operating Theatre
[4]. Unlike surgical navigation (SN) - also known as com-

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

puter assisted surgery (CAS) - systems, which are commercially available, stereoscopic AR based CAS systems
have remained in local laboratories and operating theatres
so far. Apart from the usual technological and ethical difficulties, portability of the software-based interface to dedicated devices used in AR based SN is one of the other
major problems. ARView provides a generic software architecture which can potentially interface with any possible
SN station. This includes generic classes for different types
of tracking devices, visualisation (input) devices, e.g. surgical microscope, monoscopic and stereoscopic rigid and
flexible endoscopes, and different stereoscopic (output) devices, e.g. autostereoscopic displays, shutter glasses and
customised or adapted visualisation devices. In the next sections we will describe the input/output device interface and
additional hardware and software for smooth operation.

3.2. Surgical navigation
Surgical navigation systems are commercially available
under a variety of different brands. They all have more or
less the same configuration: a monitor to display orthoplane images and usually a surface rendered volume; a
tracking device which is usually a passive optical system and corresponding adapted surgical tools with passive
markers; a calibration probe to align patient landmarks with
landmarks pointed on the ortho-slices, typically using least
squares optimisation.

3.3. Augmented Reality (AR)
Augmented Reality (AR) enhanced navigation overlays
the surgical scene with ’virtual’ visual information, typically acquired from X-ray Computed Tomography (CT) or
MR (Magnetic Resonance) image data. In simple terms:
we provide the surgeon with so called ‘X-ray’ vision as
he/she can see anatomical structures in the surgical scene
which are invisible to the naked eye. To overlay the real
scene with a virtual scene, additional visualisation aids are
needed. Which devices are used depends on the type of surgical intervention. For example, when a stereoscopic surgical microscope is used (ENT and neurosurgery), augmented
images can be projected back into the optical system of the
microscope [4] or alternatively, the surgical scene can be
captured by a stereo pair of CCD cameras and projected in
conjunction with the virtual images onto a video-based head
mounted display (HMD) or on an autostereoscopic display
[7]. Other alternatives are wearable microscopes, combining the HMD and surgical microscope in one [3]. Endoscopic surgery deals yet with another visualisation aid - the
endoscope - which can be flexible or rigid and usually is
monoscopic (some stereoscopic versions are currently on

trial). Overlays are best displayed in conjunction with the
video display which comes with the endoscope.

3.4. ARView interface
To accurately overlay augmenting (virtual) images onto
real images, a number of operations are needed: calibration
of the virtual and real camera; registration of the real and
virtual scene; tracking of all moving objects. To overlay in
real-time, a number of optimisation issues have to be considered. We discuss each of these and the above as to how
they are curretnly implemented in ARView in the next sections.
3.4.1. Calibration procedures Calibration of the visualisation device (if any is used) is a crucial procedure in AR
based microsurgery and if done properly can provide a sufficient registration of patient image data for surgical interventions which do not require sub-millimetric accuracy. A
calibration device typically contains a pattern of dots and
has a virtual counterpart in the software. The ARview software allows the specification of calibration objects of arbitrary shapes and patterns. The virtual calibration object and
real calibration object are then matched using standard calibration procedures such as the Tsai calibration procedure
[10] and improvement on the focal length and z-coordinate
using stereo properties (provided stereo is available) using
the fundamental matrix. Calibration dots are automatically
detected and labelled. The procedure is performed for each
viewpoint if a stereoscopic visualisation device is used. Figure 6(a) shows a segmented version of a real calibration object after video capture and the overlay with the virtual calibration object after calibration - Figure 6(b).
Calibration for endoscopes is slightly more demanding
as the endoscope is a freehand device. We are currently developing an improved version of the calibration platform,
which relates an initial position of the endoscope directly to
the calibration object, thus avoiding tracking devices to find
out this transformation. ARView’s class structure allows to
easily add any type of calibration procedure, completely tailored to the corresponding ‘hardware’ used for this purpose.
3.4.2. Registration of patient data Calibration is sufficient to do registration of patient image data if the calibration object is pre-operatively scanned, whilst fixed to the patient. Any fixation device can be used as the software only
requires a scan of the calibration object at a fixed and known
position on the scanned patient’s body part. In the CASSPAR (Computer Assisted Surgery, Simulation and Planning
using Augmented Reality) project2 this device is a VBH
(Vogele-Bale-Hohner) mouthpiece which is attached to the
2

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

This project is led by the first author and funded by the EPSRC (Engineering and Physics Sciences Research Council).

(a)
Figure 7. The VBH mouthpiece with attached
calibration object.

512MB RAM and an ATI Radeon 9700 Pro are:
• Real-time video capture: 7-10Hz;
• Data polling (tracking): 35Hz;
• Volume rendering (full screen): 10-15Hz;
• Augmented image generation (overall): 4Hz.

(b)
Figure 6. Segmented and labelled calibration
object image (a) and the virtual and real calibration object overlay after calibration (b).

patient’s palate using a vacuum [1] - see Figure 7. The (already calibrated) virtual calibration object is now registered
with the scanned calibration object using the ICP (iterative
closest point) algorithm [2]. Once the registration is completed the virtual and real image data can be displayed.
3.4.3. Overlays Overlaying the real and virtual images of
the patient can be done in many ways. The most basic way
to do so is cross-dissolving, also referred to as cross-fading.
The real image and virtual image are allocated an alpha
value each, where both alpha values sum to one. This AR
overlay method is shown in Figure 8. More advanced approaches, utilising techniques such as depth peeling and
real-virtual image strobing are currently being developed.
The real-time issue is however more crucial. The following performances on a PC with 1.4GHz clock speed,

Currently, the video capture and the volume rendering
result in a rather poor overall update frequency of 4Hz in
augmented image generation. Serious improvements can be
made by using sub-volume rendering in conjunction with
virtual overlays. Also, the use of two framegrabber cards instead of one should improve the video capture update rate.
Alternatively, using CCD cameras with firewire would increase the capture rate to 70Hz (currently 25Hz) and would
eliminate the use of framegrabber hardware. Once these optimisations are in place, the overall performance should be
significantly improved.
3.4.4. Tracking of moving objects Though most surgical
navigation systems use optical tracking, any tracking technology can be interfaced with ARView. Restriction of the
number of objects tracked is solely determined by the tracking hardware. The software allows an unlimited number of
tracked objects. However, for passive optical tracking devices (which are commonly used in CAS), specific care has
to be taken with occlusion problems (probe out of line of
sight) and ambiguities (two tracked objects are confused).
The solution is to design different passive marker configurations which minimises conflict. These configurations are
currently ‘hardwired’ into the code, though a minor adaptation would allow this to be done using script files or external plug-in modules.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

4. Conclusion
ARView is a generic software framework for augmented
reality microsurgery solutions. It is built on a standard PCbased volume rendering software. Its generic architecture
makes it independent of the typical hardware used in different fields of microsurgical interventions, be it endoscopic,
microscope based or even interventions which do not use
enhanced visualisation. Further development will concentrate on improving the user interface to speed up tasks such
as calibration and registration, which otherwise may slow
down the surgical intervention - hence crucial to the general
well-being of the patient. The desired goal of real-time performance will need both hardware (CCD cameras) and software (sub-volume rendering) updates.

References
[1] R. Bale, M. Vogele, et al. VBH head holder to improve
frameless stereotactic brachytherapy of cranial tumors. Computer Aided Surgery, 2(5):286–291, 1997.
[2] P. Best and N. Mackay. A method for registration of 3-D
shapes. IEEE Trans.PAMI, 14(52):239–256, 1992.
[3] W. Birkfellner, M. Figl, K. Huber, F. Watzinger, F. Wanschitz, R. Hanel, A. Wagner, D. Rafolt, R. Ewers, and
H. Bergmann. The varioscope ar - a head mounted operating microscope for augmented reality. volume 1935 of Lecture Notes in Computer Science, pages 869–877. Springer,
September 2000.
[4] P. Edwards et al. Design and evaluation of a system
for microscope-assisted guided interventions (magi). IEEE
Trans. Med.Imag., pages 1082–1093, 2000.
[5] E. Gamma, R. Helm, R. Johnson, and J. Vlissides. Design Patterns: Elements of reusable object-oriented software.
Addison-Wesley, 1995.
[6] M. Garland and P. Heckbert. Surface simplification using
quadric error metrics. In Proceedings of SIGGRAPH 97,
pages 209–216, 1997.
[7] R. Lapeer, P. Chios, G. Alusi, A. Linney, M. Davey, M.K,
and A. Tan. Computer-assisted ent surgery using augmented
reality: preliminary results on the caesar project. In Medical Image Computing and Computer-Assisted Intervention MICCAI’00, volume 1935 of Lecture Notes in Computer Science, pages 849–857. Springer, September 2000.
[8] R. Lapeer, A. Tan, and R. Aldridge. Active watersheds: combining 3d watershed segmentation and active contours to extract abdominal organs from mr images. In Medical Image Computing and Computer-Assisted Intervention - MICCAI’02, volume 2488 of Lecture Notes in Computer Science,
pages 596–603. Springer, September 2002.
[9] W. Lorensen and H. Cline. Marching cubes: A high resolution 3D surface construction algorithm. Computer Graphics,
21(4):163–169”, 1987.
[10] R. Tsai. A versatile camera calibration technique for highaccuracy 3D machine vision metrology using off-the-shelf tv
cameras and lenses. IEEE Journal of Robotics and Automation, 3(4):323–344, 1987.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

(a)

(b)

(c)
Figure 8. (a) video-captured stereo pair of images of a baby skull; (b) the corresponding
virtual image data of the brain; (c) the overlaid stereo pair of images.

