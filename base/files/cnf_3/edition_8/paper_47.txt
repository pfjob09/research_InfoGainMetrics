Keyboard Encoding of Facial Expressions
Nicoletta Adamo-Villani
Department of Computer Graphics
Technology, Purdue University
nvillani@tech.purdue.edu
Abstract
The paper focuses on the development of a humancomputer communication method which utilizes the
user’s typing skills to control the facial expression of a
computer generated three-dimensional face. The method
is based on the realization that the human face is a
movable and deformable system with 26 degrees of
freedom (the same number as the letters of the English
alphabet). Therefore it is possible to create a
parameterized graphical facial model confined to a set of
26 parameters each one controlled by a letter key. The
method is an extension of the KUI technique [1][2][3]
recently developed to encode hand gestures.

Gerardo Beni
Department of Electrical Engineering,
University of California, Riverside
beni@ee.ucr.edu
Nahas, Huitric and Sanintourens [21] developed a face
model using B-spline surfaces.
Currently facial surfaces are controlled and
manipulated using one of three basic techniques: (1) 3D
surface
interpolation;
(2)
ad
hoc
surface
parameterization; (3) physically based techniques with
pseudo-muscles.
In terms of human computer interaction (HCI), the
emphasis of facial expression research has been on
computer vision techniques for facial configuration input
and processing, and categorization of facial expressions
relevant to enhancing communication between man and
machine [22][23][24].

1. Introduction
The object of the paper is the demonstration that it is
possible to create a simple human-computer
communication of messages encoding facial expressions.
The idea is based on the realization that any significant
facial articulation can be represented with a set of 26
parameters, the same number as the letters of the English
alphabet. Since touch typing is an easily acquired and
widespread skill, it is possible to conveniently input
messages encoding facial configurations if each letter
key of the keyboard corresponds to one degree of
freedom of the face.
Modeling and animation of the human face has been
object of extensive research in the past thirty years. The
first work in developing facial models was carried out in
the early 70’s by Parke [4][5][6][7] who developed the
first interpolated and the first parametric 3D facial
models. Gillenson [8] developed the first interactive two
dimensional face models and Platt and Badler
[9][10][11] proposed the first muscle action based facial
models. All these models made use of the Facial Action
Coding System (FACS) [12][13] as the basis for facial
configuration control.
In the last few years there has been considerable
activity in the development of new facial models. Waters
and Terzopoulos [14][15][16][17][18][19] proposed a
series of physically based pseudo-muscle driven facial
models. Magnenat-Thalmann, Primeau and Thalmann
[20] presented the Abstract Muscle Action model and

In this paper we demonstrate that the human face,
which consists of 44 bilaterally symmetrical muscles
(muscles of facial expression and muscles of mastication
[25] ), can be modeled with muscle (or group of muscles)
actions totaling 22 degrees of freedom + 4 degrees of
freedom required to control the direction of the gaze.
Thus, it is possible to create a facial model confined to a
parameter space not excessively large in terms, not only
of computer representation, but also of human encoding.
It is this characteristic that has suggested the approach to
facial expression encoding described below.
A convenient facial configuration encoding is
applicable to many practical tasks: (1) teaching the facial
components (non-manual markers) of American Sign
Language. The 26 facial parameter set could be easily
optimized for keyboard encoding of facial expressions
specific to the grammar of ASL; (2) Human Computer
Interface, i.e., the possibility of building computer
interfaces which understand and respond to the
complexity of the information conveyed by the human
face. Currently information has been conveyed from the
computer to the user mainly textually or visually via ad
hoc images; (3) Testing and quantitative calibration of
vision algorithms for the analysis and recognition of
video data involving faces; (4) Communication with
patients suffering from textually impaired syndromes,
e.g., severe dislexya; (5) Development of socially adept
interfaces for the communication of social displays in the
acknowledgement of actions by other people, e.g., by
smiling in response to intention to purchase a certain

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

item; (6) web deliverable 3D character animation. A
simple set of (26) component vectors would represent a
facial configuration and could be transmitted with very
low bandwidth to animate complex face models held at
the receiver site.

input, 94% of the FACS Action Units (we have not
considered those relative to head orientation) with our 26
(22+4) parameter set.

2. Methods
2.1. Determination of the 26 facial parameters
The human face is a complex structure of muscles
whose movements pull the skin, temporarily distorting
the shape of the eyes, brows, and lips, and the
appearance of folds, furrows and bulges in different areas
of the skin [24]. Such muscle movements result in the
production of rapid facial signals (facial expressions)
which convey four types of messages: (1) emotions; (2)
emblems - symbolic communicators, culture-specific
(e.g., the wink); (3) manipulators - manipulative
associated movements (e.g., lip-biting); (4) illustrators movements that accompany and emphasize speech (e.g.,
a raised brow)[12].

Figure 1. Identification of the 22 facial regions

2.2. Description of the coding
Given the complexity of the human face, the first
challenge faced by this research has been the
determination of a relatively small set of facial
parameters (26) able to encode any significant facial
expression of a 3 dimensional computer generated face.
There are several approaches to developing facial
parameters including observation of the surface
properties of the face and study of the underlying
structure, or facial anatomy. However, which parameters
are best included in a simple model of facial expression
remains unresolved [26]. Below we propose a new set of
parameters.
The eyes and mouth are of primary importance in
facial expressions thus many of our facial parameters
relate to these areas. We have modeled a 3 dimensional
face as a continuous polygonal mesh and we have
identified 22 regions on the mesh. The definition of the
regions is based on the anatomy of the face and in
particular on the location of the muscles of Facial
Expression [23]. Figure 1 shows the 22 regions identified
on the 3D face model. Each region of mesh is controlled
by one joint. The translation of the joint produces a
proportional (quasi linear) deformation of the
corresponding skin region. The type of deformation has
been determined based on the observation of the change
of facial shape produced by the action of the muscle or
set of muscles located in that region. The Facial Action
Coding System (FACS) [12][13] has provided us with a
complete list of possible muscle contractions or
relaxations performable on a human face with relative
induced deformations. FACS lists all the basic actions
(called Action Units or AUs) that can occur on the
human face (e.g., Inner Brow Raiser, Lip Tightener, Chin
Raiser) and describes a facial expression as a
combination of specific AUs. In the next section we
demonstrate that it is possible to encode, via keyboard

Using MEL (Maya Encrypted Language) we have
created a program that encodes the facial expression of
the above described three dimensional face by mapping
each letter key of the keyboard to a degree of freedom of
the face (lower case letters induce positive translations of
the joints and positive rotations of the eyes, upper case
letters induce negative translations of the joints and
negative rotations of the eyes). Figure 2 shows the
locations of the 22 facial joints, on the left, and a
rendered image of the face with the joints’
transformation (22) and eye rotation parameters (4)
mapped to the 26 letters of the alphabet, on the right.
Table 1 shows the deformation output produced by each
letter key. We note that letter ‘z’ controls the
deformation induced by the mentalis muscle as well as
the rotation of the jaw.

Figure 2. The 22 facial joints, on the left; mapping
of the letters to the 26 dof of the face, on the right.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

Via keyboard input the face can be configured to
attain any expression: by touching a letter key the user
translates the corresponding joint a pre-specified number
of units along an axis. The letters “G H I J” control the
rotation of the eyes and therefore the direction of the
gaze. The eyes have been modeled as two separate
spheres with procedural mapped pupils. The rotation of
each sphere around the Y axis causes the eye to look left
or right; the rotation of each sphere around the X axis
causes the eye to look up or down. The transformation
“step” induced by each key touch can be changed to
increase or decrease precision. Figure 3 shows an
example of keyboard encoding of the six basic facial
expressions commonly used in animation (anger, joy,
surprise, fear, disgust, sadness), table 2 shows the letter
codes corresponding to each expression.

Emotion
Sadness
Joy

Letter Code
AABBccddklZPPQQWXYtuvv
pppqqqrrrssstttuuuvvvZZWWXXYYYnnoo
klccddaabb
Fear
KLefcccdddAAABBBPPPQQQZZZWWW
XXXYYYYYtu
Surprise
KLefcccdddAABBPQZZWWWXXXYYY
RStuvv
Disgust
CCCDDDAABBkklltttttuuuuuvvvRRSSZY
XWno
Anger
CCCDDDklZZZWWWWXXXXYYYYYtt
uuvvab
Table 2. Keyboard encoding of the six basic facial
expressions.
Table 3 shows the keyboard encoding of the Action
Units of the Facial Action Coding System (the AUs
relative to head orientation are not included). In the
example below the eyes rotation is quantized in steps of
5 degrees and the joints translation is quantized in steps
of 0.15 units.

3. Discussion of Results

Table 1. The facial deformation output produced by
each letter key.

The keyboard encoding method presents several
advantages including: (1) Simplicity of user input
requiring no additional input hardware (e.g. video
cameras or motion capture devices); (2) Familiarity of
the input method which requires no additional skills or
learning time; (3) Accuracy: although the method uses a
discretized representation of joints translation and eye
rotation, the resolution of the quantization can be
adjusted to configure the face with high precision; (4)
Low bandwidth for storage and transmission: facial
configuration/animation data can be stored in text files of
minimum size, exported cross platform or transmitted via
internet; (5) Easy extension to voice input.
There are some limitations to the method presented
here. The first limitation is the restriction to a particular
facial skeletal structure. While the method is applicable
to any polygonal facial model rigged with a 22-joint
skeleton, we have left to future developments the
extension of the method to different facial skeletal
setups.
Another limitation is the fact that the 22 regions
discussed above, with relative deformations, need to be
manually specified when the face is constructed. Future
work involves the implementation of a method of
automatically applying the 22 regions with relative
deformations to any polygonal facial model. Such
method would involve the development of a
categorization of face models based on geometrical
characteristics and skeletal structures.

Figure 3. The six basic facial expressions.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

AU
1

Description
Inner Brow
Raiser

Code
ccddAB

2

Outer Brow
Raiser

4

Image

AU
20

Description
Lip
Stretcher

Code
PQrrsstuvw
wxxyyyy

aabbbCDEF

22

Lip Funneler

PQRRSStuv
vvvwYYY

Brow Lowerer

CCDD

23

Lip
Tightener

PQRRSSTU
Vwxy

5

Upper Lid
Raiser

cdef

24

Lip Pressor

6

Cheek Raiser

CDEFkkllnn
oo

25

Lips part

PPPQQQRR
SSTTUUV
wx
PQtuvvYZZ

7

Lid Tightener

CDEFlk

26

Jaw Drop

PPQQtuvv
WWXXYY
YZZZ

9

Nose wrinkler

Unable to
encode

27

Mouth
Stretch

10

Upper Lip
Raiser

notttuuvvvw
wxxyyy

28

Lip Suck

ZZZZZYY
YYYYWW
WWWXXX
XXvv
PQrsTUVw
xyy

11

Nasolabial
Deepener

PPQQrswxy

41

Lid Droop

EEFF

12

Puller

PQwxyrstvu

42

Slit

EEFFkl

13

Cheek Puffer

NORS

43

Eyes Closed

EEEFFFkl

14

Dimpler

Unable to
encode

44

Squint

CCDDEEFF
kkllnnoo

15

Lip Corner
Depressor

PPPPQQQQ
RSTUvyy

61

Eyes turn
left

GHIIJJ

16

Lower Lip
Depressor

PPQQTUV
Y

62

Eyes turn
right

GHiijj

17

Chin Raiser

PPPQQQTU
yyzzz

63

Eyes up

efgghh

18

Lip Puckerer

PQRSvvww
wwxxxyyzz

64

Eyes down

EFGGGHH
HKL

Image

Table 3. Keyboard encoding of the FACS Action
Units

are not included in the method. The motion/inclination of
the head also conveys emotions, feelings and meaning.

Another limitation so far is the restriction to a static
head and face. Although the model of the head can be
dynamic while retaining the encoded facial expression,
other expressions obtainable by re-orientation of the head

The extension to include this motion in the interface is
straightforward and will be considered in a future
publication where keyboard encoding of facial
expressions, hand gestures and body motions are

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

combined to provide a complete human body language
representation.
Apart from the development of these current
limitations, future applications of the method can
conceivably include client-server operation via the
internet.

References
[1]

Adamo-Villani, N. and G. Beni . "A new method of hand
gesture configuration and animation". Journal of
Information, 2004, Vol.7, No.3.
[2] Adamo-Villani, N. and G. Beni . “Keyboard Encoding of
Hand Gestures”. Proceedings of HCI International - 10th
International
Conference
on
Human-Computer
Interaction, Crete, June 2003. Vol. 2, pp. 571-575
[3] Adamo-Villani, N. and G. Beni. “Design and
benchmarking of Human-Computer Interface for
keyboard encoding of hand gestures”. Proceedings of
EPFL, IST, IEEE First International Workshop on
Interactive
Rich
Media
Content
Production:
Architectures, Technologies, Applications, Tools.
Switzerland, October 2003, pp. 149-159
[4] Parke, F. I.. Computer generated animation of faces.
University of Utah, Salt Lake City, June 1972, UTECCSc-72-120.
[5]
Parke, F. I. Computer generated animation of faces.
ACM Nat'l Conference, 1972, 1, 451-457.
[6] Parke, F. I. A parametric model for human faces.
University of Utah, Salt Lake City, Utah, December
1974, UTEC-CSc-75-047.
[7] Parke, F. I. A model of the face that allows speech
synchronized speech. Journal of Computers and
Graphics, 1975, 1, 1-4.
[8] Gillenson, M.L. The interactive generation of facial
images on a CRT using a heuristic strategy. Ohio State
University, Computer Graphics Research Group, The
Ohio State University, Research Center, 1314 Kinnear
Road, Columbus, Ohio 434210, 1974.
[9] Platt, S. M. A System for Computer Simulation of the
Human Face. The Moore School, Pennsylvania, 1980.
[10] Platt, S. M. A structural model of the human face. The
Moore School, Pennsylvania, 1985.
[11] Platt, S. M. & Badler, N. I. Animating facial expressions.
Computer Graphics, 1981, 15(3), 245-252.
[12] Ekman, P. & Friesen, W. V. Facial action coding
system: A technique for the measurement of facial
movement. Palo Alto, Calif.: Consulting Psychologists
Press, 1985.

[13] Ekman, P. & Oster, H. Facial expressions of emotion.
Annual Review of Psychology, 1979, 20, 527-554
[14] Waters, K.. Expressive three-dimensional facial
animation. Computer Animation (CG86), October 1986,
49-56.
[15] Waters, K. A muscle model for animating threedimensional facial expressions. Computer Graphics
(SIGGRAPH'87), 21(4), July, 17-24.
[16] Waters, K. The computer synthesis of expressive threedimensional facial character animation. Middlesex
Polytechnic, Faculty of Art and Design, Cat Hill Barnet
Herts, June. 1988, EN4 8HT.
[17] Waters, K. & Terzopoulos, D. A physical model of facial
tissue and muscle articulation. Proceedings of the First
Conference on Visualization in Biomedical Computing,
May 1990, 77-82.
[18] Waters, K. & Terzopoulos, D. Modeling and animating
faces using scanned data. Journal of Visualization and
Animation, 1991, 2(4), 123-128.
[19] Waters, K. & Terzopoulos, D. The computer synthesis of
expressive faces. Phil. Trans. R. Soc. Lond., 1992,
355(1273), 87-93.
[20] Magnenat-Thalmann, N., Primeau, N.E., & Thalmann,
D. Abstract muscle actions procedures for human face
animation. Visual Computer, 1988, 3(5), 290-297.
[21] Nahas, M., Huitric, H., & Sanintourens, M. Animation
of a B-spline figure. The Visual Computer, 1988, 3, 272276.
[22] Xi, D., Podolak, I.T., Lee, S.W. Facial Component
Extraction and Face Recognition with Support Vector
Machines. Fifth IEEE International Conference on
Automatic Face and Gesture Recognition, Washington
D.C., 2002.
[23] Huang, T. S. & Orchard, M. T. Man-machine interaction
in the 21st century: New paradigms through dynamic
scene analysis and synthesis, Proc. SPIE Conf. on Visual
Communications and Image Processing '92, Vol. 1818
(pp. 428-429). Nov. 18-20, Boston, MA.
[24] Krinidis, S., Buciu, I., Pitas, I. Facial Expression
Analysis and Synthesis: A survey. Proceedings of HCI
International-10th Int. Conference on Human-Computer
Interaction, Crete 2003. Vol. 4, pp. 1432-1436.
[25] Hamilton, W.J. Human Anatomy. The C.V. Mosby
Company, 1976.
[26] Ekman, P., Huang, T. S., Sejnowski, T.J., Hager, J.C.
Final Report to NSF of the Planning Workshop on Facial
Expression Understanding, 1992.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

