A Visualisation Approach for Collaborative Planning Systems Based on Ontologies
Natasha Queiroz Lino and Austin Tate
Centre for Intelligent Systems and their Applications
School of Informatics, The University of Edinburgh
Appleton Tower, Crichton Street, EH9 9LE, Edinburgh, UK
{Natasha.Queiroz, a.tate}@ed.ac.uk
Abstract
In the last decades, many advances have been made
in intelligent planning systems. Significant improvements
related to core problems, providing faster search
algorithms and shortest plans have been proposed.
However, there is a lack in researches allowing a better
support for a proper use and interaction with planners,
where, for instance, visualization can play an important
role. This work proposes a way to address the problem
of visualization in intelligent planning systems via a
more general approach. It consists in an integrated
ontology set and reasoning mechanism for multimodality visualisation destined to collaborative planning
environments. This framework will permit organizing
and modeling the domain from the visualization
perspective, and give a tailored support for presentation
of information.
Keywords--- Visualisation, Intelligent Planning,
Ontologies.

1. Introduction
Visualisation is an aspect that is still not much
explored in intelligent planning systems. Although
efforts have been made in improving and developing
new techniques and approaches in the field of intelligent
planning (such as, more efficient search algorithms), few
works address the problem of visualisation.
Among the existing planning systems, some well
know planners don’t have a solution for visualisation
proposed yet, others only consider a unique approach
when this solution sometimes is not appropriate for every
situation. In this way, how will users make the most of
planning systems if they don't have appropriate support
for interaction with them? This problem is even more
enhanced when considering mixed initiative planning
systems, where human agents, that are collaborating in
the process, have different backgrounds, are playing
different roles in the process, are using different devices
to interact with the planner, and have different
capabilities and responsibilities, etc.

Visualisation can play two main crucial roles in
planning systems: allow proper interaction between the
planner and its users, and permit collaboration in the case
of mixed-initiative planning. However, the existing lack
of more elaborated approaches for the problem of
visualisation in planning systems compromises a broaden
application and use of such systems; and their
application and use in real world problems and
situations, where assisted planning services can be
applied and supported by more sophisticated
visualisation approaches.
To address the problem we propose a general
framework for visualisation in planning systems that will
give support for a more appropriate visualization
mechanism. The general approach that is being proposed
for dealing and modeling this problem consists in an
integrated ontology set and reasoning mechanism for
multi modality visualisation destined to collaborative
planning environments. This framework is divided in
two main parts: a knowledge representation aspect and a
reasoning mechanism. In the knowledge representation
aspect of this work, the ontology set will permit organise
and model the complex problem domain from the
visualisation perspective. The reasoning mechanism will
give support to reasoning about the visualisation problem
based on the knowledge bases available for a realistic
collaborative planning environment. Among the
knowledge bases available, are included: agents
preferences,
devices
characteristics,
planning
information, visualization modalities knowledge base,
etc.
Furthermore, for the solution proposed, it has been
detected opportunities in the problem area for the
integration of planning applications with mobile
computing and visualization approaches. This integration
can add value to real world applications and have been
applied in other artificial intelligence areas. More details
of artificial intelligence and mobile computing
integration can be found in (Lino et al., 2003).
The requirements considered in the approach
proposed are: to be general, technologically independent,
and extensible. General because it will consider several
aspects of planning visualisation, and should be applied

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

in general cases. Technologically independent due to the
fact that part of the knowledge and expertise for building
the framework will come from current available
technology, although, the framework should be
extensible enough to permit that future technologies
would be applicable. Also, we intend to model the
problem investigating the use, as knowledge
representation tools, real standards, such as XML (W3C,
2003) RDF (W3C, 2003) and related technologies. The
use of markup languages will permit extensibility of the
framework and its application on the Semantic Web.

2. Related Work
In mixed-initiative planning research, natural
language and direct manipulation have been the most
adopted approaches for permitting visualisation of
information and interaction between the planner and the
user agents.
Natural language processing research has been
applied in the TRAINS (Ferguson and Allen, 1996) and
TRIPS (Ferguson and Allen, 1998) systems, where
interaction between the user and the system is viewed as
a form of dialog. TRAINS and TRIPS approaches for
interaction consist in being conversational systems that
provides problem-solving assistance.
Direct manipulation and GUI (Graphical User
Interface) techniques dominate modern interactive
software. However in planning systems there is a lack of
mixed initiative interaction through visual medias.
Nevertheless, some approaches has been explored this
aspect, as is the case of (Amant et al., 2001) (Pegram et
al., 1999). In these works communication between
human planners and intelligent planning systems is
proposed via shared control of a three-dimensional
graphical user interface, using an ecological view of
human-computer interaction. The planning environment
is focused on strategic and physical planning problems in
AFS (Abstract Force Simulator) simulator.
The O-Plan (Tate and Drabble, 1995) planning
system user interface approach focuses on the
differentiation of the various roles played by users. The
system gives support for user agents to interact with the
planning agent, and also with the others system agents
that compose the O-Plan architecture, such as: task
assign (in the strategic level), planner (in the tactical
level), and execution system (in the operational level).
The support is given by providing different views of plan
structure that can be technical plan views (charts,
structure diagrams, etc.), and domain-oriented views,
called world views (simulations, animations, etc.). In
(Tate and Drabble, 1995) an useful analysis of basic
requirements for graphical interaction in planning
systems is also provided.
In brief, although spoken dialog is a natural and
powerful way of human-computer interfacing, it has also
being argued that speech recognition has limitations
(Shneiderman, 2000) in some aspects: viability to present
information due to slow processing, difficult to review
and edit information, etc. Nevertheless speech is suitable

in specific situation, for instance, for interaction for blind
and motor-impaired users, messages, and alerts in busy
environments.
However, what it is missing is a more general way
to approach visualization in mixed initiative planning
that will consider several aspects and different forms of
visualisation. It is necessary consider the related aspects
including: collaboration, multi agents and their roles,
capabilities and responsibilities, multi modalities of
visualisation, devices diversity, type of planning
information, etc.
A related approach (but not for planning domains) is
under development by the W3C (W3C, 2003), the
Multimodal Interaction Activity. The objective is to
adapt the Web to allow multiples forms of interaction,
enhancing interaction between humans and computers
considering the paradigms anywhere, any device and any
time. The way intended to archive the objective is via the
developing of standards for a new class of mobile
devices that support multiples modes of interaction. Our
approach however, despite be also based on W3C
standards and markup languages, is more specialised
since is concerned with collaborative environments of
intelligent planning and related aspects.

3. A General Framework for Multi-Modality
Visualisation in Mixed-Initiative Planning
To propose a new solution for visualization in
intelligent planning systems we are building a general
framework that will permit describe and model the
problem and provide adequate reasoning about
visualization, intelligent planning components and
concepts involved.
The proposition of an ontology set and reasoning
mechanism for describing and reasoning about multimodalities visualization intends to be a general and
extensible framework destined to collaborative planning
environments. The framework is divided in two parts:
knowledge representation and reasoning mechanism.
In the knowledge representation aspect of this work,
the ontology set will permit describing planning
information, in the context of a realistic collaborative
planning environment. The ontology set will include
vocabulary for information about agents (members of
virtual organisations), virtual organisation characteristics
and relationships, information about devices (mobile
devices, desktops) for visualization display (capabilities,
resources available, screen characteristics, etc.), and also
a vocabulary of visualisation modalities.
The reasoning part of this work will be able to
reason and map, based on the ontology set, from a global
scenario description to specific, tailored and suitable
visualisation categories and modalities.
This work has a potential to be extended to the
Semantic Web. So, in addition, correlations and
applications of this work on the Semantic Web are being
considered and investigated. With this propose, the
development of the ontologies is considering Semantic
Web concepts that will permit a natural extension and

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

adaptation of the proposed ontology for applications in
the Semantic Web.

3. An Ontology Set
In this section it is being described in more details
the ontology set. First it will be explained about which
scope each ontology will be concerned. Second, the
knowledge representation approach investigated for
building the ontology set will be discussed. Later,
considerations will be made about the ontology set
integration. Finally, examples will be demonstrated.
An alternative for multi-modality visualisation is
proposed in (Moran et al., 1997). In this approach, a
multi agent architecture, called Open Agent Architecture
(OAA), is used to support multimodal user interfaces.
The Open Agent Architecture is a multi agent system
that supports the creation of applications from agents,
where part of the focus is on the user interface of the
applications. The supported modalities are: spoken
language, handwriting, pen-based gestures, and
Graphical User Interface (GUI). The user can interact
using a mix of modalities. When certain modality is
detected by the system, the respective agent receives a
message and processes the task. In this way, the
Facilitator Agent is the key for cooperation and
communication between agents, since it’s his job register
capabilities of agents, receive requests, and delegate the
agents to answer requests. However, the Facilitator
Agent can be a potential bottleneck in this approach.
In the past 2003, 2002, and 2001 International
Conferences on Information Visualisation, a session has
been dedicated to visualisation and ontologies. Several
works are proposing the use of ontologies in
visualisation problems, ant their application in the
Semantic Web. In (Telea et al., 2003) is proposed a
graph visualisation tool that allows construction and
tuning of visual exploratory scenarios for RDF (Resource
Description Framework) data. Other approach (Fluit et
al., 2002) shows how visualisation of information can be
based on ontological classification of that information,
by a cluster map visualisation.
The approach proposed here has similarities and is
inspired in a mix of concepts of these last mentioned
works. It intends to be a multi-modality visualisation
framework for intelligent planning systems based on
ontological representation.

3.1 Ontology Set Description
The ontology set will be composed by the following
sub-ontologies:
3.1.1. Multi-Modality Visualisation Ontology.
This ontology is able to express the different modalities
of visualization considered in the approach, but the
ontology vocabulary is extensible to express new
modalities that eventually will be necessary in the
problem domain or due to new technologies. Each

visualization category is described following its
characteristics.
3.1.2 Plan Ontology: <I-N-C-A> Ontology. The
ontology used to represent plans is based on the <I-N-CA> ontology (Tate, 2001). <I-N-C-A> (Issues-NodesConstraints-Annotations) is the I-X Project (Tate, 2000)
ontology that permit represent a product, such as a plan,
as a set of constraints on the space of all possible
products in the application domain. In order to be
integrated, extensions concerning the visualisation
perspective are considered in the definition of this
ontology.
3.1.3 Devices Ontology. The devices ontology
permits describe the types of devices being targeted, for
example, mobile devices, such as, cell phones, PDAs,
pocket computers, etc. The representation will be made
in terms of their characteristics: screen sizes, features,
capabilities, etc. However, the representation is intended
to be generic enough to permit easy extensions to future
technologies. This is a positive aspect, mainly because
the mobile computing area is presenting a very fast
development.
3.1.4 Agents Organisation Ontology. This
ontology permits represent agents organizations,
including agents’ relationships (superiors, subordinates,
peers, contacts, etc.) aspects, agents’ capabilities and
authorities for performing activities, etc. This ontology is
inspired in some of the I-Space concepts. I-Space is the
I-X project concept for managing agents structures and
relationships in a virtual organization.
3.1.5 Agents Mental States Ontology. Based on
BDI (Beliefs-Desires-Intentions) (Rao et al., 1995)
approach, this ontology permits represent agents’ mental
states.
3.1.6 Environment Ontology. This ontology allows
the representation of information about the general
scenario. For instance, localisation of agents in terms of
global positioning (GPS), etc.

3.2 Knowledge Representation Approach
The knowledge representation approach is based on
XML - Extensible Markup Language (W3C, 2003) and
related technologies, following W3C standards. In a first
phase, markup languages are used as knowledge
representation tools, but a Semantic Web (W3C, 2003)
application will not be aimed.
These technologies filled a gap, providing first a
syntax for structured documents (XML, XML Schema),
and second a simple semantic for data models (RDF –
Resource Description Framework), that evolved for more
elaborated schemas (RDF Schema, OWL). RDF Schema
permits semantics for generalization-hierarchies of
properties and classes. OWL – Web Ontology Language,
adds more vocabulary with a formal semantics, allowing

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

more expressive power, permitting, for example, express
relations between classes, cardinality, equality, and
characteristics of properties, among others.
OWL (W3C, 2003) is an evolution of DAML+OIL
(McGuinness, et al., 2002) and is aimed for use when is
necessary to process information, and not only present it,
because facilitates machine interpretability via its
additional vocabulary and formal semantics. OWL is
divided in three sub-languages, with increasing
expressiveness: OWL Lite, that provides classification
hierarchy and simple constraints; OWL DL that has
maximum
expressiveness
with
computational
completeness and decidability, founded by description
logics; and OWL Full that allows maximum
expressiveness and syntactic freedom of RFD, but
without computational guarantees.
The OWL ability of processing the semantic of
information fits our needs to be used in the general
framework, to build the integrated ontology set, and
reasoning mechanism in the problem domain. Hence, the
resulting framework is considering the semantic of
information available, and it will be capable of reasoning
based on real standards.
An important aspect to consider, however, is that the
use of W3C standards doesn’t mean necessarily a
Semantic Web application. Nevertheless, further
investigation will be carried on, in order to consider the
framework extension for application on the Semantic
Web. One potential application is to provide mechanisms
for a semantic and automatic knowledge bases update.
For example, directions can be formalised to build agents
for mobile devices profile update.

4 A Reasoning Mechanism
The set of ontologies allows the development of
reasoning mechanisms related to visualization in
collaborative planning environments. In this section we
will be given an example of reasoning considering
devices profiling.
Current device’s profiles approaches available
permit express information about device’s features, for
example, via a device profile it is possible to obtain the
information that a specific device is Java enabled. It
means that it is possible to run Java 2 Micro Edition
(J2ME) in this device. However this type of information
can have a more broaden meaning when considering
visualization of information in mobile devices. What is
that really means a device be Java enable, apart from
being able to run J2ME? Is J2ME supporting Java 3D for
instance?
To answer questions like that is that the ‘Devices
Ontology’ will be used, permitting a proper reasoning for
tailored information visualization and delivery.
Figure 1 has extracts of the ‘Devices Ontology’,
using OWL as knowledge representation language. It
shows the definition of classes and properties that permit
the Java question example of the previous paragraph be
represented, and used to reason upon. The class
PDADevice allows the instantiation of individuals that

represent a particular device. Through the JavaEnable
property defined for this class, it’s possible to express if
a specific PDA is Java enable. The unique instance of the
J2ME class specifies the features of the J2ME platform.
For instance, this class has the property 3DSupport that
expresses the semantic of supporting features of 3D
visualization models or not.
...
<!-- Information about the Ontology -->
<owl:Ontology rdf:about="">
<rdfs:comment>Devices Ontology</rdfs:comment>
</owl:Ontology>
...
<!- Classes Definition -->
<owl:Class rdf:ID="PDADevice">
<rdfs:comment>An instance of the class
PDADevice represents the details of a
particular device.</rdfs:comment>
<rdfs:subClassOf
rdf:resource="#CLDCConfigDevices"/>
</owl:Class>
<owl:Class rdf:ID="J2ME">
<rdfs:comment>The instance of the class J2ME
express the features of this
platform.</rdfs:comment>
<rdfs:subClassOf>
<owl:Restriction>
<owl:cardinality rdf:datatype=
&xsd;nonNegativeInteger">1</owl:cardinality>
</owl:Restriction>
</rdfs:subClassOf>
</owl:Class>
...
<!- Datatype Properties -->
<owl:DatatypeProperty rdf:ID="JavaEnable">
<rdfs:domain rdf:resource=#PDADevice" />
<rdfs:range rdf:resource="&xsd;boolean"/>
</owl:DatatypeProperty>
<!- Datatype Properties -->
<owl:DatatypeProperty rdf:ID="3DSupport">
<rdfs:domain rdf:resource=#J2ME" />
<rdfs:range rdf:resource="&xsd;boolean"/>

Figure 1 – Devices Ontology example
Using the classes and properties defined in the
‘Devices Ontology’ it’s possible to express instances of
real world devices used by human agents in collaborative
environments of planning. Hence, the reasoning
mechanism uses the knowledge base and reasons upon
for a tailored deliver and visualization of information.

Conclusions
In this work it is proposed an integration of
ontologies and reasoning mechanism for multi-modality
visualisation in collaborative planning environments.
The set of ontologies and its integration will permit the
expressiveness of several aspects related to real world
applications in environments of mixed initiative planning
in a visualisation perspective. The reasoning mechanism
will allow a tailored delivery and visualisation of
planning information. The main contributions of this

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

framework are: (1) it consists in a general framework; (2)
the ontology set will permit organising and modeling the
domain from the visualization perspective; (3) the
reasoning mechanism will permit proper presentation of
information for each situation; (4) the framework will
serve as base for implementations, and (5) the framework
is based on real standards (W3C) that will facilitate
communication and interoperability with other services
and systems and also permit extensions for its use on
Semantic Web applications .

Acknowledgements
The first author is sponsored by CAPES Foundation
under Process No.: BEX1944/00-2. The University of
Edinburgh and research sponsors are authorised to
reproduce and distribute reprints and on-line copies for
their purposes not withstanding any copyright annotation
here on. The views and conclusions contained here in are
those of the author and should not be interpreted as
necessarily representing the official policies or
endorsements, either express or implied, of other parties.

[10] Shneiderman, B. 2000. The Limits of Speech
Recognition. In Communications of the ACM, Vol. 43,
No. 9, pp. 63-65, September, 2000.
[11] Tate, A. 2000. Intelligible AI Planning. In Research and
Development in Intelligent Systems XVII, Proceedings
of ES2000, The Twentieth British Computer Society
Special Group on Expert Systems International
Conference on Knowledge Based Systems and Applied
Artificial Intelligence, pp. 3-16, Cambridge, UK,
December 2000, Springer.
[12] Tate, A. 2001. I-X and <I-N-C-A>: an Architecture and
Related Ontology for Mixed-Initiative Synthesis Tasks,
In Proceedings of the Workshop on Planning/ Scheduling
and Configuration/Design, Vienna, Austria.
[13] Tate, A. and Drabble, B. 1995. O-Plan's PlanWorld
Viewers. In Proceedings of the Fourteenth UK Special
Interest Group on Planning and Scheduling, Wivenhoe
House Conference Centre, Essex University.
[14] Telea, A., Frasincar, F., Houben, G. 2003. Visualisation
of RDF(S)-based Information. In Proceedings of the
Seventh International Conference on Information
Visualization - IV 2003, pp. 294-299, London, England.
[15] W3C. 2003. World Wide Web Consortium (W3C).
<http://www.w3.org >

References
[1]

[2]

[3]

[4]

[5]

[6]

[7]

[8]

[9]

Amant, R. S, Healey, C. G., Riedl, M., Kocherlakota, S.,
Pegram, D. A., and Torhola, M. 2001. Intelligent
visualization in a planning simulation. In Proceedings of
IUI. 2001, pp. 153-160.
Ferguson, G., and Allen, J. 1996. Trains-95: Towards a
mixed-initiative planning assistant. In Proceedings
Artificial Intelligence Planning Systems, pp. 70-77.
Ferguson, G., and Allen, J. 1998. Trips: An intelligent
integrated problem-solving assistant. In Proceedings
AAAI-98, pp. 567-573.
Fluit, C., Sabou, M., Harmelen, F. 2002. Ontology-based
Information Visualisation. In Proceedings of Visualising
the Semantic Web - VSW 2002. Springer Verlag,
Vladimir Geroimenko Editor, pp. 546-0554.
Lino, N., Tate, A., Siebra, C. and Chen-Burger, Y. 2003.
Delivering Intelligent Planning Information to Mobile
Devices Users in Collaborative Environments, Workshop
on Artificial Intelligence, Information Access and
Mobile Computing (AI-IA-MC) at the International Joint
Conference on Artificial Intelligence (IJCAI-03),
Acapulco, Mexico, August 2003.
McGuinness, D. L., Fikes, R., Hendler, J. and Stein, L.
A. 2002. DAML+OIL: An Ontology Language for the
Semantic Web. In IEEE Intelligent Systems, Vol. 17, No.
5, pp. 72-80, September/October 2002.
Moran, D. B., Cheyer, A. J., Julia, L. E., Martin, D. L.,
and Park, S. 1997. Multimodal user interfaces in the
Open Agent Architecture. In Proceedings of the 1997
International Conference on Intelligent User Interfaces
(IUI97), Orlando, Florida, pp. 61-68, 6-9.
Pegram, D. A, Amant, R. St., and Riedl, M. 1999. An
approach to visual interaction in mixed-initiative
planning. Mixed-Initiative Intelligence workshop, AAAI99, pp.15-23.
Rao, A. S., Georgeff, M. P. 1995. BDI Agents: From
Theory to Practice. In Proceedings of the First Intl.
Conference on Multiagent Systems, pp. 312-319.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

