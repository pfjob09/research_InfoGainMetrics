Vocabulary Spectral Analysis as an Exploratory Tool for Scientific Web Intelligence
Mike Thelwall
School of Computing and Information Technology, University of Wolverhampton, Wulfruna Street,
Wolverhampton WV1 1SB, UK.
{m.thelwall@wlv.ac.uk}
Abstract
A key Scientific Web Intelligence goal is to produce
visualizations of academic Web spaces in order to reveal
subject structures and identify relationships between
different fields. We introduce an exploratory technique,
Vocabulary Spectral Analysis, designed to build intuition
to help design effective procedures for clustering. We apply
VSA to New Zealand university web sites. The results
suggest that subject-based academic Web site clustering is
possible but will require extensive data filtering to give
effective subject-based clusters.
Keywords web mining, scientific web intelligence

1. Introduction
Scientific Web Intelligence [27] is a field closely
related to Knowledge Domain Visualization, aiming to use
Web mining and Web Intelligence techniques to exact
information from academic web spaces. One of its goals is
to produce visualizations of academic web spaces in order
to identify subject areas and inter-subject relationships.
This research is in an early stage. A prerequisite for
effective visualizations is the ability to cluster the web data
effectively. Although link-based clustering may be
possible, the approach that we will explore is text
clustering using the vector space model. There are choices
to be made concerning the words to include in the
clustering and we introduce a technique, Vocabulary
Spectral Analysis (VSA), to build an intuitive
understanding with which to make those decisions. VSA
also helps choose words to be excluded. The rationale for
the introduction of a new technique is the combination of
two factors. First, web pages provide text data that is
inherently difficult to mine with a high degree of
reliability. This is partly due to the occurrence of many
types of spurious words (e.g., ‘next’, ‘previous’, and
author/software credits) and partly because there is no
uniformity in academic publishing strategies: different
universities/departments/researchers can have vastly
different policies. The latter is an issue because an SWI

goal is to extract information about research relationships
through the medium of the web, rather than to study the
web as an end in itself. The second rationale for
introducing a new technique follows from this point. The
clustering and subsequent visualization must give
meaningful information about academic subjects and not
any other aspects of web content. A generic solution will
not achieve this end. In data mining terminology, VSA is a
data preparation technique. Data preparation is a critical,
but often overlooked, aspect of data mining [21].
Within the field of domain visualization [4], although
citations are the typical object of study, text has also been
used. Leydesdorff [14] has shown that word and co-word
analysis of journal article titles can be used to map
intellectual organization within a given field. These results
are echoed and extended by a study that extracted text from
the titles and abstracts of articles within a field, obtaining a
meaningful structure graph [10]. Kohonen et al. [12] have
used self-organising maps to map the structure of a
collection of 7 million patent abstracts. Although
consuming a large amount of computing time, their
approach gave good results.
There have been several attempts to map academic
web sites. Chen [7] used hyperlinks to map Scottish
computer science departments. Others have used
hyperlinks in different ways to plot maps of interuniversity connections, either within a country [25] or
internationally [20,22]. No study so far has attempted to
map academic web sites with the objective of clustering by
subject and identifying inter-subject relationships. There
have, however, been attempts to cluster or map the Web on
the basis of topic similarity, applied to unrestricted
domains. These have been part of experiments with
objectives very different to domain visualization: typically
to improve web information retrieval performance.
Examples of non-visualization approaches include
Kleinberg’s [11] topic authority identification algorithm
HITS, Eckmann and Moses’ reciprocal links approach [8],
and Flake’s Community Identification Algorithm [9]. Intertopic link structures have been investigated from different

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

perspectives [5,16], showing both that links can be used to
identify topic clusters on the general Web. Within
academic spaces, however, link creation is unsystematic
and relatively arbitrary, although about half of interlinked
domains share a common subject [23]. Liu et al. [15]
attempt a complementary activity to topic mapping;
identifying important pages relevant to a topic. They use
machine learning and natural language processing
techniques on text fragments in pages, and have sets of
rules to decide whether text fragments or entire pages
should be ignored. This approach has been very successful
when applied to the whole web. Topic visualization
approaches include Kohonen/WEBSOM’s Self-Organizing
Maps of Open Directory Project web sites [28].
Section 2 introduces the unit of web text that the
research will be using, and section 3 gives the necessary
background for the Vector Space Model. The following
sections introduce Vocabulary Spectral Analysis and a
simple technique for clustering power estimation. Section 6
applies the techniques in an experiment, using New
Zealand university web sites. Finally, we speculate about
the strategies that will be needed to produce an effective
academic web domain visualisation tool.

2. The Domain Document Model
Although most web research takes the web page as the
basic indivisible unit of analysis, others subdivide pages
[18] or aggregate pages together in various ways [3,19].
Knowledge Domain Visualization using academic journal
data sets, in contrast, never uses pages as the unit of study,
the choice is to use articles or, for different approaches,
whole journals [2] or authors [29]. Since visualization
techniques are computationally expensive and university
web sites can contain hundreds of thousands of pages, it is
desirable to use a higher level of aggregation. This has the
additional effect that text clustering should be more
effective for the larger aggregated documents because they
contain more text to cluster with. The highest level of
aggregation that has been proposed, and could conceivably
be subject based, is the Domain [3,26]. If all pages with the
same URL domain name are regarded as part of the same
document then this is known as the domain Alternative
Document Model (domain ADM) [26]. For example, all
pages with domain name www.maths.vuw.ac.nz would be
regarded as a single document and to find out how many
times “maths” is mentioned in the document
www.maths.vuw.ac.nz, its frequency in each constituent
page would be totaled.
It is not necessarily possible to cluster by subject:
Leydesdorff, [14] in an analysis of word usage in the full
text of scientific articles has shown how word usage varies
significantly between individual articles. Variation is likely
to be much higher in a medium such as the web, where
publications are not refereed, genres are not fixed and
audiences are much more diverse (e.g. [17]). Clustering

domains is therefore unlikely to be susceptible to standard
clustering techniques without first taking steps to remove
sources of anomalous word use.

3. The Vector Space Model
Suppose that we have n documents containing a total
of m unique words. Let the number of occurrences of word
i in document j be fij. Let ni be the number of documents
that contain word i and fjmax the highest word frequency in
document j. The standard weighting for word i in
document j is wij =

n
log
 ni

f ij
f j max


 . Intuitively, this rates


words as most important to diagnosing document contents
if they occur in few other documents and have a high
frequency relative to other words in the document. The fjmax
factor, in particular, compensates for document size so that
larger documents containing more words should not have
an inappropriate representation. In the vector space model,
documents are represented by word frequency weight
vectors. With our terminology, document j is represented
by the vector (wij)i=1…n. Word order is irrelevant in this
representation. The distance between two documents can
be measured with any desirable double-vector argument
metric. The one considered here, the cosine measure is
defined as follows for two documents j and j’ [1].
n

∑w w

ij '

ij

(1)

i =1

n

n

∑w ∑w
2
ij

2
ij '

i =1

i =1

For vectors in geometric space this formula gives the
cosine of the angle between them. More similar vectors
should give higher values.
It is sometimes desirable to perform the above
calculations not over all words but over some subset S of
the full vocabulary, as shown below.

∑w

ij

d S ( j, j ' ) =

wij '

i∈S

∑ wij2

∑ wij2'

i∈S

(2)

i∈S

4. Vocabulary Spectral Analysis
Let C be a set of documents that has been identified as
being a cluster by some manual or automatic method. The
cluster can be used to help decide which types of words
will be useful in diagnosing cluster membership. For each
word i in the vocabulary we define its cluster membership
indicator (CMI) as follows.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

∑w

ij

cmi(C , i ) =

j∈C

C

∑w

ij

−

j∉C

n− C

(3)

Intuitively, a CMI score close to zero indicates that the
word is equally important to documents inside the cluster
as to those outside. Values divergent from zero suggest that
the word may be an effective discriminator of cluster
membership. A large positive value indicates that the word
is predominantly inside the cluster, whereas a large
negative value indicates that the word is predominantly
outside the cluster.
For academic web sites we expect to find large
anomalies in word frequency data. For example the name
of each university’s library catalog software is likely to
occur frequently. Other high frequency words, such as
subject names, may be desirable and so selective removal
is needed. We therefore need a strategy to identify
anomalous words and to build intuition about which
approaches for choosing the subset S of the vocabulary are
likely to work.
The Vocabulary Spectral Analysis technique is to
calculate Cluster Membership Indicators for document
clusters and to use CMI-ranked word lists for each cluster
to select words to be excluded and monitor the operation of
the clustering. There are two types of VSA, positive and
negative.
In positive VSA, the clusters are user selected, for
example by a manual classification of a subset of the
documents. The VSA word rank lists then indicate words
that are important to the clusters. The primary purpose of
this is to identify types of words that are useful for
obtaining the desired clustering, and should therefore not
be added to stopword lists. Words that are highly ranked
but are not semantically relevant should be excluded,
however, otherwise they may cause irrelevant connections
in the eventual visualization of the clusters.
Negative VSA clusters the documents using an
automatic technique (e.g. k-means using the cosine
measure on the vector space model) and ranked CMI lists
are produced for each cluster. The purpose of negative
VSA is identify unwanted clusters, use the CMI-ranked
lists to identify the words that are central to creating the
clusters, and add these words to the stoplist. Negative VSA
is an explicitly biased technique: its objective is to destroy
the natural clustering of the data so that the desired
clustering is more likely to be found in subsequent
clustering of the same documents.

∑ ∑d

S

( j, j ' )

j∈C j '∈C \ { j }

C ( C − 1)

−

∑ ∑d

S

( j, j ' )

j∈C j '∈C

(4)

CC

For a set of user clusters, the average of these
differences can be used as a measure of the clustering
effectiveness of sub-vocabularies S. We now have a single
clustering power indicator with which to compare different
sub-vocabularies. The user-defined set of clusters does not
have to be complete, nor does the data set have to be able
to be clustered for this technique to work.

6. The New Zealand Universities Data Set
The web sites of universities of New Zealand were
crawled using a spider designed to identify and eliminate
pages and, with manual intervention, to avoid pages not
created by the host institution [24]. A program was then
written to convert the web pages into word frequency
vectors, which were then combined to give word frequency
vectors for each domain name. Only words outside tags in
the body of a page, or inside its official title were extracted.
A total of 2,152,386 pages were indexed, yielding
60,998,403 words.
We used positive and negative VSA to help cluster
New Zealand web sites by subject and clustering power
estimation to select the proportion of low frequency words
to be excluded from the vocabulary.
The domains were categorized by the author using
broad subject headings suggested by the web pages in the
domain. This produced 19 subjects with at least three
domains, a few isolated subjects and many general-purpose
domains. No stemming was used, neither was a stopword
list.
The working assumption was made that the humanidentified clusters were correct and that effective clustering
would be aided by choosing a vocabulary that best
discriminated between members and non-members of
clusters.

5. Clustering Power Estimation

Table 1: Top 10 weighted words in business
domains
Word

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

Domains

CMI

business

59806

408

0.005902

marketing

16987

242

0.004476

finance

The user-defined clusters created for positive VSA can
be used for an additional purpose: to compare the
effectiveness of different sub-vocabularies for clustering.
For a given sub-vocabulary S and cluster C, the power of S
to discriminate between members of C and non-members
of C is the average similarity of distinct pairs of documents
in C (using equation 2) minus the average similarity of
documents in C with documents outside of C, as shown
below.

Freq

economics
banking
management

8300

217

0.002826

15509

261

0.002726

2010

123

0.002717

76754

465

0.002569

sitemap

2419

62

0.001874

accounting

8162

197

0.001613

auckland

55604

414

0.001546

research

186161

546

0.001482

actually less discriminatory than the larger ones. Hence all
words should be used for effective clustering.

Frequency Domains
3356
15610

240 0.001187

16780

278 0.001166

changed

7172

284 0.001152

199

33 0.001072

sst
semester

18364

319

0.00101

systems

44521

451

0.00095

lab

7709

261 0.000862

comments

16931

354 0.000842

disclaimer

27252

288 0.000838

Figure 1 shows the results of restricting the vocabulary
by eliminating different amounts of low frequency words.
The discriminatory power of the words of academic
domains varies greatly. The key to Figure 1 shows them in
descending order of discriminatory power for words in at
least 160 domains. At one extreme, Law web sites
significantly discriminate themselves in text use but at the
other extreme, art domains are more dissimilar to each
other than to non-art domains. In the latter case the cause is
that many art domains were actually digital media
exhibitions, creatively striving to be unique. Figure 2 gives
the average for the 19 subjects. The trend shows that
eliminating words that occur in only a few domains
increases the clustering differential, with the optimum
attained when words in less than 160 domains were
excluded. Figure 2 gives a misleading impression,
however, because most inter-domain similarities increase
as the vocabulary size shrinks. The difference needs to be
normalized through dividing by the average inter-subject
similarity, to compensate for this. The result (not shown,
but similar to Figure 4) is that the smaller vocabularies are

0.1

640

320

80

160

40

20

9

10

8

7

6

5

4

3

2

0
-0.1

Chemistry
Business
Education
Medicine
Env. Sci.
Food
Computing
Biology
General
Arts

-0.2

Figure 1: Subject clustering statistics
0.16

CMI

copyright

Planning
Social studies
Engineering
Languages
Physics

0.2

Minimum domains containing word

147 0.001356

assignment

0.3

Intra-subject average correlation minus inter-subject
average correlation

Word

0.4

-0.3

Table 2: Top 10 weighted words in computing
domains

template

Law
Psychology
Architecture
Sport
Maths

0.5
Intra-subject average correlation minus intersubject average correlation

A positive Vocabulary Spectral Analysis was
conducted on the 19 identified subjects. Table 1 is the ‘best
case’ where almost all top ranked words related to the
subject, business. Table 2 is the ‘worst case’, a table barely
recognizable as relating to computing. The tables indicate
that many words irrelevant to a subject are in practice good
online discriminators. This is a problem for the overall
domain visualization goal because influential meaningless
terms will cause irrelevant apparent relationships between
different subject clusters. Two logical consequences are (a)
the need to use a stopword list to prevent common words
without subject-relevant meanings being influential, and
(b) the need to exclude influential words that would
normally not be considered to be stopwords (e.g. sst, which
stands for Social Sciences Tower and is an office block
housing many computing staff at one university), and
academic words.

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0
2

3

4

5

6

7

8

9

10

20

40

80 160 320 640

Minimum domains containing word

Figure 2: Average subject clustering
In conjunction with tables 1 and 2, figures 1 and 2
confirm that high frequency words are useful for cluster
discrimination and so a general rule to eliminate all high
frequency words should not be made. Generic subject
descriptor words such as ‘Mathematics’ and ‘Business’
may be key to clustering.
We clustered the domains using an estimated optimal
setting of 2 (i.e. words must be in a minimum of two
different domains) and 240, using k-means clustering [3]
for 31 clusters. The resulting clusters aligned very poorly
with our pre-selected ones in both cases. For example, in
the 240 clustering, the only one of our subjects that was
substantially clustered together was engineering, in which
6 of the 7 domains were in a single cluster, although
sharing the cluster with 66 non-engineering domains (see
Table 3). Clearly, without exclusion of influential noise
words effective clustering will not be possible. A negative
Vocabulary Spectral Analysis examination of the clusters
found showed that the strongest clustering factor was
university membership. Five of the clusters were almost
exclusively from a single university, although not the same
one each time. Table 4 is a Massey University cluster,
particularly relating to its Palmerston North campus.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

Clearly, the extended choice of stopwords must include all
university names and be sensitive to words frequently
found within a single university. From inspection of the
VSA lists, these should include campus names and
locations as well as popular software.
Table 3: Top 10 weighted words in cluster 6 (72
domains)
CMI

engineering

33159

332 0.23145

and

1883534

674 0.10954

the

3605107

689 0.09955

of

2263812

683 0.09583

in

1317941

655 0.08636

research

186161

546 0.08318

a

1254004

659 0.07173

systems

44521

451 0.0673

design

42075

417 0.0647

modelling

9984

240 0.06037

0.2
Intra-subject average correlation minus intersubject average correlation

Word Frequency Domains

words in 2 or more domains) 13 of the 30 Environmental
Science domains were in a cluster of size 34 and there was
significant clumping of computing, business, law and
languages, although across more than one cluster. A
similar picture emerged from the 256 clustering. In
summary, the subject clusters were starting to emerge as a
result of the extended stopword list but not by enough to
give a coherent subject-based clustering.

0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
2

8

16

32

64

128

256

512

Minimum domains containing word

Figure 3 Average subject clustering, after
removing extended stopwords and merging
plurals

Table 4: Top 10 weighted words in cluster 26 (112
domains)
CMI

massey

32991

364 0.30587

palmerston

9023

305 0.09137

and

1883534

674 0.0794

the

3605107

689 0.0746

of

2263812

683 0.06782

in

1317941

655 0.06556

north

21348

414 0.06431

students

127178

550 0.05753

research

186161

546 0.05687

a

1254004

659 0.05616

3.5
Normalised intra-subject average correlation
minus inter-subject average correlation

Word Frequency Domains

4

3
2.5
2
1.5
1
0.5
0
2

4

8

16

32

64

128

256

512

Minimum domains containing word

We created an extended set of stopwords consisting of
the influential non-subject specific words found from the
Vocabulary Spectral Analysis of the clusters. These were
mainly web-specific terms, general university-related
words and geographic names associated with New Zealand
universities. The purpose of the extended stopword list was
to remove influential terms that were an obstacle to
subject-based clustering.
We used a standard stopword list of high frequency
terms together with the extended stopword list to cluster
the data again. From Figure 3, the intra-subject similarities
have increased their difference to inter-subject similarities.
Figure 4 shows the normalized differences. The extended
stop words have improved the chances of the domains
being clustered by subject.
The domains were again clustered using k-means and
the results were better. For the largest vocabulary (all

Figure 4 Normalized average subject clustering,
after removing extended stopwords and merging
plurals

7. Conclusions
Vocabulary Spectral Analysis is a simple tool for
building intuition about the text properties of the
documents to be clustered. Its negative variant can be used
to extend stopword lists in order to eliminate tendencies to
cluster in undesired ways. This is more scalable but less
automatic than the use of probabilistic mixture models [3],
which could also be used to minimize the effect of
undesired types of clustering. The natural language
processing and machine learning techniques of Liu et al.
[15] could be adapted for similar purposes, however, and it
is not clear which approach would be better. Potentially, a

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

combination of the two would give improved results,
although Liu’s techniques seem to be most suitable for
extracting information that is manifest in web pages, rather
than that which is more hidden in word frequency patterns.
The domain ADM New Zealand universities web data
set did not cluster naturally by subject and so naive
clustering approaches would not yield useful results. VSA
was able to deliver improvements in clustering, as
measured by the clustering power estimation technique. As
an aside, one of the important advantages of the clustering
power estimation technique for comparisons of intersubject and intra-subject differences is that the results are
clear-cut and numerical, whereas directly comparing the
results of clustering is more difficult and subjective. Of
course, direct cluster evaluation is necessary for evaluation
of the final product.
Further use of VSA for other academic data sets and
increases in extended academic stopwords lists are likely to
improve results, perhaps to the extent of separating out the
major subjects. Once a basic effective subject clustering of
domains has been achieved then it will be possible to work
on refining all stages of the data processing algorithms to
improve the quality of the clustering and to visualize the
data in a way that will usefully illustrate inter-subject
relationships. Given the amount of spurious words that
significantly influence clustering, an interface that allows
spurious connections to be highlighted and rejected by
users would be desirable. It is likely that no automatic
techniques will be completely successful in removing
anomalies.

[9]

[10]

[11]
[12]

[13]

[14]

[15]
[16]

[17]

[18]

[19]
[20]

8. References
[1]
[2]

[3]

[4]

[5]

[6]
[7]

[8]

R. Baeza-Yates and Ribeiro-Neto, B. Modern Information
Retrieval. Wokingham, UK: Addison-Wesley. 1999.
E. Bassecoulard and M. Zitt. Indicators in a research
institute: A multi-level classification of journals.
Scientometrics, 44(3): 323-345, 1999.
K. Bharat, B. Chang, M. Henzinger, M. Ruhl: Who links to
whom: Mining linkage between web sites. ICDM 2001: pp.
51-58
K. Börner, C. Chen, and K. Boyack Visualizing Knowledge
Domains. Annual Review of Information Science &
Technology, 37: 179-255, 2003.
S. Chakrabarti, M. Joshi, K. Punera, and D. Pennock, The
structure of broad topics on the Web. In Proceedings of the
WWW2002 Conference.
S. Chakrabarti. Mining the Web. Morgan-Kauffmann, New
York. 2003.
C. Chen: Structuring and visualising the WWW by
Generalised Similarity Analysis. In Hypertext 1997: 177186
J. Eckmann and E. Moses. Curvature of co-links uncovers
hidden thematic layers in the World Wide Web. Proc Natl
Acad Sci, 3099 (9): 5825–5829, 2002.

[21]
[22]

[23]

[24]
[25]

[26]

[27]

[28]
[29]

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

G. Flake, S. Lawrence, C. Giles, and F. Coetzee. Selforganization and identification of web communities. IEEE
Computer, 35: 66-71, 2002.
G. Heimeriks, M. Hörlesberger, and P. van den Besselaar,
Mapping
communication
and
collaboration
in
heterogeneous research networks. Scientometrics, 58(2):
391-413, 2003.
J. Kleinberg. Authoritative sources in a hyperlinked
environment. Journal of the ACM, 46(5):604-632, 1999.
T. Kohonen, S. Kaski, K. Lagus, et al. Self organization of
a massive document collection. IEEE Transactions on
Neural Networks, 11(3): 574-585, 2000.
Leydesdorff, L. Words and co-words as indicators of
intellectual organization, Research Policy 18, 209-223,
1989.
Leydesdorff, L. Why words and co-words cannot map the
development of the sciences, Journal of the American
Society for Information Science 48 (5):418-427, 1997.
B. Liu, C. Chin, H. Ng. Mining topic-specific concepts and
definitions on the web. In WWW 2003, May 20-24, 2003.
F. Menczer, (in press). Lexical and semantic clustering by
Web links. Journal of the American Society for Information
Science and Technology.
I. Middleton, M. McConnell, and G. Davidson. Presenting
a model for the structure and content of a university web
site. Journal of Information Science, 25(3):219-227, 1999.
T. Miles-Board, L. Carr, and W. Hall. Looking for linking:
associative links on the web. In Proceedings of ACM
Hypertext 2002, pp. 76-77.
L. Page, Method for node ranking in a linked database,
United States Patent 6,285,999, 2001.
X. Polanco, M. Boudourides, D. Besagni, and I. Roche.
Clustering and mapping web sites for displaying implicit
associations and visualising networks, 2001, Patras,
Greece: University of Patras.
D. Pyle. Data preparation for data mining. Morgan
Kaufmann, 1999.
M. Thelwall, R. Tang, and E. Price. Linguistic patterns of
academic Web use in Western Europe. Scientometrics,
56(3):417-432, 2003.
M. Thelwall, and D. Wilkinson. Finding similar academic
web sites with links, bibliometric couplings and colinks.
Information Processing & Management, 40(3):515-526,
2004.
M. Thelwall, A web crawler design for data mining.
Journal of Information Science 27(5):319-325, 2001.
Thelwall, M. An initial exploration of the link relationship
between UK university web sites. ASLIB Proceedings,
54(2): 118-126, 2002.
Thelwall, M. Conceptualizing documentation on the Web:
an evaluation of different heuristic-based models for
counting links between university web sites. Journal of the
American Society for Information Science and Technology,
53(12): 995-1005, 2002.
Thelwall, M. Scientific Web Intelligence: Finding
relationships in university webs. Communications of the
ACM, in press.
WEBSOM url:http://websom.hut.fi/websom/
H.D. White and K. W. McCain. Visualizing a discipline.
JAIS, 49(4):327-355, 1998.

