Information Provenance and the
Knowledge Rediscovery Problem
Dennis P. Groth
Indiana University School of Informatics
Bloomington, IN 47408, USA.
Email: dgroth@indiana.edu

Abstract
Visualizations leverage innate human capabilities for recognizing interesting aspects of data. Even if users might
agree on what is interesting about a visualization, the
steps that they use in the knowledge discovery process
may be significantly different. This results in an inability to effectively recreate the exact conditions of the discovery process, which we call the knowledge rediscovery
problem. Because we cannot expect a user to fully document each of their interactions, there is a need for visualization systems to maintain user trace data in a way
that enhances a user’s ability to communicate what they
found to be interesting, as well as how they found it. We
present a model for representing user interactions that articulates with a corresponding set of annotations, or observations that are made during the exploration. Such an
ability is critical to addressing the knowledge rediscovery
problem, and is a fundamental component for systems that
must provide information provenance.

1 Introduction
Visualization research is frequently presented in terms of
a graphic image of a visual representation, along with a
verbal description of what the observer should recognize.
While this traditional approach in reporting results is necessary and meaningful, it is important to note that, in presenting the result in such a way, the researcher is not reporting a visualization. Rather, the researcher is reporting
a presentation graphic [23]. While it is necessary to generate a presentation graphic to report the results, the graphic
is substantially inadequate for other researchers or practitioners to apply the results.
The final presentation of the visualization output results from a stream of actions performed against the data.
A common view of the transformation process is represented by the visualization pipeline, as shown in Figure
1. Under this model, data may be transformed through
any number of processes prior to display. The output then

Figure 1: The visualization pipeline.

may undergo an indeterminate number of user specified
view transformations. If we assume a 3D representation,
the most basic view transformations are rotation, translation and scaling, with others supported by specific applications. Other researchers have developed taxonomies
of visualization/interaction techniques, most notably Chi
[8], Shneiderman [22] and Card and Mackinlay [6].
The main contribution of this research is a conceptual
model of user interaction and observation for data visualization. The model is generic, in the sense that it concisely
captures changes to the state of the visualization made by
the user in a way that provides recall of steps the user took
to achieve the visual representation. Details of the model
are provided in a later section. In addition to the conceptual model we describe an instantiation of the model,
demonstrating how the model can be adapted to support
a variety of modalities of interaction tracking. A prototype implementation of the model is used to demonstrate
how the model can be used to enhance user navigation of
visualizations.
Our intent is to consider the interactions with data visualizations as knowledge itself. Armed with this knowledge, researchers will be in a position to share not only
what was found to be interesting in a visualization, but
exactly how it was found. In a broader context, we con-

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

sider this research to be a fundamental contribution to
developing solutions for an emergent research area: information provenance. Problems in knowledge and data
provenance[5, 4] are gaining interest, with broad applications to the advancement of scientific discovery [18].
Provenance is a term that refers to the lineage of an
item. While some people associate the term with artwork,
and the lineage of who owned, or possessed the piece, we
use it in the context of the information discovery process.
The model that we are presenting supports provenance by
fully documenting the discovery process. The prototype
demonstrates how users can interact with the history of
interactions and capture annotations in the same context.
Conceptually, the model separates the interaction from the
data. This allows for the exchange of not only the result of a visualization, but precisely how the result was
achieved. Another user may take the interaction data and
use it against a different dataset, to see how general the
technique may be.
We have also considered the value of the model for assisting in the peer review process of publications. Another
interesting idea we have envisioned is the creation of digital libraries of interactions, which could be used to study
search behavior in complex spaces. Such a collection of
interactions can also be used in pedagogical exercises to
provide “tours” of data and training materials.
The remainder of this paper is structured in the following way. In the next section we describe work that is related to this research. Following that, we define the model
and describe two instantiations of the model. We then
present implementation considerations for the model using a prototype for demonstration purposes. Lastly, we
conclude with a discussion of how the model can be applied and a description of our planned efforts.

2 Related Work
The interaction model builds upon work from three areas
of research: knowledge discovery in databases (KDD),
annotation; and user tracking.
This research makes an assumption that the knowledge
discovery process is, indeed, a process, and that the steps
that a user takes to discover knowledge are as important as
the knowledge itself. Described by Fayyad, et al [11], the
KDD process is frequently depicted in terms of a number of iterative steps, as shown in Figure 2. There are,
of course, obvious similarities between the KDD process
and the data visualization pipeline [24, 19, 7].
A critical aspect of the process is the implied interaction
with a user. Obviously, the user is involved with problem
selection, as well as the interpretation of the results. Often, the user may review the results and develop a more
refined problem statement, which initiates another itera-

Interpretation
Knowledge
✙
✢

✚
✣

✙
✢

✚
✣

✙
✢

✚
✣

✛
✜

Data Mining
✙

✢

✚
✣

✛
✜

✙
✢

✚
✣

✤

✤

✥

✥

✛
✜

✙
✢

✚
✣

✤

✤

✕
✥

✥

✖

✛
✜

✗
✘

✙
✢

✚
✣

✤

✕

✤
✥

✥

✖

✑
✒

✗
✘

Patterns

✕
✖

✓
✔

✑
✒

✗
✘

✕
✖

✓
✔

✑
✒

✗
✘

✕
✖

✓
✔

✑
✒

✗

Transformation

✘

✕
✖

✓
✔

✑
✒

✗
✘

✕
✖

✓
✔

✑
✒

✗
✘

✕
✖

✓
✔

✑
✒

✗
✘

✕
✖

✓
✔

✑
✒

✗
✘

✓
✔

✑
✒

✓
✔

✌

✌

✍

✌

✍

Preprocessing
 

✄

 
☎

✂

✂

✌

✍

✆

✄

✝✆
☎

✌

✍

✌

✍

✌

✍

✞
✟

✎

✠

✞
✡

✟

✠

✍

☛

☞☛
✡

Transformed
Data

✎

✏

✏

✎
✏

✎
✏

Preprocessed
Data

Selection

Data

✌

✍

Target Data

Figure 2: The KDD Process
tion of the process.
In the context of this research, annotation is the adding
of information to existing data by a user. For visualizations, numerous approaches have been described. Gertz,
et al [14] describe an annotation framework, including
a graph-based model of annotation data. In the context
of scientific data, annotations are representative of critical observations by human observers, and are considered
metadata. Cousins, et al [9] provides an analysis of annotation techniques. Marshall and Brush [17] discuss the
issue of shared annotations. Additional annotation concepts are described in the following contexts: web annotation [20]; document annotation [2]; and video/multimedia
annotation [10, 1].
User tracking involves the recording of actions taken
by users in the course of completing a task. Scaife and
Rogers [21] critically examine the linkages between external representations (e.g. visualizations) and a user’s
corresponding internal representation of the information.
They describe the concept of a cognitive trace, which may
include explicit marks, or highlighting, of information. A
need to record the corresponding parameter settings for
the software is also identified. Fitzgerald, et al [12] define a framework for describing event-tracking for multimodal user interfaces. Such a framework can be used to
develop a more comprehensive model of user interaction.
Franklin, et al [13] describe a tracking mechanism for an
electronic classroom environment, in which the users actions are tracked for playback purposes. In addition to
these works, user tracking is extensively described in various contexts: security; performance; information retrieval
[3]; and version control.
Tracking of user interactions within a visualization environment has been studied by Lee and Grinstein [16] and,
more recently, Jankun-Kelly, et al [15]. These previous

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

efforts are particularly relevant to our work at the conceptual level. Where we differ from them is in the capturing
of meta-information, such as annotations, along with the
interactions.

3 The Model
We have approached the development of our model from
two different perspectives. First, from a pure tracking
standpoint, the model must accurately capture the user’s
interactions. We will refer to a model that tracks the user’s
exploration as an “Interaction Model”, or I-Model for
short. The second viewpoint considers the observations
that the user may make while exploring the visualization.
In this extension to the I-Model users add annotative information to the visualization, thereby expanding the information base - a natural part of the scientific discovery
process. We extend the I-model to contain the annotation data, resulting in what we refer to as an “InteractionAnnotation Model”, or IA-Model.
The model is designed to be domain-independent.
However, in order to illustrate the model we use molecular visualization of crystal structure as an example.

3.1 The Interaction Model
We base the model on directed graphs, with nodes signifying measurable states of the visualization system, and
edges denoting transitions between the states. The states
of the system are generically captured in the model, leaving it up to the implementation to define the specific contents of the state and transition information. For example,
as described in [15], the transitions might contain discrete
interactions, such as zoom, rotate, translate, or other interactions as described in [8]. Pictorially, the graph can be
depicted as shown in the example in Figure 3.

Figure 3: An example instance of an I-Model. In this
exploration, the user zoomed and then rotated the data.
The I-Model does not restrict multiple modes of interaction. If two stimuli occur in parallel, then two edges

would connect the states. Each transition edge would contain the specific details of the interaction stimulus. It is
important to note that stimuli may not always be generated by the user in a direct manner. Such stimuli may be
system generated. For example, simulation programs or
sensor readings may be continuously modifying the visualization. The I-model handles these stimuli as interactions. Note that the encoding of the interactions is system
specific.

3.2 The Interaction-Annotation Model
The I-Model supports a method for capturing the user interactions. However, it is insufficient for capturing what
the user’s perceptions of the visualization are. The capturing of annotative data is a natural component of the
scientific discovery process. In this section we extend the
I-Model to support user annotations. We call the extended
model the “Interaction-Annotation Model”, or IA-Model.
The IA-Model generalizes the graph structure defined
by the I-Model, with the addition of a type attribute to
both edges and nodes. We refer to this generalized structure as the “interaction graph”. The type attribute for
nodes and edges are taken from a set of types that may
vary based on particular implementations. The model
does not presuppose any interpretation of the types. More
specifically, the definition and interpretation of the types
is an implementation detail.
The IA-Model allows for the articulation of annotation
data with the interactions that are being applied to the visualization. We use the following example to illustrate
how the model can be used.
We will use a small set of the types for the example
model: 1) States of the visualization, 2) User Interactions,
3) Annotations, and 4) We will use the same interactions
(i.e. ZOOM and ROTATE) that were used in the previous
subsection. In this example we will assume that the user
follows the general process of: 1) observing the display;
2) making an annotation; and 3) applying an interaction.
For this example, the annotation data is represented by
text. However, there is no restriction on the mode of input used to perform the annotation. Figure 4 shows the
interaction graph for this example. The contents of the
annotation nodes are represented as text.
The IA-Model does not place any restrictions on the
number of incoming edges to a node. For example, to
support multiple modes of annotation the graph shown in
Figure 5 demonstrates a typed and drawn annotation for
the visualization.
The IA-Model can also support a dialogue-like interaction. For example, Figure 6 shows an interaction graph
in which the first annotation is remarked upon by a second annotation. Note the new “EXPLAINS” type that has
been introduced in the example.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

Figure 4: An example instance of an interaction graph
for the IA-Model. The user annotates each state of the
visualization and then performs the interaction.
Figure 6: In this interaction graph the user has created
two annotations for the same state and a second node attempts to explains how to answer the question posed by
the user. An example of what might be displayed to the
user is shown by combining the annotations with the visualization.

4 Implementation
The IA-Model is foundationally a model for tracking user
interactions. It would be advantageous for systems to
track the interactions so that the user could use the interaction graph to help improve the discovery.

Figure 5: In this interaction graph the user has created
two annotations for the same state. An example of what
might be displayed to the user is shown by combining the
annotations with the visualization.

The examples we have used in this section demonstrate
how the IA-Model extends to support the natural scientific
process of exploration and annotation of observations. In
the next section we discuss implementation considerations for the model and demonstrate how the model can
be used to enhance the discovery process.

We consider two different implementation strategies in
this section. The first is a “push” strategy, in which discrete user interactions are sent as messages to a program
that maintains the interaction graph. The second is a
“pull” strategy, in which a program that maintains the interaction graph interrogates the user’s system to get the
state of the system.
In the descriptions that follow we assume that there is
an interactive visualization system that we simply refer
to as the “system”. We describe the IA-Model as a standalone program that the system can communicate with. We
refer to this program as the model manager. The discussion is purposefully informal, since the focus of this
project is really the conceptual model. In the last subsection of this section we utilize examples from our prototype
to illustrate an example of how the IA-Model could be incorporated into visualization systems.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

4.1 The Push Strategy
The push strategy assumes that the system sends an event
message to the model manager every time that the user
performs some action. This seems a reasonable strategy,
since the system certainly must be programmed to react
to each allowed user interaction. The resulting graph is
essentially a queue, with nodes being essentially empty
states with interactions connecting the nodes. All information of interest is maintained in the edges of the graph.
Using the push strategy the system can replay the interaction by requesting a traversal of the interaction graph
to be enumerated, again using some message passing protocol. Clearly, the user can step through their interaction
history, however, the cost of doing so might be too high.
Consider, for example, a lengthy interaction with many
hundreds of interactions. In order to position the user at
any point in their history the system would have to redo
each interaction. Unless the system has an infinite “Undo”
functionality the user would be subject to excessive time
delays in repositioning themselves in their history.

4.2 The Pull Strategy
The pull strategy assumes that the system has the ability
to report the entire state of its processing whenever requested by the model manager. Under this scenario the
model manager essentially polls the system and receives
state information in response to its polling request. Like
the push strategy the resulting interaction graph using the
pull strategy is also a queue. However, the nodes of the
graph hold state information and the edges are essentially
empty. The only thing the model manager can determine
is that a new state of the system has occurred, it just can’t
be certain what the user did to get there. Note that a hybrid approach is possible, in which the response from the
system to the polling request could return both state and
user interactions.
The main benefit of the pull strategy over the push strategy is performance. The maintenance of the state information in the interaction graph allows the system to traverse the graph extremely efficiently. For example, the
system can position the user at any point in the history of
their interaction without redoing the previous interactions.
The pull strategy architecture is depicted in Figure 7.
One issue to be considered with the pull strategy is the
granularity of the tracking model. If the model manager
polls too slowly then the interaction graph risks missing
important states of the visualization. At the same time,
polling too frequently may degrade performance. A possible implementation feature may be to have the visualization system notify the model manager when the state has
changed sufficiently. Certain user interactions for particular visualizations may be deemed overkill. For instance,
tracking the exact position of the mouse cursor will result

Figure 7: An overview of the pull implementation strategy. The visualization system has three methods that are
callable by the model manager.
in huge interaction graphs, which might not be interesting.

4.3 Prototype Implementation
Our prototype implements the pull strategy as described
in the previous section. We modified a system for visualizing multivariate data to have the CheckState, GetState
and SetState methods as depicted in Figure 7. The state
information that we tracked within the system was a viewpoint model - camera position and direction within a 3D
environment.
The most interesting aspect of our prototype is the
model manager interface, that exposes the interaction
graph to the user. The resulting application allows the
user to interact with the visualization system as well as the
interaction graph. Figure 8 shows a screen capture from
the model manager. The user interaction was a simple sequence of zooming operations to display an overview of
the entire dataset.
The prototype model manager supports annotation directly through either typed comments, or recorded voice.
This capability saves the visualization system the effort of
performing annotation capabilities. We are working on a
tablet PC based interface to support direct scribbling of
annotations. The interaction graph displays visual cues to
indicate current position within the interaction history as
well as the location of annotations.
The prototype implementation supports navigation
through a direct manipulation of the graph. For example,

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

Figure 10: This graph shows multiple branches in the interaction. The larger, red nodes indicate the location of
annotation data. The green node with the bullseye signifies the current state of the visualization system.

Figure 8: The prototype that shows the interaction graph
to the user.
Figure 9 shows a closeup view of the model manager interface. The user interface supports direct navigation to a
previous state by clicking on a node in the graph. The user
can click on a node and then drag the mouse to replay the
states in either direction, depending on the direction the
mouse is moving.

Figure 9: The user can select a node with the mouse to
directly set the state of the visualization.
It is worth pointing out that there is no restriction in the
model for branching, or non-linear behavior represented
in the interaction graph. However, in order for the model
to support non-linear behavior tracking the model manager must keep track of where the user is relative to the
interaction graph. The prototype supports this feature by
creating branches in the graph if the current state is not a
leaf node. An example graph is shown in Figure 10. Note
the use of color to provide visual cues to the user. Larger,
red nodes signify the location of annotation data, a single green node with a larger circle around the node is the
current location in the graph.

The prototype system uses a directed graph data structure to manage the interaction graph in memory. When
the graph is saved to disk the data is stored in an xml
document. We envision further work on this area of the
system. The general approach defined by the IA-Model
suggest that a standard DTD should be developed to facilitate interoperability between systems.

5 Conclusion
Because visualization systems provide a high degree of
interaction, the user cannot be expected to fully document
each of their interactions. Consequently, there is a need
for visualization systems to maintain user trace data in a
way that enhances a user’s ability to communicate what
they found to be interesting, as well as how they found
it. Such an ability is critical to addressing the knowledge
rediscovery problem, and is a fundamental component for
systems that must provide information provenance.
The main contribution of this research is a model for
representing interactions that users apply to their visualizations. This model articulates each interaction a user
makes with a corresponding set of observations relative
to what the user sees. We also presented a prototype implementation of the model that shows how the model can
be incorporated within a data visualization environment.
Furthermore, we have highlighted the need and opportunities presented by systems that support information provenance and address the knowledge rediscovery problem.
Our future efforts include the development of collaborative interaction tools that track multiple users’ explorations of visualizations. Such systems will support comparison of explorations, which will yield better understanding of how users search through visual representations. We are also working with chemists to focus on
domain-specific implementations.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

References
[1] A HANGER , G., AND L ITTLE , T. A survey of technologies
for parsing and indexing digital video. Journal of Visual
Communication and Image Representation 7, 1 (1996),
28–43.
[2] BARGERON , D., AND M OSCOVICH , T. Reflowing digital ink annotations. In Proceedings of the Conference
on Human Factors in Computing Systems (CHI03) (2003),
pp. 385–393.
[3] B UDZIK , J., AND H AMMOND , K. User interactions with
everyday applications as context for just-in-time information access. In Proceedings of the 2000 International Conference on Intelligent User Interfaces (2000), pp. 44–51.
[4] B UNEMAN , P ETER AND K HANNA , S ANJEEV AND
TAJIMA , K EISHI AND TAN , WANG -C HIEW. Archiving
Scientific Data. In Proceedings of ACM SIGMOD International Conference on Management of Data (June 2002),
pp. 1–12.
[5] B UNEMAN , P ETER AND K HANNA , S ANJEEV AND TAN ,
WANG -C HIEW. Why and Where: A Characterization
of Data Provenance. In International Conference on
Database Theory (ICDT) (2001), pp. 316–330.
[6] C ARD , S., AND M AC K INLAY, J. The structure of the information visualization design space. In Proceedings of
the IEEE Symposium on Information Visualization (InfoVis ’97) (1997), pp. 92–99.
[7] C ARD , S. K., M ACKINLAY, J. D., AND S HNEIDERMAN ,
B., Eds. Readings in Information Visualization:Using Vision to Think. Morgan Kaufmann Publishers, Inc., 1999.

[13] F RANKLIN , D., B UDZIK , J., AND H AMMOND , K. Planbased interfaces: Keeping track of user tasks and acting
to cooperate. In Proceedings of the 2003 International
Conference on Intelligent User Interfaces (2002), pp. 79–
86.
[14] G ERTZ , M., S ATTLER , K.-U., G ORIN , F., H OGARTH ,
M., AND S TONE , J. Annotating scientific images: A
concept-based approach. In Proceedings of the 14th International Conference on Scientific and Statistical Database
Management (2002), pp. 59–68.
[15] JANKUN -K ELLY, T. J., M A , K.-L., AND G ERTZ , M. A
model for the visualization exploration process. In Proceedings of the IEEE Conference on Visualization (2002),
pp. 323–330.
[16] L EE , J., AND G RINSTEIN , G. An architecture for retaining and analyzing visual explorations of databases. In Proceedings of IEEE Visualization ’95, Atlanta GA (October
1995), pp. 101–109.
[17] M ARSHALL , C. C., AND B RUSH , A. B. From personal
to shared annotations. In Proceedings of the Conference
on Human Factors in Computing Systems (CHI02) (2002),
pp. 812–813.
[18] M YERS , JAMES D., C. A. R., E LDER , M., G EIST, A.,
AND S CHWIDDER , J. Re-integrating the research record.
Computing in Science and Engineering (May/June 2003),
44–50.
[19] N ORTH , C., AND S HNEIDERMAN , B. Snap-together visualization: A user interface for coordinating visualizations via relational schemata. In Advanced Visual Interfaces (2000), pp. 128–135.

[8] C HI , E. A taxonomy of visualization techniques using the
data state reference model. In Proceedings of InfoVis 2000
(Salt Lake City UT, October 2000) (2000), pp. 69–75.

[20] ROSCHEISEN , M., M OGENSEN , C., AND W INOGRAD ,
T. Interaction design for shared world-wide web annotations. In CHI 95: Human Factors in Computing Systems, CHI 95 Conference Companion: Mosaic of Creativity (1995), pp. 328–329.

[9] C OUSINS , S. B., BALDONADO , M., AND PAEPCKE , A.
A systems view of annotations. Tech. Rep. Technical Report P9910022, Xerox PARC, 2000.

[21] S CAIFE , M., AND ROGERS , Y. External cognition: How
do graphical representations work. International Journal
of Human-Computer Studies 45 (1996), 185–213.

[10] DAVIS , M. Media streams: An iconic visual language for
video representation. In Readings in Human-Computer Interaction: Toward the Year 2000, R. M. Baeker, J. Grudin,
W. A. Buxton, and S. Greenberg, Eds. 1995, pp. 854–866.

[22] S HNEIDERMAN , B. The eyes have it: A task by data type
taxonomy for information visualizations. In Proc. IEEE
Symp. Visual Languages, VL (3–6 1996), pp. 336–343.

[11] FAYYAD , U. M., P IATETSKY-S HAPIRO , G., AND
S MYTH , P. From data mining to knowledge discovery:
An overview. In Advances in Knowledge Discovery and
Data Mining. 1996, pp. 1–34.
[12] F ITZGERALD , W., F IRBY, R., AND H ANNEMAN , M.
Multimodal event parsing for intelligent user interfaces.
In Proceedings of the 2003 International Conference on
Intelligent User Interfaces (2003), pp. 53–60.

[23] T UFTE , E. R. The Visual Display of Quantitative Information. Graphics Press, 1997.
[24] W RIGHT, H., B RODLIE , K., AND B ROWN , M. The
dataflow visualization pipeline as a problem solving environment. In Virtual Environments and Scientific Visualization ’96, M. G¨obel, J. David, P. Slavik, and J. J. van
Wijk, Eds. Springer-Verlag Wien, 1996, pp. 267–276.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

