3D Reconstruction of a Region of Interest Using Structured Light and Stereo
Panoramic Images
Duke Gledhill Gui Yun Tian Dave Taylor
School of Computing & Engineering,
University of Huddersfield, HD1 3DH
{d.gledhill,g.y.tian,d.taylor}@hud.ac.uk
Abstract
Construction of 3D environments from photorealistic
images is important for many industries, including virtual
reality, games and tourism among others. Extensive work
has been undertaken using multiple stereo vision systems
and fusion processing to create full 3D virtual
environments. To improve the efficiency and accuracy, a
stereo panoramic imaging system is used. A 3D
environment can be created by using two panoramic
images from a one-shot camera system. However a whole
3D scene is not always required, so a system which
reconstructs 3D data only in a region of interest has been
developed. A new structured light projection system for
region of interest reconstruction is presented and tested.
Based on the experimental tests, conclusions have been
derived for highlighting the advantages of the approach.

1. Introduction
Stereo panoramic imaging is important and has many
uses in today’s games, robotics, tourism and engineering
industries. The automatic generation of 3D scenes has
attracted much attention. Stereo imaging overcomes the
slow manual 3D construction process by automatic 3D
reconstruction from two or more 2D images.
Stereo imaging is a method of recovering depth
information about a scene from two or more images taken
from different viewpoints. Once correspondence has been
established between points in the images, triangulation can
be used to determine the depth of the points. Hemayed et
al [1] classified stereo imaging into five methods, point
matching, scan line matching, curved-segment matching,
line-segment matching and edges with region matching.
However, traditional stereo systems have a small field of
view, limiting their usefulness in some applications. If a
whole scene is to be reconstructed then multiple stereo
reconstructions must be fused to create the 3D scene for a
full 360° view. An advanced method to capture the scene
data for stereo processing is to use a panoramic imaging
system which has a much larger field of view [2]. For
example the tests in this paper were completed using a
parabolic mirror mounted above a camera, yielding a 360°

David Clarke
Rotography Ltd
david@rotography.com

x 115° field of view image. There are five possible
panoramic imaging methods which can be used to capture
the images for stereo processing detailed as follows [2].
Optical systems which can be used for stereo panoramic
imaging have been researched by Pritch et al [3] and
Svoboda et al [4], who also looked at the epipolar
geometry needed for stereo panoramic imaging [5].

1.1. Current approaches for stereo panoramic
imaging
The first of the five methods mosaics images from a
single ‘off the shelf’ style camera to create panoramic
images. The images are joined together using a mosaicing
technique. Usually the camera is rotated about its nodal
point, but this does not enable 3D structure reconstruction.
There are many commercial packages able to mosaic
images like these. Peleg and Ben-Ezra [6] moved the point
about which the camera rotates to a point behind the
camera. Two panoramic images are then constructed from
left and right halves of the captured images. A diagram of
this system is shown in Fig. 1. This enables stereo viewing
for a human, but no depth can be calculated. Ishiguro et al
[7] rotated a standard camera around a circular path. Two
images from different parts of the path can be compared
and depth computed. Unfortunately it takes about 10
minutes to capture a panorama in this way. Peer et al [8]
proposed a method of extracting 3D information from a
mosaiced panorama.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

Fig. 1 – Single camera rotated above an
axis behind the camera. Stereo viewing
only

The second method uses a camera with moving parts,
for example Benosman et al [9] mounted 2 line scan
camera systems above each other. The line scanning
cameras are then rotated about a vertical axis. The distance
between the cameras is the baseline. A diagram of the
system is shown in Fig. 2. Because the system is rotated it
means it can not be used for real time situations. Petty et al
[10] also used two line scan cameras to construct and test a
stereo panoramic system.

the scene as possible. Fig. 4 shows a commercial parabolic
mirror from 0-360.com [12].

Axis of rotation

Fig. 4 – A commercial single shot mirror
panoramic system from 0-360.com

Baseline

Optical axis

Point in 3D

Fig. 2 – Two line scan cameras mounted
above each other, rotating about the axis
of rotation
The third method uses a fisheye lens, and is used widely
in the commercial environment for panoramic imaging,
such as the Ipix [11] system shown in Fig. 3. The wide
field of view is ideal for capturing the minimum number
of images possible for creating panoramic images. A
camera with a wide angle lens, such as a fisheye lens, can
be calibrated and used for 3D reconstruction if the camera
model is extended to include distortion parameters.

Nayer et al [13] presented prototypes of panoramic
cameras which use a parabolic mirror. Geb [14] proposed
a spherical mirror for robot navigation. Hicks and Bajcsy
[15] proposed a range of reflective surfaces which provide
a large field of view. Chahl and Srinivasan [16] and Yagi
et al [17] proposed a conical mirror. Yamazawa et al [18]
and Svoboda et al [19] proposed a system with a
hyperbolic mirror.
The fifth method is to use a single camera with multiple
mirrors. A single camera with two planar mirrors is
equivalent to a two camera stereo system. Gluckman et al
[20], Arnspang et al [21], Gothasby et al [22] and Nene
and Nayar [23] all used two planar mirrors in front of a
single camera to compute depth. Southwell et al [24]
proposed a single camera with a double lobed mirror,
shown in Fig. 5.

Fig. 5 – A double lobed mirror

1.2. Correspondence

Fig. 3 – The commercially available
fisheye panoramic system from Ipix
The fourth method uses a camera with a single mirror.
Using a single mirror on a camera usually means mounting
the mirror above the camera in order to capture as much of

The most important requirement of stereo imaging and
stereo panoramic imaging for 3D object and scene
reconstruction is correspondence. Many applications of 2D
image registration, or correspondence, are covered in
Odone and Fusiello’s paper [27] and readers are referred
to the paper by Brown [28] for an in depth review of
image registration techniques. There are two main
methods of detecting and matching features in the images,
area based methods and feature based methods.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

Area based techniques use correlation of intensity
patterns of a pixel with the intensity pattern around the
corresponding pixel in another image. First a point of
interest is chosen in one image. Then cross-correlation is
used to search for the corresponding pixel in the other
image. Area based methods suffer because they use the
intensity values at each pixel directly, and are therefore
sensitive to changes in viewing position, absolute
intensity, contrast and illumination. Also occlusions can
give erroneous depth information. Zitnick and Kanade
[29] use a co-operative stereo algorithm for stereo
matching, which also takes into account occlusion
detection.
Feature based methods are based on features of intensity
change in the images, rather than image intensities
themselves. There are two features that are most
commonly used, edge based and interest points or corners.
Edge detectors attempt to recover the discontinuities in
the photometrical, geometrical and physical characteristics
of objects in the images. This information creates
variations in the grey-level image. There are three main
ways to use the variation information, discontinuities,
local extrema and the 2D features formed where at least
two edges meet. Bao and Xu [30] use edge-preserving
visual perception modelling to build mosaics. Kim et al
[31] use edges in their graph matching algorithm and use
camera focal length to improve stitching reliability, which
finds corresponding edges for the matching. Edge
detection is used in Bourque’s et al [32] work for
determining interesting points from where a robot should
capture a spherical panorama.
Interest point detectors attempt to find locations in the
image where the signal changes two-dimensionally,
including locations such as corners, T-junctions or
significant changes in texture. Many interest point
detectors exist, as reviewed by Schmid et al [33]. Interest
points, as local invariant features [34], are robust to
changes in lighting, viewpoint, rotation, translation and
scale [35] and are used in many areas including image
retrieval and indexing [36], object recognition [37] and
mosaicing [34]. An evaluated interest points based Harris
corner method is used in this paper.

accurate matches, instead of the previous approach of
interest points based stitching [38]. The structured light
pattern is removed from the images to ensure a realistic
textured result.

2.1. Structured light pattern projection for
correspondence
Correspondence methods are less accurate in areas of
low texture. For example in an outdoor environment where
texture is in abundance correspondence is very accurate,
but an indoor environment usually has walls, and indoor
walls usually have low texture, e.g. white paint. To
overcome this lack of texture, it is proposed that a light
pattern is projected onto the low texture areas to aid the
correspondence search. Once a texture has been applied
the correspondence algorithms achieve higher accuracy
results. For this system a Gaussian noise pattern is
produced. The image is filtered to ensure that no two dark
pixels are next to each other, so that no ‘blocks’ of black
are produced. Large areas of black result in inaccurate
removal of the noise later. The structured light pattern has
to be dense enough to create a useable texture for the
correspondence algorithm, but with small enough dots to
be able to remove them for visualisation. The result is then
projected into the environment. Fig. 6 shows an example
of the structured light pattern used for projection. Because
the pattern of structured light is known, a filter such as a
median filter can be used to remove the pattern.

Fig. 6 – An example of the structured light pattern
projected onto low texture surface areas

2. 3D reconstruction of a region of interest
from stereo panoramic images

2.2. 3D reconstruction of the region of interest

It is not always necessary to build the entire 3D scene
from the stereo panoramic capture system. If only a small
region is required then less computing time needs to be
used to build that region. By using feature based 3D
reconstruction it is difficult to find enough feature points
in low texture areas. To overcome this problem a
structured light pattern is projected onto the region of
interest to aid the correspondence algorithm to find

It is proposed that the user should be able to select a
desired region of interest within the panoramic image and
that the region of interest is then reconstructed in 3D for
visualisation. In order to be able to reconstruct 3D from
the panoramic images, the cameras are mounted vertically
as shown in Fig. 8.
Vertical configuration of the panoramic system means
that the epipolar lines are vertical, making the
correspondence search much faster. Fig. 7 shows the

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

epipolar lines from a vertically mounted single parabolic
mirror system [25]. The system used in the project is a
parabolic mirror system.

correspondence accuracy has also been used for 3D face
reconstruction.

Fig. 7 – The epipolar lines in the captured
image from a parabolic mirror, and
corrected showing their vertical
orientation

(a)
(b)
Fig. 9 – Showing the structured light pattern in an
image (a) and after it has been removed (b)

3. Results
The top and bottom of the imaging position can not be
captured in this way though. For example an ornate ceiling
would be missed, or an exquisitely pattern floor. Fig. 8
shows the computation of depth from Gluckman et al [25].

Fig. 8 – Calculating depth from vertically
mounted panoramic images

A Canon 300D with 18-55mm lens and a 0-360.com
‘one-shot’ mirror were used in the experiments. The
structured light pattern is projected onto the scene using an
over head projector. The panoramas were taken vertically
with a 150mm baseline. Fig. 10 shows the difference a
structure light pattern projection makes to the results on a
flat surface. Outside the red line where no structured light
pattern exists there is a lot of depth noise, where as inside
the red line the depth is smooth. Fig. 11 shows another
example, with the original image and two depth maps, the
first from panoramas where no structured light pattern
projection was used. It can be seen from the depth map
that where there is an area of low texture information
correspondence is less accurate. The second depth map is
from two panoramas where the structured light pattern
projection has been used. The flat low texture surface has
been constructed more accurately due to the fewer errors
in the correspondence search.

The alternative to vertical orientation is to mount the
systems on the same horizontal level. This can be on
tripods, or like Zhu et al [26] mount one camera on a
tripod and the other on a mobile platform, or robot.
The user is presented with a user interface from which the
region of interest is selected. Using the calculations from
Gluckman et al [25] the depth of points are calculated.

2.3. Structured light pattern removal
Once the 3D model has been created a texture needs to
be applied if realism is to be achieved. The texture needed
for the 3D model needs to be extracted from the captured
image data, which has a structured light pattern projected
onto it. For a realistic texture pattern the noise has to be
removed. Median filtering techniques are used to remove
the pattern from the image. Fig. 9 shows an example of
the structured light pattern being removed from a wall
image. The noise pattern projection for increased

Fig. 10 – Structured light pattern projection
results showing fewer errors in the depth map
where the structured light is projected (inside
the red line)

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

4. Conclusion

(a)
(b)
(c)
Fig. 11 – An example region of interest (a) and
the depth without a structured light pattern (b)
and with a structured light pattern (c)
Fig. 12 shows an example panorama taken in an office
environment. The panorama shown is the base, also the
lowest, panorama. Fig. 13 shows an example 3D model
with the texture applied for visualisation. The depth data is
taken from the structured light pattern panoramas, and
only the region of interest has been reconstructed. The two
diagrams illustrate the visualisation of a 3D reconstruction
of a region of interest and panoramic environment. The
area marked in the red square is the region of interest, a
manual selection by the user. Once the user has made a
selection from the whole panorama the system then
calculates the depth from that region of interest for
visualisation. The user is then able to manipulate the
viewpoint to see the chosen area in 3D.

Panoramic imaging is a popular field of study, as is
stereo vision. Mixing the two fields has yielded stereo
panoramic imaging, enabling whole scenes to be
reconstructed from multiple panoramic images. The
problem with some scenes is they have areas of low
texture, such as plain walls. The paper has presented a new
method of projecting a structured light pattern onto these
low texture areas in order to increase the correspondence
search accuracy. Chosen areas of the panoramas can then
be reconstructed in 3D. The structured light pattern is
removed from the image using median filtering and the
result used to texture the 3D model for visualisation. The
visualisation can combine the panoramic render and 3D
object display together efficiently. Further work will be
undertaken to extend the approach for visualisation of
panoramic video and regions of interest within it.

Acknowledgement
The authors would like to thank the EPSRC and
Rotography Ltd for funding the program of study.

References
[1] E.E. Hemayed, A. Sandbek, A.G. Wassasl, A.A. Farag,
[2]
Fig. 12 – Example panorama taken in office
environment with region of interest selection in
red

[3]
[4]
[5]
[6]
[7]
[8]

[9]
Fig. 13 – 3D model of the region of interest from
the panorama with the structured light pattern

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

“Investigation of Stereo-Based 3D Surface Reconstruction”,
Proc. SPIE (Vol. 3023), Feb. 1997, pp. 191-202.
D. Gledhill, G.Y. Tian, D. Taylor, D. Clarke, “Panoramic
Imaging – A Review”, Computers and Graphics 27(3),
2003, pp. 435-445.
Y. Pritch, M. Ben-Ezra, S. Peleg, “Optics for Omnistereo
Imaging”, Foundations of Image Understanding, Kluwer
Academic, July 2001, pp. 447-467.
T. Svoboda, T. Pajdla, “Panoramic Cameras for 3D
Computation”, Czech Pattern Recognition Workshop,
Perslak, Czech Republic, February 2000.
T. Svoboda, T. Pajdla, V. Hlavac, “Epipolar Geometry for
Panoramic Cameras”, Fifth European Conference on
Computer Vision, Freiburg, Germany, June 1998.
S. Peleg, M. Ben-Ezra, “Stereo Panorama with a Single
Camera”, Proc. Computer Vision and Pattern Recognition
Conf., 1999, pp. 395-401.
H. Ishiguro, M. Yamamoto, S. Tsuji, “Omni-directional
Stereo”, IEEE Transactions on Pattern Analysis and
Machine Intelligence 14(2), February 1992, pp. 257-262.
P. Peer, F. Solina, “Mosaic-Based Panoramic Depth
Imaging with a Single Standard Camera”, Proceedings of
the IEEE workshop on Stereo and Multi-Baseline Vision,
2001, pp. 75-84.
R. Benosman, T. Maniere, J. Devars, “Panoramic
Stereovision Sensor”, 14th International conference on
Pattern Recognition, IEEE Computer Society Press,
Brisbane, Australia, August 1998, pp. 767-769.

[10] R. Petty, M. Robinson, J. Evans, “3D Measurement using

[27] F. Odone, A. Fusiello, “Applications of 2D Image

rotating line-scan sensors”, Measurement Science and
Technology 9(3), March 1998, pp. 336-346.
[11] IPIX, www.ipix.com
[12] 0-360.com, www.0-360.com
[13] S.K. Nayar, “Catadioptric Omnidirectional Camera”,
International Conference on Computer Vision and Pattern
Recognition, IEEE Computer Society Press, Puerto Rico,
USA, June 1997, pp. 482-488.
[14] T. Geb, “Real-time Panospheric Image De-warping and
Presentation for Remote Mobile Robot Control”, IEEE
Transactions on Robotics and Automation, 1998.
[15] A.R. Hicks, R. Bajscy, “Reflective Surfaces as
Computational Sensors”, The Second IEEE Workshop on
Perception for Mobile Agents, June 1999.
[16] J. Chahl, M. Srinivasan, “Range Estimation with a
Panoramic Visual Sensor”, Journal of the Optical Society of
America 14(9), September 1997, pp. 2144-2151.
[17] Y. Yagi, Y. Nishizawa, M. Yachida, “Map-based
Navigation for a Mobile Robot with Omnidirectional
Images Sensor COPIS”, IEEE Transactions on Robotics and
Automation 11(5), October 1995, pp. 634-648.
[18] K. Yamazawa, Y. Yagi, M. Yachida, “Obstacle Detection
with Omnidirectional Image Sensor Hyperomni Vision”,
IEEE International Conference on Robotics and
Automation, 1995, pp. 1062-1067.
[19] T. Svoboda, T. Pajdla, V. Hlavac, “Epipolar Geometry for
Panoramic Cameras”, 5th European Conference on
Computer Vision, Vol. 1406 of Lecture Notes in Computer
Science, Germany, June 1998, pp. 218-232.
[20] J. Gluckman, S. Nayar, “Planar Catadioptric Stereo:
Geometry and Calibration”, Conference on Computer
Vision and Pattern Recognition Vol. 1, IEEE Computer
Society Press, Colorado, June 1999, pp. 22-28.
[21] J. Arnspang, H. Nielsen, M. Chritensen, K. Henriksen,
“Using Mirror Cameras for Estimating Depth”, 6th
Conference on Computer Analysis of Images and Patterns,
Vol. 970 of Lecture Notes in Computer Science, Czech
Republic, September 1995, pp. 711-716.
[22] A. Goshtasby, W. Grover, “Design of a Single-Lens Stereo
Camera System”, Pattern Recognition 26, 1993, pp. 913937.
[23] S.A. Nene, S.K. Nayar, “Stereo with Mirrors”, 6th
Conference on Computer Vision, India, January 1998, pp.
1087-1094.
[24] D. Southwell, A. Basu, M. Fiala, J. Reyda, “Panoramic
Stereo”, 13th International Conference on Pattern
Recognition Vol. A, IEEE Computer Society Press, Austria,
August 1996, pp. 378-382.
[25] J. Gluckman, S.K. Nayar, K.J. Thoresz, “Real-Time
Omnidirectional and Panoramic Stereo”, Proceedings of the
Image Understanding Workshop, DARPA, Monterey,
California, November 1998.
[26] Z. Zhu, K.D. Rajesekar, E.M. Riseman, A.R. Hanson,
“Panoramic Virtual Stereo Vision of Cooperative Mobile
Robots for Localizing 3D Moving Objects”, IEEE
Workshop on Omnidirectional Vision, Hilton Head Island,
June 2000, pp. 29-36.

Registration”, Research Memorandum RM/99/15, HeriotWatt University.
[28] L.G. Brown, “A Survey of Image Registration Techniques”,
ACM Computing Surveys 24(4), December 1992, pp. 325376.
[29] C.L. Zitnick, T. Kanade, “A Cooperative Algorithm for
Stereo Matching and Occlusion Detection”, IEEE
Transactions on Pattern Analysis and Machine Intelligence
22(7), July 2000, pp. 675-684.
[30] P. Bao, D. Xu, “Complex Wavelet-Based Image Mosaics
using Edge-Preserving Visual Perception Modelling”,
Computers and Graphics 23, 1999, pp. 309-321.
[31] H.S. Kim, H.C. Kim, W.K. Lee, C.H. Kim, “Stitching
reliability for estimating camera focal length in panoramic
image mosaicing”, 15th International Conference on Pattern
Recognition, 2000, pp. 596-599.
[32] E. Bourque, G. Dudek, “Automated Creation of Image
Based Virtual Reality”, Proceedings of SPIE, “Sensor
Fusion and Decentralised Control in Autonomous Robotic
Systems” 3209, 1997, pp. 292-301.
[33] C. Schmid, R. Mohr, C. Bauckhage, “Evaluation of Interest
Point Detectors”, International Journal of Computer Vision
37(2), 2000, pp. 151-172.
[34] I. Zoghlami, O. Faugeras, R. Deriche, “Using Geometric
Corners to Build a 2D Mosaic from a Set of Images”, In
Proceedings of the International Conference on Computer
Vision and Pattern Recognition, Puerto Rico, June 1997.
[35] K. Mikolajczyk, C. Schmid, “Indexing Based Scale
Invariant Interest Points”, In International Conference on
Computer Vision, July 2001, pp. 525-531.
[36] C. Schmid, R. Mohr, “Image Retrieval using Local
Characterisation”, Proceedings of the 3rd International
Conference on Communicating by Image and Multimedia,
May 1996, pp. 45-50.
[37] D.G. Lowe, “Object Recognition from Local Scale Invariant
Features”, Proc. Of the International Conference on
Computer Vision, Corfu, September 1999.
[38] G.Y. Tian, D. Gledhill, D. Taylor, “Comprehensive Interest
Points Based Imaging Mosaic”, Pattern Recognition Letters
29(9-10), 2003, pp. 1171-1179.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

