Improving the Modified Global Smoothing Method for spatial distributed and
large Data-Sets to use it in the Real-Time Visualisation of Simulation and
Measurement Information
Gernot Opriessnig and Gernot Beer
University of Technology Graz
Institute for Structural Analysis/SiTu-Research Group
Lessingstraße 25/II A – 8010 Graz Austria
Tel.: +43-316-873-6686
Fax: +43-316-873-6185
email: gernot.opriessnig@ifb.tu-graz.ac.at

Abstract
With the high-speed development of computer
systems having performance values of calculation
machines, which needed one or more rooms of place and
a special air conditioning a few years ago, the usage of
methods dealing with a high amount of data at one side,
and the need of doing a enormous number of calculation
work at the other side became used more and more
frequently.
Tunnelling is one major field, where this advance is
very important. That field of science is regarded as one of
the more difficult engineering tasks. This is due to the fact
that tunnels are excavated in a material whose
mechanical and hydraulic properties are highly variable
and whose behaviour is difficult to describe to a
numerical simulation model. Numerical simulation of
tunnel advance can serve as a powerful tool in the design
and decision making on site. But this has to be done
almost in real-time, that means that the calculation and
the necessary post-processing must be finished, before the
engineer on site is forced to make his decisions. This
circumstance places a very specific challenge to the used
hard- and software.
In the beginning of 1997 the Austrian Science Fund
established a Joint Research Initiative (JRI) Numerical
Simulation in Tunnelling (SITU) [1, 2] with a budget of
about 3 Million € for 5 years. Currently about 24
professionals from 3 universities are working on the this
research initiative. The aim of the JRI is to develop a
framework for the application of computer based

numerical simulation tools. These tools can then be used
for the estimation of the required tunnel support and the
required construction measures at the planning stage and
during construction with the view of improving the
economy and safety of tunnels.
A very important task in this field is the visualisation
of the available usually very large amount of data. An
excellent quality, technical and physical correctness,
combined with real-time and user-friendliness in this area
is absolutely necessary to sell this kind of technology to
the engineers on the tunnel site. This paper describes an
algorithm, its development and improvements, which have
been done, to find a method of data processing for the
purpose of scientific visualisation with the necessary
properties.

1. INTRODUCTION
In our days a notebook, that can be seen almost as
standard, has a performance, which seemed to be
unreachable even in the beginning 90es. When I started
my research work in the field of scientific visualisation
1997, the hardware, I had to use was a SiliconGraphics
HighImpact Workstation. One year later, after receiving
extra funding, an SGI Octane MXI became affordable
and was bought. At that time, this was the best and most
expensive available desktop computer hardware. The
most important feature, and the major reason for
purchasing this was the graphics-performance, OpenGLcompatibility, the support of hardware texturing and the

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

ability of Quad-Buffering, which is necessary for smooth
OpenGL-Stereo animations in real-time.
Now this features, and a much more, can be reached
by a Dell Inspiron 8100 notebook, which is now the
actual developing environment for the Tunnelling
Visualisation System – TVS [3]. The major advantage of
this hardware is – without considering the price (less than
10% compared to the SGI-hardware) – is the mobility. As
mentioned above, a good visualisation is absolutely
necessary to sell the novel simulation techniques to
tunnelling engineers. Tunnelling has a very long history,
and a lot of people working in this area are very
conservative, so it is not easy to receive acceptance, when
novel methods are presented. Compared to a SGIWorkstation, which is very hard to transport, a handy
hardware, like the used notebook is an excellent argument
for the usability to the novel techniques.

2. AN OVERVIEW ON THE FEATURES OF THE
TUNNELLING VISUALISATION SYSTEM –
TVS
TVS allows the user to perform a virtual walk
through a tunnel which exists in computer memory only.
During the virtual walk through the model the user may
observe different results of numerical simulations and
geological features.

Tunnelling engineers usually are not very familiar
with the use of complicated computer-software, and they
should not be required to study huge user-manuals, to be
able to walk through the virtual tunnel. This was a major
aspect during the development of the Tunnelling
Visualisation System. Tests with persons without any
experience in the use of such a system (6 years to > 65
years) showed, that this has been very successfully. TVS
offers a high number of possible display options which
are detailed described in [4].

3. THE DATA-BASE AND THEIR SPECIAL
PROBLEMS FOR THE VISUALISATION
Basically there are two principally different kinds of
data-sources, which must be processed by the
visualisation tool: Simulation results and measured
values. Both types have different properties, but it is
absolutely necessary, to be able to picture the data from
both sources using one visualisation system. This is so
important, because the engineers on the tunnel-site must
be able to compare both types of information using one
software.

4. THE MODIFIED GLOBAL SMOOTHING
ALGORITHM
4.1 Introduction

Figure 1:Virtual Walk through the virtual Semmering
Tunnel (Austria) with photo realistic Textures, using a
head mounted display (HMD)
With the use of electronic shutter-glasses or a head
mounted display (HMD; Cybermind I-glasses 800*600)
the model appears 3D.

During the planning and building phase of a
tunnelling project a lot of data is collected. The
visualisation of this information and a user-friendly
possibility to compare it to the simulation results must be
possible with a state of the art visualisation system like
TVS. The first and most important step to reach this is
common geometric base for both types of data. As the
mesh, generated for numerical simulation is the most
accurate available description of the scene to be pictured,
the Tunnelling Visualisation System uses this one.
Unfortunately the measurements on site, e.g.
deformations of the tunnel-shape or the surface above the
tunnel (Figure 2), are not performed on points which are
defined on the simulation mesh. But even simulation data
are not always localised in a position, where they would
be needed for the visualisation. That has been the reason
to develop the modified global smoothing algorithm
[6,7,8], which has been implemented to the Tunnelling
Visualisation System; a very essential feature for this
visualisation tool.

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

z1

z2

a correct output, if the distance between all known values
and the searched one is the same. Figure 3 shows such a
configuration, and the equation to solve the problem.

zn= ?
z3

wi

Figure 2:Measured points (Z1, Z2 and Z3) and a location
(Zn), where a value is searched

n

w
The task, to be solved by the algorithm is to find one
unique value in each point of the space, if a couple of
localised data points is known. In the described case the
known values are the simulation or measurement results
and the values to be calculated are localised in meshpoints, which are needed for the visualisation
Compared to simulation data, the amount of data,
produced by measurements is very small. But the
requirements to the methods used are the same – no
change of the basic characteristics of the information.
The resolution of the mesh usually is much higher
than the number of measured points. A circumstance,
which must be considered during data processing. The
less information is available, the smoother the results
must become. This is caused by the theorems of Shannon
and Fourier [9,10], which say, that the content of
information can never be increased by any procedure.
That means that conversion algorithm, which makes
mesh-data out of the measurement information must
prevent any change of the resulting data which is more
detailed than the distance and number of input-data-points
can represent according Shannons-theorem. E.g. if there
are five points available, the curve connecting theses ones
cannot have a grade higher than four.
The modified global smoothing algorithm [6,7,8]
has been tested in a very extended way and after these test
it can be said that it fulfils the necessary requirements in
an excellent way.

4.2 Theoretical overview
The first part of the chosen solution is to take the
average of all known values, to find the searched one. In
[8] is explained, why the arithmetic average has been
chosen out of the other known possibilities to average
values. Performing only this calculation will only produce

w

1 n
* ¦ wi
n i1

wi
¦
i 1 n

n:

r

number of equidistant known points
Figure 3: Equidistant points

In practical applications this model in unusable,
because a configuration like this cannot be found very
often.

wi
n

i 1

2

i

n

1

i 1

i

¦r
n:
ri:

w

1

¦w * r
i

w

ri

2

number of known points
distance between searched value and point i
Figure 4: Random distributed points

Therefore it is necessary, to consider the distance
between each known point to the location, where a unique
value is looked for. A weighting function is added to
reach this. The chosen one [8] is a very simple one: the
square of the distance. Figure 4 shows the geometrical
properties and the now weighted average function to
solve the shown problem and find the value w.
The third aspect, which has to be considered is the
geometrical distribution of the known values. Ignoring
this produces results, will become unacceptable, if the
geometry is distorted, or the distance between measuring

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

points is irregular. The solution of this problem is the
following: To each point an area – in 3D a volume is
associated. It has been shown, how this parameter A can
be found very easy: in [11] for measurement data and in
[8] for simulation data. This value is added as an
additional weighting factor. That means, that values,
which are alone in their region have a greater influence to
the result as one in a couple of points
Figure 5 shows an overview of the some points in
2D, and their associated area, and the resulting algorithm.

A3

A2

& 1 n
&
x
* ¦ Ai * wi * xi
n i1
&
x:
Coordinates of the middle point
&
xi :
Coordinates of the point i

w3

w2

ri

w

w1

A1

wi

Ai

A4
w4

n

w

n:
ri:
Ai:

far away from the searched point. A tunnelling simulation
usually has this properties, because the area around the
tunnel is described by a very fine mesh, compared to the
other regions.
These circumstances cause, that all known values
must be considered. The idea to fulfil this and reduce the
number of terms to calculate is to subdivide the whole
domain in a certain number of sub regions. For each of
these parts the representing value is calculated like the
algorithm in Figure 5. This value is localised in one point.
The coordinates are found using a similar algorithm:

1
Ai * wi * 2
¦
ri
i 1
n
1
Ai * 2
¦
ri
i 1
number of known points
distance between searched value and point i
Area associated to point i

Figure 5: Random distributed points and their associated
area

4.3 Runtime optimisation
The results of the described algorithm has been
shown as excellent, but if the amount of data becomes too
high, the calculation becomes very time expensive, a
circumstance, that is disadvantage in real-time
visualisation. The best way to find the parameter A for
measurement data is shown in [11], for simulation data
this value is available due to the calculation method, so no
additional afford is necessary. The distance is found using
Pythagoras, which cannot be optimised. So the only way
to save time is decreasing the number of necessary steps
for one value. Theoretically the number of known points
is the number of terms to be calculated and added for each
searched point. Experience has shown, that points, which
are far away from the searched one have only few
influence to the results. But only ignoring such regions of
values causes a theoretical failure which becomes visible
in specific cases, e.g. if a high number of high values is

wi :
Ai :
n:

Value in Point i
Corresponding Area associated to point I
Number of known points in the sub domain

For the configuration shown in Figure 6 the failure
due to the described simplification can be found as
follows: (wi =1, Ai=1 is assumed)
Exact solution:

Using the simplification:

4

rexact

1
1
*¦ 2
4 i 1 ri

125 rsim

4*

1
2
rm

122

Which gives an acceptable percentage failure of
2,4%. This value decreases to zero for increasing
distances.

lim rexact  rsim  0

rmin o f

rmin :

Minimum of the involved distances

ri

rm

Figure 6: Comparison between the exact solution (left) an
the described simplification (right)
In 3D the domain is subdivided into cubic subdomains. The one containing the searched value and those
ones, that share one surface with it are considered exactly.
The others are included using the simplification. In the

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

worst case, if the searched point is not part of a surface
sub-region, for one point seven sub-domains must be
calculated exactly, all others simplified. This is an
enormous reduction of calculation time (app. 95% in
practical examples), because, as mentioned above, this is
being saved for each of the searched points.

T1
T2
T3
T1:
T2:
T3:

Without subdivision n*m*k terms have to be
evaluated and added.
m:
n:
k:

Exact Solution
4,03
6,05
11,10
11,10
5,44
5,05

2,01
11,10
6,47

8,07
11,10
5,44

10,09
11,10
6,47

linear increasing in x-direction
linear increasing in y-direction
distance from the centre

number of sub domains
number of known points/sub domain
number of searched points

For the subdivided domain the following steps are
necessary:
n*m calculations for the representing points of each
sub domain (the time for calculation for the location of
the representing point is very small compared to the one
for one averaging term, so it is ignored here)
In the worst case, if the searched point is an internal
one k*(7*n+m-7) steps are needed.
So the time 't rel saved by defining the sub domains
is given by:

't rel

Figure 8: Test configuration and the exact values in the
centre of the sub-domains along the line
The performed tests show an impressive quality of
the smoothed values. If the theoretically values are
compared to the smoothed results like shown in Figure 9,
this can be seen.

k * (7 * n  m  7 )  n * m
k *n*m

T1
T2
T3

This term goes to zero (Fig. 7) for large values for k,
m and m, that means that the larger the data set is, the
better the increase of performance is.

2,00
11,00
6,40

Smoothed Values
4,01
6,05
11,00
11,00
5,36
4,95

8,09
11,00
5,36

10,10
11,00
6,40

12,00

Trel
'

10,00

3
2.5

8,00

2
1.5

6,00

1
0.5

T1 (exact)

4,00

T2 (exact)

20

40

60

80

100

Number of subregions

T3 (exact)
T1 (smoothed)
T2 (smoothed)

2,00

Figure 7: Decrease of time due to subdividing

T3 (smoothed)
0,00
1

4.4 Parameter tests in 3D
The tests have been performed using the following
configuration: A couple of values in a cubic domain,
containing 106 distributed known values are calculated
one time without any sub dividing, and another time using
125 subdivisions. The smoothed values are calculated
along the line shown in Figure 8. Three sets of source
data are used: different types of linear changing and
parabolic changing ones.

T1
T2
T3

-0,47%
-0,90%
-1,11%

2

3

Failure
-0,58% 0,00%
-0,90% -0,90%
-1,54% -1,97%

4

0,29%
-0,90%
-1,54%

5

0,09%
-0,90%
-1,11%

Figure 9: Smoothed values; comparison to exact
ones and failure percentage
These tests have also been performed along other
lines crossing the cube and different distributions and

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

numbers of known values and subdivisions, all with the
same accuracy of the results. A failure percentage of less
than 2% for the purpose of visualisation is a value, which
can be accepted in every aspect of technical or scientific
applications.
The theoretically time for the tested configuration
compared to the one, needed for not optimised smoothing
is approximately 5%, that means the tested subdivision
into 125 parts with 106 known values gives almost the
same results 20 times faster than without the
modification, described in this paper. This save of
necessary calculation time also could be observed during
the tests. The original global smoothing algorithm took
about 7 seconds on an Intel Pentium III 1000 MHz, the
optimised one less than a half second.

6. ACKNOWLEDGMENT
The work reported here was supported by the
Austrian Science Found (Project Nr.: FWF-S08001-TEC)

7. References
[1] Beer, G. et. al. „Austrian Research Initiative on Numerical
Simulation in Tunnelling“ Proceedings of the World Tunnel
Congress 1999, A.A. Balkema/Rotterdam/Brookfield
[2] Beer, G. et. al. „Joint Research Initiative on Numerical
Simulation in Tunnelling“ Felsbau 1/99, Verlag Glückauf
GmbH Essen
[3] Opriessnig G. „Visualisation in Tunnelling – a Virtual
Reality Environment for Tunnelling Engineers“
Felsbau 1/99, Verlag Glückauf GmbH Essen

5. CONCLUSION
Numerical Simulation, advanced techniques of
measurement
and
high
performance-real-time
visualisation are inextricably parts in modern engineering.
Specifically a user friendly real-time system to picture the
data is necessary for the usability and with this for the
acceptance of this techniques in conservative fields of
engineering like rock-mechanics or tunnelling.
The improvements of data-processing for the
visualisation, the runtime optimised modified global
smoothing algorithm presented in this paper makes it
possible, to run a whole simulation software package on
an notebook The calculations and the tests made, are
showing, that the quality of the results is excellent, and
the time is no longer a problem, even for large data sets.
That means a mobile high-end visualisation system
– TVS is available on a platform, on which also the
numerical simulation (e.g. using the Boundary and Finite
Elements simulation software BEFE [12]) can run with an
excellent performance. This platform is a Dell Inspiron
8100 notebook, GeForce2 GO graphics-board,
Pentium III processor running with 1000 MHz. The
operating system is Windows 2000 professional.
This configuration makes it possible to use the
system, where it is needed: on a tunnel site or at any other
place e.g. if you want to provide a customer with a guided
tour through a future tunnel. This fact and the novel and
very innovative visualisation-techniques used by TVS
sum up to a system, which will be a milestone on the
absolute necessary and inevitable way to the application
of simulation and visualisation in almost all fields of
engineering as a matter of course

[4] Opriessnig G, Beer G. „Real-Time Visualisation of large
volumetric Data-Sets“ Fourth IASTED International Conference
Computer Graphics and Imaging (CGIM 2001) Honolulu,
Hawaii, USA
[5] Opriessnig, G. Beer, G.
”Application of virtual reality in rock mechanics” Proceedings
EUROCK 2001 ISRM
[6] Opriessnig G., Beer G. „Visualisation in Tunnelling –
New Developments“ Proceedings of 1999 IEEE Conference on
Information Visualisation IV´99
[7] Opriessnig G., Beer G. „Data Optimisation for the
Visualisation of FEM-Results“ Proceedings of 2000 IEEE
Conference on Information Visualisation IV´2000
[8] Opriessnig, G, et. al.
”Elimination the Influence of the Mesh Geometry to the Quality
of Global Smoothing” Proceedings 2001 IEEE Conference on
Information Visualisation. IV´2001
[9] Marks, Robert J.II; “Advanced Topics in Shannon
Sampling and Interpolation”; Springer Berlin, 1993
[10] Lenze, Burkhard; „Einführung in die Fourier Analyse (in
German)”; Logos Berlin, 1997
[11] Feigl, Wolfgang; “Entwicklung und Implementierung von
Algorithmen zur Visualisierung von FEM-Daten und
Messresultaten in einem Modell (in German)” Masters Thesis;
Institute for Structural Analysis, University of Technology Graz
2001
[12] „BEFE Users and Reference Manual“
Computer Software and Services (CSS) Graz Austria

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

