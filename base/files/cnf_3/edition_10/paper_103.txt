Representing Spatial Information through Multimodal Interfaces
R. Dan Jacobson,
Department of Geography, Florida State University, djacobso@garnet.acns.fsu.edu
Abstract
The research discussed here is a component of a
larger study to explore the accessibility and usability of
spatial data presented through multiple sensory
modalities including haptic, auditory, and visual
interfaces. Geographical Information Systems (GIS) and
other computer-based tools for spatial display
predominantly use vision to communicate information to
the user, as sight is the spatial sense par excellence.
Ongoing research is exploring the fundamental concepts
and techniques necessary to navigate through multimodal
interfaces, which are user, task, domain, and interface
specific. This highlights the necessity for both a
conceptual / theoretical schema, and the need for
extensive usability studies. Preliminary results presented
here exploring feature recognition, and shape tracing in
non-visual environments indicate multimodal interfaces
have a great deal of potential for facilitating access to
spatial data for blind and visually impaired persons. The
research is undertaken with the wider goals of increasing
information accessibility and promoting “universal
access”.

1. Introduction
People live and interact within a space-time continuum,
accessed and negotiated through their senses. As
geographers and others note, an understanding and
knowledge of the spatial world is fundamental to human
existence. Access to information, particularly that of a
spatial nature (maps, diagrams and graphs, for example) is
central to education, to navigation and ultimately to
finding employment. Throughout education and daily life
visualization is an increasingly important method for
understanding complex information (using tables, graphs,
plots, maps etc.) and for navigating around structured
information, (e.g. computer software, interfaces to
libraries and the world wide web).
The rationale for the research is that the multi-modal
presentation of data can give increased access to
information, and facilitate increased possibilities for
interpretation of scientific data through redundancy and

augmentation. Multi-modal interfaces promise to increase
the reliability of data interpretation through redundancy of
representation, increase the number of data characteristics
that can be analyzed simultaneously, and improve
navigation through higher dimensional datasets.
Redundancy differs from pure repetition, and means the
display of identical or related information in different
formats, such as text that relates to a map, or a verbal
commentary that accompanies a film. This is a wellvalidated principle in the human factors of design [1].
Wickens and Baker [2] list three potential advantages of
employing redundancy: (1) the information flow from a
computer to a learner is less vulnerable to shifts in
attention of the user (e.g. between the auditory and visual
channels) as the information is presented in both
modalities; (2) learning theory demonstrates more longterm memory storage of information presented in different
formats [3]; (3) information presented through different
modalities allows a user to adapt to the format of
information display that suits their own cognitive learning
style, or that they have sensory access to.
This reflects the way humans acquire information
about their environment, through direct experience (for
example, travel, wayfinding) or indirect means (maps,
diagrams, models). Information is acquired through
multiple modalities and frames of reference, including:
vision, audition, touch (haptic), movement (kinesthesis /
proprioception), vestibular (balance), smell (olfaction),
taste (gustation), and language [4].
Three broad areas for applied multimodal interface
research have been identified. (1) Non-visual interfaces
for use by vision impaired people. Here, through sensory
substitution, access is provided to information that would
normally be perceived visually, or with limited vision.
This promotes and addresses “universal access”,
facilitating novel ways of representing and exploring
spatial data. This in turn benefits all users, and can be
tailored to specialist needs, such as those of young
children and elderly people. This is the focus of the
research presented here. (2) In occluded visual interfaces
vision may be restricted: by data, for example in multidimensional displays, or virtual environments where
objects in the foreground occlude those in the

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

Traditional tactile diagrams are static, unintelligent
and inflexible - they can only be read by one person at any
time, they cannot be 'questioned', they cannot be
manipulated to change scale or perspective. Each diagram
has to be carefully produced for the tactile sense, which
exaggerates the classic cartographic representation
problems of simplification, generalization, aggregation
and classification [6]. These problems are further
compounded by labeling in Braille, as cells are fixed in
size [7]. Map interpretation is disrupted when separate
Braille legends are used [8]. Additionally only 15-25% of
legally blind people are able to read Braille [9].
Current conventional techniques for displaying
information non-visually rely mainly on synthetic speech
and Braille. This is problematic as it is difficult to access
the structure or form a holistic overview of even the
simplest information, in that it contains nothing of the
semantics of the information presented. The properties of
human short-term memory mean that listeners are unable
to hold in mind enough information to make any nontrivial observations – they become overloaded [10]. This
appears true whether listening to a reading of a table of
numbers or trying to image a map from a verbal
description. There have been longstanding research
agendas in the auditory domain (International Community
for Auditory Display [11]), and more recently the
emergence of haptic research [12, 13]. Some of these
techniques have been applied to convey spatial
information non-visually [14]

These problems have been highlighted by research
agendas, and political and legal agendas. For example, in
the United States, the National Academy report “More
than screen deep” (1997) and the development of “Every
citizen interfaces” to access the National Information
Infrastructure.
Presently there are only limited means for presenting
information non-visually that are not equivalent in terms
of ease of use, flexibility or speed, such as tactile
diagrams and text to speech interfaces. Being largely
unable to access this spatial information further
marginalizes vision impaired people. For a non-sighted
student to compete on equal terms with their sighted
counterpart they must be able to access and use the same
information.
Non-visual interfaces have the greatest potential impact
in the daily lives, and in the education of vision impaired
people, but offer benefits for all. The use of tactile, haptic
and auditory interfaces has the potential to make
technology more universally accessible. Information
technology transformations are affecting how we
communicate, how we store and access information, how
we become healthier and receive proper medical care,
how we learn, how we conduct business, how we work,
how we design and build things, and how we conduct
research. It is essential that disabled people are not shut
out from this technology by an emerging digital divide
(President’s Information Technology Advisory Committee
Report to the President, 1999). There are significant
benefits for investing in the National Information
Infrastructure (NII) and people with disabilities in that it:
Removes communications and information access barriers
that restrict business and social interactions between
people with and without disabilities; Removes age-related
barriers to participation in society; Reduces language and
literacy-related barriers to society; Reduces risk of
information worker injuries and; Enhances global
commerce opportunities. However, the transformation of
the Internet from a text-based medium to a robust multimedia environment has created a crisis – a growing digital
divide in access for people with disabilities [15]
Increasingly the Web is becoming the dominant internet
medium, with 60 percent plus of traffic, and of the Web
traffic, almost three-fourths is images which are
inaccessible to vision impaired people. As of July 25,
2000 President William J. Clinton released an executive
memorandum calling for a strategy for the accelerated
development and transfer of assistive technology and
universal design.

3. Societal relevance

4. Variables in Multimodal Interfaces

background. Alternatively the visual sense may need to be
directed elsewhere, such as in vehicle navigation systems.
(3) In augmented visual interfaces the interpretation of
two dimensional geospatial data, such as maps, can be
enhanced through the use of additional modalities to
present simultaneously information such as scale,
granularity, or metadata topics such as uncertainty and
reliability. These tools can be adapted to promote expert
learning and to guide or assist naive users through
information systems. These applied research avenues are
central to issues relating to of spatial representation. There
are naturally multimodal and naturally interdisciplinary,
blending ideas from Geography, Geographic Information
Science,
Cartography,
Psychology,
Philosophy,
Linguistics, Cognitive Science, Computer Science and
Artificial Intelligence [5].

2. Non-visual interfaces to spatial
information

For some sensory modalities, the variables for graphic
presentation are well-research and documented: for

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

example, visual variables, shape, texture, color, etc. [16];
tactile variables, roughness, etc. [17]; audio/cartographic
variables [18]; and less clearly, haptic variables, outline,
vibration, etc.. However, combinations of modalities are
task and scene specific, varying with sensory disabilities
and cognitive style. Few researchers have evaluated the
fundamental cognitive constraints in these forms of
display or further usability and interface issues.
A sample of interface variables, and their underlying
perceptual and cognitive factors that can be manipulated
in multimodal displays would include: In haptic interfaces
- force feedback; tactile feedback; proprioception,
kinesthesia. In auditory display – the general properties of
sound: temporal; spatial; amplitude; pitch; envelope;
timbre (spectrum, character, etc.); rhythm; repetition,
variation; causal properties (familiar sounds etc.). In
language based interfaces there are issues of spatial
linguistics; natural language; frames of reference; fuzzy
spatial prepositions [19]. In non-speech interfaces:
audiation; sonification; music; auditory icons; earcons,
and abstract sounds [20].
Oviatt[21]: 95) stresses that “…future map systems
ideally should be designed to be accessible to a broad
range of people, irrespective of age, sensory impairment,
skill level, or other considerations.” For ‘average’ users
the multi-modal presentation of data can provide intuitive
interaction with geographic information, making it
accessible and easy to use [21]. For expert users it has
significant potential to allow novel visualization [22].

Fourteen experimental participants (seven male and
seven female) were recruited from a human subjects
testing pool. A different set of participants were used for
each experiment.
Each subject was presented with
twenty-four shapes, with six small shapes being followed
by six large shapes in two sets. The order of shapes
within a size category was randomized. Each subject was
given one minute to explore the shape using the stylus and
touch window, haptic mouse, or a combination of haptic
and auditory cues. Data collected included, the choice of
shape identity, and a spatio-temporal coordinate log, of
their cursor movements. The mouse tracking program was
used to replay the tracing patterns to determine any
consistent techniques used by the subjects to identify
shapes and any general patterns of non-visual exploration
in the auditory realm.
Figure 1. Edmark Touch Window

5. Preliminary research
The preliminary research investigated the potential for
non-visual shape identification. Shape identification, or
“identity” being a spatial primitive [23]. This was
investigated in three different experimental settings: (1)
auditory spatially referenced sound, accessed through a
pen tablet. The tablet was filled with 2 auditory cues, one
indicating the background, the second auditory cue was
triggered when the stylus was over the shape; (2) Using a
force-feedback haptic mouse, with a “virtual wall”
surrounding the shape and; (3) the combination of haptic
and auditory information. Participants attempted to
classify 6 shapes at 2 different scales. The shapes
included: circle, cross, triangle, square, diagonal bar,
rectangle. The participants were sighted undergraduates,
who were blindfolded throughout the experiments. Each
participant was familiarized with the equipment, and after
several iterations of training sessions, to ensure that they
were fully aware of the goals of the experiment.

6. General methodology

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

Figure 2. Logitech WingMan Force Feedback
Mouse

7. Results
Figure 3: Sample visualization of the log of 4
participants mouse track.

haptic and auditory cues, in the core research areas listed
below put forward by National Center for Geographic
Information Analysis (NCGIA), University Consortium
for Geographic Information Science (UCGIS), Center for
Spatially Integrated Social Science (CSISS), and the
Association of Geographic Information Laboratories in
Europe (AGILE). Such applications would potentially
include: distributed computing, extensions to geographic
representation, visualization, scale, cognition of
geographic information, interoperability, spatial analyses,
education, public participation, access to spatial
representations in cross modal / amodal formats, and
accessibility.

8. Acknowledgements
Colleagues involved in this work include Prof. Reg
Golledge, and Matthew Rice of UCSB. Initial funding was
provided by the Research across the disciplines program
at the University of California at Santa Barbara.
Continued funding is provided by the National Science
Foundation Award HRD0099261.

9. References
[1]

Qualitative analyses of the mouse tracking logs showed
two main strategies for shape identification, ‘scanning’
and ‘probing’. Scanning involved movement across the
complete scene in broad sweeps left to right and up and
down. Probing involved the location of the shape, and
then movement around the shape, navigating on-shape /
off –shape, to trace perimeter of the shape.
Chance performance in shape identification was 1 in 6.
In all three experimental scenes, Participants as an
aggregate group performed significantly above chance. (1)
auditory spatially referenced sound, accessed through a
pen tablet the mean identification rate was 44 % with a
variance of 2.3%. (2) Using a force-feedback haptic
mouse the mean identification rate = 65.8 % with a
variance of 6.3%. (3) Using the combination of haptic and
auditory information, mean identification rate = 72.9%
with a variance of 3.5%. Further analysis is ongoing.
These preliminary results indicate the potential for
haptic and auditory interfaces to convey spatial
information to a non-visual user. Clearly there is a need
for the development of a theoretical schema and more
usability testing. These results are a starting point for a
future agenda of research in the multimodal presentation
of spatial information. There is the potential to integrate

[2]

[3]

[4]

[5]

[6]

[7]

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

W. R. Garner and G. L. Felfoldy, “Integrality of
stimulus decisions in various types of
information processing,” Cognitive Psychology,
vol. 1, pp. 225-241, 1970.
C. D. Wickens and P. Baker, “Cognitive issues in
virtual reality,” in Virtual environments and
advanced interface design., Thomas A. Furness,
Woodrow Barfield, Eds.: Oxford University
Press, New York, NY, US, 1995, pp. 514-541.
E. Tulving and D. M. Thompsom, “Encoding
specificity and retrieval processes in episodic
memory,” Psychological review, vol. 80, pp.
352-373, 1973.
D. R. Montello and S. M. Freundschuh, “Sources
of spatial knowledge and their implications for
GIS: An introduction,” Geographical Systems,
vol. 2, pp. 169-176, 1995.
National Center for Geographic Information and
Analysis, “The research plan of the National
Center for Geographic Information and
International
Journal
of
Analysis,”
Geographical Information Systems, vol. 3, pp.
117-136, 1989.
R. D. Jacobson, “Spatial cognition through tactile
mapping,” Swansea Geographer, vol. 29, pp. 7988, 1992.
A. F. Tatham, “The design of tactile maps:
Theoretical and practical considerations,”

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

presented
at
Proceedings
International
Cartographic Association: Mapping the Nations,
London, 1991.
R. A. L. Hinton, “Tactile and audio-tactile
images as vehicles for learning,” Non-visual
human computer interaction, vol. 228, pp. 169179, 1993.
J. Gill, Access Prohibited? Information for
designers of public access terminals. London:
Royal National Institue for the Blind, 1997.
S. A. Brewster, “Using non-speech sound to
overcome information overload,” Displays,
Special issue on multimedia displays, vol. 17, pp.
179-189, 1997.
G. Kramer, “An intoduction to auditory display,”
in Auditory Display: Sonification, audification,
and auditory interfaces, G. Kramer, Ed. Reading,
MA: Addison-Wesley, 1994, pp. 1-77.
M. L. McLaughlin, J. P. Hespanha, and G. S.
Sukhatme, “Touch in Virtual Environments:
Haptics and the design of interactive system,” :
Prentice Hall, 2001.
S. Brewster and R. Murray-Smith, “Haptic
human-computer interaction,” : Springer Verlag,
2001.
A. Hardwick, S. Furner, and J. Rush, “Tactile
access for blind people to virtual reality on the
World Wide Web,” presented at IEE Colloquium
on Developments in Tactile Displays (Digest
No.1997/012) IEE Colloquium on Developments
in Tactile Displays, London, UK, 1996.
C. D. Waddell, “The growing digital divide in
access for people with disabilities: overcoming
barriers to participation,” presented at
Understanding the Digital Economy, Washington
D.C., 1999.
J. Bertin, Semiology of Graphics: Diagrams,
Networks, Maps. Madison: University of
Wisconsin Press, 1983.
R. Vasconcellos, “Knowing the Amazon through
tactual graphics,” presented at Proceedings, 15th
Conference of the International Cartographic
Association, Bournemouth, UK, 1992.
J. B. Krygier, “Sound and Geographic
Visualisation,” in Visualisation in Modern
Cartography, A. M. MacEachren and D. R.
Fraser-Taylor, Eds.: Pergamon, 1994, pp. 149166.
P. A. Bloom, M. A. Peterson, L. Nadel, and M.
F. Garrett, “Language and space,” . Cambridge,
MA: The MIT Press, 1996.
G. Kramer, “Sonification report: status of the
field and research agenda,” National Science
Foundation White Paper, 1997.

[21]

[22]

[23]

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

S. Oviatt, “Multimodal Interactive maps:
Designing for human performace,” HumanComputer Interaction, vol. 12, pp. 93-129, 1997.
A. D. Blaser, “User interaction in a sketch-based
GIS user interface,” presented at COSIT '97,
Pittsburgh, 1997.
R. G. Golledge, “Primitives of spatial
knowledge,” in Cognitive Aspects of HumanComputer
Interaction
for
Geographic
Information Systems, T. L. Nyerges, D. M. Mark,
R. Laurini, and M. J. Egenhofer, Eds. Dordrecht:
Kluwer Academic Publishers, 1995, pp. 29-44.

