Cooperative Robot Teleoperation through
Virtual Reality Interfaces
*Monferrer, Alexandre

&

* Facultat de Nàutica.
Depart. Enginyeria de Sistemes, Automàtica i
Informàtica Industrial.
Universitat Politècnica de Catalunya.
Pla de Palau #18. 08003. Barcelona. Spain.
E-mail: <amonferrer@esaii.fnb.upc.es>

Abstract
Robots are employed to do exacting routines,
ranging from the common place to the difficult and
from the relatively safe to the highly dangerous.
Remote-controlled robots -or teleoperation- is one
way to combine the intelligence and maneuverability
of human beings with the precision and durability of
robots. Teleoperation can be difficult, due to the
complexity both of the system and of information
management –and more difficult still in a cooperative
environment in which multiple teams are working
together in different locations.
To facilitate
teleoperation, information visualization and a clear
communication reference must be deployed in
conjunction with an enhanced human machine
interface (HMI) among all participating teams.
The aim of this paper is to present a set of
guidelines defining an ideal user interface utilizing
virtual reality desktop for collaborative robot
teleoperation
in
unknown
environments.
Enhancements in information visualization are
discussed, and the case of an underwater robot is
presented, because of the special challenges they
present: a slow response system and six degrees of
movement.

1. Introduction
Teleoperation is the manipulation of an object (in
this case a robot), in such a way as to allow an
operator to perform a task at a distance. This is
usually done because of a hostile environment where
human access is difficult, but human intelligence is
necessary [11]. Teleoperated robots use different
sensors to let the user operate them safely. In the Fig.
1, a schematic of GARBÍ (the UPC underwater robot)

**Bonyuet, David
** Delta Search Labs
400 Technology Square
Cambridge, MA 02472
USA
E-mail: <dbonyuet@deltasearchlabs.com>

is presented, describing the different elements that
interact in the robot control. To work more efficiently
in an unknown environment robots need enough
sensors to emulate human senses [1, 12]. Multiple
thrusters are also required to move robots in the
desirable direction (T1–T4, in Fig. 1). Additional
sensors are required to measure the motor’s
performance. Devices that provide outputs and
require inputs also serve to enhance control [8, 9].
Each technical improvement demands an additional
effort in control and data presentation.

Fig. 1.- UPC underwater robot: GARBÍ

User
interfaces
for teleoperated robot
started basically with
video
images
[8]
coming from cameras
installed in the remote
robot
(Fig.
2);
additional data in
Fig. 2.- Interface for teleoperated
multiple
formats
robot from [8]
(numbers, characters,
etc) were added. This was an easy start to develop the
user interface; however, the operator of such system
required extensive training and broad experience in
multiple fields [12, 13].
Virtual reality (VR) was added as a tool to
enhance the plane camera images [2, 5, 9, 14]. VR
helps to solve some of those problems -like the spatial

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

representation and object programming, for examplebut it also introduce some new issues: viewpoint
control and spatial location (i.e., where the robot is in
relation to the environment) [4]. Bejczy et al used VR
to teleoperate robotic arms in the space [2]; Fong et al
used it in robot teleoperation using PDAs [9]; Nguyen
et al are working VR in remote robot control for
planetary surface exploration [11].
Further
applications in VR and robotics can be found on
Burdea’s paper [6].

a.-

b.-

c.-

Fig 3.- Different viewpoints from a virtual world,
using the interface developed in this project

A teleoperation task must display a great deal of
information. Fig 3 shows three different perspectives
using virtual reality. There is not real camera that
allows these viewpoints. With virtual reality, one is

User

Computer

able to figure out how to view a problem and complete
a task in multiple ways. Fig. “a” shows the underwater
robot from the left side; Fig. “b” from the backside;
while Fig. “c” is inside the robot (as if the user were
operating from inside the robot). Understanding the
effects of information distribution and presentation in
control and visualization of teleoperated robots is the
key topic of research [6, 13] at the moment.

2. Information Sharing and
Collaboration
While sensor technology has improved
significantly, our ability to convey the information to
the users with ease and precision has not [13]. This
situation worsens when the data is shared among
multiple users, each in a different location and, worse
yet, working on different cooperative activities. A
group of users in a one part of the world, for example,
may be working the installation of an underwater pipe;
another group, far away, may be bringing all the
resources and tools from the cargo ship; a yet another
group could be performing analysis of the data in a
remote center. Such a project requires a common
coordination: items must be brought, and if possible
assembled, with high-level coordination among all the
teams. The situation could also arise where a team
might not directly control a robot, even though they
must visualize all the information. Fig. 4 shows the
various elements participating in robot teleoperation:
users in different locations, each of whom works with
his own human machine interfaces (databases,
computers, etc) at point distant from wherever the real
working activities are actually being developed.

Environtment

Computer

User

Robot

HMI

HMI

Location 2
Location 1
Fig. 4.- Elements in the user interaction in a collaborative robot teleoperation task

Fig. 5 describes the building blocks of a
collaborative robot teleoperation. There are three
groups: two controlling one robot apiece and a third
one functioning as a supervision center. Each user
group functions as a distinct entity with clear,
independent objectives yet also with a common goal
requiring them both to share information and to
understand the entire process, in addition to their own
particular role in or contribution to it. Collaboration is
paramount, and human machine interfaces are the key
to it. The common tool is that interface that allows

collaborating user groups to communicate their
objectives and tasks with one another. Obviously,
social environments will influence different users
differently and could lead them to various
interpretations of the same information. Nor can the
influence of the physical environment be discounted where the team is located. Permanent communication
and visibility through the interface would allow them
to resolve any such issues that arise. Communication
is based in a network link maintaining continuous
flows of information among the participating teams.

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

Control Center 1
Social
Environment

External
Conditions

User 1

Supervision Center

Control Center 2
Physical Environment

Physical Environment
Network Link

Physical Environment
Social
Environment

User 2

User n

HMI

HMI

HMI

Computer:

Computer:

Computer:

- Storage

- Storage

- Processing

- Processing

- Storage
- Processing

Robot:

Robot:

- Sensors
- Operation

- Sensors
- Operation

Network Link

Social
Environment

Working
Activities

Fig. 5.- Block elements in the user interaction in a collaborative teleoperation

3. Information to show
Technological advancement produces a deluge of
information the user risk drowning. The Internet, an
infinite highway of data, for example, can easily
become an infinite maze. When it comes to robots,
the situation is still more complex. Now it is possible
§
§

§

§

§
§

§
§

§
§

Visualization:
Working area. The users must be able to see what they are doing in
the teleoperated robot.
Position information and spatial orientation information. This
provides information about where the robot is heading in relation to
the north, the mother ship and to show to the user where they are.
This information might come from Acoustic Transponder Networks,
Doppler, Sonar, Inertial Navigation Systems, GPS, etc.
Navigation information. Some parameters are needed while the
user is driving the robot:
Where the north is.
Speed and depth.
Location of the robot in relation to the sea surface and the
bottom.
Internal system information. Information about the system status
and other technical stuff. These include the visualization of
equipment sensibility, threshold, communication parameters, data
protocol etc.
Numerical information. Show all the numerical data about the
robot. By this function, the user would be able to show or hide –at
convenience-, the task relevant information.
Enhanced displays: Virtual image, compass and keymap. These
functions enhance the robot controllability. A compass is a
traditional orientation device to signal the north. A keymap is an
intelligent map showing references about the user location, targets,
origin and location of other virtual marks.
Help information. On-line help is provided in three levels: active
tags, medium information windows and advanced help.
Individual parameters. Additional information helps the user to
understand better the robot operation:
Underwater currents affecting the robot.
Power. The robot internal battery status.
Partners Visualization. Location and movement direction of the
other member of the team in the virtual environment should be
clearly displayed.
Messages in Transit flag. This indicator would warn users about
delayed messages.

to fill any mechanical shell with literally thousands of
electronic devices and make it function as a robot.
Every one of the robot’s individual sensors provides a
minimum of data that must be processed and that
should, at least in some cases, be shown. In fact, the
sheer quantity of data available for any given situation
amount to information overload (See Table 1).
§
§
§
§
§
§

§
§

§
§
§
§

Control:
Vehicle control. There are several motors to move the robot in
the six axes of movement and rotational displacement (left and
right).
Virtual reality control. Due the virtual reality tool used, now
the user has to deal with the power to change the point of
view.
Interface control. It includes the ability to perform file
operations and other task related to the program and computer.
Arms control. T he robot is equipped with external arms to
perform different task. This function activates the robot arms.
Speed control. This function allows the user to increase the
robot speed.
External system control. External lights and specific sensors
are activat ed by this set of controls. Stabilization (keep the
robot statically in that place) icon is another example of
external system service.
Route definition. Specific routes in the workplace can be
defined using this function.
Internal system configuration. Using this function, which will
open a window with all the configurable options, can set up
the robot internal system. These include the sensors available
to measure the motors performance and calibration of sonar
system, range finder, communication parameters, data
protocol, etc.
Bottom sea configuration. The virtual bottom sea should be
configured through this function, to allow the users to updated
the working area map (according to new information).
Tracking Control. This function would allow the user to track
specific objects (pipeline, etc) or other robots in the working
environment.
Message control. Transmitting information to the other peers
and to the master is fundamental in a collaborative scheme.
Flag mark control. The user may decide to place a signal in
the virtual environment visible for other teams working with
other robots (tagging an area) or to signal places for direction
purposes.

Table 1.- Requirements in teleoperation for visualization and control for underwater robots

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

But why, in fact, does all this data have to be
shown? The operator of an automobile does not have
to watch the meters and gauges on the dashboard at
the same time while he/she is driving. Some gauges,
in fact, are designed to be checked only when the car
is in service (e.g. the oil meter, brake liquid meter,
etc). In other words, the information while available
need not be presented to the user unless he/she asks
for it specifically. A robot’s depth, for instance, is not
always relevant, but can be useful in some cases, and,
in others, indispensable.
Information relevance
depends on both user activities and context.
Controlling a teleoperated underwater robot offers
several challenges to the normal user. Depending on
the project mission, the user group will have its own
set of required skills: biologist, oceanographer, pipe
inspector, etc. Additional knowledge is also required:
robots are equipped with multiple sensors to measure
internal and external conditions; and various motors to
control their displacement and control several
appliances. Every single device in the robot provides
information that must be processed.
Cantoni et al said that “a GUI can display only a
limited number of icons without cluttering the screen”
[7]. Previous work developed by the authors has
proven that interfaces design must be evaluated
according to the target user, task and system [4 - 5].
Trading off these parameters on behalf of a unified
interface represent a recursive process of design evaluation - review [4]. Underwater teleoperated
interfaces are not very common and as Table 1 shows,
there are specific requirements in control and
visualization for this robot; data for this table were
gathered through extensive information collection
from expert users and technical requirements in the
system in authors’ previous works [1, 4, 5]. As if not
all this was enough, we must also consider the
redundancy factor -only for critical, specific data.
Some parameters must be present in different formats
for different users as well as to increase the data’s
visibility (as the old saying goes: “It is better to be
safe than sorry”). The double visibility of specific
parameters is intended to double the user’s awareness
of information vital to the task. Knowing the point of
origin can be essential, for example, for guiding the
user in the working area and for returning to the
mother ship. Displaying that information both in
different formats and in different places (in the virtual
working environment and the virtual compass) would
help the user to correlate all the information clearly a
concisely. For example, signaling the north is useful in
the working screen, but that information should be
repeated in the keymap (map with the working area);
additionally, that information should be repated in any
instrument that provide guidance, like the compass or
gyroscope.

4. Enhanced Landmarks and Tools
for Improved Visualization in
Collaborative Environments
Taking in account the issues discussed so far, it is
possible to group some VR design concerns in three
sections: user interface, technical and VR issues.
These points come from a review in similar works and
the authors’ previous research [1, 4, 5]. These issues
should be considered in the development of man
machine interfaces for teleoperated robots in
collaborative environments:

4.1. User Interface Issues
a) Visible Navigation Aids : Presenting navigation
aids in a physical way will allow users (especially
novice ones) to drive the system better, these
navigation elements should include true north and
drift direction among others.
b) Customized Reference Data: whenever the user is
around a specific point in the working area, the
ability to mark some areas (and possibly to share
that data with fellow users) would be highly
advantageous.
c) Chat Channels: Special channels for message
communication should allow members to share
information about the environment, the target and
other related data. Voice communication would be
preferable for fast transaction; it can also be
recorded for auditing purposes. For easy log
verification and review, however, a text line chat
would provide a more detailed history of the
activities performed and help beginners to track
down working instructions.
d) Redundancy with Critical Data. Informing users
about critical data is fundamental [3]; it also
underscores, for the novice user in particular, the
importance of changes in that information.
Distintive sounds can be used to signal critical data
variation, however, users should be trained about
the sounds meaning.
e) Attractive Data Presentation. Certainly there is
more than one way to skin a cat, and some are
more appealing than others to the general audience
of your system. It is worthwhile, therefore, to take
the time to find the best, user-friendly way to
present your information.

4.2. Technical Issues
a) Positional Targets: Specific working location on
the ocean floor should be marked as reference
points to help guide users to that place.

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

b) Obstacle Detection: Presentation of possible
obstacles in the working trajectory directly in the
image would be a useful feature, especially for
new users.
c) Tracking Route: Tracking activities would be
better performed if the trajectory to be tracked
were highlighted. In an automatic system, one can
rely on the tracking algorithm; in some cases
tracking is related to an inspection task -checking
the status of underwater pipelines or
communication lines.
d) Reference Information about Other Users: In a
collaborative environment where other users are
performing other activities, the presence and
activities of all concerned should be clearly
described to everyone both to facilitate
communication and prevent misunderstanding.
e) Data Fusion. Information from multiple sensors
like (sonar system, thermal image, and even the
laser system) might be integrated in a single
picture, showing the relevance of each information
to the user. In some cases, multiple images in
different windows may provide clues for specific
tasks.
f) Communication
Latency.
Working
with
teleoperated system involves in some cases to
work with delayed data, but when a collaborative
group is considered, the latency worsen the
communication (and robots’ controllability) among
the team and the robots.
Information
synchronization should be planned in the data
exchange.

4.3. Virtual Reality Issues

difficulty with spatial problems, an additional
object must be presented to simplify such inquiries.
Our solution? Virtual shadow projection on both
the surface and the bottom of the sea.
d) Virtual – Reality Synchronization. Objects in the
virtual world must be synchronized with the one in
the real world, to provide a meaning interaction.
Communication latency should be taken in account
to avoid data misinterpretation.

4.4. KISS: Keep it Simple and Short
Useless data can quickly clutter the user’s screen.
It is certainly tempting to present everything in every
possible format. The data is there anyway, so why be
stingy –or discriminating- with it? First of all,
because distract the user. Nevertheless, there are other
reasons as well:
a) The more unnecessary data on the screen, the less
space available for important data.
b) Multiple formats, like alphanumerical data, can be
hard to digest at critical moments.
c) Excess information requires a good amount of time
to understand and to use the interface correctly.
d) If an element or graphic in the interface does not
communicate anything meaning or significant, then
delete it!

5. The Virtual Reality Interfaces
Two interfaces were developed using virtual
reality desktop as a data visualization enhancement
tool, one of them focusing on information (Fig 6), the
other on controllability (Fig. 7).

a) Natural Landmarks: Certain natural landmarks
are easily utilized as references for certain
activities; not every underwater boulder, however,
can be used to signal a specific point. Users must
choose this option sparingly, considering other
possible options in their immediate environment,
and better yet, decide with the entire team about
what would be the landmarks to be used.
b) Virtual Route: Virtual reality allows the interface
to highlight specific sections with pale colors
(transparent information), offering a clear and
unmistakable path to the target. Obviously, the
task must permit this feature: in the case of a
random searching task, there is no route to
illuminate.
c) Special Marks: Objects floating about in a virtual
world have to provide additional information to the
users. Imagine, for example, an object floating in
the middle of a hall: what is its real position in
relation to the ceiling? Since some users have

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

Drift
Speed

North and
Robot
direction

Virtual
Route

Mother Ship
Virtual
shadow

Robot

Location
data

RobotData

RouteConfig
BottomConfig
Obstacles
Light
Help & Command
Control
Mode

Automatic
Mode
Manual
Mode

Arm
control

Help

Stabilization
control

Communication

Fig. 6.- Interface for robot teleoperation with emphasis in
information visualization

The interface in the Fig. 6, emphasizing
information visualization, provides all the basic
information required for the specific task in separated
windows, but offers minimum control. The interface
in the Fig. 7, focusing on controllability, displays
several icons to control all those possible elements
needed to perform specific activity-related tasks. Both
interfaces foster smooth team communication and a
cooperative environment.
Virtual
shadow

Internal
Status

Mother Ship
Robot

North

Config
Bottom
config
Stabilization
Arm control

[3] Bersen, N.O.. “Modality Theory. Supporting
multimodal interface design.” Proceedings of the
workshop ERCIM on HMI, Nancy, Nov. 1993.
[4] Bonyuet, D & Monferrer, A. “Designing Virtual
Interfaces for Teleoperated Robots”. The 2nd World
Multi-Conference on Systemics, Cybernetics and
Informatics: ISAS' 98 – SCI’98, Orlando, 08/1998.
[5] Bonyuet, D.; “An Approach to Facilitate the
Teleoperation of a Vehicle in the Space 3D”, ERCIM
W o rkshop: “Towards User Interfaces for All: Current
Efforts and Future Trends”, ICS-FORTH, Heraklion,
Crecia, Grecia, 30 - 31 de octubre de 1.995.
[6] Burdea, Grigore. “Invited Review: The Synergy
between Virtual Reality and Robotics”. IEEE
Transactions o n Robotics and Automation. Vol 15, N
3, June 1999. pp 400 – 410.

Route Setting
Save data
Numerical
data

[7] Cantoni, Virginio; Levialdi, Stefano and Roberto, Vito.
Artificial Vision. Image Description, Recognition and
Communication. Academic Press. GB. 1997.

Help
Keymap

Communication
Help

G y roscope

ViewPoint
Control

Robot
Control

Fig. 7.- Interface for robot teleoperation with emphasis in
controllability

6. Conclusions
Each new version of user interface will improve
on previous performance standards and correct
previous defects. The more logical its arrangement,
the better, in all likelihood, the interface. Functionally
purposes, logically, should be correlated to the user’s
activity; while attention paid to style and design
insures not only greater efficiency but also aesthetic
harmony. The best interface would consider the user
to be the important input in its systemic operation.
Collaborative environments, however, require an
additional step in design and to improve information
visualization should include unmistakably identified
communication elements within the interface.

7. References
[1] Amat, J.; Batlle, J.; Casals, A. & Forest, J.; “GARBI:
the UPC Low Cost Underwater Vehicle”, Joint US /
Portugal Workshop in: Undersea Robotics and
Intelligent Control, march, 2 - 3 / 1995, Lisboa,
Portugal, pp. 91 - 97.
[2] Bejczy, Antal; “Virtual Reality in Telerobotics”, ICAR,
Barcelona, Spain, 09/1996, pp 3 - 12.

[8] Fiorini, P; Bejczy, A. & Schenker, P., “Integrate
Interface for Advanced Teleoperation”, IEEE Control
Systems, Vol. 13, N 5, 10/93, pp 15 - 19.
[9] Fong, T.W.; Grange, S.; Conti, F.; and Baur, C.
“Advanced Interfaces for Vehicle Teleoperation:
Collaborative Control, Sensor Fusion Displays, and
Remote Driving Tools”, Autonomous Robots 11. Vol.
1. July 2001. pp 77 – 85.
[10] Kotoku, T. “A predictive display with force feedback
and its application to remote manipulation system with
transmission time delay”. Proc. 1992 IEEE. RSJ Int.
Conf. Intelligent Robots Systems. pp 239 – 246.
[11] Nguyen, Laurent; Bualat, Maria; Edwards, Laurence;
Flueckiger, Lorenzo; Neveu, Charles; Schwehr, Kurt &
Zbinden, Eric. “Virtual Reality for Visualization and
Control of Remote Vehicles”. Autonomous Robots.
Vol. 11. No 1. July 2001. pp 59 – 68.
[12] Rigaud, V.; Coste Manière, E.; Aldon, M.J.; Probet, P.;
Amat, J.; Casals, A.; et al.; “Union: Underwater
Intelligent Operation and Navigator”; IEEE Robotics &
Automation Magazine; Vol.5 N.1, 03/98; pp 25-35.
[13] Stanney, K; Mourant, R & Kennedy, R. “Human
Factors Issues in Virtual Environments: A review of the
literature”. Presence: Teleoperators & Virtual
Environments. MIT. Vol.7. N 4. 08/98. pp 327 – 351.
[14] Stone, R., “Advanced Human-System Interface for
Telerobotics using Virtual Reality and Telepresence
Technologies”, ICAR-91, Fifth International
Conference on Advanced Robotics, Pisa, Italy, 19 – 22,
06/91, Vol. 1, pp 168 - 173.

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

