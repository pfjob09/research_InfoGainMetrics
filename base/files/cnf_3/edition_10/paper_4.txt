Rigorous Exploration of Medical Data in
Collaborative Virtual Reality Applications
Fred Dech, Jonathan C. Silverstein, M.D.
Department of Surgery, University of Chicago, Chicago, Illinois
fdech@uchicago.edu, jcs@uchicago.edu
Abstract
We describe a virtual reality widget library and two medical applications. These applications, built on the widget
library, make use of collaborative interaction techniques.
These techniques support a high degree of precision with
respect to manipulation of data and data parameters. The
3D widgets are synchronized between collaborating clients
in order to facilitate the high degree of interactivity necessary for productive investigation of shared medical models
and data.
We discuss in detail the different challenges which face
the investigator in an immersive 3D environment as opposed to that of a 2D desktop environment and how these
differences have lead us to the criterion used for the development of the shared 3D Virtual Reality (VR) graphical
user interfaces (GUIs) used in the biomedical applications
presented.

1 Introduction
Traditional user interaction with medical diagnostic and
educational applications involves a keyboard and mouse,
with an operating system or window-manager containing
various tools. Such 2-dimensional environments are most
often displayed on a desktop CRT. It is routine on these
desktop applications to manipulate multiple parameters
which in turn affect the presentation of the medical data
being examined. A high degree of accuracy and precision
can be maintained through keyboard and mouse input and
manipulation of 2D widgets in an application's user interface.
While they may still be utilized in non-immersive or
fish-tank VR, the keyboard and traditional mouse are no
longer suitable in immersive VR. Our definition of immersive VR is a system which uses a sufficiently wide field of
view, stereo imaging, and tracked, viewer-centered perspective. Collaborators collectively immersed in such a
computer-generated environment must use a different set
of tools in order to interact.

Our decision to develop VR applications rather than 2D
desktop medical applications is motivated by the fact that
anatomical data, in its entirety, is spatially extremely complex. We believe that examination and learning about anatomic structures is greatly facilitated by using an immersive
VR environment to study them because it is more information rich and enables clinician directed collaboration and
highly intuitive, interactive visualization.
Here we would like to make a distinction between two
different methods of operating in VR. One method places
the users within a space to be navigated and explored. In
this method, remote collaborators are often far apart from
one anther in virtual space. They are typically represented
by anthropomorphic avatars of varying degrees of sophistication. They must have some means of navigating through
the large virtual space which surrounds them.
A second method places the users together with an object or objects in space to be examined. Here, the investigators are not exploring a space, per-se, but, rather they are
congregated together in virtual space to examine and manipulate a data object. This is typical of medical and biomedical investigation in VR. The data representations in
virtual space are rarely much larger than the clinicians
conducting the investigation, and are often smaller. The
need for navigation in this second method or approach is
less important or completely absent, since there is literally
no place else to go to. All remotely connected users are in
close virtual proximity to one another. The data being investigated is already virtually within touching distance to
all participants.
The first model occurs predominantly in industrial,
architectural and art related applications. The second
model occurs most commonly in clinical medicine or biomedical research. We will refer to the first method as a
Virtual Reality Space Method, or a VRSM, and to the second method as a Virtual Reality Object Method, or a
VROM.
We have chosen a workdesk-style VR system for most
of our biomedical research. The ImmesaDesk®C1 and
ImmersaDesk®R2 (Fakespace Systems Inc.) [2] are widescreen, rear-projected systems reminiscent of a drafting
table. They utilize active frame-sequential stereo, and head

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

and hand tracking via an electromagnetic tracking system.
The screen has a display area approximately 6ft x 4ft and is
elevated to about waist height. The screen is set at an angle
with the top approximately 45deg back. This angle enables
simultaneous down and out viewing. For medical applications, we prefer the Desk to the CAVE® (Fakespace Systems Inc.) [1]. While the CAVE is far superior for VRSM
applications, the Desk is at least as good, and often better,
for the medical VROM applications on which our research
is focused.
As an input device, we use the Wand™ or V-Wand™
(Fakespace Systems Inc.). Both devices have three buttons
and a joystick-like control. The hand-tracking sensor is
normally attached to, or embedded in the wand. The wand
is a 6 degrees of freedom (DOF) interaction device and is
in many ways analogues to the 2 DOF mouse.
Our VR research has not included the use of Head
Mounted Display (HMD) devices thus far. This is primarily due to the fact that our chosen projection-based devices
allow for multiple users to easily collaborate at a single VR
device. This feature increases the educational utility of our
applications, since approximately five participants can effectively learn at each VR station.

2 Design goals and implementation
The need for a re-usable library of 3D widgets capable of
precise interaction in VR environments is strongly apparent when conducting biomedical VR application development. Many add-hoc GUIs and even some with a high degree of re-usability are commonly in use in VR applications. These are often perfectly adequate, especially in
VRSM applications where frequently the primary goal is
navigating through a space, triggering events through proximity sensors or other means, and picking and then transforming objects.
Systematic investigation of biomedical information, on
the other hand, requires the ability to manipulate a vast
array of parameters; some with a high degree of precision.
Additionally, maintaining GUI and data state synchronization across multiple sites that are connecting and disconnecting to the application is critical if the collaborative VR
investigation is to remain scientifically valid.
The collaborative applications to be discussed in this
paper make use of a generalized VR widget toolkit, vwLib
[14], which we have developed in order to facilitate 3D VR
GUI construction. The 3D VR GUI is an integral aspect of
our applications. Not only does it make precise manipulation possible on a local level, but it is also tightly coupled
with the Tele-Collaborative libraries used. This results in
complete GUI and data synchronization across remote clients. It allows for multiple clients to interact with the data
in a semi-simultaneous manner. Any user at any site can
immediately take control of the GUI to modify the data

representation. No specific gesture or interface action is
required to take control. In fact all users technically have
complete privilege to manipulate the environment at all
times. This does not turn out to be chaotic. The interactions
end up working in much the same way that multiple persons carry on a polite group conversation. One user has the
floor and other remote collaborators observe and interact in
a constructive manner until control is yielded to another
site. Shared channel voice communication (with echo canceling) via streaming audio facilities the cooperative approach.

2.1 Interface library description
In designing vwLib, our primary goals were re-usability,
efficiency and ease-of-use. We use an object-oriented approach written in C++. For event generation and propagation, we use a combination of an observer pattern with a
mediator [6].
It is important to stress that these widgets are part of
the VR environment. They are represented as polygonal
geometry in the shared virtual space. All widgets have a
geometric component, and therefore constitute a portion of
the OpenGL Performer™ (SGI) scene graph which all of
our VR applications utilize. CAVELib™ (VRCO Inc.) [11]
[12] is the library used to drive the VR devices within our
applications. The interface library, vwLib, was therefore
built as a layer on top of both Performer and CAVElib. It is
made up of four generic widget classes; vwButton,
vwSlider, vwFrame and vwIncrementor (see Fig 1 for the
inheritance diagram).

Figure 1: This inheritance diagram illustrates the
class structure of vwLib.

2.2 Tele-Immersive collaboration
CAVERNSoft G2 [13] is a C++ toolkit designed specifically to enable interactive, networked collaborative applications. Rather than incorporating Tele-collaborative
methods directly into the widgets, we have developed a
GUI-state class that utilizes CAVERNSoft G2 to enable

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

our networked interaction. This is a straightforward choice
because the requirement for random and complete GUI
synchronization among remote clients makes encapsulation
of network behavior inside individual widgets problematic.
Thus, multi-user functionality is not directly embedded
into vwLib objects per se, yet the objects behave as networked elements.
We can achieve multi-user behavior due to the clientserver model used. Each connected client maintains a netGUI object of the current state information of that client.
When a local client’s state changes (e.g., a local user
moves a slider), the event cascade results in a modification
of the netGUI’s data structure. When this occurs, the netGUI is transmitted to the server application, which in turn
broadcasts the new state information to all other clients.
Synchronization is therefore constantly preserved. If a new
client connects to the server during an existing collaborative VR session, a copy of the current netGUI database is
retrieved from the server and the new client attains complete synchronization with all the other clients (see Fig 2).

nates. While we do collect head tracker information from
the clients (for future application enhancements), our avatars use only the hand (wand) tracker information.
For vwLib based GUI manipulation, we have found
that a slender cone avatar emanating from the front the
wand works best. Such a slender cone also makes an excellent device by which individual remote clients can point
out items of interest to the other remote collaborators. This
holds true, as well, for passive participants local to the
tracked person who is doing the pointing. Each participating client gets a uniquely colored pointing device which
can be seen by all connected sites. The base of the cone is
at the front of the wand in virtual space and it points along
the vector emanating from the wand (i.e., it points like a
laser pointer).
When possible, a separate CAVERN sound-server
process is also used. A sound-client utility is then run on a
machine patched into the ImmersaDesk audio system. This
CAVERN utility streams audio over a UDP socket or
sockets to the server, which reflects the data to all other
audio clients.

2.3 Interface usability

Figure 2: This flow diagram illustrates the manner
in which one client’s GUI and application data
representation maintains synchronization with
the server. All participating clients maintain this
flow diagram structure with the server.
An initial client-server connection is established using
TCP. This connection information is reflected from the
server to clients that are already actively collaborating.
These clients add the new client to their personal user database and then respond with a TCP message so that the
new client can add them to its user database. The local user
databases are used to create and manage avatars that represent the remote collaborators. In our VROM applications
we have found that enabling navigation of clients’ CAVE
coordinates through the virtual space is a hindrance rather
than an asset. Therefore, as long as all clients are on desk
VR devices, we can safely assume that everyone is facing
in more or less the same direction in virtual world coordi-

Pointing a slender cone at 3D widgets in VR is in some
ways similar to using a laser-pointer to illuminate items on
a projected image in a conference room. There are a number of issues that make this somewhat difficult. Hand-jitter
is an obvious culprit [10]. Most people would agree that
sliding a mouse over a 2D plane and letting it come to rest
over a 2D widget is a much easier method for selecting
widgets. The mouse has 2 DOF and it doesn’t move around
much when the user’s hand is resting on it. Moreover, the
mouse does not move once it is let go.
Wand manipulation in VR applications is much more
difficult than mouse manipulation largely due to the fact
the wand user has 6 DOF to deal with instead of two.
There is no haptic feedback from either a mouse-pad or
wand, and there is nothing analogues to the mousepad/table to steady the user’s hand [9] [8] [7]. One cannot
release the wand as one can release the mouse from one’s
hand. The wand therefore has no straightforward positional
and orientational memory. For this reason, cascading
menus were not even considered as a widget type in
vwLib. Tracker calibration and latency issues are also potential pitfalls. However, these can be overcome to the
point where their significance is minimal [3]. In our experience, clinicians and biomedical oriented research personnel adjust quickly to using the wand in our applications
requiring only minimal practice.
One of the most obvious factors influencing target or
widget selection by means of a pointing device is the
amount of distance in virtual space between the wand and
the widget or GUI. We minimize this distance by situating

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

the widgets on the virtual plane that corresponds to the
physical plane of the projection screen. By design, the ImmersaDesk requires that users be as close to the physical
device as possible. The closer one is to the screen, the
wider the peripheral image footprint becomes. Therefore,
in most instances, it becomes detrimental to attempt to
bring the widgets further out than the screen in virtual
space. This would risk putting the GUI behind the user’s
viewpoint in virtual space, requiring the user to move further away from the screen in order to effectively interact
with the GUI.
During a collaborative session, the application GUI is
generally fixed in virtual space and its virtual location is
identical for all participating clients. When a researcher at
one site ‘has the floor’ and is manipulating the interface,
other collaborators see that remote researcher’s pointer
manipulating the interface/data in a natural way (i.e., they
see that person’s pointer modifying their interface). We
have found that non-active, tracked participants normally
lower their wands unless they are attempting to point out or
ask about something in the medical data. The unpleasant
specter of five remote clients simultaneously waving their
wands around and through the data is needless to say not
an issue. Both applications to be described have been
tested on countless occasions. One example of successful 2
and 3-client sessions was in the spring of 2001 between
Chicago, Chile and a third Chicago site [5].
In many cases the VR widgets making up the collaborative GUI need to be sized larger than one would initially
expect in order for the user to manipulate them effectively.
The reason for this is straightforward. It is a means to
compensate for the hand-jitter that does not occur in
mouse-driven applications. For this and other reasons, such
as visual coherence, we have chosen an application interface model that consists of a main menu, usually composed
of one or more groups of buttons (they behave together as
radio buttons – one can be pressed down at a time). Some
of these buttons, when selected, bring up child interfaces
whose buttons may create second level child interfaces and
so on. Once the interface or interfaces are no longer required, deselection of their parent button destroys them.
Interface cluttering in collaborative applications is ordinarily a concern. The transient child-interface model we have
used keeps such distracting clutter to a minimum.
It is worth noting that hand-jitter and lack of support
can make slider-pips difficult to select. Despite this, we are
able to use sliders in our collaborative VR applications
through the use of several techniques. Once selected, the
slider-pips remain selected as long as the left wand button
remains depressed. Thus, slider movement is handled in
such a way that does not require continual intersection with
the slider-pip. Wand vector intersections with other widgets in the slider’s same local coordinate space (e.g., a
background vwFrame) are also used for slider-pip transla-

tion calculations. Additionally, we’ve found it useful to
experiment with a method of slider selection that does not
require the slider’s over state to be True before the slider
can be selected with a wand button press. In other words,
the left wand button can be held down, and as soon as the
wand vector intersects the slider-pip geometry, the slider
becomes selected and attached to the researcher’s wand. In
cases where high precision is desirable or required, we
often attach an observer slider and an observer incrementor
to one another. When the wand vector is intersecting a
vwIncrementor, a left button click decrements the incrementor’s value by an application-defined amount. A right
wand button click increments its value by the same predetermined amount. In this way, a slider’s value can be
nudged up and down.

Figure 3: This image illustrates the liver with
segments 4A and 4B removed. The hepatic and
portal vasculature can be seen within the opaque
liver geometry. Two user avatars are shown.

3 Medical applications
Two Tele-Collaborative applications will be discussed.
Current testing and research has remained confined to ImmersaDesk-type systems, although in principal these applications should work in CAVE and CAVE-like environments with minimal modification. Also, collaborative mixtures between CAVEs and IDesks, while not yet tested do
not present any prima-facie road-blocks.
As was discussed earlier, human interaction with the
data in VR is achieved through the use of sophisticated VR
GUIs. As new clients attach to the shared VR environments, they inherit the current GUI and data set of the application. If submenus are opened by an existing client,
these submenus become open to all other attached clients
and new clients bring up their application with the GUI in
the same state as that of the other pre-existing clients. If a

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

different remote client takes over GUI/data manipulation,
synchronization continues seamlessly. When one client
drags a slider, for example, that slider is dragged at all
other remote sites. It is a completely shared VR user interface which all can see and all can manipulate in turn.
Again, current avatars consist simply of slender cones
with each client having a distinct color. The cone represents where remote clients are and what they are pointing
at. Since navigation is disabled, each client sees one or
more avatars pointing more or less from the same general
position towards the shared GUI and the medical data object which typically occupies most of the display.
Both these applications share common transformation
modes. In the translation and rotation mode, as the left
wand button is depressed and held the biomedical data
object is attached to the changes in the position and orientation of the wand. The data remains in its final position
and orientation when the wand button is released. In the
scale mode, the data object scales up by an incremental
value as the right wand button is depressed and held. It
scales down by this incremental value as the left button is
held down. A middle button-press in this mode resets the
model to its initialized real-scale value.

Figure 4: This image illustrates the liver application with the Portal Vein drop-down menu. One
user has rendered the right branch completely
transparent via its alpha slider while the other
user points to the area of interest.

3.1 Liver anatomy application
The concept behind this application is to have a surgical
anatomy expert teach students at multiple VR sites. The
students and teacher can communicate verbally with one
another through the audio server that streams audio to and
from the various clients. The surgeon can use his or her
wand pointer avatar not only to select and manipulate wid-

gets thereby adding, subtracting and otherwise manipulating the anatomic objects, but also to point out anatomical
features to the remote collaborators.
The surgeon can independently change transparency
values for each of the eight segments of the liver. This also
holds true for the anatomical branches of the hepatic and
portal vein structures. This gives the surgeon the ability to
flexibly teach local and remote participants the liver segment anatomy and also the relationship between landmarks
in the liver and landmarks in the liver vasculature.
This anatomic model visualization application is in fact
a general-purpose tool. It can easily be customized to accept a variety of specific anatomic models that could be
pre-constructed for the purposes of Tele-collaborative surgical-anatomic education. This application makes use of
drop-down style menus that allow for transparency manipulation of specific elements in the anatomic region presented (see figures 3 and 4). In pilot testing of this application, we demonstrated educational value and knowledge
retention in a small group of senior surgical residents with
the surgical educator at a distributed site (publication pending). This application works adequately on an Octane
(SGI).
The drop-down menus work by selecting a button from
the main menu (e.g., HEPATIC VEIN). This produces a
second level child menu (drop-down menu) consisting of
an overall transparency slider and a set of other buttons
These new buttons can be selected to further refine manipulation of transparency to individual elemental structures (middle branch, left branch, right branch and trunk).
Once again, as these drop-down menus are created by one
client (in most cases by the master surgeon), they are simultaneously created at all other participating client stations. The drop-down child menus are then removed as
their parent widget is either de-selected, or another radio
button is selected in its stead.
All the separate anatomies for which there are models
are represented in the application’s menus. The anatomic
structures under investigation in our current example are
the liver, the portal and hepatic veins (see Figs 3 and 4).

3.2 Radiology application
We have developed a tool for the investigation of volumetric radiological data (e.g., CT, MR) in a Tele-collaborative
VR environment (see figures 5 and 6). This application’s
primary purpose is Tele-Collaborative examination of CT
and MR scans. It shows promise in the areas of medical
education and potentially as a clinical tool to assist in diagnosis and in pre-operative planning.
This application directly imports DICOM3 compatible
data on the fly, typically in under one minute even for large
datasets. Radiological data is transmitted automatically
from the data server to clients as they connect to the Tele-

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

collaborative application [4]. Clients are typically Onyx2
machines (SGI).
The main menu consists of two groups of radio buttons.
One group allows the user to select between one of two
object transformation modes previously discussed. These
transformation modes can be turned off by de-selecting the
currently pressed mode button (i.e., the selected radio button can be pressed again and de-selected leaving no radiobuttons in its group selected). The other main menu radio
button group consists of buttons that create four child interfaces (segmentation, region of interest (ROI), sampling
precision, and an arbitrary clipping plane (see Fig 5)). For
radio-button groups in which having no buttons pressed
results in an undefined state, the vwRadioButton’s constructor is passed an extra argument that makes deselection impossible. The square button at the top adds
gradient shading to the display.

other times. The segmentation tool is used extensively in
order to isolate CT densities, for example, which display
most clearly the characteristics and features that the clinician wishes to illustrate or study. Throughout any examination of radiological data, the orientation and also the size of
the data are constantly being manipulated in order to view
data characteristics from multiple angles.

Figure 6: This image illustrates the segmentation
interface. The wand pointer is intersecting the
incrementor widget controlling the data window
level. Magnification is on.

Figure 5: This image illustrates the arbitrary clipping plane mode while the left button is depressed. When the button is released, the clipping plane remains in the data object’s local coordinate system.
With radiological data, the context and goals of the
remote collaborations are highly variable. Unlike the previously described anatomic educational application, the
clinical uses of the radiological application vary from dataset to dataset, and may even vary when examining the
same data in different clinical contexts (e.g., diagnostic,
communication, pre-treatment planning, post-operative
assessment). Generally, users have a specific region of the
data that they would like to examine or illustrate. Often
ROI and clipping are used not only to increase performance, but also to remove data that is not of interest and may
obstruct an optimal examination. Sampling precision is
often adjusted up and down throughout a data examination
in order to maintain acceptable frame-rates at certain times
and in order to examine very fine details in the data at

The most complex child interface is the segmentation
interface (see Fig 6). It consists of two identical windowing
and leveling tools that, if used together, generate a segmentation curve mapping the higher of each of the two levels
along the dynamic range of the dataset (normally 12 bits).
Each tool has a level slider along with two window width
sliders. There is also a level incrementor and a width incrementor to assist with fine-tuning of segmentation. A
group of radio buttons allows the user to select between
four different segmentation curve functions. Two other
non-radio buttons allow for a maximum intensity projection (MIP) mapping and a function which applies an exponential value to the segmentation curve before it is mapped
to the data. The final button in the segmentation interface
is the magnification button. When magnification is activated, the current window sliders are centered in the interface, if possible, and the window width takes up half the
geometric range of the slider (see Fig 6). This allows for
precise control, no matter how small the data window.
When magnification is on, the level and width incrementors are set for increments of 1 unit.
In order for this tool to work correctly, numerous subject-observer relationships are established between the
various widgets. All of the sliders are specialized subclasses that inherit publicly from vwObserver. The same
holds true for the incrementors in the segmentation tool.

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

4 Conclusions and future work
We have developed the infrastructure for a series of biomedical Tele-immersive applications permitting distributed
collaboration with desirable user interface tools.
The two applications we have described allow the
wand-controlling participants at all connected sites to manipulate any of the widgets. The interfaces are identical
and synchronized at each location, such that a slider manipulation at one site, for example, is reflected at all other
sites in the collaboration. Whenever a child interface is
created or destroyed by one client, it is subsequently created or destroyed by all other clients. Clients can leave the
collaboration by terminating their client application and
rejoin at a later time, assuming that the server application
is still running. Since the networked GUI database is persistent on the server, it is even possible for all clients to
terminate their applications, and at a later time new client
applications can re-connect to the server, with the GUI and
data state being re-generated exactly as it was left when the
final client disconnected from the earlier session.
A logical next step is to begin testing across diverse VR
platforms simultaneously (e.g., CAVE, IDesk, etc.). In the
case of dissimilar devices, we would enable interactive
GUI translation so that the GUI could be re-situated for
optimal usability depending upon which platform was manipulating the medical data.
Recording and playback enhancements are being
planned. These features will allow particularly useful investigations to be reviewed again. This is thought to be
valuable because the highly interactive nature of the interface makes it otherwise difficult to precisely recreate an
interesting visualization. Other features such as the capability of playback of other videos in the virtual environment will also be investigated. This will allow the surgeon
instructor to play video captured during relevant operations
and will allow other related diagnostic images and video to
be shared in the radiology application.
Acknowledgements
This project has been funded in whole or in part with Federal funds from the National Library of Medicine, National
Institutes of Health, under Contract No. N01-LM-9-3543
and under Grant R01-LM-06756-01. We also acknowledge Peter Jurek and Dr. N. Joseph Espat, for development
of the liver model.
References
[1] C. Cruz-Neira, D. Sandin, T. DeFanti, “Surround Screen
Projection-Based Virtual Reality: The Design and Implementation of the CAVE,” Proceedings of SIGGRAPH 93 Computer
Graphics Conference, pp. 135-142.

[2] M. Czernuszenko, D. Pape, D. Sandin, T. DeFanti, G. Dawe,
M. Brown, “The ImmersaDesk and Infinity Wall ProjectionBased Virtual Reality Displays,” Computer Graphics, Vol. 31, No
2, May 1997, pp. 46-49.
[3] M. Czernuszenko, D. Sandin, T. DeFanti, “Line of Sight
Method for Tracker Calibration in Projection-Based VR Systems,” Proceedings of 2nd International Immersive Projection
Technology Workshop, Ames, Iowa, 05/11/98-05/12/98.
[4] F. Dech, Z. Ai, JC. Sliverstein, “Manipulation of Volumetric
Patient Data in a Distributed Virtual Reality Environment,”
Medicine Meets Virtual Reality 2001, Vol. 81, 2001, pp. 119125.
[5] F. Dech, “The Virtual Temporal Bone G2 and, Manipulation
of Volumetric Patient Data in a Distributed Virtual Reality Environment,” Science, Culture and Education over Internet2, April
2001, Valparaiso, Chile, Remote Demonstration
[6] E. Gamma, R. Helm, R. Johnson, J. Glissades, “Design Patterns – Elements of Reusable Object-Oriented Software,” Reading MA. Addison-Wesley, 1995.
[7] R. Lindeman, J. Sibert, J. Hahn, “An Intuitive VR User Interface For Design Review,” Proceedings of the Working Conference on Advanced Visual Interfaces 2000, pp. 98-101.
[8] R. Lindeman, J. Sibert, J. Templeman, “The Effect of 3D
Widget Representation and Simulated Surface Constraints on
Interaction in Virtual Environments,” Proceedings of IEEE Virtual Reality 2001, pp. 141-148.
[9] M. Mine, F. Brooks Jr., C. Sequin, “Moving Objects in
Space: Exploiting Proprioception in Virtual-Environment Interaction,” Proceedings of SIGGRAPH 1997 Computer Graphics Conference, pp. 19-26.
[10] D. Olsen, T. Nielsen, “Laser pointer interaction,” Proceedings of the CHI 2000 conference on Human factors in computing
systems 2000, pp. 17-22.
[11] D. Pape, “A Hardware-Independent Virtual Reality Development System,” IEEE Computer Graphics and Applications,
Vol. 16, No 4, July 1996, pp. 44-47.
[12] D. Pape, D. Sandin, T. DeFanti, “Transparently Supporting a
Wide Range of VR and Stereoscopic Display Devices,” Proceedings of the SPIE, Stereoscopic Displays and Virtual Reality Systems VI, Vol. 3639, Jan 1999, pp. 346-353.
[13] K. Park, Y. Cho, N. Krishnaprasad, C. Scharver, M. Lewis,
J. Leigh, A. Johnson, “CAVERNsoft G2: A Toolkit for High
Performance Tele-Immersive Collaboration,” Proceedings of the
ACM Symposium on Virtual Reality Software and Technology
2000, pp. 8-15.
[14] http://www.evl.uic.edu/fred/vwLib/docs.

Proceedings of the Sixth International Conference on Information Visualisation (IV’02)
1093-9547/02 $17.00 © 2002 IEEE

