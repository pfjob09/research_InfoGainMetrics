Optical Occlusion and Shadows in a ‘See-through’ Augmented Reality
Display
Eric W Tatham
Augmented Reality Research Group
Faculty of Mathematics and Computing
The Open University
Walton Hall
Milton Keynes, MK7 6AA
UK
Tel. +44 (0)1908 655098
Fax. +44 (0)1908 652140
Email e.w.tatham@open.ac.uk

Abstract
As distinct from virtual reality, which seeks to
immerse the user in a fully synthetic world, computeraugmented reality systems supplement sensory input
with computer-generated information. The principle
has, for a number of years, been employed in the headup display systems used by military pilots and usually
comprises an optical display arrangement based on
part-silvered mirrors that reflect computer graphics
into the eye in such a way that they appear
superimposed on the real-world view. Compositing
real and virtual worlds offers many new and exciting
possibilities but also presents some significant
challenges, particularly with respect to applications
for which the real and virtual elements need to be
integrated convincingly. Unfortunately, the inherent
difficulties are compounded further in situations where
a direct, unpixellated view of the real world is desired,
since current optical systems do not allow real-virtual
occlusion, nor a number of other essential visual
interactions. This paper presents a generic model of
augmented reality as a context for discussion, and then
describes a simple but effective technique for providing
a significant degree of control over the visual
compositing of real and virtual worlds.
Superimposing electronic graphics on our view of
the real world is a familiar feature of SLR (Single Lens
Reflex) cameras that typically present exposure and
other
information
superimposed
over
the

photographer’s view through the lens. It is evident that
there is further potential in using such an arrangement
for providing visual information adaptable to given
situations and, indeed, this was exploited by Knowlton
[7] in 1977 when he developed a system that visually
superimposed computer displays onto an input device.
The purpose was to allow users to interact with a real
physical keyboard whilst also providing flexibility of
function by optically superimposing alternative labels
onto the keys. In this way, the same physical keyboard
could be endowed with the appearance of a typewriter,
a calculator or a telephone operator’s console.
The essential generic components of such a
computer-augmented reality system are illustrated in
figure 1. Perceptual stimuli from the real environment
are augmented by computer generated elements to
provide a composite perceptual experience. Depending
on the system’s purpose, it is normally necessary for
the synthetic elements to be harmonised in some way
with the real. Usually, this will require that the
synthesising computer have access to information about
pertinent aspects of the world, such as; world geometry,
user position and orientation, illumination, or physical
object and atmospheric properties. For simplicity the
model shows the augmentation system as external to
the user’s environment. Although this generic model is
intended to be applicable for all kinds of other stimuli,
such as augmentation of a real environment with
computer-generated music, it is primarily real-virtual
visual integration on which many new applications
depend.

Augmentation
system
User’s real
world
environment
User

‘Real’
perceptual
stimuli
Real-virtual
compositing
‘Composite’
perceptual
stimuli

‘Synthetic’
perceptual
stimuli
Computerbased
synthesiser

Harmonising
information

Figure 1

A wide range of possibilities exist for usefully
augmenting reality and includes; assistance for
manufacturing and maintenance [4,5], medical imaging
[3], annotating the real world [11], training [8],
collaborative working [10]. However, a number of
teleoperation of robots [9], design visualization [1] and
technical problems need to be resolved if the promise
of augmented reality is to find fruition. Almost all
potential applications depend on the acquisition and
utilisation of appropriate harmonising information, and
this often presents a significant hurdle. For example,
accurate spatial and temporal registration of computer
graphics onto a real scene remains crucial to the
success of most applications, especially those providing
manufacturing or surgical guidance. Whereas small
tracking inaccuracies may not be noticeable in an
immersive virtual reality system, even very small
angular errors in detecting the orientation of an
augmented reality headset can result in a large
displacement in the registration of the graphics.
Ultimately, more accurate methods of position and
orientation tracking are required, as well as effective
methods of tracking over larger distances. Further
registration problems can occur due to latency between
changes in the real scene and the corresponding
computer graphics update, as the graphics almost
inevitably lag behind. The total delay is caused by the
period it takes for the tracker subsystem to take its
measurements and the time for the corresponding
images to appear on the display devices.
Not
surprisingly, these problems provide the focus for much
of the current research effort [2], but seamless visual
integration of real and virtual worlds also depends on

effective simulation of other factors [6]. It is of
fundamental importance in conveying a convincing
perception of depth that, where appropriate, real
objects appear in front of virtual, occluding parts that
cannot be seen, and that virtual objects be suitably
interposed before real. Achieving such interpositioning
is obviously dependent on the availability of
knowledge concerning the depth of real-world objects.
However, occlusion is just the tip of a much larger
iceberg of interactions that need to be resolved if
convincing integration is to be realised. For example,
virtual objects placed in a real environment should be
expected to appear as if lit by the light sources that
exist in reality. In turn, but less tractable, is the case
where the virtual object is itself a light source that
would be expected to illuminate the real environment.
In practice, almost all objects will reflect some light
that will add to the illumination of nearby objects; thus
some of the colour from a red virtual object would
appear to bleed into a real matt white surface on which
it is placed, and vice-versa. Glossy or mirrored
surfaces in reality should reflect appropriately placed
virtual objects, and the real should be reflected in the
virtual. Real shadows may fall across virtual objects
and virtual shadows across real objects. Reality should
appear refracted through transparent graphics, and
virtual objects refracted by the real. In addition,
atmospheric effects such as fog, smoke or heat haze
should affect the appearance of virtual objects in the
same way as they do real.
There is little doubt that acquisition and utilisation
of appropriate harmonising information present the
most pressing challenges for augmented reality

researchers. However, consideration of the required
visual interaction effects exposes an inherent limitation
associated with current augmented reality display
hardware in its ability to achieve effective real-virtual
compositing. In Knowlton’s system the compositing
was achieved by reflecting the computer graphics in a
semi-transparent mirror that was positioned over the
keyboard at a strategic angle, and it is this same
principle that is employed, although greater freedom of
movement obtained, by current see-through, headmounted displays; see figure 2.
Graphic display

Real scene

Eye
Beam splitter

Figure 2

A similar visual effect, but at the expense of
stereopsis, is obtained by presenting the computer
graphics to just one eye whilst leaving the other eye
free to view the real scene directly so that the task of
superimposition of the two components is left for the
brain to complete. Unfortunately, both arrangements
produce an overlaid image that is transparent and
ghost-like, providing no control over light from the real
world and precluding the possibility of occluding real
objects by the virtual. An alternative compositing
technique that overcomes this particular problem, and
is often used, is to key computer graphic elements into
a video image of the real scene. However, such an
arrangement requires head-mounted cameras, pixellates
the real-world view and, in the event of hardware
failure, obscures the user’s vision.
Graphic display
Active mask
Real scene

Eye
Beam splitter

Figure 3
In order to retain the direct-view advantages of a
see-through augmented reality display while also
providing occlusion and other desired visual interaction
effects, a modified hardware set-up is proposed. The
basic alteration to existing displays is very simple but
effective and is illustrated in figure 3. It is based on the
introduction of an active filter capable of masking
portions of the real scene.
In the illustrated

arrangement, a computer-generated image is viewed by
reflection in a part-silvered mirror as before. However,
the real world is now viewed, not only through the
mirror, but also via a transparent active panel that is
placed along the viewer’s line of sight. The computer
display and transparent panel image are both spatially
registered and temporally synchronised, with the
transparent element acting as an active mask selectively
reducing the intensity of light from the world that
reaches the viewer’s eye. On the other hand, the
reflected image selectively increases the light reaching
the eye. This arrangement allows for significant
flexibility and control as it is now possible to both
reduce and increase the intensity and colour of light
reaching the viewer from selected areas of a scene.
Using the transparent display element to create an
opaque mask and the reflected element to display the
superimposed graphic object allows virtual entities to
visually occlude a real background. The active mask
can also be used to generate areas of neutral density
that reduce light received from selected portions of the
real world enabling the simulation of virtual shadows
within a real scene. The left-hand image in figure 4,
showing a shaded sphere with black background, is
reflected into the eye by the beam splitter (see figure 3)
while the shadow image with transparent background,
shown on the right in the figure, is displayed as the
active mask. These images combine to form one of the
frames for an animated sequence in which the virtual
sphere orbits a real Lego pillar. The sphere occludes
its real background and appears to cast its shadow on
reality as it moves. Figure 5 shows a single frame from
the video sequence. Figure 6 is a photograph taken
directly through the display to show a real finger
inserted through a virtual torus. Besides producing
occlusion and shadow effects, using an active colour
mask colour permits selective filtering of areas of real
world colour. Thus, employing such a mask facilitates
production of any desired colour bleeding or
illumination effects.
Although the illustrations in this article have been
produced, for convenience, using a LCD panel as the
active mask, this is not ideal since such panels
introduce limiting attenuation and distortion.
Fortunately, there are possible alternative methods of
realising the required active masking and these form
the subject of the author’s current research effort.
Promising designs that dispense entirely with the need
for an active transparent panel, as well as obviating the
requirement for a part-silvered mirror, are likely to be
based on the use of spatial light modulator devices.
Whatever the hardware used for implementation, the
principle of incorporating active masking, as outlined
in this paper, overcomes many of the inherent
limitations of current see-through displays. Such a
technique provides a degree of control that could help
enable computer-augmented reality to fulfil its promise
of becoming a highly versatile tool for the future.

Figure 4

Figure 6

Figure 5

Acknowledgements
The ideas described in this paper arose as a result of
work supported by the author’s former employer; Coventry
University, UK. Thanks are also due to R. Kalawsky of
Loughborough University and N.Godwin, K. Monk and
R.Newman all of Coventry University.

References
1.

2.

3.

4.

Ahlers, K., Kramer, A., Breen, D., Chevalier, P.Y.,
Crampton, C., Rose, E., Tuceryan, M. Whitaker, R., and
Greer, D. Distributed Augmented Reality for
Collaborative Design Applications. Proc. Eurographics
'95 Conf., (Maastricht, Netherlands, August 1995), 314.
Azuma, R. Tracking Requirements for Augmented
Reality. Communications of the ACM, Vol. 36 (7),
(July 1993), 50-51.
Bajura, M., Fuchs, H., and Ohbuchi, R. Merging Virtual
Objects with the Real World: Seeing Ultrasound
Imagery within the Patient, Computer Graphics, 26 (2),
(1992), 203-210.
Caudell, T., and Mizell, D. Augmented reality: An
Application of Heads-up Display Technology to Manual
Manufacturing Processes, Proc. Hawaii Int. Conf.
System Sciences, (January 1992), 659-669.

5.

Feiner, S., MacIntyre, B., and Seligmann, D. Annotating
the Real World with Knowledge-Based Graphics on a
See-through Head-mounted Display, Proc. Graphics
Interface 1992, Canadian Info. Proc. Soc, (1992), 78-85.
6. Kalawsky, R.S. and Tatham, E.W. Effects of Spatial
and Temporal Mis-registration in Augmented Virtual
Environments, Proc. of the 16th Annual Conference of
Eurographics, UK, Leeds, (March 1998), 127-134.
7. Knowlton,
K.
Computer
Displays
Optically
Superimposed on Input Devices, The Bell Syst, Tech.
Journal, 56 (3), (March 1977), 367-383.
8. Metzger, P.J. Adding Reality to the Virtual, Proc. IEEE
Virtual Reality Annual Int. Symp., (Seattle, WA,
September 1993), 7-13.
9. Milgram, P., Drascic, D., Grodski, J., Restogi, A., Zhai,
S., and Zhou, C. Merging Real and Virtual Worlds, Proc
of IMAGINA '95, (Monte Carlo, February1995), 218230.
10. Rekimoto, Jun. TransVision: A Hand-held Augmented
Reality System for Collaborative Design. Sony
Computer Science Laboratory Inc., Takanawa Muse
Building, 3-14-13 Higashi-Gotanda, Shinagawa-ku,
Tokyo 141, Japan, (1996).
11. Rose, E., Breen, D., Ahlers, K., Crampton, C.,
Tuceryan, M., Whitaker, R., and Greer, D. Annotating
Real-world Objects using Augmented Reality, Proc. of
Computer Graphics: Developments in Virtual
Environments International 95, (June 1995), 357-370.
__________________________________________________
Footnote: Lego is a trademark of the Lego Group.

