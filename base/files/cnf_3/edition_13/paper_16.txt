3D Ray-Tracing on PC Networks using TCP/IP
T. D. Scott, V. F. Fusco and R. S. Ferguson
High Frequency Electronics Laboratory
School of Electrical and Electronic Engineering
The Queen’s University of Belfast, N. Ireland UK BT7 1NN
Tel: +44 (0) 1232-274087 Fax: +44 (0) 1232-667023
Email: t.scott@ee.qub.ac.uk, v.fusco@ee.qub.ac.uk, r.ferguson@ee.qub.ac.uk

Index terms:
3-D Ray Tracing, Distributed Computing,
TCP/IP.

Abstract
This paper presents a 3D graphics rendering
algorithm for photo realistic ray tracing
operating over a network of personal computers.
The implementation incorporates partitioning
methodologies for both memory and workload to
achieve efficient execution on a distributed
personal computer, PC, network. Experimental
results that compare algorithm efficiency in dual
processor machines and TCP/IP networks with
respect to execution on a single processor are
discussed.

‘rendering the scene’) at video frames rates of
30 time per second while it could take ray
tracing procedures many minutes if not hours to
achieve only a slightly better quality picture.
The independence between pixels comprising
the scene make ray tracing algorithms ideal
candidates for implementation in a multiprocessor or distributed processor computing
environment. In an ideal scenario with infinite
BW communications between processors a
linear improvement in speed of execution might
be expected.

Introduction
In the field of 3D computer graphics the
technique of Ray-Tracing has long been used to
synthesize photo realistic pictures from a
mathematical description of the subjects that are
to appear in the image. The form that this
description takes will depend on the complexity
of the subject material, i.e. the description of the
content of the image. For example a sphere is
defined by its origin and radius, a requirement of
only 4 floating point numbers. On the other hand
the object depicted in Figure 1 is specified by a
planar faceted approximation of over 20,000
triangular shapes, approximately 90,000 floating
point numbers 1.5Mb in additional data storage
is required for image maps and surface textures.
For a highly detailed scene the Z-buffer
algorithm [1] will generate images (termed

Figure 1. Planar faceted scene.
Personal computers with more than two
processors are uncommon, so a cost-effective
solution to realizing a multiprocessor computing
environment is to harness the CPU power of
desktop PCs connected together by a local area
network or wide area network (LAN or WAN).
An added benefit derived from this approach is
the large distributed memory subsequently
available. In such a distributed computing
environment, processors can work concurrently
on different sub-domains (in this paper these are

designated as scanlines) of the overall problem,
communicating with each other only when
necessary. However to achieve maximum
efficiency with this approach redesign of the
ray-tracing algorithm is an essential prerequisite.
In this paper we present an implementation of a
3D Ray tracing algorithm which can be
configured to execute in a number of threads.
The threads of execution can be deployed in
single machine, multi-processor, or, single client
multiple server network environments. The
implementation discussed here incorporates new
partitioning methodologies for both memory and
workload to achieve efficient execution.
Experimental results that compare algorithm
efficiency in a dual processor machine and a
range of network configurations relative to
performance on a single processor using the
TCP/IP protocol discussed. Some comments on
the relative merits of TCP/IP and MPI message
passing protocols are described briefly.

Ray Tracing on Distributed Network

static assignment of large tasks, or, (ii) dynamic
assignment of smaller tasks.
In this work adaptability with respect to the
number of processors is included allowing
flexibility in the dynamic adjustment of
workload. Thus if a fast machine finishes before
all ray tracing is complete the workload is
reassigned to the fastest latent machine on the
network, however, if there are only three
scanlines left to complete the current task, then
the Client will not split the workload, as any
computational advantage would be negligible
due to re-assignment, reallocation, network
overhead penalties incurred.

Network Implementation
TCP/IP Header
Each of the scanlines to be transmitted across the
network is divided into eight segments. The
TCP/IP header must be present during each and
every transmission across the network. The
TCP/IP header is shown in Figure 2 each
element of this header is defined in [5].

In the Client-Server model, communication
generally takes the form of a request message
from the Client who is requesting to the Server
that work is to be done. The Server then does the
work and sends back the result, usually, there are
many Servers using a small number of Clients.
In, the dynamic method used here individual PC
processors are assigned to single scanlines each
scanline represents a 2-D slice through the scene
being traced. This method is similar to Kay and
Greenberg’s [2] concept, except in our case, each
task is a single scan line rather than groups of
scanlines. For each of the scanlines to be
transmitted across the network to the Server,
TCP/IP is used since windows based operating
systems readily support this protocol. The
algorithm is an extension of Hue’s[3] approach
and incorporates, load balancing i.e. each
processor is used as effectively as every other
processor. This means that in the ideal case,
each processor has exactly the same amount of
work to do and will finish its work at the same
time as the others in the network thereby
removing processor latency. In our work this is
achieved by polling client tasks and relocating
slower processors (slowed due to multiple users
bidding for computer resources) onto faster
available machines (i.e. less heavily used
processors). Load balance solutions can be, (I)

Figure 2. TCP/IP Header.
Each scanline is considered to be the smallest
problem allowed. To allow transmission across a
network, the scanlines where divided into 8
segments. The first 7 packets of a scanline are
256 bytes long and the last one 128 bytes long.
The reason for this, is to keep the amount of data
sent over the network as small as possible. At
first glance this would appear arbitrary, however
each scanline is 1920 bytes in size. Dividing this
by 256 bytes, yields the result above.
An algorithm was written to divide up the
workload just before they are dispatched to a
Server for rendering. When received by the
Server(s), the scanlines are assembled before the
rendering process begins. Scanline Segments are
given tags T1 to T8, from which both the Client
and Server know in which order to reassemble
the scanline.

T1

T2

T3

T4

T5

T6

T7

T8

Figure 3. Order of tags in the re-assembly of a
scanline.
The re-assembly of the scanlines follows the
format shown in Figure 3. During network
rendering it was noted that some of the packets
where missing from the final image. Figure 4,
shows this effect in the form of white lines
appearing in the image. To eliminate these from
occurring, retransmission of missed packets must
take place.

Figure 4. Missed Packets from network
Rendered scene.
The reason for the occurrence this problem is
that the UDP (User Datagram Protocol) client
algorithm [5] provides unreliable datagram
delivery. While a simplistic UDP client can work
well on local networks that exhibit low loss, low
delay, and no packet reordering, this algorithm
may not work effectively across a complex LAN
or WAN. To work in LAN or WAN
environment, a client must implement reliability
through timeout and retransmission. It must also
handle the problems of duplicate or out-of-order
packets. Adding reliability can be difficult, and
requires expertise in protocol design.
Due to the unreliability of the UDP transmission,
scanline data retransmission had to be
implemented to ensure that the complete scene is
received by the Server. In our work if the Client
does not receive a packet from a Server, the
Client sends a status command to the Server to
retransmit the missed packet. In general it is only
necessary to execute this error correction process
either when the Client detects the packet is
corrupted, or when it is missing. Retransmission
occurs when the Client sends an error status
message to the appropriate Client, i.e. the Client

first checks if there are any missing packets with
the use of tags, the Client then performs a
numeric check on the scanline segments
received.

Results
With the software philosophy described above it
is possible to predict the relative speedup
obtained by operating on a network as the
number of processors is increased. The total time
for one picture to be traced is the time from
generation of the first ray to the time at which
the last return packet is added into the frame
buffer. For a small number of processors the
computation time will be dominated by the total
amount of processing to be done by the busiest
processor. If however the number of processors
are large (greater than the total number of rays,
for example) then the time will be dominated by
the transit time for the longest ray.
Equation 1 was used to calculate the speedup of
the system using a Dual processor system
relative to a single processor system. Using
different test scenes, the average percentage
speedup performance for the dual multiprocessor
system varied from 70% to 85%.

Dual Speedup =

( Single − Dual )
Single

(1)

Equation 2 shows how speedup is normally
determined. The time used by 1 processor for
program execution is divided by the time used P
processors for execution. The speedup is an
indication of the effective number of processors
utilized [4].

Multiprocessor Speedup =

T1 (1)
TP ( P )

(2)

Since the conditions in any computational
environment are far from ideal, the results will
be below that of ideal due to problems such as:
network contention, memory and processor
latency, scheduling, synchronization and code
modification and overhead due to adaptation will
most certainly have an affect on the performance
of the distributed computation. This observation
leads to the conclusion that simple scenes are not
worth ray tracing across the network. To do so
would involve the network overhead, which

would slow the ray tracing down rather than
speed it up.
In order to test the networked version of the
distributed Ray tracing algorithm, the number of
available processors were increased by one each
time the experiment was run, and the time taken
to ray trace the scene in Figure 4 was recorded.
From Table 1, for the test scene in Figure 4
(640X480 Pixels) the speedup varied linearly
with the available number of processors.
Network contention refers to the slowdown
involved when more than one message attempts
to use the same switch node at the same time.
The amount of time taken to serve a given
request will increase as more messages enter the
network since it may take longer to find a free
network pathway in this situation. Latency
and/or communication costs are factored out of
the times. Equation 3 shows the formula for
switching contention(SC), here superscript i
refers to the particular task i undergoing
computation. T is time, P is the number of
processors [4].

∑[T (P) − T (1) ]

#TASKS

SC =

i =1

i

i

TP ∗ P

(3)

For the example given in Table 1 for 5
processors switch contention accounts for 6.5%
of the decrease in processor run time from the
ideal situation. In addition the cost of
transferring these messages across the network is
the communication overhead. In our work we
use a 100MB Ethernet to interconnect the PCs in
the network.
NUMBER OF
PROCESSORS
1
2
3
4
5

TIME IN
(S)

MEASURED
SPEEDUP

1300
1015
1.4
510
2.6
370
3.5
275
4.7
Table 1. Measured speedup.

This overhead factor is derived by measuring the
total number of bytes transferred in the system.
Using the data for block transfer time for this
number of bytes can be defined [4], in Equation
4. Taking into account that each message is a
maximum of 256 bytes long, the total

communication overhead is derived, Tbt is the
cost per byte transferred, TSETUP is the time setup
cost for the message. When computed for the
example here, communication overhead(CO)
accounts for 17% of decrease in processor run
time from the ideal situation, this when added to
the switch contention overhead accounts for the
speedup differential between ideal linear speedup
and measured results.
 # bytes

∗ TSETUP  + (# bytes∗ Tbt )

256


CO% =
Tp ∗ P

(4)

TCP/IP Versus WinMPICH
WinMPICH the version of MPI for PC’s from
the engineering research center at Mississippi
State University is derived from the Unix version
MPICH for use with Microsoft Windows NT
platforms [6]. WinMPICH allows processes to
communicate with each other through shared
memory or over a network. (WinSock)TCP/IP
[7] and WinMPICH exhibit similar performance
characteristics for message length(Bytes) from 1
to 1k, thereafter, as the message length increases
WinMPICH is superior in terms of transfer speed
. However in our work the data packets are 256
Bytes therefore WinMPICH will not offer a
significant advantage over TCP/IP in this repect.

Conclusions
In this paper, we have described a distributed PC
processor scheme for 3D ray tracing. The
approach presented efficiently exploits all of the
distributed resources available with this type of
arrangement e.g., computation, storage and
communication resources. For a network
rendering scenario, we observed that the more
processors the nearer the theoretical maximum
speedup was achieved bar communication
overheads, latency, etc. For example it was
shown in the dual processor case a speedup of
1.8 is usually achieved and for a five networked
processor system 4.7 was achieved using the
scanline methodology as adopted for algorithm
design in this work.
The technique given here is portable since it
relies on TCP/IP message passing and can be
used on any network of PC’s supporting this
protocol.

Acknowledgements
This work was performed under EPSRC contract
GR/L23215.

References
[1] Alan Watt: “Three-Dimensional Computer
Graphics,”
Addison-Wesley
Publishing
Company, 1989, ISBN 0-201-154420.
[2] D.S.
Kay,
and
D.
Greenberg,
“Transparency for Computer Synthesized
Images,” SIGGRAPH 1979, p158-164.
[3] Hu, M.-C. and Foley, J.D. “Parallel
Processing Approaches to Hidden-Surface
Removal in Image Space.” Computers &
Graphics 9, 3 (1985) pp.303-317.
[4]
S. Witman 1992, “Multiprocessor
Methods for Computer Graphics Rendering”,
(pub: Jones and Bartlett Publishers, London),
ISBN 0-86720-229-7.
[5] TCP/IP client – server programing and
applications, COMER STEVENS, PHIPE, 1989,
ISBN 0-13-261348-4.
[6] Marker Baker, “MPI on NT: The Current
Status and Performance of the Available
Environments”, P64, Recent Advances in
Parallel Virtual Machine and Message Passing
Interface, 5th European PVM/MPI Users’ Group
Meeting Liverpool, UK, September 1998
Proceedings, Springer, 3-540-65041-5
[7] Martain Hall, Mark Towfiq, Geoff Arnold,
David Treadwell, Henry Sanders, “Windows
Sockets, An Open Interface for Network
Programming under Microsoft Windows”,
Version 1.1, 20th January 1993.

