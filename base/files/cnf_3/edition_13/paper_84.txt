Teleoperation and Java3D Visualization of a Robot Manipulator
over the World Wide Web
Igor R. Belousov 1, JiaCheng Tan2, Gordon J. Clapworthy 2
1
2

Keldysh Institute of Applied Mathematics, Russian Academy of Sciences, Moscow, Russia
Depart. of Computer & Information Sciences, De Montfort University, Milton Keynes, U.K.
ibelousov@dmu.ac.uk, jtan@dmu.ac.uk, gc@dmu.ac.uk
Abstract

A system for WWW-based remote robot tele-operation is
presented in this paper. It addresses important problems
of Internet telerobotics such as “how do we make the
system reliable?”, “how do we make the system fast?”,
and, finally, “how do we make the system open and
portable?”. The recent open technologies Java and
Java3D are used for developing the system. Key features
of the system are (1) the Java3D-based “live” virtual
representation of the real robot and its environment, and
(2) the use of a powerful tool for the remote programming
of the robot. Possible applications of the teleoperation
tools developed and methods within VR are also briefly
discussed.

1. Introduction
WWW-based remote teleoperation is an interesting
and promising field of investigation in robotics, VR and
visualization. Applications of these investigations in areas
such as space and underwater robotics, remote
manufacturing, operator training, remote education and
entertainment are of great importance. A detailed
overview of the methods applied in this area is presented
by Brady & Tarn [1].
In the last few years, many systems for WWW-based
robot control have been developed. The list of active
systems providing free access through Web browsers is
presented on the NASA Telerobotics Web-page [2]. Some
disadvantages of these systems are: slow reaction of the
system to the operator’s input, the fact that they contain
poor visualization tools, and the lack of capability of
programming the robot remotely.
The goals that we pursued while developing our
system were inspired by the desire to create an efficient
teleoperation system, that would be fast, easy to control,

and open, i.e. capable of running on a variety of computer
platforms.
One of the main problems to be overcome while
developing such a system is how to achieve fast, near
real-time, response to the operator’s actions when faced
with a relatively slow rate of WWW connection (usually
within 0.1-3.0 KB/sec). Transmission of video data with
TV images of the robot and its environment is important,
but it is usually subject to delays, which greatly
complicates the teleoperation task.
To overcome this problem, 3D visualization of the
robot and its working environment has to be implemented
to provide the operator with a “live” virtual representation
of the scene instead of the delayed TV images.
One such approach is the immersion of the graphic
robot model into TV images transmitted from the remote
work site. Calibrated Synthetic Views were used at JPL to
provide the operator with a 3D model of the robot over
the TV image, Bejczy [3]. The same technique was used
for augmented telerobotic control in the University of
Toronto, Rastogi et al [4]. The main advantages of the
method are that minimum knowledge of the remote site is
required, and the TV images are not updated except for
some changes of the camera view.
But in such an interface, the graphic robot model may
be available only as a tool for simulation of the remote
robot and the tasks, preliminary planning of the
operations, their graphical previewing and further
autonomous repetition by the robot. The manipulation
process with the real robot is not visually available to the
operator who sees only the final status of the robot and
the work site between control sessions.
We propose here another approach, which allows the
operator to see a “live” virtual representation of the
current state of real robot and the environment. A fast
Java3D model of the robot arm and its working
environment was developed to provide a rapid reaction of
the system to the operator’s actions. Nevertheless, two TV

images of the robot work space are also transmitted to the
operator’s control environment to allow him to verify that
the robot is operating as required.
The operator’s control environment also contains a
sophisticated graphic control panel and, of great
importance, a tool for remote robot programming. The
latter gives the operator the useful possibility of
programming such complicated robot actions as pick-andplace operations and assembly, within the control
environment, during the current control session. This
significantly simplifies the problem of remote robot
control and could also be useful for controlling objects in
a general VR environment.
We applied these tools and methods while developing
the system for remote control of a PUMA 560 industrial
robot over the World Wide Web. All components are
realised with open technologies, Java and Java3D, to
allow the system to be accessible through any standard
Web browser with Java support, such as Netscape
Communicator or MS Internet Explorer.
The following section of the paper describes the
system architecture and the control processes during the
operation of the system. Section 3 presents methods of 3D
visualization of the robot and its working environment.
Other parts of the robot control environment – the graphic
control panel and the module for remote robot
programming – are described in Sections 4 and 5. Section
6 contains a description of the experimental setup, current
experiments and directions of further experiments and
investigations. Section 7 concludes the paper, reflecting
on the key features and novel aspects of the system
developed.

It is important that the applets are independent as this
significantly simplifies the development of a complicated
control environment such as RCE.
The robot visualization module provides the operator
with the current coordinates of the real (on-line control)
or virtual (off-line control) robot, and a 3D picture of the
robot and its working environment. It is implemented
with a Java3D API (see Section 3 for further details).
To be more confident while working in the on-line
regime, the operator should have the possibility of seeing
“live” video images of the real robot. The RCE contains
two such images, captured by TV cameras, one of which
is located above the robot working area, and the other in
front of the robot.

2. System architecture & control processes
There are two main parts in our system – the client
and the server. The client runs at the remote, or user, site,
while the server runs at the robot location (Fig. 1).
The system is capable of operating in two control
regimes – on-line and off-line. In the on-line regime, the
operator remotely controls the real robot, while in the offline mode, he controls the virtual robot, i.e. a 3D
graphical model of the robot. In the latter mode, work is
performed fully on the client part of the system, which is
useful for preliminary testing of the operation prior to
operating with the real equipment.
The client part of the system – the robot control
environment (RCE) – is a set of interface tools located in
the robot control web page. It consists of the following
parts, running as independent applets:
• the robot visualization module
• “live” video images of the robot
• the robot control panel
• the module for remote robot programming

Figure 1. System architecture
The robot control panel (RCP) allows the user to
define the desired motion of the remote robot, to set the
control regime (on-line or off-line) and to see the current
robot state (see Section 4).
The robot remote programming module is organised
as an interpreter of the commands of the Robot control
language (Rcl). The operator can perform both individual
commands and arbitrary sets of them (i.e. programs). This
provides a very powerful tool for developing scenarios of
robot activity (such as assembly, object capturing, etc.),
for testing with the 3D robot model in the off-line regime,
and for performing tasks on-line with the real robot.
Section 5 describes the Rcl commands and their use for
remote robot control and operations with objects in VR.

The server part of the system contains the following
modules:
• a module for capturing TV images
• the robot control unit
• the robot control and client/server interconnection
module
For capturing TV images, a special application was
developed. It works as an independent process in the
server computer at the same time as the main server
application. The robot control unit is a standard hardware
module, supplied with the PUMA 560. To control the
robot from PC, this control unit was connected to the
computer via the RS-232 interface. Communication with
the client part of the system and robot control is
implemented by the Java-based server application. The
“sockets” mechanism was used for client/server data
exchange, and the Java Communication API was used for
serial port programming.
The main processes of the active system in the on-line
regime are as follows.
The first is the independent process of TV-image
transfer. The server application saves images on hard
disk, and the applet at the remote site visualizes them
several times per second (the rate is dependent upon the
communication rate between the client and server parts of
the system).
The second process, the transmission of the robot
current state and coordinates (the angles of the joints, and
the position and orientation of the robot grip) is activated
by the RCP applet several times per second. After
receiving the coordinates, the applet activates a
Visualization applet, which displays a 3D model of the
current state of the robot and its environment.
The third type of process is activated when the user
pushes a button to move the robot or to change its speed.
The corresponding input parameters are transmitted to the
server application to control the robot.

sophisticated VRML models of several robots with
powerful user interfaces for testing teleoperation tasks
were developed in the Institute for Robotics and System
Dynamics of the German Aerospace Center, [6]. In the
behaviour-control domain, the behaviour of an object in a
VRML world can be realized, in principle, through script
nodes and event routing, Brutzmann [7]. However, in
implementation, Web browsers and plug-ins often impose
restrictions on script languages. Moreover, behind the 3D
visual display of the control interface, there is intensive
real-time computation of robot inverse kinematics and
dynamics, so it is not efficient to implement these in
script languages.
By comparison, Java3D leaves the user more freedom.
In constructing and animating the 3D world, we can retain
its scene graph structure to make the program run in
retained mode or, alternately, choose to use it just as an
extension to AWT. We can run the program as an applet
within a Web browser for easy access by an Internet user
and we also can run it as a stand-alone application.
Another important factor for choosing Java3D is that,
besides 3D graphics, the other components of the control
interface can be implemented using the standard Java
language.

3.2. Model of the virtual robot
The geometrical structure of the PUMA robot is
relatively simple – it has an open-loop structure and can
easily be defined by a Java3D scene graph. Actually, the
whole graphic structure consists of seven cylinders, two
prism links and a floor object, as shown in Fig. 2. To use
as little data as possible, and to speed up the rendering

3. Java3D robot visualization
3.1. Choosing the implementation technique
PUMA robots are well-defined 6-DOF robots and are
in common use in many application areas. The graphical
counterpart has been successfully modelled in both Open
GL and C++, Dafe [5]. In our application, the Java3D API
was chosen to implement the visual interface.
At the outset, Java3D and VRML were both candidate
tools. Trial programs were written in both languages to
test some functions that were to be implemented in the
user interface.
In the implementation of the 3D world, there was little
to choose between them. If anything, the use of VRML
appeared to be more straightforward. For example,

Figure 2. Java3D model of the robot.

rate, the 3D geometry of the prism links of the PUMA is
created by defining a QuadArray object within a Shape3D
node. The floor is also a QuadArray object, on which
texture can be mapped, if desired.
The model was tested on a PC (Pentium 200, 32MB
RAM, 16Bits graphics card). The image sizes were
350×350 pixels. If we chose ViewPlatform (position and
orientation of the virtual camera) as above, the rendering
rate was approximately 11 frames per second.
The rendering rate depends upon both the image size
and the chosen ViewPlatform. The nearer the location of
the virtual camera is to the robot, the more time is needed
to produce the image. The rendering time also depends
upon the angle between the direction of the camera view
and the floor. The larger the angle, the more time is
needed for rendering.
Manipulation actions have no significant influence on
the rendering rate, which implies that the kinematic
calculations form a relatively minor part of the whole
computation.

4. Robot control panel
The robot control panel allows the user to define the
desired motion of the remote robot, to select the control
regime, and to see current state of the robot (Fig. 3).
Interface elements on the left side of the RCP allow
the user to choose the speed of the robot, the robot grip
state (open/closed) and the motion space (“World” – the
base reference frame (RF), “Tool” – the robot grip RF,
“Joint” – joint space). The push buttons on the right side
provide the possibility of controlling the robot in either
Cartesian or joint space. In Cartesian space, it is possible
to move the robot grip along the X, Y, Z axes (in both
directions) and to rotate the grip around these axes. In
joint space, the user can perform direct control of each
joint.

3.3. Robot visualization
To provide a rapid system reaction to the operator’s
actions in on-line control mode, we use a Java3D model
of the robot and its working environment to represent the
current state of real ones. Small data parcels with the
current robot coordinates (6 joint and 6 Cartesian
coordinates) are transmitted to the visualization module
several times per second and the drawing program is
invoked to visualize the robot and objects of the working
environment in the current location.
This technique accelerates the system response and
provides the operator with a synthesised view of the real
robot and the working environment. To smooth the
motion of the 3D model between the positions received,
we use the path-planner module at the local (operator’s)
site. This module calculates interim points in accordance
with the current motion regime – along a straight line or
along an interpolated trajectory in joint space.
To extend the control possibilities in the off-line
regime, a Recorder Applet was created to function as a
robot motion player. It records as many key points along
the trajectory of the robot grip as necessary for the
operator. It regards consecutive points with the same
coordinates as one effective point on the trajectory. At
any stage of the planning process, the operator can assess
the planned path by using the play function of the
recorder and viewing the motion of the virtual robot. This
facility is extremely useful for the operator to plan, view,
and edit the desired trajectory before submitting the
control commands to the real robot.

Figure 3. Robot control panel
Interface elements at the bottom of the RCP allow the
robot grip to be aligned along the axes of the base RF, and
the desired location of the robot grip to be directly defined
(absolute, in the base RF, or relative) and for it to be
moved to that point.

5. Robot remote programming
When we consider the problem of robot control
(remote or direct), it is not usually sufficient to use only
graphic control interfaces (like RCP) or input devices

(joysticks, haptic devices, data gloves, etc.) to solve all
the control problems. For example, if we consider robot
assembly, welding or another complicated tasks, it is
useful to have the possibility to program robot actions
instead of obliging the operator to repeat manually
tedious actions over a long period. Hence, we need to use
a language which provides this possibility, i.e. a robot
control language. The same consideration applies also to
controlling objects in VR, since the developer of a VR
environment may have to provide complicated behaviours
for both passive objects and actively-controlled objects in
the VR “world”.
Further, if we are controlling the robot remotely (or
controlling the object in VR), we need a tool that allows
the commands of such a control language to be performed
during the session, in real time. This requires a high level
language, in which commands are interpreted rather than
compiled. For this purpose Rcl (Robot control language)
was developed; a detailed description of Rcl commands
and variables is presented by Belousov [8]. Here we
describe only the main possibilities of the language.
There are three types of Rcl commands:
• motion commands
• commands for setting robot location
• servicing commands
Motion commands allow the user to move the robot to
predefined points, to translate the grip, or rotate it about
any axis, in base or grip reference frames. For example,
• “go A” moves the robot to the predefined point A
• “move 10 20 0” translates the robot grip along the X
and Y axes by 10 mm and 20 mm respectively
• “trot z 30” rotates the robot grip by 30 degrees
around the Z axis of the grip RF.
Robot location commands allow points in the robot
working space to be defined and operated on. For
example,
• “setp B 100 300 200 10 20 30” defines a new point B
with Cartesian coordinates (100, 300, 200) and Euler
angles, defining the grip orientation, of (10, 20, 30)
degrees, respectively;
• “shift B 0 0 100” increases the Z coordinate of the
point B by 100 mm.
Servicing commands allow the user to open and close
the grip, to calibrate the robot, to draw the current robot
joints and Cartesian coordinates, etc.
Rcl was realised using Tcl/Tk (Tool command
language/Toolkit), an interpreted scripting language
developed at the University of California at Berkeley,
Ousterhout [9].
All Tcl standard commands are
interpreted inside the Tcl-shell; that is very important – it
is an embeddable language. New commands of Rcl were
developed that could be evaluated (interpreted) within the
Tcl-shell together with Tcl standard commands. So we

have the possibility to develop and to perform
complicated programs for robot control, using both Rcl
commands, and all the capabilities of the Tcl/Tk scripts
(including control flows, procedures, access to the
Input/Output system, etc.). The Tcl/Java version of Rcl
has now been realized using the Jacl package. The Tclshell is embedded in an applet, and all Rcl commands and
their combinations can be performed within any Web
browser which has Java support.

6. Current experiments and future work
Several experiments on remote robot programming
over the WWW have been conducted. During the
experiments, the robot was located at the Keldysh
Institute of Applied Mathematics (Moscow, Russia),
while the operator controlled the robot from De Montfort
University (Milton Keynes, U.K.).
We used the following hardware on the robot (server)
site: industrial robot manipulator PUMA 560 with 6
revolving joints, robot control unit Sphere-36, two TV
cameras with frame-grabbers and a PC Pentium-166 with
the Windows 95 operating system (Fig. 4). Web server
Web Site 1.1 was running on the server site. The operator
software was running on a PC Pentium-150 with
Windows NT 4.0, with browser Netscape Communicator.

Figure 4. Robot PUMA & working environment
An initial series of experiments was conducted to
verify the basic possibilities of the system, its reliability
and its time characteristics.
It was revealed that successful robot control based on
TV-image information is impossible under the existing
communication rate between the server and client sites
(0.1 KB/sec). Despite the fact that the TV images were
quite small (resolution 192×144 pixels) and were
compressed to JPEG format with ultimate file size about
1 KB, nearly 20 seconds were needed to receive every
portion of the TV data. Thus, the only way to provide
suitable control conditions for the operator was by using
the 3D model of the robot and the environment. It allowed

the suppression of time delays and provided quite fast
response of the system to the operator’s actions.
The next experiment was connected with solving a
concrete control task. We suspended two tennis balls on
threads in the robot work area. The goal of the experiment
was to use the robot grip to strike one ball so that it would
collide with the other. The experiment was performed
successfully, verifying the correctness of the chosen
approaches for system development. A more complicated
experiment – a pick-and-place operation with a tubular
element – is currently under preparation.
Future investigations and development of the systems
will address three main problems.
The first is connected with improvements to the 3D
visualization module. These may include exhibiting the
interactive behaviour between the virtual robot and virtual
objects and the use of a stereoscopic display to obtain
depth information.
The second is preparation of experiments on the
capture of, and interaction with, moving objects. These
are very challenging tasks, which will demand further
improvements of the system reactivity, based on the use
of prediction methods to suppress time delays [10, 11].
The third goal is the further development of the
module for remote investigation of robot-manipulator
kinematics and dynamics. The most attractive part of this
module should be the possibility of investigating the
motion and control of this synthesised object, not only in
the 3D graphic environment, but also by observing its real
motion on TV images. This will be represented by the
real robot, located at the remote site (see Belousov et al
[12] for a detailed description of this approach).

7. Conclusion
An efficient system for remote robot control over the
World Wide Web has been developed. Firstly, and most
importantly, the system provides rapid response to the
operator’s actions because of both the minimisation of
data exchange and the use of the Java3D virtual robot
representation with a local path planner, as opposed to
commonly-used TV images. Secondly, the system
provides the operator with a powerful and comfortable
control environment, comprising a graphic interface and a
language for remote robot programming. Thirdly, the
system has been realised with the use of open
technologies, Java and Java3D, and is thus capable of
working on a variety of computer platforms.
Experiments on WWW-based remote control of a
PUMA manipulator have been conducted. The robot was
located at the Keldysh Institute of Applied Mathematics,
and was controlled by the operator situated at De
Montfort University. Experiments confirmed the
reliability and efficiency of the system.

Acknowledgements
This work was performed at the Keldysh Institute of
Applied Mathematics and De Montfort University. The
authors would like to thank colleagues from the Keldysh
Institute of Applied Mathematics: Prof. Victor Sazonov
for his kind assistance in preparing the robot control
experiments, and Dr. Andrey Boguslavskiy and Dr.
Sergey Sokolov for developing the software for capturing
the TV images.

References
[1]

K. Brady and T.-J.Tarn, “Internet-based Remote
Teleoperation”, IEEE International Conference on
Robotics & Automation ICRA’98, Leuven (Belgium), May
1998, pp.65-70.
[2] NASA Space Telerobotics Program, http://rainer.oact.hq.
nasa.gov/telerobotics_page/telerobotics.shtm.
[3] A. K. Bejczy, “Virtual Reality in Telerobotics”, 7th
International Conference on Advanced Robotics ICAR’95,
Saint Feliu de Guixols (Spain), Sept 1995.
[4] A.Rastogi, P.Milgram and J.Grodski, “Augmented
Telerobotic Control: a Visual Interface for Unstructured
Environments”, KBS/Robotics Conference, October 1995.
[5] O. Dafe, “Off-line Robot Programming & Simulation”,
http://www.cs.uiowa.edu/reu/summer95/OvoDafe/ovo/ovo.html
[6] The Telerobotics Activities in the Institute of Robotics and
System Dynamics (German Aerospace Center),
http://www.robotic.dlr.de/TELEROBOTICS.
[7] D.Brutzman, “The Virtual Reality Modelling Language
and Java”,
http://www.stl.nps.navy.mil/~brutzman/vrml/vrmljava.ps.
[8] Belousov I., “Rcl/Rci: Multiplatform Tcl/Tk-based Robot
Control Language and Robot Control Interface”,
International Conference on Adaptive Robots and General
System Logical Theory, St.-Petersburg (Russia), July 1998.
[9] Ousterhout J., Tcl and the Tk Toolkit, Addison Wesley,
1994.
[10] Okhotsimsky D., Platonov A., Belousov I. et al., “Vision
System for Automatic Capturing a Moving Object by the
Robot Manipulator”, International Conference on
Intelligent Robotics IROS'97, Grenoble (France), Sept
1997, pp. 1073-1079.
[11] Okhotsimsky D., Platonov A., Belousov I. et al., “RealTime Hand-Eye System: Interaction with Moving Objects”,
IEEE International Conference on Robotics & Automation
ICRA’98, Leuven (Belgium), May 1998, pp.1683-1688.
[12] Belousov I., Kartashev V. and Okhotsimsky D., “RealTime Simulation of Space Robots on the Virtual Robotic
Testbed”, 7th International Conference on Advanced
Robotics ICAR'95, Sant Feliu de Guixols (Spain), Sept
1995, pp. 195-200.

