MotionLab Sonify: A Framework for the Soniﬁcation of Human Motion Data
Alfred Effenberg
Deutsche Sporthochschule
K¨oln, Germany
effenberg@uni-bonn.de

Joachim Melzer
Institut f¨ur Informatik II
Universit¨at Bonn, Germany
melzer@cs.uni-bonn.de

Andreas Weber
Institut f¨ur Informatik II
Universit¨at Bonn, Germany
weber@cs.uni-bonn.de

Arno Zinke
Institut f¨ur Informatik II
Universit¨at Bonn, Germany
zinke@cs.uni-bonn.de

Abstract
Soniﬁcation of human movement offers a wide range of
new kinds of information for supporting motor learning in
sports and rehabilitation. Even though motor learning is
dominated visually, auditory perception offers unique subtle
temporal resolution as well as enormous integrative capacity – both are important features on perception of human
movement patterns. But how to address the auditory system adequately? A soniﬁcation based on kinematic movement data can mediate structural features of movement via
the auditory system, like polyrhythms of movement etc. And
soniﬁcation of dynamic movement data makes muscle forces
audible approximately. Here a ﬂexible framework for the
soniﬁcation of human movement data is presented, capable of processing standard kinematic motion capture data
as well as derived quantities such as force data. Force data
are computed by inverse dynamics algorithms and can be
used as input parameters for real time soniﬁcation. Simultaneous visualization is provided using OpenGL.
Keywords: Soniﬁcation, MIDI, Motion Capture, Inverse
Dynamics

1 Introduction
In the ﬁeld of motor control and motor learning watching the movement of trainers or therapists is a key function
as becoming evident with the learning theory of ‘observational motor learning’. The more precise and concise the
perceptual process can be designed, the more efﬁciently the
learning process can be arranged. While visual informationprocessing has been optimized in different ways to support
motor learning as already has been realized with videoor visualization-techniques, equivalent techniques regard-

ing the auditory perception have not been developed up to
now in a comparable way. The ear is indeed an additional
powerful channel for mediating movement related information, as indicated by the efﬁciency of auditory models for
motor learning e.g. [11]. Beyond that auditory perception
is well suitable for perceiving time-critical structures and
for integrating multiple rows of sound as becoming evident
when hearing polyphonic music. Even motor behavior can
be modiﬁed auditorily more subtly than consciously intendable at all [12].
The soniﬁcation of human movement is a new approach
to create continued kinetic-acoustic information to involve
auditory perception more comprehensively into the process
of motor learning [4]. Movement soniﬁcation can be based
on kinematic as well as dynamic movement data. But while
kinematic movement features like amplitudes of motion are
detectable easily by visual perception as well as by different sensory systems technically this applies not equally to
internal dynamic features like muscle forces. Movement
patterns are based on the coordinated excitation of various
muscles, but force perception is an internal phenomenon
and internal dynamic features are technically hardly detectable at present. With the MotionLab Sonify System
we present a motion capture-soniﬁcation-system capable
of real time soniﬁcation and computing force data by inverse dynamics algorithms. Complementary empirical data
are summarized, indicating efﬁciency of continued kineticacoustic information on perceiving, judging and reproducing sports movements in a real-world-like condition.

1.1 Previous empirical results
Empirical investigations had been conducted in two distinct areas: (1) regarding the precision of the perception
and the judgment of sports movements [5] and (2) con-

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

cerning the accuracy of the perception and reproduction of
sports movements [3]. Subjects were treated with visual,
auditory and audiovisual stimuli comparatively, a video-,
audio-, or audio/video-projection of counter-movementjumps. Judgment of jump-heights were signiﬁcantly more
precise audio-visually compared to both unimodal conditions. And also precision of movement reproduction was
signiﬁcantly highest under audiovisual treatment. These
were the ﬁrst investigations showing that a non-cyclic, nonrhythmic movement pattern can be perceived and reproduced more precise when additional acoustic information
is available. Results and further discussion can be found in
detail in [6, 7].
The investigations have been conducted for particular
movements with a very high and special systems effort that
was not directly transferable and adaptable to other settings.
With these ﬁrst experiments the lack of a ﬂexible framework
for the soniﬁcation of human movement data became obvious.

2 The MotionLab Sonify System
MotionLab Sonify is realized as a plug-in for the MotionLab [1] framework. The internal representation uses
streams of motion data for segments, so in particular it can
be used to work with motion data in skeletal representation.
Currently we have implemented a parser for the AMC/ASF
ﬁle-format, which is widely used and is supported by the
major motion capture systems, e.g. VICON. The system
can easily be extended to handle motion data in other formats and the architecture of MotionLab is designed to deal
with streams of real time motion capture data.
The skeletal motion capture data can directly be visualized in the form of “moving skeletons”. As these visualizations are standard we will not give any further details here.

2.1 Obtaining Derived Dynamic Data
2.1.1 Computing the center of mass and sums of external forces
Knowing the mass distribution of the body segments the
center of mass can be easily computed from the kinematic
data. In our system we use regression models on anthropometric data [9, 2, 8] to estimate the mass distribution on
the segments from the overall height and mass of a person.
Notice that the movement of the center of mass of the multibody system is given by the sum of the external forces on
the system. So from the trajectory of the center of mass we
can easily compute the sum of external forces!

2.2 Inverse dynamics computations
Beside kinematic also dynamic quantities were soniﬁed.
A coordinated excitation of various muscles is the basis of
distinct movement skills, but direct measurement of internal muscle forces is usually impracticable. Here we use
an inverse dynamics approach to compute pseudo-muscular
forces from motion capture data.
If the corresponding connectivity graph does not contain closed loops one may choose any body as the root of
a tree. Knowing all external forces and torques (except the
one acting on the root), the total joint torque of all concentric forces (not passing through a joint center) can be
computed. For real world applications obviously not only
muscles contribute concentric forces, but also for example
tendons and the constraints of a joint itself.
In order to ﬁlter only those fraction of the forces deﬁnitely belonging to forces produced by muscles, the projection of the joint torque onto the “subspace formed by these
constrains” can be subtracted from the total joint torque.
The calculation of these torques is done recursively, by
starting with the known external forces and torques at the
end effectors, which are the leafs of the tree. Using Newton’s laws of motion the forces are then gradually calculated
bottom up. Furthermore the external force at the root can be
reconstructed. Thus knowing the actual external forces and
torques at the root and comparing them to the reconstructed
ones gives a good impression of the quality of the chosen
biomechanical model.
A more detailed description of the basic algorithm and
all underlying principles can be found e.g. in [14] and [10].
Since the complexity of this recursive algorithm is linear
in the number of bodies, it can be easily used for real-time
applications (our human model consisted of 27 segments).
The second derivatives of the motion, required for the
computations of the joint torques, are very sensitive to the
sampling rate. Artifacts (like very high unrealistic peaks)
occur if this rate is to low. These artifacts where reduced by
ﬁrst applying a moving median ﬁlter to the positions before
applying the inverse dynamics.

2.3 Soniﬁcation Back End
Experiments in sport sciences have shown that although
simple artiﬁcial sounds like sine waves look like an apparent
solution to render sound output they should be avoided—
listening to pure sine waves for a longer period of time is
strenuous at best: Instead of supplying the user with additional information, the tone gets so annoying, that hearing
is shut down as good as possible. However, experiments
have shown that the additional information is used in a positive way on a subconscious level. A possible solution for
obtaining adequate sounds is using the MIDI interface in-

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

stead. Although different approaches—like CSound—are
imaginable, we have preferred to use MIDI due to its broad
spectrum of well known sounds that humans can adapt to
with a steep learning curve. So our soniﬁcation back end is
based on MIDI. However, the use of MIDI in the soniﬁcation back end is well encapsulated so that it can be replaced
by another interface without too much effort.
2.3.1 A brief rehash of MIDI
The ﬁrst speciﬁcation of MIDI (Musical Instrument Digital Interface) was passed in 1983. Upcoming demands for
a protocol to allow communication of musical instruments
(such as synthesizers from different companies) made it
necessary to specify a uniﬁed protocol.
MIDI has 16 different channels, each of them polyphonic
and with its own patch and volume setting. Thereby, 16 different instruments can be played at the same time. Notes
are switched on with the NOTE ON command combined
with channel number, followed by two bytes, the ﬁrst containing note number, the second containing the velocity to
be used. The note remains audible until the corresponding NOTE OFF command is sent. Apart from control of
pitch and volume, the MIDI protocol allows to change instrument patches and some special modiﬁcations such as
reverb, echo, vibrato or pitch-bending.
Table 1. Some MIDI messages used

The possibility of different instruments soon required a
standard allocation of instruments to provide consistent settings throughout synthesizers from different manufacturers;
otherwise the patch number for a piano on keyboard X could
be the number for trumpet on keyboard Y. This problem was

solved with realization of the GM standard (General Midi)
in 1991. It prescribes a standard allocation of 128 different
instrument patches, with the order being particularly important.
2.3.2 Pitch in Soniﬁcation and MIDI
Most MIDI parameters take 7-bit values, which, for example, leave us with a ﬁne grade of 128 different volume
settings. But for the most important part in soniﬁcation—
pitch—this can be a problem. Although the 128 different
tones (which contain the 88 semitones of a grand piano)
are suitable for musical needs as they cover the tonal range
of every commonly used instrument, in soniﬁcation there
should be a much ﬁner grade. This becomes even more
problematic given the fact that a tonal range of more than
two or three octaves (24 or 36 semitones) would be too
much, simply because the change in pitch is so dramatic
that it may not anymore be perceived as belonging together.
In any case, the user would be presented a series of staccato
tones, instead of a “ﬂuent” sound that would be more suitable to soniﬁcation needs. (This problem can be compared
to watching a movie with different frame per seconds settings: Although the action can be perceived with 12 fps in a
similar manner as it can be with 25 fps used in cinematography, the lesser framerate results in a much more artiﬁcial
feeling, thus making the technical aspect and its limitations
much more present and thereby distracting.)
To cope with this problem, two additional settings were
implemented. Since a tone in MIDI is switched on and off
separately, it was obvious to use this to provide more ﬂuent
sounds. By choosing a longer delay between NOTE ON
and the corresponding NOTE OFF, the sound can be made
more ﬂuent because different notes do not substitute one another, but by having two or three notes switched on simultaneously, thus being audible at the same time, the difference
is less distinguishable. Furthermore, a note is only played,
when it is different from its predecessor (since ﬂuent parameter changes are mapped to a small interval, many different
values are mapped to the same tone).
Delays were implemented in form of an event queue:
Instead of sending all MIDI commands directly to
the output device, they are stored in the event queue
with an additional timestamp.
NOTE ON is stored
using GLUT ELAPSED TIME and the corresponding
NOTE OFF is stored the same way, but with the chosen
delay added to the current time.
The second action to prevent distracting and abrupt tonal
changes is the usage of a MIDI feature called “pitch bend”.
This effect often used in country music is similar to twitching a rubber band and then changing its length and thereby
changing the pitch. This effect was integrated into the MIDI
standard by adding a section of pitch bend commands. The

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

ﬁrst of them sets a tonal range, with +/- 2 semitones being
the suggested setting. Range setting is accomplished using
the four CONTROL CHANGE messages shown in table 1,
the ﬁrst pair preparing setting change, the second pair deﬁning the new range. Since we are interested in full semitones,
ﬁne tuning in cents is always set to 0 (2nd data byte in 3rd
CONTROL CHANGE row).
The second command is the pitch wheel change, ranging
from -8192 (maximal pitch decrease) to 0 (no change) to
8192 (maximal pitch increase), resulting in a total range of
16384 different steps.
In all instrument patches that do not fade (wind section,
string section, organs), a tone played using one of these
non-fading patches stays at the same volume until the corresponding NOTE OFF is send. The combination of pitch
wheel changes with these patches results in an undistinguishable range of tones and presenting a ﬂuent experience
to the user.
Since the latter method is preferred, pitch bending is
implemented whenever possible and the combination of
NOTE ON/NOTE OFF commands with a longer delay is
used alternatively when pitching is not an option.
In a ﬁrst cycle, the complete motion is analyzed and
maxima and minima are gathered. These serve as reference
values during calculation of the required pitch. With vmin ,
vmax , ∆v = vmax − vmin as our border values, pmin , pmax ,
∆p = pmax − pmin as the user-deﬁned interval and xmin ,
xmax , ∆x = xmax − xmin as the chosen parameter (i.e. velocity, acceleration, force, etc.), the resulting formula for
mapping reads as follows:
pitchn = (

∆p
∗ (xcurrent − xmin )) + pmin
∆x

Since we deal with known interval settings in pitch-bendmode (∆p = 16384), these values can be hard coded, resulting in a second formula:
pitchp = (

16384
∗ (xcurrent − xmin ))
∆x

Additionally the user may specify a volume for the chosen channel to cope with different base volumes of different
instrument groups. If required, volume can be deﬁned as
an interval as well, the resulting formulas are analog to the
pitch formulas.
2.3.3 Scaling of Parameters
The previous formulas are on a linear basis but can be easily
adjusted to different scales e.g. logarithmic ones. As many
psycho-physical laws are related to a logarithmic scale we
provide a selection of different scales for each parameter in
our system.

2.4 Soniﬁcation Front End
Soniﬁcation was realized as a plug-in for “MotionLab”.
After loading a skeleton into MotionLab and visualizing it
using the proper commands, the user is presented the following screen:

Figure 1. MotionLab with loaded data
In the upper part you can see the skeleton, plotted in a
simple manner using spheres for nodes and interconnecting
lines corresponding to the underlying skeleton. The picture
can be turned and zoomed by mouse-movement. Since motion is involved, animation playback can be controlled via
keyboard, including frame-by-frame control, forward- and
backward animation and change of fps setting. As stated
earlier, the lower part is used for console commands.
The soniﬁcation plug-in can be launched separately and
is realized using WINAPI, so the main (graphic) window
and soniﬁcation controls can be used at the same time. Using a tabbed dialog, the user can choose between three different sub-dialogs, cf. Fig. 2.
2.4.1 The wizard
Since various experiments provided useful and less useful
settings, it was possible to code these into a wizard. It includes functions to optimize user settings and to attenuate
problems that may result from user settings. These can be
intervals which are too big, wrong volume settings or too
short or too long delays when not in pitch bend mode. Furthermore, some settings to start with are hard coded into the
wizard in order to provide a point to begin with (for inexperienced users).
To give an example, interval adjustment based on peak
values has been included: maximal values are calculated for
each node individually, so the full range matches the user
chosen interval. If the user wishes to compare the velocity

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

task can be simpliﬁed by using different patches, i.e. instruments.
In order to address the problem of sound being audible although no movement is visible a threshold value can
be deﬁned, so that all velocities, accelerations, forces, and
torques underneath the speciﬁed threshold will not be soniﬁed.
2.4.4 Parameters not corresponding to skeletal segments

Figure 2. Soniﬁcation Plug-in with MIDI Parameter as active tab

of two different nodes, then the same velocities should be
mapped to equal pitches, and the required interval adaptation can be easily accomplished by the wizard.
2.4.2 The soniﬁcation settings
All MIDI settings explained earlier are included in this dialog, meaning that the user can change patch setting, delay, interval size, and volume (interval if desired) on a perchannel-basis. Furthermore, each channel can be muted individually, thus allowing the testing of different settings on
one channel without the need of setting volume on all other
channels to zero.
For ﬁltering of data we have used a global moving median ﬁlter up to now. In our experiments this ﬁlter with a
small window size of 3 smoothed sound output sufﬁciently
in many cases. However, for some examples we had to use
windows of sizes up to 30.
2.4.3 The calculation parameters
While MIDI settings are deﬁned for each channel, calculation settings are deﬁned for each node. On this page the user
ﬁrst selects a node and then assigns the values he wished to
hear. These are velocity, acceleration, torque, and force.
The second parameter is the scale the user wants to apply—
with the linear scale being the default scale. Finally, the
user assigns a channel to the settings. As of now, three slots
are available for each node, meaning that three different parameter sets can be assigned to one node at the same time.
Thereby it is possible to directly compare two (or three) different values, for example velocity and acceleration, of the
same node and since the channel is chosen individually this

The forces and torques computed by the inverse dynamic
algorithm are attached to the skeletal segments. So they can
be soniﬁed similarly to the kinematic information attached
to the segments.
Since the center of mass is interesting to analyze, it is
added as an additional “pseudo-node”, so that it can be soniﬁed in a similar manner.
In principle other linear or even non-linear combinations
of quantities can be added as “pseudo nodes” similar to the
center of mass. This might be interesting e.g. for the ﬁrst
principal components of the kinematic or dynamic motion
data, as it is well known that many motions can be well represented by about the ﬁrst 5–8 principal components [10].
As is done with the MIDI settings, the wizard may be
used to suggest some “easy settings” that provide quick results. As an example, it is much easier to keep track of foot
or hand movement and connect vision with the sound produced by the soniﬁcation module.

3 Examples
To cope with soniﬁcation, the underlying psychoacoustic
phenomenons must be considered. As with vision, sound
can be perceived in different ways and much information
extraction and -handling is performed automatically.
An example can be classical music. You can simply listen to the music and thereby perceive it as a whole. But
by concentrating on instrument groups, timbres or volumes,
sound can be grouped in many different ways and still be
perceived as a whole. This is in strong contrast to the abilities of the visual system: humans looking at a painting can
either perceive it as a whole or pick out interesting details
and largely loosing the contexts. However, humans can
focus on musical details and nevertheless are able to hear
them in their contexts. Although the eye and the ear both
are able to focus, the eye looses the context whereas the
ear does not. Although at ﬁrst it seems more sophisticated
to sonify complex motions such as karate movements, the
soniﬁcation of such complex motions raises the following
problem: If we sonify two or more (skeleton-) nodes at the
same time, the resulting sound is perceived “as a whole”,
but connecting sounds to soniﬁed nodes is a difﬁcult task

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

for humans, mainly because there is too much movement
to be seen and selected. And auditory perception of multiple rows of sounds is susceptible to ‘auditory masking’, the
covering of softener sounds by stronger sounds. Therefore
the following example was chosen for its simplicity in order to demonstrate the basic properties of different settings
in soniﬁcation.

3.1 Examples with two segments
In order to illustrate basic soniﬁcation possibilities, two
circular movements have been combined. The inner node is
static, the middle node moves on a circular path 180 degrees
around the center, the outer node describes another 180 degrees circular path, but moves up and down. To accomplish
these movements, a sine wave has been used.
The ﬁrst setting is the soniﬁcation of the middle node’s
velocity, which was mapped to the interval C4–C5 (Fig. 2).
Changes in interval size (i.e. increasing it to two or more octaves) gives nearly the same impression, whereas raising the
interval (i.e. to C5–C6) suggests an increased velocity, and
therefore—after initial settings have been chosen—should
be avoided.
Next comes the middle node’s acceleration (Fig. 3, light
gray), with the same interval that has been used for velocity soniﬁcation. Again, the connection between hearing and
vision is not difﬁcult, but can be improved by deﬁning a
threshold on the “Calculation Parameters” tab, under which
the sound should be switched off. Thus no or very low accelerations result in no sound which is more appropriate.
The next improvement is the inclusion of a median ﬁlter.
The ﬁltering of the data is not of central importance for this
simple synthetic example but is important for soniﬁcations
of motion capture data.

wave, mapping to the same interval has no effect. Therefore, the wizard was used to adapt interval sizes and map
same velocities to same velocities. Additionally different
patches were used, so both sounds can be easily differed.

3.2 Other examples
Examples of soniﬁcations of motion data taken from the
CMU Graphics Lab Motion Capture Database [13] can be
found at our Web site at
http://cg.cs.uni-bonn.de/project-pages/
sonification/

4 Conclusion and Future Work
We have presented a ﬂexible framework for the soniﬁcation of human motion data which allows a wide range of
new applications in many ﬁelds of motor control and motor learning. This interdisciplinary work is referred especially to computer science, sport science and psychology
of perception and cognition. With the presented framework a large number of different problems in sport science,
sports and rehabilitation can be explored and treated in an
advanced mode: Enhancing movement-perception with additional movement-acoustics promises effects not only for
blind people, who could get direct information about motor skills like butterﬂy, ﬂoor exercises or dance-movements
etc. Also instructive and feedback information in general
can be enhanced via movement soniﬁcation, if movement
data are available. This offers a wide range of new applications: Stroke-patients can be additionally treated auditorily
for starting therapy earlier or high performance athletes in
high diving e.g. can perceive complex movement patterns
with a higher temporal precision.
But also some aspects are still unsolved yet: Finding
appropriate soniﬁcation parameters applicable for different
movement patterns or even to come up with a rule basedsystem in this context—our wizard is just a ﬁrst step into
this direction—is one major focus of our future work.

Acknowledgements
We are grateful to Christoph Brzozowski for his help in
implementing the system. Some of the data used in this
project was obtained from mocap.cs.cmu.edu.
Figure 3. Simple two segments example used
in soniﬁcation
The last step is to compare velocities of the middle node
and the end node. Since the underlying function is a sine

References
[1] C. Brzozowski, S. Klemme, J. Melzer, and H. Nguyen. MotionLab – User’s Guide and Programmer’s Guide. Computer
Graphics Technical Report CG-2004-1, Institut f¨ur Informatik II, Universit¨at Bonn, 53117 Bonn, Germany, 2004.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

[2] W. T. Dempster. Space requirements of the seated operator. Technical Report WADC-55-159, AD-087-892, WrightPatterson Air Force Base, Ohio, 1955.
[3] A. O. Effenberg and H. Mechling. Multimodal convergent
information enhances reproduction accuracy of sport movements. In Proc. 8th Annual Congress of the European College of Sport Science, pages 196–197, 2003.
[4] A. O. Effenberg. Soniﬁcation – Ein akustisches Informationskonzept zur menschlichen Bewegung. Hofmann, Schorndorf, 1996.
[5] A. O. Effenberg. Multimodal convergent information enhances perception accuracy of human movement patterns.
In Proceedings of the 6th Annual Congress of the European
College of Sport Science (ECSS), 2001.
[6] A. O. Effenberg. Synergien der Sinne f¨ur die Bewegungsregulation. Sportpsychologie, 7, 2004.
[7] A. O. Effenberg. Movement soniﬁcation: Effects on perception and action. IEEE Multimedia, Special Issue on Interactive Soniﬁcation, 2005. Accepted for publication.
[8] D. I. Miller and W. Morrison. Prediction of segmental parameters using the Hanavan human body model. Med. Sci.
Sports, 7:207–212, 1975.
[9] K. L. Robbins and Q. Wu. Development of a computer tool
for anthropometric analyses. In F. Valafar and H. Valafar, editors, Proceedings of the International Conference on Mathematics and Engineering Techniques in Medicine and Biological Sciences (METMBS’03), pages 347–353, Las Vegas,
USA, June 2003. CSREA Press.
[10] A. Safonova, J. K. Hodgins, and N. S. Pollard. Synthesizing physically realistic human motion in low-dimensional,
behavior-speciﬁc spaces. ACM Trans. Graph., 23(3):514–
521, 2004. SIGGRAPH 2004.
[11] C. H. Shea, G. Wulf, J. H. Park, and B. Gaunt. Effects of
an auditory model on the learning of relative and absolute
timing. Journal of Motor Behavior, 33:127–138, 2001.
[12] M. H. Thaut, B. Tian, and M. Azimi-Sadjadi. Rhythmic ﬁnger-tapping to cosine-wave modulated metronome
sequences: Evidence of subliminal entrainment. Human
Movement Science, pages 839–863, 1998.
[13] CMU Graphics Lab Motion Capture Database. mocap.
cs.cmu.edu.
[14] Joint kinetics. kwon3d.com/theory/jntkin.html.

background in musical education. In parallel to his university studies he has also started a small business by himself
on web page design, programming and photography, and he
is highly interested in graphic design as well.

Contact address
Prof. Dr. Andreas Weber
Institut f¨ur Informatik II
R¨omerstr. 164
Universit¨at Bonn
53117 Bonn
Germany
Phone: +49 (228) 73-44 26
Fax: +49 (228) 73-42 12
E-mail: weber@cs.uni-bonn.de

A brief biography of the presenter
Joachim Melzer is a diploma student of computer science
at the University of Bonn, Germany. He has specialized in
Computer Graphics and Computer Animation. Under the
supervision of Prof. Dr. A. Weber and PD Dr. A. Effenberg
he is currently writing his diploma thesis covering the topic
of a framework for the soniﬁcation of human motion data.
Since he has been playing the piano for twenty years and
has been active as a singer since his days in high-school,
where he also had been a choirmaster for two years, he has
been attracted by the ﬁeld of soniﬁcation not only due to
his specialization in computer animation but also due to his

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

