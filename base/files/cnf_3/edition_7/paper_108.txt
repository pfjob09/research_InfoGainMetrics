Mobile Augmented Reality Techniques for GeoVisualisation
Fotis Liarokapis, Ian Greatbatch, David Mountain, Anil Gunesh, Vesna Brujic-Okretic,
Jonathan Raper
City University, Department of Information Science
{fotisl, iang, dmm, anil, vesna, raper}@soi.city.ac.uk
Abstract
This paper presents the first prototype of an
interactive visualisation framework specifically designed
for presenting geographical information in both indoor
and outdoor environments. The input of our system is
ESRI Shapefiles which represent 3D building geometry
and landuse attributes. Participants can visualise 3D
reconstructions of geographical information in real-time
based on two visualisation clients: a mobile VR interface
and a tangible AR interface. To prove the functionality of
our system an educational application specifically
designed for university students is illustrated with some
initial results. Finally, our conclusions as well as future
work are presented.
Keywords--Augmented
Reality,
Mobile
Interfaces,
Human-Computer
Interaction,
Geographical Information Systems.

1. Introduction
In recent years geographic data representing real
world features, more commonly used in Geographical
Information Systems, has increasingly been used for
virtual and augmented reality (AR) applications. GIS
have established many new services and applications
including navigation, decision support and modelling of
the surrounding environment. However, GIS users are
usually constrained to the functionality accessible
through the WIMP (windows, icons, menus and pointers)
metaphor as well as executable commands existing
within a geo-visualization [1]. This produces a barrier
that does not allow users to visualise and manipulate
geographical information in a realistic and natural
manner.
Although computer graphics techniques have been
used in the past with GIS to visualise data and to enhance
the interaction of the user with geographical information
[2] there is no geo-visualisation system to our knowledge
capable of engaging a mobile device user in the outdoor
environment. One way to overcome these barriers is by
placing the user directly in control of the geospatial
display through the use of powerful but also tangible user
interfaces (TUIs). TUIs are extremely intuitive to use
since the real object manipulations can be mapped oneto-one to virtual object operations [3]. In other words,
they can give physical form to virtual information,

facilitating
direct
manipulation
of
physical
representations [4].
The intuitive manipulation of tangible user
interfaces with the prospects of AR visualisation is
referred as tangible augmented reality [5]. On the other
hand, the ultimate goal of an effective AR system is to
enhance the user’s perception and interaction with the
real environment by superimposing the real world with
2D and 3D virtual information that appear to coexist in
the same space as the real world [6]. The superimposed
information can be presented in a number of different
mobile display systems including head attached displays
such as head-mounted displays and Head-Up Displays
(HUDs) as well as other types of displays including
PDAs and 3G phones. While a lot of work has been done
in tracking and registration much less work exists in
tangible AR interfaces that allow the realistic
visualisation and manipulation of geographical
information in a natural but also meaningful manner.
In this paper we present our first prototype system of
an EPSRC Pinpoint Faraday project called LOCUS [7]
aiming to extend the current map-based approach to offer
a tangible geo-visualisation interface. Our first objective
is to model the 3D scene around the user on the mobile
device given building height/type data using
photogrammetry and GIS modelling techniques. The
second objective is to present the modelled information
into a mixed reality environment that consists of two
different visualisation interfaces; a VR mobile interface
and a tangible AR interface. Participants can interact
with the virtual geographical information using
combinations of user-oriented and/or computer-oriented
interaction techniques. Finally, based on the functionality
of the AR interface we have designed and implemented,
a simple but robust educational game for GI Science
students studying at City University.
In the remainder of this paper we describe our
system starting with section 2 which gives a brief
overview of our mobile architecture, while section 3
describes the sub-systems of our architecture in more
detail.
Section 4 presents two different mobile
approaches for visualising geographical information in
3D. In section 5, we briefly describe the basic
capabilities of the human-computer interaction
techniques that have been implemented. Furthermore,
section 6 presents our experimental educational
application, called an interactive 3D puzzle, while
section 7 concludes by presenting our plans for future
work.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

2. Architecture of the system
The architecture of our prototype system is simple
and consists of four parts including content acquisition;
3D model generation; model enhancement; and content
visualization. A diagrammatic overview of the pipeline
of our system is presented in Figure 1.

Figure 1 Architecture of the system
In the content acquisition stage (section 3.1) the
objective is to collect all the necessary data (ESRI
Shapefiles as rasters) in respect of their spatial
relationship, in our case aerial photographic data and 2D
digital maps. In the 3D model generation phase (section
3.2) the digitised map information is converted into an
appropriate 3D format for visualisation (VRML).
Furthermore, in the model enhancement stage the 3D
maps are enhanced with more information so that they
look as realistic as possible. Typical operations include
texturing, re-lighting or even polygon reduction of the
whole 3D scene (section 3.3). Finally, in the content
visualisation phase (section 4) the enhanced geographical
information can be visualised in two different mobile
visualisation clients: a mobile VR interface designed to
operate in 3G phones and PDAs and a tangible AR
interface focused for laptop computers.

3. Content Production
Content production is a very important stage
because to a great extent it determines the accuracy and
the level realism of the cognitive visualisation. The
framework for the generation of 3D geographical models
consists of three stages including: the input data stage;
the 3D scene reconstruction stage and the model
enhancement stage and are presented in the following
sections (Figure 1). Finally, the content visualisation is
described in section 4.

3.1 Input Data Stage

digital terrain model generation have been widely used in
GIS due to the efficiency and effectiveness of the
production process [8]. Therefore, the input data for the
urban models of our prototype system include: ESRI
Shapefiles and UK Ordnance Survey mapping products
converted to ESRI raster formats.
ESRI Shapefiles have been generated using a
combination of aerial photography and stereoscopic
analysis to gain height information. Ordnance Survey
mapping products were used to create the footprints of
buildings and street geometry (Figure 6). In practical
terms, multiple height readings (which can be used to
model buildings with sloping roofs or spires, for
example) are contained with the shapefiles attribute
table. These shapefiles have been provided, under
licence, by the Cambridge Geoinformation Group [9].
1:50,000 scale colour raster mapping from the
Ordnance Survey was used to provide context and
background mapping. This data was obtained via the
JISC Edina Digimap Service [10]. For the model of the
Lake District shown and for other non-urban models,
1:50,000 Panorama Digital Terrain Models were used,
downloaded again from the JISC digimap service.

3.2 3D Scene Reconstruction
One of the greatest disadvantages in 3D scene
reconstruction compared to 2D map production relates to
the high cost and effort required to produce 3D
information but benefits from a certain level of
automation [11]. In this approach we have created
models for both common GIS data structures: raster and
vector [12].
The model for the urban London area was created
using the 3D analyst extension [13] in ESRI ArcView 9.
Using this extension, the height attribute for 2D polygons
can be used to generate 3D objects using an extrusion
procedure. In ArcView the term extrusion means
changing the form of a 2D vector feature by either
turning points into vertical lines, lines into vertical walls
or polygons into 3D blocks [14]. In practical terms, a
dialog box is opened in the 3D analyst and a field
selected from the features attribute database that contains
the height values for the features. These can either be
used as an absolute value, or used as a component of a
calculation. In the case of terrain models this extrusion
technique can be used to ascribe Z-values from the
surface itself (as opposed to a value or calculation from
the attribute table).
Overall,
buildings
and
constructions
are
dimensioned and drawn as 3D objects but the overall 3D
mesh is still rather simple and not very detailed yet [15].
To overcome these limitations, we import the 3D models
into a professional 3D modelling tool (section 3.3) to
calculate normals, faces, lighting information, texture
information, shadows, and other parameters useful for
the overall realism of the geographical information.

Photogrammetry is one of the most popular tools to
acquire the 3D data and digital photogrammetric
methods for providing automatic digital surface model or

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

3.3 Model Enhancement Stage
Nowadays, commercial modelling tools offer ways
of
creating
professional-quality
3D
models,
photorealistic still images and film-quality animations. In
this research, the industry standard 3ds max was used for
two important operations. The first was to digitally
enhance the geo-spatial landmarks and buildings to
mimic the appearance of the real buildings surrounding
the environment [16] while the second to export them
into the most appropriate VRML format (triangles,
Ngons, quads and visible edges).
The enhancement part involved making adjustments
to the 3D models generated by the 3D reconstruction
stage such as replacing the material properties (colouring
information and textures) of the 3D objects. Other
operations included adding extra lighting and shading
information to achieve the best pre-rendered
representation of the geographical information.
Furthermore, small corrections are performed in the
geometry of geographical topology such as smoothing
and tessellation to achieve a balance between the best
optimisation possible and the realism of digitised
information.

it concerns geometric objects, lights, materials and other
useful information.
Figure 2 illustrates an interactive 3D map of north of
England displayed on a PDA (ipaq 5450 pocketPC). The
3D map is displayed using pocket Internet Explorer with
a VRML plug-in from pocket Cortona by
ParallelGraphics. To increase the level of realism of the
VR interface, we have added collision detection to the
3D models so that users can navigate more naturally.

4. Visualisation Techniques
In terms of visualising the geographical information
we have implemented two different mobile solutions:
one with low-realism and one with high-realism. The
low-realism solution is targeted for mobile devices which
do not have the required technical characteristics to
simulate reality accurately. On the other hand, the highrealism solution is capable of rendering large datasets of
geographic information in real time performance as well
as presenting additional multimedia content information
such as text and images to the users.
In addition the proposed geo-visualisation solutions
can be combined together through a wireless network
connection, so that both solutions could be used
simultaneously. For example, a user can first visualise a
reduced version of the virtual information on the mobile
VR interface. If interested in obtaining a cognitively
richer 3D representation then it can be visualised using
the tangible AR interface.

4.1 Mobile VR Interface
The advantage of VR interfaces in GIS is that they
can provide egocentric views of a dataset [5]. VR
visualisation in our prototype platform can be realised
through the capabilities of the well known Virtual
Reality Modeling Language (VRML) which is built on
top of Open Inventor. VRML has dominated during the
last few years because it has the advantage of presenting
3D objects or 3D environments over the Internet. This
allows a VRML browser or a stand-alone application
(i.e. mobile client) to obtain a virtual scene locally or
remotely (i.e. WWW). The main principle relates to how
graphic scenes can be created, stored and transmitted and

Figure 2 Mobile VR visualisation
Various rendering parameters can also be manually
specified by the user, e.g. what shading algorithm to
apply (flat, smooth), what hardware acceleration to
exploit (OpenGL, DirectX). The exact extent to which
the user can control the navigation and rendering differs
between viewers. In the present implementation, the
VRML viewer controls are used to manipulate all 6
degrees of freedom: the observer location in three
dimensions, as well as the orientation (yaw, roll and
pitch).
Location-aware devices offer the potential to have
the two- or three-dimensional location controlled by the
mobile device’s positional determining technology. The
addition of a digital compass, allows yaw to also be
controlled by an external sensor, hence the VR model
can be registered in a real world setting without the need
for visual markers. This offers a natural movement- and
gesture-based approach of interacting with VR models
on mobile devices.
A significant drawback of mobile devices is that
they are constrained in terms of processing power,
memory and graphics memory. For the purpose of our
application, the VRML models and the textures are
optimised so that they can be rendered in an acceptable
frame-rate. In addition, typical mobile devices have
screens no larger than 480 by 640 pixels, and thus the
representational and communicative scope of this screen
real estate need to be considered.

4.2 Tangible AR Interface
In contrast to VR interfaces, AR interfaces have the
ability of providing exocentric views of information. In

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

other words, AR supplements the real environment,
whereas VR replaces it. Although there have been many
different AR visualisation techniques proposed for
presenting geographical information [5], [17], [18], [19]
there are few AR interface systems that are focused on
visualising 3D geographical information for pedestrian
navigational purposes of urban environments [20]. In
addition, most AR visualisation systems render either
wireframe scenes or simple colour but are not capable of
providing more realism.
Our tangible AR interface, called ARGIS is
implemented based on the experiences gained from two
previous implemented interactive AR interfaces [22][23]
which can simultaneously superimpose various types of
multimedia information including 3D models, images,
text and sound. Furthermore, ARGIS is a C++ standalone computer graphics application that operates inside
a Microsoft Foundation Classes (MFC) graphical user
interface (GUI) that wraps ARToolKit’s tracking
libraries [21], OpenGL and GLUT APIs. An example of
the system’s visualisation interface is shown in Figure 3.

Figure 4 Landscape visualisation in AR
The VRML model illustrated in Figure 4 is
approximately fifteen times bigger than the VRML
model used in VR visualisation (Figure 2). However, the
overall frame-rate performance varies between 25 to 40
FPS depending on the resolution of the camera (640×480
or 320×240).

5. Human-Computer Interactions
One the most important issues in computer interface
design is the natural and meaningful interaction between
the users and the virtual information and AR seems the
most promising technology [24]. In terms of
geographical interaction, it is also important to improve
the performance of spatial decision making over existing
2D and 3D GIS software tools. In our tangible AR
interface users can interact with the superimposed
information in either a user-oriented or a computeroriented manner.

5.1 User-Oriented Interactions
Figure 3 Model and textual augmentation
ARGIS allows the user to fully control the
visualisation of the geographical information in a userfriendly manner (section 5). Another interesting feature
of the ARGIS is that it allows the users to visualise the
geographical information in a demonstration mode,
which is in essence full screen mode, irrespectively of
the display’s resolution. The user can adjust the video
resolution through the menu interface so that it can fit on
the window size. Other parameters that can be changed
in real time include the brightness, contrast and
saturation of the web camera. An example of a 3D map
representing the north part of England in full screen
mode is presented in Figure 4.

In user-oriented interactions, participants do not
have to use any type of hardware to interact with the
digital representation of a map and can examine it from
any angle and at any distance through the use of physical
marker cards [19].

Figure 5 Natural user interactions
The major advantage of integrating this approach is
that it allows the user to manipulate the superimposed
information using a tangible interface (i.e. physical
marker cards, pages of a book, etc). Another important
advantage of this technique is that no previous
experience is required or any knowledge of computer

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

related technologies. In other words, the AR interface
can be effectively used by anyone. This makes the
system accessible to all kinds of users independently of
their characteristics (age, technical expertise, etc).
On the other hand, the main disadvantage of markerbased tracking is that the markers must always be in the
line of sight of the camera. In addition, vision-based
tracking algorithms are prone to a number of sources of
errors including: lighting conditions; material of the
markers; and range of operation.

5.2 Computer-Oriented Interactions
Computer-oriented interactions include those on
which a computer system or any other type of electronics
device is involved. Based on the previous experience
[22], [23] we have integrated hardware I/O devices such
as the keyboard and the mouse; as well as softwarebased solutions including a widget menu and a GUI. The
combination of these provides a powerful and effective
interaction mechanism where users can examine the
geographical information in a great detail. Using the
interface menu on the GUI users can easily navigate
using the interactive AR interface so that they can get the
spatial information required. More computer literate
users can make use of the keyboard and mouse to
manipulate the superimposed information. An example
screenshot of this exocentric navigation is shown in
Figure 6.

combination of educational and entertainment
experiences which can be available in the future to fulltime, part-time and distance learning students. For the
purpose of the interactive 3D puzzle scenarios we have
modelled a big part of the campus of City University in
correspondence to cognitive tuning and then visualise it
in both VR and AR interfaces. An example screenshot of
the virtual model of City University’s campus rendered
in the mobile VR interface is shown in the left image of
Figure 7 while the same information displayed in the
tangible AR interface is presented in the right image of
Figure 7.

Figure 7 City’s university campus
Furthermore, using the functionality of ArcGIS, the
3D mesh was split into three equal parts that represent
the virtual pieces of the puzzle and exported into separate
VRML files (section 3.2). Each VRML file was then
imported into 3ds max for further enrichments such as
scaling, smoothing and re-lighting (section 3.3). In
addition, each model is first normalised and then
assigned into a different marker card. During the session
and as long as the camera is in sight of view with them,
the virtual components of City Campus together with
supplementary textual information can be superimposed
into the real environment as illustrated in Figure 8.

Figure 6 Exocentric views of Southwark
It is worth mentioning, that computer-oriented
interactions offer the ability to perform accurately
various computer graphics operations such as rotating,
translating and scaling the virtual geographic information
so that even the finest detail can be clearly displayed.
We believe that computer-oriented interactions can
enhance the cognitive perceptions and increase the
ability to understanding geographical information (i.e.
maps). To enhance more this, hotspots can be integrated
into the 3D map so that when the user clicks on them,
multimedia information like animations, images and
metadata are superimposed into a context aware AR
environment.

6. An Interactive 3D Puzzle Application
Based on the framework explained in the previous
sections, we have carefully designed an experimental
educational application for geography students called ‘an
interactive 3D puzzle’. Our objective is to provide a

Figure 8 Puzzle demonstration in AR
Then the user can pick up the marker cards and
examine the geometrical and geographical information in
a tangible manner. An advantage of this application is
that it is possible to collaborate with other users that
could stand around the table-top environment and either
give advice or play the game. Multiple users can

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

naturally experiment with different combinations by
randomly placing the marker cards close to each other as
depicted in Figure 9.

Figure 9 Collaborative educational environment
As soon as the interactive 3D puzzle is completed,
meaningful textual feedback is provided again, in this
time in order to congratulate the users for solving the
puzzle. The size and the colour of the superimposed text
can be changed interactively by the users using the
interface menu or predefined keyboard keys. Next, an
illustration of the solution of the interactive 3D puzzle is
shown in Figure 10.

7. Conclusions and Future Work
This paper describes our first prototype system for
building and presenting geographical information using
GIS, 3D modelling, AR and VR technologies. The most
distinctive feature of our system is its ability to allow
users to visualise the same information in a two types of
mobile environments including: a mobile VR interface
and a tangible AR interface. The mobile VR interface is
operational on PDAs and 3G phones whereas the
tangible AR interface currently works on laptop
computers. Our prototype allows interaction with 3D
geographical data in a range of virtual and augmented
views. Position determining technology and digital
compasses will be investigated as means of registering
virtual models in outdoor settings.
Currently we are developing routing tools which
will be based on the data mining of previous journey
experiences. In addition, we are building a database to
hold our geographical data as well as more educational
scenarios and feed both visualisation clients on the fly.
As soon as these two tools have been developed and
tested appropriately then we will port our tangible AR
interface to PDAs and commercial 3G mobile phones
that have a camera and a GPS receiver embedded.
In the future, we plan to develop optimisation
techniques in the visualisation side including level-ofdetail and culling to increase the overall rendering
efficiency of the system. In respect to human computer
interactions additional user-oriented techniques like
gestures and voice recognition, as well as computeroriented approaches based on VR interaction devices
such as inertia cube and digital compass will be
integrated. In addition, more types of interaction
techniques will be included such as gestures. Finally,
additional educational scenarios will be designed so that
they can be then properly evaluated and applied in
practice for our teaching purposes.

8. Acknowledgements
Figure 10 Solution of the AR puzzle
Furthermore, this system has been demonstrated in
GeoInformation Group’s Cities Revealed Event 2005
conference as well as informally to a number of
postgraduate students in the department of Information
Science (City University). The feedback we have
received was that the AR interface is very easy to use
even if there is much more work required for an
educational application that can be used in practice.
Although the application has not been yet evaluated
informal feedback from the demonstrations showed that
the average time for solving the puzzle was measured to
be around 3-4 minutes depending of the student.
Nevertheless, we strongly believe that in the near future
similar educational applications will start to appear in
universities while in the longer term commercial 3D
applications will become part of our lives.

This research work was funded by the EPSRC
Pinpoint Faraday project GR/T04212/01, called LOCUS.
We would to thank the Cambridge GeoInformation
Group for providing the Cities Revealed data.

9. References
[1]

[2]

[3]

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

Brewer, I., Cognitive Systems Engineering and
GIScience: Lessons learned from a work domain analysis
for the design of a collaborative, multimodal emergency
management GIS, GIScience 2002, Boulder, Sep 25-28,
2002.
Verbree, E., G. V. Maren, R. Germs, F. Jansen, M. J.
Kraak, Interaction in virtual world views-linking 3D GIS
with VR, International Journal of Geographical
Information Science. 13(4): 385-396, 1999.
Fitzmaurice, G., and Buxton, W., An Empirical
Evaluation of Graspable User Interfaces: towards
specialized, space-multiplexed input, Proceedings of the

[4]

[5]

[6]

[7]
[8]

[9]

[10]

[11]

[12]
[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

ACM Conference on Human Factors in Computing
Systems, 43-50, 1997.
Ishii, H., Ratti, C., et al., Bringing clay and sand into
digital design – continuous tangible user interfaces, BT
Technology Journal, 22(4): 287-299, 2004.
Hedley, N., Billinghurst, M., et al., Explorations in the
use of Augmented Reality for Geographic Visualization,
Presence: Teleoperators and Virtual Environments,
11(2): 119-133, 2002.
Azuma, R., Baillot, Y., et al., Recent Advances in
Augmented Reality, IEEE Computers & Graphics,
November/December, 21(6): 34-47, 2001.
LOCUS, Available at: [http://www.locus.org.uk/],
Accessed at: 09/03/2005.
Zhou, G. Q., Song, C., et al., Urban 3D GIS from LiDAR
and digital aerial images Computers & Geosciences, 30,
345-353, 2004.
The
Geoinformation
Group,
Available
at:
[http://www.cjdirectory.com/company-30808877.html],
Accessed at: 22/02/2005.
EDINA
Digimap
Service,
Available
at:
[http://www.edina.ac.uk/digimap],
Accessed
at:
22/02/2005.
Zlatanova, S., and Heuvel, F., 3D GIS for outdoor AR
applications, Proceedings of the Third International
Symposium on Mobile Multimedia Systems &
Applications, December, Delft, The Netherlands, 117124, 2002.
Longley, P., Geographic information systems and
science, Wiley, Chichester, 66, 2001.
Booth, B. and Environmental Systems Research, I.,
Using ArcGIS 3D analyst: GIS by ESRI, Environmental
Systems Research Institute, Redlands, CA, 2000.
Batty, M., and Smith, A., Virtuality and Cities In Virtual
reality in geography (Eds, Fisher, P. and Unwin, D. J.)
Taylor & Francis, London, 270-291, 2002.
Verbree, E., Van Maren, G., et al., Interaction in virtual
world views - linking 3D GIS with VR, International
Journal of Geographical Information Science, 13, 385396, 1999.
Raper, J. Liarokapis, F. et al, Personal navigation using
digital mobile devices, Geomatics World, March/April,
13(3): 36-38, (2005).
Hinn, R., Redmer, B., and Domik, G., AR-Campus, The
First IEEE International Augmented Reality Toolkit
Workshop, Darmstadt, Germany, 2002.
Ghadirian, P., and Bishop, I., Composition of
Augmented Reality and GIS to Visualise Environmental
Changes, Joint AURISA and Institution of Surveyors
Conference, Adelaide, South Australia, 2002.
Höllerer, T., Feiner, S., at al., Exploring MARS:
Developing Indoor and Outdoor User Interfaces to a
Mobile Augmented Reality System, Computers &
Graphics, Dec, 23(6): 779-785, 1999.
Reitmayr, G., and Schmalstieg, D., Collaborative
Augmented Reality for Outdoor Navigation and
Information Browsing. In: Gartner, G. (Ed.), Symposium
Location Based Services and TeleCartography 2004,
Vienna, Austria, 31-41, 2004.
Kato, H., Billinghurst, M., et al., Virtual object
manipulation on a table-top AR environment,
Proceedings of the International Symposium on
Augmented Reality, 111-119, 2000.
Liarokapis, F., White, M., Lister, P.F., Augmented
Reality Interface Toolkit, Proceedings of the

International Symposium on Augmented and Virtual
Reality, London, 761-767, 2004.
[23] Liarokapis, F., Sylaiou, S., et al., An Interactive
Visualisation
Interface
for
Virtual
Museums,
Proceedings of the Fifth International Symposium on
Virtual Reality, Archaeology and Cultural Heritage,
Brussels, 47-56, 2004.
[24] Rauterberg M., New directions in User-System
Interaction: augmented reality, ubiquitous and mobile
computing, IEEE Proceedings Symposium Human
Interfacing; May 20, Eindhoven University of
Technology, 1999.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

