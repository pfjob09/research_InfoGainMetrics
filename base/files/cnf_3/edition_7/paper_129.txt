Visual Exploration of Time-Varying Matrices
 

 

✁

 

Ermir Qeli , Wolfgang Wiechert , Bernd Freisleben
Department of Mathematics and Computer Science, University of Marburg, Germany
ermir,freisleb @informatik.uni-marburg.de
Institute of Systems Engineering, University of Siegen, Germany
wiechert@simtec.mb.uni-siegen.de
✂

✄

✁

Abstract
In this paper, we present several extensions of our previous work on combining the multidimensional scaling technique and the reorderable matrix method to visualize timevarying matrices: (a) the Sammon mapping is employed as
another dimension reduction technique that in contrast to
multidimensional scaling pays more attention to small distances; (b) a novel method for the interactive colored visualization of covariances/correlations is presented; (c) the
K-means clustering algorithm is used and its results are
directly visualized in the mentioned dimension reduction
plots; (d) a novel view, namely the visualization of the timely
evolution of the cluster membership, is proposed. The latter is based on calculating a cumulated adjacency matrix
that gathers the information regarding membership of objects in clusters for each point of time. The color visualization of this matrix allows the investigation of changes in
cluster memberships and possible outliers, i.e. objects that
change clusters frequently. Results are presented by visualizing sensitivity matrices generated during the simulation
of metabolic network models.

1 Introduction
Several visualization techniques have been proposed to
explore static multidimensional data. The focus of this paper is the visualization of dynamic multidimensional data
that changes over time. In particular, the visualization of
time-varying matrices is considered. Two problems can be
distinguished in this case: (a) ﬁnding patterns/structures
locally, i.e for a single point of time and (b) ﬁnding patterns that persist for several points of time or globally for
all points of time. We have developed a toolkit to offer
solutions to these problems. It consists of several visualization methods that provide multiple coordinated views to
the user. Two of these, the multidimensional scaling view
and the colored reorderable matrix view, have already been

described in previous work [23]. Multidimensional scaling
is a nonlinear dimension reductioning technique, while the
reorderable matrix method allows the visualization of multidimensional data by mapping numerical values to colors,
gray-scale values or symbols, which are then ordered automatically or interactively with the help of the user. Furthermore, in [24] we explored different techniques for adapting
the reorderable matrix method for time-varying data as well
as algorithms for searching for similar reorderings, i.e. permutations of the columns which are consistent within certain time ranges. However, these techniques are restricted
in the sense that they search for complete permutations of
the columns whereas in fact often only parts of these permutations may be consistent within all points of time.
In this paper, multidimensional scaling is extended and
combined with an interactive version of the K-Means clustering algorithm to allow the direct exploration of the clustering results in the multidimensional scaling view. Furthermore, the Sammon mapping is provided for dimension
reduction purposes since it preserves small distances better.
In addition, two novel views are proposed: the reorderable
covariance/correlation matrix view and the colored cluster membership evolution view. The reorderable covariance/correlation matrix view allows the visualization of the
covariances and correlations for a point of time as well as
the visualization of their cumulative counterparts. The colored cluster membership matrix view represents the timely
evolution of cluster memberships for the objects clustered
by the K-Means algorithm. It is based on calculating a cumulated adjacency matrix that gathers information regarding the membership of objects in clusters for each point of
time. By examining the color visualization of this matrix,
changes in cluster memberships and possible outliers, i.e.
objects that change clusters frequently, can be extracted.
Furthermore, groups of objects which belong to the same
cluster for a certain number of points of time can be distinguished.
The current application area of our toolkit is the visualization of time-varying sensitivity matrices generated dur-

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

ing the simulation of metabolic network models [14, 13].
The sensitivity matrices of our application indicate how the
changes in the parameters of a metabolic network model affect the output of the model. These matrices are analyzed
in order to ﬁnd redundancies, which are then used to derive a simpler model with the same properties, and important parameters which could be properly changed to achieve
speciﬁc effects in the output of the model.
The paper is organized as follows. Section 2 gives a
survey of related work in the ﬁeld. Section 3 presents the
details of the proposed visualization toolkit. Section 4 describes an application example. Section 5 concludes the paper and outlines areas for future research.

2 Related Work
A general overview of visual data mining techniques can
be found in [16]. In this section, we focus on existing approaches that are relevant in the context of our work.
The reorderable matrix method proposed by Bertin [2, 3]
is a simple but robust approach to visualize tabular data.
The main idea of the reorderable matrix method is to convert a tabular data set into an interactive 2D view. The 2D
view has the same dimensions as the original data, and according to Bertin the data size should not exceed dimen, where X and Y represent the disions of
mensions of the rows and columns of data. Data values are
replaced by ﬁlled circles whose size depends on the actual
value. With manual interaction or automatic permutations,
different patterns in the data are made visible. Minnotte
et al. [21] use reorderable matrices under another name,
data image, to explore high-dimensional data. Marchette
et al. [19] use data images for outlier detection in data.
Corrgrams proposed by Friendly [8] is an approach similar
to the reorderable matrix method to visually explore correlation matrices which are important in multivariate statistics. Bezdek and Hathaway [4] use an approach called
ODI (Ordered Dissimilarity Image) for visually clustering
data. A similar approach is used in [18, 11] for reordering the so called pairwise conditional entropy matrix of dimensions (in high-dimensional datasets) for ﬁnding interesting subspaces of dimensions, i.e. features. The pairwise
conditional entropy is a measurement for detecting the mutual interaction between two dimensions. In [9], the usual
graph visualization approach is compared with a matrixbased representation whose visualization is similar to the
reorderable matrix. However, the reordering of the matrix
based representation is not considered, since it aims to assess the difference between the two representations. In [27],
Siirtola combines parallel coordinates with reorderable matrices [2, 3] to visualize multidimensional data. Chen and
Liu [5] present their VISTA framework for visually validating and reﬁning clusters.
☎

✆

✞

✠

☛

✌

✌

✌

✌

Dimension reduction is another alternative to visualize
multivariate data in 2D or 3D. There are two types of dimension reduction techniques: linear and nonlinear. Principal component analysis (PCA) is a linear projection method
where the projection is formed as a linear combination of
the input. Multidimensional scaling (MDS) and the Sammon Mapping are two related nonlinear projection methods,
with the former method preserving large distances and the
latter preserving small distances. A survey of these techniques can be found in [7]. Chernoff faces [6] represent
multidimensional data by means of faces with changing attributes. Thus, the problem of ﬁnding similar vectors is
converted into the problem of ﬁnding similar faces, which
is somehow easier for the human eye. Parallel coordinates
introduced by Inselberg and Dimsdale [15] allow visualizing multidimensional data in parallel axes. Andrews curves
[1] is a visualization method similar to parallel coordinates
based on a transformation similar to a Fourier transformation.
The techniques for visualizing time-varying data usually
deal with univariate or vectorial data only. A survey of visualization techniques for time-dependent data is given by
M¨uller and Schumann [22]. Groenen and Franses [10] use
multidimensional scaling to visualize time-varying stock
market correlations. Kosara et al. [17] have proposed TimeHistograms as an extension of standard histograms to visualize time-varying data.
Finally, we have previously addressed the problem of visualizing time-varying data in [23, 24]. In [23], a combination of MDS with reorderable matrices is proposed, whereas
in [24] the problem of ﬁnding an optimal reordering over all
points of time is discussed. However, these techniques are
restricted in the sense that they search for complete reorderings, i.e permutations of the columns, whereas in fact often
only parts of these permutations may be consistent within
all points of time.

3 Overview of the Toolkit
The developed toolkit consists of several coordinated
views that interact with each other. The input data to the
toolkit is a set of matrices, one for each point of time. In
our application scenario, the matrices contain parameters
of a metabolic network model and the metabolites that are
affected by the changes in the respective parameters. The
matrix in Figure 1(a) shows an extract consisting of only
one point of time, which is obtained from the sensitivity
analysis of a metabolic model representing the Shikimate
pathway. This matrix shows how the changes in metabolites
(represented by the rows) are affected by changes in the parameters (represented by the columns) of the model. For
example, if we consider the model as a black box, a change
of one unit in the parameter bs aaa6 rmax (8th column)

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

(a) The Raw Data Matrix

(b) The Unordered Color Visualization

(c) The Reordered Color Visualization

Figure 1. The Raw Data and its Color Visualization
would bring an increase of 0.147 in the metabolite EPSP
and a decrease of 3.140 in the metabolite DHQ. Although
the methods contained in the toolkit are tailored to this kind
of time-varying matrices, no further domain knowledge information is used. In this way, the framework can be used
to visualize any kind of similar data set.

3.1

Raw Data Reorderable Matrix

For the human user, it is not easy to understand the semantics of data based only on raw numbers. Replacing the
numbers with colored boxes is one solution to this problem. Thus, the basic idea of the raw data reorderable matrix
method is to transform a matrix of numbers into a matrix of
colors. For this purpose, a color spectrum is needed to transform numbers into colors. Throughout this paper, the redwhite-blue spectrum is used corresponding to the segment
[-1,1] of numerical values whereas the spectrum white-blue
is used to present values in the segment [0,1]. Figure 1(b)
represents the colored reorderable matrix of the data given
in Figure 1(a). The graph plotted on the side of the matrix visualization represent the vector of L-inﬁnity norms
for each row based vector (L norm for a vector is deﬁned as
) which is used to normalize the
data within all points of time in order to compare visualizations for two different points of time. To simplify the exploration process, several algorithms and interactive methods
for reordering the matrix have been implemented. They are
described in detail in [23, 24]. Figure1(c) is the reordered
matrix corresponding to 1(b) where similar columns appear
near each other.
✓

✑

✓

✔

✒

✓

✔

✠

✗

✙

✛

✢

✓

✛

✢

✒

3.2

Multidimensional Scaling and Sammon Mapping

For high-dimensional data sets, it is very difﬁcult to visualize different structures using only linear projections.
There are several approaches for reproducing nonlinear,
high-dimensional data structures. These approaches map
objects into the lower-dimensional space and then try to
optimize this mapping so that the new distances in the
low-dimensional space reﬂect the original distances in the
higher-dimensional space. The methods are distinguished
from each other by the weighting scheme for the distances
and the optimization algorithm used.
Multidimensional scaling is a widely used technique for
nonlinear dimension reduction. Its purpose is to construct
a conﬁguration of n entities in Euclidean space using information about the distances between these entities (by entities we mean any multidimensional vectors). By allowing
the use of similarity/dissimilarity measures instead of strict
distances, MDS enables to ﬂexibly view the relationships
between data items. The Sammon Mapping is a projection method similar to MDS. In contrast to MDS, it preserves smaller distances better than large ones. We have
implemented a modiﬁed version of the classical algorithm
for metric multidimensional scaling [29, 26, 20] and a version of the Sammon Mapping [25]. The performance of the
classical MDS algorithm is strongly dependent on the procedure for ﬁnding the eigenvalues of the distance matrices.
For the size of sensitivity matrices that we visualize, the
time for calculating the MDS and Sammon Mapping solution for one point of time ranges from tens of milliseconds

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

(a) MDS View

(b) Sammon Mapping

Figure 2. The MDS and Sammon Mapping Views of the Data in Figure 1(a)
to hundreds of milliseconds on a typical PC (Pentium IV,
1.8 Ghz, SuSE Linux 8.1). Figure 2 shows the two respective views of the classical MDS and the Sammon Mapping.

3.3

K-Means

K-means is a popular iterative descent algorithm for nonhierarchical clustering [12]. It is used for quantitative data
and tries to partition the data into K clusters, where K
should be known beforehand. For each cluster, a mean feature vector is calculated and the algorithm tries to ﬁnd the
clusters with the minimum Euclidean distance with respect
to this mean feature vector. Two problems arise when applying the K-means algorithm: 1) a change in the number of
clusters could possibly bring quite different clusters and 2)
the initialization of the model is crucial. The ﬁrst problem
is addressed using an interactive version of K-means: the
user selects the number of clusters and the partition is then
directly reﬂected in the respective projection view (MDS or
Sammon Mapping). The initialization process is carried out
using principal component analysis as described in [28]. In
Figure 2, objects belonging to different clusters are plotted
using different colors. From Figure 2(b) it is evident that
clusters are visually better separated by the Sammon Mapping than by the classical MDS. This results from the fact
that the Sammon Mapping is initialized with the projection
produced by MDS and thus tries to improve MDS further.

3.4

Reorderable Covariance/Correlation Matrix

Covariance and correlation are closely related parameters that indicate the extent to which two random variables

co-vary. In our case they give us a measure on how the parameters of the model are related to each other. Whereas
the approach described in [8] is a good step forward in the
exploration of correlation matrices, we have implemented
a similar approach containing two additional aspects: (a)
Prim’s ordering algorithm for proximity matrices is implemented, and (b) interaction with the user is allowed. Figure
3 shows the reorderable correlation matrix for the data in
Figure 1(a). Figure 3(a) shows the initial unordered correlation matrix and Figure 3(b) shows the same matrix after
applying Prim’s algorithm. In Figure 3(c), this ordering is
then changed by iterative click-and-drag steps to obtain a
more satisﬁable ordering. The data in Figure 3(b) is divided
into three large groups of correlated parameters which by
themself are divided further into smaller groups. One parameter, which is uncorrelated with the rest is represented
by a row of white boxes. Although some of the information
present in Figure 3(c) could also be extracted from Figure
1(c), the correlation matrix emphasizes it in a much better
way.

3.5

Reorderable Cluster Membership Evolution
Matrix

The K-means algorithm creates an assignment array
A containing information about cluster membership for
each object clustered. For a run of K-means for n objects, A[i] would have a value between 0 and K-1 for
i=1..n. This assignment vector for the vector of parameters in the example in Figure 1(a) has the form
(1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,0,1,1) in the case when
we run K-means with K=2, meaning that only three el-

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

(a) Unsorted View

(c) Sorted View after Manual Permutation

(b) Sorted View

Figure 3. The Reorderable Correlation Matrix of the Data in Figure 1(a)
ements belong to the cluster labeled as cluster 0; the
rest belongs to the cluster labeled as cluster 1. Since
we have time-varying data, it is interesting to ﬁnd out
how these cluster memberships evolve over time. The
vector representation given above does not help much in
this case since (1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,0,1,1) and
(0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0) give us the same
information; only the cluster labels have changed. For this
reason, we transformed the assignment vectors into what
we call a membership adjacency matrix. This matrix contains a 1 if the two objects are in the same cluster, 0 otherwise. An element is considered as being not in the same
cluster with itself since this is not important for our application. It can be veriﬁed that for both assignment vectors the
same matrix would result. Another advantage of this matrix representation is that these matrices can be cumulated
for different segments of time, resulting in matrices that describe how often the respective objects were in the same
cluster for the selected time segment. Algorithm 1 shows
the pseudocode of the above described procedure. The matrix in Figure 4(b) shows the result of cumulation for the
entire time range for the example used for illustration. The
advantage of this cumulation matrix is that it contains information about cluster membership for all points of time.
However, the numerical values in this matrix are difﬁcult
to interpret as the dimensions of the matrix grow. For this
reason, this matrix is transformed into a frequency matrix
with values in [0,1] by dividing the values by the number
of points of time which were used in the cumulation. This
transformed matrix is then visualized as a reorderable matrix, which we call ’Cluster Membership Evolution Matrix’,
as shown in Figure 4. Figure 4(a) represents the cumulative
adjacency matrix where all time points are taken into consideration. The number of clusters for each point of time is
three in this case. Figure 4(b) represents the corresponding

colored visualization. Figure 4(c) is the result of reordering of columns of Figure 4(b). For comparison purposes we
have included Figure 4(d) which differs from Figure 4(c) in
the fact that it was generated using two clusters instead of
three. The lighter colors in Figure 4(c) suggest that three
clusters are more appropriate. Possible outliers in this context, i.e. parameters that change clusters frequently over all
points of time are represented by columns/rows visualized
with lighter colors. Though K-Means is used as the input of
the Cluster Evolution Algorithm, any other partitional clustering algorithm could be used instead of K-Means.
Data: Input Data
Result: Visualization of Cluster Evolution
Initialize all elements of A with 0;
for t=1 to
do
for i=1 to n do
for j=i+1 to n do
if samecluster(i,j) then
C[i,j]=1;
end
end
end
A= ((t-1)A+C);
Update the Reorderable Matrix Display of A;
end
Algorithm 1: Algorithm for Displaying Cluster Evolution
☎

✮

✺

✲

✗

✵

✦

✷

✦

★

✪

✬

✮

✰

★

✻

4 Application Example
Our project partners of the biotechnology group at the
Research Center J¨ulich, Germany are the end users of the

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

(a) Cumulative Adjacency Matrix

(b) Unsorted Cluster Membership Evolution Matrix

(c) Sorted Cluster Membership Evolution Matrix

(d) Sorted Cluster Membership Evolution Matrix (Two Clusters)

Figure 4. The Reorderable Cluster Membership Evolution Matrix
proposed toolkit. They create models of the metabolic networks of biological cells. However, these models are often
very complicated and depend on a large set of parameters,
typically in the range of tens to hundreds of parameters. The
effect that the change of one parameter brings to the output
of the model varies from parameter to parameter. When the
output is inﬂuenced by small changes of a certain parameter, then the parameter is deﬁned as a sensitive one and is
thus relevant for the modeling. On the other hand, there
are parameters which do not affect the output of the model
and could possibly be eliminated, and parameters which are
correlated with each other and could be used to derive a
simpler model. In this context, sensitivity analysis [14, 13]
is carried out, in order to analyze the sensitivity of a model
with respect to changes in parameters. The calculation of
sensitivities is described in detail in [13]. The results of the

sensitivity analysis are time-dependent sensitivity matrices,
which can be visually explored using the presented toolkit.

5 Conclusions
In this paper, we have presented a toolkit consisting of
a set of methods to visualize time-varying matrices. Several extensions to existing approaches were made. First,
dimension reduction techniques, such as multidimensional
scaling and the Sammon mapping, were combined with an
interactive version of the K-means clustering algorithm in
order to create a graspable view of the multidimensional
data. Second, a novel method for visualizing the covariances and correlations for a point of time as well as the visualization of their cumulative counterparts was presented.
Third, the cluster membership evolution view enabling the

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

user to distinguish clusters or parts of them which are consistent within all time points was proposed. The beneﬁts of
this toolkit were illustrated by visualizing sensitivity matrices generated during the simulation of metabolic network
models.
There are several areas for future work. For example, we
plan to enrich the proposed visualization methods by distortion techniques to give the user the possibility to explore
very large matrices, since the current techniques are limited
by the screen dimensions. Furthermore, the possibility of
distributed methods (running on multiple networked computers) for computationally intensive visualizations will be
investigated. Finally, the application of the proposed approaches in domains other than metabolic modeling is an
interesting area for future research.

6 Acknowledgments

[12]
[13]

[14]

[15]

[16]

[17]

This work is ﬁnancially supported by the Deutsche
Forschungsgemeinschaft (DFG, Schwerpunktprogramm
1063, Teilprojekt FR 791/8-1).

[18]

References

[19]

[1] D. Andrews. Plots of high dimensional data. Biometrics,
28:125–136, 1972.
[2] J. Bertin. Graphics and Graphic Information Processing.
Walter de Gruyter, 1981.
[3] J. Bertin. Semiology of Graphics. University of Wisconsin
Press, 1983.
[4] J. Bezdek and R. Hathaway. Vat: A tool for visual assessment of (cluster) tendency. Proceedings of the International
Joint Conference on Neural Networks(IJCNN), pages 2225–
2230. IEEE, 2002.
[5] K. Chen and L. Liu. Vista: Validating and reﬁning clusters
via visual rendering. Journal of Information Visualization,
3(4):1–14, 2004.
[6] H. Chernoff. The use of faces to represent points in kdimensional space graphically. Journal of the American Statistical Association, 68:361–368, 1973.
[7] I. Fodor. A survey of dimension reduction techniques.
Lawrence Livermore National Laboratory (LLNL) Technical Report, UCRL-ID-148494, 2002.
[8] M. Friendly. Corrgrams: Exploratory displays for correlation matrices. The American Statistician, pages 316–324,
2002.
[9] M. Ghoniem, J.-D. Fekete, and P. Castagliola. A comparison of the readability of graphs using node-link and matrixbased representations. IEEE Symposium on Information Visualization 2004, pages 17–24. IEEE, 2004.
[10] P. J. Groenen and P. H. Franses. Visualizing time-varying
correlations across stock markets. Journal of Empirical Finance, 7:155–172, 2000.
[11] D. Guo, M. Gahegan, D. Peuquet, and A. MacEachren.
Breaking down dimensionality: An effective feature selection method for high-dimensional clustering. Workshop

[20]
[21]

[22]

[23]

[24]

[25]
[26]

[27]

[28]
[29]

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

on Clustering High Dimensional Data and its Applications,
The Third SIAM International Conference on Data Mining,
pages 29–42. SIAM, 2003.
J. Hartigan and M. Wong. A k-means clustering algorithm.
Applied Statistics, 28:100–108, 1979.
M. D. Haunschild and W. Wiechert. Sensitivity analysis of
metabolic network models. ASIM 2003, 17. Symposium
Simulationstechnik, pages 415–420, 2003.
B. P. Ingalls and H. M. Sauro. Sensitivity analysis of stoichiometric networks: An extension of metabolic control
analysis to non-steady state trajectories. Journal of Theoretical Biology, 222(1):23–26, 2003.
A. Inselberg and B. Dimsdale. Parallel coordinates: A tool
for visualizing multidimensional geometry. IEEE Conference on Visualization ’90, pages 361–378. IEEE, 1990.
D. A. Keim. Information visualization and visual data mining. IEEE Transactions on Visualization and Computer
Graphics, 7(1):1–8, 2002.
R. Kosara, F. Bendix, and H. Hauser. Timehistograms for
large, time-dependent data. Proceedings of Joint Eurographics - IEEE TCVG Symposium on Visualization (VisSym
2004), pages 45–54, 2004.
A. MacEachren, X. Dai, F. Hardisty, D. Guo, and
G. Lengerich. Exploring high-d spaces with multiform matrices and small multiples. 2003 IEEE Symposium on Information Visualization, pages 31–38. IEEE, 2003.
D. J. Marchette and J. L. Solka. Using data images for outlier detection. Computational Statistics & Data Analysis,
43:541–552, 2003.
K. Mardia. Multivariate Analysis. Academic Press, 1979.
M. C. Minnotte and R. Webster. The data image: A tool for
exploring high dimensional data sets. 1998 Proceedings of
the ASA Section on Statistical Graphics, pages 25–33, 1998.
W. M¨uller and H. Schumann. Visualization methods for
time-dependent data: An overview. 2003 Winter Simulation
Conference, pages 737–745, 2003.
E. Qeli, W. Wiechert, and B. Freisleben. Visualizing timevarying matrices using multidimensional scaling and reorderable matrices. Proceedings of the 2004 International
Conference on Information Visualization (IV2004), London, UK, pages 561–567. IEEE, 2004.
E. Qeli, W. Wiechert, and B. Freisleben. The timedependent reorderable matrix method for visualizing evolving tabular data. Proceedings of the 2005 IST/SPIE Conference on Visualization and Data Analysis, San Jose, USA,
pages 199–207. SPIE, 2005.
J. Sammon. A nonlinear mapping for data structure analysis.
IEEE Transactions on Computers, 18(5):401–409, 1969.
I. Schoenberg.
Remarks to m. fr’echet’s article sur
la d’eﬁnition axiomatique d’une classe d’espaces vectoriels distanci’es applicables vectoriellement sur l’espace de
hilbert. Annals of Mathematics, 36:724–732, 1935.
H. Siirtola. Combining parallel coordinates with the reorderable matrix. Coordinated and Multiple Views In Exploratory
Visualization (CMV’03), pages 63–74. IEEE, 2003.
H. Sp¨ath. The Cluster Dissection and Analysis Theory FORTRAN Programs Examples. Prentice-Hall, 1985.
G. Young and A. Householder. Discussion of a set of points
in terms of their mutual distances. Psychometrica, 3:19–22,
1938.

