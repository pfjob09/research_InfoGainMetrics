A Vision-based Augmented Reality System for Visualization Interaction
Wen Qi
Faculty of Industrial Design
University of Technology Eindhoven
P.O.Box 513, 5600 MB Eindhoven
The Netherlands
w.qi@tue.nl

Abstract
This paper presents a vision-based video see-through
augmented reality (AR) system for 3D interaction with volumetric data. Traditional desktop system and ﬁsh-tank virtual reality system are basic choices for volume visualization application which give the user limited interaction capabilities. AR provides a new scheme of 3D interaction
between human and computer. We have integrated optical
tracking and hardware accelerated volume rendering components into a video see-through AR system framework. It
presents the user with a more natural environment for interactive visualization task, which is different from the traditional visualization system. Rich 3D experiences and efﬁcient interaction with scientiﬁc data have been achieved by
the user through this system. This prototype is an appropriate platform to investigate issues of 3D interaction with
volumetric data and to build further complex interactive application.

1. Introduction
Direct volume rendering is an innovative method of displaying volumetric data efﬁciently [12]. It is comparably
the faster way to visualize volumetric data in comparison
with other algorithms. However, interpreting and interacting with 3D data is still quite tough because understanding the spatial relations between volumetric data and working environment requires mental fusion of these two that
have been observed separately [8]. Virtual reality provides
a technique for the user interacting with volumetric data
with the feeling of presence. However the familiarity of
real environment makes the user ask for better techniques
to enrich the presence feeling during the interaction. Also
being fully immersed in an virtual environment create other
difﬁculties, such as lack of spatial awareness. Augmented
reality (AR) system tries to solve this problem by adding

Mixed Reality (MR)
Real
Environment

Augmented
Reality (AR)

Augmented
Virtuality (AV)

Virtual
Environment

Virtuality Continuum (VC)

Figure 1. Milgram’s reality virtuality continuum

electronic data from a cyberspace on the physical space as a
base.It allows the users to see the real world with virtual objects superimposed upon [7]. This gives the user additional
information besides the computer-generated model. Moreover, AR helps the user interact with the virtual object in
a more realistic way. For example, for surgeon, AR could
help the surgeon in a way that the display of correctly registered medical anatomy structure on a real environment or
a patient phantom that will provide a new method of surgical training and guidance [13, 9]. Milgram [15] deﬁned a
continuum of real-to-virtual environments, in which AR is
one part of the general area of mixed reality(Figure 1). In
short, AR has received a great deal of attention as a new
method for generating new types of visual forms and interaction scheme [2].

2. Previous work
The core technical difﬁculties for building an AR system are real time tracking and precise registration. Precise
registration between virtual objects and real environment
is dependent on appropriate dynamic tracking method and
precise orientation information derived from it. Many of
the current AR systems rely on active tracking system such

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

Figure 3. Triplane with labels over interesting regions (Patrick Sinclair, University of
Southampton [17])

Figure 2. The Studierstube (Dieter Schmalstieg, Vienna University of Technology [16])

as magnetic trackers, ultrasonic tracker and so on [1]. The
”Studierstube” project use the Ascension’s Flock of Birds magnetic tracking system which is shown in Figure 2. It has
a high quality user-interface which is called Personal Interaction Panel (PIP). PIP is a two-handed interface to control
Studierstube [16]. It consists of a small panel (about 20x20
cm) and a pen which can be used as ”3D mouse”. Both
of them are separately tracked by the tracker. The Studierstube serves as a very general framework for augmented reality systems. The framework can be easily used to embed
computer-generated images into the real work environment.
But the system can only render simple 3D graphic object
without optimizing the rendering algorithm to improve the
efﬁciency. There is no example of volume visualization application with this system.
Another class of approach into building an AR system to
register the real and virtual worlds is the pure vision-based
method. These methods are referred to rely on the video sequences to track a camera, estimate the pose of the camera
attached the user viewpoint. A good example is ”The MagicBook”. The ”MagicBook” project is an early attempt to
explore how people can use a physical object to smoothly
transport users between reality and virtuality. The ”MagicBook” [5] uses ARToolKit, a vision-based library for building (AR) applications. These are applications that involve
the overlay of virtual imagery on the real world. For example, a three-dimensional virtual character appears standing
on a real card. It can be seen by the user in the head set
display they are wearing. When the user moves the card,
the virtual character moves with it and appears attached to
the real object. Sinclair [17] creates an AR prototype for
presenting information about aircraft ((see Figure 3). Aircraft generally have many features that can be described and
there are many sources of background information. There

are also many types of complex information that can be
viewed in the domain of aircraft: for example military history, mechanical details and so on. Their prototype has been
developed using the ARToolKit. In their prototype application, information is presented by using labels around the
airplane, with leader lines being drawn between each label
and respective feature .

3. Method
Our system uses video information exclusively which
makes it well suited for AR visualization. We focused on
the AR system that adopts the combination of vision sensor and video see-through head mounted display(HMD).
All the processes from the acquisition of the user viewpoint
to displaying the composed image can been done in real
time. ARToolkit [5] is the basic platform that uses special
designed mark to track object. Since the video captured by
a camera is used both to estimate camera parameters and to
show the user the video image of the real environment, the
synchronization of the real and virtual environment could
be easily done.
The AR system mainly consists of three parts: video
capture device, display device and computation platform(Figure 4). The video acquisition and digitalization was performed on a PC (Pentium III 400MHz with
GeforceTM 4 graphic card). The video camera tested in
this prototype is a DCAM IEEE-1394 Firewire camera from
Videredesign Inc.
Since it is a digital interface camera, so no frame grabber
is necessary. The camera is capable to provide the full color
video image to a resolution of 640x480 pixels at 15 fps. The
camera is mounted on the opaque HMD from Deyang Inc.
The intrinsic parameters of the video camera that include
focal length, image center and distortion factor are calculated by the camera calibration in advance.
The video sequences are used to search for the landmark
and overlaid by the virtual volume structure. Augmented

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

Image

Firewire
Camera

Graphic card

Augmented Reality Image

Video Stream

Firewire Card

Camera

Head Mounted Display

PC Graphic Engine

Tracked Mark
Tracking Mark
Coordinate System

Figure 4. The framework of the prototype system.

MeasuredTransformation
Derived Transformation

images are output from PC to HMD in VGA signal format.

3.1. Camera Model and Calibration
To register the virtual object with real environment, it is
required to have a precise camera model. The video camera
has been modelled as a pin-hole model here. This model deﬁnes the basic projective imaging geometry with which the
object and landmarks are mapped onto the 2D image plane.
This model is simpliﬁcation of the optics of a real camera
and is used often in computer graphics and computer vision.
The detailed description of camera model can be found in
the literature [10].
To achieve precise orientation of virtual object, specially
designed landmarks are needed. The system automatically
locates and tracks landmark without the need for any
intervention. It calculates the correct spatial relation
between different coordinate based on the previous camera
parameters, which is important for AR system to overlay
the virtual objects on the landmarks. Overall, there are
ﬁve coordinate systems exist in our AR system(Figure 5).
These coordinate systems are camera coordinate system,
tracking mark coordinate system that coincide with world
coordinate system, texture coordinate system, and image
coordinate system.

3.2. Volume visualization with texture
Volume rendering is a technique for visualizing 3D arrays of sampled data. Examples of volumetric data can
range from computational ﬂuid dynamics, medical data (CT
or MRI scanners), seismic data, or other volumetric information where geometric surfaces are difﬁcult to generate or unavailable. Volume rendering provides a way to
see through the data, revealing complex 3D relationships.

Figure 5. The coordinate systems and their
transformation within the prototype.

There are several algorithms for visualization of volumetric
data. The texture mapping approach is a direct visualization technique, using 2D or 3D textured data slices, combined using a blending operator [3]. Although the 3D texture approach is simpler and yields superior results overall,
3D textures are currently still an EXT extension in OpenGL
and are not universally available like 2D textures. It is only
available as part of OpenGL 1.2. Since here our PC graphics card doesn’t support OpenGL 3D textures, we choose
to use 2D textures to build up the prototype system. The
approach is equivalent to ray casting [12] and produces the
same results. Unlike ray casting, where each image pixel is
built up ray by ray, this approach takes advantage of spatial
coherence.
The detailed steps for rendering a volume using 2D textures are following:
• Generate the data sets of 2D textures from the volume
data. Each set of 2D textures is oriented perpendicular
to one of volume’s major axes. It is done only once for
a particular data volume.
• Choose the number of slices to render, based on the
certain criteria. Usually this matches the texel dimensions of the volume data cube.
• Set up the desired viewpoint and view direction.
• Find the set of 2D textures which are most perpendicular to the direction of view. Generate data slice polygons parallel to the 2D texture set chosen. Use texture coordinate generation to texture each slice prop-

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

Figure 6. Augmented results from the different viewpoints. The object is on top of the
landmark

erly with respect to its corresponding 2D texture in the
texture set.
• Use the texture transform matrix to calculate the desired orientation of the textured images on the slices.
• Render each slice as a textured polygon, from back to
front. A blend operation is performed at each slice; the
type of blend depends on the desired effect.
• When the viewpoint and direction of view changes, recompute the data slice positions and update the texture transformation matrix if necessary. Orient the data
slices to the 2D texture set that is most closely aligned
with it.
There are sampling errors in the close-up views of the
volumetric data at texels that are far from the line of sight
into the data. To correct this problem, we try to use a series of concentric tessellated spheres centered around the
eye point, rather than a single ﬂat polygon, to generate each
textured ”slice” of the data. As with ﬂat slices, the spherical shells should be clipped to the data volume, and each
textured shell blended from back to front [14].

4. Results
The virtual object has been rendered by 2D texture
mapping. The data set are from CT scan of a skull. The 3D
object is overlaid on the landmark in the video image. The
enhanced image effectively provides more realistic feeling
by displaying those portions of model on the landmarks.
Different viewpoint can be achieved by moving the camera
or landmark in real time (Figure 6). The object stays in the
same location. The accuracy of the system is dependent on
the accuracy obtained during the camera calibration.

Figure 7. The comparison of 3D (left) and 2D
(right) texture mapping.

5. Discussion
The important issues that determine the success of an
AR visualization system are accurate tracking and efﬁcient
rendering if the system aims for more realistic application.
Different tracking methods have their own advantages, but
none of them can provide perfect solution. Hybrid tracking
seems to be a better alternative at current stage. Also if
AR system can only render simple structure, it will not be
very useful in certain application ﬁelds, such as medical
application, since they usually have a large amount of data
and need faster and efﬁcient visualization.

5.1. Vision based tracking
We demonstrated the registration accuracy of the method
of ARToolkit in our experimental system. But as other
normal vision-based tracking method, its ability of tracking
and recognizing marks is much inﬂuenced by the lighting
condition and the image quality of video sequences. Under
poor or sharp lighting conditions, the vision-based tracker
cannot precisely recognize the landmarks. Also if the
camera is out of focus because of the movement of a
user, the tracking will fail. Sometimes, specially harsh
or changing lighting conditions noticeable diminish the
tracker performance so that it makes the virtual object ﬂash
and jump in the video images. So in the future, we aim
to improve the accuracy of tracking to keep the system
more stable. Lots of vision-based methods use iterative
minimization techniques to reduce the cost of computation

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

that rely on frame to frame coherence. Linearization may
reduce the problem to a single global solution but require
the vision-based tracker to extract a relatively large amount
of information from features or landmarks[4].

in that it uses AR for more complex and real 3D visualization application. Improving the efﬁciency of volume rendering will be our future work. We also plan to study better
tracking method to solve the existing problems with ARToolKit.

5.2. 2D texture Vs. 3D texture

7. Acknowledgements

The availability of fast hardware for rendering texturemapping polygons make it possible to choose texture mapping for volumetric data visualization. Volume rendering
with 2D textures is more complex and does not provide as
good results as 3D textures mapping does(Figure 7). However, 2D texture can be used on any OpenGL implementationwith any entry level graphic card[6]. The problem with
2D textures is that the data slice polygons can’t always be
perpendicular to the view direction. Three sets of 2D texture maps must be created, each set perpendicular to one of
the major axes of the data volume. These texture sets are
created from adjacent 2D slices of the original 3D volumetric data along a major axis. The data slice polygons must
be aligned with whichever set of 2D texture maps is most
parallel to it. The more edge-on the slices are to the eye, the
worse the data sampling is. In the extreme case of an edgeon slice, the textured values on the slices aren’t blended at
all. At each edge pixel, only one sample is visible, from
the line of texel values crossing the polygon slice. All the
other values are obscured. For the same reason, sampling
the texel data as spherical shells to avoid aliasing when doing close-ups of the volume data, isn’t practical with 2D
textures[11].
The 3D texture is used as a voxel cache, processing all
rays simultaneously, one 2D layer at a time. Since the entire 2D slice of the voxels are ”cast” at one time, the resulting algorithm is much faster with hardware-accelerated
texture than ray casting. Using 3D textures for volume rendering is the most desirable method. The slices can be oriented perpendicular to the viewer’s line of sight, and creating spherical slices for close-up views doesn’t lead to sampling errors[14]. Up to now, there are not many graphic
cards that support 3D texture on PC platform. Those who
can support 3D texture also use speciﬁc API from the manufacturer. It causes the compatibility problem to the system
and application.

6. Conclusions
AR is a promising technique that provides wide future
in different application to improve the 3D interaction between human and computer, between human and environment. We have presented an AR prototype system that combined vision-based tracking with volume rendering by hardware texture mapping. It is different from other AR system

I would like to thanks the development team of ARToolKit.

References
[1] T. Auer and A. Pinz. The integration of optical and magnetic
tracking for multi-user augmented reality. Computers and
Graphics, pages 805–808, 1999.
[2] R. Azuma. A survey of augmented reality. SIGGRAPH’95
Course Notes, 1995.
[3] M. Bailey and D. Clark. Encoding 3d surface information
in a texture vector. Journal of Graphics Tools, 2(3):29–35,
February 1997.
[4] M. Bajura, H. Fuchs, and R. Ohbuchi. Merging virtual objects with the real world: Seeing ultrasound imagery within
the patient. Proceedings of SIGGRAPH’92 In Computer
Graphics, 26(2):203–210, 1992.
[5] M. Billinghurst, H. Kato, and I. Poupyrev. The magicbook:
Moving seamlessly between reality and virtuality. IEEE
Computer Graphics and Applications, pages 2–4, May/June
2001.
[6] J. Bushnell and J. Mitchell. Advanced multitexture effects
with Direct3D and opengl. Game Developers Conference
Proceedings 99, pages 81–99, March 1999.
[7] T. P. Caudell. Introduction to augmented reality. SPIE Proceedings : Telemanipulator and Telepresence TechnologiesSome Fine Journal, 2351, January 1994.
[8] H. Fuchs, A. Mark, and et al. Augmented reality visualization for laparoscopic surgery. The Proceedings of
First International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 11–13, October 1998.
[9] W. E. L. Grimson, T. Lozano-Perez, and et al. An automated
registration methos for frameless stereotaxy, image guided
surgery, and enhanced reality visulatization. Proceedings
IEEE Conference on Computer Vision and Pattern Recognition, pages 430–436, 1994.
[10] R. Hartley and A. Zisserman. Multiple view geometry in
computer vision. ambridge University Press, 2000.
[11] M. J. Kilgard. A simple opengl-based api for texture mapped
text. ttp://reality.sgi.com/opengl/tips/TexFont/TexFont.html,
1997.
[12] M. Levoy. Efﬁcient ray tracing of volume data. ACM Transactions on Graphics, 3(9):245–261, July 1990.
[13] W. Lorensen and H. Cline. Enhancing reality in the operating room. Proceedings of the 1993 Visualization Conference, pages 410–415, 1993.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

[14] T. McReynolds and D. Blythe. Advanced graphics programming techniques using opengl. SIGGRAPH ‘99 Course,
1999.
[15] P. Milgram and F. Kishino. A taxonomy of mixed reality visual displays. IEICE Transactions on Information Systems,
12(E77-D):1321–1329, January 1994.
[16] D. Schmalstieg, A. Fuhrmann, G. Hesina, and et al. The
studierstube augmented reality project. Presence: Teleoperators and Virtual Environments, 11(1):33–54, February
2002.
[17] P. A. S. Sinclair, K. Martinez, D. E. Millard, and M. J. Weal.
Augmented reality as an interface to adaptive hypermedia
systems. New Review of Hypermedia and Multimedia, Special Issue on Hypermedia beyond the Desktop, 9:117–136,
February 2003.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

