Relevance Vector Machine for Content-Based Retrieval of 3D Head Models
Pui Fong Yeung*, Hau San Wong*, Bo Ma*, and Horace H-S Ip*,+
Department of Computer Science, City University of Hong Kong, Hong Kong
+
Centre for Innovative Applications of Internet and Multimedia Technologies
(AIMtech) Centre, City University of Hong Kong, Hong Kong
cshswong@cityu.edu.hk

*

Abstract
In this paper, we propose a novel 3D head model
retrieval approach in which the queries are 2D face
views instead of less readily available 3D head models.
The basic idea is to characterize the corresponding
relations between 2D view feature and 3D model
feature based on a machine learning approach. Thus
the subsequent feature matching can be carried out in
3D feature space. As an effective solution to regression
problems, relevance vector machine is used in this
paper to establish an association between 2D and 3D
features. Experimental results show that our proposed
2D query based method is comparable with the direct
3D query based one.

1. Introduction
While the previous focus on content-based
multimedia retrieval is on the characterization and
representation of entities like images and videos for
query processing, more and more attention is being
directed to the possibility of developing a contentbased 3D model retrieval approach [1, 2], in which
queries in the form of 3D models are used to search for
similar models in a database. The successful
development of such an approach will have an
important implication in application areas such as
virtual world construction and game design, in which
3D models can be rapidly constructed based on a set of
pre-assembled components, instead of having to
generate each model anew. In view of this emerging
requirement, we propose a novel content-based
retrieval approach for 3D human head models, which
form an important sub-class of 3D models. In
particular, the most distinguishing characteristic of this
approach is that 2D face views, instead of 3D models,
are used as queries. This is in view of the difficulties of
obtaining 3D head models in practice, while 2D
portraits are amply available.

Specifically, we characterize the 2D face views by a
set of projection coefficients on the eigenface subspace
[3], while the 3D head models are described by the
projection coefficients associated with a suitable set of
3D basis functions. Although the set of 2D features do
not
encapsulate
adequate
information
for
reconstruction of a 3D model, we investigate the
possibility of establishing an approximate association
between these 2D features and the projection
coefficients of the 3D basis functions, such that the set
of estimated 3D basis coefficients based on the 2D
features could serve as an index to the models in the
database for the purpose of content-based retrieval.
Recently there has been more and more interest in
the adoption of Relevance Vector Machine (RVM) [4,
5] for regression problems in view of its good
generalization capability. In this paper, RVM is
adopted to establish an approximate association
between the 2D view feature and the 3D model feature.
In Section 2, we provide a brief description of RVM, in
particular, how the Bayesian framework is used for
learning in RVM. In Section 3, we describe our
proposed algorithm for 3D model retrieval based on a
2D view query. Finally, we present the experimental
results based on our proposed approach, and compare
its performance with that of neural network. The
results indicate that RVM is capable of improving the
retrieval performance to a significant extent when
compared with alternative approaches.

2. Relevance Vector Machine
In RVM, we adopt a set of input-target training
{x , t } N
pairs n n n =1 , and assume the targets are samples
from a model with additive noise which is mean-zero
2
Gaussian with variance σ . Thus the prediction of a
generalized linear model based on x is given by

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

y ( x) =

N

¦ w K (x , x
n

n =1

n

) + w0

(1)

where {w0 , w1 ,..., w N } are model weights and K (x, x n )
is a kernel function. In this way, the likelihood of the
training set can be computed from the Bayes’ Rule:
N
−
1
2
p(t| w , σ 2 ) = (2πσ 2 ) 2 exp(− 2 t - ǽw )
(2)
2σ
where t = (t1 , t 2 ,..., t N ) T , w = ( w0 , w1 ,..., w N ) T and ǽ

is a N × (N + 1) matrix containing the response of all
basis functions to the inputs, i.e. Ζ ij = K ( xi , x j −1 ) and
Ζ i1 = 1 .
With this likelihood distribution, the posterior
distribution over the weights can be calculated as
follows:
p(w| t , Į , σ 2 )

=

p(t| w ,σ 2 ) p(w | Į )
p(t| Į , σ 2 )

= ( 2 π)

−

N +1
2

−

| Γ|

1
2

1
exp(− (w − µ) T Γ −1 (w − µ))
2

(3)
where
Į : a vector of N + 1 hyperparameters. Each
hyperparameter is associated with an
individual weight.
Γ : the posterior covariance matrix, where
1
Γ= T
(4)
Ζ BΖ + A
µ : the mean, where

µ = ΓΖ T B t
A = diag( α 0 , α 1 ,..., α N )

α inew =

(6)

Differentiating and rearranging Eq. (6), we can
obtain the following equation based on the proposed
approach in [6]:
γ
α inew = i2
(7)
µi
where µ i is the posterior mean weight from Eq. (5)
and γ i is defined by:
(8)
γ i = 1 − α i Γii .
th
Γii is the i diagonal element of the posterior weight
covariance matrix Γ based on the current Į and σ 2
values. It can be interpreted as a measure of how ‘welldetermined’ each weight is by the data. Alternatively,
we can consider the weights as hidden variables and

1
Γii + µi2

(9)

In general, Eq. (7) can result in faster convergence
in the learning process so it is usually preferred.
2
Similarly, the noise variance σ can be re-estimated
by
2
t − Ζµ
(σ 2 ) new =
N − ¦ γi
i

(10)
As a whole, the learning process is basically an
iterative application of Eq. (7) (or (9)) and (10), as well
as the accompanying update of the posterior statistics
Γ and µ from Eq. (4) and (5) until some prescribed
convergence criteria have been satisfied. With the
2
estimated Į and σ values which are the most
probable values of the generalized model, where we
σ2
denote as Į MP and MP , we can thus compute the
most probable weights. Hence, predictions can be
made based on the posterior distribution over the
weights, and the uncertainty about the predictions can
be reflected by the posterior distribution in Eq. (3). For
a new input x ∗ , the predictions, t ∗ , can be calculated
from
p(t ∗ | t , Į MP , σ 2MP )

(5)

and
In
addition,
B = σ −2 I N
.
The marginal likelihood or evidence for the
hyperparameters can be obtained by integrating out the
weights as follows:
p(t| Į , σ 2 ) = ³ p (t| w , σ 2 ) p(t| Į ) dw

apply the Expectation-Maximization approach to
obtain:

= ³ p (t ∗ | w , σ 2MP ) p(w| t , Į MP , σ 2MP ) dw .
(11)

During the optimization process, many of the α i
p( wi | t , Į , σ 2 )
are driven to infinity, so that
becomes
infinitely peaked at zero and thus forcing the
corresponding model weights wi to be zero. This can
be easily observed from Eq. (3). In this way, the
corresponding kernel functions can be removed from
the training model represented by Eq. (1), and with the
resulting sparsity of the coefficients, the amount of
computation can be reduced.

3. Problem Description
Applying Principal Component Analysis (PCA), we
extract features from the 2D face views and their
corresponding 3D head models to form a training pair.
Specifically, we characterize a 2D face view by a set of
eigenfaces [3], s 1 (z ) ,s 2 (z ) ,...,s M (z ) , where z is the
position vector on the 2D image lattice. In this way,
each face view can be specified by a unique set of

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

u = [u1 ,!!, u M ]T
projection
coefficients
corresponding to the individual eigenfaces. On the
other hand, the head models are described by a set of
g (x) ,g 2 (x) ,...,g N (x)
, where x is the
basis functions, 1
position vector on the model surface. Similarly, a
particular head model can be represented by a set of
v = [v1 ,!!, v N ]T
expansion
coefficients
corresponding to the basis functions.
Afterward we have to determine an approximate
M
N
association h : R → R so that
v ≈ h(u)
(12)
is satisfied. This can be achieved by minimizing the
following error measure, Eh,
K

2

E h = ¦ v k − h( u k )
k =1

(13)
for a set of K face view and model pairs. Due to the
possible complexity of this 2D-3D association, and the
high dimensionality of the feature representations, we
implement the optimal mapping with RVM in order to
improve the generalization capability and the resulting
retrieval performance.
In the training process of the adaptive mapping
under the RVM scheme described in the previous
σ2
section, we first evaluate Į MP and MP based on the
training pairs by iterative application of Eq. (7) and
Input Query

(10) to find out the most probable weights w MP . Each
component of the output vector is evaluated based on
its own set of weights associated with an independent
RVM mapping.
In this way, given a new face view query, we
extract the corresponding set of 2D eigenface
projection coefficients.
These features are then
mapped to its associated set of 3D projection
coefficients through the RVM association. The
resulting 3D feature representation can be used as a
pseudo-query to retrieve the relevant set of 3D models
from the model database based on a nearest neighbor
matching process.

4. Experimental Results
A database containing 800 3D head models, which
are categorized into 10 classes and constructed based
on the MPI Face Database [7], is adopted for
evaluating the proposed RVM approach. Each model
consists of 6152 polygons with 6293 vertices.
This database is equally divided into two mutually
exclusive subsets, a training set and a query set. Both
subsets are constructed by extracting the eigenface
projection coefficients from the face views and its
corresponding 3D basis function expansion
coefficients from the head models.
From our experiments, it is observed that 7
eigenface projection coefficients and 4 3D basis
projection coefficients are adequate for satisfactory
characterization of the face views and head models.
Retrieved Models

Figure 1. First five retrieved models of a sample query

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

For both the 2D and 3D cases, the selected coefficients
can account for over 75% of the variation of the
original training set.
The coefficient training pairs are then used to
establish the 2D-3D association by the iterative
updating of the hyperparameters and the weights until
the changes are below a prescribed threshold. In our
experiment, we have adopted different types of kernel
functions, including Gaussian, Polynomial and
Laplacian, (Table 1) and our conclusion is that the
x − x'
k ( x , x' ) = exp(−
)
2ϕ
,
Laplacian kernel function,
can result in the best retrieval performance. As a result,
we choose the Laplacian kernel function in our
experiments. To determine a suitable value for the
width parameter ĳ in the function, we construct a
number of 2D-3D associations with different width
settings and then perform the 3D head model retrieval
on the query set.
Table 1. Retrieval performance of different kernels

Kernel Functions

Average Precision

Laplacian

0.9309

Polynomial

0.9274

'Thin-plate' Spline

0.9161

Homogeneous Polynomial

0.9145

Linear Spline

0.9107

Gaussian

0.9105

Cauchy

0.9101

Cube of Distance

0.9075

Figure. 2. Precision recall characterization of retrieval
performance with ĳ=1.5

From the figure, it can be observed that the
precision levels of RVM is much higher than that of
neural network for recall values below 0.9, while the
performance is similar for the remaining recall values.
The significant improvement in this recall value range
is important for the current application, since users will
typically focus on the initial entries of the retrieved
model list rather than going through the whole list. It
also indicates that RVM is capable of providing a
better generalization performance compared with
neural networks.
The width parameter is further adjusted to 2 and a
better result of RVM is obtained (Fig. 3). The
closeness between the curves of RVM and direct
retrieval in this figure also indicates that the
performance of the proposed approach can closely
approximate that of direct retrieval, which serves as an
upper bound for the retrieval performance.

Fig. 1 shows the first five retrieved models for two
input face views from query set. It can be observed that
each of the retrieved models bear a close resemblance
to the face view, implying that the proposed approach
is feasible and able to associate the 2D and 3D features
in two different eigenspaces.
To provide a quantitative description of the retrieval
performance, we plot the precision recall curves for
each of the following three cases: retrieval based on
RVM using a width parameter ĳ=1.5, retrieval based
on a feedforward multilayer neural network, and direct
retrieval, in which the 2D-3D association stage is
bypassed and the original 3D models are directly used
as queries. The results are shown in Fig. 2.
Figure. 3. Precision recall characterization of retrieval
performance with ĳ= 2

To investigate how the width parameter affects the
performance of the proposed retrieval approach, we
perform the retrieval with a number of different ĳ

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

settings. The effect of different width settings on the
retrieval performance is summarized by the curve in
Fig. 4. It shows that the average precision levels
increase with the value of ĳ, and does not show a
significant improvement when the width is greater than
1.5.

proposed 3D head model retrieval approach is able to
achieve a significant improvement in retrieval
performance, which is demonstrated by the higher
precision levels of RVM compared with neural
network, and the closeness of the RVM precisionrecall curve to that of the direct retrieval approach
which represents the upper bound of performance.

6. Acknowledgement
The work described in this paper was partially
supported by a grant from the Research Grants Council
of Hong Kong Special Administrative Region, China
[Project No. CityU 1197/03E] and a grant from City
University of Hong Kong [Project No. 7001596]. Part
of the dataset used in the experiments was provided by
the Max Planck Institute for Biological Cybernetics in
Tuebingen, Germany.

7. References
Figure. 4. Average precision corresponding to different
choices of the width parameter ĳ

5. Conclusions
In this paper, the Relevance Vector Machine
(RVM) approach is adopted for content-based retrieval
of 3D human head models based on a single 2D face
view. Specifically, we characterize the 2D face views
with a set of eigenface projection coefficients, and
describe their corresponding 3D head models by a set
of projection coefficients on a suitable 3D basis
function ensemble. With these example pairs, we
establish an approximate 2D-3D association based on
RVM, such that the set of estimated 3D basis
coefficients based on the 2D features could serve as an
index to the models in the database for the purpose of
content-based retrieval. Our experiments show that the

[1] Hau-San Wong, Kent K. T. Cheung, Yang Sha, Horace H
S Ip, “Indexing and Retrieval of 3D Models by Unsupervised
Clustering with Hierarchical SOM”, ICPR 2004, pp. 613616, August 2004
ˮ˅˰ʳ ˛̂̅˴˶˸ʳ ˛ʳ ˦ʳ ˜̃ʿʳ ˪˼˿˿˼˴̀ʳ ˬʳ ˙ʳ ˪̂́˺ʿʳ ˆ˗ʳ ˛˸˴˷ʳ ˠ̂˷˸˿̆ʳ
˥˸̇̅˼˸̉˴˿ʳ ˕˴̆˸˷ʳ ̂́ʳ ˛˼˸̅˴̅˶˻˼˶˴˿ʳ ˙˴˶˼˴˿ʳ ˥˸˺˼̂́ʳ ˦˼̀˼˿˴̅˼̇̌ʿʳ ˩˜ʳ
˅˃˃˅ʿʳ̃̃ˁʳˆ˄ˇˀˆ˄ˌʿʳˠ˴̌ʳ˅˃˃˅
[3] M. Turk and A. Pentland, "Eigenfaces for Recognition,"
J. Cognitive Neuroscience, vol. 3, pp. 71--86, 1994
[4] M. E. Tipping, “The Relevance Vector Machine”,
Advances in Neural Information Processing Systems 12,
S. A. Solla, T. K. Leen, and K.-R. Müller Eds., Cambridge,
MA: MIT Press 2000. pp. 652–658
[5] M. E. Tipping, “Sparse Bayesian Learning and The
Relevance Vector Machine”, Journal of Machine Learning
Research 1, 211–244.
[6] MPI Face Database: http://faces.kyb.tuebingen.mpg.de/

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

