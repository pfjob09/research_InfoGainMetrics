2012 16th International Conference on Information Visualisation

3D visualization of geophysical resistivity data to delineate
contamination anomalies in a landfill
Vítor Gonçalves1,2, Maria João Fontoura3, Paulo Dias2,4, Rui Moura,3, Beatriz Sousa Santos2,4
1
Polytechnic Institute of Castelo Branco, Portugal
2
Department of Electronics, Telecommunications and Informatics, University of Aveiro, Portugal
3
University of Porto, Faculty of Sciences, Portugal
4
IEETA, University of Aveiro, Portugal
{vitorgoncalves@ipcb.pt, mjfontoura@fc.up.pt, paulo.dias@ua.pt, rmmoura@fc.up.pt, bss@ua.pt}
defining a model of the geological area that helps, among
others, bedrock detection, drilling wells for water,
mining, monitoring environmental problems, as well as
the detection of the degree of change in rock masses.
Due to economical and physical reasons (topology of the
site, presence of a river, vegetation, etc.), only a limited
number of sections are available at the location resulting
in a very sparse data set.
This work is part of a case study that uses data
acquired in a landfill on the surroundings of Porto
(Portugal) with potential infiltration problems. Given the
specificity of the location, drilling are impossible, and
geophysical specialist are mainly interest in
understanding the influence of the contamination of the
surrounding area. The main objective of the experts is to
find anomalous volumes in subsoil (corresponding to
contamination of the subsoil by the landfill) and visually
explore in the same package both, the anomaly and data
in order to obtain a better understanding of the
phenomenon. The suitability of the method is validated
comparing the visual results with chemical analysis of
subsoil in specific locations.
Our main contributions are the combination in the
same software package of the analysis and visualization
of the data and anomaly 3D as well as the use of
statistical methods to detect anomalous values. In this
process we used statistical methods taking into account
the whole data set, subdividing it in sublevels in relation
to the surface, instead of using a single threshold (as
usual). To the best of our knowledge no other software
package offers both features simultaneously.
Beside the 3D representation of the phenomenon,
techniques are used to represent uncertainty and avoid
erroneous conclusions. Given the sparse nature of
geophysical data, interpolation must be used with caution
to allow users to easily understand where real data is
available or not [1]. Our goal regarding uncertainty
visualization techniques is to provide informative,
intuitive, non-distracting, and interactive information [2].

Abstract
Geophysical data represent subsoil structure in a
specific area and can be used to extract subsoil
information for various purposes. In this work we used
this data type to detect anomalies/contamination in the
subsoil. Our case study was based on data acquired
around a landfill and the main objective is identifying
contaminated areas as a result of leakage in landfill.
This involves the application of statistical methods to
detect anomalous values taking into account the whole
data set, subdividing it in sublevels in relation to the
surface, instead of using a single threshold (as usual).
This work combines in the same software package the
anomaly statistical analysis and several 3D
representations of the results to validate and also helps
understanding the final results of the analysis. Given that
the original data used in the analysis, resistivity sections,
is normally very sparse, a kriging geostatistical process
was used to interpolate data in order to provide a
volumetric representation of the subsoil in the area,
providing a continuous spatial visualization.
Keywords: Volumetric visualization, geophysical
data, statistical anomaly detection, interpolation,
uncertainty representation.

1. Introduction
Geophysical data represent specific subsoil
properties and is acquired in a non-intrusive way
(without subsoil perforation). The advantage of using
this kind of methods is the coverage of large areas at a
reasonable cost and without the destruction of potential
important subsoil structures, unlike other conventional
methods such as excavation or drilling. In this work a
particular type of geophysical data, the electrical
resistivity, was used. Analysis of these data permits

1550-6037/12 $26.00 © 2012 IEEE
DOI 10.1109/IV.2012.38

170

Section 2 presents the geophysical data used in the
study as well as the interpolation method and the
uncertainty associated to interpolation. In section 3 the
anomaly calculation methods are described. Section 4
presents the representations developed to visualize data
and calculated anomaly, and section 5 presents a
preliminary evaluation of the visualizations by domain
experts. Finally, section 6 addresses various issues that
came up during the work, conclusions and ideas for
future work.

Figure 1 shows the acquisition process of ER data,
and Figure 2 is a typical representation of 2D grid
sections in 3D space. As it can be seen, data are quite
sparse.

2.1. ER interpolation
As discussed in previous work [5] the visualization
of geophysical data benefits from a continuous
visualization without gaps, facilitating the exploration,
analysis and documentation, even if the visualization
process becomes more complex.
Interpolation based on statistical distribution
methods generally produces the most realistic results for
geophysical data and subsoil data in general. Other
simpler methods can be used; however, experts usually
use them only when no statistical methods are available.
For example, linear interpolation, trilinear interpolation,
amongst others, are used occasionally to rapidly obtain
an overview (methods based on statistical processes tend
to be much more time consuming), or when data are not
sparse. Otherwise these methods are discarded since
subsoil properties usually do not follow simpler linear
distribution.
In this work we use a statistical interpolation called
ordinary kriging to create a regular anisotropic grid. This
interpolation method was selected since it generally
produces the best results (i.e., smaller differences
between interpolated and real values). Also this method
is generally the first choice of experts. However it is not
always available in commercial applications, where the
most common method is the IDW (Inverse Distance
Weighting). Zimmerman, D. et al. made various
comparative tests between ordinary kriging, universal
kriging and two types of Inverse Distance Weighting in
[6] and have proved that kriging almost always produces
the best results (20-30% better than Inverse Distance
Weighting). Also, ordinary kriging produces slightly
better results comparatively to universal kriging.
The ordinary kriging implementation is the one
available in EasyKrig3.0 used to generates a volume data
based on the original sections [7]. The process we have
used involves the exportation of 3D sparse data to
EasyKrig3.0 and the importation of the resulting
interpolated volume to our application.

2. Geophysical Data – Electrical Resistivity
Electrical Resistivity (ER) is a type of data that
indicates the level of electrical resistivity in the subsoil
[3]. Figure 1 shows a typical acquisition scenario. In the
geophysical field ER is measured in a grid of points
corresponding to a perpendicular plane to the
topographic surface. For each grid point a resistivity
value is measured. For a particular and restricted area it
is typical to acquire several of these grids (Figure 2), also
called electrical grid sections, or just electrical sections.
ER values are continuous scalar values that vary
according to the subsoil material. For example, soil and
rocks are normally bad conductors (although they can
have different conductivity values depending on
moisture), increasing water content makes the soil a good
conductor; ER is measured in Ω·m (ohm · meters),
ranging from values close to zero for good conductors to
very high values for bad conductors [4].

Figure 1 – Acquisition of electrical resistivity
data.

2.2. Uncertainty associated to interpolation
A common error in visualization of interpolated data
is the representation of acquired data and interpolated
data in the same way, without any care in visually
showing the uncertainty associated with interpolated data
Most visualization used in commercial software
packages represent acquired and interpolated data in the
same way.
This may lead to errors and
misunderstanding in the evaluation of the resulting
visualization [1]. As mentioned in a previous work [5]
and according to [8, 9], uncertainty representation in

Figure 2 – Representation of electric resistivity.

171

volumetric data is a 4D problem that can be typically
coded in two ways: mapping uncertainty by an additional
piece of data or integrating it in volumetric data (through
color, transparency, etc.). However, in this work another
form was chosen: the use of interaction. We decided to
make uncertainty visualization interactive so that user
can control the degree of interpolated data he wants to
analyze, according to uncertainty. Comparatively to [8,
9] proposals’, our strategy has the advantage of avoiding
data distortion or data overlapping and transmits likewise
the uncertainty associated to interpolated data.
For each volume voxel uncertainty was computed
using the following:

normally distributed data is background and what is
removed is anomaly [12].
Adapting this methodology to the geophysical data
obtained from resistivity sections, we can obtain a
statistically defined limit for what should be the
anomalous contaminant plume as opposed to defining an
arbitrary resistivity value. The reason for this is that low
resistivity values are sometimes also associated with a
normal uncontaminated setting.
On the other hand, statistical methods can be more
reliable to define the limit between background and
anomaly. However most statistical tests are only helpful
in eliminating so-called outliers.
Model-based objective methods, as the iterative 2-σ
technique and the calculated distribution function differ
from subjective methods, as thresholds are defined by the
data rather than by an arbitrary decision of the researcher
[13].
The main advantage of these techniques over other
methods is that they calculate the normal range of
background values (true value of background is within
mean±2σ range) with 95% confidence.
The importance of subdividing the whole sample
according to depth levels is related to the fact the natural
earth model reveals some dependence on the degree of
weathering in depth. Rock weathering is the
decomposition of some minerals within fresh hard rock
into softer minerals such as clay minerals. This in turn
leads to an increase in porosity and permeability. The
weathering process is gradual being most extensive on
the surface and generally decreasing with depth. Thus
this vertically non-homogenous medium causes a
resistivity dependence that is not related totally with the
contaminated groundwater itself and we should thus
separate a few levels according to a visual
previsualisation of the 2D sections. By making a prior
observation we can assess roughly how many levels exist
and these in turn will constitute single populations that
will be analysed for anomalous and background levels.
Actual tools, such as RockWorks1 (one of the best
tools in the field) do not have the option to define what is
anomalous and what is background based on statistical
methods, so the user has to define an arbitrary value
representing the threshold between anomaly and
background which could lead to interpretation errors in
global site assessment.
Different statistical methods could be used, but
considering the necessity to find a simple and robust
statistical test, Iterative 2σ-technique and Calculated
distribution function both described by [11] were chosen.
These methods were tested in different data sets and
present rather realistic data for the background [11].

,
and

Where max_dist is the maximum Euclidian distance
registered between any voxel coincident with a sample,
and every other voxel. Exponential_Factor is a factor
used to increase resolution for values close to zero,
representative of more certain data, which is important
since uncertainty increases exponentially with distance.
This formula was obtained empirically since it produced
an interesting result. distance is the shortest distance
between a voxel and any voxel corresponding to
acquired data (sample). Even though simple, the method
provides a reasonable first approximation to display the
uncertainty associated with each voxel.

3. Anomaly calculation
Anomaly is an unusual concentration of certain
feature in a given environment. Distinguishing between
anomalous values and background values for a given
location or sample is an important task for site
assessment and proper interpretation of results obtained
by different methodologies. Therefore, the concepts of
anomaly and background are inextricably linked.
For example, in geochemical studies is important to
determine the geochemical background, in order to
detect, in a given set of data, which values may be
considered anomalous.
Statistical techniques are good tools to make that
kind of distinction. In statistical terms a variable has a
lognormal distribution if its logarithm is normally
distributed. Assuming that the concentrations of elements
have a lognormal distribution, Lepeltier described a
statistical method for the determination of geochemical
background and anomalies [10]. When plotting the
cumulative frequency versus element concentration (in
bi-logarithmic scale) a deviation from a lognormal
distribution can be perceived as an inflection at the top of
the curve, representing an anomaly [10, 11]. This
inflection point is numerically represented by the value:
mean + 2 standard deviation (σ), of the population. Using
the iterative 2σ-technique described by [11] all values
beyond the mean + 2σ are removed, until a normal
distribution is obtained. Therefore, what remains in the

4. Visualizations
This section presents the visualizations developed to
allow the exploration and analysis of data and estimated
1

172

http://www.rockware.com

anomalies. To represent these data we have used
polygonal meshes to visualize ER sections as well as
terrain surfaces, and volume render to represent ER data
and the anomaly in a continuous way. A rainbow color
scale was adopted since it is familiar to experts and
provides an intuitive interpretation for them: blue tones
characterize areas where there is a greater possibility of
having water, yellow/brown/red tones correspond to
rocky areas, and greens to intermediate zones.
After some tests we chose the black color to
represent anomalous values in ER data. This color does
not collide with the color scale used and is easy to
perceive.

Figure 5 – 2D sections without anomaly in 3D
space

4.1. Visualization of 2D sections
We use 2D visualization since it has the advantage
of allowing the analysis of individual electrical sections
in more detail and without the need of manipulation in
3D space to get a frontal view, which can be confusing
(additionally, horizontal flip is provided). This is also
the typical view experts are used to work with in most
systems.

Figure 6 – 2D sections with anomaly in 3D
space
Figure 5 shows several resistivity sections in 3D
space without anomaly representation and Figure 6
shows several resistivity sections in 3D space with
anomaly representation using black to represent it. The
anomaly was calculated using the iterative 2σ method
with 20 depth intervals.

Figure 3 – 2D resistivity section without
anomaly

4.3. Visualization of terrain surface
The representation of the surface of the area under
study was added as context to give the user the
possibility of relating data and anomaly with terrain
surface information and topography, which, according to
domain experts is important to relate data with terrain
slope.

Figure 4 – 2D resistivity section with anomaly
Figure 3 shows a resistivity section without any
anomaly, while Figure 4 shows a resistivity section with
anomaly (anomalies are represented in black). In this
case the anomaly was obtained using the calculated
distribution method with 20 depth intervals. In both cases
a topographic correction was applied. This operation is
necessary since acquired ER Sections are measured
relatively to the terrain surface (the surface corresponds
to zero depth).

Figure 7 – Isosurface representating a terrain
surface

4.2. Visualization of 2D sections in 3D space

Usually experts acquire an extensive collection of
superficial points (X, Y and Z coordinates) that represent
a model of the terrain surface (also called “topographic
model”). Figure 7 represents several resistivity sections
in 3D space with anomaly and a green isosurface
representing the topographic terrain surface which may
have different degrees of opacity. This isosurface was
obtained by triangulation from the terrain model. It is
also possible to apply a texture on this isosurface, when

3D electrical resistivity sections have the advantage
of showing data and anomaly values in real 3D positions,
making it possible to detect probable continuities and
tendencies. The 3D sections are positioned in 3D space
based on their respective end coordinates of the section
and their level positions are corrected according to a
digital topographic model. Further details are presented
in section 4.4.
173

available, easing the contextualization and making it
possible to relate data with terrain surface.

Figure 9-A shows the volume of contamination with
a low opacity, while Figure 9-B shows several
isosurfaces delimiting regions of anomaly.
Volumetric data and isosurface representations have
the advantage of showing data continuously, facilitating
the interpretation of the phenomena.

4.4. Volume visualization
As mentioned before a continuous representation of
data is desired by end users. However, the interpolation
is made based in a bounding box (parallelepiped volume)
depending on the minimum and maximum values for
coordinates X, Y and Z. This approach does not consider
the real topography of terrain surface and, thus, to solve
this problem, after the interpolation, an additional
operation is performed that consists in hiding values
(applying a null opacity) above terrain surface as well as
values of M meters below terrain surface (M is the
maximum depth of electrical sections). Moreover, each
time the anomaly is calculated with different parameters
(e.g. number of depth interval or changes on interval
values) a new volume is dynamically generated, this
takes place since the threshold value of the anomaly is
different in each depth interval (taking into account the
statistical analysis of the depth interval) and a direct
color mapping is not possible given the fact that
visualization depends of the statistical calculations
(furthermore, depth used in statistical anomaly
calculation is associated with the terrain surface and not
with the top of the volume). These steps result in a final
volume that is topographically correct, in other words,
voxels above the terrain surface are not shown (null
opacity). Finally, this volume is moved to the real 3D
location.
In this work volume ray casting was used as it
produces a good tradeoff between image quality and
rendering speed.

4.5. Interactive visualization of uncertainty
In this work each volume voxel was mapped trough
two scalar values, the resistivity and the anomaly.
Resistivity was mapped through color and uncertainty by
opacity. This allows the user to interactively discard data
with a certain degree of uncertainty; the opacity transfer
function is changed according to a desired value, as
shown in Figure 10. Uncertainty values below the
selected value are hided (totally transparent) and value
above are all shown in the same opaque level (selected
by user). This type of interactivity is only possible thanks
to the strategy of mapping two scalar values in each
voxel, avoiding the need of having to represent a new
data volume for each operation. As shown in Figure 10
the top volume presents values with an higher
uncertainty (more data values obtained by interpolation;
i.e. a larger spacing of samples) and the bottom volume
corresponds to less uncertainty (interpolated data are
nearer to acquired values).
A

C

B

A

D

Figure 10 – Interactively visualization of
uncertainty related with data interpolation.

B
Figure 8 – Volumetric representation

5. User Evaluation

Figure 8-A shows a volume of resistivity values
without anomaly representation and Figure 8-B shows a
volume with anomaly using black to represent anomaly.
The anomaly was calculated using the calculated
distribution method with 20 depth intervals.

Taking into account the specificity of this particular
case study (anomaly estimation by statistical methods
using ER data) we performed a preliminary evaluation
based on only three user experts. Given this scenario and
the current state of the work we realize that the best
practice consists in making a qualitative evaluation in
order to investigate strengths and weaknesses of the
developed package
We provide the experts used a formulary with 7
questions regarding the use of the package followed by
an open section where experts can express the difficulties
they had as well as suggesting improvements.
From this preliminary evaluation, the experts
indicated as additional values:
 Implementation of two statistical methods that
take into account the whole data set and

A
B
Figure 9 – Volume of anomalous values (A);
isosurfaces of anomalous values (B).

174

visualization tools in the same software package
(exclusive of our package);
 continuous visualization (using interpolation)
that usually is not supported to visualize
anomaly;
 Interactive visualization of uncertainty in
interpolated data, resulting from kriging
interpolation (of electrical resistivity);
Experts also refer two important benefits that reduce
significantly their work load:
 A faster achievement of the anomalous volumes.
For a case study of the same dimension experts
have reported that they need about 30 minutes
using this package to analyze anomalous
volumes and about 3.5 hours using their
traditional tools.
 Topographic correction of 2D and 3D sections
and volumetric data (more representative of
reality);
In the open section the most common problem that
experts have expressed was the difficulty in finding the
best group of depth ranges to apply. One of them wrote
“the possibility to calculate various statistical parameters
for each depth range would be important to better select
each range” other said “I suggest that the tool would
provide graphs or tables of all the statistical analysis in
order to be able to judge what the best option is”.
Other important suggestions were made, for
example the visualization of anomalous volumes
projected in the surface terrain or the insertion of tool for
edition (e.g. to delineate important structures).

terrain surface and providing informative tables and
graphics to support decisions based on statistics (e.g. find
the best depth rages).

Conclusions and Future Work

[6]

Acknowledgments
This work was partially funded by FEDER through
the Operational Program Competitiveness Factors COMPETE and by National Funds through FCT Foundation for Science and Technology in the context of
the project FCOMP-01-0124-FEDER-022682 (FCT
reference PEst-C/EEI/UI0127/2011)

References
[1]

[2]

[3]
[4]
[5]

From the preliminary user evaluation it appears that
our system is more intuitive and allows faster analysis of
data than those normally used by geophysics.
We included in the same software package the
possibility of analysis and visualization of data and
anomaly continuously. According to our preliminary
studies this strategy significantly easies the work of
experts giving them a tool that allows dynamic changes
on statistical parameters of anomaly estimation (e.g.
depth levels), as well as the possibility of changing the
anomaly estimation method and obtain immediately a
visualization result. Moreover, the continuous data and
anomaly representation using kriging is a powerful
technique for this data, reducing the amount of cognitive
work the expert has to do without visual support.
The use of statistical methods to detect anomalous
values taking into account the whole data set,
subdividing it in sublevels in relation to the surface,
instead of using a single threshold provides a more
precise estimation. The preliminary validation by experts
revealed a good fit of estimated anomaly and real values,
however additional analysis of underground samples are
required.
In the near future we plan to include edition tools
(e.g. to delineate an area), the mapping of anomaly in the

[7]

[8]

[9]
[10]
[11]
[12]
[13]

175

Streit, A., P. Binh, and R. Brown, A Spreadsheet
Approach to Facilitate Visualization of Uncertainty in
Information. Visualization and Computer Graphics, IEEE
Transactions on, 2008. 14(1): p. 61-72.
Grigoryan, G. and P. Rheingans, Probabilistic surfaces:
point based primitives to show surface uncertainty, in
Proceedings of the conference on Visualization '022002,
IEEE Computer Society: Boston, Massachusetts. p. 147154.
F.C., K.G.V.a.F., Electrical methods in geophysical
prospecting.1996: Pergamon Press Inc., Oxford.
Reynolds, J.M., An Introduction to
Applied and
Environmental Geophysics. 1997: Wiley, New York, pp:
796.
Gonçalves, V., et al., 3D visualization of sparse
geophysical data representing uncertainty. V IberoAmerican Symposium in Computer Graphics - SIACG
2011, 2011: p. 23-29.
Parrott, R.W., et al., Towards statistically optimal
interpolation for 3D medical imaging. Engineering in
Medicine and Biology Magazine, IEEE, 1993. 12(3): p.
49-59.
Chu, D. The GLOBEC Kriging Software Package –
EasyKrig3.0. 2004 [cited 2011 2011-01-16]; Available
from:
http://globec.whoi.edu/software/kriging/easy_krig/easy_k
rig.html.
Djurcilov, S., et al., Volume rendering data with
uncertainty information, in Data Visualization 2001, D.
Ebert, J.M. Favre, and R. Peikert, Editors. 2001,
Springer-Verlag Wien: Vienna.
Djurcilov, S., et al., Visualizing scalar volumetric data
with uncertainty. Computers & Graphics, 2002. 26(2): p.
239-248.
Lepeltier, C., A simplified treatment of geochemical data
by graphical representation. Economic Geology, v. 64,
1969: p. p. 538-550.
Matschullat, J., R. Ottenstein, and C. Reimann,
Geochemical background - can we calculate it?
Environmental Geology, v. 39, 2000: p. p. 990-1000.
Nakić, Z., K. Posavec, and A. Bačani, A Visual Basic
Spreadsheet Macro for Geochemical Background
Analysis. Ground Water, 2007. 45(5): p. 642-647.
A.J, S., A fundamental approach to threshold estimation
in exploration geochemistry: probability plots revisited.
Journal of Geochemical Exploration, 1991. 41(1–2): p. 122.

