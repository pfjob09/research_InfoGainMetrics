2012 16th International Conference on Information Visualisation

Hybrid Appearance Based Disease Recognition
of Human Brains
Leyla Zhuhadar, IEEE Member and Gopi Chand Nutakki, IEEE Member, University of Louisville,
Louisville, USA.

✦

Abstract—The magnetic resonance imaging (MRI) is a diagnostic and
treatment evaluation tool which is very widely used in various areas
of medicine. MRI images provide very high quality images of the brain
tissue and so can be used to study the brain conditions. This research
paper proposes a productive technique to classify brain MRI images.
Examining the MRI brain images manually is not only slow but is also
error prone. In order to both speed up the process and maintain the
quality of the classiﬁcation we need a very high-quality classiﬁcation
system. In this research work, advanced classiﬁcation techniques based
on the well known SIFT and Gabor features are applied on brain images.
From our analysis we observed that a hybrid feature derived with SIFT
and Gabor features yielded a higher accuracy than Gabor features
alone.

object identiﬁcation with low probability of mismatch
and are easy to match against a huge set of local
features. Object description by set of SIFT features is
also robust to partial occlusion. SIFT follows the steps
of, Scale-invariant feature detection, Feature matching
and indexing, Cluster identiﬁcation by Hough transform
voting [11], Model veriﬁcation by linear least squares.
In our research instead of using the SIFT interest points
we divided an image into blocks and accumulated the
resulting gradient magnitudes of each point into a bin
of length 8 per block.

Index Terms—Bioinformatics, SIFT, Gabor, Image Processing, PCA.

Gabor ﬁlters, is a linear ﬁlter used for edge detection.
The Gabor ﬁlters can be generated from one mother
wavelet by dilation and rotation. Frequency and orientation representations of Gabor ﬁlters are similar to those
of the human visual system, and they have been found
to be particularly appropriate for texture representation
and discrimination. In the spatial domain, a 2D Gabor
ﬁlter is a Gaussian kernel function modulated by a
sinusoidal plane wave. In our research we created Gabor
masks for various scale and orientation combinations.
We then convoluted the mask with the image blocks and
then populated the Gabor feature.

1

INTRODUCTION

Classiﬁcation of images can be performed by extracting
the image features and then by comparing the similarity
between the features. Two of the various existing features extracting techniques are SIFT and Gabor features.
In our research, extracted images features using these
two techniques and then obtained a hybrid feature by
concatenating the SIFT features at the end of the Gabor
features. The results were high in accuracy.
Scale Invariant Feature Transform (SIFT) performs
image recognition by extracting a local image feature
vector. The feature vector is invariant to scaling, translation, rotation, and partially invariant to changes in
illumination and afﬁne transformations of the images
and so is well suited for classifying the MRI brain
images. The calculation of the features is performed
under a multi-staged ﬁltering process that discovers
interest points in scale space. The SIFT features are local
and based on the appearance of the object at particular interest points.They are also robust to changes as
noise, and minor changes in viewpoint. In addition to
these properties, they are highly distinctive, relatively
easy and straight forward to extract, allow for correct

Appearance based stage recognition recently received
attention [4] because of its potential in accelerating the
pattern discovery of gene-gene interaction. There are
two main strategies for appearance based recognition
[2]: i) global appearance based, and ii) local appearance
based. The former strategy assumes that a human brain
image is standardized with the same orientation and
same scales and an image is represented as a single
feature vector used as the similarity measure of images.
The latter strategy does not have this assumption, but
it requests an interest point detector [2] to localize a
set of distinct local features that usually correlate with
certain geometric/structural information. An important
observation of human brain images is that their appearances can be dominated by textural information,
which challenges the local appearance based strategy.
The recognition methods proposed in [4] is under the
strategy of global appearance.

• Gopi Chand Nutakki is a prospective student at the Department of CECS,
University of Louisville.
• Dr. Leyla Zhuhadar is an Adjunct Assistant Professor at University of
Louisville.

1550-6037/12 $26.00 © 2012 IEEE
DOI 10.1109/IV.2012.99

588

2

IMAGE STANDARDIZATION

Image standardization is an important step in order
to provide a reliable and dimensionally synchronized
image database [1] for the classiﬁcation of the human
brain images. The human brain images are captured
using different scientiﬁc devices which can exaggerate
the effect of illumination variation, in our research we
used the human brain MRIs. We also need to deal
with issues such as noise, occlusion, and inconsistent
orientation in the human brain images.
2.1

(a) Human brain image
with a disease.

Human Brain Image Extraction

With very few exceptions, the brain image and the back
ground have signiﬁcantly different local texture properties. Brain images have a rougher texture with high
local variance, while the background which is darker
has smooth tonal variations, which means pixels with
low local variance. The variance of pixel intensity in
a window of a given size (say 3 × 3), centred at each
pixel of the image is calculated and the pixel is set as
foreground if the value is above a ﬁxed threshold value.
It is quite common to have brain-pixels assigned as
background, mainly at the center region of them. Thus,
after obtaining the binary image, a morphological binary
operator is applied to “ﬁll the holes” inside the brain
image region. In our research we would have manually
isolated the brain by extracting the brain image using
graphical tools and recreating another image containing
only brain. We used the Whole Brain Atlas of the Harvard Medical School which were standardized but also
contained images which do not provide enough detail
about the diseases.
2.2

(b) Human brain image
with another disease.

(c) Human brain image
with yet another disease.

Isolating Human Brain

Figure 1. Human brain images taken from individuals
containing different diseases. The images are standardized.

A human brain image set taken using an MRI scanning
device usually contains a few dozen brain images, with
various interesting brain parts spread across the image.
The brain in an image generally contains the human
skull and the neck bone occluding it. To extract the
human brain, we can use the well known, “watershed
transform” to partition the foreground region of the binary image. However, to the noisy borders and concave
shapes of the human head, the watershed approach with
a bad initial state tends to “over-segment” the brain.
We can perform a shrink-expand processing of the foreground region, ﬁrst the region is continuously eroded
until we ﬁnd two separated regions. The two partitions
of the foreground region are then the initial state for the
watershed ﬂooding algorithm. The algorithm “grows”
back the regions, until they are tough again, creating a
watershed. For an image with more than enough data
in the form of the human skull, jaws and neckbone, the
“shrink-expand” algorithm is recursively applied over
the foreground region, keeping only the center-most
region at each recursion step until the “shrink-expand”
algorithm gives only one region.We can apply a straight

forward manual method to extract the main brain image,
we can use the MATLAB image processing tool to set the
boundaries of the brain alone and then extract the region
that is highlighted. Points are set along the boundary of
the brain forming an enclosed polygon which is close
to the shape of an ellipse. The MRI scanned images we
obtained from Harvard were pre standardized and were
used in many research projects, automatic preprocessing
for obtaining only a speciﬁc brain part of the MRI
scanned image is a future extension of our research.
2.3

Image registration

Brain images extracted could have different position, orientation, scale and shape in the original MRI scans. For
a better comparison between patterns in the extracted
brain images, we can perform an image registration step
to transform the images, so that the comparison can be

589

given block, the gradient magnitude and the orientation
of the pixel are calculated.
The gradient is the square-root of the squares of the
pixel value difference around a given block pixel. The
orientation is the arc-tan of the vertical and horizontal
pixel value differences. Once the gradient magnitude
and the orientation for a pixel is calculated, the gradient
magnitude of all the pixels is added to the corresponding
bin of the block to which the pixel belongs to. All the bins
from all the blocks are then concatenated to obtain the
complete image feature which has a length of 8 times
the number of blocks. The feature is extracted using
each pixel of the image which gives an impression that
most of the image detail is preserved in the feature. The
more the information preserved the more distinct is the
feature to the other image features. The feature is ﬁnally
normalized to convert the feature into a form of unit
vector which helps in obtaining the similarity between
the feature vectors. Once the feature vector for an image
is obtained, the vector dot product is applied with the
other image features.
The strategy used in the construction of SIFT [3],
concatenating multiple orientation histograms of multiple regions, compared with using a single orientation
histogram, has the following two advantages: i) it is
more robust to inaccurate localization, and ii) it tends
to preserve more information. But the second advantage
is somehow subtle, since using a larger number of subregions not only increases the dimension of the low-level
feature, but also involves more noisy information.

performed regardless their original position, orientation,
scale and shape. Since our research is speciﬁcally designed, ignoring the orientation of the brain and considering all the human brain images to be at a standard
scale, it is very crucial to have all the brain images
pre-normalized with respect to the major axis and the
top and bottom of the human head. Once the human
brain is highlighted in the original image, a new image
with required dimensions having a dark background is
created and the extracted brain image is placed in the
new image so that the brain is close to the center in the
new image with the major axis along the vertical. The
human head is straight and the neck section pointing
downwards. Figure 1(a) shows a human brain. The
image contains dark region surrounding the brain. The
dark region varies in size as the size of the MRI scanned
brain varies. This means the dimensions of the whole
picture remains constant while the brain inside the image
varies in size.

3

FEATURE EXTRACTION

3.1 Orientation histogram
Orientation histogram works very close to the way the
SIFT algorithm works. In order to preserve the maximum possible detail of the human brain, instead of
considering only the keypoints as SIFT, the orientation
histogram processes the entire image pixels and stores
the information in the histogram.
Orientation histogram is a low-level statistical representation of a local region. The motivation of using
statistical representation comes from the texture dominant appearance of the human brain images. One implementation of orientation histogram is based on gradient
vectors, as shown in Figure 2. The upper 4 × 4 window
is a zoom-in illustration of a sub-block in the lower
window. A rectangle in the upper window represents
a pixel, and the associated arrow denotes the gradient
vector of the pixel. Note that the direction of the gradient
vector represents the local orientation of the pixel, and
the length of the vector represents the magnitude of the
local variation of pixel values.
A way to construct an orientation histogram is to accumulate the gradient magnitudes in the same direction, as
illustrated in Figure 2 (the lower window), and Figure 3.
In the lower window of Figure 2, an arrow represents
the accumulation of gradient magnitude of the same
direction. Similar to [3], we discretize the angular space
from 0◦ to 360◦ by the step of 45◦ . Therefore, we have
8 bins, as shown in Figure 3, to accumulate the gradient
magnitudes. The algorithm proceeds with taking the
gradient and orientation of each pixel of the image and
accumulating the gradient magnitude to the respective
orientation bin. Each block will have 8 bins representing
the 8 angles which are populated with the gradient
magnitudes. Each image is divided in to 8 × 8 blocks
each block will give out 8 bins each. For each pixel in a

3.2

Gabor Filter Features

Gabor ﬁlter has signiﬁcant application in the signal
processing [5] and it is also proven in the recent past
that it can be applied in image processing for head pose
identiﬁcation [6], scene analysis [7], human identiﬁcation
[8], etc. It is also applied in bio-medical physics and
geophysics to better understand the signals [9].
Gabor ﬁlters have been used in many applications,
such as texture segmentation, target detection, fractal
dimension management, ﬁngerprint matching, edge detection, image coding and image reconstruction. Gabor
ﬁlter is a linear ﬁlter that is created by modulating a
sinusoid with a Gaussian. Figures: 4(a)- 4(d) show the
visualization of the Gabor function under a constant
variables except the orientation varying from −π to π.
g(x, y; λ, θ, φ, σ, γ) = e(−

(x 2 +γ 2 y 2 )
)
σ2

cos (2π

x
+ θ)
λ

where
x = x cos(θ) + y sin(θ)
y = −x sin(θ) + y cos(θ)
where the arguments x and y specify the position of a
light impulse in the visual ﬁeld and σ, γ, λ, θ and φ are
parameters as follows:

590

Figure 2. Demonstration of a block of 4 × 4 sub-blocks
used to compute an orientation histogram, where an
arrow indicates a gradient vector.

(a) Visualization at 0◦

(b) Visualization at 45◦

(c) Visualization at 90◦

(d) Visualization at 135◦

Figure 4. Gabor function visualization from 0◦ to 135◦ .

entation combination we create a mask and use the mask
to obtain a co-efﬁcient by the convolution of the mask
with the image block which is of the same size as the
mask. The mask is a matrix of size 8×8 and is populated
with a double value obtained from the product of the
Gaussian function and COS function for a given scale
and orientation value. The scale varies from 1 to any
positive integer. We considered only 1 scale, though it
is generally 3. The orientation varies from 0◦ to 136◦ by
steps of 45◦ which gives out 4 different orientations the
image is sampled with. The image is totally sampled the
number of scales times with the number of orientations.
Gabor ﬁlter effectively samples the image with different
scale and orientation combinations and does it for all
the given scales and orientations, thus obtaining a very
distinctive image feature for a given image.
Once the mask for a given set of scale and orientation
is populated the mask is slid over each block of the image resulting in a set of co-efﬁcients from the convolution
of the mask with the image block. All the coefﬁcients
resulting from the convolution for all the combinations
of scales and orientations are concatenated to obtain the
Gabor ﬁlter features. Since Gabor ﬁlter is very close to
the way the human vision system works, the image features obtained using this procedure is highly distinctive
for a given image and so the classiﬁcation results are
better than orientation histogram.
Figure 5 shows the setup of extracting the image
features using the Gabor mask. The image is divided
in to number of sub-blocks, each of size equal to the
Gabor masks which are of 8 × 8 pixel size. A coefﬁcient
is resulted when the image block is convoluted with
the Gabor mask, this coefﬁcient is used to populate the
Gabor image feature. The complete image is convoluted
for all the given Gabor mask each obtained from a speciﬁc combination of scale and orientation. The features
are populated with the coefﬁcient obtained from the

Figure 3. An orientation histogram is a 8-dimensional
vector, where each dimension represents a bin associated with a speciﬁc directional degree, from 0◦ to 270◦
with the step of 45◦ .
σ is the standard deviation of the Gaussian factor
and determines the (linear) size of its receptive ﬁeld.
• λ speciﬁes the wavelength of the cosine factor of the
Gabor ﬁlter.
• θ speciﬁes the orientation of the normal to the
parallel stripes of the Gabor ﬁlter.
• φ is the phase offset of the cosine factor and determines the symmetry of the Gabor ﬁlter.
• γ is called the spatial aspect ratio and speciﬁes the
ellipticity of the Gaussian factor.
Gabor Filter feature is also a low-level statistical representation of a local region and the motivation of using
statistical representation comes from the texture dominant appearance of images. Our implementation of the
Gabor ﬁlter features consists of constructing a Gabor
mask and sliding the mask over the image.
Figure 4(a) is the visualization of the Gabor function
for the orientation of 0◦ and the other parameters as constants. The Gabor function is obtained as a convolution
of the Gaussian and the COS functions. The resulting
Gabor function depicts the dominating, orientation and
areas of the image.
When constructing a Gabor ﬁlter features we consider
multiple scales and orientations. For each scale and ori•

591

Figure 5. Gabor features extraction from the image using
the Gabor Mask.

Figure 7. Principal Components, Z1 and Z2

Figure 8. Projection of the data on to the ﬁrst PC
Figure 6. Spacial Distribution of the Data

Speciﬁcally, given the data in m-dimensional space.
PCA describes the location and space of the mdimensional data cloud. There are 2 steps involved,
translation and rotation of the data cloud to and about
the origin respectively. Translation is done by mean
clustering the data. If the data is not mean-centered, then
the PC axes describes not only the shape of the data but
also the location. Rotation is done by aligning the ﬁrst
PC axis along with the longest axis through the data
set. A principal component can be deﬁned as a linear
combination of optimally-weighted observed variables.
Figure 6 shows the visualization of the spacial distribution of a sample data along X1 and X2 . Each point
represents a vector in the space. The data is distributed in
the form of an ellipse to better demonstrate the Principal
Components. The idea is to ﬁnd the axis passing through
the majority of the data, such axis is called the major axis
and we also deﬁne a minor axis perpendicular to the
axis, which covers lower amounts of the data. As the
major axis passes through the majority of data, the data
is projected on to the majority axis thus representing the
complete data. Figure 7 shows the visualization of the
majority and minority axes namely Z1 and Z2 . Figure 8
shows the visualization of the projects of the data points
on to the majority axis.

convolution of the image block with the Gabor mask.

4
4.1

DIMENSION REDUCTION
Principal Component Analysis

Principal component analysis is appropriate when you
have obtained measures on a number of observed variables and wish to develop a smaller number of artiﬁcial
variables (called principal components) that will account
for most of the variance in the observed variables. The
principal components may then be used as predictor
or criterion variables in subsequent analyses. Principal
component analysis is a variable reduction procedure.
It is useful when you have obtained data on a number
of variables (possibly a large number of variables), and
believe that there is some redundancy in those variables.
In this case, redundancy means that some of the variables are correlated with one another, possibly because
they are measuring the same construct. Because of this
redundancy, you believe that it should be possible to
reduce the observed variables into a smaller number of
principal components (artiﬁcial variables) that will account for most of the variance in the observed variables.

592

4.1.1

Covariance

Singular values are equal to the square root of the
eigenvalues. Since eigenvalues are automatically normalized in the data library, they do not easily provide information into the total amount of variance they explain.
However, you may calculate the total variance explained
by each EOF by squaring the singular values.
In the data library there is a time series associated with
each structure. These time series are the principal components. The ﬁrst time series is calculated by projecting
the data matrix onto the ﬁrst eigenvector of the variancecovariance matrix of the data, the second time series
by projecting onto the second eigenvector, and so on.
The time series values indicate the amount of the given
structure needed to complete the data ﬁeld. It follows
that the structure (dimensionless) multiplied by the time
series value at a single point in time (units of the data),
summed over all structures, yields the original data at
that point in time.
Using a superscript T to denote the transpose of a
vector or matrix, we say two vectors x, y are orthogonal
if
xT y = 0

Covariance is always measured between 2 dimensions. If
we calculate the covariance between one dimension and
itself, you get the variance. So, if we had a 3-dimensional
data set (x,y,z), measuring the covariance between and
x,y or y,z and z,x would ﬁnd the variance of the x,y,z
dimensions respectively. The formula for covariance is:
cov(X) =

n
i=1

¯
¯ T
(Xi − X)(X
i − X)
(n − 1)

The covariance is always measured between 2 dimensions. If we have a data set with more than 2 dimensions,
there is more than one covariance measurement that
can be calculated. A useful way to get all the possible
covariance values between all the different dimensions
is to calculate them all and put them in a matrix.
4.1.2 SVD of Covariance
Let xi , i = 1, . . . , n be a set of training data points,
where n be the number of data points. Matrix X =
[x1 , x2 , . . . , xn ] is the data matrix. Let x
¯ be the centroid of
the training data points. By subtracting each data point
by the centroid (i.e., the translation), we get the zeromean data matrix X given as:

In two or three dimensional space, this imply means that
the vectors are perpendicular. Let A be a square matrix
such that its columns are mutually orthogonal vectors of
length 1, i.e.
xT x = 1

X = [x1 − x
¯ , x2 − x
¯ , . . . , xn − x
¯]

The A is an orthogonal matrix and

Then we construct the covariance matrix

AT A = I

C = XX T

the identity matrix. For simpler notation, assuming that a
matrix A has at least as many rows as columns (M ≥ N ).
A singular value decomposition of an M × N matrix
A is any factorization of the form

where T denotes the transpose of a matrix. SVD is now
applied to the covariance matrix C to obtain the eigendecomposition of C. A projection matrix P consists of a
set of eigenvectors associated with largest eigenvalues.
With the projection matrix P , we can obtain the subspace
representation of a data point x in the original mdimensional space by P x.
A set of images are taken, and are converted to to a
linear vector. The average of the considered vectors is
computed and a different set of vectors are computed
which are equal to the difference between the average
vector and original image vectors. All the vectors are
again grouped as a single matrix. The SVD is then
applied on the matrix. By reshaping the resulting matrix rows which are vectors into the dimensions of the
original image we get the eigen brain images.
The variance of the nth principal component is the nth
eigenvalue. Therefore, the total variation exhibited by the
data is equal to the sum of all eigenvalues. In the data
library, eigenvalues are normalized such that the sum of
all eigenvalues equals 1. A normalized eigenvalue will
indicate the percentage of total variance explained by
its corresponding structure. Structures have also been
normalized so that the root mean square equals 1. This
way, the structures can be expressed in terms of standard
deviation.

A = U DV T
where U is an M × M orthogonal matrix, and D is an
M × N diagonal matrix with all the elements except in
the diagonal of the matrix are zeros.

5
5.1

EXPERIMENTS
Classiﬁer

K Nearest Neighbor [10] (KNN from now on) is one of
those algorithms that are very simple to understand but
works incredibly well in practice. Also it is surprisingly
versatile and its applications range from vision to proteins to computational geometry to graphs.
KNN is an non parametric lazy learning algorithm
[10], it means that it does not make any assumptions on
the underlying data distribution. This is pretty useful, as
in the real world, most of the practical data does not obey
the typical theoretical assumptions made (e.g. Gaussian
mixtures, linearly separable etc). Non parametric algorithms like KNN are of a great use here.
It is also a lazy algorithm, it does not use the training
data points to do any generalization. In other words,

593

there is no explicit training phase or it is very minimal.
This means the training phase is pretty fast. Lack of
generalization means that KNN keeps all the training
data. More exactly, all the training data is needed during
the testing phase. This is in contrast to other popular
techniques like Support Vector Machines where you can
discard all non support vectors without any problem.
Most of the lazy algorithms, especially KNN makes
decision based on the entire training data set (in the best
case a subset of them). The dichotomy is very obvious
here. There is a non existent or minimal training phase
but a costly testing phase. The cost is in terms of both
time and memory. More time might be needed as in the
worst case, all data points might take point in decision.
More memory is needed as we need to store all training
data.
KNN assumes that the data is in a feature space.
More exactly, the data points are in a metric space. The
data can be scalars or possibly even multidimensional
vectors. Since the points are in feature space, they have
a notion of distance. This need not necessarily be Euclidean distance although it is the one commonly used.
In our experiments we used the Euclidean distance. Each
of the training data consists of a set of vectors and
class label associated with each vector. In the simplest
case, it will be either + or - (for positive or negative
classes). But KNN, can work equally well with arbitrary
number of classes. We can also give a single number
"k". This number decides how many neighbours (where
neighbours is deﬁned based on the distance metric)
inﬂuence the classiﬁcation.
In this section, we test the performance of the proposed features for stage recognition. Our dataset contains over 1000 human brain images obtained from the
Harvard Whole Brain Atlas. The dataset is composed of
41 different cases of diseases related to the human brain.
Figure 5.1 shows an example of human brain images.
These images have been standardized with respect to
orientation and scale, and the dimension of the images
are 256×256. The classiﬁer we used is K nearest neighbor
(K-NN). We use 2-Fold cross validation to estimate the
recognition accuracy. We will compare the recognition
accuracy of RGB features, orientation histogram, the
subspace representation of orientation histogram and
the Gabor features. We will also study their recognition
efﬁciency.
The experiments are performed on the MATLAB platform using the Image Processing toolkit. To attain higher
performance the classiﬁcation algorithms were written in
C language and compiled under MEX compiler, which
enables using the MATLAB tools inside the C programs.

Figure 9. Visualization of a single individual’s brain, using
MRI scanner. Each image is a cross-sectional representation of the individual’s brain.
optimal accuracy was 88 under K-NN with K=1. The
orientation histograms are reduced using the PCA with
PCs varying from 2 to 20. Figure 11 shows the optimal
classiﬁcation accuracy for different N. The PCA run time
is considerably lower than the orientation histogram as
the length of the features are reduced. Though PCA
is reliable and a robust technique the dimension reduction of feature from 8 × 8 × 8 to less than 20 has
resulted in a fewer number of accurate matches when
compared to that of the original orientation histogram.
The classiﬁcation occur at reduced amount of the image
details, resulting in fewer number of accurate matches.
The number of training data, which is small, in the test
also effects the classiﬁcation accuracy.
Figure 10 shows the visualization of the orientation
histogram under different nearest neighbours and the
reduced feature accuracies. The images are reduced from
a length of 8 × 8 × 8 to a length of principle components
we choose which varies from 2 to 20. The accuracies
after the reduction are lower than the original unreduced
feature accuracies as the image detail is reduced, the
dimension of the reduced features are signiﬁcantly lower
than the original features. As we can the brain images
are surrounded by dark region which varies from one

5.2 Orientation Histogram
The orientation histogram builds the features in a very
close fashion to the SIFT features. Figure 11 shows that
the orientation histogram classiﬁcation accuracy was
very close for all the Nearest Neighbour values, the

594

Figure 10. The Orientation histograms reduced by PCA.
Accuracy for the number of Principle Components from 2
to 20 and Nearest Neighbors from 1 to 5.

Figure 12. Accuracy comparison of Orientation histograms, PCA optimal values when classiﬁcation is performed under 1 Nearest Neighbors. Accuracy for PCs = 2
to 20.
Table 1
Accuracy comparison of orientation histogram, PCA at
the optimal number of Principal Components for number
of neighbors from 1 to 5
K

Orientation histogram

1
2
3
4
5

88
83
83
82
79

PCA on Orientation histogram
20,(2
19,(2
23,(2
24,(2
24,(2

PCs)
PCs)
PCs)
PCs)
PCs)

Table 2
Time cost comparison of orientation histogram, PCA at
the optimal number of Principal Components for number
of neighbors from 1 to 5. Time costs measured in
milliseconds, including the time taken to generate image
features

Figure 11. The Orientation histograms for Nearest Neighbors from 1 to 5.
to another. If the image contains a lot of dark region,
its features comparatively carries less information and
when the features are reduced, the dark region might
dominate, which can result in a near NULL/zero feature.
Classifying these near NULL features is very difﬁcult.
Figure 11 shows the optimal reduced feature accuracies under K=1 nearest neighbors. Figure 12 shows the
visualization of the accuracies of the optimal reduced
features under different numbers of nearest neighbours.
Table 1 shows the readings of the accuracies of the
original features against the reduced features under different k values of the KNN algorithm. Table 2 shows
the time cost readings to ﬁnish the classiﬁcation using
the original features against the reduced features under
different k values of the KNN algorithm.
Table 1 shows the accuracy results for different N
values of the KNN. Table 2 shows the runtime costs for
the orientation histogram and the reduced orientation
histogram.
The Orientation histogram generally takes slightly
longer time to classify the images. We can safely ignore
this extra time as we need some considerable amount
of time in creating the reduced features. Orientation
histogram involves calculation of the Gradient magni-

K

Orientation histogram

1
2
3
4
5

1002
1134
1156
1242
1211

PCA of Orientation histogram
11245,(2
10264,(2
10426,(2
10918,(2
10290,(2

PCs)
PCs)
PCs)
PCs)
PCs)

tude and obtaining the orientation for each pixel and
then populating the histogram in the correct slot. Each
image sub block will populate its 8 bins, which are
part of the bigger complete image histogram. When the
orientation histogram is reduced from 512 to a length
varying between 2 and 20 using the PCA algorithm,
the time cost remains neutral as the feature lengths are
the same. The orientation histogram outperforms the
RGB features as the orientation histogram obtains the
features which distinguishes the image not only with the
appearance but also the orientation of the pixels. Since
there is more distinct information fed into the histogram,
the orientation histogram outperforms the RGB features

595

Figure 15. Accuracy comparison of Gabor Image features, PCA optimal values when classiﬁcation is performed under 5 Nearest Neighbors. Accuracy for PCs =
2 to 20.

Figure 13. The Gabor Features reduced by PCA. Accuracy for the number of Principle Components from 2 to 20
and Nearest Neighbors from 1 to 5.

Table 3
Accuracy comparison of Gabor, PCA of Gabor features
at the optimal number of Principal Components for
number of neighbors from 1 to 5
Gabor

1
2
3
4
5

83
79
81
77
75

PCA of Gabor features
16,(2
15,(2
16,(2
14,(2
12,(2

PCs)
PCs)
PCs)
PCs)
PCs)

Table 4
Time cost comparison of Gabor features, PCA at the
optimal number of Principal Components for number of
neighbors from 1 to 5. Time costs measured in
milliseconds, including the time taken to generate image
features

Figure 14. The Gabor Features for Nearest Neighbors
from 1 to 5.
which depend solely on the appearance.
5.3

K

Gabor Filter features

The Gabor features builds the features in a very different
fashion to the RGB and the orientation histogram, but in
a very effective manner. Figure 14 shows that the Gabor
feature classiﬁcation accuracy was very close for all the
Nearest Neighbor values, the optimal accuracy was 83
under K-NN with K=1. The Gabor features are reduced
using the PCA with PCs varying from 2 to 20. Figure 13
shows the classiﬁcation accuracy of the reduced features
for different N for K values from 1 to 5. The PCA run
time is considerably lower than the Gabor features as the
length of the features are reduced. Dimension reduction
is applied on the feature and they are reduced to a length
varying between 2 and 20 . This has resulted in a smaller
number of accurate matches when compared to that
of the original orientation histogram. The classiﬁcation
occur at reduced amount of the image details, resulting
in fewer accurate matches. The number of training data,
which is small, in the test also effects the classiﬁcation
accuracy.
Table 3 shows the details about the accuracy results

K

Gabor

1
2
3
4
5

60234
60244
61039
62372
61231

PCA of Gabor features
13245,(17
15324,(11
12456,(12
13958,(04
13290,(13

PCs)
PCs)
PCs)
PCs)
PCs)

for different N values of the KNN. Table 4 shows the
runtime costs for the Gabor features and the reduced
Gabor features.
Figure 15 shows the visualization of the Gabor features
under different nearest neighbours and the reduced feature accuracies. The images are reduced from a length
of 20 × 15 × scales × orientations × blocks to a length
of principle components we choose which varies from
2 to 20. The accuracies after the reduction are lower
than the original unreduced feature accuracies as the
image detail is reduced, the dimension of the reduced
features are drastically lower than the original features.
Figure 14 shows the optimal feature accuracies under

596

Table 5
Accuracy comparison of Hybrid, orientation histogram,
Gabor
K

Hybrid

Orientation histogram

Gabor

1
2
3
4
5

87
80
80
77
75

88
83
83
82
79

83
79
81
77
75

Table 6
Time cost comparison of Hybrid, Orientation histogram,
Gabor
K

Hybrid

Orientation histogram

Gabor

1
2
3
4
5

62212
62847
63077
62895
62857

1002
1134
1156
1242
1211

60234
60244
61039
62372
61231

Figure 16. Accuracy comparison of Hybrid, Orientation
histogram, Gabor
siﬁcation of the images. Considering the alignment, an
image not properly aligned produces an image feature
varying in values compared to that of an image properly
aligned. As the features are populated with respect to the
image blocks, any variation, effects the location of the
vector in the space thus giving an inappropriate result.
An image feature incorrectly normalized with respect to
head and neck will produce a feature which is a reverse
of what the actual correctly normalized image would
have produced.

different number of nearest neighbours. Figure 15 shows
the visualization of the accuracies of the reduced features
under different number of nearest neighbours.
Table 3 shows the readings of the accuracies of the
original features against the reduced features under different k values of the KNN algorithm. Table 4 shows
the time cost readings to ﬁnish the classiﬁcation using
the original features against the reduced features under
different k values of the KNN algorithm.

6

CONCLUSION AND FUTURE WORK

Using the orientation histogram features alone provides
a good rate of accuracy. The performance can further be
improved greatly by implementing the PCA dimensionality reduction technique on the orientation histogram
features and the Gabor features and combining them
to form a hybrid feature. Training the algorithm with
enough number of image samples may improve the
accuracy of classiﬁcation using the reduced orientation
histogram features. The human brain looks very similar
under different disease conditions, with variations in
only a part of the brain, so when the features are reduced, the distinctiveness is lost and the reduced feature
is dominated by the common regions, this can be the
reason for the poor performance of the PCA reduced
features. The subspace representation of orientation histogram features considerably reduces the computation
cost.
In the future, we plan to study stage classiﬁcation
under unconstrained localization (i.e., without the assumption of orientation and scale normalized human
brain images).

5.4 Hybrid features
The hybrid features are derived by using the orientation
histograms and the Gabor features. As we have seen
in the previous sections, each image is run through the
feature extraction techniques and a feature is obtained
representing the image. As a rule of thumb we know that
a well distinctive set of features will give the best classiﬁcation environment. The idea of creating the hybrid
features is to make the feature more distinctive. We used
the simple feature concatenation where each Gabor feature is concatenated with a orientation histogram feature
at the end. In our experiments the Gabor feature length
was 4906 and the orientation histogram length was 512.
By simply combining the two features the accuracy of the
classiﬁcation was 87 compared to 83 when only Gabor
features were used, here we need to understand that the
Hybrid feature is dominated by the Gabor feature. We
also used the PCA dimension reduction technique on the
hybrid feature.
Table 5 and Figure 16 show the comparison of all the
different features. Table 6 shows the computation costs
of all the different features.
The effects of brain images not being aligned correctly
and the normalization of the image with respect to the
up-rightness clearly plays a very major role in the clas-

R EFERENCES
[1]

597

S. Kumar, K. Jayaraman, S. Panchanathan, R. Gurunathan, A.
Marti-Subirana and S.J. Newfeld, BEST: A novel computational
approach for comparing gene expression patterns from early stages of
Drosophila melanogaster development. Genetics, Vol. 16(4), 2037-2047,
2002.

[2]

Q. Li, J. Ye, M. Li, and C. Kambhamettu, Adaptive appearance based
face recognition, IEEE International Conference on Tools with Artiﬁcial
Intelligence (ICTAI), pp 677-684, 2006.
[3] D. Lowe, Distinctive Image Features from Scale-Invariant Keypoints,
IJCV, Vol. 60(2), 91-110, 2004.
[4] Jieping Ye, Jianhui Chen, Qi Li, and Sudhir Kumar, Classiﬁcation
of Drosophila embryonic developmental stage range based on gene
expression pattern images, Computational Systems Bioinformatics
Conference (CSB2006), 2006.
[5] Rioul, O. and Vetterli, M. Wavelets and signal processing Signal
Processing Magazine, IEEE
[6] Shen, L.L. and Bai, L. and Fairhurst, M. Gabor wavelets and general
discriminant analysis for face identiﬁcation and veriﬁcation Image and
Vision Computing
[7] Itti, L. and Koch, C. and Niebur, E. A model of saliency-based
visual attention for rapid scene analysis Pattern Analysis and Machine
Intelligence, IEEE Transactions, 2002.
[8] Zhu, Y. and Tan, T. and Wang, Y. Biometric personal identiﬁcation
based on iris patterns Published by the IEEE Computer Society, 2000.
[9] Margrave, G.F. and Lamoureux, M.P. Gabor deconvolution CSEG
annual mtg., Expanded Abstract, 2002.
[10] Zhang, M.L. and Zhou, Z.H. ML-KNN: A lazy learning approach to
multi-label learning Elsevier, 2007
[11] Yamada, H. and Yamamoto, K. and Hosokawa, K. Directional
mathematical morphology and reformalized Hough transformation for
the analysis of topographic maps Published by IEEE Pattern Analysis
and Machine Intelligences Transactions, 1993.

598

