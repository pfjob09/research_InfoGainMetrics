2011 15th International Conference on Information Visualisation

Adaptive Visual Symbols for Personal Health Records
Heimo Müller1, Herman Maurer2, Robert Reihs1, Stefan Sauer1, Kurt Zatloukal1
(1) Medical University of Graz, (2) Graz University of Technology

Abstract

maintained by a patient, who controls the access to it
and it includes a lot of additional information about the
lifestyle and the subjective well-being of patients.
PHR exist a long time in paper format, e.g. “baby
books”, a personal calendar for women who track their
menstrual cycles, medication lists or notes about
medical directives. However WEB 2.0 technologies
empowered PHRs with the possibility to share medical
data between different computers and users, and to
interlink personal medical notes with information
sources on the Internet. Several commercial and nonprofit institution already provide PHR data
repositories. Microsoft’s Health Vault, Google Health,
and Dossia are the biggest players in this field.
A PHR typically holds information about the
(family) medical history, medications, allergies and
reactions, problem lists (diseases and conditions),
contact information from healthcare institutions or
immunizations and links to patient portals. PHRs can
even include advanced features as healthcare
providers’ exams, scanned images, such as CT scans or
dental images, drug interaction checks, therapeutic
modalities, occupational therapies, advance directive
forms, living wills, organ donor authorization,
appointments, scheduling, and financial information,
such as explanation of benefits. [2]
This comprehensive list shows, that the amount of
information within a PHR can be incredibly large,
which makes it difficult to (a) overlook the whole data
set, (b) access detail information in an easy and
effective way and (c) communicate the semantics of
data items to different user groups. All three challenges
are directly related to the development and evaluation
of user interfaces dealing with PHRs. Usability
engineers have today quite a number of methods to
evaluate the quality of an interface [3]. If such usability
tests give good results the design process is finished

As a hub of information controlled by the patient,
personal health records (PHR) collect information
from the patient medical history including a wide
variety of data sources as patient's observations, lab
results, clinical findings and in the future maybe even
personal genetic data and automatic recordings from
monitoring devices. This development will on the one
hand make health care more personalized and user
controlled but on the other hand also overloads
consumers with a huge amount of data. To address this
issue we developed a framework for adaptive visual
symbols (AVS). An AVS can adapt its appearance and
level of detail during the communication process.
Finally we demonstrate the AVS principle for the
visualization of personal health records.

1. Introduction
A Personal Health Record (PHR) is in line with the
following definition of the National Alliance for
Health Information Technology:
“An electronic record of health related
information on an individual that conforms to
nationally recognized interoperability standards
and that can be drawn from multiple sources
while being managed, shared, and controlled by
the individual. [1]
Traditionally electronic health records (EHR) are
produced by health care providers, maintained at
hospitals and doctors offices and are archived at some
central place. In contrast to this a PHR is owned and
1550-6037/11 $26.00 © 2011 IEEE
DOI 10.1109/IV.2011.87

220

moving images will be as ubiquitous as today’s mobile
phones. This will for the first time allow to implement
a novel concept of communication systems that
employs modern multimedia concepts (animations,
movies, interactive pictures, dynamic maps, etc.) in
combination with dynamic visual languages (i.e. visual
languages in which symbols can change in shape,
color, size, etc. in time).
Putting it differently, there is no reason why
information in the future should be recorded only using
static text, static images and such, but rather it is
conceivable to build on earlier attempts to construct
new artificial languages that are not based on letters,
but on icons. It was Otto Neurath [5] who showed with
his isotypes that in many aspects symbols are superior
to textual representations; a number of attempts to
construct full communication systems based on
symbols have been developed since. A brief survey of
the main approaches is given in [6]. However, even in
earlier papers such as [7], [8], [9], [10] and [11] the
idea to use dynamic symbols, rather than static
symbols has been discussed: the idea that symbols can
change size, colour, shape, contours or can move and
even change position to convey additional semantic
meaning is rather appealing: why not e.g. use a symbol
for ‘eye’ and to use the same symbol when slightly
moving to indicate the verb associated with the noun,
i.e. ‘seeing’. This and other techniques as explained in
above cited papers, techniques such as orthogonality,
macros and using picture dictionaries that explain the
same items in a variety of languages, changing the
level of abstraction where desirable, etc. are the
reasons why it does not seem far-fetched that written
language, communication and interfaces as we now
know them will be partially replaced by methods
involving dynamic visual languages.

and the users and the designer are happy, however
when this is not the case we have two choices :
(A) The user adapts its behaviour to the (not perfect)
user interface; or
(B) The user interface will be redesigned according to
the input from the usability tests, and we restart
the evaluation.
Usually alternative (A) is chosen, not due to the fact,
that the user interface designer and the usability
engineer are lazy guys, but because of the long delay in
the feedback cycle (B) and the orientation of the user
interface toward the lowest common denominator of all
users requirements, see Figure 1.

Figure 1 - Feedback cycles in user interface design
The solution to this problem seems to be obvious: just
let the user interface adapt its visual appearance and
codes to the needs of the user. This can be achieved by
capturing user input and adapt visual signs according
to the previous experience and knowledge of the user,
just in line with the statement of Heinz von Foerster
”the hearer and not the speaker determines the meaning
of an utterance” [4].

2.1 Adaptive Interfaces
A good overview about adaptive interfaces can be
found in [12] and [13]. Intelligent interfaces do not
exist in isolation, but rather improve their ability to
interact by constructing an user model based on the
interaction. This brings the problem of intelligent
interfaces close to the area of machine learning, where
the user plays the role of the environment in which the
learning occurs and the user model corresponds to the
learning knowledge base. In such a scenario the
interaction acts as performance task, on which learning
should lead to improvements [14]. Many applications
of adaptive interfaces focus on information filtering
and recommendation task, e.g. in content-based
filtering and collaborative filtering applications.
Intelligent interfaces can be divided into 3 classes [13]:

2. Related Work
2.1 Dynamic Visual Languages
The underlying assumption of Dynamic Visual
Languages is that by the beginning of the next decade
computers allowing the presentation of high resolution

221

An AVS consists of

(A) Adaptation within direct manipulation interfaces
by adding extra interface objects for predicted
future commands.
(B) Intermediary interfaces: The nature of interaction
is changed in order to act as an intermediary
between the user and the direct manipulation
interface.
(C) Agent interfaces: In this case the user retains full
control over the direct manipulation interface and
is advised by an autonomous agent.

Intended Denotation

Formal or natural language
description of its mission.

Coding

Representation of the
intended denotation.

Semantic Inspection

Method for the analysis of
the receiver reactions.

An AVS is modelled in an object oriented way, i.e. it
has an internal state (data) and autonomous behaviour
(methods), see Figure 3. In order to achieve the overall
goal - congruence between the intended denotation and
the constructed denotation – the semantic inspection
method adapts the presentation process (rendering,
level of detail, presentation speed, additional
explanations). The fundamental innovation of an AVS
lies in the distinction between the semantics of a
message and the used visual sign.

Our approach focus on a combination of user adaption
and intermediary interfaces introducing active
communication objects, which can adapt their semantic
depth (the level of detail, which is presented to the
user) according to needs of the user.
All intelligent interfaces have as central part a user
model [14] [15]. Extensions to the simple model of
stereotypical user models are programmable user
models [16], user models for demonstrational user
interfaces [17] and comprehension based user models
[18]. In a programmable users model the mental
representations and the user behavior results in a
cognitive model of the user. Using an Instruction
Language (IL) the user interface designer describes the
knowledge, which a user needs to perform a specific
task. The Instruction Language can be seen as
programming, which is translated to a run able
cognitive model. [19]

2.2 Visualization of PHR
Figure 3 – Adaptive Visual Symbols

Aigner and Miksch [20] give an overview of
visualisation methods for computerized protocols and
temporal patient data. Their work takes in account
graphical patient record summary by Powsner [21],
VIE-VISU by Horn 2001 [22], time lines and life lines
by Plaisant [23] and time tubes by Konchady [24].
Jiye An et al. present a level of detail (LOD)
information navigation model, which classifies
patient's whole EHR into different detail levels
according to their clinical relevance and they introduce
a novel navigation and visualization method based on
the LOD model. [25]

A semantic inspection method does an analysis of
the communication process in three constitutive levels:
Level 1: The communication process was successful
The receiver has seen the symbol and the
construction of the denotation has started. Semantic
inspection can be achieved by recording user
interactions (e.g. the symbol was touched) or by a
simple eye tracking systems. [26]
Level 2: The construction of the denotation at the
receiver is finished
Analysis of user interactions and/or facial
expressions [27] can be used as indicators for the
completion of the interpretation task This does not
mean that the intended denotation is concordant
with the constructed denotation.

3. Adaptive Visual Symbols
Building on a number of previous studies in the field of
static and dynamic visual languages [5], [6], [7], [8]
and [9] we developed a model, where each basic sign is
able to adapt its visual appearance and level of detail.
In order to achieve this goal we propose a new type of
interface object, the Adaptive Visual Symbol (AVS).

Level 3: The constructed denotation is concordant to
the intended denotation

222

In this case we can distinguish between (a) simple
denotations, e.g. a command or question, where the
fulfilment of the command deals as direct
confirmation, and (b) complex denotations, e.g. a
part of a medical treatment. In the case of a complex
denotation concordance can only be measured in a
wider context.

by trained medical experts in their day-to-day business,
but on the other hand still too complicated and not selfexplaining for patients. Based on input gained in focus
groups from patients and considering design principles
described by John Maeda [30] we re-worked the visual
language, see figure 6.

Experiments have shown, that it is sufficient to provide
feedback mechanisms at level 1 and 2 in order to
implement an adaptive visual communication process.
If the receiver has a completely wrong constructed
denotation, the following communication steps will fail
even at level 2 tests, if they are constitutive.

Figure 6 – Visualization of the same disease history as
in figure 5, but from a patient’s perspective. Each
symbol tells the “what” and “when” and has an
optional indicator for the outcome (green checkmark)

4. Symbols for PHRs

In particular the “patient style” visual language builds
on the following principles and methods:

The basic element of a medical history is an event.
Main attributes of an event are it’s type together with a
short description, localisation, main outcome and the
event’s temporal range.

•
•
•
•

Remove obvious facts and add more meaningful
symbols for patients.
Focus on emotions.
Organize information.
Provide a gesture-based interface for information
organization and semantic inspection.

Figure 4 – Visualization of a medical event
Together with medical doctors we developed a visual
language [28], which transform a part of textual
diagnosis in a visual symbol, see figure 4. In this
approach each visual symbol represents either an
observation or procedure in a treatment history and
corresponds to an unique ICD10 coding [29] as widely
used in health care institutions. With the help of such a
visual summary experts can overview a medical case
within seconds, see figure 5.

Figure 7 – Selection of additional information
embedded within the symbol
Figure 7 shows the application flow of the user
interaction with adaptive symbols. When the user
touches the border (1) the symbol shows additional
information, e.g. personal notes, the doctors diagnosis,
web links or a even a video recording (2). The user can
move the new information item to the center, where an
iconic representation is shown (3). In this case the

Figure 5 – Visualization of a medical finding, as used
by medical experts.
This very condensed (visual) representation of a
medical finding is on the one hand effective when used

223

master symbol of the event, e.g. a chemotherapy
treatment is moved to the bottom right area. The
iconic representation itself holds the user-interface, i.e.
the video content is played by pushing the play button
in the center of the symbol (4).

order to support multitouch tablet devices and gesture
based interaction paradigms.

6. Acknowledgments
This work was funded by the FIT-IT programme (813
398) and by the Austrian Fonds zur Förderung der
wissenschaftlichen Forschung (FWF, L427-N15).
Medical data were provided in the context of the
Austrian Genome Programme GEN-AU and the CRIP
project. Our thanks are due to all partners projects, for
their contributions, critical reviews and various
discussions. The study has been approved by the
Ethical Committee of the Medical University of Graz.

7. References
[1]

The National Alliance for Health Information
Technology report to the Office of the National
Coordinator for Health Information Technology on
defining key health information technology terms
April 28, 2008. Chicago, IL: National Alliance for
Health Information Technology; 2008, pg 6.
[2] S. Kahn (Eds), A Community View on How Personal
Health Records Can Improve Patient Care and
Outcomes in Many Healthcare Settings, Northern
Illinois Physicians for Connectivity (NIPFC) and
Northern Illinois University Regional Development
Institute (NIU RDI), 2009.
[3] A. Holzinger, Usability Engineering Methods for
Software Developers, Communication of the ACM,
Vol. 48, 2005, pp. 71-74.
[4] H. von Foerster, Cybernetics of Cybernetics (2nd
edition), Future Systems, Minneapolis, 1996.
[5] O. Neurath, International picture language, University
of Reading, 1978.
[6] H. Maurer, R. Stubenrauch, D. Camhy, Foundations of
MIRACLE - Multimedia Information Repository, A
Computer-supported Language Effort, J.UCS Vol 9,
No 4., 2003, pp.309-348.
[7] J. A. Lennon, H. Maurer, MUSLI: A hypermedia
interface for dynamic, interactive, and symbolic
communication; J.NCA vol 24, (2001), 273-291.
[8]
J. A. Lennon, H. Maurer, Augmenting text and voice
conversations with dynamic, interactive abstractions
using P2P networking; J.NCA vol. 24 (2001), 293-306.
[9] H. Maurer, P. Carlson, Computer Visualization, a
Missing Organ and a CyberEquivalency; Collegiate
Microcomputer, vol. 10, no. 2 (1992), 110-116.
[10] J. A. Lennon, H. Maurer, MUSLI- A Multisensory
Interface; Proc. ED-MEDIA’94, AACE (1994), 341348.
[11] D. H. Jonassen, R. Goldman-Segal, H. Maurer,
Dynamicons as Dynamic Graphic Interfaces;
Intelligent Tutoring Meida vol. 6, No.3-4 ( 1996), 149158.

Figure 8 – Explanations and configuration of symbols
Figure 8 shows how a symbol explains itself. By
touching the border of the symbol, a small question
mark appears in the top left area (1). This button
activates an explanatory interface (2), where the user
gets a short textual explanation and can change (or
upload) an alternative symbol (3). In our example the
symbol for chemotherapy was changed from a pill to
an infusion bottle. Please note, that the symbol is
changed for every chemotherapy event (4).

5. Conclusion
We developed a framework for adaptive visual
symbols for the visualization of personal health
records. With the help of adaptive symbols an user can
summarize his medical history with the desired
complexity and visual style. The adaptive and flexible
approach can even support the visualization of
"universal health records" (UHR) as proposed by John
Morgenthaler [31]. UHRs are the union of electronic
health records as used by health care provides and PHR
both shared in a granular way. With the help of the
framework, implemented as actionscript library
(FLEX), a demonstrator application was realized. We
prospectively plan to move to a HTML-5 approach in

224

[22] W. Horn, C. Popow. L. Unterasinger. Support for fast
comprehension of ICU data: visualization using
methaper graphics. Methods of Information in
Medicine 40, 2001, pp. 421-424.
[23] C. Plaisant, B. Milash, A. Rose S. Widoff. B.
Shneiderman, LifeLines: visualizing personal histories.
Proceedings of the ACM CHI 96 conference on
Human Factors in Computing Systems, 1996, pp. 221227.
[24] M. Konchady, R. D’Amore, G. Valley, A web based
visualization for documents. In: Proceddings of the
workshop on new paradigms in information
visualization and manipuation, ACM press, 1998, pp.
13-19
[25] Jiye An, Zhe Wu, Hushan Chen, Xudong Lu, Huilong
Duan: Level of detail navigation and visualization of
electronic health records, Prpceedings of Biomedical
Engineering and Informatics (BMEI), 2010, pp. 2516 –
2519.
[26] M. Argyle, M. Cook, Gaze and Mutual Gaze,
Cambridge University Press, London, (1977).
[27] M. Pantic, Facial gesture recognition from static dualview face images, International Conference on
Measuring Behaviour, (2002), pp. 195-197.
[28] H. Müller, S. Sauer, K. Zatloukal, T. Bauernhofer.
Interactive Patient Records. Proceedings of IV'10 14th International Conference on Information
Visualization, London (2010)
[29] ICD-10: international statistical classification of
diseases and related health problems: tenth revision.
World Health Organization 2004.
[30] J. Maeda, The Laws of Simplicity (Simplicity: Design,
Technology, Business, Life), The MIT Press, 2006.
[31] J. Morgenthaler, Moving Toward an Open Standard
Universal
Health
Record,
http://www.smartpublications.com/articles/view/moving-toward-anopen-standard-universal-health-record/, last visited
Feb. 2011.

[12] P. Patrik, Intelligent User Interfaces – Introduction and
survey – Research Report DKS03-01 / ICE 01, Delft
University of Technology, 2003.
[13] E. Ross, Intelligent User Interfaces: Survey and
Research Directions, Technical Report: CSTR-00-004,
Bristol, 2000
[14] P. Langley, User modeling in adaptive interfaces.
Proceedings of the Seventh International Conference
on User Modeling. Banff, Alberta: Springer, (1999),
pp. 357-370.
[15] P. Brusilovsky, D. W. Cooper, Domain, task, and user
models for an adaptive hypermedia performance
support system, Proceedings of the 7th international
conference on Intelligent user interfaces, (2002), pp.
23-30.
[16] R. M. Young, T. R. G. Green, T. Simon,
Programmable User Models for Predictive Evaluation
of Interface Designs, in K. Bice and C. Lewis (eds.)
Proceedings of CHI'89 Human Factors in Computing
Systems, ACM Press, New York, (1989).
[17] B. Myers, Creating User Interfaces by Demonstration,
Academic Press, (1988).
[18] W. Kintsch, Comprehension: A Paradigm for
Cognition, MA: Cambridge University Press, (1998).
[19] Y. W. Sohn, & S.M. Doane, Evaluating
Comprehension-Based User Models: Predicting
Individual User Planning and Action. User Modeling
and User Adapted Interaction. 12(2-3), (2002), pp.
171-205.
[20] W. Aigner, S. Miksch, CareVis: Integrated
Visualization of Computerized Protocols and Temporal
Patient Data. Presentation: Workshop on Intelligent
Data Analyis in Medicine and Pharmacology
(IDAMAP-2004), Stanford, USA; 06-09-2004; in:
"Workshop Notes of the Workshop on Intelligent Data
Analyis in Medicine and Pharmacology", (2004)
[21] S. M. Powsner S.M., E. R. Tufte., Graphical summary
of patient status. The Lancet 334, 1994, pp. 386-389.

225

