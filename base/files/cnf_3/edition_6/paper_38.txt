Visualizing Concept Associations Using Concept Density Maps
Nees Jan van Eck, Flavius Frasincar, and Jan van den Berg
Faculty of Economics, Erasmus University Rotterdam
P.O. Box 1738, 3000 DR Rotterdam, The Netherlands
{nvaneck,frasincar,jvandenberg}@few.eur.nl
Abstract
The concept mapping algorithm proposed in an earlier
paper is one of the dimensionality reduction techniques that
can be used for knowledge domain visualization. Using this
algorithm to visualize large knowledge domains may not
always provide a good overview of the domain due to visual cluttering of concepts. In this paper, we propose to apply kernel density estimation to the visualization of concept
maps in order to be able to better explore large knowledge
domains. Kernel density estimation proves to be useful for
the identiﬁcation of concept clusters at different levels of detail. In addition to the visual exploration of large knowledge
domains, we are also able to visually verify the hypothesis
that the concept mapping algorithm places related concepts
close to each other. The ﬂexibility and effectiveness of our
approach is validated by applying the proposed technique
to different visualization scenarios for the ﬁeld of computational intelligence.

1

Introduction

Knowledge domain visualization (KDViz) (e.g., [2]) is
concerned with the creation of maps that help to present,
analyze, and discover important aspects of the information
speciﬁc to a certain scientiﬁc ﬁeld. Following [2], we divide the process of KDViz into the following six steps: (1)
collection of raw data, (2) selection of the type of item to
analyze, (3) extraction of relevant information from the raw
data, (4) calculation of similarities between items based on
the extracted information, (5) positioning of items in a lowdimensional space based on the similarities, and (6) visualization of the low-dimensional space. The ﬁrst step of the
KDViz process is the collection of appropriate data. Since
domain maps are typically constructed on the basis of a corpus of scientiﬁc texts, one has to collect these texts ﬁrst.
The second step of the KDViz process is the selection of
the type of item to analyze. The type of item to analyze
depends on the question one wants to answer. The most

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

common types of items are journals, articles, authors, and
descriptive words or terms. Each type of item can be used
to visualize a different aspect of a scientiﬁc ﬁeld. The third
step of the KDViz process is the extraction of relevant information from the raw data collected in the ﬁrst step. In many
cases, the relevant information consists of co-occurrence
frequencies of items. The fourth step of the KDViz process is the calculation of similarities between items based
on the information extracted in the third step. A possible
approach that can be taken to calculate similarities between
items based on co-occurrence frequencies is to normalize
the co-occurrence frequencies. The ﬁfth step of the KDViz
process is the positioning of items in a low-dimensional
space based on the similarities calculated in the fourth step.
This step is usually performed using dimensionality reduction techniques. The sixth step is the visualization of the
low-dimensional space that results from the ﬁfth step. The
low-dimensional space has to be visualized in such a way
that it can be effectively and accurately explored by human
users.
In a previous paper [6], we have focused on the ﬁfth step
of the KDViz process, i.e., the positioning of items in a lowdimensional space. In that paper, we have presented an algorithm for constructing concept maps. In the remainder of
this paper, we refer to this algorithm as the concept mapping
algorithm. A concept map is a domain map that visualizes
the associations between concepts in a scientiﬁc ﬁeld. In a
concept map, concepts are located in such a way that the
distance between two concepts reﬂects the strength of their
association. The stronger the association between two concepts, the smaller the distance between them. A concept
map can be used to obtain an overview of a scientiﬁc ﬁeld
and, more speciﬁcally, of a ﬁeld’s important concepts and
their mutual associations.
A deﬁciency of a concept map is that concept labels have
the tendency to overlap when more concepts are displayed.
This results in a decrease in insight into the structure of
the concept map. Consequently, it may be difﬁcult to get
a quick overview of a scientiﬁc ﬁeld.
In this paper, we want to focus more on the visualiza-

tion of the low-dimensional space, i.e., the sixth step of the
KDViz process. Our goal is to visualize a concept map that
is generated by the concept mapping algorithm in such a
way that its structure is clear at a ﬁrst glance. This is accomplished by visualizing the density of the concepts rather
than all individual concepts. We refer to maps that visualize the density of concepts as concept density maps. To
calculate the density of concepts, we use kernel density estimation (KDE) (e.g., [8]). In a case study that we describe
in this paper, KDE in combination with the concept mapping algorithm is used to visualize the associations between
concepts in the ﬁeld of computational intelligence. It turns
out that the resulting concept density maps give, at different
levels of detail, a quick overview of this ﬁeld.
The rest of the paper is structured as follows. Section 2
discusses related work. Section 3 presents the methods that
are used in this paper for the positioning and visualization
steps of the KDViz process. The application of these methods to the computational intelligence ﬁeld is described in
Section 4. Finally, Section 5 concludes the paper and proposes future research directions.

2

Related Work

In this section, we discuss some of the existing methods that can be used for the positioning of items in a lowdimensional space and for the visualization of the lowdimensional space.
The positioning step of the KDViz process is generally performed using dimensionality reduction techniques.
These techniques are able to represent multivariate data in a
small number of dimensions. In the case of KDViz, this
means that high dimensional item similarities are represented in a two- or three-dimensional space that can be visually interpreted by humans. Some of the dimensionality
reduction techniques that can be used for KDViz are multidimensional scaling, principle component analysis, factor analysis, pathﬁnder network scaling, and self-organizing
maps [2]. Due to space limitations, we will only discuss
multidimensional scaling in more detail.
Multidimensional scaling (MDS) (e.g., [1]) is the most
commonly used positioning method in the literature on
KDViz. Given a set of items and the dissimilarities between
these items, MDS positions the items in a low dimensional
space in such a way that the distances between the items
correspond as close as possible to the dissimilarities. The
degree of correspondence is measured by a so-called stress
function that penalizes the overall disparity between distances and dissimilarities. The optimal positioning is obtained by minimizing the stress function.
The concept mapping algorithm proposed in [6] can be
seen as an alternative to MDS. In [6], we made a comparison between this algorithm and MDS by using both methods

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

for constructing a concept map of the computational intelligence ﬁeld. We compared the positioning generated by the
concept mapping algorithm with the positioning generated
by MDS. It turned out that the concept mapping algorithm
generated a more satisfactory concept map than MDS.
We now consider the visualization step of the KDViz
process. The simplest method to visualize the result of the
positioning step is the so-called scatter visualization. In the
scatter visualization, the spatial positions of items are visualized using points. When a lot of items have to be visualized, the scatter visualization tends to suffer from cluttering. The landscape visualization aims to improve on this.
In the landscape visualization, a smooth terrain-like surface
is constructed in such a way that the height of the surface
indicates the concentration of items in an area. The concentration of items in an area can be calculated using density estimation methods. Most density estimation methods
are based on a nearest-neighbor model, a histogram model,
a kernel-based model, or a (Gaussian) mixture model [5].
KDViz tools that offer a landscape visualization are, e.g.,
VxInsight [3] and IN-SPIRE ThemeView (formerly known
as ThemeScape [9]).
Methods adopted from the ﬁeld of graph visualization
are sometimes also used for the positioning and visualization steps of the KDViz process. In the ﬁeld of graph visualization, spring embedding methods (also known as force
directed methods) are typically used to position the nodes of
a graph into a layout that satisﬁes similarity requirements as
well as presentation requirements (e.g., as few as possible
crossing edges). In spring embedding methods, nodes are
seen as physical bodies that cause repelling forces on one
another and edges between nodes are seen as springs that
cause attraction forces between nodes. The ﬁnal layout of
the graph is a solution in which the forces on each node in
the graph are in equilibrium.
GraphSplatting [7] is a method for visualizing large
graphs as two-dimensional continuous ﬁelds. From the layout of the graph, a continuous ﬁeld is obtained by placing two-dimensional Gaussian shaped basis functions on
each node and then summing all basis functions. Although
the authors of [7] do not mention it, this is (almost) the
same as two-dimensional KDE with Gaussian kernel functions. In [4], GraphSplatting has been successfully applied
to the visualization of domain model representations using
resource description framework graphs.

3

Methods

In this section, we present the methods that we use for
the positioning and visualization steps of the KDViz process. The methods are described in Subsection 3.1 and 3.2,
respectively.

3.1

Concept Mapping Algorithm

In this section we brieﬂy describe the concept mapping
algorithm that is proposed in [6]. To position concepts at
appropriate locations in a concept map, the algorithm needs
a concept association matrix as input. Let c1 , . . . , cn denote
the concepts of interest, where n indicates the number of
concepts. The concept association matrix A is an n×n matrix that contains for each combination of two concepts the
strength of their association. Element aij of A is referred to
as the association strength between concepts ci and cj . In
the case study that is described in Section 4, the association
strength between two concepts is calculated as the number
of texts in which the concepts co-occur.
The underlying idea of the concept mapping algorithm is
that each concept should be positioned as close as possible
to its ideal location. For a two dimensional concept map,
the location of concept ci is denoted by the vector xi =
(xi1 , xi2 )T and the ideal location x∗i of concept ci is deﬁned
as
n
j=1 aij xj
x∗i =
.
(1)
n
j=1 aij
The only way to position each concept at its ideal location is
to assign all concepts to the same location. This, of course,
does not result in a useful concept map. The algorithm
therefore attempts not only to position concepts as close as
possible to their ideal location but also to prevent concepts
from being located too close to each other. To achieve this,
the algorithm minimizes the following objective function
⎛
⎞
n

E=
i=1

⎜
∗
⎜w
⎝ ¯i xi − xi

2

n

+β

e−

xi −xj

j=1
j=i

⎟
⎟,
⎠

(2)

where w
¯i is a weight indicating the importance of concept
ci , β is a parameter, and · denotes the Euclidean norm.
For the exact calculation of w
¯i , we refer to [6]. In (2), the
ﬁrst term within the parentheses is responsible for positioning concepts as close as possible to their ideal location. This
term pays more attention to concepts with higher weights.
The second term within the parentheses is responsible for
preventing concepts from being located too close to each
other. In the case study in this paper, a gradient descent algorithm is used to ﬁnd a (local) minimum of the objective
function.

3.2

Kernel Density Estimation

Kernel density estimation (KDE) (e.g., [8]) is a statistical
method for constructing a smooth estimate of a probability
density function from observed data points. In this paper,
KDE is used as a method for displaying the density of concepts in a concept map. The general idea of this approach

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

is that an estimate of the density of concepts is obtained
by ﬁrst placing a symmetric probability density function,
called a kernel function, at each concept location and then
taking the average of the kernel functions. To speed up calculations, the density of concepts in a concept map is only
estimated for a ﬁnite grid of points that is speciﬁed by the
user. The kernel density estimate of a grid point at location
x = (x1 , x2 ) is given by
ˆ (x1 , x2 ) =
D

1
nh1 h2

n

K
i=1

x1 − xi1 x2 − xi2
,
h1
h2

, (3)

where K(·) is a two-dimensional kernel function centered at
each concept location xi = (xi1 , xi2 ), and h = (h1 , h2 ) is
a bandwidth parameter that controls the degree of smoothness.
In the case study that is described in Section 4, the twodimensional kernel function is taken to be the product of
two Laplace density functions, leading to
K (t1 , t2 ) =

1 −(|t1 |+|t2 |)
.
e
4

(4)

In our experience, the use of other density functions, such
as the Gaussian, the Epanechnikov, or the triangular one,
leads to worse results with respect to the identiﬁcation of
concept clusters.
Choosing good bandwidths is important. Too small
bandwidths do not remove insigniﬁcant bumps and result
in too rough density estimates, while too large bandwidths
smear out real peaks and result in too smooth density estimates. In the case study in this paper, we use the so-called
normal scale bandwidth selector [8] to produce estimates of
the bandwidths. In the case of the Laplace product kernel,
the normal scale bandwidth selector is equal to
ˆj =
h

√

π
6n

1/5

σ
ˆj

j = 1, 2,

(5)

where σ
ˆj is the standard deviation of the concept locations
in the jth dimension.

4

Case study: Visualizing Concept Associations

To illustrate the usefulness of KDE in combination with
the concept mapping algorithm, we used both methods to
construct visualizations of the associations between concepts in the ﬁeld of computational intelligence (CI). The CI
ﬁeld, which can be seen as a part of the larger artiﬁcial intelligence ﬁeld, deals with topics like neural networks, fuzzy
systems, and evolutionary computation. Table 1 summarizes the approach taken in our research.

fisher information matrix
unsupervised competitive learning rule
finite state machine
neighborhood function
vapnik chervonenki
gaussian mixture model
nonlinear activation function
model
selection
locally excitatory globally inhibitory
oscillator
network criterion
single
layer
perceptron
cascade
correlation
model
selection
target
function
finite state
automaton
sigma
piinput
unit
decision
boundary
layer
fault generalization
tolerance
trainingerror
error
internal representation
boltzmann
memory
pattern machine
linear
discriminant
feature
vector analysis
training
sample
classification
accuracy
pattern
recognition
problem
mixture
of experts
meanunit
squared
error
associative memory model output
self
organizing
map
training
example
learning
algorithm
chaotic
dynamics
layer
probabilistic
neural
feature
map network
storage
capacitycompetitive
output
layer
classifier
feature
extraction
synaptic
weight
classification
input
principal
pattern
component
analysis
training
pattern
independent
component
analysis
hopfield
model
neural
network
classifier
inuous time hopfield neural network
vector quantization
error
rate
decision
tree
principal
component
basin ofstable
attraction
fuzzy
art
error
function
sigmoid
function
weight
pattern
recognition
state
nearest
neighbor
classifier
classification
problem
multilayer
perceptron
short activation
term
memory
expectation
synapse
nearestmaximization
neighbor
function
input data
chaotic
neural network
associative
memory
neuron
modeltraining
fuzzy
artmap
feedforward
neural
network
hardware
implementation
recognition
rate
network
output
noise
neural
computation
transfer
learning
function
algorithm
weight
space
connection
matrix
self
organization
training
data
networkassociative
dynamics
support
vector
machine
data
point
receptive
field
bidirectional
memory
mutual
information
network
topology
very large
scale
integration
data
fuzzy
measure
local
minimum
prototype
input
space
network
architecture
generalization
art
network
network
structure
radial
basis
function
cluster
energy
function
radial
basis
function
network
backpropagation
algorithm
hopfield
network
discrete time recurrent
neural
network
multilayer
neural
network
neural
network
model
adaptive
resonance
theory
weight
matrix
least
squares
signal
processing
neural
network
algorithm
field programmable
gate
array
feature
gradient
descent
output
error
neural
system
continuous time
neural
network
recurrent
neural
network
time
cost
series
linearfunction
programming
neural
network
stable cellular
equilibrium
point
classifier system
fuzzy integral
basis
function
recursive
least
squares
output
space
long term dependency problem
cerebellar model articulation
controllervalue
prediction
signal
singular
decomposition
multilayer
feedforward
network
time seriesneural
prediction
error
delta
rule
input signal
convergence
fuzzy c means
function approximation
objective
function
nonlinear neural
kalman filter
equilibrium
point network
agent
fuzzy
clustering
orthogonal least
squares
model
selection
outputconnectionist
fuzzy classifier
dynamic
individual
learning classifier system
timeneural
delay network
domain of attraction
rule
combinatorial
problem
optimaloptimization
solution local
optimum
curse
of dimensionality
identification
universal
approximator
genetic programming
generation
solution
linear system
global exponential
stability
optimization
algorithm
quadratic
programming
problem
input variable
parameter
nonlinear dynamic
system
evolution
optimization problem
dynamical system
expert system
network
controller
one point crossover
evolutionary
program
global neural
asymptotic
stability
local
search
population
input output data optimizationevolutionary
computation
evolutionary programming
adaptive programming
fuzzy system
fuzzy neural neuro
network
fuzzy system
dual heuristic
neural controller knowledge base
global
search
fitness
population
size
lyapunov function
approximation
error
fitness
function
evolutionary search
linguistic information
fuzzy number
genetic algorithm
offspring
genetic diversity
rule
baseprogramming problem
parameter
identification
inference
linear
structurefuzzy
identification
fuzzylogic
rule
base
stability
evolutionary
algorithm
inference
engine
search multiobjective
space
evolutionary
mutation
pareto
front twoalgorithm
point crossov
mutation
operator
fuzzy
set
measurement
noise
control input
operator
crossover
t norm
control problem
control scheme
membership
function
recombination
fuzzy rule
tournament
selection
multiobjective
control
evolution
strategyoptimization proble
control
system
self adaptation
control
law
crossover operator
closed
loop algorithm
system
control
design
method
control performance
fuzzy modeling
uncertainty
stability condition
memetic
algorithm operator
modeling
error
recombination
defuzzificationfuzzy
method
stability
analysissystem
relation
nonlinear
controller
design
fuzzy
controller
pidlogic
controller

unit

neural network

input

system

fuzzy automaton
supervisory
controller
sliding
mode
control
controller
chaotic
system
adaptive
fuzzy
controller
control
rule
fuzzy
system
closed loop fuzzy control
system
control
design
fuzzy
control
linear matrix
inequality
fuzzy
controller
linguistic term
sliding
modedisturbance
controller
external
linguistic model
takagi sugeno system
discrete time fuzzy system
closed
loop fuzzy system
fuzzy
observer

Figure 1. Concept map of the CI ﬁeld constructed using the concept mapping algorithm.

4.1

Figure 2. Colored concept density map of the
CI ﬁeld constructed using KDE.

in the corpus in which the concepts co-occur.

Data

4.2
The visualizations were constructed on the basis of a corpus of scientiﬁc texts. The corpus that we used was taken
from our previous research [6]. This corpus consists of
about 3,800 English-written abstracts that were taken from
ﬁve leading scientiﬁc journals in the CI ﬁeld using the Science Citation Index Expanded (SCIE). Using a thesaurus of
the CI ﬁeld, 294 different concepts were identiﬁed in the
corpus of abstracts. The association strengths of the identiﬁed concepts were calculated and stored in a concept association matrix. The association strength of two concepts
was calculated as the co-occurrence frequency of the concepts in the corpus of abstracts, i.e., the number of abstracts

Table 1. Summary of the way in which the
KDViz process is implemented in this paper.
Step of the KDViz Process
(1) Collection of data
(2) Selection of type of item
(3) Extraction of information
(4) Calculation of similarities
(5) Positioning of items
(6) Visualization

Implementation
Corpus of CI abstracts
Concepts
Co-occurrences frequencies
Association strengths
Concept mapping algorithm
(Subsection 3.1)
Kernel density estimation
(Subsection 3.2)

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

Map Overview

The concept mapping algorithm was used to map the
concept association matrix to a two-dimensional concept
map. A more detailed description of this mapping, together
with parameter settings, can be found in [6]. Figure 1 shows
the resulting concept map of the CI ﬁeld. The goal of the
concept map is to obtain an overview of the CI ﬁeld, but due
to the overlap of concept labels it is hard to get this overview
at a ﬁrst glance.
We applied KDE to the concept map in Figure 1 to gain
more insight into the structure of the CI ﬁeld. We used
the Laplace kernel and the normal scale bandwidth selector. Figure 2 shows the resulting colored concept density
map constructed using a grid size of 500 × 500. The used
color scheme ranges from the colors blue to red, and passes
through the colors green, yellow, and orange. Blue denotes
low densities, while red denotes high densities.
By looking at the colored concept density map, we can
easily detect some clusters (i.e., areas of high densities),
which indicate the presence of a large number of highly associated concepts. A large and very dense cluster of concepts can be identiﬁed in the top center of the map (colored
by red). In addition, three smaller and less dense clusters
can be found in the bottom left and the bottom right of the
map (colored by green). The concept labels that are shown
on top of the map should give an indication of the topic of

each cluster, and consequently may point to the main research topics in the CI ﬁeld. The concepts in the top center
cluster of the map (indicated by the labels neural network,
unit, weight, and training) are related to the topic of neural
networks. The concepts in the two bottom left clusters (indicated by the labels system, control, controller, fuzzy system, and membership function) are related to the topic of
fuzzy systems. The concepts in the bottom right cluster (indicated by the labels evolutionary algorithm, and genetic
algorithm) are related to the topic of evolutionary computation.
To validate the hypothesis that related concepts are
placed in the same cluster, we constructed a contoured concept density map in which the locations of the concepts are
also displayed. This map is shown in Figure 3. For each of
the concepts, we manually determined whether the concept
is relevant to the topic of neural networks, to the topic of
fuzzy systems, to the topic of evolutionary computation, or
to more than one of these topics. A green dot (•) refers to
a neural network concept, a red cross (×) refers to a fuzzy
systems concept, a blue plus sign (+) refers to an evolutionary computation concept, and a grey star (∗) refers to a general concept. The colored contour lines indicate points that
have the same density. Since the contour lines roughly indicate the boundaries of the clusters, we can see that the top
center cluster contains mainly neural networks concepts, the
bottom left clusters contain mainly fuzzy systems concepts,
and the bottom right cluster contains mainly evolutionary
computation concepts. From this observation we can conclude that the hypothesis that related concepts are placed in
the same cluster has been proven valid.

4.3

Zooming into the Map

Up to now, we have only looked at a concept density
map (Figure 2) that is constructed on the basis of the complete concept map of the CI ﬁeld (Figure 1). As we have
seen, this concept density map gives a quick overview of
the global structure of the CI ﬁeld. To gain more insight
into local details of the CI ﬁeld, we zoomed into a region
that seems interesting. The region of interest is indicated by
a dashed bounding box in Figure 1 and 2 and corresponds to
the area in which most of the concepts related to fuzzy systems are located. Figure 4 shows the concept map of this
region of interest. Subsequently, we applied KDE to this
concept map. Again, we used the Laplace kernel and the
normal scale bandwidth selector. Figure 5 shows the resulting colored concept density map constructed using a grid
size of 500 × 500. It should be clear from Figure 5 that we
take a look at a more detailed level and that we again obtain
a quick overview of the knowledge structure, this time that
of the selected region. The most important concepts in the
region are now visible while most of them are not visible in

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

Figure 3. Contoured concept density map of
the CI ﬁeld including the concept locations.

Figure 2. In addition, the concept density map shows many
more details like the distinct, red colored clustering in three
regions, which is not that well visible in Figure 2.
The concept density map in Figure 5 was inspected more
carefully, both for validation purposes and for knowledge
discovery. We showed the map to two experts in the ﬁeld
of fuzzy logic and fuzzy systems. They both agreed that
(fuzzy) inference and rule base are indeed semantically
close concepts, relatively different from concepts like control system and controller. They also observed that the concepts that lie just between the three clusters reveal the most
interesting information. The fact that the concept parameter is situated in the upper right corner could easily be explained since this concept is also of importance for neural
networks (see also Figure 2). The fact that the concept Lyapunov function is situated in the upper left came for both experts as a surprise. One of the experts knew that this concept
is used in the ﬁeld of neural networks, especially related to
the concept associative memory (see Figure 2). Since the
concept Lyapunov function lies between associative memory and (fuzzy) control system, he started to think that this
concept is also of importance within fuzzy control, which is
indeed the case. For the other expert, who is familiar with
fuzzy control, it was a new fact that Lyapunov functions are
used in the ﬁeld of neural networks. So, careful inspection
of the concept density maps by two domain experts revealed
that these maps enable a process of knowledge discovery.

system
global exponentiallinear
stability

parameter
input variable

nonlinear dynamic system

dynamical system
expert system

neural network controller
global asymptotic stability

input output data
adaptive fuzzy system
dual heuristic programming

fuzzy neural network

neuro fuzzy system

neural controller

system

approximation
error informationknowledge base
linguistic
fuzzy number
rule base
parameter identification
inference
structure identification
fuzzy logic
fuzzy rule base
inference engine

lyapunov function

stability

fuzzy set

measurement noise
control input

t norm

control problem

control scheme

membership function

control

fuzzy rule

control system
control law
closed loop system
control algorithmdesign method
control performance

uncertainty

stability condition

modeling error

fuzzy modeling
defuzzification method

stability analysis

fuzzy relation

nonlinear system
controller design
fuzzy logic controller
pid controller

supervisory controller
sliding modecontroller
control
chaotic
system
adaptive
fuzzy controller
control rule
em

fuzzy system

control design
fuzzy control
linear matrix inequality fuzzy controller

linguistic term

sliding mode
controller
external
disturbance

Figure 4. Concept map of a region of interest.

5

References

Conclusions

To visualize knowledge domains consisting of a large
number of concepts, one can use the concept mapping algorithm in combination with kernel density estimation. In
this paper, we employed as similarity measure the number of concept co-occurrences in a collection of texts from
the computational intelligence ﬁeld. After experimenting
with several kernels, we found that the Laplace kernel gives
the best results with respect to the identiﬁcation of concept
clusters. Using our visualization approach, we were able to
identify clusters at different levels of detail by zooming into
regions of interest. We were also able to test our assumption
that related concepts are placed in the same cluster.
At the moment, we can extract concept associations from
a large set of scientiﬁc texts and visualize these associations
as spatial relationships. In the future, we would like to extend our method by (1) extracting the semantic relationships
between concepts in a knowledge domain and (2) providing a suitable visualization metaphor to graphically depict
these semantic relationships. Also, we would like to complement the current visualization techniques with querying
facilities that provide an enhanced exploration of knowledge domains. In our endeavor, we are encouraged by the
recent developments for the Semantic Web that we plan to
investigate for the discovery, representation, and visualization of semantic concept maps.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

Figure 5. Colored concept density map of a
region of interest.

IEEE

[1] I. Borg and P. J. Groenen. Modern Multidimensional Scaling.
Springer, New York, 2nd edition, 2005.
[2] K. B¨orner, C. Chen, and K. W. Boyack. Visualizing knowledge domains. Annual Review of Information Science and
Technology, 37:179–255, 2003.
[3] G. S. Davidson, B. Hendrickson, D. K. Johnson, C. E. Meyers, and B. N. Wylie. Knowledge mining with VxInsight: Discovery through interaction. Journal of Intelligent Information
Systems, 11(3):259–285, 1998.
[4] F. Frasincar, A. Telea, and G. J. Houben. Visualizing the Semantic Web, chapter 9: Adapting Graph Visualization Techniques for the Visualization of RDF Data, pages 154–171.
Springer, 2nd edition, 2006.
[5] B. W. Silverman. Density Estimation for Statistics and Data
Analysis, volume 26 of Monographs on Statistics and Applied
Probability. Chapman & Hall, London, 1986.
[6] N. J. van Eck, L. Waltman, and J. van den Berg. A novel algorithm for visualizing concept associations. In Proceedings
of the 16th International Workshop on Database and Expert
Systems Applications, pages 405–409, 2005.
[7] R. van Liere and W. de Leeuw. Graphsplatting: Visualizing
graphs as continuous ﬁelds. IEEE Transactions on Visualization and Computer Graphics, 9(2):206–212, 2003.
[8] M. P. Wand and M. C. Jones. Kernel Smooting, volume 60 of
Monographs on Statistics and Applied Probability. Chapman
& Hall, London, 1995.
[9] J. A. Wise, J. J. Thomas, K. Pennock, D. Lantrip, M. Pottier, A. Schur, and V. Crow. Visualizing the non-visual: Spatial analysis and interaction with information from text documents. In Proceedings of the 1995 IEEE Symposium on Information Visualization, pages 51–58, 1995.

