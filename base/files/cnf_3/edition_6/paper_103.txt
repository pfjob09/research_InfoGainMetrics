Enhanced High Dimensional Data Visualization through Dimension Reduction and
Attribute Arrangement
Almir Olivette Artero, Maria Cristina F. de Oliveira, Haim Levkowitz
Universidade do Oeste Paulista, Universidade de São Paulo, University of Massachusetts Lowell
{almir@unoeste.br, cristina@icmc.usp.br, haim@cs.uml.edu}

Abstract
Researchers and users are well aware of the difficulties
related to finding an appropriate configuration of the axes
mapping attributes in multidimensional visualization
techniques, particularly in visualizations that show a large
number of attributes simultaneously. We address this
problem with a simple strategy that offers both dimension
ordering and dimension reduction. Dimension ordering is
based on attribute similarity heuristics, and the basic
rationale is extended to support dimension reduction. We
discuss the performance of our algorithms and present some
results of their application to several data sets. The
algorithms improve the capability of visualization
techniques to segregate clusters present in the data and
reduce the visual clutter aggravated by arbitrary
distributions of the axes.

1. Introduction
Information Visualization techniques face severe
limitations when the dimensionality of the data to be
displayed is very high. Similarly to clustering tasks, known
to be severely hampered by high-dimensionality [7],
generating meaningful visualizations of high-dimensional
data sets usually requires some dimensionality reduction or
attribute selection. Moreover, the outcome of many
visualization techniques, such as Parallel Coordinates [6],
RadViz [5] and Viz3D [2], is affected by the order by which
attributes are displayed.
In fact, the general problem of determining an optimal
visualization axes lay out is equivalent to the well-known
NP-hard Traveling Salesman problem [1, 3]. In this paper
we argue that it is possible to considerably enhance the
quality of visualizations by embedding some simple
heuristics into visualization systems to handle the axes
layout problem, rather than leaving the user to search for a
good layout on her/his own. Based on heuristics that
consider attribute similarity, we introduce a very simple
technique, called SBAA (Similarity-Based Attribute
Arrangement), which produces
attribute display
arrangements that yield improved visualizations. As in
Ankerst et al. [1], the rationale behind the proposed strategy
is to arrange axes so that attributes with similar behavior –

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

according to some similarity measure – are positioned as
close as possible to each other. In SBAR (Similarity-Based
Attribute Reductiont), and its variant mSBAR (modified
SBAR), attributes highly correlated with others are removed
from a visualization display, either automatically or by a
user-activation. Such a facility helps to reduce visualization
clutter and assists the user in finding out the most relevant
data attributes for a particular task.
In Section 2 we briefly discuss some previous work on
attribute arrangement and attribute selection in general data
visualization techniques. Our proposed strategies to arrange
and
select
attributes
by
similarity,
algorithm
implementations, and a brief performance analysis are
described in Section 3. In Section 4 we show how the
algorithms may be adapted to reduce the dimensionality of
visualizations. In Section 5 we illustrate the proposed
techniques on several data sources using two visualizations
techniques, Parallel Coordinates and Viz3D. In Section 6
we discuss our results and look into the future.

2. Related work
Several multidimensional visualization techniques operate
by projecting the data, originally defined on an ndimensional space (where n is the number of data
attributes), onto the two-dimensional (2D) screen space. In
Parallel Coordinates each record is mapped onto a polyline
drawn on a parallel coordinates reference system. In RadViz
n axes mapping n data attributes emanate from a circle
center and terminate on its perimeter. Each axis is
associated with an attraction factor as in an imaginary
spring system, and the position of a glyph is defined by
weighting their respective attribute values so as to
equilibrate the spring forces. RadViz has low computational
complexity, O(mn), and it favors cluster identification,
because similar records in nD space are projected close
together in 2D. However, record overlapping is high for
large data sets and very different records may be projected
close to each other. Viz3D is an alternative technique that
keeps the major advantages of RadViz, while reducing some
of its limitations. In Viz3D data records are projected on the
surface and interior of a 3D cylinder, creating a
representation that accommodates a larger number of

graphical markers, and offers a more suitable alternative to
display large high-dimensional data. A Viz3D
representation is obtained by mapping the n-dimensional
coordinates of m data records into 3D coordinates (xi, yi, zi)
according to Eq. 1.
1 n −1 d i , j − min j
§ 2ʌj ·
cos¨
¸
¦
n j =0 max j − min j
© n ¹
1 n −1 d i , j − min j
§ 2ʌj ·
sen ¨
yi = yc + ¦
¸
n j =0 max j − min j
© n ¹
1 n−1 d i , j − min j
zi = zc + ¦
n j =0 max j − min j
xi = x c +

(1)

where Dmxn is the data matrix; maxj=Max(dk,j);
minj=Min(dk,j) for k = 1,..., m and (xc,yc,zc) is the origin of
the 3D radial axes system.
For any axis-based technique there are n! possible
arrangements to display n axes mapping n attributes, though
many of these would be equivalent. Ankerst et al. [1]
identify several attribute- (axis-) configurations. In
particular, the Linear 1D configuration is adopted in
Parallel Coordinate visualizations, whereas the Circular 1D
configuration is adopted in techniques such as RadViz and
Viz3D. Generating all possible arrangements, thus, poses a
non-polynomial problem analogous to the Traveling
Salesman Problem, for which a wide range of optimization
approaches have been reported.
Several authors consider specifically the problem of
finding good axis layouts in visualizations, so that
visualization quality is improved with the adoption of a
favorable layout. Ankerst et al. [1] suggest choosing a
display order such that similar attributes are positioned
close to each other. The argument here is that a choice of
order based on similarity reduces visual clutter and favors
cluster identification. The authors exploit the traveling
salesman analogy and suggest a heuristics to solve it. They
start from a similarity matrix that gives the similarities
between each pair of data attributes, and then search for an
optimal arrangement that minimizes the similarities
between adjacent attributes.
Yang et al. [15] establish a hierarchy amongst the
attributes, which are thus classified and displayed in a tree
structure so that similar attributes are positioned near each
other. A user may select a reduced attribute collection by
visually choosing a depth in the tree hierarchy, which in
turn, specifies the number of desired attributes. The idea is
extended in a later paper [16] that introduces an interactive
hierarchical approach for attribute spacing and filtering
applicable to high-dimensional data sets, based on attribute
hierarchies derived from attribute similarities.
Peng et al. [11] and Peng [12] are also concerned with
reducing clutter in multidimensional visualizations by
manipulating the arrangement of data attributes. In their
approach, called CBDR (Clutter-Based Dimension
Reordering in Multi-Dimensional Data Visualization)
different metrics are proposed to estimate visual clutter in

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

different visualization techniques. The authors then
construct visualizations with different attribute orders,
looking for the one arrangement that minimizes the defined
clutter metrics. The computational cost O(n.n!) hampers
application of this technique to large high-dimensional data.
Some of the previous approaches explicitly deal with
the problem of searching for attribute mapping sequences
that improve the quality of resulting multidimensional
visualizations. Nevertheless, the somewhat related problem
of reducing a data set’s dimensionality has attracted a lot of
attention from researchers in Machine Learning and Data
Mining. Classical dimension reduction techniques are
widely used in connection with visualization. Examples
include Principal Component Analysis [10] and FastMap
[4], in which n-dimensional data are projected onto a kdimensional space (where k < n). A major disadvantage of
such projection approaches is that, although they strive to
preserve data characteristics, the original attribute values
are not preserved.
Feature selection algorithms [9], on the other hand, try
to identify those attributes most relevant for a particular
analysis task, offering a dimension reduction alternative
that preserves the relevant original attribute values. In
general, feature selection techniques can be classified into
one of two categories: In Sequential Backward Selection
(SBS), attributes are gradually removed from the data base,
following a specific criterion, until the desired number of
attributes is reached. In Sequential Forward Selection
(SFS), attributes are gradually inserted into an initially
empty database. Interesting solutions that may be
categorized as SBS feature extraction use the data fractal
dimension as a criterion to select attributes to remove [14].
Despite a general awareness of the difficulties related
to handling a large number of attributes in information
visualizations, most tools and implementations assume that
it is up to the user alone to search for a suitable layout for
the axes or to decide which attributes should be kept in, or
removed from a visualization. We believe that, although
user control must be preserved, it is possible to enhance
traditional visualizations and empower the user by deriving
some simple and useful metrics from the data and
embedding such knowledge in the visualization. In
particular, for the axis layout problem we steer axis
arrangements based on attribute similarity information.

3. Arranging attributes based on similarity
Let Dmxn be a data set with m records of n-attributes
each. We start our strategy with a (lower diagonal)
similarity matrix S containing similarity measures between
all the pairs of attributes:
ª−
«s
« 1,0
S = « s1,0
«
« ...
« sn , 0
¬

−
−
s2,1
...
sn ,1

−
−

− º
− »»
− − »
»
... − »
... sn ,n »¼

(2)

where si,j is a similarity measure of the ith and jth attributes,
such as the one in Eq. 3:
si , j = 1 −

ª
d k , j − Min j
1 « m d k ,i − Min i
−
¦
m « k =1 Max i − Min i Max j − Min j
¬

º
»
»
¼

(3)

where Mini = min(dk,i); and Maxi = max(dk,i) for k=1,...,m.
An alternative similarity measure to define the elements in
matrix S is Pearson’s Correlation Coefficient. Our proposed
process, called SBAA (Similarity-Based Attribute
Arrangement) follows the same rationale as the solution by
Ankerst et al. [1], but it is much simpler because it solves
the attribute arrangement problem with a straightforward
variation of the Nearest Neighbor Heuristic (NNH) method
[8] applied to the Traveling Salesman problem. In NNH the
traveling salesman starts from an arbitrary city and moves
on to the closest one, always in the same direction, and
returning to the first city after traversing all of them. In the
SBAA algorithm, once a similarity matrix S has been
computed the algorithm searches for the largest element si,j
of S. This element determines the two attributes i and j that
will form an initial axis arrangement “ij”. In this case,
attributes i and j define, respectively, the axes positioned in
the left and the right of, say, an initial Parallel Coordinates
arrangement. Subsequently, the algorithm searches the rows
and columns of S for attributes k1 and k2 to be appended to
the arrangement, which should be the most similar
attributes to either i or j. Attribute k2 is appended to the
extreme right of the arrangement, yielding “ijk2”, if
s j ,k2 > si ,k1 or k1 is appended to the extreme left, yielding
“k1ij”, otherwise. In the first case, i.e., when k2 is appended
to the extreme right, elements in row j and in column j of
matrix S will be ignored in future searches, while in the
second case, row i and column i will be dropped. The
process is repeated until all the remaining attributes of the
data collection have been inserted into the arrangement,
which may be labeled, say, as arrangement A. A global
similarity measure ¦A may be associated with arrangement
A, and can be computed by summing up the similarity
values of all consecutive attribute pairs in the sequence. For
example, supposing A=“ijk”, then ¦A(“ijk”) = si,j + sj,k. The
Algorithm SBAA, which takes as input the data matrix Dmxn,
is shown in Box 1.
1. A = "";
// A = {empty_string}
2. Compute Matrix S; // whatever similarity measure
3. Get largest value s_{i,j}, ==> most similar i & j (i <> j);
4. EL = i;
// Extreme Left
ER = j;
// Extreme Right
5. Get k1 in S s.t. s_{EL,k1} largest
and k2 in S s.t. s_{ER,k2} largest, k1, k2 not_in A;
6. If s_{EL,k1} > s_{ER,k1}
EL = k1
A = EL.A
// New A by appending EL on the left
Else
ER = k2
A = A.ER
// New A by appending ER on the right
7. Repeat Steps 5 through 6 until inserting all into A.

Box 1 Algorithm SBAA.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

Figure 1 illustrates Parallel Coordinates and RadViz
visualizations of two six-attribute records (Fig. 1(a)), where
the relative positioning of the attributes was generated by
SBAA (Fig. 1(d) and 1(e)), as compared to the visualizations
generated with an arbitrary layout (Fig. 1(b) and 1(c)).

Figure 1 RadViz and Parallel Coordinates visualizations
of two records of six attributes.

Note that interpretation of the RadViz visualization in
its default axis configuration is limited because the two data
records are projected onto the same spatial location; on the
other hand, these records are clearly separated when axes
arrangement is by attribute similarity.As for Parallel
Coordinates, the visualization that adopts an arbitrary axis
arrangement (Fig. 1(c)) has many polygon crossings. Most
are eliminated when we adopt the similarity-based
arrangement (Fig. 1(e)). This suggests that the average
length of the poly-lines, as defined in Eq. 4, is a possible
measure for visual cluttering in Parallel Coordinates: line
crossings are avoided if poly-line length is minimized.
m
£ = 1 ¦ Li
(4)
mn i =1
where Li is the length of the ith poly-line.

4. Reducing visual dimensionality
Visual clutter in multidimensional visualization
techniques is typically aggravated by high dimensionality.
Users of cluttered high-dimensional visualizations always
wonder which attributes are actually important, and which
ones could be removed from the visualization with little
information loss. The same strategy adopted in SBAA to
define a display arrangement for the axes can assist users in
exploring the set of relevant attributes. We have developed
SBAR (Similarity Based Attribute Reduction), an automatic
process for attribute removal from visualizations.
Given the similarity-based arrangement A and its
associated similarity measure ¦A, both obtained from the
similarity matrix S, SBAR implements an automatic process
that eliminates attributes that are most similar to their
neighbors in the defined arrangement. It is possible to
identify the two consecutive attributes i and j that are most
similar to each other by identifying the highest valued entry
si,j in the matrix. Of the two, the one to be eliminated is the
one (i or j) that yields the lowest global similarity value ¦A
for the arrangement. The process can be repeated until a
pre-determined number of attributes is reached, or until the
user is satisfied with the resulting visualization. Such

decisions can alleviate situations in which too many
attributes hamper visual interpretation and contribute little
to the analysis. Metrics could be used to quantify the ‘loss’
incurred by excluding a particular attribute [14]. Algorithm
SBAR is shown in Box 2.

resulting visualization shows a clear visual separation
between the four groups in the data.
(a)

1. Compute matrix S as described in Algorithm SBAA
2. Get Arrangement A from matrix S, as in SBAA
3. Get the greatest value si,j such that i & j appear
as consecutive elements in A
4. Remove either attribute i or attribute j from A, selecting the
one yielding the lowest value for ¦A
5. Repeat Step 4 until reaching the (user-defined) number of
attributes in A.

(b)
Figure 2 Viz3D visualizations of Quadruped Mammals
data.

Box 2 Algorithm SBAR.

As it is, the user should inform SBAR how many
attributes must not be included in the visualization. It would
be straightforward to implement an alternative in which the
user sets a similarity threshold to decide which attributes to
remove: in this approach, attributes whose similarity with
an attribute already included in the sequence is above the
threshold are not included in the visualization. One might
also modify the algorithm to rearrange the sequence of
remaining attributes after each removal before eliminating
the next one, as presented in Box 3.

Figure 3 allows comparing the Viz3D visualizations of
a data source named Synth32. This is a synthetic data set
with 38,850 20-attribute records, of which 14,831 define
eight clusters (of sizes 2,272 : clusters # 1, 3, 7, 1,521 : #2,
1,518 : #4, 2,271 : #5, 1,352 -- #6, 1,353 -- #8), and the
remaining 24,019 records consist of noise with uniform
distribution.
7

1. Compute matrix S as described in Algorithm SBAA
2. Get Arrangement A from matrix S, as described in SBAA
3. Get the greatest value si,j such that i & j appear as
consecutive elements in A
4. Remove attribute i or j from A, selecting the one yielding
the lowest value for ¦A
5. Repeat Steps 2 to 4, ignoring previously removed attributes,
until reaching the desired number of attributes in A

5. Results
In this section we present some results obtained by
applying SBAA and SBAR, to visualizations of different data
sets with Parallel Coordinates and Viz3D. Figure 2 shows
visualizations of a synthetic data set known as Quadruped
Mammals1, widely used to test and train classifiers, which
has 158 72-attribute records. Each data record gives a
cylinder representation to the eight members (head, tail,
four legs, torso and neck) describing four different animals
(1-cat, 2-dog, 3-horse and 4-giraffe). Figure 2(a) shows a
Viz3D visualization with attribute axes ordered as they
appear in the data source, which results in an overlapping of
Groups 1 (cats) and 2 (dogs). Figure 2(b) was produced
with the same data source and technique, but utilizing
SBAA with the similarity measure given by Eq. 3. The
1

http://www.ics.uci.edu/~mlearn/MLRepository.html

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

4
1

2

6

3
1

Box 3 Algorithm mSBAR.

This variation, entitled mSBAR (modified Similarity
Based Attribute Reduction), yields different results than
SBAR, but there is no significant difference in execution
times, since similarity information does not have to be
recalculated at each step.

7

4

8

6
5

8
3

5
2

Figure 3 Viz3D visualizations of Synth3 data.

The visualization in the left was created using the
original attribute sequencing in the data source, and the one
in the right uses the sequence arrangement determined by
algorithm SBAA with the similarity measure given by Eq. 3.
In both cases, all attributes were used to generate the
visualizations. Note that in the left one can visually identify
only seven out of the eight groups in the data, whereas in
the right one all eight clusters are identified. One may also
analyze these results by inspecting the intra- and intercluster distances computed for the data clusters as defined
in the original 20-dimensional data space and in the 3D
projected Viz3D visualization spaces. Measures of cluster
quality are typically obtained from some distance
computation in the n-dimensional data space. One may take
as a possible intra-cluster distance measure the average
distance amongst all cluster elements [13]. However, such a
measure is not suitable for clusters with elongated shapes.
We consider the cluster as a complete graph in which
records are nodes, and take the minimal set of shortest
edges required to connect all cluster records into a single
2

http://fipp.unoeste.br/~almir/visualization.htm

connected component. The intra-cluster distance is then
taken as the average edge length of this edge subset. The
inter-cluster distance measure between any pair of clusters
may be taken as the distance between their centroids [13].
However, again, the measure is not suitable for elongated
clusters. We take the inter-cluster distance between any two
clusters Sk and Sl as the shortest distance between any two
records ri ∈ Sk and rj ∈ Sl. Table 1 shows the inter-cluster
measures computed for two pairs of clusters – (1, 5) and (2,
5) – in data set Synth3. Table 1 (a) shows the inter-group
Euclidean distances of the selected pairs in Synth3,
computed from the attribute values as given in the original
20-dimensional data space. Table 1 (b) shows inter-group
distances obtained from the three-dimensional Viz3D
projected coordinates, considering the attributes in the same
sequence given the data set. Finally, Table 1 (c) shows the
distance measures also obtained from the Viz3D projected
coordinates, but now performing the projection from the
attribute sequence arrangement generated by algorithm
SBAA with the similarity measure given in Eq. 3. The
entries in Table 1(b) show the distances between cluster
pairs #1 and #5 (equal to zero after rounding to two digit
precision) and #2 and #5 (equal to 0.07). One may compare
those with the distances computed for the same clusters in
the original 20-dimensional data space, shown in Table
1(a). After arranging the attributes with SBAA and
generating again the Viz3D projection, the clusters are
much better separated, as it may be observed both in Table
1(c) and in Figure 3(b).
Table 1 Relative inter-cluster distances for data source

(a)
(b)
(c)

Cluster #5 vs. Clusters #1 and #2
#1
Original, 20 attributes
1158.2
Viz3D, original sequence
0.00
Viz3D, SBAA sequence
419.2

#2
572.1
0.07
52.7

In the particular case of cluster pairs (#1, #5), and (#2,
#5), the distances are now around 419 and 53, respectively
and the smallest inter-cluster distance is now equal to 1.0
(between clusters #4 and #7), enough to obtain a good
visual separation of the groups.
The SBAR and mSBAR algorithms also offer a dimension
reduction strategy that may be useful to reduce visual
cluttering when the number of axes is very high, while
preserving the structure of the data in the original space.
This is illustrated in Figure 4, which shows Parallel
Coordinates visualizations of the Quadruped Mammals data
(global normalization applied). The visualization in 4(a)
was created with the original attribute sequence in the data
source, while that of 4(b) was generated by SBAA, using
Eq. 3. The arrangement of attributes in 4(b) reveals more
clearly the underlying data patterns and allows identifying
those attributes that effectively separate the four classes.
4(c) shows the visualization obtained using the 30 most
dissimilar attributes selected by SBAR (again using Eq. 3).

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

(a)

(b)

(c)

Figure 4 Parallel Coordinates visualizations of
Quadruped Mammals data.

Comparing the visualization in 4(b), which employed the
72 attributes, with the one in Figure 4(c), created with just
30 attributes, it can be seen that the latter preserves the
structure of the data quite well.
In fact, we computed the cluttering metric defined in
Eq. 4 for all possible attribute sequence arrangements that
could be generated for three data sets: Iris1, Cars4 and
Pollen3. We thus generated the 24 possible arrangements
for the Iris (4 attributes); the 720 possible arrangements for
the Cars (6 attributes); and the 120 possible arrangements
for the Pollen (5 attributes). For each one of these data sets
the arrangement obtained with SBAA is also the optimal one
according to this metric.
In Figure 5 we show some results obtained with SBAA
on a real data set and compare these with results from the
CDBR attribute layout strategy [11]. The Rubber4 data has
30 six-attribute records (a1: observation, a2: hardness, a3:
tensile.strength, a4:abrasion loss, a5:ts.low, a6:ts.high),
originated from an industrial experiment that collected three
measurements, a2, a3 and a4, taken for a sample of thirty
rubber specimens.
a4

a1

a2

a5

a3

a6

a1

(a)

a3

a5

a6

a2

a4

(b)

Figure 5 Visualizations of the Rubber data.

Figure 5(a) depicts the SBAA result, whereas 5(b)
shows the CDBR result. One notes that the SBAA ordering
is quite effective in enhancing the perfect relationship
between attributes a5, a3 and a6. In this case £SBAA = 218.62
and £CBDR = 274.78 and, in fact, poly-line length metrics

3 http://lib.stat.cmu.edu/datasets
4 http://davis.wpi.edu/xmdv/datasets.html

£SBAA has actually the smallest value amongst the (n=6)!
possible axes configurations. The £ metrics for SBAA and
CDBR have also been computed for the Iris data set, and
again we verified that £SBAA < £CBDR < £original.

6. Concluding Remarks
We introduced a strategy that relies on dimension
reduction and attribute configuration arrangement to
alleviate clutter in visualizations of high-dimensional data.
We have presented three new algorithms: SBAA, SBAR and
mSBAR, primarily based on similarity among attributes, and
showed some results of applying them to several data sets.
Despite their simplicity, the algorithms demonstrate
important qualities as related to the visual analysis of
groupings, in that, in all observed cases, their application
yielded better visual group separation than arbitrary order
attribute displays.
The proposed strategy deals simultaneously with both
dimension ordering and dimensionality reduction in
visualizations in a single integrated framework that can be
easily handled by a user, even at early exploratory stages
when little is known about the data. Using the ordering
algorithms to reduce data dimensionality has also proved
effective, in that, even after eliminating some attributes one
can still identify the structure of the data in the
visualization. This is particularly important to support early
exploratory visualizations of high-dimensional data, where
sometimes the user has no clue on which attributes are
relevant and no dimension reduction or feature selection
has yet been applied.
We have also proposed some objective evaluation
metrics to measure the improvement caused by the
dimension ordering approaches introduced in this paper.
For the Vis3D technique inter- and intra-cluster distances
are used to measure the quality of the display before and
after ordering. For Parallel Coordinates we show that the
total length of the poly-lines provides a measure of the
clutter of the display.
Execution times of the algorithms are low enough to
make their application to large, high-dimensional data
sources viable (450.9 sec. for thousands of 100-attributes
data). Most of the processing effort lies in the computation
of matrix S, which has n2/2 elements for an n-dimensional
collection of records. Matrix S also determines the storage
space required.
The ease of substituting the similarity measures is
another interesting feature. In fact, the choice of similarity
measures to compute the attribute sequences merits
additional research. More sophisticated strategies to provide
users with mechanisms for a more informed decision on
which attributes to keep and which to remove, based on
factors other than similarity or correlation alone – e.g., the
visualization task – and the integration with other feature
selection approaches also deserve further research.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

Acknowledgements
This research was funded by FAPESP – the State of
São Paulo Research Foundation (Grants 01/07566-2,
04/01756-2) and CNPq – the Brazilian Federal Research
Foundation (Grants 572560/1998-2, 521931/97-5). H.
Levkowitz was a Fulbright U.S. Scholar to Brazil from
August 2004 till January 2005 (Fulbright Fellowship).

References
[1]

[2]

[3]
[4]

[5]

[6]
[7]
[8]

[9]

[10]

[11]

[12]

[13]
[14]

[15]

[16]

M. Ankerst, S. Berchtold, D.A. Keim; Similarity Clustering
of Dimensions for Enhanced Visualization of
Multidimensional Data, Proc. IEEE Symp. on Information
Visualization, pp. 52-60, 1998.
A.O. Artero, M.C.F. Oliveira; Viz3D: Effective Exploratory
Visualization of Large Multidimensional Data Sets, Proc.
Brazilian Symp. on Computer Graphics and Image
Processing, pp. 340-347, 2004.
T.H. Cormen, C.E. Leiserson, C. Stein; Introduction to
Algorithms, MIT Press, Second Edition, 1180p, 2001.
C. Faloutsos, K.I. Lin; Fastmap: A Fast Algorithm for
Indexing, Data Mining and Visualization of Traditional and
Multimedia Datasets, Proc. ACM SIGMOD Int. Conf. on
Management of Data, pp.163-174, 1995.
P.E. Hoffman; Table Visualizations: A Formal Model and
its Applications. Doctoral Diss., Computer Science
Department, University of Massachusetts Lowell, 1999.
A. Inselberg; The Plane with Parallel Coordinates, The
Visual Computer, Vol. 1(2), pp. 69-92, 1985.
A.K. Jain, M.N. Murty, P.J. Flynn; Data Clustering: A
Review, ACM Computing Surveys, Vol. 31(3), 1999.
L.L. Karg, G.L. Thompson; A Heuristic Approach to
Traveling Salesman Problems, Management Sci. Vol. 10,
pp.225–248, 1964.
L.C. Molina, L. Belanche, A. Nebot; Feature Selection
Algorithms: An Survey and Experimental Evaluation, Proc.
IEEE Int. Conference on Data Mining, Vol. 1, pp.306-313,
2002.
K. Pearson; On Lines and Planes of Closest Fit to System of
Points in Space, Philosophy Magazine, Vol. 6, pp.559-572,
1901.
W. Peng, M.O. Ward, E.A. Rundesteiner; Clutter Reduction
in Multi-dimensional Data Visualization Using Dimension
Reduction, Proc. IEEE Symposium on Information
Visualization, pp. 89-96, 2004.
W. Peng; Clutter-Based Dimension Reordering in MultiDimensional Data Visualization, MSc. Dissertation,
Worcester Polytechnic Institute, 2005.
J.T. Tou, R.C. Gonzalez; Pattern Recognition Principles,
Addison-Wesley, 1974.
C. Traina JR, A.M.J. Traina, L. Wu, C. Faloutsos; Fast
Feature Selection using Fractal Dimension. Proc. 15th
Brazilian Symposium on Databases, pp.158-171, 2000.
J. Yang, M.O. Ward, E.A. Rundensteiner; Visual
Hierarchical Dimension Reduction for Exploration of High
Dimensional Datasets, Proc. ACM Symposium on Data
Visualisation, pp.19-28, 2003.
J. Yang, W. Peng, M.O. Ward, E.A. Rundensteiner;
Interactive Hierarchical Dimension, Spacing and Filtering
for Exploration of High Dimensional Datasets, Proc. IEEE
Symp. Information Visualization, pp.105-112, 2003.

