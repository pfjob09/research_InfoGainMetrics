A Graph Theoretic Framework for Trust - From Local to Global
Paul Sant and Carsten Maple
University of Luton
Department of Computing and Information Systems
Park Square, Luton, LU1 3JU, United Kingdom
{Paul.Sant,Carsten.Maple}@luton.ac.uk
Abstract
Traditional approaches to trust, be it in agent-based societies, or within a more theoretical framework often consider trust to be a local phenomenon. Here we propose that
trust should be viewed from a global perspective. Our motivation is the area of pervasive computing although we believe that our formal framework applies in many domains.
Here we present our framework and formalise it in the form
of Graph Theory. We present some open problems and discuss the wider application of our work.

1 Introduction
Trust has many definitions (and many feel that trust is
difficult to define). One definition of trust (the one we shall
use here) is: An individual’s trust is the degree of belief that,
for a particular situation, an entity (an individual or a software agent) has the capacity to harm the individual but is
not expected to exercise this capacity. Trust has a number
of levels, for example if someone tells us that they are Joe
Bloggs and then shows us some official identification which
says they are Joe Bloggs, then we will have a certain trust
that the individual is who they say they are, but also we
have trust in the authority that issued the badge, that they
have certified that this person is Joe Bloggs.
Over the past few years Graph Theory has been applied
to a wide range of different problems. Here our aim is to
view trust from a graph theoretic perspective. Some of the
previous approaches to trust have involved logical models
[1]. Recently there has been some work on applying graph
theoretic techniques to trust. This is a natural formulation
and our aim here is to apply a formal graph theoretic model
of trust.
Previous work on trust has mainly concentrated, or perceived trust from a local perspective. This means that there
are several important elements that are not fully encompassed by such methods. In this paper we provide a formal
Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

graph theoretic framework for trust which treats trust from
a wider global perspective rather than a local one. We believe that this way of looking at trust provides a richer model
and allows a more realistic view of trust. Our viewpoint
here is from the notion of pervasive computing (e.g., mobile phones, computerised home systems etc.) which are of
increasing importance in today’s increasingly computerised
world. The future of computing may lie in such embedded systems. It is out of the scope of this paper to provide
a comprehensive background of pervasive computing. The
interested reader should consult, for example [10, 11] for
the background of such systems.
Within heavyweight models of trust, there has been a
great deal of work undertaken that attempts to model trust
based on experiences in areas such as sociology, psychology, economics, management and political science. Models have been built that are very sophisticated and consider
many factors in an attempt to make them as realistic as possible. A lot of work has been based on the study by McKnight et al. [14]. In that paper, trust is conceptually classified into the following categories:
• Dispositional: This relates to the natural inclination of
one entity to trust another.
• Situational: The trust of an entity of a particular scenario.
• Structural: This is when an entity trusts impersonally
the structure that another entity belongs to.
• Belief: When one entity believes another to be trustworthy.
• Intentional: When one entity is willing to depend on
another.
Dispositional trust is defined to consider basic notions:
faith in humanity and trusting stance. Faith in humanity regards the assumption that an entity believes that the another
is well-meaning and dependable. The notion of trusting

stance is based upon the assumption that better outcomes
result from dealing with entities or people as if they were a
priori well-meaning and dependable. Situational trust relies
on an assumption that trust can be gained when there exists
a belief that the environment is in good order and success is
likely, since the situation is normal or favourable. Structural
assurance is based upon the assumption that the structure of
an institution to which an entity belongs, increase the likelihood of success. Examples of such structures might be regulations, legal considerations or guarantees. Trusting beliefs
refer to the attributes of one entity and how a trusting entity
can view these attributes as being beneficial to achieving
some goal. There are three categories of beliefs defined in
McKnight et als work: competence (the ability of a trusted
entity to do what the trusting entity needs), benevolence (the
motivation a trusted entity to act in the trusting entitys interests) and integrity (the trusted entitys honesty and ability
to keep a promise). Trusting intention relates to a trusting
entitys willingness to depend, or intention to depend, upon
a trusted entity. Using McKnight et als work as a starting
point, many have gone on to develop the work in an attempt
for greater realism, contextualism or greater formalism. A
good example of a formal method developed directly upon
the work of McKnight is that of Carbone et al. [4].
The remainder of this paper is structured as follows. In
the next section we discuss the previous work in the area
of trust. We discuss several models that have been previously proposed and set the background for our own model.
In Section 3 we provide the preliminary material required
in the remainder of the paper. We introduce our graph theoretic formalism for trust and trust networks. For an introduction to graph theory see, for example, [5, 6]. We follow
on from Section 3 by presenting our formal graph theoretic
model in Section 4. Here we discuss how trust is not a local
phenomenon, but rather a global one. We present several
examples to support our arguments and we provide a rationale for considering trust as global rather than simply local.
We argue that a local view of trust is too limited and that the
global view provides a much richer model, which is both
realistic and useful.
In section 5 we discuss some of the open problems related to our model, providing a detailed explanation of each
and provoking some thoughts for further work.
Section 6 suggests further avenues of exploration and we
conclude the paper in Section 7.
Let us begin by discussing the previous work on trust
which will set the scene for our approach.

2 Previous work
In this section we discuss the previous work on trust and
use this work to put our own work into context. We do not
intend to cover all of the known systems, but we choose to
Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

discuss those most relevant to our own work.

2.1

Golbeck - trust on the semantic web

There has been much previous work on trust, see, for
example [9]. The most recent is that of Golbeck who has
developed a model of trust and defines the notion of a
Trust Network [9, 7]. The work of Golbeck has lead to a
real-world project which has seen great success [8]. The
FilmTrust initiative is a system in which users can rate films
on the basis (or at least use the perceptions of others to see
whether a film is worth watching) of the views of others. In
essence this is based on a network of “friends” who submit
film reviews. Here there is a notion of trust - how much do
you trust the views of others ? The FilmTrust network is an
ongoing project and is the subject of some recent research
[8]. For further information the interested reader can visit
trust.mindswap.org/FilmTrust.

2.2

ReGreT

The work of Sabater represents another model for trust.
In this case the parties involved are autonomous agents.
In his thesis [12] (and several other papers, for example
[16, 17]) Sabater et al. describe the ReGreT system. This
system is tested using an economic market where there are
sellers and buyers. Sabater defines SuppWorld to represent this scenario and discusses the issues of trust. Within
Sabater’s work he discusses many factors that can affect
the level of trust that an agent, a gives to another agent b.
Sabater uses the notions of direct trust, witness reputation,
neighbourhood reputation and system reputation.
The work of Sabater therefore provides a rich model for
trust which considers many factors. This is realistic as if one
thinks about the real world we (as humans) take many different factors into account when trusting (or rather, deciding
whether to trust) someone. In the human context we take
into account factors such as : views of colleagues, views of
close friends, whether we have had bad experiences in the
past etc. Therefore the model of Sabater closely fits a human social model. Due to lack of space we cannot provide a
comprehensive discussion of the ReGreT system, but the interested reader can consult [12] for a detailed discussion of
both the ideas behind ReGreT and the intricate implementation details.
The two approaches above are not really graph theoretic
(although they do, from time to time, discuss some graph
theoretic concepts).
The final system that we will discuss is one that was developed by Raph Levien [13], a PhD student at UC Berkeley. The system that Levien has developed is called Advogato and we discuss this system in the next section.

2.3

Advogato

The Advogato system [13] was developed by Raph
Levien during his thesis work at UC Berkeley. This system uses the graph theoretic concept of Network Flow from
some source node, s, to some sink node, t. This framework
is used to obtain a value of trust for two communicating
parties. This theoretical work lead to the implementation
of Advogato, a system which combats phenomena such as
SPAM. The advantage of the Advogato system is that it is attack resistant and therefore provides a strong model of trust.
Let us provide a brief overview of the graph theoretic
model used by Advogato.
In the Advogato trust model, given a so-called certification graph Gc = (V, E), a source node s ∈ V and a sink
vertex, t the evaluation of a trust model M results in a real
number which represents how much trust s has in t. In their
model Levien et al. represent the trust as some threshold
θ(s) which determines whether t is trustworthy. In their paper [13] Levien and Aiken present a number of attacks that
can be performed against their model and show that their
proposed model is attack resistant. Their paper then goes
on to rigorously define the reliability of their trust metric.
Due to space limitations we cannot detail this here, but the
interested reader can consult [13] for further details.
The Advogato approach is one which considers (to some
extent) the global view as it considers paths through the certification graph, and as it is doing so, it takes into account
global information. The system of Advogato is somewhat
similar to our work, but there are many ways in which the
two models differ.
Now let us turn our attention to the preliminary material
(graph theoretic) required for this paper.

3 Preliminaries
Let us start by defining the notion of a Graph of Trust,
GT = (V, E). Here, the vertex set V consists of a human agent, H and a set A = {a 1 , a2, . . . , am−1 , am } of m
agents i.e., V = H ∪ A. Here our agents are computerised
entities that can act on our behalf. The size of V , denoted
by |V | is m + 1. The edge set E = {(u, v) : u, v ∈ V }
represents a notion of u trusting v. G T is a directed graph
in which an edge e = (u, v) ∈ G means that an agent (human or software) trusts another agent. However, this is not a
symmetric relation since ∃e = (u, v)
∃e = (v, u). That
is, it is not the case that if u trusts v that v necessarily trusts
u back.
Figure 1 shows a sample Graph of Trust, G T , with a single human agent H, and a set of seven (software) agents,
A = {a1 , a2 , . . . a7 }.
In our framework we define the neighbourhood of an
agent (human or software) x i ∈ V as Nxi = {xk :
Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

a3

a2

IEEE

H

a7

a6

a5

a

1

a

4

Figure 1. An example Graph of trust GT .

(xi , xk ) ∈ E}, i.e. the neighbourhood of the agent x i are
those nodes, xk , for which there exists an edge between x i
and xk in the Graph of Trust, G T .
Now that we have defined the preliminaries we can now
go on to define (and later, discuss) our formal model for
trust.

4

Formal model

Within our formal framework for trust we take a graph
theoretic approach. This is a natural formulation and stems
from much work in Social Network analysis (see, for example, [2, 3]).
The work of Buskens [2] is an important precursor to our
own work. In [2] a global perspective is discussed which is
based upon the notions of density, centralisation and outdegree (i.e. the number of arcs emanating from a particular
vertex v).
There are parallels between our work and that of Buskens
[2, 3] and in order to get a richer understanding of the ideas
and approaches that we present in our paper the interested
reader may want to consult [2] for further information.
Let us now turn to look at the notion of trust as both a
local and a global phenomenon.

4.1

Trust - a local view

The natural view of trust is a local one. For example,
consider the scenario where we are using a mobile phone to
access online services, for example to surf the web. In this
scenario there is only one place where authentication takes
place, namely when the user turns on the phone. This means
that if a phone is left unattended, it is possible for a malicious user to come along and carry out malicious attacks on

others without any further kind of authentication. The local
view of trust can be seen in Figure 2.

N
H

even though we have a high level of trust in our own agent,
we cannot know (if we only consider a local view) whether
the agents with which our own agent will interact, have been
compromised in some way.
Figure 3 shows an example of such a network.

D

a3

a2

A
M
Figure 2. A local view of trust.

H

Here the only authentication point is at the point, G.
Here, G may represent a device, agent, or even perhaps another human being (although in our case it would usually be
an agent). This means that once this authentication is successful then three types of services can be accessed (without
any further authentication): Normal services (used in everyday situations), Abnormal services (which are events which
may be flagged as representing an action which is suspicious) and Malicious services (which may be triggered as a
result of an agent being compromised).
Many current technological devices, e.g., mobile phones,
provide the possibility to authenticate each time a service is
required, e.g., enter you pin every time you want to make a
call. However, many users find this a burden and simply turn
off such authentication services. Here we stress that having
some form of global authentication mechanism is extremely
important and in the following section we discuss our model
of global trust.

4.2

Trust - a global model

When making a decision to trust an entity (be it a human
being, computerised system, or an agent) we often take a
global view of the world. We usually make judgements on
the basis of things such as: views of our peers, (in the case of
a political party) previous history of the party involved etc.
We do not simply rely upon local information, but rather we
incorporate global information into our decision to trust the
entity.
As a concrete example, consider a mobile phone device
which also allows us to employ an agent to autonomously
buy and sell products. Such a system may involve a network of agents buying and selling products (with a human
authenticating to their agent to act upon our behalf). In this
case, since we are enabling a agent to act upon our behalf we
need to ensure that we can obtain a global view of the world
(in this case a system of software agents which are trading
between one another) since it would be naive to assume that
Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

a7

a6

a5

a

1

a

4

Figure 3. Beyond a local view of trust.

Within any trust network there are a number of points
of potential weakness (for example, the agents can be compromised) therefore a global view is important in order to
ensure that the potential for any malicious event occurring,
is minimised.
It is important to note that if we build a so-called trust
chain which incorporates a number of entities then this
chain is only as strong as the weakest link (i.e., the entity
which is easily compromised). In addition, increasing the
length of this chain can only weaken the chain, or at best,
maintain the current level of trust.
Of course, a global view of things is not the only issue
with which to contend. Trust has an inherent temporal dimension. Both humans and software agents use previous
experience when deciding upon a level of trust and we turn
to this issue in the next section.

4.3

Trust - temporal issues

Here we assume a dynamic value for trust (i.e., there is
a temporal dimension). That is, we start with some initial
value, δ for trust and then we reach an authentication point.
We define time points t = {t0 , t1 . . . tn } as discrete time
steps and we define the trust value, δ as the level of trust
in an agents ability to assist in achieving a goal and that
during this time the agents (authenticity) has not been compromised. If such authentication is successful then we see
an immediate (substantial) increase in the trust value (representing the fact that we are who we say we are). We then see

a degradation of trust until we reach the next authentication
request which, if successful, will see an increase in the level
of trust. This pattern repeats over time. We can therefore
view trust as a saw function (see Figure 4).
We denote the trust between two entities a and b as
T (a, b) = δ.

Level of Trust

T(a,b)

degradation

t
0
auth1

Time
auth
2

auth3

Figure 4. A function for measuring trust (simple case).
We modify the standard saw function to account for the
fact that there may be a previous history of distrust for some
agent, ai . In the case where there is a previous history of
distrust the speed at which trust degrades will see a sharp
increase (i.e. the trust value will drop significantly faster
than normal). Our modified trust function can be seen in
Figure 5.

Level of Trust

T(a,b)

malicious event

t
0
auth1

Time
auth2

auth3

Figure 5. A trust function which accounts for
distrust.
Why have we chosen to use such a saw-like function ?
The answer is that we believe that this is a realistic view
of how trust works in humans. Initially, when we meet a
person for the first time then we have an initial (probably
low) level of trust in this person. Once this person authentiProceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

cates themselves then our level of trust in this person will increase. We note here that if an entity authenticates with the
same identification at each authentication point, auth i then
it is realistic to assume that the level of trust which another
entity has in them will always rise by an equal amount, α.
If, however, the entity authenticates with a different form of
identification then this increase in trust will not necessarily
be constant. The level of trust will degrade over time until,
at some point in the future, we meet this person again. Obviously, in the case where we are in constant contact with
someone, then it is probably the case that our trust levels
(in this person) will increase, or at the very least remain at
the same level (assuming that they do not take part in some
event which is malicious, in which case our level of trust
will decrease).
We generally consider networks of entities (which can
be human or software agents) and in a pervasive computing
environment it may well be the case that the majority of
communication is between different software agents, with
an initial authentication at time t = t 0 by a human.
However, our model applies to the general case (for example insurance brokers) in which a human may ask another
entity (e.g., an insurance company) to act on their behalf. In
this case, the insurance company may well need to communicate with many human brokers in order to achieve the
best possible quote. Therefore, our model works in the case
where the network consists of both humans and software
agents.
As an example to put forward our reasoning consider a
home utilities scenario. In this scenario we have a home
owner, H and a utilities engineer, E (e.g., a gas engineer).
When H first meets E there will be a low level of trust (here
we denote trust by δ). In order for H to allow E into their
home to fix the boiler, E will need to authenticate (e.g., show
an official badge) themselves to H. Now if E successfully
authenticates themselves to H then this level of trust, δ that
H has in E will increase (this will be instantaneous) and E
will be admitted into the house to fix the boiler. Conversely,
if E cannot authenticate themselves then the level of trust, δ
that H has in E will drop sharply. Similarly, if E successfully authenticates themselves, but the repair work is poor,
or he defrauds H then the level of trust that H has in E will
drop sharply.
Now, assume that H has their boiler fixed every year. If
E is the same person from year to year, and they perform
the same tasks, ζ, every year then H will build up a level of
trust with E and it may be the case that H’s level of trust, δ,
(between yearly visits) may degrade at a much slower rate
(call this degradation rate, λ ) than if a different E visits each
year. However, if the same E turns up each year, but they
have performed malicious tasks in the past then the trust
level that H has in E will be likely to degrade at a much
faster rate.

This is the idea behind our modified saw function. If
a subject has had bad experiences in the past (time steps
t0 , t1 , . . . , tk ) then this will be taken into account at time
point, tk+1 and the speed of degradation will be modified
accordingly.
There are many other scenarios that can be used to support our suggested trust function, for example: access to
mobile phone services, single sign-on systems, multi-level
management systems etc. All of these seem to point towards
the functions that we suggest.
A further issue with which we need to contend is the
scale with which we measure trust. There are many possible scales, for example the Trust Networks (FilmTrust) of
Golbeck [9] use the scale [1 . . . 10] with 1 representing complete distrust, 5 representing neutrality and 10 representing
complete trust (with varying scales between 2 ≤ k < 5 and
5 < k < 10). In contrast, the ReGreT system of Sabater
[17] and the Advogato system of Levien [13] use different
scales. The result is, however, similar in each case.
In essence the scale chosen is somewhat irrelevant. As
long as there is a way of distinguishing between trust and
distrust the choice is essentially left to the modeller. Here
we choose the level of trust to be a real value in the interval
[−1, 1] with -1 representing complete distrust and 1 representing complete trust (0 being neutrality).
In our framework the trust value is represented as a
weight, w(e) for each e ∈ E of the Graph of Trust G =
(V, E).
Let us now turn to some questions and open problems.

5 Open problems
In this section we discuss some open problems and questions whose answers would prove useful for our framework.
One natural question that arises is the following: “At
what point do we know enough global information to make
a confident judgement of trust ?”. This is an important question. Obviously we could choose to observe the whole of the
Graph of trust, but as we collate more information together
the problem becomes more complex. Therefore there must
be a cut-off at which we know enough information to make
a strong judgement and that at which the usefulness of any
further information becomes negligible.
Another interesting area for consideration is whether
trust is transitive. That is, given three parties, P, Q and
R, if P trusts Q and Q trusts R then should P trust R ?
Additionally, what level of trust, δ, should P attach to R ?
In relation to levels of trust and transitivity there are several possible values for δ. We could of course choose
δ = φ+ψ

0-7695-2602-0/06 $20.00 © 2006

IEEE

Another factor to consider in the calculation of δ is the
distance, k (number of edges) between the parties in the
Graph of Trust, G. This adds yet another parameter (a somewhat important one) in the calculation. One suggestion that
we have come up with is:δ = φ × ψ − k(d)

(3)

, where d is some constant and k is the distance between the
parties in G. Does such a calculation provide a good value
for the level of trust ? This is an open question and one that
we hope to address in our future work.

6

Further work

In our future work we aim to undertake real world experiments to modify and enhance our formal model of trust.
We would like to perform user experiments to see just how
accurate our model is (compared to the real world). We believe that models of trust need to be rich, yet not so complex
that they become unusable.
We aim to follow up this publication with others and to
establish our model within the community.

7

Conclusions

In this paper we have presented a graph theoretic model
for trust. This model includes both human and software
(computer-based) agents. We have discussed some of the
issues associated with trust and compared our model to others, e.g., the Trust Network of Golbeck et al. [9], the ReGreT system of Sabater [17] and the Advogato system of
Levien [13].
It is important to note that there are parallels between our
work and that of Levien. However, here we have introduced
a system that is (to the best of our knowledge) completely
novel. We have stressed here that trust is not merely local,
but rather it is a global phenomenon. We believe that it is
important to take a global view in order to ensure a high
(and accurate) level of trust in networks (agents, pervasive
systems etc.).
We strongly believe that our model accurately models
trust and we hope that our future work will build upon the
foundations given here.

References

(1)

, where φ is the trust that P has for Q and ψ is the trust that
Q has for R. This is, however, somewhat unrealistic since
Proceedings of the Information Visualization (IV’06)

we do not necessarily know R. Perhaps a more realistic
value is :δ =φ−ψ
(2)

[1] J. Audun, E. Gray, and M. Kinateder. Simplification and
analysis of transitive trust networks. Web Intelligence and
Agent Systems, 2005.

[2] V. Buskens. The social structure of trust. Social Networks,
20:265–298, 1998.
[3] V. Buskens. Social networks and trust. PhD dissertation,
Utrecht University, 1999.
[4] M. Carbone, M. Nielsen, and V. Sassone. A formal model for
trust in dynamic networks. In Proc. of the First International
Conference on Software Engineering and Formal Methods,
pages 54–61, 2003.
[5] T. H. Cormen, S. Clifford, C. E. Leiserson, and R. L. Rivest,
editors. Introduction to Algorithms. MIT Press, second edition, 2001.
[6] A. Gibbons, editor. Algorithmic Graph Theory. Cambridge
University Press, 1986.
[7] J. Golbeck. Semantic, web interaction through trust network
recommender systems. In End User Semantic Web Interaction Workshop at the 4th International Semantic Web Conference, Sept. 2005.
[8] J. Golbeck. Generating predictive movie recommendations
from trust in social networks. In Proc. Fourth International
Conference on Trust Management, page To appear, Pisa,
May 2006.
[9] J. Golbeck, B. Parsia, and J. Hendler. Trust networks on
the semantic web. In Proc. Cooperative Information Agents
2003, Nov. 2003.
[10] R. Grimm, T. Anderson, B. Bershad, and D. Wetherall. A
system architecture for pervasive computing. In Proc. Ninth
ACM SIGOPS European Workshop, Sept. 2000.
[11] R. Grimm, J. Davis, B. Hendrickson, E. Lemar, A. MacBeth,
S. Swanson, T. Anderson, B. Bershad, G. Borriello, S. Gribble, and D. Wetherall. System directions for pervasive computing. In Proc. HotOS 2001, May 2001.
[12] J.Sabater. Trust and Reputation for Agent Societies. PhD
dissertation, Institut d’ Investigaci´o Intel-lig`ecia Artificial,
2003.
[13] R. Levien and A. Aiken. Attack resistant trust metrics for
public key certification. In Proc. of the 7th USENIX Security
Symposium, pages 265–298, Jan. 1998.
[14] D. McKnight, V. Choudhury, and C. Kacmar. Developing
and validating trust measures for e-commerce: An integrative typology. Information Systems Research, pages 334–
359, 2005.
[15] J. Sabater and C. Sierra. Regret: A reputation model for gregarious societies. In Proc. Fourth Workshop on Deception,
Fraud and Trust in Agent Societies, pages 61–70, 2001.
[16] J. Sabater and C. Sierra. Social aspects of regret: a reputation model based on social relations. In Proc. 5`e Congr´es
Catal`a d’ Intel.lig`encia Artificial (CIIA02), pages 336–343,
Oct. 2002.
[17] J. Sabater and C. Sierra. Social regret: a reputation model
based on social relations. In Proc. SIGecom (ACM), pages
44–56. ACM, Mar. 2002.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

