2012 Ninth International Conference on Computer Graphics, Imaging and Visualization

Real-time Hand Gesture Recognition from Depth Image Sequences

Hong-Min Zhu and Chi-Man Pun
Department of Computer and Information Science
University of Macau, Macau SAR, China
{yb07422, cmpun}@umac.mo

Abstract‚ÄîAs a certain case in the domain of human actions,
hand gestures can be expressed by the motion of user‚Äôs hand to
provide nature interaction in many applications. In this paper
we proposed a real-time hand gesture recognition system based
on robust hand tracking from depth image sequences. Using
hidden markov models (HMM) with varying states, gesture
models are trained online along with user‚Äôs feedback, and the
real-time classification is taken simultaneously. A gesture may
be falsely classified as the models are trained insufficiently at
beginning, in which case we provide a feedback and update the
gesture model with this gesture sample. The performance of
the system can always be improved by more updating, and in
our experiment we give an appropriate result after a
reasonable number of samples are used for training.

experimental results on hand signed digit gestures. Finally
we conclude the paper in section V.
II.

There exists a wealth of approaches on topic related to
human action and hand gesture recognition. In this section
we review only some of closely related work.
Video representation based on spatial-temporal (ST)
interest points has been studied in several literatures
recently, which is extended from approaches for static
images. The authors in [3] presents a space-time interest
point detector based on the idea of the Harris and Forstner
interest point operators. They detect local structures in
space-time where the image values have significant local
variations in both dimensions. In addition, [4] propose a
detector based on a set of separable linear filters which
generally produces a high number of detections. This
method responds to local regions which exhibit complex
motion patterns, including space-time corners. Also, a
number of descriptors are proposed for cuboids which are
the resulting video patches around each interest point.
Combined with discriminative classifiers to learn and
recognize human actions, the performance on KTH dataset
[5]was improved to 81.2%. The best performance achieved
on the same dataset by combining the cuboids descriptor
with more advanced classification solutions, is the approach
[6] which introduces a discriminative Semi-Markov Models
(SMM) to recognize the descriptor and get the accuracy of
95%. Although the cuboids feature descriptor has been
explored in many works and showed good potentials on
KTH dataset as well as other human actions, it is
unsatisfactory for human actions that the pattern is greatly
rely on relative spatial position and temporal occurrence,
since the feature descriptor as cuboids prototypes ignores
the positional arrangement, in space and time, of the ST
interest points.
With real-time depth cameras, foreground objects can
more easily be segmented from the background. Even with
the detection of human skeleton and silhouette, it‚Äôs possible
to reconstruct the 3D human model and capture the details
of body poses and motions. The authors in [7] introduces an
efficient filtering algorithm for tracking human pose using

Keywords- Gesture recognition; Depth image; Hand tracking;
Online training/real-time classification

I.

INTRODUCTION

Human action recognition is an active area of research
in the vision community, mainly for the purpose of sign
language recognition and Human-computer Interaction
(HCI). After the progress achieved in several decades, even
the best existing systems still exhibit limitations due to
complexities and variances of video applications, such as
cluttered background, camera motion, occlusion, viewpoint
changes, and geometric and lighting conditions variances.
In recently years, the task has greatly simplified by the
introduction of real-time depth cameras such as Kinect[1],
which is released for Microsoft‚Äôs Xbox gaming system. The
main advantages of the Kinect are its low price and the
efficient acquisition of depth data with high resolution
(480x640). In a game system, typically human pose and
body motion can be captured by the camera to provide a
natural interaction. As a certain case of human actions, we
propose a real-time hand gesture recognition system based
on the extraction of hand motion trajectory where the
motion is extracted from depth image sequences. Motivated
by dataset used in [2], we train and recognize hand signed
digit gestures with Hidden Markov Models (HMM).
The rest of the paper is organized as follows. We
review some previous related work in section II. In Section
III we describe our approach of real-time hand gesture
recognition system in detail. In section IV, we present the
978-0-7695-4778-7/12 $26.00 ¬© 2012 IEEE
DOI 10.1109/CGIV.2012.13

RELATED WORK

49

monocular depth images, they uses body part proposals to
tracks the skeleton with kinematic and temporal information.
Another attractive human pose recognition algorithm is
proposed in [8] which uses a single depth image from
Kinect. With per-pixel classification, they represent body
parts and map to difficult pose estimation problem, the
system shows high processing rate (200fps) and robustness
on variance of pose, body shape, and clothing.

with the OpenNI framework and NITE middleware). A
gesture ‚Äúwave‚Äù is firstly performed to activate the hand
tracking process, then the motion of the hand will be tracked
to give the corresponding real-time trajectory. Figure 2
shows a simple example of motion tracking after ‚Äúwave‚Äù
gesture has been detected.

Figure 2. Hand motion trajectory tracked with NITE‚Äôs middleware. Left:
RGB image. Right: depth image with motion trjectory.

We can see that 2D hand locations from hand tracking
form the motion trajectory. To identify the start and end of
gesturing, we continuously check the standard derivation of
a fixed number of last hand locations to determine the state:
(1)

Where TN is the set of last N hand locations of trajectory.
When the hand keeps relatively static, the standard
derivation std(TN) will less than a threshold T, and the
gesturing should then be started/ended. The set of tracked
hand locations during gesturing is used as features after two
processes. Firstly we interpolate
(2)

points into two hand locations with distance D greater than a
threshold D0. Secondly we represent the trajectory as motion
vectors, where each vector is the motion direction from the
current hand location Pi+1 to the next location Pi and is to
be encoded into one of eight code words as:

Figure 1. Workflow of hand gesture recognition system

(3)
III.

REAL TIME HAND GESTURE RECOGNITION SYSTEM

Figure 1 summarizes steps involved in our proposed
real-time hand gesture recognition system. It can be
considered as two main stages: 1). Hand tracking and
feature extraction. 2). Real-time recognition with HMMs,
which involves online training and updating with user‚Äôs
feedback. We will give the details of our system in this
section.

(4)

(5)

(6)

A. Feature extraction of hand motion
As a basis of our system, we use utilize the hand
tracking algorithm from PrimeSense NITE Middleware
(More details can be found in the documentation supplied

50

B. Online HMMs training and Real-time recognition
As a traditional tool for temporal sequence
classification, we adopt HMMs for the recognition of
motion trajectory. HMMs are capable of dealing with
length-varied feature vectors, so that the recognition can be
taken in real-time manner, as the trajectory vector is
considered incomplete before finishing gesturing.
Motivated by the work of [9], we use the Left-Right Banded
(LRB) model as the HMM topology. The initialization of
state transition matrix A and observation matrix B are:
¬≠ N
¬∞1  T if i j z N
¬∞
(7)
if i j N
¬∞1
A {aij} ¬Æ
¬∞N
if j i  1
¬∞T
¬∞0
otherwise
¬Ø

B {bim}

1
M

of HMMs, we summarize the distribution of length of the
easy test set from [2] as Figure 4, and the length of each
class is averaged. Each model is initialized with un-unified
number of states, which are determined by the complexity
and motion direction changes of each class, table 1 gives the
states used for each gesture model. We note that the model
for the simplest gesture ‚Äú1‚Äù has 2 states, and has 6 states for
more complex gestures ‚Äú5‚Äù and ‚Äú8‚Äù.

Figure 3. Hand signed digit gestures[2].

(8)

Where N is the number of states, T is the length of gesture
trajectory, and M is the number of discrete observation
symbols which is 8 code words in our case. In training stage,
a model for each gesture class is initialized and recursively
updated on the given feature vector by BaumWelch
algorithm. In testing stage, an unknown sequence of code
word is classified to the gesture class whose model
generating the sequence with highest probability among all
models, by the Viterbi algorithm.
The HMMs for each gesture class are trained online. In
the beginning, a gestured trajectory cannot be recognized as
its corresponding class by the initial model. The user may
provide a feedback, to indicate the gesture‚Äôs class and times
of updating, so that the gesture model can be updated by the
trajectory sample for a fixed number of times. After all
models are updated sufficiently, gestures can be recognized
in real-time, when the hand motion is tracked, the
classification result is given frame by frame which tends to
be correct as close to the end.
IV.

Figure 4. Length distribution of each gesture class.

Table 1. HMMs states for each gesture class

Gesture

0

1

2

3

4

5

6

7

8

9

State

3

2

4

5

3

6

4

3

6

5

Figure 5 shows the interface of our gesture recognition
experiment. Start with initialized HMMs with varied states,
we perform 20 instances for each class, and update
corresponding models by providing user‚Äôs feedback to
indicate the classes. By default, we update the model once
with an instance. During gesturing for testing, the partial
trajectory sequence can be recognized in real-time, and the
result class ‚Äú8‚Äù comes out when the sequence can be
distinguished from other gesture classes by the models. To
state the performance of the updated models, we tested by
perform 10 gestures for each class. The result is summarized
in table 2, and the overall recognition rate is 92%. We can
see that more complex gestures have a better recognition
such as ‚Äú3‚Äù, ‚Äú5‚Äù and ‚Äú8‚Äù, their trajectories have more
motion vectors changes and thus can be well modeled. The
gesture ‚Äú1‚Äù is more likely to be misclassified due to its
simple motion pattern. And partially similar motion
appearance is another cause of misclassification, such as

EXPRIMENTAL RESULT

We demonstrate our real-time hand gesture recognition
system on hand signed digit gestures (0-9) that the pattern is
represented by the trajectory of single hand motion in front
of Kinect camera. The gestures are defined as the same as
dataset from [2] (the easy test set), Figure 3 shows the
gesture samples with starting position indicated.
For start/end gesturing detection, we check whether the
standard derivation of the last 15 hand locations is less than
5. In the interpolation of trajectory, the distance threshold is
set with 20 reasonably by user‚Äôs experiment. And for the
trajectory length used in the transition matrix initialization

51

gesture ‚Äú0‚Äù and ‚Äú6‚Äù. We state that the performance can be
improved by updating the models with more samples. Our
system runs at 20 fps, which is capable of real-time
processing.

V.

CONCLUSION

Benefit from Kinect camera with depth image
sequences and robust hand tracking provided by PrimeSense
NITE middleware, we propose a real-time hand gesture
recognition system. Hand motion trajectories are modeled
by HMMs, which are updated online with user‚Äôs feedback.
Ten classes of hand signed digit gestures (0-9) are tested in
real-time, while 92% recognition rate achieved with limited
model updating, we conclude that gestures with more
complex structures and motion vector changes can be better
recognized.

REFERENCES
[1].Microsoft Corp. Redmond WA. Kinect for Xbox 360.
[2].Alon, J., et al., A Unified Framework for Gesture Recognition and
Spatiotemporal Gesture Segmentation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 2009. 31(9): p. 1685-1699.
[3].Laptev, I., On Space-Time Interest Points. International Journal of
Computer Vision, 2005. 64(2): p. 107-123.

Figure 5. Example of real-time gesture recognition

[4].Dollar, P., et al., Behavior recognition via sparse spatio-temporal
features. Visual Surveillance and Performance Evaluation of Tracking
and Surveillance, 2005. 2nd Joint IEEE International Workshop on,
2005: p. 65-72.

Table 2. recognition result

[5].Schuldt, C., I. Laptev, and B. Caputo. Recognizing human actions: a
local SVM approach. in Pattern Recognition, 2004. ICPR 2004.
Proceedings of the 17th International Conference on. 2004.

Recognized As
gestures

0

0

9

1

2

7

8

[6].Shi, Q., et al., Human Action Segmentation and Recognition Using
Discriminative Semi-Markov Models. International Journal of
Computer Vision, 2011. 93(1): p. 22-32.

9

2

[7].Ganapathi, V., et al. Real time motion capture using a single time-offlight camera. in Computer Vision and Pattern Recognition (CVPR),
2010 IEEE Conference on. 2010.

1

[8].Shotton, J., et al. Real-time human pose recognition in parts from single
depth images. in Computer Vision and Pattern Recognition (CVPR),
2011 IEEE Conference on. 2011.

10
1

4

[9].Elmezain, M., et al., A Hidden Markov Model-Based Isolated and
Meaningful Hand Gesture Recognition. International Journal of
Electrical, Computer, and Systems Engineering, 2009. 3(3): p. 156-163.

9
10

5

9

6
1

3

8

5

10

2

7

4

7

1

6

3

2

8
1

9
10
10

52

