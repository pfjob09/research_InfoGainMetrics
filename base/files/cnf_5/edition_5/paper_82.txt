Flexible Tracking of Object Contours Using LR-Traversing Algorithm
Russel Ahmed Apu and Marina L. Gavrilova
Department of Computer Science, University of Calgary
{apu@cpsc.ucalgary.ca, marina@cpsc.ucalgary.ca}

Abstract
Tracing the contour of an object in an image is
an important problem in computer vision. This
paper presents a new contour detection algorithm
using an adaptive vision framework. The proposed
method is different from conventional algorithms in
several ways. First, we introduce a new adaptive
tracking algorithm called LR-traversing. LRtraversing is unique as it progressively adapts to the
thickness of an edge while tracking the contour of an
object with variable sharpness. Secondly, the
method employs adaptive selection process that can
optimally extract features based on an error metric.
By utilizing this flexible run-time technique our
method can detect and track object contours in realtime. Experiments demonstrate that the method is
significantly faster than other algorithms that can
achieve similar result.
Keywords--- Adaptive Vision, LR-Traversing,
Active Contour, Tracking, Edge Detection.

1. Introduction

discriminating gradient. For each set of regions
separated by a contour, there exist some quantifiable
distance metric λ such that the numerical
difference between any arbitrary set of intra-regional
sampling is less than that of any arbitrary set of
inter-regional sampling:

⎛ ( ( x, y ∈ C1 ) ∧ ( z ∈ C2 ) ) → ⎞
⎜
⎟
∀x∀y∀z ⎜ ⎛ ( λ ( x, y ) < λ ( x, z ) ) ∧ ⎞ ⎟
⎜⎜
⎟ ⎟
⎜ ⎜ ( λ ( x, y ) < λ ( y , z ) ) ⎟ ⎟
⎠ ⎠
⎝⎝
Here C1 and C2 are two distinct color groups
separated by a perceptible edge boundary.
Contours can divide regions based on their
relative intensity [2], color [6] or even texture [9]. In
this paper, only regions of uniformity are considered
(for textured region, an extra step of texture
discrimination can be applied [12]).

The task of tracing object contour is somewhat a
more difficult problem to solve [2][10]. This is due
to the fact that the contour of an object does not
always conform to continuous string of clearly
identifiable edge [1][13][16][19]. In addition, the
local thickness of the region of contrast may vary
substantially. Thus, tracing the contour of an object
in an image is an important problem in computer
vision [2][11][19].

In this paper we present novel approach outlining
an efficient algorithm called LR-Traversing to trace
object contours in images. This adaptive LRTraversing algorithm is designed so that it can
identify the most potent candidates and
automatically trace the features without any user
interaction (i.e. setting control points). The method
also restricts the run-time by specifying a
computational limit. The features are extracted
within that specified limit. The contours of higher
importance are extracted first. The importance of a
feature can be defined using any arbitrary multicriteria priority function (also known as error
metric). The tracing of the contour is flexible
enough to accommodate variations of contour
thickness and contrast. The result is a highly
efficient optimal contour tracing algorithm that can
effectively perform within a real-time constraint.
Experimental section demonstrates that the
effectiveness of the method is comparable to the
computationally expensive methods.

A contour in the most elementary sense is
defined to be the border of two regions of
discrimination located at the center of the

The paper is organized as follows. Section Two
provides a brief survey of relevant literature. Then,
the method is described in section Three. The

In the vision research community, it is a common
knowledge that edges of a sampled image constitute
significant information that assists peripheral vision
and object recognition [2][11]. A large domain of
research is dedicated to the processing of images to
highlight edges [12][19]. However, edge detection is
not sufficient to differentiate objects of an image or
to identify contours that are apparent to human eye
[3][11].

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

detailed analysis of the method is given in section
Four. Finally, the summary is provided in section
Five.

2. Literature Survey
The early attempts to highlight the borders of
various objects are known as Edge Detection. These
methods can highlight the regions of variation by
means of a differentiating convolution filter. The
most utilized method in that category is proposed by
Canny [5]. The Canny Edge detection algorithm is a
three step process. First, the image is processed
using a Gaussian filter to eliminate high frequency
noise [19]. This is followed by application of a
differential convolution filter [19][20]. Finally, a set
of thinning algorithm is applied known as nonmaximal suppression and hysteresis [5][19].
The limitations of edge detection in general come
from the fact that edge bitmaps often do not carry
useful information to identify or even discriminate
objects [14][17]. A major drawback of the edge
detection methods is that they are not very effective
when there are blurred edges, roof edges or low
contrast regions [19]. In computer vision, it is often
necessary to track a specific area of interest (i.e. a
particular contour) and ignore other unimportant
details [4][8][18]. Edge detection method cannot be
used in such cases. In addition, edge detection
algorithms cannot divide the image in regions of
interest or cluster information to fit a specific
semantic [9].
To avoid this limitation some researchers
proposed to differentiate regions of interest using an
image segmentation method [6][7]. This approach is
computationally more expensive, but is capable of
dividing an image into a specified number of regions
(i.e.
K-means segmentation) [15].
Image
segmentation is advantageous because it can be used
to cluster information into groups or labels that can
be interpreted meaningfully [6][7][9]. However, the
inherent limitation of both edge detection and
segmentation method is that it is dependent on user
defined thresholds that must be adjusted to process a
specific image for a specific vision goal.
A recent approach to address the detection and
tracking of a specific contour (or a region of
importance) is known as the Active Contour [15]. In
active Contour based methods such as Snake [10], a
set of control points is strategically defined by a user
around (or inside) the border of an object. An energy
minimization algorithm is then executed to shrinkwrap the contour to fit the object. However, this
method is computationally expensive. The results
are sometimes reported to be erratic. The method is
not always effective in general cases although some

success is reported in applications such as medical
imaging [15][18]. Another significant drawback of
the method is that it requires too much user
interaction. Motivated by these limitations, the
method devised in this paper is designed to address
the need to develop an easy and efficient method to
trace contours. We devise it in such a way that the
information obtained is in a suitable form (i.e. set of
vertices) that can be used in a high-level vision
algorithm effectively.

3. Method Description
A detailed description of the proposed method is
illustrated in this section. First, a mathematical
definition of the problem is provided. In our
proposed method, the definition of the edge line of a
contour differs from the definition used in an edge
detection algorithm. An edge can be described as
points of maximal variation along a discriminating
region. In contrast, a contour edge is the center-point
of such variations. Each subsequent sub-section
illustrates the different steps of this method
sequentially.

3.1. Definition of a contour edge
A contour edge is the center line border of two
distinct color groups. A color is any perceivable
multi-stimulus photometric intensity C ∈ Ω ( »n ) in
a given Color space Ω emanating from an object. A
color group Grλ (C i ∈ Ω(» n )) is a subset of the
color space Ω , such that there exists a quantifiable
scalar distance metric λ : Ω(»n ) ×Ω(»n ) → » and
a radius r that defines membership of any color
inside that spheroid in the color space of Ω :

Grλ (C i ) = {C j | λ (C i ,C j ) ≤ r }

(1)

An Edge is simply the minimal intersection of the
geometric realization of the abstract simplicial
complexes G1 (R ⊆ I ) and G2 (R ⊆ I ) of Region
R in a bitmap I such that the vertex scheme of the
Gi could be defined as:

⎛⎛
⎞⎟
⎞
V (Gi (R)) = ⎜⎜⎜⎜⎜⎜ ∪ C (p)⎟⎟⎟ ∩ Grλ (C i )⎟⎟
⎜⎝⎜⎝ p∈R
⎠⎟
⎠⎟
In other words, if we look at a region R of an
image I , we may find two distinct color groups that
encompass all pixels in that region. The edge is the
subset R ′ ⊂ R so that R ′ is in both of the color
groups. There can be more than one such
partitioning in a region R . A proper edge is the

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

(2)

partitioning where V (G1 (R )) ∩ V (G2 (R ))
is
minimized. In average case, this minimization can
be approximated by the arithmetic mean of the
gradient curve.
The definition given above makes the assumption
that the region R has only two distinct color groups.
When a region has more than two color groups, it
must be partitioned in such a way such that each
partition has two distinctive groups, as a result the
region contains a junction point where several edges
meet. There are several key properties of edges
according to this definition. The following are
crucial to the design of the methods:
•

Edge Thickness: The thickness of an edge in a
region is a transition from one color group to
the other along the cross section of the edge.
Therefore, an edge more than just a maximal
gradient. The algorithm should be able to track
edges with high thickness and low gradient.

•

Color Variation: Edge is not just photometric
intensity variation; it can also be affected by the
colors of the region. None of the convolution
based methods is able to take this into account.

⎡−1 +1⎤
⎡−1 −1⎤
⎥ K =⎢
⎥
KGX = ⎢⎢
GY
⎥
⎢
⎥
1
1
1
1
−
+
+
+
⎥⎦
⎣⎢
⎣⎢
⎦⎥
Using a larger kernel size and a different edge
detection filter (i.e. Sobel) may yield better edge
quality. However, it is not a required to identify high
contrast image coordinates. Therefore, we use the
simplest 2x2 Kernel to keep the preprocessing
computation time to a minimum.
The typical convolution based edge detection
does not take the color component of the pixel into
account. As a result, edges do not appear the way
they are expected. Often there are gray scale
variations in the RGB values that are insignificant to
human perception but significant for the computer to
produce artifacts. Differentiating shades of color is a
difficult problem by itself. Let us assume that a
distance function Λ exists that measures distance of
between two shades of colors C 1 and C 2 . Color
enhanced edge gradient can be computed as
followed:

ΔC (x , y ) = Λ (I (x , y ), I (x , y + 1))
+ Λ (I (x , y ), I (x + 1, y )) + Γ

3.2. Step 1: The image preprocessing
Our method is designed in such a way that
computation is only applied to the region of
importance (i.e. a contour). To assess this selection
process, we apply a basic image preprocessing step.
This step is carried out using known Edge detection
algorithm found in the literature. First, we apply the
Robert Cross operator [19], which is the simplest
edge detection method:

⎡+1 0 ⎤
⎡ 0 −1⎤
⎥ K =⎢
⎥
KGX = ⎢⎢
GY
⎥
⎢
+1 0 ⎥⎥
0 −1⎥
⎣⎢
⎦
⎣⎢
⎦
GX = KGX ⋅ I
GY = KGY ⋅ I
G = GX 2 + GY 2
⎛GX ⎟⎞ 3π
θ = tan−1 ⎜⎜
−
⎝GY ⎟⎟⎠ 4
In practice, a simple average filter is combined to
eliminate some of the image noise [10][18]. The
combined filter can be defined as follows:

(3)

GX (x , y ) = ΔC (x , y )
1

1

× ∑ ∑ KGX (i, j ) × I (x + i, y + j )

(4)

i =0 j =0

GY (x , y ) = ΔC (x , y )
1

1

× ∑ ∑ KGY (i, j ) × I (x + i, y + j )
i =0 j =0

The quantity Γ is a constant called Color bias
coefficient. This constant controls the importance of
color variation. We have experimented with this
constant and found that Γ = 0.2 yields satisfactory
result in the general case. Although, this particular
technique devised specifically for this method,
similar approach can be found in the literature
[6][7].
The required computation of this step has a perpixel dependency. However, it can be implemented
efficiently by using a High Level Shading Language
(HLSL) executed by the GPU. The results obtained
are stored as textures and used in the adaptive
selection process presented in the following subsection.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

(5)

H_HEAP

V_HEAP

Fig. 1: Priority metric computation using scan lines

relative importance of criterion

3.3. Step 2: The Adaptive Selection Process

These quantities are defined based on how we
perceive importance in the context of a specific
application. For example, if our goal is to detect
moving objects in a video, then we can assign a
higher percentage of the weight to kmotion .

To identify the most suitable candidate of a
contour trace we employ a unique approach. We
show that an efficient technique can be devised if we
can describe the problem as a spatially-temporally
adaptive phenomenon. An algorithm is spatially or
temporally adaptive if it satisfies the monotonic
refinement principle for the corresponding
i

dimension. A refinement R is the approximate
solution of a problem (i.e. in our case the current set
of contour points extracted) which can be assessed
by an energy function called the error metric ε of
the refinements Ri . In principle, it can be stated that
the problem could be broken into a number of
refinements Ri (i ∈ [ 0, ∞)) such that:
ε (Ri ) ≤ ε (Ri +1 )

(6)

i +1

During each subsequent refinement R , the
error is monotonically reduced by a defined
refinement procedure that iteratively solves the
problem. Error metric ε is dependent on the local
regional errors for each region s ∈ I such that:

s ∈I

Consequently, this will force the algorithm to select
and trace contours of moving objects (by
minimizing ε motion ). After the energy of the current
refinement is evaluated, each subsequent refinement
monotonically approaches optimally towards the
solution. The method is similar to an iterative
approach except for one key difference. A
monotonic refinement must guarantee a nondecreasing error metric during each iteration. An
error metric is a measure of convergence to the
optimal solution. The task of the adaptive algorithm
is to compute the error metric of the current iteration
(in most cases temporally adapted) and refine the
sub-region with the highest error index. This
guarantees an optimal refinement for each operation.
The error metric computation is the process of
quantifying the importance of a region of interest.
To find the error metric for a particular application,
we generally address the following questions:

k

ε j (I ) = ∑ ∑ κj ε j (s )

(7)

a.

What is the quantitative measure of the
importance of a feature point?

b.

How can we access important feature
points without having to access the
whole image?

j =1

Here κj is the weight of the error component in a
multifactor energy function.

k j specifies the

j ( ∑ k j = 1 ).

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

The answer to the first question is simple. The
importance of any point p = (x , y ) in an image is its

Error metrics are now represented as a function
ε ′ : » 2 → » 2 such that:

error metric ε(x , y ) . The multiple components of the
error metric reflect different criteria of importance
of a point. One way to assess a generalized dynamic
vision problem is to incorporate the following
factor:
•

ε ( x, y ) =

2

2

(8)

nx

H_HEAP[i ] = ∑ (εx ( j, i ))

α

j =0

•

nx

V_ HEAP[i ] = ∑ (εy (i, j ))

Motion stimuli: Regions with temporal
photometric variances are more important. In
fact, those are the regions where edges of a
moving object exist. The error metric should
use some form of threshold to eliminate noise
inherent to the sensing device:

i =0

m = Accumulation coefficient
•

Vision Emphasis: Vision emphasis is the way
the algorithm can concentrate on a region of
interest (ROI). The Emphasis is represented in
an emphasis field Ε(x , y ) . This bitmap can
provide a bias for certain pixel coordinates. For
example, in a car vision system emphasis can be
given on the roads (bottom-center part of the
image under the horizon).

•

Dynamic Feedback: A higher level process
may state interest in certain regions and coarse a
high constant to force extraction of features
from that region.

Priority functions (or error metric) can be
computed using any arbitrary explicit function not
just a weighted linear combination. However, the
function must be linearly independent in the
component space. Otherwise, the monotonic
refinement principal will be violated.
Once the error metric has been defined, our task
is to outline a method to search for the highest
priority region efficiently. High priority regions are
not easy to find given that we have a O(n ) search
space. Fortunately, there exists an effective
technique that allows far less computation. We
propose a dimensional reduction of the error metric.

(11)

α

j =0

Here, α is the Gradient Steepness Bias
Coefficient. If higher order α is selected, the scan
line with fewer but high temperature edges will
dominate lines with cooler edge gradients.

m

εmotion (x , y ) = ∑ I t −i +1 − I t −i

(10)

For example, the edge importance metric can be
broken down into horizontal and vertical
components which are GX (x , y ) and GY (x , y ) .
We then represent two priority queues using binary
heap data structures (as shown in Fig. 1). The
component of the heap can be computed in the
following manner:

Edge strength (Gradient): This is the most
intuitive and the most important factor. Features
are likely to be in regions with high gradient
intensity.
Therefore edge importance
component can be computed as:

εedge (x , y ) = GX (x , y ) + GY (x , y )

{ε x′ ( x, y)} + {ε ′y ( x, y)}

(9)

After the priority queue is built, we need to
obtain feature points. This is done by acquiring the
location of the maximal scan-lines by extracting top
element from the heaps. The algorithm then scans
along the horizontal line and initiate tracing
algorithm whenever a cross section of an edge is
detected. Fig. 2 shows how scanning H_Heap and
V_Heap in our first prototype return the cut lines
that intercept most number of edges in the image. In
practice, we have noticed that in one sweep the
algorithm is able to track a significant number of
contours that are most prominent in the image.
We conclude this section by stating that the
computation of the priority queues need not be
carried out on every frame of processing. While
contours are being tracked, it can provide a feedback
by subtracting the local error from the corresponding
H_Heap and V_Heap elements. This automatically
updates the values in the queues. The algorithm only
needs to rebuild the queue on every tracing passes.
This requires O( n ) runtime, and therefore, the
priority queue maintenance has less than per pixel
dependency. Last but not the least, the technique
presented in this section has not been attempted
before to solve an image processing problem.

3.4. Step 3: The LR-Traversing Algorithm
The LR-traversing method that we propose in this
section is the most significant contribution of this
paper. The method allows tracking of an edge that

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

and L and R are adjusted to it accordingly. The
location of the new edge point C i +1 can be
computed as:

C i +1 =

Li +1 + Ri +1
2

(13)

The directional and the cross section vectors are
computed using (14):

T = (R − L )
N =

⎛
⎞
Ty
⎜
−Tx
⎟⎟⎟
,
= ⎜⎜
⎟
⊥ (T ) ⎜⎜⎝ Tx 2 + Ty 2 Tx 2 + Ty 2 ⎟⎠⎟
⊥ (T )

(14)

Fig. 2: Horizontal and vertical scan lines
Each LR trace cycle extends the contour by N
has a spatially variant thickness. LR-traversing is
simple, easy to implement and can be used to
traverse borders of high contrast. The most
significant advantage of using LR traversal is it
continuity. LR traversal is minimally affected by
cellular image quantization.
The idea in LR traversing is simple. It starts with
an initial position C , direction of Travel N and a
cross section vector T (where T ⋅ N = 0 ). In
addition, two more points are defined: A point L in
the left uniform color region and a point R in the
Right Uniform color region. The LR traversing is
based on a concept that must satisfy the following
property in (12):

⎛((L ∈ Φ1 ) ∧ (R ∈ Φ2 ))
⎞⎟
⎜⎜
⎟⎟
⎜⎜
⎟
⎛
⎞
⎜⎜ ⎜ ∀ (p ∈ Φ1 ) ∀ (q ∈ Φ2 ))⎟⎟⎟⎟
⎟
⎜
⎜
⎟⎟ ≡ true
∃S ⎜ ⎜
⎜⎜→ ⎜⎜⎛(λ(L, p) ≤ λ(L, q )) ⎞⎟⎟⎟⎟⎟⎟
⎟⎟⎟⎟⎟
⎜⎜ ⎜⎜⎜⎜
⎟
⎜⎜ ⎜⎜⎜⎜
⎟⎟⎟⎟⎟⎟⎟
⎟⎠⎟⎠
⎜⎝ ⎝⎜⎝⎜∧ (λ(R, q ) ≤ λ(R, p))⎟⎟⎠⎟⎟
Where, Φ1 = V (G1 (S ))

(12)

and Φ2 = V (G2 (S ))
In other words, the inter color space variability
must be greater than intra color space variability.
The algorithm depends on a threshold that
differentiates intra color space variations from inter
color space variation.
The idea of the algorithm is to maintain a lock on
the left and right color uniformity, while proceeding
along N (hence the name LR-Traversing). At each
iteration, the left and right region are investigated

pixels. According to Eqn. (14), this length is exactly
1 pixel. However, it is possible to implement
adaptive stepping to process straight line edges
faster.
Method 1 shows how the LR tracing is integrated
with the error metric computation. Method 1 is quite
general in a sense that LR traversing can be replaced
by other traversing algorithms without affecting the
priority cycle. We can do so by replacing the
procedure call in line 14 and 17.
Method 1: Feature Extraction Cycle
1. Procedure HV_Scan(H_Heap,
V_Heap)
2. y
Dequeue(H_Heap);
3. x
Dequeue(V_Heap);
4. H
Extract_Feature_H(y)
5. V
Extract_Feature_V(x)
H ∪V
6. F
7. for i 1 to Length(F)-1
8.
p F[i]
9.
q F[i+1]
10.
if Scan_Line(p)=Scan_line(q)
and λ(p, q ) ≥ α then
Path ← ∅
11.
12.
T ← (q − p)

⊥ (T )

13.

N ←

14.
15.
16.

C ← LR_Traverse(T,N,p,q)
while C ≠ ∅ do
Path ← Path ∪ C
C ← LR_Traverse(T,

17.
18.

⊥ (T )

N,p,q)
Add_To_Shape_Graph(Path)

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

In the algorithm presented above, α is called the
Edge Threshold. This threshold is matched to a
reasonable perceptible difference in edge. In our
application we set α = 0.2 . The value of α is
assigned by determining what could be considered
as a perceptible contrast for edges. This quantity is
not dependent on specific image or object type.
Therefore, the use of this threshold does not limit
the generality of the algorithm.
One important aspect of the method is that the
LR traversing itself takes O(1) processing time for
each stepping. At the beginning of the method, we

T

L

L

Adjusting
Stepping
R

(a) Advancing

2.

Move successful, region consistency
preserved: proceed with the new point

L
R

R
L’
L

(c) Case 2: Turning using crossed edge

There is a major color shift in either L or R and
color distance between new L, R has dropped:
Execute turning maneuver Using preserved
cross edge.
There is a major color shift but new L, R has
color region consistency: Proceed with the new
point

4.

New L,R region consistency fails: Stop tracking

5.

New L,R reached another contour: previously
traced: Stop tracking

These cases are illustrated in Fig. 3. The
algortithm depends on Two thresholds α and β .
The β threshold is called the Color Invariance
Coefficient, which is the minimum distance of two
shades of colors that has perceivable difference.
This algorithm is outlined in Method 2.
Method 2: LR Traversing Step
1. Procedure LR_Traverse(T,N,L,R)
2. L ′ ← L + N
3. R ′ ← R + N
L′ + R′
4. C ←
2
5. if Boundary(C) then
6.
return ∅
7. pL1 Bilinear_Sample(L)
8. pR1 Bilinear_Sample(R)
9. pL2 Bilinear_Sample( L ′ )
10. pR2 Bilinear_Sample( R ′ )
αi + 0.8 × λ(L, R)
11. αi +1 ←
2
12. β i +1 ← β i × (1 + λ (L ′, R ′))

R’

R

is

3.

(b) Adjusting LR

L

compute the next step by forwarding along N . Then
we handle the following cases:
1.

R

C3
C1

C2
(d) Case 3: Proceed when 3 color regions exists
Fig. 3: Different stages of the LR Traversing

13. m i +1
14.
15.
16.

17.
18.
19.

Col_Group(pL1,
pL2,pR1,pR2)
if Marked(C)=true then
return ∅
if Major_Shift(L,R, L ′ , R ′ ) and
Major Shift( λ(L ′, R ′) , λ(L, R) )
then
if m i +1 = 4 then
return ∅
else if m i +1 = 3 and

m i = 3 then
20.
21.
22.
23.
24.

return ∅
D Shift_Dir(L,R, L ′ , R ′ )
if D=LEFT then
L′ ← L
else

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

(a) Original Image

(b) Tracing using LR-Traversing

Fig: 4: Contour Points obtained by applying LR-Traversing algorithm on a sample MRI image

25.
R′ ← R
26. Adjust_Region( L ′ , R ′ ,N)
27. if λ(L ′, R ′) < β i +1 then
28.
return ∅
L′ + R′
29. C ←
2
30. return {C}
The distance between L and R may vary from
sub-pixel to multiple pixels. Therefore, to achieve
image continuity the method performs a simple
Bilinear sampling of neighboring pixels in Line 710. This can be replaced by cubic or higher order
filters if a more complex image (high noise or
information content and low resolution) needs to be
processed. The function Col_Group() in line 13
executes a simple clustering algorithm in the
corresponding color space (i.e. HLV or LA*B*).
Each cluster has a maximum radius of α . The
output is the minimum number of clusters for the
given set of sample points. Since, the number of
these points are constant the procedure executes in
O(1) time.
In addition to the methods illustrated above, we
also maintain a stencil buffer to determine
interception with existing contours. The function
Marked() in Line 14 tests the stencil buffer for
possible intersection. Junction points are also
inserted to the shape graph in this case. The
Adjust_Region() function in Line 26 is illustrated in
Fig. 3. This is an important part of the algorithm.
The purpose of the function is to re-align L and R so
that the center of the gradient variation is at

( L + R)

2 . The method is executed iteratively by

adjusting the position of L and R along the Line

LR . At first, either L or R (or both) is extended if
there is a color-shift not exceeding α . The iteration

stops when Color-shift is less than β or greater
than α . The runtime of this function is dependent
on the thickness and the regularity of a contour.
However, it runs in constant time in the average
case.
i

It is worth nothing that the actual implementation
of the method is more complex due to degenerate
cases of LR regions. By providing additional
stability test, we were able to avoid some failures
when the property in (12) is not satisfied.

4. Experimental Results
To analyze the effectiveness and the performance
of the method presented in this paper, a contour
tracing program has been designed in Visual C++
using the DirectX 9.0™ graphics platform. The
performance was measured using the Windows
Performance Counter API. The program takes as
input an arbitrary image and produces a point cloud
of contour points. This point cloud is grouped as
their individual scan sequence while tracing along a
particular contour. We have tested the method
against a variety of artificial and natural images.
Results obtained from some of these test runs has
been presented in Fig. 4 and Fig. 5.
Fig. 4 demonstrates the use of the algorithm to
process a blurry MRI image. The original image is
shown in Fig 4a. The adaptive computational limit

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

(a) House Image: 500 Contour Points

(b) House Image: 1000 Contour Points

(c
(c) Car Image: 500 Contour Points

(d
(d) Car Image: 1000 Contour Points

Fig 5: Contour Points Generated by the Adaptive LR-Traversing Algorithm (Each image has a resolution of
512 x 512 pixels, 24BPP)

of the method is controlled by stating the maximum
number of contour points generated by the program.
The higher this number, the more detailed contour
extraction would be. When the program is executed
with the computational limit of 1000 contour points,
the contours in Fig 4b was generated (Yellow lines).
To highlight the contour points, the brightness and
contrast of Fig. 4b has been altered, therefore some
contour points may appear to be deviating from the
white region in Fig. 4b. However, author can
confirm the validity of these edges by comparing to
Fig 4a on the corresponding regions. Thus, the result
in Fig. 4 clearly demonstrates that this method can
be used successfully with a moderately blurry and
noisy image.
The adaptive runtime and complexity is one of
the most unique contributions of this method. This

characteristic of the algorithm is demonstrated in Fig
5. We ran the program to process a number of
images. For each image, the performance is recorded
for several target complexity. For testing we
selected two images that constitute both high
contrast and low contrast contours. Fig. 5a and Fig.
5b shows the selection of contour points (yellow
lines) with increasing target complexity. The figure
clearly demonstrates how the method first selects
highest contrast features followed by the less
apparent details. Fig. 5c and 5d also confirms this
phenomenon using a different input image. This
experiment clearly demonstrates that the method is
highly suitable for applications where a particular
object of interest needs to be tracked efficiently.
Therefore, the method can be used effectively in
aerial image analysis and recognition, synthetic
marker and sign recognition, image registration,

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

Run-time Efficiency of LR-traversing
50

Frame(s) per Seco

45
40
35
30
25
20
15
10
5
0

1000

2000

3000

4000

256x256

45.77

25.08

16.19

12.31

512x512

35.94

19.62

12.86

10.85

1024x1024

26.91

14.04

9.97

7.5

Number of contour points per frame
256x256

512x512

1024x1024

Fig. 7: Failure of LR traversing Turning fails when
edge is not strong around the turning point

Fig 6: Runtime performance of the LR-Traversing
Algorithm.

Object recognition, medical image processing and
many other similar applications.
The output of the algorithm is a set of paths each
comprising a sequence of contour points (not an
unordered point cloud). For object recognition, these
sequences can be processes as followed. First, a
subdivision operator is applied to each sequence to
eliminate high frequency detail (see the windows in
Fig. 5). An extrapolation algorithm can be executed
to close the curve (or join two nearby paths). This
process is governed by the law that an extrapolation
is valid only if there are no point stepping where
there is a major shift exceeding α for both L and R
(in one step). This will allow the creation of a shape
graph which could be used by a higher level object
recognition method. Therefore, the result obtained
by this method is comparable to the Active Contour.
The method was also tested for its runtime
efficiency and performance. The graph in Fig. 6
provides a summary of this experiment. We
discovered that the runtime of the method does not
depend significantly on the type of image being
processed. However, if the entire image is
substantially blurred, a slight decrease in
performance will occur. The runtime of the method
depends mostly on two factors: the resolution of the
image and the target complexity. One phenomenon
apparent from the graph in Fig. 6 is that, when the
target complexity of the system is raised, the effect
of increasing image resolution is substantially
reduced. The graph also demonstrates that the
method can be executed in real-time. To compare
this runtime with existing method, we tested an
active contour program using the GVF Snakes
algorithm [17]. In our trial run, the algorithm took
approximately 59 seconds to process the image in
Fig 4a (inner control points inside the central white

mass). The iteration was terminated when the
contour was at least as accurate as Fig 4b. Thus, we
confirm the run-time efficiency of this method is
unprecedented.
Finally, it should be noted that the method has
some limitations that should be taken under
consideration before the method is applied to solve a
certain problem. Firstly, the method is incapable of
differentiating borders of distinct texture patterns.
For example, the method would be unable to
identify the contour between grasses and a bush if
their color and average intensity levels are similar.
This limitation can be avoided by executing an extra
texture discrimination algorithm during Step 1
(Section 3.2). The second limitation is that the
method fails to follow a curved contour if the interregional contrast diminishes rapidly. This effect is
shown in fig. 7 where the contour of a finger is
being traced. At the tip of the finger, the contrast
drops at a substantial rate. As a result, the method
fails to execute case 2 (Fig. 3c) of the algorithm.
Consequently, the method fails to turn and the
contrast drops below the threshold. This terminates
the tracing according to Case 5 (Fig. 3). Although
this anomaly cannot be avoided, it can still be
minimized by using a corrective local segmentation
algorithm that is executed locally when a tracing
fails. This will circumvent the limitation at an extra
computational cost.

5. Conclusions
In this paper we presented a new real-time
algorithm to trace object contours in photographic
and synthetic images. The method provides a
flexible runtime control and the complexity of the
result can be regulated. When the computational
capacity is limited in a demanding run-time
environment, the method can extract features
optimally based on an importance evaluation metric.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

Experimental results clearly demonstrate that the
method is effective and highly efficient. Due to its
performance and effective result, the method can be
used in a variety of image processing and computer
vision application as a feature extraction tool.

Acknowledgements
This paper is partially funded by the GEOIDE
grant.

[9]

[10]

[11]
[12]

References
[1]

[2]

[3]

[4]

[5]

[6]

[7]

[8]

Arslan, A., Turkoklu, I., “An Edge Tracing Method
Developed for Object Recognition”, Proceedings of
WSCG’99, 1999.
Bahrat B., Biswal and Hyde J. S. Contour-based
registration techique to differentiate between taskactivated and head motion-induced signal variations
in fMRI. Magnetic Resonance in Medicine, 38:470476, 1997.
Biederman, I. (1985): Human image understanding:
Recent research and a theory. Comput. Vision
Graphics Image Processing 32: 29-73.
Billinghurst M., Poupyrev I., Shared Space: Mixed
Reality Interface for Collaborative Computing.
Imagina'2000 Official Guide: Innovation Village
Exhibition. 2000. INA. pp. 52.
Canny J., A Computational Approach to Edge
Detection, IEEE Transactions on Pattern Analysis
and Machine Intelligence, Vol 8, No. 6, Nov 1986.
Comaniciu D. and Meer P., Robust Analysis of
Feature Spaces: Color Image Segmentation,
Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition, San Juan, Puerto
Rico, June 1997, 750-755.
Deng Y., Manjunath B. S. and Shin H. "Color
image segmentation", Proc. IEEE Computer Society
Conference on Computer Vision and Pattern
Recognition, CVPR '99, Fort Collins, CO, vol.2,
pp.446-51, June 1999.
Drascic D., Grodski J. J., Milgram P., Ruffo K.,
Wong P., and Zhai S. Argos: A display system for

[13]

[14]
[15]

[16]

[17]

[18]

[19]

[20]
[21]

augmenting reality: Abstract. In ACM SIGGRAPH
1993.
Dubuisson-Jolly, M. P., Gupta, A. Color and texture
fusion: Application to aerial image segmentation
and gis updating. Image and Vision Computing 18
(2000) 823-832
Ferrie F. P., Lagarde J., and Whaite P. Darboux
frames, snakes, and super-quadrics: Geometry from
the bottom up. IEEE Trans. on Pattern Anal.
Machine Intell., 15(8):771-784, August 1993.
Gonzalez R. and Woods R. Digital Image
Processing, Addison Wesley, 1992, pp 414 - 428.
Haralick, R., Shanmugan, K., Dinstein, I. Texture
features for image classification. IEEE Transactions
on Systems, Man, and Cybernetics 3 (1973) 610621.
Heng, W. J., “An Object-based shot boundary
detection using edge tracing and tracking”, Journal
of Visualization Communication and Image
Representation 12, pp. 217-239, 2001.
Horn B., Robot Vision, MIT Press, 1986, Chap 8.
Huang, Q., Dom, B. Quantitative methods of
evaluating image segmentation. In: IEEE
International Conference on Image processing.
Volume III., Washington DC (1995) 53-56
Iyengar, S. S., Wu, Y. and Min, H. “Efficient edge
extraction of images by directional tracing.” Hipc,
p. 233, Third International Conference on HighPerformance Computing (HiPC’96), 1996.
Kass M., Witkin A., and Terzopoulos D. Snakes:
Active Contour models. Int. J. Computer Vision,
1(4):321-331, 1987.
Lowe D. G., "Distinctive image features from scaleinvariant keypoints," International Journal of
Computer Vision, 60, 2 (2004), pp. 91-110.
Maintz J. B. A. and Viergever M. A. A survey of
medical image registration. Medical Image
Analysis, 2(1):1-36, 1998.
Marion A., An Introduction to Image Processing,
Chapman and Hall, 1991, p 274.
Sumengen B., Manjunath B. S., Kenney C., "Image
Segmentation using Curve Evolution and Flow
Fields," Proceedings of IEEE International
Conference on Image Processing (ICIP), Rochester,
NY, USA, September 2002.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

