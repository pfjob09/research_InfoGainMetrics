Visual Speech Recognition Using Image Moments and Multiresolution Wavelet
Images
Wai C. Yau, Dinesh K. Kumar, Sridhar P. Arjunan and Sanjay Kumar
School of Electrical and Computer Engineering, RMIT University, GPO Box 2476V
Melbourne, Victoria 3001,Australia
s3115288@rmit.edu.au, dinesh@rmit.edu.au
Abstract
This paper describes a new technique for recognizing
speech using visual speech information. The video data
of the speaker’s mouth is represented using grayscale images named as motion history image (MHI). MHI is generated by applying accumulative image differencing on the
frames of the video to implicitly represent the temporal
information of the mouth movement. The MHIs are decomposed into wavelet sub images using Discrete Stationary Wavelet Transform (SWT). Three moment-based features (geometric moments, Zernike moments and Hu moments) are extracted from the SWT approximate sub images. Multilayer perceptron (MLP) type artiﬁcial neural
network (ANN) with back propagation learning algorithm
is used to classify the moments features. This paper evaluates and compares the image representation ability of the
different moments. The initial experiments show that this
method can classify English consonants with an error rate
less than 5%.
Keywords— visual speech recognition, motion history image, image moments, discrete stationary wavelet transform

1

Introduction

Speech recognition is one of the key areas of research
among the computer science and signal processing community. Extensive research efforts are put into developing speech-based devices to replace the conventional interfaces such as keyboard and mouse to enable users to communicate with the computers using natural speech. However, these systems are based on audio signals and are
sensitive to signal strength, ambient noise and acoustic
conditions[2].
This research proposes to use the visual speech information extracted from the video of the mouth to classify
utterances. The visual speech information refers to the
movement of the speech articulators such as the tongue,
teeth and lips of the speaker. The complex range of reproducible sounds produced by people is a clear demonstration of the dexterity of the human mouth and lips- the

key speech articulators. The possible advantages are that
such a system is not sensitive to audio noise and change
in acoustic conditions, does not require the user to make a
sound, and provides the user with a natural feel of speech
and the dexterity of the mouth. Such a system is termed by
the authors as ’Audio-less Speech Recognition’ system.
Audio-less speech recognition requires using only the
sensing of the facial movement. There are a number of
options that have been proposed, such as visual, mechanical sensing of facial movement and movement of palate,
recording facial muscle activity, facial plethysmogram and
measuring the intra-oral pressure. Kumar et. al.[8] have
reported on a speech recognition method based on surface
Electromyography (EMG) signals of the speaker’s facial
muscles that shows the movement of these muscle during
speech. Speech recognition based on visual speech signal is the least intrusive and this paper reports such a system for human computer interface applications. The visual
cues contain far less classiﬁcation power for speech compared to audio data [14] and hence it is to be expected that
the visual only systems would have only a small vocabulary. Such systems are also known to be user dependent,
and hence it is important for such a system to be easy to
train for a new user.
Visual speech recognition techniques reported in the literature in the past decade can be categorized into two main
categories -shape-based and the appearance-based. The
shape-based features rely on the geometric shape of the
mouth and lips and can be represented by a small number of parameters. The ﬁrst visual speech recognition system was developed by Petajan[13] two decades ago using shape-based features such as height, width and area
of the mouth. Appearance-based features are derived directly from the pixel intensity values of the image around
the mouth area [10, 14]. One common difﬁculty with visual systems is that these systems are ’one size ﬁts all’ approach. Due to the large variation in the way people speak,
especially if we transgress the national and cultural boundaries, these have very high error rate, with error of the order

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

of 90% [14], which demonstrates the inability of these systems to be used for such applications. What is required is a
system that is easy to train for a user, which works in realtime and can provide active real-time feedback to the user.
The system needs to be robust under changing conditions
such as angle and distance of the camera, and insensitive to
factors such as variation of speed of the speech, skin color
and texture.
To achieve the above mentioned goals, this paper proposes a system where the camera is attached in place of
the microphone to the commonly available head-sets. The
advantage of this is that using this, it is no longer required
to identify the region of interest, reducing the computation required. The video processing proposed is the use
of accumulative image differencing technique to directly
segment the movement of the speech articulators. The proposed motion segmentation approach is based on the use
of motion history image (MHI) where the video data is
multiply with a ramp function and temporally integrated
with greater weight to the recent movements. The resultant
is a 2-D grayscale image which is suitable for representing
short duration ﬁne movement of the mouth during speech.
MHI is a spatial-temporal template that shows where and
when movement of speech articulators occurs in the image
sequence. This paper reports the use of 2-D stationary
wavelet transform (SWT) at level 1 to decompose the MHI
into four sub images, with the approximate image used
for further analysis. This paper proposes to use image
moments as features extracted from the approximation of
MHI and ANN to classify these features. The fundamental
concept of this technique can be traced to the research reported by Kumar et. al[9, 7] in hand gesture recognition.

2
2.1

Theory
Motion History Image

Motion history image (MHI) is a view-based approach
and is generated using difference of frames (DOF) from
the video of the speaker. Accumulative image difference
is applied on the image sequence by subtracting the intensity values between successive frames to generate the
difference of frames (DOFs). The delimiters for the start
and stop of the motion are manually inserted into the image sequence of every articulation. The MHI of the video
of the lips would have pixels corresponding to the more recent mouth movement brighter with larger intensity values.
The intensity value of the MHI at pixel location (x, y) of
time t (or the tth frame) is deﬁned by

threshold a and B(x, y, t) is given by
B(x, y, t) =

1 if Dif f (x, y, t) ≥ a,
0 otherwise

(2)

a is the predetermined threshold for binarisation of the
DOF represented as Dif f (x, y, t). An optimized ﬁxed
threshold of a is selected manually. The DOF of the tth
frame is deﬁned as
Dif f (x, y, t) = |I(x, y, t) − I(x, y, t − 1)|

(3)

I(x, y, t) represents the intensity value of pixel location
with coordinate (x, y) at the tth frame of the image sequence. In Eq. (1), the binarised version of the DOF is
multiplied with a linear ramp of time to implicitly encode
the timing information of the motion into the MHI. By
computing the MHI values for all the pixels coordinates
(x, y) of the image sequence using Eq. (1) will produce a
scalar-valued grayscale image (MHI) where the brightness
of the pixels indicates the recency of motion in the image
sequence[7].The proposed motion segmentation approach
is computationally simple and is suitable for real time implementation.
2.1.1

Speed of Speech and Skin Colour Invariance

The motivation of using MHI in visual speech recognition
is the ability of MHI to remove static elements from the sequence of images and preserve the short duration complex
mouth movement. MHI is also invariant to the skin color of
the speakers due to the DOF and image subtraction process
involved in the generation of MHI.
There could be inter and intra speed variations of
speech. The speed of the mouth movements when the
speakers is pronuncing a consonant might be different for
each of the video data recorded. To account for the variation of the rapidity of speech, the intensity values of the
MHI are normalized to between 0 to 1. This will ensure
that the MHI of the same utterance spoken under varying
speed of speech will be almost consistent[7].
2.1.2

Factors affecting the MHI

MHI is a view sensitive motion representation technique.
Therefore the MHI generated from the sequence of images
is dependent on factors such as:
• position of the speaker’s mouth normal to the camera
optical axis

(1)

• orientation of the speaker’s face with respect to the
video camera

N is the total number of frames used to capture the mouth
motion. B(x, y, t) is the binarisation of the DOF using the

• distance of the speaker’s mouth from the camera
(which changes the scale/size of the mouth in the
video data)

N −1

M HIt = max

B(x, y, t) × t
t=1

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

• small variation of the mouth movement of the
speaker while uttering the same consonant
It is difﬁcult to ensure that the position, orientation and
distance of the speaker’s face are constant from the video
camera for every sample taken. Thus, descriptors that are
invariant to translation, rotation and scale have to be used
to represent the MHI for accurate recognition of the consonants. The features used to describe the MHI should
also be insensitive to small variation of mouth movement
between different samples of the same consonants. This
paper adopts image moments as region-based features to
represent the approximation of the MHI. Image moments
are chosen because they can be normalized to achieve
scale, translation and rotation invariance. Before extracting the moment-based features, SWT is applied to MHI to
obtain a transform representation of the MHI that is insensitive to small variations of the mouth and lip movement.

2.2

Stationary Wavelet Transform (SWT)

SWT is used for denoising and to minimize the variations between the different MHI of the same consonant.
While the classical discrete wavelet transform (DWT) is
suitable for this, DWT results in translation variance [11]
where a small shift of the image in the space domain will
yield very different wavelet coefﬁcients. SWT restores the
translation invariance of the signal by omitting the downsampling process of DWT.
SWT decomposition of the MHI generates four images,
namely approximation (LL), horizontal detail coefﬁcients
(LH), vertical detail coefﬁcients (HL) and diagonal detail
coefﬁcients (HH). The approximate image carries the highest amount of information content among the four images.
Hence, the image moments features are computed from the
approximate sub image.

2.3

Moment-based Features

Image moments are low dimensional descriptors of image properties. Moments computed from the images are
very concise and can represent the global characteristics of
the objects’ shapes within the image[12, 17]. Image moments features can be normalized to achieve translation,
rotation and scale invariance thus are suitable to be used as
features to represent the approximation of MHI.
2.3.1

Geometric moments

Geometric moments are the projection of the image function f(x, y) onto a set monomial function.The regular geometric moments are not invariant to rotation, translation
and scaling.
Translation invariance of the features can be achieved
by placing the centroid of the image at the origin of the co-

ordinate system (x, y), this results in the central moments.
The central moments can be further normalized to achieve
scale invariant. The normalized central moments are invariant to changes in position and scale of the mouth within
the MHI[16].
The normalized central moments can be derived up to
any order. In this paper, the 49 normalized geometric moments are computed from the MHI as one of the feature
descriptors to represent the different consonants. For the
purpose of comparison of the different techniques, the total number of moments has been kept the same for Zernike
moments.
2.3.2

Hu moments

Hu [4] introduced seven nonlinear combinations of normalized central moments that are invariant to translational,
scale and rotational differences of the input patterns known
as absolute moments invariants. The ﬁrst six absolute moment invariants are used in this approach as features to represent the approximate image of the MHI for each consonant. The seventh moment invariant is skew invariant deﬁned to differentiate mirror images and is only invariant to
rotation and changes sign under reﬂection. The seventh invariant moment is not used because it is not necessary to
differentiate mirror images in this application.
2.3.3

Zernike moments

Zernike moments are computed by projecting the image
function f(x, y) onto the orthogonal Zernike polynomial.
The main advantage of Zernike moments is the simple
rotational property of the features. Zernike moments are
also independent features due to the orthogonality of the
Zernike polynomial[15]. This paper uses the absolute
value of the Zernike moments as the rotation invariant
features[5] of the SWT of MHI. 49 Zernike moments that
comprise of 0th order moments up to 12th order moments
have been used as features to represent the approximate
image of the MHI for each consonant.

2.4

Classiﬁcation using Artiﬁcial Neural Network

Classiﬁcation involves assigning of new inputs to one of
a number of predeﬁned discrete classes. There are various
classiﬁer choices for pattern recognition applications such
as Artiﬁcial Neural Network (ANN), Bayesian classiﬁer
and Hidden Markov Model (HMM). This paper presents
the use of ANN to classify moment-based feature input
into one of the class of viseme. ANN has been selected
because it can solve complicated problems where the description for the data is not easy to compute. The other
advantage of the use of ANN is its fault tolerance and high

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

Figure 1: The three consonants and the MHI for the consonants
computation rate due to the massive parallelism of its structure [6]. The functionality of the ANN to be less dependent
on the underlying distribution of the classes as opposed to
other classiﬁers such as Bayesian classiﬁer is yet another
advantage for using ANN in this application.
A supervised feed-forward multilayer perceptron
(MLP) ANN classiﬁer with back propagation learning
algorithm is integrated in the visual speech recognition
system described in this paper. The ANN is provided with
number of training vectors for each class during the training phase. MLP ANN was selected due to its ability to
work with complex data compared with a single layer network. Due to the multilayer construction, such a network
can be used to approximate any continuous functional
mapping [1]. The advantage of using back propagation
learning algorithm is that the inputs are augmented with
hidden context units to give feedback to the hidden layer
and extract features of the data from the training events
[3]. Trained ANNs have very fast classiﬁcation speed thus
making them an appropriate classiﬁer choice for real time
visual speech recognition applications.

3

Methodology

Experiments were conducted to evaluate the performance of the proposed visual speech recognition approach.
The experiments were approved by the Human Experiments Ethics Committee of the University. The experiment
was designed to test the efﬁciency of different moments
features in classifying 3 consonants when there was minimal shift between the camera and the mouth of the user
between the training and the testing data.
The ﬁrst step of the experiment was to record the video
data from a speaker uttering the three consonants. The
three consonants selected were a fricative /v/, a nasal /m/
and a stop /g/. Each consonant was repeated for 20 times
while the mouth movement of the speaker was recorded
using an inexpensive web camera. This was done towards

having an inexpensive speechless communication system
using low resolution video recordings. The video camera
focused on the mouth region of the speaker and the camera was kept stationary throughout the experiment with a
constant window size and view angle. A consistent background and illumination was maintained in the experiments. The video data was stored as true color (.AVI) ﬁles
and every AVI ﬁle had a duration of 2 seconds to ensure
that the speaker had sufﬁcient time to utter each consonant.
The frame rate of the AVI ﬁles was 30 frames per second
which is within the range of standard frame rate for video
cameras. One MHI was generated from each AVI ﬁle. A
total of 60 MHI were produced for the 3 consonants, 20
for each consonant. Example of the MHI for each of the
consonants /v/, /m/ and /g/ are shown in Figure 2.
SWT at level-1 using Haar wavelet was applied on the
MHI and the approximate image was used for analysis.
The moment-based features were extracted to characterize
the different mouth movement of the approximate image of
MHI generated by the SWT. 49 moments -each of Zernike,
geometric and ﬁrst six Hu moments were computed. These
features were tested to determine the efﬁciency of the different moments in representing the lip movement.
In the experiment, features from 10 MHI (for each consonant) were used to train the ANN classiﬁer with back
propagation learning algorithm. The architecture of the
ANN consisted of two hidden layers and the numbers of
nodes for the two hidden layers were optimized iteratively
during the training of the ANN. Sigmoid function was the
threshold function and the type of training algorithm for
the ANN was gradient descent and adaptive learning with
momentum with a learning rate of 0.05 to reduce chances
of local minima. The trained ANNs were tested by classifying the remaining 10 MHI of each consonant that were
not used in the training of the ANN to test the performance
of the proposed approach. The performance of these three
moment-based features was evaluated in this experiment
by comparing the accuracy in the classiﬁcation during test-

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

Table 1: Classiﬁcation results for different image moments extracted from the SWT approximate image of the MHI.
Predicted Consonants
Actual
Zernike
Geometric
Hu
Consonants /v/ /m/ /g/ /v/ /m/ /g/ /v/ /m/ /g/
/v/
10
10
10
/m/
10
10
1
9
/g/
10
10
10

Table 2: Recognition rates for the different moment features of the MHI.
Type of Moments
Recognition Rate
Zernike Moments
100%
Geometric Moments
100%
Hu Moments
96.67%

ing.

4

Results

The experiment investigates the performance of the different features in classifying the MHI of the 3 consonants.
The classiﬁcation results are tabulated in Table 1. It is observed that the classiﬁcation accuracies of the three features (Zernike moments, geometric moments and Hu moments) are very similar, with Zernike moments and geometric moments yielding marginally higher recognition
rate (100%) compared to Hu moment features. The results
also indicate a very high level of accuracy for the system
to identify the spoken consonant from the video data when
there is no relative shift between the camera and the mouth.
Table 2 summarizes the recognition rates for the different moment features. The results indicate that the MHI
based technique is able to recognize spoken consonants
based on lip reading with a high degree of accuracy.

5

Discussion

The results indicate that the technique provides excellent results for identifying the unspoken sound based on
video data, with error less than 5%. The results using the
three different image moments are all very comparable. Hu
moment has marginally higher error, but has signiﬁcantly
smaller number of moments used (only 6 moments) to represent the MHI compared with Zernike moments and geometric moments, which have 49. The higher order moments contain more information content of the MHI [16].
While this error rate is far lower than the error rate reported in the review by [14], the authors believe that it is
not appropriate to compare the work reported there to our

work as this system has only been tested for limited and
selected phones.
The promising results obtained in the experiment indicate that this approach is suitable for classifying consonants based on the mouth movement without regard to the
static shape of the mouth. The results also demonstrate that
a computationally inexpensive system which can easily be
developed on a DSP chip can be used for such an application. At this stage, it should be pointed that this system is
not being designed to provide the ﬂexibility of regular conversation language, but for a limited dictionary only, and
where the phones are not ﬂowing, but are discrete. The
current systems require the identiﬁcation of the start and
end of the phone, and the authors propose the use of muscle activity sensors for this aim. The authors would also
like to point out that this system is part of the overall system. This system is designed for consonant type of sounds,
where there is facial movement during the phone. The authors have also designed a separate system that is suitable
for vowels, and the two need to be merged together for the
complete system.
The authors believe that one reason for better results of
this system compared with the other related works is that
it is not only based on lip movement, but is based on the
movement of the mouth. While lips are important articulators of speech, other parts of the mouth are also important,
and this approach is closer to the accepted speech models.

Conclusions
This paper describes a visual speech recognition approach that is based on direct mouth motion representation and is suitable for real time implementation. The low
complexity of the proposed visual speech recognition system is achieved by using image differencing technique that

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

represent the mouth motion of the image sequence using
grayscale images, motion history image (MHI).
This paper focused on classifying English consonants
because pronunciation of consonants results in more visually observable movement of the speech articulators
as compare to vowels. 2-D SWT and image moments
(Zernike moments, geometric moments and Hu moments)
are used to extract visual speech features from the MHI and
classiﬁcation of these features is performed by ANN. The
experimental results indicate that such a system can produce high classiﬁcation rate (approximately 100%) using
moment based features extracted from SWT approximate
of MHI.
The preliminary proof of concept experiments were
conducted using three consonants consisting of very different mouth movements. Further work is required to investigate the robustness of the proposed technique in classifying speech sounds with similar mouth movements. There
is also a need to identify the limitation of this technique
by testing the system on a large number of speech sounds,
with the intent to determine the possible vocabulary that
maybe supported by such a technique. Such a system could
be used to drive computerized machinery when in noisy
environments. The system may also be used for helping
disabled people to use a computer and for voice-less communication.

References
[1] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995.
[2] T. Chen. Audiovisual speech processing. IEEE Signal Processing Magazine, 18:9–21, 2001.
[3] K. Y. Haung. Neural network for robust recognition
of seismic patterns. In IIJCNN’01, Int Joint Conference on Neural Networks, 2001.
[4] M. K. Hu. Visual pattern recognition by moment invariants. IEEE Transactions on Information Theory,
8:179–187, 1962.
[5] A. Khontazad and Y. H. Hong. Rotation invariant
image recognition using features selected via a systematic method. Pattern Recognition, 23:1089–1101,
1990.
[6] A. D. Kulkarni. Artiﬁcial Neural Network for Image
Understanding. Van Nostrand Reinhold, 1994.

[7] S. Kumar and D. K. Kumar. Visual hand gesture classiﬁcation using wavelet transform and moment based
features. International Journal of Wavelets, Multiresolution and Information Processing(IJWMIP),
3(1):79–102, 2005.
[8] S. Kumar, D. K. Kumar, M. Alemu, and M. Burry.
Emg based voice recognition. In Intelligent Sensors,
Sensor Networks and Information Processing Conference, 2004.
[9] S. Kumar, D. K. Kumar, A. Sharma, and N. McLachlan. Classiﬁcation of visual hand movements using multiresolution wavelet images. In International
Conference on Intelligent Sensing and Information
Processing, pages 373 – 378, 2004.
[10] L. Liang, X. Liu, Y. Zhao, X. Pi, and A. V. Neﬁan.
Speaker independent audio-visual continuous speech
recognition. In IEEE Int. Conf. on Multimedia and
Expo, 2002.
[11] S. Mallat. A Wavelet Tour of Signal Processing. Academic Press, 1998.
[12] R. Mukundan and K. R. Ramakrishnan. Moment
Functions in Image Analysis : Theory and Applications. World Scientiﬁc, 1998.
[13] E. D. Petajan. Automatic lip-reading to enhance
speech recognition.
In GLOBECOM’84, IEEE
Global Telecommunication Conference, 1984.
[14] G. Potamianos, C. Neti, G. Gravier, and A. W. Senior.
Recent advances in automatic recognition of audiovisual speech. In Proc. of IEEE, volume 91, 2003.
[15] M. R. Teague. Image analysis via the general theory
of moments. Journal of the Optical Society of America, 70:920–930, 1980.
[16] C. H. Teh and R. T. Chin. On image analysis by
the methods of moments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 10:496–513,
1988.
[17] D. Zhang and G. Lu. Review of shape representation and description techniques. Pattern Recognition
Letters, 37:1–19, 2004.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

