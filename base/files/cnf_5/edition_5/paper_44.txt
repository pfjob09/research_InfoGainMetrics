Facial Composite System Using Genetic Algorithm
Dr. Mayada F. Abdul Halim, Hussein Hamdi Al-Fiadh
University of Bahrain, College of IT, Department of CS, Kingdom of Bahrain
University of Baghdad, College of Science, Department of CS, Iraq
{mayadafa@yahoo.com, Baker.Hussain@yahoo.com}
Abstract
Although humans have excellent facial recognition
ability, it is widely recognized that they often have great
difficulty recalling facial characteristics in sufficient
detail to produce an accurate composite. In this paper,
we propose a newly facial composite system –to be
coined as Genetic-based Facial Composite SystemGFCS- that depends on genetic algorithm as a facial
search method. The system consists of two main
activities. In the first activity (database preparation), a
number of steps are applied to extract face parts from
the database image. While in the second activity (facial
composite reconstruction), genetic algorithm is used for
reconstructing facial composite image seemed likeness to
eye witness facial image. Moreover, GFCS provides
utility tools to enhance the resulted facial composite
image. The results from using GFCS clarified that the
system is successful in producing a good likeness to a
witness mental image.
Keywords--- facial recognition based strategy,
facial composite image, genetic algorithm.

1. Introduction
Facial composite images are often used in the
criminal investigation process when the identity of an
offender is unknown and when witnesses do not make
identification from a lineup or collection of mug shots
offered by the police [1]. Under these circumstances,
witnesses are often asked to participate in the process of
constructing a facial image of the offender.
There are, broadly speaking, two strategies to
build facial composite system, namely individual feature
recall strategy and recognition-based strategy. In the first
strategy,
the
facial
composite
system uses
interchangeable photographs of several face parts, such
as forehead/hair, eyes/ eyebrows, mouth, nose, and
chin/cheeks, and accessory items such as beards and
glasses. These facial composite systems require that a
witness be able to recall isolated individual facial

features. On the basis of psychological studies indicating
that people find it difficult to apply single-feature recall,
psychologists Johnston and Caldwell concluded that a
different kind of system was needed. They state that
while “humans have excellent facial recognition ability,”
they “have great difficulty recalling facial characteristics
with sufficient detail to provide an accurate
composite”[2].
To take advantage of the remarkable human ability
to recognize faces, another facial composite systems are
built, which use a recognition-based strategy that allows
a crime witness to create interactively a composite face
without requiring isolated feature recall ability [2][3].
Most researchers of composite face systems depend on
this strategy utilize the features provided by genetic
algorithm for designing their systems [3][4]. Johnston
and Caldwell claimed that systems that are employ
recognition-based strategy is more effective than systems
that use an individual feature recall strategy [2].
In this paper, a new facial composite system using
genetic algorithm coined as genetic-based facial
composite system (GFCS) is formed. Traditionally, a
genetic algorithm requires the specification of survival
fitness criteria to be evaluated by the computer for
evolving its individuals. This is typically one of the most
difficult tasks in designing a GA. In our system, the user
needs to only apply personal fitness criteria, not state or
even understand them. In that way, our facial composite
system uses rates of each face parts then according to
this rate a new methods of mating individual are applied
which is differs from other systems that used whole face
in rating an individual solution.

2. The Proposed System
The proposed genetic-based facial composite systemGFCS consists of two main activities:
• Prepare database: extract face parts from the
face images in the database.
• Reconstruct facial composite image: from face
parts images available in the database, mental
face image is reconstructed.
Additionally, GFCS provides utility tools for
painting, smoothing, and sharpening the resulted

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

face composite image. The following sections
describe each activity in details.

2.1 Preparing Database
Since GFCS construct the facial image from face parts,
the database must contain in addition to the facial
images, information about face parts of the image. The
database face images acquire through ORITE vc-3210
camera with resolution of (72 by 72 pixels) with 256
gray levels. The size of facial image is fixed (256 by 256
pixels) under approximately the same illumination.
The acquired face image first must divide into
its feature parts. In GFCS, the method for extracting face
parts is based on the work of Vanger, Hoenlinger and
Haken [5]. Twenty-Seven feature annotations are defined
on each face. Five annotations lie on the central axis of
the face; eleven annotations lie almost symmetrically on
each side of the face (see figure 1(a)). These are:
Lower face
Upper face
1

tip of the chin

inner eye corner (left,
right)
2
middle of upper lip
9
outer eye corner (left,
right)
3
under the tip of the
10 middle of upper lid
nose
(left, right)
4
middle of lower lip
11 middle of lower lid
(left, right)
5
lip corner (left, right) 12 middle of eye brow
(left, right)
6
nostril (left, right)
13 tail of eye brow (left,
right)
7
under the ear lobe
14 head of eye brow
(left, right)
(left, right)
15 top of the forehead
16 forehead side (left,
right)
The user select the feature annotations manually
using mouse click. Then coordinates of each annotation
will be saved for extracting the face parts. Next, the face
parts are structured in rectangles according to the feature
control points (see figure 1 (b)). Finally, the coordinates
of feature parts are stored with its original image in the
database. Figure 2 shows each face part and the
corresponding information extracted from the face
image.

(a)

8

(b)

Figure 1 Face parts extracted depending on
feature annotations

Figure 2 Face parts and the corresponding
information extracted face parts

2.2 Reconstruction of facial composite image
The second activity of the proposed GFCS is to
reconstruct a facial image nearest to eye witness using
genetic algorithm. The following algorithm gives the
steps toward reconstruction facial image used by GFCS:
1- Generate randomly initial population of facial
composite images.
2- Select the nearest composite image to eye
witness from the generated population. If there
is no composite image near to eye witness, then
go to step 1.
3- Rate face parts of the selected composite image.
4- Evaluate fitness for each individual in the
population.
5- Advance a copy of the nine fittest individuals
into the mating pool.
6- Applied crossover and mutation on the mating
pool individuals to generate a new population.
7- Stop if a termination criterion is satisfied. Else,
go to step 2.
2.2.1 Initial Population and Composites
blending The witness mental image can degrade or
become confused as a result of viewing a large number
of faces. So, an initial population of sixteen composite
faces is created randomly from the face parts of twenty
images stored in database. Composite faces are
constructed out of face parts from images. The feature
annotations made it possible to automatically
recombine face parts from several different
photographs and still get (most of the time) composites
in which the pieces fit together fairly well. The
composite is constructed starting with the face
supplying the cheeks, ears and chin as a background.
The remaining face parts (i.e., forehead, eyebrows,
eyes, noise and mouth) are superimposed on top of this
background face, with the rectangle sizes for some
parts adjusted as necessary to fit the pieces together
tightly.
To eliminate the variation that arise in generated
composite images when a feature from a dark face is
superimposed on a light face, 3x3 mean filter is
convolved the boarder of each face part within the
generated composite images.
2.2.2 Individual Representation in a population,
each individual (composite image) has a chromosome.
The chromosome contains six genes (background,

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

forehead, eyebrows, eyes, nose and mouth) that represent
the genotype of the face part. Figure 3 gives the
chromosome representation. Where N represents the
number of original face part image as stored in database,
X and Y are the top left corner coordinate of a face part,
W is the width in pixels of face part, and finally H is the
height in pixels of a face part.

2.2.4 Selection Operator after the fitness values are
calculated for all sixteen individuals (including selected
composite image), eight individuals with higher fitness
values are selected and advance a copy of them together
with selected composite image into the mating pool
while the seven lesser fitness individuals are killed.
Figure 5 gives an example of applying selection
operation. In this figure, the empty places with label
denoted the seven lesser fitness individuals.

Figure 3 Chromosome structure
2.2.3 Fitness Function to calculate the fitness
function for each individual in the population, first the
witness must interactively select the nearest composite
image to the witness mental image from the population
of composite faces. Then the witness rates each face part
of the selected composite image. The rate is measured by
the percent of resemblance between selected composite
image and the witness mental image. Figure 4 depicts the
selected composite image (the composite face in the
upper) with its face part rates (below selected image).
Finally, the fitness value of each individual in the
population is calculated using the similarity between an
individual image and the selected composite image.
Similarity between individual image and selected
composite image is calculated according to the following
formula:
6
1
fitness = ∑
*R
i = 1 D + Pen

Where D is the difference value of face part i between
selected image and individual, R is the rate of
i
individual face part i, and Pen is one if D equal zero and
zero otherwise.

Figure 4 Selected composite image with its
rates

Figure 5 Example of selection operator

2.2.5 The Proposed Crossover Operator After parent
individuals are selected for the next generation, a process
of combining face parts through crossover is revealed. In
the proposed crossover, the selected composite image
and a parent image drawn from the mating pool is
recombined in their traits in a form face part against face
part in a sense as a synonym for the type of the substance
of inheritance. In the proposed crossover operator,
instead of crossover is applied with one crossover
probability p c , the new operator uses six crossover
probabilities one for each feature part according to the
rates of the selected composite image. The crossover
occurs between the selected composite image and the
eight individuals selected from the mating pool. Figure 6
gives an example of applying crossover operation.

Figure 6 Example of crossover operator

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

2.2.6 The Proposed Mutation Operator the mutation
operator may cause a more gradual change in the
appearance of individual facial features, and this might
have a substantial impact on the effectiveness of the
search procedure. Here, the mutation operator depends
also on the rate percent of image parts similarity to the
selected composite image parts. A single mutation occurs
on all offspring with the rates of face parts as probability
values. In case of low probability, less than thirty
percent, a mutated part can be replaced by any relevant
part that selected randomly from the database. While, a
mutated part can be replaced with uniform probability, if
the rate of similarity ranging from thirty to eighty.
Finally, in case of high probability, greater than eighty,
no mutation will be occurred. A single mutation could
produce a face that differs from the original face only at,
for example, the nose, but that new nose may be
completely different from the original one (e.g., we
might jump from a short pug nose to a long aquiline
one).

of sixteen different randomly created images from the
database, the initialization button must be selected.
After the population of composite images is
initialized, the user has to select one composite image
from these initial images. Then user has to rate each face
part of the selected composite image according to
witness mental image. By rating each face part, one can
then determine the similarity of each of the remaining
composite images of the initial generation with respect to
the selected composite one. In other words, calculate the
fitness function. Calculating the fitness function lets the
GFCS to select the best eight composite images from this
initial population. Figure 7 displays the eight composite
images with higher fitness values together with the
selected composite image. Note, figure below also
demonstrates how each face part is rated according to the
witness mental image.

2.2.7 Termination Criteria the termination criterion is
generally either a perfect solution or a predetermined
number of generations. So, the termination criteria used
by GFCS are:
• Facial composite image that is similar to the
witness mental face image.
• After fifteen generations.

2.3 Utility Tools
In GFCS, three main utility tools are used for improving
the quality of the resulted composite image. These are:
painting, smoothing, and sharpening.
The painting tool permits the user to works well to
alter the hairline, lengthen the chin, or make other
adjustments to various gross image features. The paint
color (i.e., the pixel intensity value) is selected by
clicking directly on the desired gray level in an image.
On the other hand smoothing and sharpening give the
resulted composite image more reality to photographic
quality images.

3. Case Study
Due to the intensive pictorial application of GFCS
program, it is coded in Visual Basic 6.0 for Windows,
which made the pictures handling subroutines simple.
When the GFCS is ran with thirty one different
images stored in database, the experimental results
showed it is possible to produce a good likeness
composite image to a user mental face image using
genetic search algorithm. To clarify the experiments, the
following paragraphs and figures give one case of these
experiments.
When GFCS start running, the main window will be
displayed on the screen. To generate an initial population

Figure 7 selected composite image, rating, and
best eight composite images
The next population of the composite images
presented to the user should be sufficiently varied, but
must also stay within the user’s space-of-interest. The
proposed crossover and mutation operators are found to
have an impact on the variation of composite images. To
generate the next sixteen composite image offspring,
each of the eight parents of composite images are crossed
with the selected composite one to generate two new
offspring per one parent crossing. For each parent, the
crossover is applied independently to each face part with
crossover probability determined according to the
selected composite image rates. This continues until a
total of sixteen new offspring are then generated.
Followed step is the mutation of each offspring part,
again according to its rate. Figures 8 and 9 show the
resulted composite images after respectively crossover
and mutation operations of one generation. The GA
operators are repeated until the target composite image is
satisfied after five generations. The last generation result
is given in figure 10.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

Conclusions

Figure 8 After crossover operation

From the GFCS design and results we conclude the
following. The proposed GFCS is an automatic facial
composite system that is based on recognition strategy.
Genetic algorithm is used in such away to improve the
search for a mental composite image. In addition, the
generated composite images at each generation are
generally good, but due to lighting, pose, and feature size
variations, some problems do arise. For example, the
boundary smoothing of each face parts within the
generated composite images is not always sufficient to
blend the differences when a feature from a very dark
face is super imposed on a very light face. This problem
can be solved using the method in [6].
Unlike previous works on facial composite systems,
in GFCS the user has to rate each facial part instead of
the whole image. GFCS work has concentrated on the
self-adaptation of crossover; however, the proposed
crossover used a self-adaptive approach that adjusted the
rate at which crossover is allowed to swap genes of face
parts. In addition to the crossover adaptation, GFCS also
incorporate self-adaptive mechanisms for selecting rates
of mutation.
Morphing algorithm [7] can be added for morphing
target composite image to reconstruct a facial image
nearest to eye witness.

References

Figure 9 After mutation operation

Figure 10 Results from generation Five

[1] McQuiston and Malpass, Use of Facial Composite
Systems in U.S. Enforcement Agencies, Eyewitness
Research Laboratory, University of Texas, El Paso,
2000.
[2] E. Baker, The Mug-Shot Search Problem A Study of
the Eigenface Metric, Search Strategies, and
Interfaces in a System for Searching Facial Image
Data, Ph.D. Thesis, Harvard University,
Cambridge, Massachusetts, 1999.
[3] C. J. Solomon, S. J. Gibson, and A. P. Bejarano,
EigenFit- the Generation of Photographic Quality
Facial Composites, University of Kent at
Centerburg.
Available
at:
http://www.citesser.nj.nec.com/, 2000.
[4] E. Baker and M. Seltzer, Evolving Line Drawing,
Proceeding Graphics interface '94, Wayne Davis
and Barry Joe, Editors, Pages 91-100 may 1994,
Morgan Kaufmann Publishers, 1994.
[5] P. Vanger, R. Hoenlinger, and H. Haken, Computer
Aided Generation of Prototypical Facial
Expressions of Emotion, Methods of Psychological
Research, Vol. 3, No. 1, 1998.
[6] Mayada F. Abdul-Halim and Nada H. Mohammad
Ali, Luminance Pre-processing with Facial
Composite System, 3rd Al-Rafidain University
College Conference, 2005.
[7] T.D. Bui, M. Poel, D. Heylen and A. Nijholt,
“Automatic Face Morphing for Transferring Facial
Animation”, Computer Graphics and Imaging,
2003, USA.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

