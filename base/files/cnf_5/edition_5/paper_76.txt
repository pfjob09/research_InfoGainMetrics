Augmented Reality Tangible Interface for Distributed Design Review
Ronald Sidharta, James Oliver, Adrian Sannier
The University of Tokyo, Iowa State University, Arizona State University
{ronalds@gmail.com, oliver@iastate.edu, sannier@asu.edu}
Abstract
Design review is a crucial step in the Product Creation
Process (PCP). In a design review, CAD software is often
used to show the design result, however, CAD software’s
interface complexity frequently prevents diverse team
members from directly interacting with the CAD models..
This is further exacerbated when the design review is
conducted in distributed fashion. The goal of this paper is
to consider an alternative to 2D desktop based interfaces
for design review, called Augmented Tangible Interface
(ATI). Moreover, as a continuation of our previous work
[16], this paper describes the use and the implementation
of the ATI for distributed design review.

Keywords--augmented
reality,
virtual
environment, CAD, collaborative design, information
architecture.

1. Introduction
Today’s designers and engineers rely on CAD
systems to help them finish a product efficiently and
accurately, while reducing cost and increasing
productivity in the midst of competition. As products get
more complex, the design review meeting requires the
participation of diverse team members. The diversity of
team members requires that collaborative tools be
transparent with little or no learning curve required of
their casual users. However the complexity of CAD
system’s interface can introduce a barrier for
participants. In a design review context, this barrier
prevents diverse team members from directly interacting
with the CAD objects. Communication is throttled by the
CAD system’s operator, restricting the flow of ideas.
This paper proposes an alternative to the traditional,
2D design review interface for the CAD system, an
alternative that breaks with the traditional mouse and
keyboard paradigm. In place of indirect 2D interface
widgets, we want to create something akin to a physical
object, an interface device that fuses input and output,
that can be handled, passed back and forth, inspected
visually and positioned relative to other objects. The goal
is to facilitate an interface that has almost no learning
curve in order to encourage focus on the design
discussion instead of the mechanics of interactions.

In considering this natural interface, we will focus
on several tasks that characterize a typical design review.
The tasks are:
3D browsing – the selection of one or more virtual
objects for detailed considerations from among larger
sets.
3D positioning and orientation – the six-degree of
freedom translation and orientation of virtual parts
relative to one another.
3D assembly/disassembly – the joining of virtual
parts to form complex assemblies, or the deconstruction
of more complicated parts into their constituent subassemblies.
Furthermore, as design teams grow larger and
concurrent design becomes increasingly common,
today’s design process requires teams from different
locations to collaborate, and to communicate the designs
collectively through a design review meeting. As an
effort to reduce the development cost, many enterprises
have opted to conduct the design review meeting
remotely by taking advantage of the wider availability of
broadband network.
In order to support distributed design review
meetings, application sharing software, such as
Microsoft Net Meeting, is commonly used on top of
the native CAD software. However, because most native
CAD software was not designed to be a distributed
application, this brute-force approach introduced more
challenges during the meeting, such as significant delay
in transmitting the pixels that depict the CAD software,
and confusion in determining the current controller of the
interface.
In this paper we will describe ATI, its
implementation, and our system’s architecture to support
distributed design review.

2. Related Research
A Virtual Reality (VR) environment immerses its
users to a virtual world, separated from the real world
environment. In contrast, Augmented Reality (AR) let its
users experience virtual objects in the real world [2][12].
Ishii [8] described Tangible User Interfaces (TUI) as
interfaces that couple digital information to physical
objects and environments. Kato and Billinghurst [9]
argued that a TUI is powerful because its physical
constraints can restrict how it can be manipulated and
because it is easy to use. However, it is difficult to see
the state of digital data associated with the interfaces. AR

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

overcomes the limitation of TUI’s display possibilities
by overlaying virtual images onto the physical interfaces.
In applying his concept, Kato [10] described the use
of an augmented reality tangible interface (ATI) for an
interior design application. In that application, a physical
paddle served as a tool to select and place furniture in a
miniature room. In 2003, Kato and Tachibana [11]
described a tabletop city planning system that used an
ATI to interact with the system. Specifically, it used a
physical cup to arrange various elements of a virtual city.
There are related researches that addressed
distributed design review mechanisms. Daily and
Howard [6] described the use of virtual reality
environment such as the CAVE for distributed design
review. Ahlers and Kramer [1] described a distributed
interior design application in the AR environment. Ishii
and Brave discussed the use of tangible interfaces in
remote collaboration setting [5]. We have not been able
to find a research that addressed distributed design
review with augmented reality tangible interfaces.

A computer vision algorithm (ARToolkit [9])
analyzes the images coming from the camera. It looks for
the black square markers on the faces of the cards and
cube or on the surface of the table and then uses those
markers to determine their 3D positions and orientations.
Using these data, an image fusion algorithm places
virtual objects in the scene, on top of the physical
interfaces, for display in the HMD. This creates an
augmented reality environment that combines virtual
objects and real world objects in a common scene (see
figure 2). The user interacts with the virtual objects by
manipulating the physical interfaces and by giving
speech-based commands. The speech commands are
captured by a microphone, recognized by the speech
recognition engine, and used to control the scene.The
system is connected to the network to send or to receive
data from other participants.

3. System Architecture
3.1. Hardware Overview
The system we created serves multiple participants
engaged in distributed design review of a set of CAD
models. Similar to the system that we built previously for
co-located design review [16], each participant wears a
head mounted display (HMD) with a video camera
attached to the front of it. Each user’s world view is
mediated through the camera, allowing the world view to
be augmented with virtual objects before being displayed
in the HMD. Through the combination of the HMD and
the camera, each user views the real world, and sees a
custom set of physical interfaces – a table, a set of cards
and a set of cubes – with CAD models overlaid on them.
These interfaces are equipped with black and white
markers with distinct patterns that allow our system to
identify the interfaces and their positions (see figure 1).

Figure 2. Above, a controller participant is using
the ATI to interact with a CAD model. The viewer
participant on the right, views the operation in
his HMD.

3.2 Network Architecture

Figure 1. In our collaborative system, each user
wears an 800x600 Olympus Eye-Trek 700 HMD
with a Logitech Quickcam attached to the HMD.

To support distributed design review, we connected
each user’s system to the high-speed network. In this
setup, we had to distinguish between the controller
participant, and the viewer participant. The controller
was a group member that used the ATI to control the
interaction with the CAD models, and the viewers were
the group members that strictly viewed the controller’s
interaction. This is similar to a common co-located
design review structure, where typically a group member
typically explains the design by showing a physical
mock-up model, while the other members observe the
controller’s demonstration. Figure 3 illustrates the
network architecture of our distributed design review
system. Every client was connected to the same server,
and the controller broadcasted his/her data to the viewers
via the server. There can only be one controller
participant at a time during the design review session,
and the rest are viewer participants. It is possible for a

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

viewer to request control. In that case, the controller
must agree to release his/her control.

virtual objects in their intended place in the real world.
The Image Analyzer module performs this view
registration in our AR system.

Figure 3. Network architecture of our system
Unlike a shared application model, in which there is
only one copy of the application running at a time, our
system uses a replicated architecture, in which each user
runs a copy of the application in his/her local machine.
Considering that the viewer participants are only
concerned about the CAD models, and the operation
being performed on them, we only distribute CAD
models’ transformation data, and other interaction
commands to the viewers instead of distributing the
pixels of the controller’s view. As a result, the controller
has an augmented reality view (figure 2), and the viewers
have a virtual reality view in their HMD.
Our replicated architecture approach assumed
that each user has an updated copy of the CAD models in
their local machine. This way, we were not concerned
with the challenges of distributing CAD data. Also, in
order to support verbal communication during the
meeting, we used a third party Voice-Over-IP application
(e.g. Skype).

3.3. Software Overview
During the design review, there is one controller
participant, and several viewer participants. The
controller participant uses ATI to manipulate the CAD
models, while the viewers view the controller’s operation.
Figure 4 illustrates the details of the ATI system for a
controller. In this setup, Image Analyzer and Interaction
Module process the video frames coming from the
camera. The results of the process, which are CAD
transformation data and interaction commands, are
forwarded to the scene graph to be displayed along with
the captured video frames, creating an augmented reality
view. In this section, we will describe the software
implementation for the controller participant.

3.3.1 Image Analyzer
In order to maintain the illusion that the real world
and the virtual objects coexist in the same space, an AR
system must perform view registration, recognizing the
real world’s structure in order to accurately place the

Figure 4. Illustrates the details of the ATI system
for the controller participant.
There are many ways to do view registration in AR.
In our work, since the virtual objects were always
associated or attached to physical objects, we realized
that we could confine the view registration problem to
the tracking of the physical objects with which the user
interfaces in the scene. Since the user’s eyes focus on the
physical objects while interacting with them, we decided
to use an optical tracking method. Furthermore, with
optical tracking, we didn’t need to attach bulky sensors
and its cables to the physical objects.
Square markers were attached to the physical objects
that the user would be manipulating. Computer vision
techniques were then used to analyze each video frame to
identify the markers in the frame and to determine the
3D position and orientation of those markers. The code
we wrote to perform this analysis was based on an open
source library called ARToolkit [9]. Video frames from
the camera were processed by ARToolkit, which
identified every visible marker’s unique pattern through
template matching algorithm and computed their 3D
position and orientation relative to the camera. The
identification numbers of the markers visible in the
frames were forwarded to the Interaction module along
with their orientation matrices.

3.3.2 Interaction Module
Based on the markers’ data from the Image Analyzer,
the Interaction module controls the ATI in our system.
There are three types of ATI, namely the Card Stack
interface, the Cube interface, and the Table interface.
There may be more than one ATI type in a given time,
and each ATI can interact with the others. In the next
section, we will discuss these ATI in detail, followed by
their interaction possibilities.
3.3.2.1 Card Stack Interface
The first task of most engineering design review is
model selection. Using a typical 3D visualization

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

program, model selection involves opening up some sort
of file navigator or database browser and then choosing
the correct 3D model from a list of files, which may or
may not be accompanied by their corresponding
thumbnail photographs.
Even when the thumbnails are present they do not
provide a useful representation of a 3D object because
they have limited degrees of freedom. File name and
thumbnail mediated interaction is an artifact of the
evolution of CAD interfaces. It does not model how
people work with real objects. If physical prototypes of
designs existed, they would likely be spread out on a
table for the participants to pick up and look at. The Card
Stack interface is designed to reduce 3D browsing
complexity by simulating the selection of physical
objects. The stack can be picked up, and manipulated,
providing direct control of the virtual objects as if the
physical prototypes existed.
The Card Stack interface consists of three cards with
a square marker on each of the card faces. The three
cards are stacked on top of each other, so the user can see
only one card at the time. Associated with the interface is
a 3D object database. When a user looks at the Card
Stack, a virtual object is displayed in the place of the
marker. By placing the bottom card on top of the stack,
the user tells the computer to display the next object in
the database; by moving that top card back to the bottom
of the stack, the user tells the computer to show the
previous object (see figure 5). By turning the Card Stack
over, the user can see the backside of the virtual object
(see figure 5).
Card Stack association with 3D object database must
be previously defined in a text file. To facilitate
browsing 3D objects originating from different folders,
we utilize multiple Card Stack interfaces; each folder is
represented with a Card Stack interface.

Figure 5. The Card Stack Interface
3.3.2.2 Cube Interface
After selecting 3D parts from the database, the
controller needs to interact with the parts, point out
particular features, manipulate them and position them
relative to one another.
Previously when the controller wanted to position
and orient the 3D parts, he/she were limited to the typical
CAD system’s 2D GUI interface, which did not allow
direct handling of the parts. The 2D GUI and mouse
combination is a time-multiplexed input, which means a
user has at most two degrees of freedom when
manipulating a 3D object. It is not possible to rotate and
translate that object simultaneously.
The ideal interface for a 3D position and orientation
task is the one that has a low learning curve, is intuitive

and allows simultaneous translation and orientation. The
Cube interface is designed to fit those criteria in order to
aid in the 3D translation/orientation task of design
review. The Cube interface is a wooden cube with a
marker on each side, each Cube has a 3D object
associated with it and the 3D object’s transformation
corresponds to the Cube’s transformation.
In our system, the Cube is useful for improving the
rotation control when viewing a 3D object and for
assembling and disassembling groups of objects. After
selecting an object from the Card Stack interface, a user
can copy that virtual object from the Card Stack onto the
Cube. This is done by exploiting the spatial relation of
the two interfaces. The user places the Card Stack
interface next to an empty Cube interface (see Figure 6).
Using a speech command, “Computer, Transfer,” the
user tells the computer to copy the virtual object
displayed on the Card Stack onto the visible face of the
Cube.

Figure 6. Transfer operation from the catalog to
Cube Interface
The scene graph in figure 7 shows the details of the
transfer operation. Before transferring, the Cube
interface does not have an object node, thus even it is
visible in the user’s view, it does not display any virtual
object (see figure 6 above). The Card Stack interface
currently displays the model with a certain orientation,
and it is defined by the matrix M. The matrix M is a
multiplication result of the position and orientation of
Card Stack interface (matrix C1), with the Object Group
modifier (matrix G1), and the object modifier itself (such
as scaling), such as matrix O1. So, M=C1*G1*O1. To
transfer the model to the Cube interface, we copied the
object node from the Card Stack node to the Cube node.
This copies all the information about the object, such as
the vertices, and the transformation matrix. When the
model is transferred to the Cube interface, we still want
to maintain the orientation M. In the Cube tree,
M=C2*G2*O2, with O2=O1 (because it was copied). C2
and G2 have been defined, so to get M for the Cube tree,
we need to modify O2. Thus: O2=((C2*G2)-1*M)*O2.
We want the virtual object to appear exactly on top of the
cube interface, thus we reset the translation part of O2
matrix to zeroes.
Using two Cube interfaces, our users are able to
position and orient 3D models with both hands, as if
interacting with the real physical prototypes. We use this
interface to demonstrate how these mechanical parts
would be assembled if someone were to build them in
the physical world.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

Figure 7. Scene graph illustrating the transfer
operation

proximity with each other before performing the join
operation.
After getting the translation information, we need to
compute the rotation for box 1 so that it orients itself
properly with box 2. To do that, we first take vector u,
and negate it to yield –u. Our goal is to determine the
transform that will line up vector –u with vector v. The
dot product of –u with vector v yields the angle between
them; call it theta. We choose the cross-axis, an
appropriate axis of rotation, by crossing the vector –u
and vector v.
With this information, we can create a rotation
matrix R, which is a rotation in –theta degrees about the
cross-axis. With the rotation matrix R and translation
vector T, we can create a matrix that joins Box 1 onto
Box 2 correctly. We call that matrix M.
This algorithm fits well with our fishing reel
assembly task, because all the connected parts on fishing
reels can rotate freely on one axis. To assemble two parts
that do not rotate, an up-vector must be defined with the
normal vector on the snap point.
To disassemble, we need two Cube interfaces. The
first Cube has the assembled part attached, and the
second Cube has no model attached to it yet. Through the
speech command “Computer, break” or “Computer,
release,” the Interaction module breaks the assembled
part by obtaining the last part assembled from the source
assembly, and putting it on the empty Cube interface.
This is similar to an undo operation.

Figure 8. Assembly using two Cubes
To assemble two related objects together, a design
reviewer would use two Cubes to put the two objects’
previously-defined snap points next to each other (see
Figure 6). The reviewer then gives the speech command,
“Computer, Join,” telling the program that it should join
the two images of the parts together. If the snap point
IDs match, an algorithm (described below) computes the
rotation and translation so the two objects snap correctly
(see Figure 9).
We had an idea of using the two cubes physical
proximity as an automatic trigger for the assembly
operation. But after further observation and discussion,
we realized that there are times when a user compares
two objects with two cubes and might not want to
assemble them. We chose require the user to specifically
command the computer to execute the operation for
assembly, and for consistency we chose this method also
for other operations.
Figure 9 illustrates the snap algorithm. Box 1 is
located at position A, and it has two attachment IDs. In
our example we use the colors red and green. Attachment
point red has a normal vector u. Box 2, located at
position B, has an attachment point red with normal
vector v. To join box 1 onto box 2, we first compute the
translation necessary to move an object from position A
to B, T=B-A. Our algorithm checks for matching
attachment point IDs, in this case, red with red, and
makes sure that the two attachment points are at certain

Figure 9. Assembly algorithm illustration

3.4 Data Distribution
The viewer participants in our system observe
controller participant’s browsing, positioning/orientation,
assembly/disassembly interaction in their HMD. Our

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

replicated system approach requires the viewers’ scene
graph to replicate the controller’s.
As seen in figure 10, the controller’s interaction data,
e.g.: CAD model number, its orientation and translation,
and interaction commands such as assembly are
transferred to viewer’s system via the network. These
data are used to update the scene graph in each viewer’s
system. Let’s consider figure 2 above as an example. The
controller was using the Cube interface to translate and
rotate a CAD model.

Figure 10. Software structure for viewer
participant
In this example, the translation and orientation data
in the Cube Interface’s matrix are transferred through the
network for the viewer participants. The result is that the
viewer participant can see the same CAD models being
rotated and translated in his virtual reality view (see
figure 2).
Lag is inherently unavoidable when
distributing the data to the remote participant. However,
because we only transfer basic data such as matrices, and
string commands, they are not bandwidth consumptive
compared to transferring the pixels that depict the CAD
system window. Our strategy works well for our system,
it managed to minimize the lag to an acceptable tolerance.
As mentioned previously, there can only be one
controller participant that controls the interaction with
CAD models during a distributed design review meeting.
However, it is possible for a viewer to request to control
the interaction from the current controller. The
mechanism works as follows. First a viewer, let’s say
viewer 3, must use the speech command “computer,
request control”, in which the local computer will
indicate the server that viewer 3 is waiting for a control.
Then the current controller will use the speech command
“computer, release control” that will let the server know
that viewer 3 can have the control. A controller cannot
release control if a viewer has not requested for a control.
When the control is given to that viewer, he/she uses the
ATI in his/her view to manipulate the CAD models in
his/her local system. This replicated approach ensures
that when a viewer becomes a controller, he/she can
directly interact with the CAD models with no lag. If we
ran the application on a centralized machine (similar to
NetMeeting’s approach), the interaction would be
significantly lagged.

4. Conclusions and Future Work
This paper presents three-dimensional tangible
interfaces for interacting with CAD models. We focused

on three main tasks during design review: browsing,
positioning/orientation, and assembling..
Our system employed vision based tracking for
recognizing the position and orientation of the tangible
interfaces. It depended on the marker recognition
algorithms implements in the ARToolkit library.
Compared to the magnetic sensor approach, in which
bulky magnetic sensors must be attached on the physical
objects, our approach provides an untethered user
interface. With this type of user interfaces, our users can
translate and rotate the objects freely, without being
limited by cables. However, ARToolkit requires that
every tracked object must have a distinct marker attached
to it. This approach facilitates reasonable accuracy
because the virtual objects show up directly on the
marker. Even though our implementation provided a
convincing illusion, the virtual objects still “shook”
noticeably at times due to tracking instabilities.
However, as camera resolution increases this shake
should be reduced to insignificant levels.
The rendering latency in our system is also
noticeable, a function of the frame rate of the camera.
Images become “blurry” if the head rotation rate is more
than 30 degrees per second. However, as camera frame
rates increase this motion blur would naturally reduce to
insignificant levels. Thus marker based tracking is a
promising avenue for overcoming the accuracy and
latency requirements of AR. However, using markers for
tracking (optical tracking) limits its range to marked
domains. The markers must never be too far from view,
and larger domains require larger markers in order to in
order to be seen from a distance. Comparable to a formal
user study result by Billinghurst [4], our users observed
that they have difficulties in pointing to the virtual
objects, because if their finger covers up the marker then
the virtual objects will disappear from view. And
obviously it is not possible to attach markers to every
object in the world if we want to have a ubiquitous AR
system.. Some of the users also found the bulkiness of
AR’s HMD-camera pair to be inconvenient. We need
better AR interface with better resolution, wider field of
view, faster refresh rate, and small in size in order to
make it more acceptable to more users.
In addition, we implemented a data distribution
mechanism to support ATI-based distributed design
review. Each user runs his/her own copy of the
application, while receiving interaction data (such as
matrices) from the controller member. This decentralized
approach was chosen over the application sharing
method (such as Net Meeting) in order to decrease lag.
Several studies have shown that non-verbal cues
(such as face and eye contact to assist with
conversational turn-taking) and spatial-cues (to
determine who’s talking to whom) are important during
collaborative meeting [7] [14]. However, currently our
distributed design review system does not support this. In
the future we plan to provide non-verbal and spatial cues
to our users. For example, to provide non-verbal cues,
we can use live video feed of the remote participants; to
provide spatial cues, we plan to position the live video

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

feed in the augmented reality space as to simulate the
physical position of remote participants.
Finally, a plan for formal user studies to measure the
effectiveness of remote design review using ATI is in
development. Billinghurst [4] described a user study that
measured the benefits of using AR interfaces in face-toface collaborative work. In our study, we plan to evaluate
our users (through performance timing and
questionnaires) in three design review situations. As a
baseline, we plan to evaluate the ideal situation of colocated design review using real physical mock-up
models. Second is the situation where the users have to
conduct the design review from separate physical
location (remote) using the usual 2D CAD interfaces.
The third is the situation where the users use ATI to
conduct design review remotely. Specifically, we are
interested in measuring the performance and users’
satisfaction in using 3D interfaces, and in executing the
control transfer mechanism.

[12] Milgram, P., Takemura, H., Augmented Reality: A Class
Of Displays On The Reality-Virtuality Continuum. SPIE
Vol. 2351, Telemanipulator and Telepresence
Technologies, 1994
[13] Regenbrecht, H., Wager, M., Interaction In A
Collaborative
Augmented
Reality
Environment,
Proceedings to CHI 2002, April 20-25, 2002.
[14] Sellen, A., Buxton, B., Using Spatial Cues to Improve
Videoconferencing. In Proceedings of CHI’92, May3-7,
1992, ACM: New York, pp. 651-652.
[15] Sutherland, I., Sketchpad: A Man-Machine Graphical
Communication System. Proceedings of AFIPS Spring
Joint Comp. Conf, 1963.
[16] Sidharta, R., Sannier, A., Oliver, J.,
Augmented
Tangible Interface For Design Review. Proceedings of
International Conference on Product Lifecyle
Management, 2005.

5. References
[1]

Ahlers, K., Kramer, A., Breen, D., et al., Distributed
Augmented
Reality
for
Collaborative
Design
Applications, In Proceedings of Eurographics’95,
September 1995.
[2] Azuma, R., A Survey Of Augmented Reality, Presence,
6(4), pp. 355-385
[3] Billinghurst, M, “Crossing The Chasm,” Proceedings of
International Conference on Augmented Tele-Existence
(ICAT 2001), Tokyo, Japan, December, 2001.
[4] Billinghurst, M., Kato, H., Kiyokawa, K., Belcher, D.,
Poupyrev, I., Experiments with Face to Face
Collaborative AR Interfaces, Virtual Reality Journal, Vol
4, No. 2, 2002.
[5] Brave, S., Ishii, H., Dahley A., Tangible Interfaces For
Remote Collaboration And Communication. Proceedings
Of CSCW 1998
[6] Daily, M., Howard, M., Jeralrd, J., et al., Distributed
Design Review in Virtual Environments. Proceedings of
third international conference on collaborative virtual
environments 2000, pp.57-63
[7] Hindus, D., Ackerman, M., Mainwarning, S., Starr, B.,
Thunderwire; A Field Study of An Audio-Only Media
Space. In Proceedings of CSCW’96, Nov 16th-20th, 1996,
New York, NY
[8] Ishii, H., Ullmer, B., Tangible bits: towards seamless
interfaces between people, bits and atoms, Proceedings
of the SIGCHI conference on Human factors in
computing systems, pp. 234-241
[9] Kato, H., Billinghurst, M., Marker Tracking and HMD
Calibration for a Video-based Augmented Reality
Conferencing System, Proceedings of the 2nd IEEE and
ACM International Workshop on Augmented Reality’99,
pp. 85-94
[10] Kato, H., Poupyrev, I., Virtual Object Manipulation On
A Table-Top Environment. Proceedings of ISAR2000,
pp. 111-119
[11] Kato, H., Tachibana, K., A City-Planning System Based
On Augmented Reality With A Tangible Interface,
Proceeding of ISMAR’03

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

