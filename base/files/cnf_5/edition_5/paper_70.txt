Facial Animation Using Emotional Model
Chihiro Kozasa, Hiromichi Fukutake, Hirokazu Notsu, Yoshihiro Okada and Koichi Niijima
Graduate School of Information Science and Electrical Engineering, Kyushu University
6-1 Kasuga-koen, Kasuga, Fukuoka, 816-8580 JAPAN
{c-kozasa, h-fuku, h-notsu, okada, niijima}@i.kyushu-u.ac.jp

Abstract
Recent 3D graphics hardware technologies have
made it possible to create visually realistic 3D
characters and 3D scenes in real time. And then, the
video game has become one of the main applications of
3D graphics technologies. However, behaviors of Non
Player Characters (NPCs) in video games are not
satisfactory because they are still predefined and then
very simple. To attract game players, NPCs should have
more complicated behaviors like the human. Facial
expression is one of the most important factors for such a
humanlike NPC. In this paper, the authors propose an
NPC which interacts with the human. The NPC changes
its facial expression according to its emotion during the
interaction. For realizing such an NPC, the authors
implemented a neural network based emotional model
unit [5]. By some experiments, this paper also shows
that the NPC can change its facial expression according
to its emotion like the human.
Keywords: CG Animation, CG Character, Facial
Animation, Neural Network, Emotional Model.

most important factors for such a humanlike NPC. In
this paper, we propose an NPC which interacts with the
human. During the interaction, the NPC changes its
facial expression according to its emotion. For realizing
such a NPC, we implemented a neural network based
emotional model unit [5] and developed the NPC using
IntelligentBox, a 3D graphics software development
system [7-9]. We performed some experiments of
interactions between the NPC and the human, and found
that the NPC can change its facial expression according
to its emotion like the human.
The remainder of this paper is organized as follows.
First of all, Section 2 presents related work, and we
introduce an overview of our NPC system in Section 3.
Next Section 4 describes our emotional model. Section 5
explains interactions between the NPC and the human.
Section 6 shows an artificial neural network used as the
emotional model unit of the NPC. Section 7 describes
implementation of the NPC with IntelligentBox.
Experiments of interactions between the NPC and the
human are shown in Section 8. In Section 9, we also
introduce some prospective applications of our NPC
system. Finally we conclude the paper in Section 10.

2. Related Work
1. Introduction
Recently 3D computer graphics has become
frequently used in movies and video games. In this
situation, the role of None Player Characters (NPCs),
which are 3D CG characters used in video games, has
become important, and personalities of NPCs have
become required for various believable behaviors to
attract game players. However, game designers have to
write a lot of scripts for all NPCs to define their
behaviors, and this requires much computation and
sophisticated data structures if there are too many NPCs
in the game. For these reasons, behaviors of NPCs in
conventional games are very simple, and then game
players can easily predict behaviors of NPCs and lose
their interest in playing it after they play several times.
To attract game players, NPCs should have more
humanlike behaviors. Facial expression is one of the

As work related to humanlike NPCs, Egges, et al.
developed a facial communication system for modeling
3D virtual characters with emotion and personality [1].
This system is for conversation among 3D virtual
characters, so emotion and facial expression changes of a
character are carried out by the conversation.
Interactions within the conversation are encoded as finite
state machines, and updates of the mood are performed
by using Bayesian Belief Network. As strongly related
work, Notsu, et al. proposed a component-based
approach for emotional behavior of 3D CG characters [6].
Similarly to our proposed system, their system was
developed using IntelligentBox and it also employed an
emotional model unit based on a neural network.
However, their system does not provide any component
for generating facial expressions of NPCs. As another
interesting research, Ioannou, et al. studied on the

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

emotion recognition through the facial expression
analysis based on a neurofuzzy network [3].

3. System Overview
Figure 1 shows an overview of our NPC system. In
the system, the user can communicate with an NPC by
keyboard inputs or voice inputs. When receiving such
inputs from the user, the emotional model unit generates
next facial expression and dialog message to be
presented to the user as outputs of the NPC system. In
this way, interactions between the NPC and the user are
carried out.

USER
Interaction䊶out

According to Byron [10], extroversion and
agreeableness are more significant rather than other
factors to characterize humans. Also we consider that
their two factors are enough to define the personality of
an NPC in games.

4.2. Mood
A character’s mood is specified by two-dimensional
factor, valance and arousal. Valance represents degree of
positive or negative, and arousal represents emotional
excitement. These two measures are fundamental
elements to express the mood and independent on each
other, so the combination of these measures determines
basal emotions. For example, we feel anger when
valance is low and arousal is high.

Interaction䊶in

4.3. Relationship
Output Expression
(Facial Animation)

Chat
(Voice)

Chat
(Voice)

Input Expression

Emotional Model
(Neural Networks)

Figure 1: Overview of the NPC system.

4. Emotional Model

The relationship among characters is an important
factor to determine their interaction. Human relationship
is dependent on various factors, social position, career,
family and so on. In this paper, interactions are done
only between the user and an NPC, so we consider it is
enough to express relationship by two-dimensional factor,
likableness and intimacy. For example, we feel the
yearning when likableness is high and intimacy is low.
Consequently, our emotional model consists of the
above six factors, i.e., Extroversion, Agreeableness,
Valance, Arousal, Likableness and Intimacy. Our NPC
system treats these factors as emotional parameters.

For realizing an emotional model of an NPC, we
referred to Social Unit of Persistent Agent Architecture
(PPA) proposed by Namee, et al. [5]. Social Unit
implemented mainly by a neural network decides NPC's
behavior from its Personality, Mood and Relationship. In
this section, we introduce our emotional model.

5. Interactions

4.1. Personality

5.1. Actions

To define the personality of an NPC, we employ five
factors model called Big Five [11]. Big five is based on
the idea that our human personality can be specified by
the combination of the fundamental characteristics. Five
factors and their characteristics are as follows:
1: Extroversion
Activity and energy level traits, sociability and
emotional expressiveness.
2: Agreeableness
Altruism, trust, modesty, pro-social attitudes.
3: Conscientiousness
Impulse control, goal directed behavior.
4: Neuroticism
Emotional stability, anxiety, sadness, and
irritability.
5: Openness
Breadth, complexity, and depth of an
individual life.

The user input entered to communicate with an NPC
is called “Action”. In our current NPC system, the
interaction is only conversation and the system supports
the following six actions:

An NPC interacts by receiving an input (Action)
from the user. During one interaction, the NPC performs
Emotion Change, Reaction and Expression Change based
on six emotional parameters.

1: Greet, 2: Chat, 3: Praise, 4: Scold, 5: Sorrow, 6:
Apologize

5.2. Input expressions
Generally, when a person communicates with other
person, he/she senses other person’s feeling by not only
contents of the conversation but also other person’s
facial and behavioral expression. So we employ the
user’s (facial) expression as input data. Types of the
user’s expressions are the same as those of NPC’s
expressions described later.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

NPC system, the interaction is only conversation and the
system supports the following nine reactions:

5.3. Emotion change
Six emotional parameters explained in Sec. 4,
Action and Expression explained above are input data
entered to a neural network for the emotional model and
the six emotional parameters are once updated by one
interaction. Other outputs from the NPC system, i.e.,
Facial expression, Reactions of an NPC, are yielded after
this update.

1: Greet, 2: Chat, 3: Rejoice, 4: Angry, 5: Sorrow
6: Apologize, 7: Tangle, 8: Forgive, 9: None

6. Artificial Neural Network for NPC
According to current values of the emotion
parameters and the user’s input, action and input
expression, an NPC changes its emotion parameters and
determines its output expression and reaction by an
artificial neural network. The back propagation of error
was used to train the network. The network is a three
layer perceptron network, which has a single middle
layer.

5.4. Output expressions
Facial expressions of an NPC represent its emotional
feelings. At each interaction, a corresponding expression
is determined based on emotional parameters and the
user’s input, action and expression. Our current NPC
system can generate five kinds of facial expressions as
shown in Figure 2. Among of them are:

6.1. Neural network construction
There are three kinds of output for the emotion
update, the facial expression and the reaction. They are
independent on each other, so we construct three neural
networks. Each neural network for the emotion change,
for the facial expression and for the reaction has six
output nodes corresponding to six emotional parameters,
five output nodes corresponding to five facial
expressions of a 3D character, and nine output nodes
corresponding to nine reactions, respectively. The output
layer nodes output a value in 0.0 to 1.0. These three
networks have the same input layer and middle layer.
The input layer consists of eight nodes. The six nodes of
them corresponding to six emotional parameters,
extroversion, agreeableness, valance, arousal, likableness
and intimacy, accept a value in 0.0 to 1.0. Input
numerical value for the node corresponding to the action
is converted into one of the six actions, for example,
action “Greets” is represented as a value “0.1”, action
“Chat” is represented as a value “0.2”, and so on. The
user’s expression is also converted into numerical value
similarly to the case of the action. The middle layer
consists of twenty nodes.

1: Neutral, 2: Smile, 3: Anger, 4: Sadness, 5: Surprise.
This facial animation component is also realized as a
certain composite component of IntelligentBox. Its
implementation was achieved based on the method
proposed in the paper [4].

1: Neutral

2: Smile

3: Anger

4: Sadness
5: Surprise
Figure 2: Five kinds of facial expressions.

5.5. Reactions

6.2. Training of neural networks

The other output from the NPC system besides facial
expressions of an NPC is “Reaction”. In our current

The neural networks require a training data set for
their training. However there are no available databases

Sample input data

Sample output data

Emotional parameters
Action
Extro

Agreeable

Valance

Arousal

Likability

Intimacy

0.4
0.5
0.7
0.9
0.2
0.6
0.1
0.3
0.8

0.2
0.8
0.3
0.7
0.9
0.1
0.4
0.5
0.6

0.7
0.5
0.2
0.6
0.3
0.2
0.9
0.1
0.8

0.7
0.3
0.8
0.1
0.4
0.7
0.2
0.5
0.9

0.6
0.2
0.3
0.8
0.4
0.3
0.1
0.9
0.5

0.5
0.1
0.8
0.2
0.6
0.9
0.7
0.3
0.4

0.1
0.2
0.3
0.4
0.5
0.6
0.1
0.2
0.3

Output expressions

Input
expression

Neutral

Smile

Anger

Sadness

Surprise

0.1
0.2
0.3
0.4
0.5
0.1
0.2
0.3
0.4

0.5
0.7
0.4
0.4
0.5
0.5
0.7
0.6
0.01

0.7
0.5
0.01
0.01
0.01
0.01
0.3
0.01
0.7

0.01
0.1
0.7
0.1
0.01
0.7
0.01
0.01
0.01

0.01
0.01
0.01
0.8
0.7
0.01
0.01
0.5
0.1

0.01
0.4
0.8
0.3
0.3
0.01
0.01
0.2
0.3

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

which contain the believable information on how people
interact with each other in a certain situation. So we
manually created an artificial data set as follows. For
example, we can assume that when a selfish person, i.e.,
his/her extroversion is high and agreeableness is low, is
angry, i.e., his/her valance is low and arousal is high,
he/she will be anger expression and will take anger
reaction at higher probability if he/she is praised by the
other party who look down on him/her. In this way, we
can create a training data set subjectively. One example
of an artificial data set for output expressions is shown in
Table 1. The value in bold-face is the highest in each
output data and the corresponding expression is used as
output expression of an NPC.

7. Components of IntelligentBox for NPC system
IntelligentBox is a constructive visual 3D software
development system developed by Okada, et al. [7, 8, 9].
IntelligentBox provides 3D reactive objects as its
primitive components called boxes. Each box has a
unique functionality and a 3D visible shape. Its
functionality is related to variables called slots. By
transmitting slot values among several boxes, their
functionalities are combined. IntelligentBox also
provides a dynamic data linage mechanism called ‘slot
connection’ that allows users to construct 3D graphics
applications only by combining existing boxes through
direct manipulations on computer screen.
There is previous work about NPCs using
IntelligentBox [6] and some useful boxes already exist
[2] so that we developed our NPC system using
IntelligentBox.

7.1. New boxes
To implement the NPC using IntelligentBox, we
developed the following four boxes, i.e., NNEmotionBox
ActionDialogBox,
ExpressionInBox
and
ReactionDialogBox. One new composite box which has
the function of the NPC is also constructed by
composing these newly created boxes and by connecting
their slots. Figure 3 illustrates parent-child relationship
hierarchy of the component boxes.
3DFace

NNEmotionBox

7.2. Functionality of each box
First of all, the function of NNEmotionBox is to
hold six emotional parameter values, constructs three
neural networks each for the emotion change, for the
facial expression and for the reaction by loading trained
data files and calculates output results of the neural
networks from the input values. Action parameter, one
of the input data is received through ActionDialogBox as
an integer value called “Action-type” that corresponds to
one of the six actions explained in 5.1 and the integer
value is converted into a corresponding float number as
input value “Action”. For example, 1 is converted into
0.1 because 0.1 means Action No. 1, Greet. User’s
expression, i.e., the input expression, is received through
ExpressionInBox as an integer value called “ExpressionIn” that corresponds to one of the five expressions shown
in 5.4 and the integer value is converted into a
corresponding float number as input value “Input
expression” as well as “Action”. Emotional parameters
are updated by the calculated output results of the neural
network dedicated for the emotion change. Output
expressions are calculated as output results of the neural
network dedicated for the output expression. The one of
the output expressions that has the highest value is
chosen as next expression of an NPC just after the
interaction. Similarly, reactions are calculated as output
results of the neural network dedicated for the reaction.
The one of the reactions that has the highest value is
chosen as “Reaction-type”, next reaction of an NPC just
after the interaction.
Information of the chosen
expression is sent to the facial animation component
described in 5.4 to display its corresponding facial
expression. Information of the chosen reaction is also
sent to ReactionDialogox to make a 3D character take its
corresponding reaction.
The functions of the other boxes are as follows: The
function of ActionDialigBox is to receive the user’s
action as the text input from InputBox and to send its
corresponding integer value as “Action-type” to
NNEmotionBox. The function of ExpressionInBox is to
receive the user’s expression selected from five kinds of
expressions by the user’s pushing the corresponding
ToggleSwitchBox and to send a corresponding integer
value as “Input-expression” to NNEmotionBox. The
function of ReactrionDialogBox is to generate a text
string as “Reaction-type” corresponding to the integer
value sent from NNEmotionBox and to send it to
StringBox to display it on a computer screen.

8. Experiments
ActionDialogBox

ExpressionIn

ReactionDialogBox

ToggleSwitchBox

InputBox
Voice Input Box

ToggleSwitchBox

StringBox
…

or

or

Speech Box

Figure 3: Parent-child relationship hierarchy of boxes
and their data flow.

Figure 4 shows a screen snapshot of the NPC
developed using IntelligentBox. We performed some
experiments. In these experiments, another component
called VoiceInputBox [2] is used instead of InputBox (1
in Figure 4), and SpeechBox [2] is used instead of
StringBox (2 in Figure 4). The function of
VoiceInputBox is the dictation of the human voice, and
generates a text string. The function of SpeechBox is to

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

convert a given text string into the corresponding voice
sound. One interaction is carried out as follows: The
text string generated by VoiceInputBox is sent to
ActionDialogBox (3 in Figure 4). This box sends Actiontype to NNEmotionBox (4 in Figure 4).
ExpressionInBox (5 in Figure 4) has five
ToggleSwitchBoxes each of which corresponds to each
expression. NNEmotionBox receives the Expression-In
from ExpressionInBox, and calculates each neural
network. Output expression chosen by the calculation
result is sent to a 3D face character (the center of Figure
4), and then the character makes the corresponding facial
expression. Reaction-type chosen by the calculation
result is sent to ReactionDialogBox (6 in Figure 4), and
the corresponding reaction text string will be generated.
SpeechBox receives that text string and generates a voice
sound. The group of purple boxes (7 in Figure 4)
indicates the output results of the neural network of the
reaction, Greet, Chat, and so on. The group of blue boxes
(8 in Figure) indicates the output results of the neural
network of the output (facial) expression, Smile, Anger,
and so on. The group of green boxes (9 in Figure 4)
indicates emotional parameter values, Extroversion,
Agreeableness, and so on.

Figure 5 shows a screen snapshot of Experiment 1.
The NPC said “sorry (Action-type is Apologize)” against
the user with expression “sadness”, when NPC’s
Extroversion, Agreeableness, Likableness and Intimacy
are high, and Valance and Arousal are low. The output
result of the neural network of the output expression
shows that Sadness is the highest value. So the NPC’s
facial expression is sadness. The output result of the
neural network of the reaction shows that Forgive is the
highest value so the NPC says “Don’t mind”.
Figure 6 shows a screen snapshot of Experiment 2.
The NPC said “sorry” against the user with expression
“sadness”, when NPC’s Extroversion is high and
Agreeableness is low (that personality called “selfish”),
and Valance is low and Arousal is high (that mood called
“Angry”), and Likableness and Intimacy are low. The
output result of the neural network of the output
expression shows that Anger is the highest value. So the
NPC’s expression is anger. The output result of the
neural network of the reaction shows that Angry is the
highest value. So the NPC says “Shut up!”

Figure 6: Screen snapshot of Experiment 2.
Figure 4: Screen snapshot of the NPC using
IntelligentBox.

Figure 5: Screen snapshot of Experiment 1.

A
B
C
Figure 7: Screen snapshot of Experiment 3.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

Figure 7 shows a screen snapshot of Experiment 3.
That indicates emotion changes. The part A of the figure
shows the emotional parameters before a certain
interaction, Valance is 0.46 and Arousal is 0.42. The
part B of the figure shows the parameters after the
interaction that the user says “good (Action-type is
Praise)” with expression “Smile”. Valance is 0.63 and
Arousal is 0.62, so both are increased. In addition,
Likableness and Intimacy are increased, and
Extroversion and Agreeableness are increased slightly.
The part C of the figure shows the parameters after
the other interaction that the user says “no (Action-type
is Scold)” with expression “Anger”. Valance is 0.28,
decreased, and Arousal is 0.49, increased. Likableness
and Intimacy are decreased, and Extroversion is
decreased slightly, but Agreeableness is increased
slightly.

9. Prospective Applications
Although our NPC system proposed in this paper
takes reactions only as the facial animation and the sound
reply, its emotional model unit can be used for various
applications. For example, it seems possible to realize an
intelligent agent represented as an emotional 3D
character using our emotional model unit. Such a
humanlike character might be useful for the
rehabilitation of the person who cannot take good
communications with other persons. We are also
supposed to introduce the emotional model unit of the
system into a four-legs entertainment robot AIBO to
make it enables to take more realistic behaviors
emotionally besides NPCs in video games.

10. Conclusions
In this paper, we proposed an NPC which interacts
with the human with changing its facial expression
according to its emotion. For realizing such an NPC, we
implemented a neural network based emotional model
unit and developed the NPC using IntelligentBox. Our
emotional model is realized from the personality, the
mood and the relationship.
We performed some
experiments of interactions between the NPC and the
human, and we found that the NPC can change its facial
expression according to its emotion like the human. In
this paper, we also showed some prospective
applications of our NPC system.
As future work, we will improve our NPC system to
enable an NPC to behave autonomously without the user
input because a more humanlike NPC should behave
autonomously and its emotion should change
automatically according to the time passing. Also, we
will develop a more sophisticated model where several
NPCs take communications mutually automatically. And
we must develop a better conversation system because
the communication with NPC that has few pattern of
conversation is not satisfactory.

References
[1]

Egges, A., Zhang, X., Kshirsagar, S. and Thalmann, M,N., Emotional communication with virtual humans. The
9th International Conference on Multi-Media Modeling.
January 7-10, 2003, Taiwan.
[2] Fukutake, H., Okada Y. and Niijima, K., 3D VISUAL
COMPONENT BASED VOICE INPUT/OUTPUT
INTERFACES FOR INTERACTIVE 3D GRAPHICS
APPLICATIONS. Proc. of 5th Game-On International
Conference on Computer Games: AI, Design and
Education (CGAIDE 2004). pp. 216-220, 2004.
[3] Ioannou, S., Raouzaiou, A., Tzouvaras, V., Mailis, T.,
Karpouzis, K., and Kollias, S., Emotion recognition
through facial expression analysis based on a neurofuzzy
method. Neural Networks: 18, pp. 423-435, 2005.
[4] Kalra P., Mangili A., Thalmann, M,-N. and Thalmann D.,
Simulation of Facial Muscle Actions Based on Rational
Free Form Deformations. Proc. Eurographics '92. Issue
3 pp. 59-69, 1992.
[5] Mac Namee, B., Cunningham, P., Proposal for an Agent
Architecture for Proactive Persistent Non Player
Characters. Proceedings of the Twelfth Irish Conference
on Artificial Intelligence and Cognitive Science. pp. 221-232, 2001.
[6] Notsu, H., Okada Y. and Niijima, K., COMPONENT
BASED APPROACH FOR EMOTIONAL BEHAVIOR
OF 3D CG CHARACTERS. Proc. of 5th Game-On
International Conference on Computer Games: AI,
Design and Education (CGAIDE 2004). pp. 20-24,
Reading, UK, November 2004.
[7] Okada, Y., Tanaka, Y., IntelligentBox: A Constructive
Visual Software Development System for Interactive 3D
Graphic Applications. Proc. of Computer Animation '95,
IEEE Computer Society Press. pp. 114-125, 1995.
[8] Okada, Y., Tanaka, Y., Collaborative Environments of
IntelligentBox for Distributed 3D Graphics Applications.
The Visual Computer. Vol. 14, No. 4, pp. 140-152, 1998.
[9] Okada, Y., Itoh, E., IntelligentBox: Its Aspects as a
Rapid Construction System for Interactive 3D Games.
Proc. of First International Conference on Intelligent
Games and Simulation, SCS Publication pp. 114-125,
2000.
[10] Reeves, B. and Nass. C. 1998 “The Media Equation:
How People Treat Computers, Television, and New
Media Like Real People and Places”, Univ. of Chicago
Pr.
[11] Tanno, Y. 2003 “Psychology of Personality: Personality
considered from Big five and Clinical,” SAIENSU-SHA,
Tokyo (in Japanese).

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

