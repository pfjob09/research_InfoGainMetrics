Unlabeled Data Classification via Support Vector Machines and k-means
Clustering
Li Maokuan,Cheng Yusheng,Zhao Honghai
Department of Underwater Acoustic Engineering
Navy Submarine Academy
Qingdao 266071 China
E-mail: limaokuan@sohu.com
produced as digital data format, generally the data is
unlabeled. It is impossible to classify this data with one‚Äôs
own hand one by one in many realistic problems, so that
the research on unlabeled data classification has been
grown. In addition improvements in databases technology,
computing performance and artificial intelligence have
contributed to the development of intelligent data analysis.
Studies on data mining and knowledge discovery from
databases approach information flood problem from
information sources‚Äô side. This paper presents a learning
algorithm based on support vector machines to classify
unlabeled data more accurately after training the data
labelled by k-means clustering algorithms.
For classifying unlabeled data, which is our principal
purpose here, we will make use of the k-means algorithm
in combination with SVMs as follows: The k-means
algorithm is used to label the data firstly into a certain
number of subsets, then the labelled data will be trained
by SVMs.
This paper is laid out as follows: First, a description of
SVMs, then the k-means clustering algorithm, at last the
result, which show the performance of proposed
algorithm and the conclusion.

Abstract
Support vector machines(SVMS), a powerful machine
method developed from statistical learning and have
made significant achievement in some field. Introduced in
the early 90‚Äôs, they led to an explosion of interest in
machine learning. However, like most machine learning
algorithms, they are generally applied using a selected
training set classified in advance. With the repaid
development of the internet and telecommunication, huge
of information has been produced as digital data format,
generally the data is unlabeled. It is impossible to classify
the data with one‚Äôs own hand one by one in many
realistic problems, so that the research on unlabeled data
classification has been grown. Improvements in
databases technology, computing performance and
artificial intelligence have contributed to the development
of intelligent data analysis. In this paper, a SVMs
classifier based on k-means algorithm is presented for the
classification of unlabeled data.
Key words: Support Vector Machines, Data Mining, kmeans clustering

1. Introduction

2. Overview of SVMs

SVMs have been employed in a wide range of real
world problems such as text categorization, hand-written
digit recognition, tone recognition, image classification
and object detection, microarray gene expression data
analysis. It has been shown that SVMs is consistently
superior to other supervised learning methods. However,
like most machine learning algorithms, the traditional
statistic machine learning technologies require large
number of labelled training data to learn accurately. They
are generally applied using a selected training set
classified in advance. To obtain enough training data, we
have to label on these huge training data by hand. With
the repaid development of the internet and
telecommunication, huge of information has been

Support Vector Machines are powerful tools for data
classification. Since it was first introduced by Vapnik and
his group from the theory of structural risk minimization
at AT&T Bell Labs in 1995, there has been a surge of
interest in Support Vector Machines for many aspects of
data mining. Support Vector Machines have empirically
been shown to give good generalization performance on a
wide variety of problems. SVMs find the discrimination
hyperplane by solving a mathematical programming
problem with an objective function, which balances
between maximal separation and errors in linear
inseparable case.
In this section we shortly review some basic work on
support vector machines for classification problems.

-1Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV‚Äô04)
0-7695-2178-9/04 $20.00 ¬© 2004 IEEE

Consider the problem of separating the set of training
vectors belonging to two separate classes:
( x , y ),( x ,y ),xi¬èR n ,y ¬è{ 1,1 }
1 1
l l
l
firstly training vectors are mapped into a higher (maybe
infinite) dimensional feature space
I ( x ) ( M 1 ( x ),M 2 ( x ),M N ( x ))

max W a  
s, t : aT x

[ i t 0 ,i

(1)

a

1

l
l
¬¶ ai ( yi ( w x I( xi )  b )  1  [ j  ¬¶ J i [ j )
i 1
i 1

(2)

where ai , J i are the Lagrange multipliers, and
ai t 0 ,J i t 0 , i

1, , l

From the optimization condition

wL
ww

0,

wL
wb

0,

wL
w[

0

(3)



T

[ 1

1  1

[ y1

y2



y l 1

 1 ],
yl ]

T

3. k-means clustering algorithm

derived ¬¶ ai y i 0 , w ¬¶ ai y i I ( xi ), c  ai  J i 0
(4)
substitute (4) for (2),the problem is maximizing the
following function
max W (a)

(6)

based on KT condition, the hyperplane is
(8)
¬¶ ai y i K ( x , xi )  b 0
support vectors
The classifier is
f ( x , w, b ) sgn(
ai yi K ( x , xi )  b ) (9)
¬¶
support vectors
We use SVMs for classification. However, they are
binary classifiers. It is well known that a multi-class
problem can be reduced to a number of binary-class
problems. There are several approaches for extending
SVMs to multiclass classification problems. One-againstall multiclass SVMs is used in this paper. A set of binary
classifiers is learnt for each class against the all remaining
classes The classification strategy is based on selecting
such class, which has the highest value of discriminate
function.

Solving the optimization problem by Lagrange multipliers
algorithm

l
w x w  c ¬¶ [i 
i 1
2

T

f

1, , l

L( w, b,[ , a,J )

0,

0 d ai d c, i 1,  , l
The optimization problem leads to the following
quadratic optimization
1 T
T
min x Hx  f x
2
(7)
T
s .t : a x 0 ,0 d x d c
where
x a , H i , j y i y j K ( xi , xi ), i , j 1 , l ,

Then constructs an optimal separating hyperplane
y( x ) sgn[ w x I ( x )  b ]
In this way, the linearly non-separable problem is solved.
That is to say, the support vector machines require the
solution of the following optimization problem:
l
1
min J ( w ,[ )
w x w  c ¬¶ [i ,
i 1
2
s .t : y i [( I ( xi ) x w  b )] t 1  [ i

l
1 l
¬¶ a a y y K ( X i, x j )  ¬¶ ai
2 i, j 1 i j i j
i 1

k-meams clustering algorithm is an unsupervised
classification and is useful in data mining ,document
retrieval, image segmentation, and pattern classification.
It is easy to use and popular.

l
1 l
¬¶ a a y y (I ( xi ) x I ( x j ))  ¬¶ ai (5)
2 i, j 1 i j i j
i 1

Let X be denoted by a vector X

In fact, the support vector machines do not make dot
product I ( xi ) x I ( x j ) in high dimensional feature space,

( x1 , x 2  , x n ) in

n

the n-dimensional real space R ,where the components
xi are called features. A data point set is denoted by

but replace it with a kernel function K( xi ,x j ) The kernel

F

( X 1 , X 2 , , X l ) and the i-th data point X i in F is
represented by
X i ( xi1 , xi 2 , , xin ) .

function K ( xi ,x j ) is a symmetric positive definite
function, which satisfies Mercer‚Äôs Conditions. It is easier
to solve the optimization problem

The k-means algorithm is as follows:

-2Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV‚Äô04)
0-7695-2178-9/04 $20.00 ¬© 2004 IEEE

Step 1.Divide l samples into k subsets S i ( i
where S i  S j

I for i z j

1,2 , , k ) ,

Let the number of element

of the i -th subset S i be N i .Compute the centroid mi of
each subset S i ,error function J e and set t
mi

1
Ni

¬¶ X i and J e

X i ¬èSi

2

k

¬¶0 ¬¶

i 1 X i ¬èSi

Step 2. Select randomly a pattern

0 ,where

X i  mi

X i and suppose that

X i ¬è Si .
Step 3.If

Ni

1 ,go to step2.else continue.

Step 4.Compute
¬≠ Nj
|| X i  m j
¬∞
¬∞ N j 1
Uj ¬Æ
Nj
¬∞
|| X i  m j
¬∞ N j 1
¬Ø
Step 5.If U c

Figure 1. The plotted original (testing) data with
label l

|| 2 , j z i
|| 2 , j z i

min {U j } and

( j 1, 2,, k )

c z i ,then remove

X i from S i into S c else go to step 2.
Step 6. Recompute

mi , mc and J e .

Step 7.If || J et l  J el || H stop, else set t
to step 2, where H is a tolerance.

t  1 ,and go

4. Data classification implementation
Figure 2. The plotted original(training) data
without label

The implementation of the proposed algorithm is
promising. The experiments were run on a PC with a
Pentium IV 2400 MHz processor and 256MB memory.
To test the effectiveness of this algorithm, a artificial data
[2h549] was produced with the label l ,2 is the data
dimension, and 549 is the number of data(Fig 1).We use
the labelled data as an testing data and the data without
label as training data(Fig.2). The aim is to divide the data
into k classes(k=5) Firstly, partitioned the data into k
mutually exclusive clusters using k-means clustering, and
returns a vector of indices indicating to which of the k
clusters it has assigned each observation ,that is the new
label l' (shown in Fig 3) ,then trained the multiclass
SVMs classifier with the clustered data and new label to
determine the decision boundary and support vectors(Fig
4).At last, test the classifier with the testing data . The
SVMs classifier used is one-against-all multiclass
SVMs.The used CPU time when training is 1.8280
second and the number of support vectors is 60, the most
exciting result is that the classification error is less than
2%.

Figure 3. Plotted data with new label l'

-3Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV‚Äô04)
0-7695-2178-9/04 $20.00 ¬© 2004 IEEE

improves the performance of SVMs classifier using
labelled data created by k-means clustering. And the
algorithm will show great performance in automatic text
classification and other data mining fields.

6. Acknowledgements
The authors thank Mr. Xie Jun(Office of Naval
Submarine Academy) for providing valuable advice. and
Mr. Li Yan, a postgraduate in Naval Submarine Academy,
for his programming.

7. References
Figure 4.Plotted decision boundary and support
vectors

[1] Bian,Z.G.Pattern recognition.2nd edition,Beijing,Tsinghua
University Press,2000.
[2] V. Vapnik, The Nature of Statistical Learning Theory,
Springer-Verlag, New York, 1995.
[3] P. S. Bradley and O. L. Mangasarian , Massive data
discrimination via linear support vector machines, Optimization
methods and software 13,1-10, 2000.
[4] C. J. C. Burges, A Tutorial on Support Vector Machines for
Pattern Recognition, submitted to Data Mining and Knowledge
Discovery,
http://svm.research.bell-labs.com/SVMdoc.html,
1998.
[5] O. L. Mangasarian, Mathematical programming in data
mining, Data Mining and Knowledge Discovery 1,183-201,
1997.
[6] O. L. Mangasarian and D. R. Musicant, Data Discrimination
via Nonlinear Generalized Support Vector Machines, Technical
Report 99-03,Computer Sciences Department, University of
Wisconsin, 1999.
[7] C. Cortes and V. Vapnik, Support vector networks. Machine
Learning, 20:1-25, 1995.
[8] B. Sch¬®olkopf, A. Smola, R. C. Williamson, and P. L.
Bartlett. New support vector algorithms. Neural Computation 12,
1207 ‚Äì 1245, 2000.
[9] H. Drucker, C.J.C. Burges, L. Kaufman, A. Smola, and V.
Vapnik. Support vector regression machines. Advances in
Neural Information Processing Systems, 9: 155-161, 1997.
[10] E. Osuna, R. Freund, F. Girosi, Training Support Vector
Machines: An Application to Face Detection, Proc. Computer
Vision and Pattern Recognition ‚Äô97, 130-136, 1997.
[11] T. Joachims, Text Categorization with Support Vector
Machines, LS VIII Technical Report, No. 23, University of
Dortmund.

Figure 5.the classification result

5. Conclusion
The algorithm is fast, accurate and robust for data
analysis. k-means clustering is an unsupervised learning
method, and does not create a tree structure to describe
the groupings in your data, but rather creates a single
level of clusters. Another difference is that k-means
clustering uses the actual observations of objects or
individuals in your data, and not just their proximities.
These differences often mean that it is more suitable for
clustering large amounts of data. The support vector
machines is a supervised learning method in which
labelled data must be used when training. This algorithm

-4Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV‚Äô04)
0-7695-2178-9/04 $20.00 ¬© 2004 IEEE

