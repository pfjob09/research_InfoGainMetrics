2012 45th Hawaii International Conference on System Sciences

An Integrated Decision Support Model to Assess Reviewers for Research
Project Selection
Wei Xu
School of Information,
Renmin University of China,
Beijing, 100872, China
weixu@ruc.edu.cn

Wei Du
School of Information,
Renmin University of China,
Beijing, 100872, China
duwei_0127@163.com

Qian Liu
Division of Finance,
Agricultural University of Hebei,
Baoding, 071001, China
qianliu@hebau.edu.cn

Wei Wang
School of Information,
Renmin University of China,
Beijing, 100872, China
wangwei_idb@hotmail.com

agency. The review results are collected, and ranked
based on the aggregation methods [1]. For example,
there are the following stages for research project
selection in the Natural Science Foundation of China
(NSFC): proposal submission, proposal grouping,
proposal assignment to reviewers, peer review,
aggregation of review results, panel evaluation, and
final awarding decision [2]. As can be seen from the
research project selection process, reviewers play a
significant role in research project selection, because
their opinions will have great influence on the decision
of the projects which will be funded.
Previous research deals with general project
evaluation and selection process and several formal
methods and models are available for this purpose.
Decision support methods/systems have been
developed for project selection process [2-6]. A fuzzy
AHP method is applied to government-sponsored
project selection [7]. A fuzzy DEA and knapsack
formulation integrated model is offered for project
selection [8], while a hybrid fuzzy DEA and Bayesian
belief network evaluation model is suggested [9].
Similarly, an AHP and fuzzy TOPSIS method is
proposed for project selection [10], and an integrated
DEA and balanced scorecard approach is suggested
[11].
In recent years, some specific certain phases to
support research project selection are proposed. A
multilingual ontology framework is suggested to
support research project selection [12]. A hybrid
knowledge-based and modeling approach is offered to
assign reviewers to proposals for research project
selection [13], while a decision support approach is
presented for assigning reviewers to grouped proposals,
and genetic algorithm is employed to solve the
assignment problem [14]. An integrated method for

Abstract
Reviewers play a significant role in research
project selection, because their opinions will have
great influence on the decision of the projects which
will be funded. Current methods mainly focus on the
evaluation of reviewers’ competitiveness in a certain
research areas, while ignoring the performance of
reviewers whether is suitable to review a proposal. In
this paper, a group decision support model is proposed
to assess reviewers’ performance in peer review
process. In the proposed model, competitiveness and
relevance of reviewers are considered to evaluate
reviewers. It integrates analytic hierarchy process
(AHP), scoring method and fuzzy linguistic processing.
An illuminating example is used to show the
effectiveness of the proposed method. The results find
that the proposed method is a reasonable method to
assess reviewers, and it can be potentially applied to
reviewer evaluation in the National Natural Science
Foundation of China (NSFC).

1. Introduction
Research project selection is an important and
recurring activity in many organizations, and it is also
a challenge task that takes place in a complicated and
multi-stage decision-making process. It begins with a
call for proposals (CFP), which is distributed to the
relevant communities, such as universities and research
institutions. Proposals are then submitted to the body
(e.g., funding agencies) that issued the CFP. Then
these proposals are sent to reviewers for peer review.
Reviewers normally review the proposals according to
the instructions on the rules and criteria of the funding

978-0-7695-4525-7/12 $26.00 © 2012 IEEE
DOI 10.1109/HICSS.2012.103

Jian Ma
Department of Information
Systems, City University of
Hong Kong, Hong Kong, China
isjian@cityu.edu.hk

1414

collaborative research project selection is proposed
according to competitiveness and collaboration of
candidates [15]. A method of optimal allocation of
proposals to reviewers is presented in order to facilitate
the selection process [1]. Finally, a group decision
support method is suggested to evaluate experts for
research project selection [16].
Among the literatures, previous studies mainly
focus on general project selection process but not
specific process such as expert evaluation. Furthermore,
specific certain phases to evaluate reviewers mainly
focus on the reviewers’ competitiveness in certain
research areas [16], while ignoring the performance of
reviewers whether is suitable to review a proposal. So,
this paper contributes an integrated decision support
model to assess reviewers in research project selection
to cover this research gap.

2. Research background
In China, in order to improve scientific or social
development and cultivate talents, government sets up
a lot of funding agencies, such as National Natural
Science Foundation of China (NSFC, http:
//www.nsfc.gov.cn). NSFC, as one of the largest
funding agencies, was set up in 1986 with the aim to
promote the reform of natural science and technology
and push ahead economic and social development in
China. As the main way to support the national
strategic development of basic and applied basic
research, NSFC has received over 115 thousand
proposes and funded over 13 thousand projects in 2010.
According to its functions, NSFC is structured by
seven scientific departments, five bureaus and two
offices, as well as four units directly under its
jurisdiction. The seven departments are responsible for
the evaluation and management of projects in various
categories, while bureaus, offices and associated units
are in charge of policy making, administration and
other related affairs.
NSFC receives thousands of research proposals
from universities and institutions all over the country
each year, and then prepares peer reviews and panel
evaluation, and selects the proposals of higher potential
to grant. There is a strict evaluation system to decide
whether fund a proposal or not. First, applicants submit
applications to their own institutions for review after
the committee release the guides to projects and the
notice for accepting applications. Institutions make
united reports to NSFC via the Internet-based Science
Information System (ISIS, http://isis.nsfc.gov.cn/).
Second, seven scientific departments of NSFC would
separately carry out a preliminary examination for
proposals. After the examination, the fund

1415

management agency would choose several experts as
reviewers for peer review. Then, the scientific
departments analyze the comments and opinions of the
peer review and select the valuable projects. Finally,
funding management agency would set up a committee
composed of responsible reviewers with high academic
achievements and good professional ethics to discuss
the approved proposals from the peer review. The
qualified projects to be funded by NSFC would be
published in this stage.
As can be seen from the process, peer review is
critical stages, and selecting appropriate reviewers to
review their familiar proposals is critically important.
However, for a certain proposal, how to measure the
performance of reviewers and select some proper
experts to review is not well studied. Therefore, this
paper proposes a group decision support model to
evaluate experts’ performance in reviewer assignment
process.

3. The proposed integrated decision
support model
In this section, an integrated decision support
model is proposed to assess reviewers’ performance.
The overall performance of reviewers can be measured
by two basic indicators, namely competitiveness and
relevance.
On one hand, reviewers’ competitiveness reveals
their professional level. It’s undoubted that a reviewer
with high professional level and rich experience in
project selection will make more fair and scientific
judgments on proposals. There are two criteria to
measure a reviewer’s competitiveness, named
objective
competitiveness
and
subjective
competitiveness. Objective competitiveness can be
measured by three sub-criteria including publications,
research projects and historical performance in project
selection. Objective information can be collected from
expert’s database. Subjective information like other
experts’ opinions is used to supplement the
insufficiency of objective information. If the reviewer
is more competitive, he/she may be more reliable in
research project selection.
On the other hand, to assign a reviewer to a
proposal, the relevance between the research areas of a
reviewer and the content of a proposal should be also
considered. There are two criteria to measure a
reviewer’s relevance, named objective relevance and
subjective relevance. Objective relevance is
determined by two sub-criteria, namely the similarity
of publications between a reviewer and an applicant
submitting a proposal, and the similarity of a proposal
and projects the reviewer funded. Subjective relevance

consists of three sub-criteria including the similarity of
discipline codes between research areas of a reviewer
and a proposal, the similarity of abstract between a
proposal and projects a reviewer funded, and the
similarity of keywords in a reviewer’s information and
a proposal. The overall performance of relevance can
be achieved from the similarity of subjective and
objective relevance. The measurement of text
similarity can be done using text mining technique and
cosine coefficient of text vectors [17-18]. If the
research areas of reviewers are higher relevance with
the content of proposals, it will be preferred to make
more accurate in research project selection.
Hence, a hierarchy structure with four levels for
assessing reviewer can be shown in Figure 1.

There are three sub-criteria to measure a reviewer’s
objective competitiveness including publications,
research projects and historical performance in project
selection. The details to calculate the reviewer’s
objective competitiveness are as follows.
1) Objective competitiveness in publications
Generally, reviewers publish their research results
in different academic journals. Their research
publications reflect their academic contribution and
activity in a certain degree. Three attributes are taken
into consideration. The first one is the quantity of
publications. It is an important indicator to evaluate a
reviewer’s performance in publications. The number of
publications in one disciplinary field indicates a
reviewer’s contribution to this field. The second
attribute is the quality of the expert’s publications.
Academic journals are not only classified into different
disciplines but also different grades, such as grade A, B
and C. The grade of the journal reflects the quality of
the article a reviewer publishes. The third one is the
time distribution of publications. Since the nearer the
date of article an expert published, the more active
researcher he/she is. The active reviewer with more
publications in recent years will be preferred to select
in project evaluation.
There are three attributes listed above. The quantity
of publications is numerical value. Time distribution,
which is presented in the format of date, can be
represented by the time interval between publication
date and current date. The smaller the value of time
interval, the nearer the date of article a reviewer
published. The quality of an article can be measured by
the grade of the journal. Journals are generally
classified into grade A, B and C in China. A represents
the top-level journal. A, B or C are not computable
values, so it’s necessary to determine the weight of
each grade. AHP is used to deal with this task for
several reasons. First, it is user friendly since users can
directly input judgment data without the in depth
knowledge of mathematics [19]. Second, relevant
inconsistency in managerial judgments is dealt with
appropriately [20]. Last, the power of AHP has been
validated by empirical application in diverse areas [21].
Group decision making is used in order to minimize
the dominance by a strong member. Pair-wise
comparisons are made by the group with the grades
raging from 1-9. The ranking scales and explanation
are shown in Table 1. The detailed algorithms of AHP
be found in [16, 19-20].

Reviewersÿperformance

Reviewersÿcompetitiveness

Objective
competitiveness

ĂĂ

ĂĂ

Reviewersÿrelevance

Subjective
competitiveness

ĂĂ

ĂĂ

Objective relevance

ĂĂ

ĂĂ

Subjective relevance

ĂĂ

ĂĂ

Figure 1 The hierarchy structure for reviewer evaluation

Research data used in our model can be collected
from Scholarmate (http://www.scholarmate.com). With
the collected information, reviewer’s performances of
competitiveness and relevance can be measured by
analytical hierarchy progress (AHP) and fuzzy
linguistic representation. Each reviewer’s overall
performance integrated competitiveness and relevance
can be achieved by weighted geometric average
method. There are different formats of information
collected. Some information like numerical data can be
used to calculating directly. Other information like
natural language should be converted to the calculable
format by appropriate algorithms. Performances of
competitiveness and relevance can be then aggregated
by using the processed data. The overall performance
can be achieved from equilibrium between
performances of competitiveness and relevance. Fund
management agency would select appropriate
reviewers according to their overall performances. The
proposed group decision support method to evaluate
potential reviewers is summarized as follows.

3.1. Measuring a reviewer’s competitiveness
A reviewer’s competitiveness can be divided into
objective
competitiveness
and
subjective
competitiveness. The calculation process of the
reviewer’s competitiveness can be decomposed in the
following subsections.

Table1 The scales for comparison judgments [19]
Absolute values Definitions
1
Equal importance
3
Moderate importance of one over another
5
Strong or essential importance of one over

3.1.1. Objective competitiveness.

1416

7
9
2, 4, 6, 8
Reciprocals

another
Very strong or demonstrated importance
of one over another
Extreme importance of one over another
Intermediate values
Reciprocals for inverse comparison

Let P111 represent the reviewer performance in
publications. w1i (i=1, 2, 3) denotes the weight of
journal grade (A, B or C). t1j (j=1, 2, …, n )denotes the
interval time j between publication time and current
time. We use b1ij to represent the quantity of academic
publications a reviewer publishes in journals with
grade w1i at time t1j. Then each reviewer’s performance
in publications can be represented by
n

p111

3

1

¦¦ ( t
j 1 i 1

1
j

u wi1 u bij1 )

(1)

2) Objective competitiveness in research projects
Various research projects that a reviewer undertakes
reveal his/her academic ability. Like reviewer’s
publications, there are also three attributes to measure a
reviewer’s performance in research projects he/she had
undertaken. One hand, the quantity of research projects
reflects a reviewer’s experience in R&D projects. The
other hand, research projects can also be graded into
different levels like journals, so quality of research
projects should be taken into consideration. According
to the different funding agencies of central government,
ministries, provinces and local cities, R&D projects
can be classified into the national (N) level, ministry
(M) level, provincial (P) level and local city (C) level.
Projects with different levels make different
contribution and values. Generally, a project with
national level funded by central government would
make more contribution to the whole country in
comparison with projects with other levels. Besides the
quantity and quality of research projects, the time
distribution of projects a reviewer undertakes is also
important. A reviewer undertakes projects in recent
years indicate that he/she is still active in research
areas, which is helpful in evaluating projects.
Processing of information is similar to that of
reviewer performance in publications. Let P112
represent the reviewer performance in research projects.
w2i (i=1, 2, 3, 4) denotes the weight of project level (N,
M, P or C). t2j (j=1, 2, …, n )denotes the interval time j
between project completion time and current time. We
use b2ij to represent the quantity of research projects a
reviewer undertakes with level w2i at time t2j. Then
each reviewer’s performance in research projects can
be represented by
n

p112

4

j 1 i 1

3)

Objective

1

¦¦ ( t

2
j

u wi2 u bij2 )

competitiveness

in

performance in project selection
A reviewer’s historical performance in project
selection can reflect his/her evaluation ability in a
certain degree. A reviewer may give different
evaluation results to various proposed projects at
different time, which can be obtained from proposal
review database. A reviewer’s historical performance
in projects evaluation can be measured by three
attributes: reviewer’s evaluation grade, quality and
time distribution. In projects evaluation, a reviewer
would choose a final result like A, B, C or D to each
project as his/her judgment, which means that the
project is “excellent”, “good”, “moderate” or “poor”.
Funding management agency decides whether to fund
a project or not according to reviewers’ evaluation. A
reviewer’s evaluation grade can be acquired by his/her
judgment and final result of project evaluation. If the
project is approved finally, then a reviewer who gives
grade A will get score 4. It means he/she has high
evaluation ability. Reviewers who give grade B, C and
D will respectively get score 3, 2, 1. If the proposed
project fails, reviewers’ score is zero. Projects’ quality
represented by levels of N, M, P and C has listed above.
The time interval between the evaluation date and
today negatively reflects the reviewer’s performance.
Let P113 represent the reviewer’s historical
experience in project selection. W3i (i=1, 2, 3, 4)
denotes the weight of project level (N, M, P or C). t3j
(j=1, 2, …, n)denotes the interval time j between
project evaluation time and current time. M denotes the
number of projects that a reviewer evaluated in a given
period. We use b3ijk to represent the score of evaluation
on project k (k=1, 2, …,m )with level w3i at time t3j.
Then each reviewer’s historical performance in m
projects can be obtained. A reviewer’s historical
performance in project evaluation can be represented
by average score, the formula is

p113

1 m 1
( 3 u wi3 u bijk3 )
¦
m k 1 tj

(3)

4) Objective competitiveness integration
The performance of objective competitiveness P11
can be concluded from above three sub-criteria. The
normalization of criterion values is used as for
commensurability between various criteria. According
to the existing method [22], the normalization of each
sub-criterion is obtained by following formulas:
For benefit criteria,

p11' i

p11i
p11i (max)

, p11i (max) z 0; i 1, 2,

For cost criteria,

(2)

p11' i

historical

1417

p11i (min)
p11i

, p11i z 0; i 1, 2,

,l

,l

AHP is used to determine the relative importance of
these three sub-criteria. Let v11i denotes the weight of
sub-criterion i (i=1, 2, 3), P11 can be computed by the
formula
3

p11

¦ (v

11i

u p11' i )

3.1.3. Calculating a reviewer’s overall
competitiveness
Since the performance of objective competitiveness
and subjective competitiveness are measured, a
reviewer’s overall competitiveness can be obtained
from the integration of these two criteria. The
normalization of criterion is stated above. AHP is used
to determine weight of each criteria v1i (i=1, 2). The
overall evaluation of reviewer’s competitiveness p(com)
can be represented by

(4)

i 1

3.1.2. Subjective competitiveness.
Objective information listed above is still not
enough to measure a reviewer’s overall performance.
Other indicators like citation times and other expert’s
evaluation, which are important to access a reviewer’s
academic level, have not been used when measure a
reviewer’s performance in publications. Besides, a
reviewer’s performance in research projects he/she
participated has not been considered. However, it’s not
easy to collect information for these indicators in
China. So, subjective information like other peer
experts’ opinions is a necessary supplement. Other
experts’ opinions can be represented by their judgment
to a reviewer like low, middle, high or other linguistic
values.
In order to quantify other experts’ opinions, we will
use 2-tuple fuzzy linguistic representation model to
convert other experts’ linguistic comments to
numerical values [23]. Suppose S is an ordered set with
seven scales to grade a reviewer, namely
S={s1:VL(very low), s2:L(low), s3:ML(more or less
low), s4:M(middle), s5:MH(more or less high),
s6:H(high), s7:VH(very high)}. The ranks and grades of
importance are listed in Table 2. The detailed
algorithms can be found in [16, 23].

2

p ( com )

u p1' i )

(6)

3.2. Measuring a reviewer’ relevance
A reviewer’s relevance can be divided into
objective relevance and subjective relevance. The
calculation process of the reviewer’s relevance can be
decomposed in the following subsections.
3.2.1. Objective Relevance
There are two sub-criteria to measure a reviewer’s
objective relevance including the similarity of
publications between a reviewer and an applicant
submitting a proposal, and the similarity of a proposal
and projects the reviewer funded. The details to
calculate the reviewer’s objective relevance are as
follows.
1) The similarity of publications between a
reviewer and an applicant
Publications that a reviewer published reveal
his/her research areas. Meanwhile, publications in
applicant’s proposal also show his/her research areas.
The similarity of publications between a reviewer and
an applicant can represent the consistency of their
research areas.
Let P211 represent the similarity of publications
between a reviewer and an applicant. Suppose n
publications a reviewer published, and l publications
an applicant published. We use c1ij to represent the
similarity of ith publication by a reviewer and jth
publication by an applicant. Then the similarity can be
calculated by

Suppose m experts have participated in the
evaluation of a reviewer. 2-tuple (si, аi) (i=1, 2, …,
m)is used to represent the linguistic evaluation of
expert i. Then a group of 2-tuples (si, аi) can be
transformed into numerical values by function Δ-1. The
average score of a reviewer grading by other experts
can be computed by

1 m 1
¦ ' (si , ai );
mi1
si  S , ai  [ 0.5, 0.5)

1i

i 1

Table 2 The seven scales and grade of importance [23]
Seven scales of reviewer grading
S1=VL: very low
S2=L: low
S3=ML: more or less low
S4=M: middle
S5=MH: more or less high
S6=H: high
S7=VH: very high

p12

¦ (v

p211

max{cij1 }, i 1,..., n, j 1,..., l

(7)

2) The similarity of a proposal and projects the
reviewer funded
Like the similarity of the publications, the
similarity of a proposal and projects the reviewer
funded can also show the consistency of their research
areas.
Processing of information is similar to that of

(5)

1418

projects the reviewer funded. Suppose m projects a
reviewer funded. We use c2i to represent the similarity
of ith project a reviewer funded and a proposal. Then
the similarity can be calculated by

reviewer relevance in publications. Let P212 represent
the similarity of a proposal and projects the reviewer
funded. Suppose m projects a reviewer funded. We use
c2i to represent the similarity of ith project a reviewer
funded and a proposal. Then the similarity can be
calculated by

p212

max{ci2 }, i 1,..., m

2

¦ (v

21i

u p21i )

(11)

3) The similarity of keywords in a reviewer’s
information and a proposal
Let P223 represent the similarity of keywords in a
reviewer’s information and a proposal. Suppose the
keyword set in reviewer’s information S1, and the
keyword set in proposal S2. Then the similarity can be
calculated by

(8)

AHP is used to determine the relative importance of
these two sub-criteria. Let v21i denotes the weight of
sub-criterion i (i=1, 2, 3), P21 can be computed by the
formula

p21

max{ci3}, i 1,..., m

p222

(9)

p 223

i 1

3.2.2. Subjective Relevance

where

# S1

S2

# S1

S2

(12)

#  represent the number of the keywords in

each keyword set.
AHP is used to determine the relative importance of
these three sub-criteria. Let v22i denotes the weight of
sub-criterion i (i=1, 2, 3), P22 can be computed by the
formula

There are three sub-criteria to measure a reviewer’s
subjective relevance including the similarity of
discipline codes between research areas of a reviewer
and a proposal, the similarity of abstract between a
proposal and projects a reviewer funded, and the
similarity of keywords in a reviewer’s information and
a proposal. The details to calculate the reviewer’s
subjective relevance are as follows.
1) The similarity of discipline codes between
research areas of a reviewer and a proposal
When an expert is as a reviewer, expert can fills in
two or more familiar discipline codes. Also, applicant
fills in two discipline codes in submitted proposal. The
similarity of discipline codes between a reviewer and a
proposal can represent the consistency of their research
areas.
Let P221 represent the similarity of discipline codes
between research areas of a reviewer and a proposal.
Suppose dr1 the first discipline code and dr2 the second
discipline code filled in by reviewers, and dp1 the first
discipline code and dp2 the second discipline code in
proposal. Then the similarity can be calculated by
­ 1 dr1 dp1 , dr2 dp2
°0.8 dr dp , dr dp
1
2
2
1
°
°0.7 dr1 dp1 , dr2 z dp2
(10)
°
p 221 ®0.6 dr1 dp2 , dr2 z dp2
°0.6 dr dp , dr z dp
2
1
1
2
°
°0.5 dr2 dp2 , dr1 z dp1
°
otherwise
¯ 0
2) The similarity of abstract between a proposal
and projects a reviewer funded
The similarity of abstract between a proposal and
projects the reviewer funded can also show the
consistency of their research areas.
Let P222 represent the similarity of a proposal and

3

p22

¦ (v

22 i

u p22i )

(13)

i 1

3.2.3. Calculating a reviewer’s overall relevance
Since the performance of objective relevance and
subjective relevance are measured, a reviewer’s overall
relevance can be obtained from the integration of these
two criteria. AHP is used to determine weight of each
criteria v2i (i=1, 2). The overall evaluation of
reviewer’s relevance p(rel) can be represented by
2

p ( rel )

¦ (v

2i

u p2i )

(14)

i 1

3.3. A Reviewer’s Overall Performance
The performances of competitiveness and relevance
of a reviewer can be measured respectively by above
steps and formulas. Thus we can obtain the overall
performance of a reviewer by weighted geometric
mean method. The formula is as follows:

p

p ( com) p( rel )

(15)

4. An illuminating example
As stated above, applicants submit proposals to
NSFC via ISIS. NSFC focuses on the project selection,
but informal reviewer selection throw doubt on the
credibility of projects selection. Therefore, we

1419

proposed an integrated model to solve this problem in
this paper. The validation and potential application of
the proposed model will be illustrated.
Research data can be acquired from Scholarmate
(http://www.scholarmate.com). Other information like
experts’ opinions has been gathered. Since there are
different emphases on subjective and objective
information in the measurement of competitiveness
and relevance, we will select three decision makers
(DMs) respectively to determine the weights of these
two criteria in each dimension. Besides, different
disciplines have different focuses, it is necessary to
organize panels with different disciplines to determine
the different weights of sub-criteria. Priorities of
journal grade and project level are also determined by
AHP.

Pair-wise
comparison
matrices

Table 5 Pair-wise comparison matrices on publications,
projects and historical performance
Decision
DM1
DM2
DM3
makers
Pair-wise
§1 1 3·
§1 2 1·
comparison §¨ 1 2 1 ·¸
¨
¸
¨1
1¸
1
1
1
1
4
1
matrices
2
3
¨
¸
¨2 1 4¸
¨
¸
¨1
©

©4

3 1 ¸¹

¨1
©3

1
4

1 ¸¹

¨1
©

4

1 ¸¹

Table 6 Pair-wise comparison matrices on publication
similarity and project similarity
Decision
DM1
DM2
DM3
makers
Pair-wise
comparison
§1 2·
§1 1·
§1 1·
matrices
¨
¸
¨
¸
¨
¸

Table 3 Pair-wise comparison matrices on objective and
subjective competitiveness
Decision
DM1
DM2
DM3
makers
Pair-wise
1·
1·
§
§
comparison
§1 1·
¨1 3 ¸
¨1 4 ¸
matrices
¨
¸
¨
¸
¨
¸

©1 1¹

§ 1 3·
¨
¸
¨¨ 1 1 ¸¸
©3
¹

According to the hierarchy structure, there are
several sub-criteria for each criterion. Objective
competitiveness can be measured by three sub-criteria:
publications, research projects and historical
performance in project selection. Other experts’
opinions represent subjective competitiveness.
Objective relevance is determined by publication
similarity and project similarity. Subjective relevance
consists of three sub-criteria: discipline area, abstract
and keywords. The same DMs give three sets of pairwise comparison matrices for various sub-criteria for
each criterion as shown in Table 5, Table 6 and Table 7.
The weights of publications, research projects and
historical performance can be derived as v111=0.4284,
v112=0.2466 and v113=0.3250, CR is equal to 0.0077
less than 0.1.The importance of publication similarity
and project similarity can be derived as v211=0.5575
and v212=0.4425, CR=0. The importance of discipline
area, abstract and keywords can be derived as
v221=0.7494, v222=0.1474 and v223=0.1031, CR=0.01.

A reviewer’s overall performance can be obtained
from the equilibrium between competitiveness and
relevance. Each indicator can be measured respectively
by two criteria: objective criterion and subjective
criterion. Different disciplines have different focuses
on these two criteria of each indicator in NSFC. So the
first step is to determine the relative importance of
these two criteria in different disciplines. As stated
above, v1i (i=1, 2) denote the weights of objective
competitiveness and subjective competitiveness, v2i
(i=1, 2) denote the weights of objective relevance and
subjective relevance. A panel with three DMs in a
certain discipline has been organized to give pair-wise
comparison matrices for the criteria. Two sets of
matrices for two indicators are shown in Table 3 and
Table 4. The final results are obtained from the three
DMs’ judgments by weighted geometric mean method.
According to the algorithm of AHP, the weights of
criterion on objective competitiveness and subjective
competitiveness can be obtained as v11=0.7654 and
v12=0.2346 respectively, the consistency ratio (CR) is
equal to 0. Similarly, v21=0.3020, v22=0.6980, CR =0.

1 ¸¹

§ 1 3·
¨
¸
¨¨ 1 1 ¸¸
©3
¹

4.2. Determining the weights of sub-criteria for
each criterion

4.1. Determining the weights of objective
criterion and subjective criterion

¨3
©

§ 1 5·
¨
¸
¨¨ 1 1 ¸¸
©5
¹

©1 1¹

©1 1¹

1

1¹

©2

Table 7 Pair-wise comparison matrices on discipline area,
abstract and keywords
Decision
DM1
DM2
DM3
makers
Pair-wise
§1 6 5·
§1 5 7·
§1 6 8·
comparison ¨ 1
¸
¨1
¸
¨1
¸
1
2
1
1
5
6
¨
¸
¨ 6 1 2¸
¨
¸
matrices
¨1 1
¸
¨1 1
¸

1¹

Table 4 Pair-wise comparison matrices on objective and
subjective relevance
Decision
DM1
DM2
DM3
makers

©7

1420

2

1¹

¨1
©5

1

1 ¸¹

©8

2

1¹

4.3. Determining the weights of journal grades
and project levels
Similarly, DMs give pair-wise comparison matrices
for the different grades of journals and different levels
of projects as shown in Table 8 and Table 9. w1i (i=1, 2,
3) denotes the weight of journal grade (A, B or C)
while w2i (i=1, 2, 3, 4) denotes the weight of project
level (N, M, P or C). The relative importance of journal
grade A, B and C can be derived as w11=0.6946,
w12=0.2035 and w13=0.1019. CR is equal to 0.02. The
relative importance of nation, ministry, province and
city level of projects can be derived as w21=0.6108,
w22=0.2422, w23=0.0913 and w24=0.0556. CR is equal
to 0.04.
Table 8 Pair-wise comparison matrices on journal grade A, B
and C
Decision
DM1
DM2
DM3
makers
Pair-wise
§1 5 7· §1 3 5·
§1 4 6·
comparison ¨ 1
¨1
¸ ¨1
¸
¸
1
2
¨4
¸
¨ 5 1 3¸ ¨ 3 1 2¸
matrices
¨ 1 1 1¸
¨ 1 1 1¸ ¨ 1 1 1¸
¹
©6 2
¹
©7 3
¹ ©5 2
Table 9 Pair-wise comparison matrices on project level N, M,
P and C
Decision
DM1
DM2
DM3
makers
Pair-wise
3
5 8·
5
7 9·
4
6 7·
§1
§1
§1
¨
¸
¨
¸
¸
comparison ¨¨ 1
¨1
¨1
1
4 5¸
1
4 6¸
1
3 4¸
¨3
¸
¨5
¸
¨4
¸
matrices
¨
¸
¨
¸
¨1
¸
1
1
1
1
1
¨
¨6
¨1
¨
©7

3
1
4

1

1
2

2¸
¸
¸
1¸
¹

¨
¨7
¨1
¨
©9

4
1
6

1

1
2

2¸
¸
¸
1¸
¹

¨
¨5
¨1
¨
©8

4
1
5

1

1
2

Table 10 The reviewer’s publication information
Time distribution
2007
2008 2009 2010 2011
1/5
1/4
1/3
1/2
1
Grade A
1
0
0
1
1
Grade B
0
1
2
0
1
Grade C
0
1
1
1
0
Table 11 The information of projects reviewer had
undertaken
Time distribution
2007 2008 2009 2010
2011
1/5
1/4
1/3
1/2
1
Nation
0
1
0
0
1
Ministry
0
0
1
1
0
Province
1
0
0
1
1
City
1
0
1
1
0
Table 12 The reviewer’s historical performance in project
evaluation
Time distribution
2007
2008
2009
2010
2011
1/5
1/4
1/3
1/2
1
Nation
-(A,Y)
--(B,Y)
Ministry (B,Y)
-(A,Y)
--Province (C,N) (B,Y)
-(B,Y) (D,N)
City
(C,Y)
--(D,N)
-Table 13. Other experts’ opinions
Experts
E1
E2 E3
E4
Their opinions
MH H
VH M

2¸
¸
¸
1¸
¹

E5
H

Table 14 The publication similarity matrix
Publications of applicant
A1
A2
A3
A4
A5

Publications of reviewer

4.4. Collecting information of the reviewer and
proposal with its applicant
The information of the candidate reviewer in recent
five years and the proposal with its applicant will be
used to evaluate the reviewer’s overall performance.
We select an expert as a candidate reviewer from the
database in NSFC. The detailed information can be
collected as follows. The reviewer’s publication
information including time distribution, quality and
quantity is listed in Table 10. Information of research
proposal the reviewer undertook is presented in Table
11. Table 12 contains the historical performance of the
reviewer in project evaluation. There are five other
experts giving their opinions to this reviewer as shown
in Table 13. The publication similarities as well as
project similarities between the reviewer and the
applicant are listed in Table 14 and Table 15. Table 16
contains their discipline codes and similarities.
Information of abstract similarity has been listed in
Table 17. The similarity of keywords between the
reviewer and the applicant are shown in Table 18.

0.0097

0.4911

0.3139

0.3559

0.0139

0.4803

R2
R3
R4
R5
R6
R7
R8
R9
R10

0.5632
0.2902
0.6388
0.5777
0.3773
0.4521
0.1563

0.5880
0.1555
0.1100
0.0367
0.3736
0.0919

0.4618
0.0620
0.4583
0.1644
0.4308
0.1738
0.6114
0.5498
0.4940

0.2693
0.4545
0.4592
0.6033
0.4553
0.1119
0.2707
0.6008

0.6112
0.6183
0.4494
0.4231
0.1908
0.5015
0.3740
0.5782
0.3970

0.3542
0.6267
0.5058
0.6149
0.1105
0.4205
0.5449
0.5000
0.0061

0.6151
0.3047

0.4393
0.4765
0.1116

0.1080

projects reviewer had
undertaken

Table 15 The project similarity matrix
Proposal
0.0432
R1
0.0886
R2
0.3197
R3
0.4891
R4
0.2262
R5
0.4134
R6
0.4284
R7
0.1819
R8
0.3692
R9
0.1852
R10

1421

A6

R1

Table 16 The relevance in discipline area
Discipline codes of the reviewer
R1
R2
Applicant

A1

1

0

A2

0

1

Reviewer’s historical
proposals

proposed method improved the quality of reviewer
evaluation. Also, the proposed method can be used to
expedite the proposal assignment process in the NSFC
and elsewhere. (It is an extension of the Internet-based
Science
Information
System
ISIS,
https://isis.nsfc.gov.cn). The proposed method can also
be used in other government research funding agencies.
Future work can be done to empirically validate the
results of reviewer evaluation. Also, there is a need to
assign research proposals to reviewers systematically.
Finally, the method can be expanded to help finding a
better match between proposals and reviewers.

Table 17 The abstract similarity matrix
Proposal
0.1342
P1
0.5623
P2
0.3127
P3
0.4038
P4
0.1330
P5
0.2839
P6
0.3721
P7
0.5112
P8
0.5201
P9
0.1287
P10

Acknowledgement
The authors would like to thank the minitrack chair
and the anonymous reviewers for their valuable
comments and suggestions which have helped
immensely in improving the quality of the paper. This
work was supported in part by National Natural
Science Foundation of China under Grant (No.
71001103), and the Fundamental Research Funds for
the Central Universities, and the Research Funds of
Renmin University of China.

Table 18 The reviewer’s research areas and proposal’s
keywords
keywords
electronic commerce, information
reviewer technology, economic management,
enterprises information
information technology, computer
proposal science, enterprises information, data
mining

References
[1] W.D. Cook, B. Golany, M. Kress, M. Penn, and T. Raviv,
“Optimal allocation of proposals to reviewers to facilitate
effective ranking,” Management Science, Vol. 51, No. 4, pp.
655-661, Apr. 2005.
[2] Q. Tian, J. Ma, and O. Liu, “A hybrid knowledge and
model system for R&D project selection,” Expert Systems
with Applications, Vol. 23, No. 3, pp. 265-271, Oct. 2002.
[3] Q. Tian, J. Ma, J. Liang, R. Kowk, O. Liu, and Q. Zhang,
“An organizational decision support system for effective
R&D project selection,” Decision Support Systems, Vol. 39,
No. 3, pp. 403-413, May 2005.
[4] F. Ghasemzadeh and N. P. Archer, “Project portfolio
selection through decision support,” Decision Support
Systems, Vol. 29, No. 1, pp. 73-88, Jul. 2000.
[5] P.V. Chu, Y.Hsu, and M. Fehling, “A decision support
system for project portfolio selection,” Computers in Industry,
Vol. 32, pp. 141-149, 1996.
[6] J. Klapka, and P. Pinos, “Decision support system for
multicriterial R&D and information systems projects
selection,” European Journal of Operational Research, Vol.
140, pp. 434-446, 2002.
[7] C. Huang, P. Chub, and Y. Chiang, “A fuzzy AHP
application in government-sponsored R&D project
selection,” Omega, Vol. 36, pp. 1038-1052, 2008.
[8] P. Chang, and J. Lee, “A fuzzy DEA and knapsack
formulation integrated model for project selection,”
Appeared in Computers & Operations Research, 2011.
[9] T. Chiang, and Z.H. Che, “A fuzzy robust evaluation
model for selecting and ranking NPD projects using Bayesian

4.5. Evaluating reviewers
According to formulas in section 3, the relevant
performance of each sub-criterion is P111=1.6813,
P112=1.1780, P113=0.3659, P12=4.6, P211=0.6388,
P212=0.4891, P221=1, P222=0.5623, P223=0.3333. Then
the reviewer’s performance in competitiveness can be
obtained as P(com)=0.3464 (0<P(com)<1) through
normalization and AHP method. The numerical value
of relevance can be computed as P(rel)=0.7778
(0<P(rel)<1). Therefore, his/her overall performance is
derived as P=0.5191. Then other candidate reviewers’
overall performance can be calculated similarly.
According to the ranking order of reviewers, we can
select appropriate reviewers attending the project
evaluation.

5. Conclusions
This paper presents an integrated decision support
model for evaluating reviewer’s performance. In the
proposed model, competitiveness and relevance of
reviewers are used to evaluate reviewers, and AHP,
scoring method and fuzzy linguistic processing are
employed to measure reviewers’ competitiveness and
relevance. The experimental results showed that the

1422

belief network and weight-restricted DEA,” Expert Systems
with Applications, Vol. 37, pp. 7408-7418, 2010.
[10] M.P. Amiri, “Project selection for oil-fields
development by using the AHP and fuzzy TOPSIS methods,”
Expert Systems with Applications, Vol. 37, pp. 6218-6224,
2010.
[11] H. Eilat, B. Golany, and A. Shtub, “R&D project
evaluation: an integrated DEA and balanced scorecard
approach,” Omega, Vol. 36, pp. 895- 912, 2008.
[12] O. Liu, and J. Ma, “A multilingual ontology framework
for R&D project management systems”, Expert Systems with
Applications, Vol. 37, pp. 4626-4631, 2010.
[13] Y. Sun, J. Ma, Z. Fan, J. Wang, “A hybrid knowledge
and model approach for reviewer assignment,” Proceedings
of the 40th Hawaii International Conference on System
Sciences, pp.1-10, 2007.
[14] Y. Xu, J. Ma, Y. Sun, G. Hao, W. Xu, and D. Zhao, “A
decision support approach for assigning reviewers to
proposals,” Expert Systems with Applications, Vol. 37, pp.
6948-6956, 2010.
[15] B. Feng, J. Ma, and Z. Fan, “An integrated method for
collaborative R&D project selection: supporting innovative
research teams,” Expert Systems with Applications, Vol. 38,
pp. 5532–5543, 2011.
[16] Y. Sun, J. Ma, Z. Fan, and J. Wang, “A group decision
support approach to evaluate experts for R&D project

selection,” IEEE Transactions on Engineering Management,
Vol. 55, No. 1, pp. 158-170, Feb. 2008.
[17] Feldman, R. and J. Sanger, The Text Mining Handbook:
Advanced Approaches in Analyzing Unstructured Data.
Cambridge, New York, 2007.
[18] S. Hettich and M. Pazzani, “Mining for proposal
reviewers: lessons learned at the national science
foundation,” in Proc. 12th Int. Conf. Knowl. Discov. and
Data Mining, pp. 862–871, 2006.
[19] T.L. Saaty and L.G. Vargas, Prediction, Projection and
Forecasting. Kluwer Academic Publishers, Dordrecht, 1991.
[20] T. L. Saaty, “A scaling method for priorities in
hierarchical structures,” Journal of Mathematical Psychology,
Vol. 15, No. 3, pp. 234-281, June 1977.
[21] J. Aczel, and T.L. Saaty, “Procedures for synthesizing
ratio judgements,” Journal of Mathematical Psychology, Vol.
27, No. 1, pp. 93-102, March 1983.
[22] C.L. Hwang, and K. Yoon, Multiple Attribute Decision
Making: Methods and Applications. Springer, Berlin, 1981.
[23] R. C. Wang and S. J. Chuu, “Group decision-making
using a fuzzy linguistic approach for evaluating the flexibility
in a manufacturing system,” European Journal of Operational
Research, Vol. 154, No. 3, pp. 563-572, 2004.

1423

