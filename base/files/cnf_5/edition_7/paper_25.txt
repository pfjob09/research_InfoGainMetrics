3D Object Recognition Using 2D Moments and HMLP Network
1

M. Y. Mashor, 2M. K. Osman and 3M. R. Arshad
Control and ELectronic Intelligent Systems (CELIS) Research Group,
School of Electrical & Electronic Engineering,
Universiti Sains Malaysia, Engineering Campus,
14300 Nibong Tebal, Pulau Pinang, MALAYSIA.
Tel: +604-5937788, Fax: +604-5941023
E-mail: 1yusof@eng.usm.my, 2khusairi@eng.usm.my, 3rizal@eng.usm.my

Abstract
This paper proposes a method for recognition and
classification of 3D objects using 2D moments and HMLP
network. The 2D moments are calculated based on 2D
intensity images taken from multiple cameras that have
been arranged using multiple views technique. 2D
moments are commonly used for 2D pattern recognition.
However, the current study proves that with some
adaptation to multiple views technique, 2D moments are
sufficient to model 3D objects. In addition, the simplicity
of 2D momentâ€™s calculation reduces the processing time
for feature extraction, thus decreases the recognition time.
The 2D moments were then fed into a neural network for
classification of the 3D objects. In the current study,
hybrid multi-layered perceptron (HMLP) network is
proposed to perform the classification. Two distinct
groups of objects that are polyhedral and free-form
objects were used to access the performance of the
proposed method. The recognition results show that the
proposed method has successfully classified the 3D object
with the accuracy of up to 100%.

1. Introduction
Nowadays, 3D objects recognition has dominated the
attention of many computer vision researchers. It is the
necessary step for the development of an effective vision
system that is capable to operate in a variety of
applications such as the automation of manufacturing
process [1]. Model based vision system is the most widely
used approach for shape or object recognition. In this
approach, features extracted from the objects to be
recognized would be matched against the previously
stored features of object models [2]. Earlier researches in
3D object recognition attempt to recover full 3D shape
information before performing the recognition task. This
method is known as object based representation. In

contrast to methods that rely on predefined geometry
model for recognition, view-based method has been
proposed by some researchers. This method attempts to
avoid 3D object models, but instead to use 2D model
views. In view-based technique, 3D object is described
using a set of 2D characteristic views or aspects. Paggio
and Edelman [3] showed that 3D objects can be
recognized from the raw intensity values in 2D images,
using a network or generalized radial basis functions.
They demonstrated that full 3D structure of an object can
be estimated if enough 2D views of the object are
provided. Murase and Nayar [4] developed a parametric
eigenspace method to recognize 3D objects directly from
their appearance. Eigenvectors are computed from set of
images in which the object appears in different poses.
Main disadvantage of view-based technique is the
inherent loss of information in the projection from 3D
object to 2D image [5]. Moreover, the 2D image of a 3D
object depends on factors such as the camera viewpoint
and the viewing geometry. A single 2D view-based
approach may not be appropriate for 3D object
recognition since only one side of an object can be seen
from any given viewpoint [6]. One solution to this
problem is to use several 2D views of the object. There
are several researches that are based on active object
recognition system [5][7], where the camera is moved
around the object to gather additional multiple 2D views
until enough features are gathered to sufficiently classify
the 3D objects. However, this approach requires a
complicated and expensive setup that is difficult to realize
[8]. A better alternative is to obtain the features from
several 2D views from a few static cameras as suggested
in [2][9].
In the current study, multiple views technique from
static cameras is proposed to obtain the features of 3D
objects. This study focuses on the recognition of the
isolated objects using shape information. Due to the
inherent loss of information in the 3D to 2D image
projection process, an effective representation of 3D

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIVâ€™04)
0-7695-2178-9/04 $20.00 Â© 2004 IEEE

object properties using 2D images should be considered.
2D moments are used in the current study as features for
3D object modeling. Although moments are commonly
applied to 2D object or pattern recognition, an adaptation
with multiple views technique enables this technique to be
used in 3D object modeling.
Recently, neural network becomes a popular choice
for 3D object recognition. Compare to conventional 3D
object recognition approaches, neural network normally
provides a better generalization, robustness and parallel
implementation paradigm properties [11]. Hybrid
multilayered perceptron (HMLP) network has been
selected to perform the recognition task in the current
study. The network has been proved by Mashor [16] to
significantly improve MLP network performance and has
better learning rate and generalization properties.

Thresholding provides a good separation between object
and background in several applications [14]. In feature
extraction stage, Huâ€™s moments [15] were used as the
features for 3D modeling.

2. Image Acquisition and Features Extraction
Three cameras are used in the current study to obtain
three 2D images of the 3D objects. The proposed cameraobject setup is shown in Figure 1. The three cameras are
placed at points A, B and C. A and B are located on the
same horizontal, but differ 900 from each other. Point C is
perpendicular to the turntable. Each object to be
recognized must be placed in its stable condition at the
centre of circular turntable, which can be rotated 360
degree. Illumination using controlled lighting condition is
provided to have an object without shadow and reflection.
Figure 1 shows the location of the points and object. Since
all points have the same distance from the centre of the
turntable, all cameras must have the same focal lengths.
For features stability, cameras at point A and B are
proposed to be fixed at 450 from perpendicular view rather
than at the x-y plane. This position is proposed to
minimize the change of shapeâ€™s description while the
object is rotated. Camera at point C is fixed at the top of
the object. Figure 2 shows how these three cameras are
fixed.
After an object of interest is placed at the centre of
the turntable, the 2D images of the object are acquired.
Then, the object will be rotated 50 and the three 2D
images will be acquired again. Each time the object will
be rotated at 50 until 3600 is completed. Hence, for each
object 72 2D image sets are obtained. These images are
divided into two groups, 36 image sets for training data
and 36 image sets for testing data. The acquired images at
00, 100, 200,â€¦, 3500 were used as the training set and the
rest of the images (image at 50, 150, 250,â€¦, 3550) were
used as the testing set. The training data set is used to
build the 3D object model in the recognition stage.
The 2D captured images are then digitized and sent
to the pre-processing and feature extraction stage. In the
pre-processing stage, images will be automatically
thresholded using iterative thresholding method [12][13].

Figure 1. Image acquisition set-up

Figure 2. Camera position for point A, B and C
In order to understand how to utilize moment
invariant method, let f (i, j ) be a digital image with i = 1,
2, 3â€¦M and j = 1, 2, 3â€¦N. Two-dimensional moments
and central moments of order (p+q) of f(i, j) are defined
as:
M

m pq

N

Â¦Â¦ i

p

j q f (i , j )

(1)

i 1 j 1
M

U pq

N

Â¦Â¦ (i  i )

p

( j  j ) q f (i , j )

(2)

i 1 j 1

where
m10
m01
and
(3)
j
m00
m00
From the second and third order moments, a set of seven
invariant moments which is invariants to translation,
rotation and scale derived by Hu are as follow:
i

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIVâ€™04)
0-7695-2178-9/04 $20.00 Â© 2004 IEEE

M1 -20  -02
M2
M3

M4

(4)
2

2
11

(-20  -02 )  4-

(5)

2

(-30  3-12 )  (3-21  -03 )
2

(-30  -312 )  (-21  -03 )

2

(6)

2

(7)
2

(-30  3-12 )(-30  -12 )[(-30  -12 )  3(-21  -03 ) 2 ]

M5

 (3- 21  - 03 )(- 21  - 03 )[3(- 30  -12 ) 2  (- 21  - 03 ) 2 ] (8)

M6

In this study, the number of input depends on the
number of cameras used (3 cameras) while the number of
outputs depends on the number of objects to be
recognized. In the recognition step, output node which
has the largest value is determined as 1. Otherwise, the
node is considered as 0. HMLP network has been trained
using MRPE algorithm and the description of MRPE
algorithm could be found in Mashor [16].

(-20  -02 )[(-30  -12 ) 2  (-21  -03 ) 2 ]
 4-11 (-30  -12 )(-21  -03 )

(9)
2

(3-21  -03 )(-30  -12 )[(-30  -12 )  3(-21  -03 ) 2 ]

M7

(-30  3-12 )(-21  -03 ) [3(-30  -12 ) 2  (-21  -03 ) 2 ]
(10)

where - pq are the normalized central moments defined by
- pq

U pq

(11)

U 00r

and
r

[( p  q ) / 2]  1 , p  q

2,3,4....

(12)

3. Recognition
The current study uses Hybrid Multilayerred
Perceptron (HMLP) network trained by Modified
Recursive Prediction Error (MRPE) algorithm for
recognition and classification. A hybrid multilayered
perceptron network with one hidden layer is shown in
Figure 3. HMLP network with one hidden layer can be
expressed by the following equation [16]:
nh
Â§ ni
Â· ni
yË† k t  Â¦ w 2jk F Â¨Â¨ Â¦ wij1 vi0 t   b1j Â¸Â¸  Â¦ wik" vi0 t ;
j 1
Â©i 1
Â¹ i0
for 1 d k d m
(13)
where wij1 , w 2jk , wik" denote the weights between input
and hidden layer, weights between hidden and output
layer, and weights between input and output layer
respectively. b1j and vi0 denote the thresholds in hidden
nodes and inputs that are supplied to the input layer
respectively; ni, m and nh are the number of input nodes,
output nodes and hidden nodes respectively. F( x ) is an
activation function that is normally be selected as
sigmoidal function. In this paper, sigmoidal function was
used for the activation function of the HMLP network.
The weights w 2jk , wik" , wij1 and thresholds b1j are
unknown and should be selected to minimise the
prediction error defined as:
H k t  y k t   yË† k t 
(14)
where y k t  and yË† k t  are the desired outputs and
network outputs respectively.

Figure 3. One-hidden layer HMLP network

4. Results and Discussion
Two types of objects have been used to test the
performance of the proposed approach. Each type consists
of eleven 3D objects. The first type, will be referred as
Type 1 object, contains simple 3D shape like cylinder,
box, trapezoid, sphere etc. The second type, will be
referred as Type 2 object contains free-form objects.
Figure 4 and 5 show these types of objects.
The HMLP network has been trained using 11
hidden nodes and 11 output nodes to represent 11 objects
for both types of objects. Inputs to the networks were
assigned as in Table 1 and Table 2 respectively. The
designing parameters for MRPE were selected as
Dm 0 0.00001 ,
a 0.01 ,
D (t ) g D m (t )(1  D m (t )),
b 0.85, O0 0.99, O 0 0.95 and P 0  10000 I .
Please refer to Mashor [16] for the definitions of these
parameters.
Table 1 and Table 2 show the recognition
performance of the proposed method for simple objects
(Type 1) and free-form objects (Type 2), respectively. The
inputs of the neural networks for both cases were Huâ€™s
moment and the results were produced after 100 training
epochs. Generally, the networks that used lower order
moments achieved better recognition rate compared to the

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIVâ€™04)
0-7695-2178-9/04 $20.00 Â© 2004 IEEE

ones that used higher order moments. Higher order
moments change rapidly for each rotation and normally
more sensitive to noise compare to lower order moments
[17]. Consequently, the features stability will decrease
thus reduce the recognition rate. Better recognition rate
could be achieved by combining Huâ€™s moments. 100%
accuracy were obtained for both training and testing data
sets when the first three Hu moments were used to train
the network models for both types of 3D objects.
However, the combination of all Hu moments seems to
confuse the neural networks as indicated by low
recognition rate in Table 1 and Table 2.

Figure 4. Type 1- simple 3D shape

global features, it can be applied arbitrarily to any 3D
objects. 100% recognition rates for the two types of
objects show that this method can be successfully applied
to 3D object recognition.

Table 1: Recognition performance for 3D object
type 1 using Huâ€™s moment
Hu
Moment

Input
Nodes

Training
Accuracy

Testing
Accuracy

Final
MSE(dB)

M1
M2
M3
M4
M5
M6
M7
M1 M 2
M1 M 3
M1 M 2 M 3
All M ' s

3

93.94

91.67

-14.91

3

85.61

85.35

-11.21

3

98.23

96.23

-19.39

3

66.16

57.58

-6.31

3

66.41

59.09

-5.69

3

76.01

67.17

-8.21

3

67.17

57.07

-6.49

6

98.74

97.73

-29.60

6

100

99.50

-37.99

9

100

100

-40.38

21

93.43

86.11

-12.71

Table 2: Recognition performance for 3D object
type 2 using Huâ€™s moment

Figure 5. Type 2 - free-form object

Hu
Moment

Input
Nodes

Training
Accuracy

Testing
Accuracy

Final
MSE(dB)

M1
M2
M3
M4
M5
M6
M7
M1 M 2
M1 M 3
M1 M 2 M 3
All M ' s

3

98.23

97.48

-15.28

3

89.14

87.12

-10.97

3

89.14

87.88

-10.68

3

81.82

76.01

-9.05

3

77.53

75.25

-7.79

3

81.82

76.01

-9.27

3

66.16

60.86

-5.84

6

99.50

99.75

-23.44

6

100

99.24

-24.43

9

100

100

-30.25

21

99.75

95.96

-21.26

Acknowledgement
5. Conclusion
A 3D object recognition method is proposed using
2D multiple views technique and HMLP network. The
recognition results show that with some adaptation to
multiple-views technique, Huâ€™s moments are adequate to
model the 3D objects. By using 2D moments the proposed
method do not require complex features calculation as for
3D representation, thus reduce processing time in feature
extraction stage. In addition, since Huâ€™s moments are

I like to thank University Sains Malaysia and
Malaysian Government for supporting this research in
term of research grant.

References
[1]

Y. Shirai, Three-Dimensional Computer Vision, New
York: Springer-Verlag, 1987.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIVâ€™04)
0-7695-2178-9/04 $20.00 Â© 2004 IEEE

[2]

[3]
[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

M. F. S. Farias, & J. M. de Carvalho, Multi-view
Technique For 3D Polyhedral Object Recognition Using
Surface Representation, Revista Controle & Automacao.
1999, 10(2): 107-117.
T. Paggio, & S. Edelman, A Network That Learns to
Recognize 3D Objects. Nature, 1990, 343: 263-266.
H. Murase, & S. K. Nayar, Visual Learning and
Recognition of 3D Objects from Appearance,
International Journal of Computer Vision, 1995, 14: 5-24.
S. D. Roy, S. Chaudhury, & S. Banerjee, Active
Recognition Through Next View Planning: A Survey,
Pattern Recognition (Accepted for Publication), 2003.
U. BÃ¼ker, & G. Hartmann, Knowledge-Based View
Control of Neural 3D Object Recognition System, In
Proceeding of International Conference on Pattern
Recognition, 1996, D:24-29.
B. Schiele, & J. L. Crowley, Transinformation for Active
Object Recognition, InProceeding of the 6th International
Conference on Computer Vision. 1998, 249-254.
A. Selinger, & R. C. Nelson, Appearance-Based Object
Recognition Using Multiple Views, In Proceedings of the
2001 IEEE Conference on Computer Vision and Pattern
Recognition, 2001, 1: 905-911.
J. Mao, P. J. Flynn, & A. K. Jain, Integration of Multiple
Feature Groups and Multiple Views into a 3D Object
Recognition System, Computer Vision and Image
Understanding, 1995, 62(3): 309-325.
S. Ullman, Three-Dimensional Object Recognition Based
on the Combination of Views, Cognition. 1998, 67: 2144.
Y. K. Ham, & R. â€“H. Park, 3D Object Recognition In
Range Images Using Hidden Markov Models And Neural
Networks, Pattern Recognition, 1999, 32:729-742.
T. W. Riddler, & S. Calvard, Picture Thresholding Using
an Iterative Selection Method, IEEE Transactions on
Systems, Man and Cybernetics. 1978, 8: 630-632.
H. J. Trussell, Comments on Picture Thresholding using
an Iterative Selection Method, IEEE Transactions on
Systems, Man and Cybernetics, 1979, 9(5): 311.
R. Klette, & P. Zamperoni, Handbook of Image
Processing Operators, England: John Wily & Sons,
1996.
M. K. Hu, Visual Pattern Recognition By Moment
Invariants, IRE Transactions on Information Theory,
1962, 8(2):179-187.
M. Y. Mashor, Hybrid Multilayered Perceptron Networks,
International Journal of System and Science, 2000,
31(6):171-185.
R. J. Prokop, & A. P. Reeves, A survey of Moment-Based
Techniques for Unoccluded Object Representation and
Recognition, CVGIP: Graphics Models and Image
Processing, 1992, 54(5): 438-460.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIVâ€™04)
0-7695-2178-9/04 $20.00 Â© 2004 IEEE

