Fifth International Conference on Computer Graphics, Imaging and Visualization
Visualisation

Parallel Visualisation Approach of a 3D Heart Model
Kalpana Kanthasamy1, Alwin Kumar Rathinam2
1
Faculty of Computer Science and Information Technology
2
Virtual Reality Centre
University of Malaya
Kuala Lumpur, Malaysia
kalpana0611@perdana.um.edu.my, alwin@solutioniser.com
These huge amounts of dataset usually possess a
challenge in the visualisation’s rendering performance.
Visualisation usually consumes a lot of processing and
execution time when conducted on a standalone
machines. This has, however, been solved to some
extent with the introduction of supercomputers.
Nevertheless, the disadvantages still exist, particularly
in the cost of acquiring and maintaining this machine.
We present a cost effective method of visualising
simulation datasets. The paper outlines the method
used in converting the simulation results of the heart to
point cloud and eventually to a parallel version. A time
varying visualisation is then tested on a standalone
machine. Parallel computing environment are
conversed as well.

Abstract
This research done introduces a parallel approach
in visualising a three dimensional (3D) virtual heart
model simulation dataset. The paper outlines briefly
the Immersed Boundary Method (IBM) and how they
differ from regular CFD. The dissimilarities motivate
the need to show graphical representation of the IBM.
The Immersed Boundary Method was then used to
compute the 3D heart model simulation. The resulting
simulation data was then visualised utilising a parallel
adaptation of the Visualisation Tool Kit (VTK), an
open source library. The visualisation was conducted
in a cluster computing environment. The computers
used in the cluster were off-the-shelf commodity
personal computers. The number of processors
involved in the parallelism would help to speed up the
rendering performance. This research will assist in low
cost building of visualising biological modelling
applications in the future.

2. Background
The IBM differs from regular CFD in terms of
their boundary conditions. In regular CFD, the
boundary’s movement is positioned in advance before
the fluid flow takes place. In the case of the IBM, the
boundary’s movement causes the fluid to move. The
reverse force by the fluid reacts on the boundary and
generates movement of the boundary to a new position.
Thus, for the IBM, both the fluid flow and the
boundary’s motion have to be computed
simultaneously.
A similar methodology takes place in the human
heart. The muscles in the heart’s wall pump the blood
around the circulatory system. The blood flow in turn
causes the heart’s wall to move.
The human heart consists of four chambers, the
right atrium and ventricle, and the left atrium and
ventricle. The movements of the muscles are in a
complex spiral motion. The direction of blood flow in
the heart is controlled by a number of valves to ensure
a one-way flow[1].
As the structural aspect of the heart is complex, it
is quite a difficult task to accurately model the heart.
The IBM Virtual Heart Model is an attempt to closely

Keywords--- Immersed Boundary Method
(IBM), Computational Fluid Dynamics (CFD),
visualisation, visualization, Virtual Heart Model,
point cloud, cluster computing, VTK

1. Introduction
Mathematical modelling is an imitation of processes
that occur in a real world system using the language of
mathematics. Mathematical modelling is used regularly
in the physical sciences and is now being deployed into
life sciences especially in the biology field. One such
area of interest is computational modelling in biology
fluid dynamics.
This leads us to our research area, a computational
model of a 3D heart, and, consequently visualisation of
the model itself. It is always easier to understand a
graphical representation instead of deciphering the
numerical simulation. Even so, visualising a simulation
generated data is no easy task as the data can appear to
be massively large.

978-0-7695-3359-9/08 $25.00 © 2008 IEEE
DOI 10.1109/CGIV.2008.40

362

resemble the heart in function using geometrical shapes
such as tubes, cones and spheres. Each geometrical
shape represents a portion of the real heart. For
example the ventricles are modelled as upside down
cones and the vessels are modelled as tubes. These
shapes are then set on motion based on the IBM
formulae.
This requires extensive calculations and
processing capabilities to generate the model result
dataset and to visualise it.
A standalone computer is unable to have this
processing capabilities, hence the need for a parallel
approach. Parallel computing is utilized by many
visualisation programmes, namely weather and climate
forecast, biological modelling, geological and seismic
activities. Parallel computing is defined as the
concurrent use of multiple compute resources to
resolve a computational task using multiple
processors[2].

2.3 Virtual Heart Models
Visualising a human’s heart in today’s world
remains a challenge to the computer scientists and
cardiologists. Though many work has been done on
modelling and visualising the left ventricle(LV)
chamber of the heart, modelling other areas such as the
tricuspid and bicuspid valve remain as a problem
because of its complex structure.
Other areas encompassed by the heart visualisation
are in the heart-muscle fiber reconstruction. Leonid
Zhukov and Alan H.Barr[4, 5] reconstructed heart
muscle fiber from diffusion tensor Magnetic
Resonance Imaging(MRI) to show that the 3D
spiraling diagonal orientation of the muscle fibers on
the heart and systematic smooth variation in pitch and
direction of the fibers from endocardium to
epicardium. Diffusion tensor MRI is a technique used
to measure the anisotropic diffusion properties of
biological tissues as a function of the spatial position
within the sample. It classifies the various types of
tissues and can be used for tissue segmentation. This
classified tissue is then visualised using two methods,
glyph based visualisation and fiber tracing.
As mentioned above, many of the heart
visualisation involves the left ventricular (LV)[6]. In
order to visualize the LV, the heart is first constructed
as a mathematical model which resembles the actual
LV. The LV is represented as a set of points, the
isolated set of points were connected by lines to
simulate the surface. These connected lines are then
defined as a fiber. The ventricular fibers were arranged
so that the geometry of the modelled left ventricle
closely matched the real LV[7] [8].
Besides that, image slices of heart can also be used
to visualize the heart, a fly through the heart can then
be deployed to see if there is any deformation in
heart’s internal organs[9, 10].
Many of these modelling and visualisation assist
in identifying heart deformation which shows
symptoms of heart diseases. The deformation of the
myocardium (heart muscle) which leads to heart attack
can be identified. One example of the contribution of
modelling work on the heart is in the design of
artificial heart valves[11]. It is also noted that many of
the visualisation research area were on the left
ventricular area.
Model simulation data are usually large and
requires a fast processor and adequate memory to
render them. Given this constrain, most models only
attempt to model a certain portion of the heart and not
the whole heart as in the IBM virtual heart model.

2.1. Immersed Boundary Method
The Immersed Boundary Method (IBM) technique
concurrently simulates both the fluid and the
boundary’s motion. It is an extension of the Navier
Stokes equation which is used to calculate the fluid’s
flow. This fluid flow would then trigger the movement
of the elastic boundary. The fluid is positioned in a
grid lattice[3].
In the human heart, the blood is represented as the
flow and the muscle fibers are considered as the
boundary. The fibers contract as the blood is pumped
in and out.

2.2. Visualisation of Biological Modelling Data
Visualisation is carried out to investigate the fluid
flow density, momentum and pressure values from the
resulting simulation’s data. The visualisation of fluid
flow simulation data serves many different purposes.
Scientists are interested in comparing the visualisation
outcome to the mathematical calculations of the fluid
flow and boundary movement. Also, the visualisation
would help to identify errors in the modelling
calculation as shown in Figure 2.
Biological modelling produces vast amount of
simulation data. The outcome would be a massive
quantity of dataset waiting to be visualised.
Visualisation would assist to understand more about
the phenomena. The large datasets often demand
higher processing and rendering capabilities of
personal computers and technical workstations.

363

3. Our Method

Source Data FileV(x,y, z )

At Virtual Reality Centre of University of Malaya,
we are working on parallelising visualisation
techniques. We are interested in visualising biological
fluid dynamic because of it high scientific contribution.
We will be using the dataset from the IBM Virtual
Heart model.
The methodology used consists of 2 phase, first
the standalone visualisation phase and second, the
parallel visualisation phase.
The standalone visualisation phase starts with data
acquisition of the 52000 time steps of simulation data
into point clouds data. The conversion of point clouds
to unstructured grid is explained. These converted
dataset are then visualised in 3D using the VTK
library[12].
The second part focuses on the parallel
visualisation of the derived time steps. Here, three
parallel environments are discussed.

Reader
PolyDataMapper
Actor

Camera
Renderer
Figure 1. The Visualisation pipeline used
in the particle reader
The particle reader loads in the points in an array of
vx-vy-vz triplets each accessible by a cell id. It then
applies the Poly Data Mapper to create a topology out
of the point clouds. Once the geometry has been
created, the image is then rendered using the Actor,
Camera and Renderer modules. An example of this
output is as in Fig 3.

3.1. Point Clouds
The modelling team at VRC has successfully run
the 3D heart model simulation. We used the IBM
method proposed by Peskin to model the heart[3, 13].
This generated 52000 time step of the heart’s
movement of which every 1000 step is saved as the
output. Hence, the 52 sets of data.
The resulting dataset is in a complex format
comprising of pressure, velocity, stress etc. Only the
x,y and z co-ordinates from the results are extracted
out to generate the point cloud format to be used in the
visualisation process. This point cloud format
represents the muscle fibers, veins, arteries and valves
in a 3D domain.
Particle Reader
The point cloud dataset was first visualised in a
standalone computer using a particle reader algorithm
present in the VTK library. Total amount of point
clouds rendered were 758258 points, positioned using
x, y and z co-ordinates. The diagramme below outlines
the visualisation pipeline.

Figure 2. The visualisation of the virtual heart
shows that there is a error in the modelling
calculations

364

was then converted to the unstructured gird format and
eventually visualised using ParaView.
An earlier version of the time variant
Visualisation was done by Somasundram et al[15]. The
following is a snapshot of two time steps at 1000 and
at 20000. Notice the deformation of the heart wall as
calculated by the IBM in the lower part of Fig 4. The
colouring scheme was done based on pseudocolour on
the Y-axis which does not represent any special
meaning.

Figure 3. Different colours used to represent
different components of the virtual heart.
Parallel Unstructured Grid Format
The particle reader program was then modified to
generate a parallel unstructured grid format of the
dataset. The parallel format is in an eXtendable
Markup Language(XML) format. XML is heavily used
as a format for document storage and processing. Since
the parallel visualisation will be done using
ParaView[14], the dataset format should be adaptable
to it, hence the choice of the XML. Paraview is a
graphical implementation of VTK libraries which
allows for a systematic parallel visualisation. The serial
point cloud(PC) data was transformed into an
unstructured grid(VTU) format by adding a header and
footer. The footer consists of the topology list and cells
that interconnect the PC data. This is then converted
into the parallel version of the unstructured grid
(PVTU) in an XML format. Here, the headers and the
footers were modified to suit the parallel XML
standard.
PC Æ VTU Æ PVTU
Figure 5. Two different visualisation taking
place according to the time variable specified.

Figure 4. Conversion of a serial point cloud to
the parallel format

3.3. Challenges in a Standalone Visualisation

3.2. Visualisation of Time-Varying Data

The main concern in visualising a huge dataset
would be the inadequate processing capabilities in a
standalone machine. A single processor is unable to
read, process geometry algorithm and render the point
clouds all at the same time. It took even longer when
filters was applied to it. The filters here refer to cutting,
zooming and rotating algorithms.

As the simulation data consists of multiple time
steps, the information is best interpreted when the
visualisation is programmed to continuously execute
each frame at a given time step. Each time step data

365

In short, heart visualisation is very resource
exhausting and requires fast processors that can speed
up the rendering process. Time remains the main
constraint in standalone visualisation
These challenges could be solved with the
deployment of a supercomputer which is expensive or
we could use a less expensive approach by
implementing the visualisation pipeline over a cluster
of computers.

4.1. Parallel Visualisation Approach
The following are the factors that encouraged us to
perform the heart visualisation in a heterogeneous
computing environment.
Visual Representation
The quality of visual representation matters the most
when it comes to visualisation. The quality is measured
by the time taken to load the data, the execution time
and rendering time. It takes longer time to visualise
758,258 points in a standalone Pentium 4 machine
compared to a 4 nodes of Pentium 4 cluster computers.
A simple combination like this can easily speed up the
time taken to load data into the visualisation pipeline.

4.0. Parallel Computing Environment
Parallel computing lays the foundation for this
parallel visualisation[16].Parallel computing is defined
as the concurrent use of multiple compute resources to
resolve a computational task using multiple processors.
Parallel computing can also be designated as parallel
processing where a task is divided into several sub
tasks that will be processed simultaneously.
These days, researches are opting to use the cluster
and the grid as their parallel platform. The following
paragraphs define two different clusters and a grid.

Cost Effectiveness
The most important criteria of a cluster computing
visualisation would be the ability to drastically reduce
cost and yet have the same performance of a super
computer.

4.2. Expected Performance

Homogeneous Cluster
Homogeneous computing can be defined as
components that are built using the same processor
architecture. For example, a homogeneous computing
may consist of all Pentium 4 machines. Each node in
the cluster is identical to each other in terms of their
memory and processor speed.

It is vital for us to know the difference in the
computation time when visualisation is done on a
standalone and in a cluster computing environment.
Therefore, for comparison purposes, a first run was
conducted on a standalone machine and the second one
using multiple processors that resides in a cluster. The
significant increase in the loading and execution time
of the data would be useful for other similar
applications.

Heterogeneous Cluster
Heterogeneous cluster computing consists of
components that are built using different architecture.
In such a system, the components can consist of
machines that differ in their processors' speed or
operating system. Visualisation applications, most of
the time require special hardware support, certain
computation require extensive use of processing and
some less processing. The computation which requires
less processing can be done on normal a single core
machines and the highly resource hungry
computational commands can be processed by dual
core or quad core machines. Heterogeneous computing
system may also combine 32 and 64 bit machine in a
parallel form.

5.0. Discussion
This 3D Virtual Heart model is an attempt to
resemble the heart.
The processing capability of a cluster computing
environment was found to be increased as compared to
a standalone computer. This is so, as more processors
are available in the clustered environment. The results
in this research strongly support this statement and
further research to produce numerical supporting
evidence is in progress in our lab.
Allocating the processor’s power for a certain
computation is also important. The processing power
in a cluster computing can only be utilised wholly if
the allocations are appropriate using the correct data
format as we have shown in this research. Issues like
load balancing and modifying current load balancing
algorithm to suit application as such can be carried out
in future to enhance its performance.

Grid
A grid is distribution of computers interconnected on a
wider geographical scale. In a grid, each node is able to
use its own resource to act on a task and there is no
presence of a single system image.

366

[6]

6.0 Conclusions
The parallelisation of the medical visualisation
raises many issues concerning bandwidth and latency.
In order to parallelise any kind of data visualisation,
we need to study the visualisation pipeline thoroughly.
The visualisation pipeline could be done in parallel to
enhance its performance and reduce the time taken.
This research paves a way for new experiments in
visualising medical related data. The virtual heart
model can be used for virtual drug testing and
analysing deformations in the cardiac muscle which
would be visible through visualisation

C.C. Vesier, J.D. Lemmon Jr, R.A. Levine, and A.P.
Yoganathan, “A three-dimensional computational model
of a thin-walled left ventricle,” Proceedings of the 1992
ACM/IEEE conference on Supercomputing1992, pp. 7382.

[7] H. Zhenhua, M. Dimitris, and A. Leon, “Computational
modeling and simulation of heart ventricular mechanics
with tagged MRI,” Book Computational modeling and
simulation of heart ventricular mechanics with tagged
MRI, Series Computational modeling and simulation of
heart ventricular mechanics with tagged MRI, ed.,
Editor ed.^eds., ACM, 2005, pp.
[8] C.R. Jegathese, G.L. Guan, L.A. Rajiv, E.Y.K. Ng, D.
Ghista, and E.C. Prakash, “Cyber heart: the employment
of an iterative design process to develop a left
ventricular heart graphical display,” Cyberworlds, 2003.
Proceedings. 2003 International Conference on2003,
pp. 237-244.

Acknowledgements
We would like to extend our gratefulness to the
late Prof. Dr. Ir. N. Selvanathan of University Malaya
for his help in initiating this project. We are also
thankful to the Heart Modeling team, specifically, Dr.
Woo Chaw Seng, Mr. Somasundaram and Mr. Kevin
for their role in generating the simulation data from the
IBM Virtual Heart Model. This project was carried out
using grants made available by University of Malaya,
the Ministry of Science Technology and Innovation
(MOSTI) and MDEC’s Creative Application and
Development Center (CADC) of Malaysia.

[9] F.B. Sachse, C.D. Werner, M.H. Stenroos, R.F. Schulte,
P. Zerfass, and O. Dössel, “Modeling the anatomy of the
human heart using the cryosection images of the Visible
Female dataset,” Proc Third Users Conference of the
National Library of Medicine's Visible Human
Project2000.
[10] J.S. Park, M.S. Chung, S.B. Hwang, Y.S. Lee, D.H.
Har, and H.S. Park, “Visible Korean Human: Improved
serially sectioned images of the entire body,” Medical
Imaging, IEEE Transactions on2005, pp. 352-360.

References
[1] D.J. Taylor., N.P.O. Green., and G.W. Stout.,
“Biological Science,” 3rd ed., R.Soper ed., Cambridge
University Press, 1997, pp. 469 - 478.

[11] P.N. Watton, X.Y. Luo, X. Wang, G.M. Bernacca, P.
Molloy, and D.J. Wheatley, “Dynamic modelling of
prosthetic chorded mitral valves using the immersed
boundary method,” Journal of Biomechanics2007, pp.
613-626.

[2] R. Buyya, High Performance Cluster Computing:
Architectures and Systems, Volume I, Prentice Hall,
1999.

[12] W. Schroeder, K. Martin, and B. Lorensen, “The
Visualization Toolkit An Object-Oriented Approach To
3D Graphics, Kitware,” Inc. publishers2004.

[3] C.S. Peskin, and D.M. McQueen, “Fluid dynamics of
the heart and its valves,” Case Studies in Mathematical
Modeling: Ecology, Physiology, and Cell Biology (HG
Othmer, FR Adler, MA Lewis, and JC Dallon, editors),
Prentice-Hall, Upper Saddle River NJ1996, pp. 309–
337.

[13] D.M. McQueen, and C.S. Peskin, “A three-dimensional
computer model of the human heart for studying cardiac
fluid dynamics,” ACM SIGGRAPH Computer
Graphics2000, pp. 56-60.
[14] A. Henderson, and J. Ahrens, The ParaView Guide,
Kitware, 2004.

[4] L. Zhukov, and A.H. Barr, “Oriented tensor
reconstruction: tracing neural pathways from diffusion
tensor MRI,” Proceedings of the conference on
Visualization'022002, pp. 387-394.

[15] N. Soma, J. Kevin, K.R. Alwin, C.S. Woo, and S. Diljit,
“Time variant visualization of the Immersed Boundary
heart model,” Technical Report University Malaya
Virtual Reality Centre. 2007.

[5] L. Zhukov, and A.H. Barr, “Heart-muscle fiber
reconstruction from diffusion tensor MRI,” Proc.
Visualization, 2003. VIS 2003. IEEE, 2003, pp. 597602.

[16] W.L. Goffe, and M. Creel, “Multi-core CPUs, Clusters
and Grid Computing: a Tutorial.”.2007

367

