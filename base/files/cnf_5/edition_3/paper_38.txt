Fifth International Conference on Computer Graphics, Imaging and Visualization
Visualisation

Integrating Audio Visual Data for Human Action Detection
Lili Nurliyana Abdullah1 and Shahrul Azman Mohd Noah2
1. Dept of Multimedia, Faculty Computer Science and Information Technology, 43400 UPM
Serdang, Selangor, Malaysia
2. Dept of Information Science, Faculty of Technology and Information Science, 43600 UKM
Bangi, Selangor, Malaysia
{ liyana@fsktm.upm.edu.my, samn@ftsm.ukm.my}
The increasing availability and use of video has
raised demands for better modeling of video and the
provision of more sophisticated indexing and
retrieval techniques. A one minute movie clip may
contain about 2000 video frames, a mixture of
sounds (audio), and several lines of close caption
(text). Human action is defined as a sequence of
body postures with a specific timing constraint.
Human actions are characterized by the spatiotemporal structure of their motion pattern. The
detection and understanding of human action in
videos is of high value for many applications: human
computer interaction, surveillance, collaborative
environments, training and entertainment, and
medical support systems. Human action detection
from video is an active research area in recent years.
Detecting human actions has been one of the most
interesting and challenging problems in multimedia
applications. The automatic recognition of human
actions has an increased importance in the last years,
due to its utility in the surveillance and protection on
civil areas.

Abstract
This paper presents a method which able to
integrate audio and visual information for action
scene analysis in any movie. The approach is topdown for determining and extract action scenes in
video by analyzing both audio and video data. In
this paper, we directly modelled the hierarchy and
shared structures of human behaviours, and we
present a framework of the Hidden Markov model
based application for the problem of activity
recognition. We proposed a framework for
recognizing actions by measuring human actionbased information from video with the following
characteristics: the method deals with both visual
and auditory information, and captures both spatial
and temporal characteristics; and the extracted
features are natural, in the sense that they are
closely related to the human perceptual processing.
Our effort was to implementing idea of action
identification by extracting syntactic properties of a
video such as edge feature extraction, colour
distribution, audio and motion vectors. In this
paper, we present a two layers hierarchical module
for action recognition. The first one performs
supervised learning to recognize individual actions
of participants using low-level visual features. The
second layer models actions, using the output of the
first layer as observations, and fuse with the high
level audio features. Both layers use Hidden Markov
model-based approaches for action recognition and
clustering, respectively. Our proposed technique
characterizes the scenes by integration cues
obtained from both the video and audio tracks. We
are sure that using joint audio and visual
information can significantly improve the accuracy
for action detection over using audio or visual
information only. This is because multimodal
features can resolve ambiguities that are present in
a single modality. Besides, we modelled them into
multidimensional form.

Recognizing human action is a challenging task
due to the multiple body parts of interacting persons.
First, it involves in getting the whole body motion
data. For this, various techniques such as infrared
rays can be considered. Then, it involves the
interpretation of the human motion, which includes
modelling of action, feature extraction, classification,
and detection of the action. In general, almost all
human action recognition systems work mainly at
visual level only, but other information modalities
can easily be available, and used as complementary
information to retrieve and explain interesting action
in a scene.
Our proposed technique characterizes the scenes
by integration cues obtained from both the video and
audio tracks. We are sure that using joint audio and
visual information can significantly improve the
accuracy for action detection over using audio or
visual information only. This is because multimodal
features can resolve ambiguities that are present in a
single modality. Besides, we modelled them into
multidimensional form.

Keywords: audio feature, visual feature, hidden
Markov model, human action detection
1. Introduction

978-0-7695-3359-9/08 $25.00 © 2008 IEEE
DOI 10.1109/CGIV.2008.65

242

recognition and classification) directly based on
multiple features. The HMMs too are well-known
framework to deal with uncertainty and dynamic
data, and often used for action recognition. A first
attempt of gesture recognition based on HMM was
realized by Yamato et al [7]. They used discreet
HMM to recognize six different tennis strokes among
three players. D M Gavrila [5] used HMM to acquire
a symbol representation of action from time series
joint angles of human’s whole body by motion
capture, and generate whole body motions from
HMM for humanoids.

2. Related Works
Previous works on audio and visual content
analysis were quite limited and still at a preliminary
stage. Current approaches for audiovisual data are
mostly focused on visual information such as colour
histogram, motion vectors, and key frames [1, 2, 3].
Although such features are quite successful in the
video shot segmentation, scene detection based on
the visual features alone poses many problems.
Visual information alone cannot achieve satisfactory
result. However, this problem could be overcome by
incorporating the audio data, which may have
additional significant information. For example,
video scenes of bomb blasting should include the
sound of explosion while the visual content may vary
a lot from one video sequence to another. The
combination of audio and visual information should
be of great help to users when retrieving and
browsing audiovisual segments of interest from
database. Boreczky and Wilcox [4] used colour
histogram differences, and cepstral coefficients of
audio data together with a hidden Markov model to
segment video into regions defined by shots, shot
boundaries and camera movement within shots.

Most of the existing work relies on using only a
single source of information (example, either audio
or visual track data alone). In [4], the average video
shot activity and the duration are used as features for
the categorization of movies according to the actions.
An action scene was characterized by temporally
localized properties of video shots which have little
or no recurring similar visual contents [5]. Although
these visual characters are undoubtedly good
indicators of rapidly evolving action contents, they
are not enough to determine the desired action. On
the other hand, audio-based action detection was
independently performed on the sound track in [6].
However, this audio alone method may lead to many
potential false detected cases because many sounds
often mix different noises and other similar
background sound.

Human tracking and, to a lesser extent, human
action recognition have received considerable
attention in recent years. Human action recognition
has been an active area of research in the vision
community since the early 90s. The many approaches
that have been developed for the analysis of human
motion can be classified into two categories: modelbased and appearance-based. A survey of action
recognition research by Gavrila, in [5], classifies
different approaches into three categories: 2D
approaches without shape models, 2D approach with
shape models and,3D approaches; the first approach
to use 3D constraints on 2D measurements was
proposed by Seitz and Dyer in [6].

3. Methodology
Unlike previous approaches, our proposed
technique characterizes the scenes by integration
cues obtained from both the video and audio tracks.
These two tracks are highly correlated in any action
event. We are sure that using joint audio and visual
information can significantly improve the accuracy
for action detection over using audio or visual
information only. This is because multimodal
features can resolve ambiguities that are present in a
single modality. Besides, we modelled them into
multidimensional form.

To improve the effectiveness of detection,
techniques for genre classification are wide studied.
For audio tracks, some techniques [5] are proposed
to discriminate different types of audio and music
genre classification [7]. For video content, genres of
films [9] are automatically classified by exploring
various features. In this action analysis technique,
various features from audio, video and text are
exploited, and many multimodal approaches are
proposed to efficiently cope with the access and
retrieval issues of multimedia content. In this work,
we combine information from features that are based
on image differences, audio differences, and motion
for action analysis. In [4], HMMs are used to build
scenes from video which has already been segmented
into shots and transitions. Here, HMMs are used to
perform to perform action analysis (detection,

The audio is analysed by locating several sound
effects of violent events and by classifying the sound
embedded in video. Simultaneously, the action visual
cues are obtained by computing the spatio-temporal
dynamic activity signature and abstracting specific
visual events. Finally, these audio and visual cues are
combined to identify the violent scenes. Create table
on comparison recognition percentage with using
single source either visual or audio track alone, or
combined audio-visual information.
This section gives an overview of the overall
system and describes in detail the part of the
information fusion to analysis action. The framework

243

contains two levels. The first level is to compute
features and segmentation. The second level is to
apply the computation algorithms to the
classification, detection and recognition function to
obtain the desired action.

Feature
Extraction

Basically the task of the first level is to split the
image/video data into several regions based on
colour, motion, texture or audio information. The
goal of the feature extraction process is to reduce the
existing information in the image into a manageable
amount of relevant properties. First, what we require
is the shot boundary between two different frames is
clearly detected. We use a pixel-based approach for
conducting the shot detection. Assume X and Y are
two frames, and d(X,Y) is the difference between the
two frames. PX(m,n) and PY(m,n) represent the
values of the (m,n)-th pixels of X and Y,
respectively. The following equation shows the
approach to do shot change detection. below
graphical objects, as in Figure 1.

d

XY

Shot Change
Detection

HMM 1

HMM 2

...

Motion Activity
Extraction

HMM n

Semantic Context
Computation

Fusion Computation

HMM 1

Classification

HMM 2

Detection / Recognition

...

HMM n

Second Level

Figure 1. Overview of the 2-Layer Framework

We will use multi-stage/multi-dimensional HMM
to fuse the multimodal features. Multi-dimensional
HMM:
- normal HMM trained on three feature sets:
audio, colour and motion;
- multi-stream HMM combining individual
audio and visual streams;
- asynchronous HMM combining audiovisual
streams.

and
1
¦¦ (m, n)
m * n m n d XY

Video Feature
Extraction

First Level

­1, | PX (m, n)  PY (m, n) |! T1
(m, n) ®
else
¯0,
(1)

d(X ,Y )

Audio Feature
Extraction

(2)

If d(X,Y) is larger than a threshold T1, a shot
change is detected between frames X and Y.
For extracting audio features, we use an audio
feature vector consisting of n audio features which is
computed over each short audio clips (100ms
duration). The extracted features include: volume,
band energy ratio, zero-crossing rate, frequency
centroid and bandwidth. These adopted audio
features have been widely used in many audio
applications, e.g, [2,9] and are known to perform
reasonably well.

The lower layer is where a basic HMM is trained
on individual modal features as seen in Figure 2.
Each stream is modelled independently. An audio
based HMM classifier (HMM Classifier 1) is first
used to separate the input video sequence into few
types of sound: scream, cries, explosion, etc. In the
second stage, visual based HMM classifiers (HMM
Classifier 2) are used to recognize colour of blood,
flames and darkness. Same goes to third stage to use
motion (HMM Classifier 3) to calculate the intensity
of fast or slow motion. Shot break can be detected
based on frame differences in both colour histogram
and motion field. Separate audio and visual
mappings (X) containing different combination rules
of different multimodal information. This is where
basic HMMs are trained on combined audio-visual
features. This method involves aligning and
synchronizing audio-visual features to form one
concatenated set of features which is then treated as a
single stream of data. Then to detect high-level
semantic content we will be using pseudo-semantic
feature modelling by applying two statistical
techniques: GMM and HMM. The final detection is
based on the fusion of the outputs of the three
modalities by estimating their joint occurrence.

HMM is very effective for capturing the dynamic
behaviour of a temporal and time-varying event. It
can integrate multimodal features easily. Multimodal
interaction can enhance the content findings of one
source by using similar content knowledge extracted
from the other sources. A discrete HMM is
characterized by O = (A, B, 3), where A is the state
transition probability matrix, B is the observation
symbol probability matrix, and 3 is the initial state
distribution.

244

measures the proportion of actions that are
recognized. The correctness of the detection results
and missing detection of the correct actions are
judged by humans. We identified the percentage of
recall and precision of the proposed framework. See
Table 1.

Recognition was done simply by selecting the
HMM that was most likely to generate the given
sequence of feature vectors. The main advantage of
such an approach is that adding a new action can be
simply done by training a new HMM.

Table 1: Action detection results by audio
only, audiovisual combination

4. Results and Discussion
For each event, short video clips of each 5 – 10s
in length are selected as the training data. Based on
the results extracted from the training data, a
complete specification of HMM with two model
parameters (model size and number of mixtures in
each state) would be determined. Each HMM must
be trained so that it is most likely to generate the
symbol patterns for its category. Training an HMM
means optimising the model parameters (A, B,S) to
maximise the probability of the observation sequence
P (T|O).

By audio only
Feature
Video
Clip
A
B
C
D

See Figure 3 to view some samples of collected
data.

E

Motion Intensity

Plots of Activity Intensity

F
10
Action

5

G

0
1

3

5

7

9

11 13 15 17 19 21 23 25

Shot Number

AVE
RAGE

Preci
sion
15/2
2
=
68.2%
14/1
6
=
87.5%
55/7
7
=
71.4%
51/8
7
=
58.6%
11/1
5
=
73.3%
75/1
09
=
68.8%
40/4
9
=
81.6%
72.77%

Reca
ll

15/2
0 = 75%

14/1
5
=
93.3%
55/8
4
=
65.5%
51/7
3
=
69.9%
11/2
8
=
39.3%
75/9
8
=
76.5%
40/9
3
=
43.0%
66.07%

By
audio/visual
combination
Preci
sion
20/2
7
=
74.1%
15/1
6
=
93.8%
84/8
9
=
94.4%
70/7
9
=
88.6%
27/3
0
=
90.0%
93/1
06
=
87.7%
92/9
8
=
93.9%
88.93%

Reca
ll

20/2
0 = 100%
15/1
5 = 100%
84/8
4 = 100%
70/7
3
=
95.9%
27/2
8
=
96.4%
93/9
8
=
94.9%
92/9
3
=
98.9%
98.01%

5. Conclusion
We presented an approach to characterize and
abstract human activity to support high level video
indexing in movie databases. Human activities are
represented by combining multiple audio-visual
features. We hope that the proposed algorithm
detects human actions by detecting spatio-temporal
(time space and movement) phenomena which are
physically associated with a human action in nature.
The use of audio visual information and the adaptive
components (HMM) to learn the entire actions
represents on the important difference to related
works.

Figure 3. Some samples of collected data on motion
and audio
We will be testing our algorithm on action
movies. The original frames sequences captured
contain complicated background and the positions of
the actor/actresses moves during sequence. So,
human area extraction and tracking are needed. We
will be using the image pre-processing operations
described below:
a) preparing background
b) blurring the images with a low pass filter
c) extracting human area
d) binarized the extracted images so that the
white and black pixels corresponded to
background and human areas
We will be using precision and recall to measure
our recognition results, which are well known
metrics originally, defined in the information
retrieval literature. Precision measures the proportion
of correctly recognized actions, while recall

6. References
[1]
S. W. Smoliar and H. Zhang, “Contentbased Video Indexing and Retrieval”.
IEEE
Multimedia, pp.62 – 72. 1994.
[2]
W. Niblack, et al., “Query by Images and
Video Content: The QBIC System”. Computer, vol.
28 no. 9, pp. 23 – 32, 1995.
[3]
S. F. Chang, W. Chen and H.J. Meng, et al.,
“A Fully Automated Content-based Video Search
Engine Supporting Spatio-temporal Queries”, IEEE

245

Images using Hidden Markov Models”. Proceedings
of Computer Vision and Pattern Recognition, pp. 379
– 385, 1992.
[8]
K. Sato & J. K. Aggarwal, “Tracking and
Recognizing Two-Person Interactions in Outdoor
Image
Sequences”.
Proceedings of IEEE Workshop on Multi Object
Tracking, pp. 87 – 94, 2001.
[9]
S. Hongeng, F. Bremond and R. Nevatia,
“Representation and Optimal Recognition of Human
Activities”. IEEE Proceedings of Computer Vision
and Pattern Recognition, pp. 818 – 825, 2000.

Trans. Circuits System Video Technology, vol. 2, pp.
602 -615, 1998.
[4]
J. S. Boreczky and L.D. Wilcox, “A Hidden
Markov Model Framework for Video Segmentation
using Audio and Image Features”, in Proceedings of
the International Conference Acoustics, Speech,
Signal Processing, pp. 3741 – 3744, 1998.
[5]
D M Gavrila. “The Visual Analysis of
Human Movement: A Survey”, Computer Vision and
Image Understanding, vol. 3 no.1, pp.82 - 98, 1999.
[6]
S. Seitz and C.R. Dyer, “View
MorthiMorphing: Uniquely Predicting Scene
Appearance from Basis Images”. Proc. Image
Understanding Workshop, pp. 881 – 887, 1997.
[7]
J. Yamato, J. Ohya, and K.
Ishii,
“Recognizing Human Action in Time-Sequential

Detected Action

Training & Testing

GMM

HMM

Training & Testing

X

HMM
Classifie

Aud

X

X

HMM
Classifie

Col

246

X

HMM
Classifier

Mot

