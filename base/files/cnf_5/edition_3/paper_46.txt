Fifth International Conference on Computer Graphics, Imaging and Visualization
Visualisation

Visualization of Hybrid, N-body and Octree-based Adaptive Mesh Resolution
Parallelized Simulations
D. Pomar`ede, Y. Fidaali, R. Teyssier
Institut de Recherche sur les Lois Fondamentales de l’Univers
DSM/IRFU SEDI/LILAS and SAp, CEA Saclay, 91191 Gif-sur-Yvette, France
daniel.pomarede@cea.fr, yncia.fidaali@cea.fr, romain.teyssier@cea.fr
Abstract

high-levels of precision, the description of both scalar and
vector fields with their own challenges, and the parallelization of the simulations on thousands of processors.
We describe a software package developed to provide
an interactive visual access to hybrid simulations. As part
of the more general SDvision visualization tool, it relies
on the use of the IDL Object Graphics framework. It offers interactive ways to visualize and compare in the scene
objects associated with the adaptive mesh, the scalar and
vector fields described on this mesh, and the N-body system.

In this paper, we report on the development of a visualization software which adress the issue of the interactive visualization of three-dimensional hybrid simulations combining an N-body module and a grid-based solver
with an Octree-based adaptive mesh. The difficulty inherent to the hybrid nature of the data and the complexity of the mesh structure used to describe both scalar and
vector fields is enhanced by the fact that simulations are
parallelized. Large-scale simulations are conducted on
high-performance mainframes with potentially thousands
of processors associated with a non-trivial domain decomposition. The software we present is deployed in the
framework of IDL Object Graphics as part of the SDvision graphics tool used primarily in the visualization of astrophysical simulations. It combines in a single scene the
graphics objects of the N-body system and the grid-based
fields, together with a visual representation of the octree
mesh, suitable for interactive and immersive navigation.

2 Software environment
The SDvision visualization tool [1, 2] has been developed as part of the COAST project dedicated to Computational Astrophysics [3, 4]. The environment is the IDL
platform version 7.0 [5]. The code relies on the use of the
Object Graphics, a collection of pre-defined object classes,
each of which is designed to encapsulate a particular visual representation and supports OpenGL-based hardware
accelerated graphics. A detailed description of its software
implementation may be found in [1].

Keywords— Scientific visualization, Data visualization, Visualization frameworks and infrastructures, Visualization
tools and applications

3 Hardware environment
1 Introduction

As a consequence of the hybrid nature and the potentially large size of the data, several requirements are posed
to the hardware configuration where the application is to be
used. The need to handle large tri-dimensional grids makes
it necessary to have large memory available. Volume rendering based on ray-casting technique on such grids is optimized by the use of multiple cpu units. As a consequence
of these two requirements, mainboards with 4 to 8 cores
and large shared memory of 16 to 32 GB are used. The interactive visualization of iso-surfaces as Gouraud-shaded
polygon and the visualization of particles clouds relies
on graphics card with excellent capabilities and memory.
Given the high resolution reached by the simulation, largesize, high-resolution screens are favored. Finally, large and
efficient disk space is needed for fast data access. The table

With the potential offered by the new generations of
massively parallel mainframes, Computational Physics is
adressing problems of highly increasing complexity and
size. New generations of simulation programs are optimized to fit the resources offered by high-performance
computing centers in terms of available cpus, randomaccess memory and disk storage. Keeping pace in the posttreatment, visualization, and analysis stages is proving to
be a challenging task.
In this context, hybrid simulations, mixing together an
N-body system and finite-volume elements computations
are faced with the challenges of their interactive, combined visualization. The difficulty is exacerbated by the
use of meshes with adaptive resolution needed to reach

978-0-7695-3359-9/08 $25.00 © 2008 IEEE
DOI 10.1109/CGIV.2008.64

295

below summarizes the specifications of the graphics stations employed to run our software application :
hw component
mainboard :
Tyan
Thunder n3600M
video :
NVIDIA Quadro
FX5500
display :
Apple
Cinema HD
storage :
RAID5
Controller

description
2 dual core AMD
16 memory slots
32GB shared mem
1 GB memory
high-resolution
vertex & shader
model 3.0
30 inches
2560x1600 pixels
8x150GB Raptor
10000 rpm

percomputing Center as part of the Horizon Project [7].
The Cluster6 dataset is part of a 144-steps time sequence
used to create movies. The 100 Mpc Box and the Cluster6 data were produced at the CCRT Computing Center
at CEA.The hydro section of the data is made of six fields
: density, pressure, metals, and the three components of
velocity.
The AMR-Octree data structure is a fully threaded tree
for which cells are refined on a cell-by-cell basis in the
three directions, as illustrated in the following section. Refinement is triggered when density is above a threshold.
Domain decomposition is achieved through the use of a
space-filling curve algorithm which distribute the cells to
the available process. This results in a complex spatial distribution of the data versus the cpu number, that has to be
unpacked by the visualization program in a preprocessing
stage.
The basic elements of the AMR is an oct which, in three
dimensions, at level L, is the father of 8 siblings at level
L+1. This mechanism results in potentially large memory reduction in comparison to a standard Cartesian grid,
which at level L of refinement, has (2L−1 )3 octs. This illustrated in the Figure below for the three data sample and
a Cartesian grid, where are shown the number of octs per
level of refinement :

benefit
parallel volume
rendering and
large grids handling
interactivity
Shaded polygons
N-body systems
Large Mesh display
high-quality
fine details for
high-resolution
fast access
large space

While the use of IDL as software framework guarantees a cross-platform support, the operating system used
to operate these stations is Linux. With such a configuration, very large datasets made of tables of 12003 floats
amounting to ∼ 6.9 GB of memory each can be handled
confortably, and visualization performed interactively.

4 Description of the simulations
Hybrid simulations are performed using the RAMSES
code developed to study cosmological large scale structures and galaxy formation [6]. This code relies on
an AMR “Adaptive Mesh Refinement” module with grid
refinement, parallel communications and time stepping,
an hydrodynamics module with a second-order Godunov
solver, an N-body module for collisionless particles dynamic used to simulate the Dark Matter component, and a
Poisson module with a multigrid solver for the coarse grid
and a Conjugate-Gradient solver on the fine grids.
To illustrate the potential of our application, we employ
three different data samples with distinct characteristics, as
described in the table below :
name
# of cpus
data size
Total
AMR
Hydro
particles
AMR Levels
# of octs
# of particles

100 Mpc Box
32

Cluster6
16

MareNostrum
2048

430 MB
73 MB
292 MB
65 MB
13
1 590 450
2 097 152

638 MB
108 MB
359 MB
171 MB
16
2 346 952
5 582 242

131 GB
20 GB
78 GB
33 GB
14
435 602 390
1 073 741 824

The efficiency of the AMR mechanism in terms of
memory usage is often characterized by the filling factor
which is equal to the ratio of the integrals of the simulation
curves and the Cartesian grid curve. It has to be stressed
that the use of the AMR makes it possible for the simulation to reach high level of precision that can not be reached
with a Cartesian grid. For instance, at level 14, a single
Cartesian grid of floats would require 17.6 TB of memory,
to be compared with the data sizes given in the table above.

5 Visualization of the mesh
The mesh of the Octree-AMR has a complex threedimensional structure, especially in the context of cosmological simulations where regions devoid of any matter
(hence with low resolution, coarse-grained mesh) and very
dense halos (hence with high resolution,fine-grained mesh)

The 100 Mpc Box and the MareNostrum samples are
simulations of large scale structures formations, while the
Cluster6 sample is a zoom on a galaxy cluster. The
MareNostrum data were obtained at the Barcelona Su-

296

are found nearby. This structure is particularly interesting
to visualize in order to identify regions where high precision has been reached by the simulation and check whether
the AMR fields are refined accordingly.
The visualization of the mesh is achieved by the threedimensional display of the octs using polylines [8]. In Figure 1 are shown examples of mesh visualization. The SDvision AMR interface allows to give different colors and
transparencies to the octs versus the AMR level. Slices of
the AMR mesh can also be displayed in the scene. In this
implementation, meshes with millions of octs can be safely
handled with the hardware configuration described in section 3. For what regards larger simulations with on the
order of a billion of octs such as the MareNostrum dataset,
it is necessary to restrict the visualization to regions that
can be interactively set through the AMR interface. This
mechanism, which amounts to proceeding to synchroneous
spatial and resolution zooms, is described in section 9.

ble in section 4), the Hydro AMR octs that lies below Level
9, the resulting Cartesian grid, and the graphics objects.
Alternatively, scalar fields are visualized by isosurfaces,
the value of which is set interactively. Gouraud-shading of
the polygons benefits from the performances of the highrange graphics card and the transformation of the resulting scene is fully interactive. An example of isosurface is
shown in Figure 3. Various sources of illumination can be
switched on and off to highlight some features in the surface topology.
Finally, slices of the data are visualized as image. The
volume can thus be scanned by moving the slice position
interactively and the color palette can be adjusted at will.
The image is a texture-mapped polygon subject to the 3D
model transform and the hardware acceleration. An example of image in shown in Figure 3, as obtained from the
MareNostrum dataset by processing the AMR-Octree up
to level 10. The resulting 1024x1024 image reveals finelyresolved details.

6 Visualization of the AMR Scalar Fields

7 Visualization of the AMR Vector Fields

The visualization of AMR-Octree scalar fields is
achieved by the mean of three different representations [1] :
volume rendering by raycasting, isosurface displayed as
shaded polygons, and slices displayed as image transformed in the current 3D model. In the current implementation of SDvision, these three representations have as input a 3D Cartesian grid. Hence the visualization has to follow a preprocessing stage whereby the AMR hydro structure is projected onto a regular grid up to a maximum level
set by the user, compatible with the available random access memory. In this process, the benefit of low-memory
consumption that characterize the AMR is lost. However,
the highest levels of the AMR resolution can still be visualized by interactively proceeding to synchroneous spatial
and resolution zooms (see section 9).
Volume rendering can be achieved by either a maximum
intensity projection or by alpha-blending. A dedicated interface is used to interactively define the red, green, blue
tables of the color palette, and the opacity lookup table.
This visualization is accelerated by parallelization on the
four cores of the graphics station. In Figure 2 are shown
examples of volume rendering obtained for the MareNostrum and Cluster6 datasets by the maximum intensity projection method.
The visualization of the AMR-Octree fields in the
MareNostrum dataset is challenging and highlights the
need to use a graphics station with large amount of shared
memory. To obtain the visualization shown in Figure 2,
left, the density field is read from the 2048 processor outputs and projected onto a 5123 Cartesian grid (Level 9 of
the AMR). The AMR processing can be achieved confortably thanks to the 32 GB RAM of the graphics station, necessary to handle the 20 GB size of the AMR Mesh (see Ta-

The velocity vector field is visualized by either a collection of streamlines or a collection of three-dimensional
arrows. The three components of the field can also be visualized individually as scalar field using the infrastructure
described in the previous section. Streamlines are obtained
by tracing the path of a massless particle through the field
and visualized using a polyline object. As for the scalar
field, the AMR structure is processed in order to build a
Cartesian grid. The input seed point are picked up interactively in the scene or distributed on a 2D or 3D grid, as
seen in Figure 4.

8 Visualization of the N-body component
The N-body module of the hybrid simulation is used to
simulate dark matter particles subject to gravitation. The
particles are either visualized using a cloud of points or
three-dimensional orbs. For what regards the cloud of
points, a preprocessing stage is necessary to compute the
local density around each particle. The particles are then
sorted by increasing density and stored in this order in a
polyline object. To obtain a view where the particles of
highest density dominate the scene, the depth testing in the
Z-buffer is disabled and a color palette together with an
opacity lookup table are mapped to the density distribution. An example is shown in Figure 5 where dense spots,
associated with the formation of galaxies, can be clearly
pinpointed. This implementation can withstand several
millions of particles such as for the 100 Mpc Box and
Cluster6 datasets. The visualization of particles as threedimensional Gouraud-shaded orbs is performed with depth
testing enabled so that these objects may be compared with
other objects of the simulation in terms of position in the Z-

297

Figure 1: Display of the AMR Mesh. Left : wide-angle perspective view of the octs from level 11 to level 16 of the Cluster6
dataset. Different colors are used to separate the levels and the transparency of the polylines is tuned to favor the highest,
less populated levels. Right : isometric projection of the mesh at levels 8 and 9 of the 100 Mpc Box dataset.

Figure 2: Volume rendering of AMR Scalar Fields. Left : density field in the MareNostrum dataset. The scene shows the
full volume built upon the 2048 processor outputs, with the AMR-Octree structure processed up to level 9. It reveals the
distribution of galaxy clusters interconnected by a network of dense filaments. Right : density field in the Cluster6 dataset.
The scene is restricted to a subregion with 20% of the total size around the volume center, where the AMR-Octree hydro
structure is processed up to level 11.

298

Figure 3: Visualization of AMR Scalar Fields. Left : density isosurface in the 100 Mpc Box datataset. Right : image slice
in the MareNostrum sample.
buffer. Only a few thousands of particles can be displayed
in this way in order to preserve interactivity in the model
transform.

volved. By combining in a single scene the multiple objects associated to the AMR octree scalar and vector field,
the AMR Mesh, and the N-body module, all the components of the simulation can be compared visually and interactively. This is illustrated in Figure 7 for the case of
the 100 Mpc Box dataset. From such visual comparison,
the coherency between the distribution of each fields can
be controlled. For example, it is observed that the group
of galaxies is created within a very dense dark matter halo,
where the AMR mesh refinement has been pushed to its
maximum level. Also, the various visual representations
of a single field can be combined to improve the knowledge of its spatial distribution, as can be seen for the case
of the density scalar field, visualized by the three following concurrent ways : volume rendering, isosurfaces, and
image slices.

9 Spatial and resolution zooms
The treatment of the AMR-Octree fields suffers from
the processing to a Cartesian grid with potentially huge
memory overheads. The handling of a large number of particles in the N-body module of the simulations can also be
challenging. In order to cope with these limitations and
provide a visual insight into the highest levels of precisions offered by the AMR structure, an interactive facility
is provided to define the bounds of a subregion into which
the AMR processing to higher levels will be restricted [8].
To deal with the huge size of the AMR data in the largest
simulation (e.g. the MareNostrum dataset), a search algorithm is invoked that scan the space filling curve and
finds those cpus that have contrinuted to the subregion.
Thus, by proceeding to synchroneous spatial and resolution
zooms, the entire content of the simulation can be visualized interactively in a single session up from large-scale
low-resolution structures down to local high-precision details. This is illustrated in Figure 6 in the case of the 100
Mpc Box dataset.

11 Prospects
11.1

Direct AMR-Octree Rendering

In the current implementation, the AMR-Octree is processed to a Cartesian Grid, the size of which is limited by
the available memory. In this process, the advantage of
low memory usage inherent to the AMR mechanism is lost.
Perspectives of developments of new algorithms providing
a direct visualization of AMR-Octree data are under study.
The visualization of another class of adaptive mesh refinement structure referred to as patch-based AMR, where

10 Combined Visualization
The main challenge in the visualization of hybrid simulations lies in the different nature of the various fields in-

299

Figure 4: Visualization of AMR Vector Fields. Streamlines of velocity in the 100Mpc Box dataset are seeded on a regular
163 grid. The opacity of the polyline is set at 44% and a maximum number of 80 iterations is required in the process of
computing the path.

300

Figure 5: Left : rendering of dark matter as a cloud of ∼ 2 million particles for the 100 Mpc Box sample . Right : rendering
of dark matter as Gouraud-shaded three-dimensional orbs for the Cluster6 data. Only a subsample of ∼ 5000 particles is
displayed.
the domain is covered by subgrids with increasing resolution, is widely studied, see e.g. [9, 10]. Several graphics
package, such as Visit [11] and Paraview [12], already propose facilities for the visualization of this class of AMR
data. However, the direct visualization of the AMR-Octree
structure remains largely unexplored. Possible areas of investigation include direct isosurface extraction using a new
AMR-Octree data format where the informations relative
to the identification of each oct’s parent and siblings are
available. In this project, the main difficulty lies in the
treatment of cracks found at the interface between regions
of different resolution [13]

11.2

ters is a challenging task. As a contribution to this area of
research, we have reported on our experience in the visualization of hybrid simulations that combines an N-body
module and an AMR-Octree infrastructure. Using highrange hardware component and a versatile software suite,
the various components of the simulation are visualized interactively on a desktop graphics station.

Acknowledgements
The authors would like to thank P.-F. Honor´e of the
LILAS laboratory for technical support and members of
the COAST Project for providing materials useful to the
development of this software. We would like to acknowledge useful discussions, insights, perspective of developments and support received from ITTvis France.

Implementation of GLSL shaders

Beginning with version 6.4, a shader functionality has
been implemented in IDL Object Graphics that provides
access to the advantages of the hardware-based OpenGL
Shading Language (GLSL) features available on modern
graphics cards. The development of new visualization facilities using this infrastructure is under study : this includes a new shading algorithm for the isosurface rendering, and a new volume rendering based on interactive
blending of multiple textures.

References
[1] D. Pomar`ede, E. Audit, A.S. Brun, V. Gautard, F.
Masset, R. Teyssier, B. Thooris : “Visualization of astrophysical simulations using IDL Object Graphics”,
Proceedings of the Computer Graphics Imaging and
Visualization 2007 Conference, CGIV07, Bangkok,
Thailand, August 14-17, 2007, ed. E. Banissi, M. Sarfraz, and N. Dejdumrong, IEEE Computer Society,
ISBN 0-7695-2928-3, p. 471-480.

12 Conclusions
The visualization of complex simulations produced by
the new generation of high-performance computing cen-

301

Figure 6: Illustration of the synchroneous spatial and resolution zooms mechanism. The AMR octree of the 100 Mpc Box
simulation is processed to higher levels in regions of decreasing extents, as follows, from top to bottom, left to right : 1)
visualization of the large-scale structures (halos and filaments), the AMR is processed up to level 9 in a 5123 grid. A
subregion of 20% of the total size in each direction is selected for the next step. 2) the AMR is processed up to level 11
in a 4093 grid and a spatial zoom is performed. The resolution is four times better in each direction than at the previous
stage, revealing finer details. 3) a new, smaller subvolume of 5% of the total size in each direction is centered on a group of
galaxies. 4) the AMR is processed up to level 13 in a 4093 grid and a spatial zoom is performed. The maximum resolution
is thus reached. 5) and 6) navigation around the group of galaxies. The inprint of the AMR structure is clearly visible in
the Cartesian grid, with coarse-grained structures in low-density regions found next to fine-grained details in high-density
sectors.

302

Figure 7: Combined visualization of the Hybrid simulation. The 100 Mpc Box dataset is used and the AMR is processed
up to level 13 in a subvolume of size 5% of the total size, centered on a group of galaxies. The scene is visualized from a
fixed viewpoint with the following objects, from top to bottom, left to right : 1) volume rendering by maximum intensity
projection, 2) an isosurface of high density is added, 3) the AMR octs at level 13 are displayed, 4) the AMR octs at level
12 are displayed, 5) slice of the density, clearly exhibiting five levels of refinement, 6) addition of the dark matter particles
rendered as orbs (∼ 18000 particles in the subvolume), 7) density isosurface and particles 8) dark matter halo considered
alone 9) slice of density, isosurface, and streamlines of the velocity seeded on the slice.

303

[2] D. Pomar`ede, Y. Fidaali, E. Audit, A.S. Brun, F. Masset, R. Teyssier : “Interactive visualization of astrophysical plasma simulations with SDvision”, Proceedings of the IGPP/DAPNIA International Conference on Numerical Modeling of Space Plasma Flows
ASTRONUM2007, Paris, France, 11-15 June 2007,
ed. N.V. Pogorelov, E. Audit, and G.P. Zank, to appear in the Astronomical Society of the Pacific Conference Series, vol. 385 (2008), P.327-332, ISBN:
978-1-58381-333-1.

[7] http://www.projet-horizon.fr/
[8] Y. Fidaali : “Visualisation du maillage adaptatif du
code RAMSES”, Rapport de Stage, Master 1, Universit´e Paris EST 77, CEA/DAPNIA Saclay (2007).
[9] R. Kahler, “Accelerated Volume Rendering on Structured Adaptive Meshes”, Ph.D. Thesis, Fachbereich
Mathematik u. Informatik, Freie Universitat Berlin,
2005.

[3] E. Audit, D. Pomar`ede, R. Teyssier, B. Thooris :
“Numerical Simulations of Astrophysical Plasmas:
status and perspectives of the Saclay/DAPNIA software project”, Proceedings of the 1st IGPP-CalSpace
International Conference on Numerical Modeling
of Space Plasma Flows ASTRONUM2006, Palm
Springs, California, USA, 26-30 March 2006, ed.
N.V. Pogorelov and G.P. Zank, Astronomical Society of the Pacific Conference Series, vol. 359 (2006),
p. 9-14, ISBN: 978-1-583812-27-3.

[10] G. Weber :
“Visualization Tools for Adaptive Mesh Refinement Data”, Proceedings of the
IGPP/DAPNIA International Conference on Numerical Modeling of Space Plasma Flows ASTRONUM2007, Paris, France, 11-15 June 2007, ed.
N.V. Pogorelov, E. Audit, and G.P. Zank, to appear in
the Astronomical Society of the Pacific Conference
Series, vol. 385 (2008), ISBN: 978-1-58381-333-1.
[11] http://www.llnl.gov/visit/

[4] http://irfu.cea.fr/Projets/COAST/
[12] http://www.paraview.org/

[5] http://www.ittvis.com/idl/

[13] R. Westermann, L. Kobbelt, T. Ertl : “Real-time Exploration of Regular Volume Data by Adaptive Reconstruction of Iso-Surfaces”, The Visual Computer,
15 (1999) 100-111.

[6] R. Teyssier : “Cosmological hydrodynamics with
adaptive mesh refinement - A new high resolution
code called RAMSES”, Astronomy & Astrophysics,
385 (2002) 337-364.

304

