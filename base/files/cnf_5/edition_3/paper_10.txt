Fifth International Conference on Computer Graphics, Imaging and Visualization
Visualisation

User Experience Using Motion Capture:
Simulation Of Human Motion For Multimedia Applications

Jong Sze Joon*, Harold Thwaites * and Khong Chee Weng*
*Faculty of Creative Multimedia, Multimedia University, 63100 Cyberjaya, Selangor, Malaysia
{sjjong@mmu.edu.my, harold.thwaites@mmu.edu.my, cwkhong@mmu.edu.my}

Abstract
This paper presents a motion-capture-based control
framework by simulating three core motions namely
walking, jumping and running to enhance user
experience for Multimedia applications. Using
MWalker V1 application, a user can directly control
the full body motion of an avatar in virtual
environments with different physical attributes for
various multimedia applications. However, there are
still a few fundamental problems. This research
outlines how different physique produces different
behavioural patterns based upon mass and
proportion. This research considers ‘motion’ to
identify the differences in each subject’s physical
attributes by sampling subjects of physical differences.
The purpose of the research is to study the detail of
motion of various subjects with differences in physical
attributes, for simulation of human motion for
multimedia applications.

Figure 1 Motion capture of simulated user
walking, jumping and running
As shown in the Figure 1, the experiment utilises
Vicon8i® Optical Mocap system to study the detail of
human motion by extracting the subjects’ core motions
for analysis. Sampling various subjects of physical
differences, this research concentrates on acquiring
various motion parameters based on certain predefined
actions. In addition, the research also applies motion
editing techniques by using Motion Builder2008® to
retarget and constraint the captured data.
The
experiment findings of this research will be formulated
to show the relationship between height and weight
against motion frequencies in 3D space.
The research considers that biological human motion
varies depending on environmental physical and
psychological influences. Regardless, the subjects’ core
motions are still limited to a certain physical patterns.
The outcome of the research serves as a guideline to
understand the basic motion flow of looping motions.
The research contributes to set a basis for researchers to
reference the analysis of data sets based on subjects’
variables.

Keywords: human motion, actor physique, motion
capture, motion editing, multimedia applications

1. Introduction
Motion Capture or MOCAP has been widely used
for the past seven years especially in entertainment
industries such as games and film productions. Even
though the technology and content development based
on MOCAP is still considered at developing stage
compare to key-frame animation, many researchers
have shown a great interest towards finding ways to
improve and seek for the full potential of the tool.

978-0-7695-3359-9/08 $25.00 © 2008 IEEE
DOI 10.1109/CGIV.2008.27

69

2. Previous studies on human motion
analysis

Several techniques have been proposed for reusing
or altering existing motions. Both Witkin et al. (1995)
[6] research on motion warping and Bruderlin et al.
(1995) [7] who study on motion displacement mapping
discussed motion-editing technique based on direct
manipulation of data curves. Bruderlin et al. (1995) [7]
and Unuma et. al. (1995) [8] utilised signal processing
techniques for motion editing. Wiley et al (1997) [9]
proposed the interpolation synthesis algorithm that
chooses and combines most relevant motions from the
database to produce animation with a specific positional
goal.
In addition, Boulic and Thalmann (1992) [10]
presented the combined direct and inverse kinematic
control technique for motion editing. The concept
called coach-trainee metaphor is very similar to the
motion retargeting problem formulation. The
fundamental idea is to consider the joint motion of
coach as a reference input to trainee motion for the
secondary task exploiting the null space of the Jacobian
when solving inverse kinematics. A method which is
devoted to the motion retargeting problem was
recommended by Gleicher (1998)[11]. He used the
spacetime constraint method that minimises an
objective function subject to the constrains of the form.
The constraints can represent the ranges of parameters,
or various kinds of spatial-temporal relationship among
the body segments and the environment. The objective
function is the time integral of the signal displacement
between the source and destination motion. Gleicher
(1998) [11] concluded that even when two articulated
figures share structure, the motion of one may not
trivially apply to the other and therefore require
adaptation. In general, mapping and simulation based
approaches to animation offer representations
independent of the character and therefore may be used
to generate new motions for new characters. Many of
the mapping and simulation controllers are able to
adjust to different characters easily.

Several studies reported that motion capture
systems involves the process of recording a live motion
event and translating it into digital transformation
values are now being widely used in many applications
such as medicine, sports, entertainment industry, and in
the study of human factors. Since, there has been
remarkable work in these areas toward achieving
realistic data regarding human motion, stability, and the
way human interact with their environment.
Human visual system is very sensitive to the
detection of animated motion patterns [1]. We can
efficiently detect another living being in a visual scene,
recognise human action patterns and attribute many
features of psychological, biological and social
relevance to other persons. Pullen (2002) claimed that
when digitally animating a walk-cycle or any loop
sequence, the playback of the sequence of motion
would always be the same [2]. This does not apply to
realistic motion due to the fluctuation of each specific
movement. In reality, no one can perform the exact
same movement twice in space and time. Human gait
systems are unique in the sense that every action differs
due to the nature of the anatomy structure (nonmechanical movements).
The human visual system is so sensitive to human
motion patterns that in a visual scene, one can quickly
and efficiently identifies human behaviours and study
many aspects of human motion. As mentioned by
Zhang and Troje (2005), what our visual system seems
to have achieved effortlessly so far is still a challenging
problem for artificial vision systems, although the
computerised analysis of human motion is gaining more
and more interest [3].
Researchers have used many ways to study motion.
One of the earlier methods is by using light-dot
displays, similar to markers in an optical Mocap
system, to study the perception of human movements.
Hodgins et.al. (1998) [4] found that this would be
easier to enable people to study the details of common
motion like walking, running and jumping. Johansson
(1973) added that one is able to visualise these
movements based on the least of ten to twelve lightdots that outlines the proportion of the subject [5].
However, by understanding the essence of human
motion pattern, we can better utilize Mocap data
acquired from a particular subject to study various
forms of physical motion characteristics, each
portraying the nuances caused by the volume
movement of mass and proportion. This includes
identifying the centre of gravity of the subject in
motion, traced by the line of action that flows in
distinctive motion paths.

2.1 Motion editing
captured data

and

readaptation

3.
Research methodology
processing MOCAP Data

and

Post

Based on the study conducted in the previous sub
chapter on the elements of animation, the core motion
studies of the subjects will be analysed in this section.
As shown in Figure 2 and 3, the core motion is an
extract from the sequence of the cycle or loop motions.
This means that the walk, run and jump will be broken
down into each step within the subjects’ motion. This
will enable more specific measurements to be taken.
The core motion studies will be categorized into
two distinctive experimental type listed below:

of

70

Start End
Physique
Frame Frame

Conclusive Experimentation
x
x

Distance of steps
Steps Frequency (cycles)

Step
Dur
Dist per Dist per Freq per
per
step (cm) sec (cm) frame
step
(Hz)

Fat

81

104

23

253.01

275.01

0.0435

Thin

81

102

21

216.31

257.51

0.0476

Conditional Experimentation

Tall

81

101

20

307.97

384.96

0.0500

x
x

Short

81

100

19

215.03

282.93

0.0526

Shoulder and hip balance (centre of gravity)
Cycle patterns (arcs)

JUMP MOTION
Physique
Fat

Figure 2 Distance of steps within motion
sequence

Start End
Frame Frame
50

82

Step
Dur
Dist per Dist per Freq per
per
step (cm) sec (cm) frame
step
(Hz)
32

141.83

110.80

0.0313

Thin

50

82

32

166.04

129.72

0.0313

Tall

50

81

31

176.05

141.98

0.0323

Short

50

77

27

138.92

128.63

0.0370

Figure 3 Motion graph showing the key frames
of the motion sequence
Chart 1 Distance of step per frame for walk
motion

The following results were obtained from the
experiment.

3.1 Outcomes and results
With reference to the analysis, the following results
were obtained from the experiment.
Table 1 Experiment results
WALK MOTION
Physique
Fat

Start End
Frame Frame
101

130

Step
Dur
Dist per Dist per Freq per
per
step (cm) sec (cm) frame
step
(Hz)
29

155.49

134.04

0.0345

Thin

101

128

27

153.00

141.67

0.0370

Tall

101

128

27

157.44

145.78

0.0370

Short

101

125

24

129.11

134.49

0.0417

Chart 2 Distance of step per frame for run
motion
According to Chart 1and 2, based on the combined values of
each individual action (walk, run, jump), the average
frequency rate per frame and the average distance per step

RUN MOTION

71

were calculated
independently.

to

generate

the

standard

accuracy)

deviation

Table 2 The mean for frequency
per frame and distance per step

When height increase, distance of steps
increase
(high accuracy, low precision)

Mean
Mean
Physique Weight (kg) Height (cm) frequency distance per
per frame
step
Fat
110
181
0.0364
183.4433
Thin
55
173
0.0386
178.4500
Tall
85
199
0.0398
213.8200
Short
57
158
0.0438
161.0200

Chart 3 and Chart 4 outline the result of the
predefined equations. The standard deviation ‘plus and
minus’ markers of the Physique Error Bars have the
variance of the multiplication of 4.

As shown in Table 1 and 2, the standard deviation mean
frequency derived from the sampled values is 0.0036. The
standard deviation mean per step is 12.3249.
The following charts outline the result of the predefined
equations. The standard deviation ‘plus and minus’ markers
of the Physique Error Bars have the variance of the
multiplication of 4.

Figure 4 Comparison of walk motion of all
subjects. Duration of frame is included
As shown in Figure 4 above, the experiment
conducted focuses on the subjects’ core motion
parameters, which was extracted from the multiple sets
of looping motion. Samples of the core motions were
blended to acquire the average data sets for
measurement. New data was measured and a
classification framework was constructed to document
the results. The results fulfil the research questions
independently by outlining different approaches to
achieve the desired outcomes. With reference to the
experiment results, certain interpretation can be made
based on the relationship between subjects’ motion to
subjects’ height and weight, in terms of time and space.

Chart 3 Weight in frequency space graph. To
show how weight changes influence the step
frequency per frame

Firstly, the experiment results show that the
duration per step is influenced by the subject’s height
and weight to a certain extent. Though minimal, the
motion data of the sequence of each action; walk, run
and jump, shows that the duration per step of the thin
subject is less than the fat subject, and that the short
subject is less than the tall subject. As shown in Chart
3 and Chart 4, all values of the standard deviations
within the multitude of the four samples contained by

Chart 4 Height in distance space graph. To
show how height changes influence the step
distance per frame

When weight increase, steps frequency do not
necessary increase (high precision, low

72

development and creation of the MWalker V.1
application.
As shown in Figure 6 below, this
application enables real-time interpolation between
motions. This application used the findings from the
experiment to establish the relationship between height
and weight against motion frequencies in 3D space
based on different gender. Hence, users can retrieve
the motion and visualise the motion patterns based on
the selected criteria for various multimedia
applications.

the ‘Y-axis’ error bars (physique). Consequently, the
mean samples proven the following variables:
x

When height increase, distance of steps
increase (high accuracy, low precision)

but,
x When weight increase, steps frequency do
not necessary increase (high precision, low
accuracy)
According to Troje (2003) [12], the perceived size
in fact depends on the step frequency: The larger the
step frequency the smaller appears the subject. Instead
of referencing the size of the subject, this research
focuses on weight and height of the subject. Based on
the statistics of the analytical data, it shows that the
change in the sampled subjects’ parameters only
partially affects the change of motion parameters. The
changes are only evident in the height but not the
weight.

4. Simulation Of Walking, Running And
Jumping Motion

Figure 6 The GUI of MWalker V.1 application
Figure 6 shows the user interface of MWalker V.1
application, which serves several functions. Firstly, it
can be used as library archives of motion sets.
Secondly, it can be a testing application for motion
research. Thirdly, it can be used to study human
motion based on emotional and behavioural patterns of
different genders with different physical attributes.
The MWalker V.1 application enables real-time
interpolation between motions. This provides the user
an interactive control to blend multiple motion
parameters to create new sets of motion characteristics.
The data are subsequently transformed into a
representation, which allows for linear morphing. The
resulting ‘walking space’ is then transformed using
principal component analysis. Sex and weight of each
walking are directly available from our records. The
other two attributes are derived from psychophysical
experiments. A number of users acted as observers are
presented with point-light displays of participants. For
each of them they have to rate the attributes such as
emotions (nervous or relax, happy or sad) on a scale of
several steps. The procedure will be recorded and
described further in the future.

Figure 5 The BML Walker application (Courtesy
of Troje [12], 2003)
With reference to Troje’s [12] BML Walker application
as in figure above , the proposed animation prototype
MWalker V.1 application for this research demonstrates
a framework for retrieving and visualising biologically
relevant information from biologically motion patterns
for multimedia applications. It is based on walking,
running and jumping data based on different physical
attributes of actor physique of different gender. This
research involves on exploring how information is
encoded in animated motion patterns and how this
information can be retrieved. This application can be
used to find out which parts of the information can be
obtained by the human visual system, and therefore, are
behaviourally relevant. Resulting in this research is the

73

Computer Graphics and Interactive Techniques ACM
SIGGRAPH ’98, pp. 33– 42.
[12] Troje, N. F. 2003. Cat walk and western hero –motion is
expressive. Cognitive Neuroscience IGSN-Report. Bochum,
USA: Ruhr University, The BioMotion Lab.

5. Conclusion
In a nutshell, the outcome of this research serves as
a guideline to understand the basic motion flow of
looping motions. This research contributes to set a
basis for researchers to reference the analysis of data
sets based on the subject variables.
Another approach to continue the research can be
to narrow down the portions of the core motion studies.
This research studies full body motion. Future research
might focus on only one predefined locomotion or
articulation type, limited to selected joints.
It has analysed the data sets based on the subject
variables that can be used for considering motion
preparation for animation, games and other related
industries. Further experiments will be presented later
with a few other motion actions.

References
[1] Jokisch, D. and Troje, N.F. 2003. Biological motion as a
cue for the perception of size. Journal of Vision, 3(4), 252264.
[2] Pullen, K. A., Bregler C. 2002. Motion Capture Assisted
Animation: Texturing and Synthesis. [Online]. Available:
http://graphics.stanford.edu/~pullen/papers375.pdf
[2006,
December 23].

[3]Zhang, Z., and Troje, N. F. (2005) Viewindependent person identification from human gait.
Journal of Neurocomputing, 69, 250-256.
[4] Hodgins, J., O'Brien, J., and Tumblin, J. 1998. Perception
of human motion with different geometric models.
Proceedings of IEEE Transactions on Visualization and
Computer Graphics, 4 (4), 101-113.

[5]Johansson, G. (1973). Visual perception of
biological motion and a model for its analysis. Journal
of Perception and Psychophysics, 14(2), 201 –211.
[6] Witkin, A., and Popovic Z. 1995. Motion warping. In R.
Cook (Ed.), Computer Graphics, Proceedings of ACM
SIGGRAPH ‘95, Annual Conference Series, pp. 105–108.
[7] Bruderlin and Williams, L. 1995. Motion signal
processing. Proceedings of ACM SIGGRAPH ‘95, Annual
Conference Series, pp. 97–104.
[8] Unuma, M., Anjyo, K., Takeuchi, R. 1995. Fourier
principles for emotion-based human figure animation.
Proceedings of ACM SIGGRAPH ’95, 4 (8), pp. 91-96.
[9] Wiley, D. J. and Hahn, J. K. (1997). Interpolation
synthesis of articulated figure motion. IEEE Computer
Graphics and Applications, 14 (6), 39–45.
[10] Boulic, R. and Thalmann, D. 1992. Combined direct and
inverse kinematic control for articulated figure motion
editing. Computer Graphics Forum, 11(4), 189–202.
[11] Gleicher, M. 1998. Retargeting motion to new
characters. Proceedings of the 25th Annual Conference on

74

