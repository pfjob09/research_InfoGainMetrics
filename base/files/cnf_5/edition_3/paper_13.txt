Fifth International Conference on Computer Graphics, Imaging and Visualization
Visualisation

Development of a Vision System for a Floor Marking Mobile Robot
Zulkifli Zainal Abidin, Member, IEEE, Syamsul Bahrin Abdul Hamid, Member, IEEE,
Ahmad Anis Abdul Aziz, and Azlan Ab. Malek
traditional method of floor marking [1].
Another problem present in the traditional method of
floor marking is that the shapes of the booths are often
restricted to the mundane rectangular shape. By using the
autonomous robot, attractive shapes such as ellipse,
circular, and other non-rectangular shapes. By designing
the autonomous robot, we also will be able to improve
working conditions because the traditional method entails
great physical strain, especially to the knees, as workers
have to bend down to mark the floor [1].
This project uses a mobile robot to automate the process
of measuring and marking the exhibition floor space to the
correct predetermined size using the help of video cameras
overlooking the exhibition floor. The robot will use a
digital map of the booths arrangement as a guide to mark
the exhibition floor with the help of video cameras to
determine its position.

Abstract— participants of an exhibition are usually
provided with booths to display their work. Workers usually
consturuct these booths by measuring the exhibition floor by
hand and marking the floor to determine the size of the
booths. This project aims to develop a system that uses video
cameras overlooking the exhibition floor to automate the
process of measuring and marking the exhibition floor space
to the correct predetermined size, i.e. the robot will use
machine vision to determine its position and to control its
motion in marking the floor according to a predetermined
floor plan.

M

I. INTRODUCTION

OST universities encourages its staff and students to
exhibit their research work in exhibitions and fairs.
Because of this, exhibitions and trade fairs are held
frequently around the country which is joined by both local
and international participants. Usually, during an
exhibition, participants will be allocated booths where their
work will be displayed. To construct these booths,
organizers usually measure and mark the exhibition floor
manually to determine the correct size of the booths.
This project seeks to develop an autonomous mobile
robot to perform the floor marking process, i.e. to automate
the floor marking process. The traditional method of floor
marking is tedious and time consuming, and with
exhibitions and fairs being held frequently, such jobs
should be managed and executed more efficiently. By
doing so, more time could be spent on other tasks; this will
lead to greater productivity.
To accomplish this, the problem is addressed with a
Mechatronics engineering approach. The robot for the floor
marking process will be designed to work autonomously,
hence, requiring minimal supervision. Furthermore, the
autonomous mobile robot will be designed to be able to
perform the floor marking process much quicker than the

II. DESCRIPTION OF THE SYSTEM’S SETUP

Fig. 1 Setup of the Experiment

The aim of this study is to develop an autonomous
mobile robot that can be used to mark the floors in an
exhibition hall. Fig. 1 illustrates the schematic diagram of
the autonomous mobile robot for floor marking developed
in this study. A map of the desired floor marking is drawn
and saved in a digital image format. The map is then
overlaid on the image captured by the fixed camera that is
mounted on top of the area to be marked [5]. The path of
the mobile robot is then calculated and planned [3]. To
determine the position of the mobile robot, three white
round shape markers are placed on top of the robot. With

Manuscript received April 1, 2008.
Zulkifli Zainal Abidin is with the Mechatronics Engineering
Department, International Islamic University Malaysia (e-mail:
zzulkifli@iiu.edu.my).
Syamsul Bahrin Abdul Hamid is with the Mechatronics Engineering
Department, International Islamic University Malaysia (e-mail:
syamsul_bahrin@iiu.edu.my).
Ahmad Anis Abdul Aziz is an undergraduate at the Mechatronics
Engineering Department, International Islamic University Malaysia (email: ahmadanis85@gmail.com).
Azlan Ab. Malek is an undergraduate at the Mechatronics Engineering
Department, International Islamic University Malaysia (e-mail:
azlanmalek@gmail.com).

978-0-7695-3359-9/08 $25.00 © 2008 IEEE
DOI 10.1109/CGIV.2008.69

88

these markers, the position and heading of the robot can be
calculated.
The autonomous mobile robot is designed to operate in
large halls using the existing CCTV system available in the
halls [4]. However, in the development stage, a controlled
test environment is set up. Fig. 1 illustrates the setup of the
experiment of the autonomous mobile robot for floor
marking. A personal computer is used to draw the layout of
where the markings should be made. It is also used to
process the image captured from the CCD camera that is
mounted above the area of interest. Before the image from
the camera can be used to determine the position and
heading of the mobile robot, the CCD camera needs to be
calibrated. To achieve this, a sheet of paper with known
size squares is placed on the floor. After calibration, the
ratio of millimeter/pixel is obtained [10]. Based on the
image captured, the position and heading of the mobile
robot can be determined and hence the speed of the left and
right motors calculated to steer the mobile robot in the right
direction.

III. CONTROL OF THE MOBILE ROBOT
A. Robot Recognition
The robot will be recognized by the vision system by
using a predetermined marker placed on top of the robot.
The robot’s marker is designed to be three circles placed at
the vertices of an isosceles triangle [9]. The marker is as
illustrated in Fig. 4.

Fig. 4 Circular robot marker

The markers on the robot are circular, which is a special
case of ellipse where the two foci are at the same position.
Hence, the pattern can be generally classified as ellipse.
Furthermore, the markers appear to be more elliptical as the
robot goes further away from the center of the area covered
by the camera, i.e. when it is not directly under the camera,
because of its viewing angle.
The marker recognition process can be summarized as in
Fig. 5:

Fig. 2 The robot

Fig. 2 shows the photo of the autonomous mobile robot
for floor marking developed in this study. The mobile robot
is driven by two servo motors via the two back wheels
which are arranged in a differential drive configuration.
RS232 communication protocol is used by the personal
computer to communicate with the micro-controller [8].
Based on the command received from the computer, the
Basic Stamp micro-controller used will move the two
motors at the appropriate speed to navigate the mobile
robot to the desired position. The floor marking mechanism
is placed at the rear end of the mobile robot to ensure that it
is positioned close to the axis of rotation of the mobile
robot. This configuration will produce better accuracy when
the floor marking is done. The hardware structure of the
mobile robot is illustrated in Fig. 3.

Input image
Grayscale and smooth image
Threshold
Contour extraction
Contour classification
Get center of circular patterns
Fig. 5 Marker recognition process

After the input image from the camera is grayscaled,
after the image has been grayscaled, the image is then
smoothed by Gaussian pyramid down sampling. This is
done to reduce noise from the environment. The image is
then process using the thresholding technique where
appropriate threshold is selected to get the desired
processed image [7]. The thresholding method converts the
image into binary image where any pixel of the image that
has a value larger than the predetermined threshold value

Fig. 3 Hardware structure of the mobile robot.

89

will be white (1-pixel) and pixels with value less that the
predetermined threshold value will be black (0-pixel) [7].
Next, the contours are extracted from the threshold
image based on the Suzuki-Abe algorithm. In this
algorithm, the contours are extracted from the binary image
by raster scanning the image; this will retrieve border
points that will then be used in a border following
procedure to retrieve and store the border in the Freeman
chain format [7].
These contours are then classified whether it is an
ellipsoid, or otherwise. To accomplish this ellipse
recognition process, numerous steps are taken. First of all,
the zeroth and first moments of the contours are calculated,
this is achieved by using the following equations:

Fig. 6 Result of ellipse recognition

B. Acquisition of Robot’s Position and Heading
To determine the robot’s heading, we must first identify
the shortest side of the triangle. Once we have identified the
shortest side of the triangle, the center of this shortest side
is determined. The center of this shortest line is defined as
the robot’s position. By plotting a straight line from this
point to the center of the circular pattern on the robot’s
front end, we can obtain the robot’s heading by finding the
relative position between these two points.

M 00 = ∑ f ( x, y) (1)
xy

M 10 = ∑ xf ( x, y) (2)
xy

M 01 = ∑ yf ( x, y )

(3)

xy

The center of each circle is then determined by finding
using the zeroth and first moments of the contours.
x=

M 10
M 00

(4)

y=

M 01 (5)
M 00

After obtaining relevant information about the contours,
undesired contours are to be discarded. Since the center of
an ellipse should coincide with the center of the minimum
bounding rectangular of the ellipse, we use this test to filter
out any contours that do not meet this requirement. In other
words, we are eliminating the contours that are not
symmetrical [7].
In the next step of the filtering process, the Hu set of
invariant moments is used to classify an elliptical shape.
The Hu set of invariant moments is set of values that are
derived from the second and third order normalized
moments. Since the 7 Hu moments of a shape is invariant
of image scale and rotation [11][12], an ellipse shape can
be recognized if the values of the Hu moments can be
obtained. The values of the 7 Hu moments were obtained
through experimentation. The result of the ellipse
recognition is shown in Fig. 6.
From the result of the recognition, we see that the three
circular patterns on the robot is recognized where the
patterns are highlighted with thicker lines compared to
other undesired shapes such as the triangle, rectangle, the
very small circle, and the random shape, which are online
traced with very fine lines.

x rel = xcircle − x position

(6)

y rel = y circle − y position

(7)

xˆ rel =

x rel
x rel

(8)

yˆ rel =

y rel
y rel

(9)

θ heading = tan −1

yˆ rel
xˆ rel

(10)

Fig. 7 Robot motion Parameters

C. Motion Control of the Robot
There are two motions that are to be controlled: 1) to
rotate to a desired angle, 2) to move from one point to

90

another. Since the mobile robot uses a differential rear
wheel drive, the motions of the robot are controlled by
varying the speed and rotational direction of each wheel
[8].
For the robot to rotate itself to a desired angle, first, the
robot’s desired angle is compared with the robot’s current
heading to obtain the error in heading angle. This
determines which direction the robot must rotate, to get to
the desired angle with the least amount of rotation. The
robot will rotate using a proportional controller.
v rotation = Kp rotation ⋅ θ error
(11)
To control the robot’s point to point motion, first the
robot must adjust its heading so that its posture points to the
desired point. Once it is facing the desired point, it
calculates the distance it must between its current position
and the desired position. As the robot moves towards the
desired point, it keeps updating the distance between its
current position and the desired position it reaches the
desired position [6].
As the robot moves to the desired position, it
continuously updates its desired heading to make sure that
it heads towards the desired point. The error in heading
angle is used to control the robot’s movement where the
rotational direction and velocities of the two motors are
controlled accordingly [2].
vl = v min ± Kp ⋅ θ error (12)

Fig. 9 Actual triangle plot by the mobile robot

Fig. 10 shows the detailed plotting of the robot’s motion
compared with the theoretical path of the mobile robot. The
mean error and standard deviation of the actual plot is
given as below:

Mean error = 7.961382 (pixels) x 1.943555 (mm/pixel)
= 15.47 mm
Std. dev.

= 4.558691 (pixels) x 1.943555(mm/pixel)
= 8.86 mm

450

v r = v min ± Kp ⋅ θ error (13)
Where v r and vl represent the velocity of the right and left

400
350
Y Position (pixels)

motor respectively.
IV. EXPERIMENTAL RESULTS
To evaluate the performance of the robot, two tests were
conducted.

300
250
200
150
100

Experiment 1: Plotting a Triangle
In this experiment, a triangle shape was to be plotted by
the mobile robot. Fig. 8 shows the travel path drawn in
OpenCV based on the digital map provided. Fig. 9 shows
the actual result of the experiment.

50
0
0

50

100

150

200

250

300

350

400

450

500

550

600

X Position (pixels)
Actual Plot

Desired Plot

Fig. 10 Comparison of the actual triangle plot and the desired triangle plot

Experiment 2: Plotting a Diamond
In this experiment, a diamond (rectangular) shape was to
be plotted by the mobile robot Fig. 11 shows the travel path
drawn in OpenCV based on the digital map provided. Fig.
12 shows the actual result of the experiment.

Fig. 8 Desired triangle plot reconstructed in OpenCV

91

V. CONCLUSION AND FUTURE DEVELOPMENT
The autonomous mobile robot for floor marking has
shown great potential in its early stages of development as
shown in the results of the experiments.
For the future development of this project, the motion
control algorithm can be improved by introducing the
derivative component of the PID controller.
The camera setup can also be improved by using CCTV
cameras that is available in the exhibition hall to capture
the image to be processed.
This system can also be applied in many different
applications such as delivering and security systems.
Fig. 11 Desired diamond plot reconstructed in OpenCV

REFERENCES
[1]

Patric Jensfelt, Erik Förell, and Per Ljunggren (2007). Automating
the marking process for exhibitions and fairs – the making of Harry
Plotter. IEEE Robotic and Automation Magazine 14(3), 35-42.
[2] Sun-Li Wu; Ming-Yang Cheng; Wen-Chung Hsu; (2005). “Design
and implementation of a prototype vision-guided golf-ball collecting
mobile robot”, Mechatronics, 2005. ICM '05. IEEE International
Conference on 10-12 July 2005 Page(s):611 - 615
[3] Seals, R.C.; Twum, E.; (1993), “Computer generated images for path
following for an unmanned vehicle”, Image Processing for
Transport Applications, IEEE Colloquium on 9 Dec 1993
Page(s):1/1 - 1/8
[4] Adam Hoover and Bent David Olsen (1999). “Path Planning for
Mobile Robots Using a Video Camera Network”, International
Conference on Advanced Intelligent Mechatronics, September 19-23,
1999.
[5] Hornberg, A. (2006). Handbook of machine vison. Weinhem:
WILEY-VCH Verlag GmbH & Co. KGaa.
[6] Kim, J. (2003). Lecture Notes on Soccer Robotics. Republic of
Korea. Korean Advanced Institute of Science and Technology.
[7] Robert van Liere, and Nguyen Duc Quoc Anh (2005). Robust
Tracking of Input Props with Webcams. Amsterdam, University of
Amsterdam.
[8] Kuo-Yang Tu (2005). “Design and Implementation of a Cheap
Middle Size Soccer Robot with Wide Vision Scope for RoboCup”,
Mechatronics, 2005. ICM '05. IEEE International Conference on 1012 July 2005 Page(s):130 - 135
[9] Jong-Hann Jean, Tien-Pao Wu, Jian-Hong Lai and Yaou-Chang
Huang (2005). “A Visual Servo System for Object Tracking
Applications of Mobile Robots Based on Shape Features”, 2005
CACS Automatic Control Conference on 18-19 November 2005.
[10] Vadim Pisarevsky (2007). Introduction to OpenCV. China, Intel
Corporation.
[11] Mikael Soron (2003). Robot Waiter – Visual Servoing and Top Level
Implementation. Sweden, Department of Technology, Orebro
University.
[12] http://www.cs.indiana.edu/cgipub/oleykin/website/OpenCVHelp/ref/OpenCVRef_Cv.htm

Fig. 12 Actual diamond plot by the mobile robot

Fig. 13 shows the detailed plotting of the robot’s motion
compared with the theoretical path of the mobile robot. The
mean error and standard deviation of the actual plot is
given as below:

Mean error = 5.25578 (pixels) x 1.943555 (mm/pixel)
= 10.21 mm
Std. dev.

= 6.72591 (pixels) x 1.943555(mm/pixel)
= 13.07mm

450
400

Y Position (pixels)

350
300
250
200
150
100
50
0
0

50

100

150

200

250

300

350

400

450

500

550

600

X Position (pixels)
Actual Plot

Desired Plot

Fig. 13 Comparison of the actual diamond plot and the desired diamond
plot

92

