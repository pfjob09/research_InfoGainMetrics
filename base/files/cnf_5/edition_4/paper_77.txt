Microscopic Image Segmentation for the Clinical Support System
ByoungChul Ko, MiSuk Seo, JaeYeal Nam
Dept. of Computer Engineering, Keimyung University, Daegu, Korea
{niceko, forever1004, jynam}@kmu.ac.kr
Abstract
This paper presents an AAW (Adaptive Attention
Window)-based microscopic cell image segmentation
method. For semantic AAW detection, a luminance map
is used to create an initial attention window, which is
then reduced close to the size of the real ROI (Region of
Interest) using a quad-tree. The purpose of the AAW is to
facilitate background removal and reduce the ROI
segmentation processing time. Region segmentation is
performed within the AAW, followed by region clustering
and removal to produce segmentation of only ROIs.
Experimental results demonstrate that the proposed
method can efficiently segment one or more ROIs and
produce similar segmentation results to human
perception.

1. Introduction
With the increase of genome projects that have
decoded the genomes of several species, created genetic
maps, and analyzed the order of the chromosome maps,
various medical assistance systems[2-4] have also been
introduced that integrate information communication,
computer networking, database management, and a user
interface.
Traditionally, medical images have been indexed and
retrieved using just text. Yet, traditional text-based
retrieval can produce irrecoverable mismatches
according to the subjectivity and viewpoint of the writer.
Furthermore, this kind of retrieval is costly and time
consuming. Thus, to overcome these problems, various
types of Content-Based Image Retrieval (CBIR) [1-2,6]
have been proposed over the last few decades.
In the medical field, region-based image retrieval is
helpful for diagnostic purposes. For example, diagnosis
systems based on cytology and histophysiology are used
to analyze tissue specimens to detect lesions as an early
signal of latent cancer. Plus, measuring the cell cycle
using a diagnosis system can enhance the effectiveness
of drug discovery and development. However, existing
diagnosis systems are restricted when dealing with cells
and due to the subjective variance of an observer.
Therefore, an integrated diagnosis system [2-4] with an
automatic aid method was recently developed to assist
with the detection of cancer.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

For a semantic analysis, an automatic aid method
requires an ROI-based approach rather than a pixel-based
approach to detect an abnormal nucleus or lesion. In
particular, the ROI segmentation is a crucial preprocess
to enable successful cell classification or diagnosis.
Comaniciu and Meer [2] developed the IGDS (Image
Guided Decision Support) system to analyze tissue
structures and organ states to support diagnosis and
identify factors in clinical pathology. The system extracts
an ROI (Region-of-Interest) within the attention scope
using the Mean-Shift segmentation method. However, to
extract the ROI before segmentation, the attention
window must be defined by hand.
Chen and Zhou [3] developed a method of image
analysis to resolve the problems of touching cells and
ambiguous correspondence, resulting in a computational
bio image system that facilitates the automated
segmentation, tracking, and classification of cancer cell
nuclei in time-lapse microscopy images.
Tscherepanow and Zollner [4] proposed a method for
classifying segmented regions in bright field microscope
images. However, since an active contour is used for the
cell segmentation, the performance can deteriorate when
an image contains cells with a complex structure.
Unlike general natural images, microscopic images
have different characteristics with distinct meanings
based on human estimation and a varying brightness
according to the fluorescence staining. For example,
salient parts, such as cell nuclei, tend to be brighter,
while the remaining parts have a more monotonous
appearance.
All existing medical image-segmentation methods
segment regions from an image regardless of the
meaning of the ROIs, meaning that exact ROI
segmentation is impossible without human interaction.
Thus, for semantic ROI segmentation, such as salient
cells, knowledge of the exact positions of relevant ROIs
is crucial.
Accordingly, this paper presents an AdaptiveAttention -Window (AAW)-based ROI microscopic cell
image segmentation method. For semantic AAW
detection, an Initial Attention-Window (IAW) is created
using a luminance map, then the IAW is reduced close to

the size of the ROI cell using a quad-tree. The purpose of
the AAW is to determine the rough position of relevant
ROIs, thereby reducing the amount of processing time
for segmenting ROIs. Finally, region-level segmentation
is performed within the AAW, along with background
removal and region clustering to segment only the ROIs.
Figure 1 shows the architecture of the proposed
AAW-based segmentation. The AAW generation is
based on human perception in the pre-processing step.
Then, as a result of the proposed method, only the ROIs
remain in the segmentation step.

(a)

(b)

The remainder of this paper is organized as follows.
Section 2 explains the AAW generation based on human
perception, then Section 3 describes the ROI
segmentation within the AAW. Section 4 evaluates the
accuracy and applicability of the proposed ROI
segmentation based on experiments, and some final
conclusions and areas for future work are presented in
Section 5.

2. AAW Detection based on HumanPerception
2.1. Initial AW Generation using Luminance
map
Human perception plays an important role in
computer vision and pattern recognition, and many
studies have attempted to use it to analyze the semantic
meaning within an image. For example, Itti et al. [5]
proposed a saliency-based visual attention model based
on color, luminance, and orientation, then selected the
most salient area based on a Winner-Take-All (WTA)
competition.
However, in this paper, the IAW within an image is
detected using a luminance map and quad-tree split
based on human perception during the image
segmentation preprocessing. In medical images, since the
intensity is the unique component, a luminance feature
map is used to detect the IAW, rather than a color feature
map, as the color can change according to the dye used.
Figure 2 shows the AAW generation process within
an image.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

(d)

First, to generate luminance map ( L ), two different
sized filters (7x7, 13x13) are applied to a 1/2 downsampled gray image (L’(s)) and luminance contrast are
computed. The filters estimate the center-surround
difference between the center point and the surrounding
points within the filter scale s and this difference yields
the feature map. By this way, this sum yields two feature
maps.
L=

Figure 1. Architecture of proposed method

(c)

Figure 2. The AAW generation (a) input image
(b) luminance map (c) Initial AW (d) final AAWs

1
( ∑ L ′( s ) )
2 s∈{7×7 ,13×13}

(1)

These maps are then summed and normalized into
one luminance feature map L , which is then up-sampled
to the size of the original image and smoothed with a
Gaussian filter to eliminate any pixel-level noise and
highlight the neighborhood of influence for the output
map. Examples of luminance maps are shown in Figure
2-(b).
After generating the luminance map, the IAW is
detected to remove useless regions, such as background,
thereby reducing the amount of processing time required
for ROI segmenting and improving the extraction
performance. ROIs are generally located near the center
area, however, since this is not always true, the size of
the IAW is very important.
Therefore, this paper proposes a top-down IAW
shrinking method that uses the created luminance map.
The initial rectangular IAW is three quarters the size of
the image, and reduced until it meets predefined
conditions. This size was determined by experiments and
the analysis of reference images in a database, where the
largest cell size as the ROI in the experimental database
set was found to be less than three quarters the size of the
image.
To determine the proper location (AWcx,cy) and size
(AWx,y) of the IAW, the window that includes the
maximum magnitude from the luminance map ( L ) for
the full image is initially chosen, then the size of the
IAW is shrunk to the approximate size of the ROIs.
Figure 3 shows the shrinking steps and conditions that
are presented as a pseudo code for the IAW generation,
while Figure 2-(c) shows some examples of IAWs.
Step 1. Initialize the size of IAW
IAW x = width × 3 / 4 IAW y = height × 3 / 4

Step 2. Find the candidate position of IAWcx,cy
IAW cx ,cy = Max ( SUM ( L ))

Step 3. Calculate the mean (TLm) of L within the IAW
Step 4. Repeat : Shrink boundary pixel IAWx,y of IAW
Until (IAWx,y < TLm)
Step 5. Final IAW Generation

Figure 3. Initial AW generation steps

2.2. AAW Extraction using quad-tree split
After the IAW is selected, the IAW needs to be
shrunk to the most approximate size of the salient ROIs.
The existing split method for extracting close-shape
ROIs initially considers the image as one region, then
iteratively splits according to a homogeneity criterion
into smaller and smaller regions.
To split into real ROIs and reduce the processing time,
the proposed method extracts an AAW with a size close
to the real ROI within the limited IAW using a quad-tree
that splits based on a square structure. Unlike previous
research [5], the IAW is shrunk using the luminance
feature map as the split condition within the IAW. Figure
4 shows the splitting steps for extracting the AAW. The
shrinking steps and conditions are as follows.
First, the average ( μ AW ) of the luminance map is
calculated within the IAW, and the IAW split into 4 × 4
scale 1 sub-blocks. The average is then used as a
threshold to further divide the sub-blocks. As such, for
every sub-block, if the average ( μ AW ) of the luminance
map within a sub-block is above the threshold, that subblock is split again into 4 × 4 scale2 sub-blocks, otherwise
it is removed.
t: split level, t = 0

Step 1. Set a threshold( μ AW ) using luminance map(Lm) for
block split within Initial AW
1
μ AW =
∑ Lm
N AW
Step 2. Split IAW into 4 × 4 sub-blocks(Sbi)
μ
Step 3. Calculate average( bi ) for each sub-block using
luminance map
μ bi =

1
∑ Lm
N Sb
i

μ
Step 4. If ( bi < μ AW ), then remove block,
Otherwise, 4 × 4 sub-block re-split
t = t +1
If (t < 3), then repeat from Step 2,
Otherwise, proceed to Step 5.
Step 5. Merge or remove block

Figure 4. AAW extraction steps using a quadtree split
This process of removing sub-blocks is based on the
assumption that humans usually concentrate on special
regions that have a high luminance contrast [5]. That is,
if the average of the luminance map is low, this means
that the sub-block has a lower chance of containing
salient regions, alternatively, if the average is high, this
means the sub-block has a higher chance of containing
salient regions. Therefore, a sub-block with a higher
luminance feature map compared to the predefined
threshold is split again to detect close ROIs. This process
is repeated to scale3. Finally, the scale3 sub-blocks are
either merged into adjacent large sub-blocks or removed
if they are far away from major sub-blocks.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Figure 2-(d) shows some examples of final AAW
and Figure 5 shows the proposed AAW extraction
method.

Figure 5. Adaptive AW Extraction

3. AAW Segmentation
While object-based image segmentation is useful in
many applications, it is still generally beyond current
computer vision techniques, due to the uncontrolled
nature of the available images and requirement of much
processing time. Since an object is generally a group of
related regions, the present study proposes a way of
segmenting an image into regions, then merging these
regions into an ROI.
Thus, after the AAW is created, the regions within
the AAW are classified as ROIs or background regions.
To do this, the AAW is segmented into several regions
that are then clustered into ROIs according to the
proposed algorithm. For semantic region segmentation,
Ko et al.’s [6] image segmentation is applied in the
proposed method.

3.1. Background Region Removal
After an image is segmented within the AAW, the
segmented regions still consist of ROIs and some
meaningless regions. Thus, for semantic ROI extraction,
these meaningless regions need to be removed as
background.
In conventional background removal methods, such
as Watershed [7], Otsu [8], and the Triangle algorithm
[7], a threshold is used to separate ROIs, like cells, from
background regions, such as cytoplasm. Although
background removal using a global threshold is simple
and efficient as it uses a histogram to create a bi-modal
representation of an image, this separation method is not
always suitable, as a histogram does not necessarily
provide a bi-modal representation and can fail to
accurately segment ROIs. Therefore, this paper proposes

a background removal method using the boundaries of
the detected AAW and segmented regions.
If region segmentation is performed, boundary regions
are defined as background within the AAW and removed.
This definition is based on the following assumption: the
major region is most likely to exist inside the AAW,
while the background regions are more likely to exist at
the AAW boundaries.
Therefore, the ratio for each region that connects with
the AAW boundary is estimated using the following
Equation (2).
card (S AAW ∩ S k )
k = 1,2..., n
(2)
card(S k )
where SAAW and Sk represent the boundary coordinate
set for the AAW and kth region, respectively, card (A)
represents the cardinality of each set, and the symbol
B_Pk indicates the ratio of the kth region that connects
with the AAW boundary. These ratios are then compared
with a predefined threshold and, if larger, that region is
regarded as a background region and removed. Figure 6(d) shows separated ROI by proposed background
removal algorithm.

the area of Obi,k is updated. However, if Obi,k is 0, which
means BRi is located outside Obi,k, BRi is removed and
merged with the background region according to
Equation (4):
0 < Obi,k ≤ 1 Then

if
else,

BRi ⊂ ROIk

(4)

BRi ⊂ Background

B _ Pk =

(a)

(b)

(c)

(d)

(e)

(f)

Figure 6. ROIs extraction steps
Figure 6 shows the ROIs extraction steps and Figure
7 shows the ROI segmentation results when using the
proposed algorithm.

3.2. Region Clustering
After the background region removal, the major
regions are selected for clustering, whereby similar
adjacent regions are merged sequentially if the adjacent
regions satisfy a few conditions.
The merging steps according to clustering conditions
are based on Equation (3). For the regions segmented
within the AAW, the size of each region is calculated,
except for the removed background regions. Then, n seed
regions with a saliency above a predetermined threshold
and larger size compared to the other regions are selected
as the major regions. In this paper, the number of seed
regions was fixed at five based on the finding that the
cell images in the experimental database tended to
include fewer than five nuclei. Second, the ratio that the
adjacent region is connected with the boundary or
belongs to the seed region is calculated.
Starting from each seed region sequentially, two
regions are merged if the two regions satisfy the
following conditions:
Ob i , k =

card ( BR i ∩ ROI
card ( Pm )

Pm = min( BR i , ROI

k

k

)

(3)

)

where BRi is the set of boundary pixels in the ith
comparison region, ROIk denotes the set for the kth seed
region, Pm denotes the set for the relatively smaller
region between BRi and ROIk, the symbol card(A)
represents the cardinality of set A, and Obi,k denotes the
ratio of the inclusion relation between the kth ROIk and
ith BRi. The symbol Obi,k becomes zero when the BRi
region is not connected and exists outside of ROIk, and
becomes one when the BRi region surrounds ROIk.
Finally, if Obi,k is above 0, BRi is merged with ROIk and

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Figure 7. ROI segmentation results

4. Experimental Results
In this paper, a set of 200 cell images was used that
included plant cells, white blood cells, and red blood
cells, where the histology was taken from a microscope.
The image set consisted of 106 images of normal cells
(fewer than 5 cells) and 94 images of abnormal cells. To
verify the ROI segmentation performance, the proposed
segmentation performance was first compared with
related segmentation method proposed by Comaniciu
and Meer [2]. In this method, the user creates the initial
AW by hand and the segmentation is then performed
using a mean-shift algorithm that is normally used for
natural image segmentation.
The segmentation results for each method were
compared with the manually extracted segmentation
result and the error ratio estimated using Equation (5).

SU =

card(M − (M ∩ S ))
card(S − (M ∩ S ))
, SO =
SM
SS

Over_ext. Cmp

(5)

AVG _ S = (1 − (SU ⊕ SO ) × 100

1.0

where M represents a manually extracted ROI and S
represents an automatically extracted ROI using each
segmentation method, SM and SS denote the total size of
the pixels of an extracted ROI, SU and SO represent the
inaccuracy of the under-extraction ratio and overextraction ratio, respectively, and AVG_S represents the
accuracy between a manually cropped ROI and a
systematically extracted ROI, where a number closer to
100 represents a lower error and higher accuracy, while a
number closer to 0 represents a higher error and lower
reliability.
Figures 8 and 9 show the performance evaluation
results using Equation (5).
When compared to Comaniciu and Meer's method,
the proposed method showed a similar under-extraction
ratio, as shown in Figure 8. However, the proposed
method had a lower over-segmentation ratio of 14.8 %
compared to Comaniciu and Meer's segmentation
method at 43.1 %, as shown in Figure 9 and Table 1,
respectively.
From the experimental results in Figure 9, since
Cominiciu and Meer's method segmented the ROIs
within a static rectangle region defined by the user, this
led to a high over-extraction error ratio and also created
many fragments as useless regions or ROIs.
In addition, the over-extraction error with Cominiciu
and Meer's method was higher for the images between
106 and 185 that included numerous normal cells and
abnormal cells, as the window was almost the same as
the original image. In contrast, as the proposed method
created an AAW with a form close to the ROIs and the
segmentation was performed within the created AAW,
this enabled the position of the ROIs to be detected
accurately. As such, the proposed method had a lower
over-extraction error ratio and high accuracy AVG_S at
73.6 %. Therefore, the proposed segmentation method
produced a better performance than the comparative
algorithm. Furthermore, the standard deviation for the
under and over-extraction error ratio represented a
regular rate with the proposed segmentation.
Under_ext. Cmp
Under_seg.
1.0
0.8
0.6
0.4
0.2
0.0
1

21

41

61

81

101

Com.&Meer under

121

141

161

181

proposed under

Figure 8. Under-extraction comparison

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

201

file no.

Over_seg.

0.8
0.6
0.4
0.2
0.0
1

21

41

61

81

101

121

Com.&Meer over

141

161

181

proposed over

201

file no.

Figure 9. Over-extraction comparison
Table 1. Error comparison
Method
Under Ext. Over Ext.
AVG Acc.
Com.& Meer
0.109
0.431
46.0 %
Proposed
0.116
0.148
73.6 %
method
Next, to verify the applicability of the proposed
method as a medical diagnosis aid system, the
importance of the regions segmented using each method
was compared with the manually cropped regions. Figure
10 shows examples of the abnormal cell images
according to various diseases.
Howell-Jolly bodiesGiant platelet

Bacteria

Parasites

Dohle

Auer rod

Platelet

Yeast

Pappenheimer
bodies

Nucleated red
blood

Schiiffner’s
granules

Toxic
granulation

Figure 10. Examples of abnormal cell images
For the evaluation, a count was made of the number
of cells segmented from the total 200 images and number
of abnormal cells segmented from the 94 images that
coincided with the manually cropped ROIs.
Table 2 compares the number of manually
segmented cells with the number of cells automatically
segmented by the two methods.
Table 2. Comparison of ROI extraction
No. of abnormal
Method
No. of ROIs
ROIs
Manually
829
122
Segmented ROIs
Com.&
Meer
475
74
method
Proposed method
703
108
Within
Adaptive
AW

As shown in Table 2. since Cominiciu and Meer's
method had a very low extraction number at 74
compared with the number of manually extracted
abnormal cells at 122. However, the proposed method
segmented 703 cells, resulting in an 89% extraction ratio,
which was very close to the manually extracted results.
Furthermore, in the case of the abnormal cells, the
proposed method had a relatively high extraction number
at 108 out of the total 122 abnormal cells.
Since the salient parts in cell images, such as normal
and abnormal cell nuclei, tend to be relatively bright
against the background according to the dye material, the
proposed method utilized this characteristic by splitting
the images into small sub-regions with ROIs using a
luminance contrast feature map to create AAWs, and
then segmented the exact ROIs within the AAWs. As
such, the proposed method was able to extract the
abnormal cells very efficiently.
Thus, the experimental results confirmed the
effectiveness of the proposed method as a diagnostic aid
in relation to detecting abnormal cells.

5. Conclusion
In this paper we proposed a ROI cell image
segmentation method based on AAW for some
applications such as medical image retrieval and clinical
diagnosis. The salient part, nuclei and abnormal nuclei in
the cell images tend to have strong brightness according
to dye material. By using this characteristic, proposed
method splits an image into small sub-regions based on
luminance contrast to make the AAW and segments exact
ROIs within the AAW. Extracted AAW not only reduces
the amount of processing time for segmenting but also
extracts really exact ROIs. The experimental results
confirm that the proposed algorithm improves
segmentation result and shows availability for the
medical diagnosis aid system.
As future works, we will apply our method to a
medical
image
retrieval
system
considering
communication with hand-held devices such as PDA and
Tablet PC.

Acknowledgements
This work was supported by grant RTI04-01-01 from
the Regional Technology Innovation Program of the
Korean Ministry of Commerce, Industry, and Energy
(MOCIE).

References
[1]

[2]

Seo, M., Ko, B., and Nam, J. ROI-based image retrieval
using Human-Perception and MPEG-7 Visual
Descriptors. Lecture Notes in Computer Science, Vol.
4071, Springer-Verlag, 231-240, 2006.
Comaniciu, D. and Meer, P. Mean-Shift: A Robust
Approach toward Feature Space Analysis. IEEE Trans.
on Pattern Recognition and Machine Intelligence, Vol.
24, 603-619, 2002.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

[3]

[4]

[5]
[6]

[7]

[8]

Chen, X., Zhou, X., and Wong, T.C. Automated
Segmentation, Classification, and Tracking of Cancer
Cell Nuclei in Time-Lapse Microscopy. IEEE Trans. on
Biomedical Engineering, Vol. 53(4), 762-766, 2006.
Tscherepanow, M., Zollner, F., and Kummert,
F.:Classification of segmented regions in brightfield
microscope images. In Proceedings of Int. Conf. on
Pattern Recognition, Vol.3, 972-975, 2006.
Itti, L., Koch, C., and Niebur, E. A Model of Saliencybased Visual Attention for Rapid Scene Analysis. IEEE
Trans. on PAMI, Vol. 20, 1254- 1259, 1998.
Ko, B.C. and Byun, H. Frip: A region-based image
retrieval tool using automatic image segmentation and
stepwise boolean and matching. IEEE Trans. on
Multimedia, Vol. 7(1), 105-113, 2005.
Ropers, S. -O., Bell, A. A., Wurflinger, T., Bocking, A.,
Meyer-Ebrecht, D., "Automatic scene comparison and
matching in multimodal cytopathological microscopic
images," IEEE Int. Conf. on Image Processing, Vol. 1,
pp. 1145-1148, 2005.
Chen, X., Zhou, X., Wong, S.T.C, "Automated
Segmentation, Classification, and Tracking of Cancer
Cell Nuclei in Time-Lapse Microscopy," IEEE Trans. on
Biomedical Engineering, Vol. 53, pp. 762-766, 2006.

