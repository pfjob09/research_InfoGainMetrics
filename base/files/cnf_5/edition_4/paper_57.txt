Decision Fusion Based on Voting Scheme for IR and Visible Face Recognition
M.D. Shahbe and S. Hati
Faculty of Information Technology, Multimedia University, Malaysia
{shahbe,subhas}@mmu.edu.my
Abstract
In this paper we present an evaluation study of
decision fusion strategies for infrared (IR) and visible
face recognition. Several decision fusion methods
based on voting scheme (minimization, product and
averaging) are discussed and experiments for various
conditions of probe set are performed on two
databases with paired IR and visible face imageries.
The Eigenfaces and Fisherfaces classification
techniques are used to extract the face features and the
performance of fusion methods are discussed.

1. Introduction
In the field of pattern recognition and computer
vision, we have witnessed that extensive research has
been made on face recognition. Research contributions
have been made in various domains such as evaluation
methods, pre-processing, performance metrics and
classification techniques. The works are done primarily
by using visible face imageries. However, in recent
years, there has been an increasing interest among
researchers in face recognition using IR imageries
[7,9-17]. Fig. 1 shows sample paired of IR and visible
face images.
Pose

Expression

Glasses

Dark

Fig. 1: IR and visible imageries

The face recognition performance in visible
spectrum is limited in uncontrolled environments such
as outdoor scene and darkness. Nevertheless, IR
images are independent on lighting conditions because
IR sensor only measures the heat emitted by objects
and human body is a good heat emitter. On the other
hand, IR spectrums also have their own limitations.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Firstly, glasses are opaque to infrared wavelengths.
This fact causes the problem of identifying faces with
eyeglasses in IR wavelengths. Secondly, IR spectrums
are susceptible to temperature changes. This
contributes to dependency of IR face recognition on
the surrounding temperature and the heat of human
face. Consequently, face recognition will be a very
difficult task if it relies only on single image spectrum
which is either IR or visible face images.
Therefore, the motivation of our work is to utilize
the information given by IR and visible images in
order to compensate the drawbacks of both image
spectrums. Our work is inspired by recent publications,
[10,13,15], where simple fusion strategies are applied
in their face recognition study. In [10] we find the
comparison of face recognition performance between
the different modalities based on Monte Carlo analysis.
Here a simple adaptive weighting fusion scheme is
introduced to obtain new ranked list. The brief report
in [13] employed both data and decision fusions. The
averaging and highest ranking as decision fusion
methods are described in this paper. The effect of
lighting, facial expression and passage of time between
the gallery and probe images are examined in [15]. In
general, we find from these papers that the
combination of the decision scores outperforms
individual recognizers.
The rest of the paper is organized as follows. In
Section 2, we present an overview of Fisherfaces
technique. In Section 3, we discuss the distance
metrics and in Section 4, we discussed decision fusion
schemes. Section 5 discussed the experimental setup.
The results of the experiments are presented and
discussed in Section 6 followed by conclusions in
Section 7.

2. Fisherfaces
Fisherfaces is an approach that combines Principle
Component Analysis (PCA) and Linear Discriminant
Analysis (LDA) techniques. PCA has been widely

adapted in face recognition to reduce dimensionality
[1,2]. In LDA, data is transformed into a new subspace
in such a way that the data are separable into classes.
The difference between the two approaches is that
PCA finds the directions that are efficient for
compressing higher dimensional data into lower
dimensional data while LDA finds the directions that
are efficient for discrimination. The LDA process is
carried out via scatter matrix analysis as proposed in
[5,4].
Following the algorithm of PCA in [5], the data
dimension is reduced to N=60 and N=120 (N =T-M)
where T is the number of training images and M is the
number of face class) for OTCBVS and Equinox
datasets respectively. By applying Fisherfaces
technique in our experiments, the data dimension is the
reduced to N=19 and N=39 (N=M-1) for both datasets
respectively.

3. Distance metrics
The matching score di is defined by the distance
between probe y and each target fi in Fisherfaces space.
The distance tells us how much the probe y is likely to
face class i. In this paper we have adopted four
distance functions: Manhattan (L1), Euclidean (L2),
Euclidean angle (Ea) and Mahalanobis (MH) distances.
These distance metrics are represented by the
following functions:
N

dik = L1( y k , fi k ) = å ( y (kn ) - f ( kn ) i )

(2)

n= 1

dik = L 2( y k , fi k ) =

N

å

n= 1

( y (kn ) - f ( kn ) i ) 2
N

dik = Ea( y k , fi k ) = cos- 1

dik = MH ( y k , fi k ) =

å

n= 1

L 2(0, y
N

å

n= 1

y (kn ) * f ( kn ) i
k

)* L 2(0, fi k )

(3)

(4)

C - 1 ( y (kn ) - f ( kn ) i ) 2 (5)

where k Î { IR , V } , N is the size of feature vector and
C -1 is the covariance matrix which represent the face
data distribution. The identification of an unknown
input face is made based on the new matching scores
Z={z1,z2,…,zM}. In section 6 we report the performance
results of IR, visible and fusion scheme by using these
distance metrics.

4. Decision Fusion

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

For multi-class classifiers, rankings and numerical
score measurements, are always used to represent the
matching scores of each of the class. It is claimed in
[3] that ranked-based scoring contains more
information
compared
to
numerical
score
measurements. In this work, we study both the
distance-based and rank-based scoring methods to
represent the two individual classifiers. The ranking of
all of the classes is obtained based on the distance
distribution of the face classes. The ranking is scaled
from 1 to M where M is the number of face class.
These rankings or distances will be the preliminary
decision scores for IR and visible spectrums,
z = { z1 , z 2 , ..., z M }, where k = {IR , visible} .
k

k

k

k

These

two decision scores are then combined yielding new
decision scores Z.
We investigate the voting schemes which include
minimization, product and averaging decision rules.
Compared to other methods such as Logistic
Regression (correlation approach), these methods are
simple as they do not require observation of the
classifiers [6]. Correlation method is computationally
expensive as we need to correlate input image with all
images in the training set. Minimizing method for
multiple classifiers combination is discussed in [3,6].
For this method, we make use the strength of both the
classifiers where the classifier that performs better is
considered for recognition. The class distance or
ranking for the selected classifier will be the new score
of the corresponding class. The averaging and product
schemes are also the commonly used techniques for
multi-classifiers fusion. These techniques are discussed
in [8]. The methods are easy to be implemented and
they need no training. Compared to minimizing rule,
they treat the classifiers equally where both classifiers
have the same strength for recognition. Product rule is
employed by multiplying the scores of the different
classifiers in order to obtain new decision scores. For
averaging method, the new score is acquired by
summing up the scores of the classifiers and divide it
by two. Additionally, we discuss on averaging method
based on weighted distance where some weighted
factor is assigned to distance parameter for each of the
image spectrums. The weighted distance score is
defined as follows:

d ik =

a d ik
sk

, where k Î

(1)

{ IR , V }

where alpha, a and beta, b = 1 - a are the weighted
factors for IR and visible scores respectively. In order
to normalize the scores measurement, the scores are
divided by standard deviation of the scores, s

IR

and

s V respectively. The weighted factors a and b are
obtained via simple experimental analysis. The
analysis is further discussed in Section 6.

5. Experimental Data
In our experiments, we use two IR and visible face
datasets; Equinox [18] and OTCBVS benchmark [19].
The original image size in the datasets is 240x320
pixels. The images are preprocessed prior to the
experiment by using CSU Face Identification
Evaluation System [20] which was written in ANSII C.
The preprocessing is performed in order to reduce the
effects of unrelated background, lighting variation and
face geometric transformations which could undermine
the recognition performance. The similar step of
preprocessing was implemented and reported in [15].
This program requires the eye coordinates of the faces
as input, thus the coordinates are manually obtained
via a simple program. The image size after the
preprocessing is 92x112 pixels.
For OTCBVS dataset, we intended to evaluate the
influence of illumination, facial expression and face
poses on recognition performance. The dataset consists
of 4228 pairs of IR and visible face images under
variables expressions, illuminations and poses. The
face images were taken at eleven different poses which
range from left to right side view of a face as
illustrated in Fig. 2. In this experiment, we used the
images from pose 4 to pose 8. The expression
conditions include surprised, laughing and angry. The
illumination conditions include left light on (Lon),
right light on (Ron), both right and left lights on (2on),
dark room (Dark), and both left and right lights off
(Off). The Off condition faces which are taken at pose
6 with no effect of expression and illumination are
considered as normal images.
Pose11
Pose10
Pose9
Pose8
Pose7

Pose6

Pose1
Pose2
Pose3
Pose4
Pose5

Fig. 2: OTCBVS dataset at different poses

The data used for the experiment is divided into
three sets: gallery, probe and training sets. The size of
the training set is T=80 images (face class M=20,
sample per class S=4). Due to the availability of this
dataset, some images in the training set are included in
galley set. The samples in gallery set are chosen from
various variables: (i) Off at pose 6, (ii) Off at pose 5,
(iii) 2on at pose 5 or pose 6, (iv) and expression

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

surprise at pose 5 or pose 6. The probe set is also
divided into five different sets: (1) normal, (2)
illumination, (3) expression, (4) pose and (5)
illumination/pose. Note that the intersection between
gallery and probe sets is empty.
For Equinox dataset, we attempted to evaluate the
effect of various illuminations, facial expressions and
appearance of eyeglasses. The data are divided into
gallery; training and probe sets (illumination,
expression and eyeglasses). The training set includes
randomly selected expression and vocal pronunciation
frames with and without wearing glasses which are
taken under various lighting directions and facial
expression. The size of training set is T=160 images
(face class M=40, sample per class S=4). The gallery
set consists of four vocal pronunciation frames of 40
subjects wearing eyeglasses. This same gallery set is
used in all probe tests. Note that the intersection
between gallery and probe set is empty. The
illumination probe set differs from gallery set in such a
way that the lighting conditions of corresponding
subjects are not similar. For the expression test, the
expression frames are selected with the lighting and
eyeglasses conditions of the corresponding subjects
similar to the gallery set. Measuring the influence of
eyeglasses is done on vocal pronunciation frames; the
lighting directions are the same, but the eyeglass
condition is different from the samples in the gallery
set. The descriptions of gallery and probe sets are
summarized in Table 1.
Variable
Frame
Lighting
direction
Eyeglasses
ON/OFF
No of
images

Gallery
set
Vocal
Front
Left
Right
ON
OFF
M=40,
S=4

Lighting

Probe sets
Expression

Eyeglasses

Vocal
Left/Right
Front/Right
Front/Left
ON
OFF
3312
frames

Expression
Front
Left
Right
ON
OFF
118
frames

Vocal
Front
Left
Right
OFF
ON
1596
frames

Table 1: The description of Equinox data setup

The Fisherfaces algorithm discussed in [5], suggests
picking the number of principal components to keep in
the PCA dimension reduction step as N=T-M, where T
is the number of training images and M is the number
of face class. This choice of N guarantees that the
resulting matrix will be non-singular. In our
experiment, we use 60 and 120 eigenfeatures for
OTCBVS and Equinox datasets respectively.
Fisherfaces is applied and face data dimension is
reduced to M-1. We use all the 19 and 39 components
of LDA representation for both datasets respectively.

6. Experimental Results and Discussion
We first show the analysis on the choice of
weighted distance for average weighted distance fusion
(avgWD) in Fig.2. This method depends on the
weighted factor of image spectrum and the variation of
face data as defined in (1). The values of weighted
factors are obtained via a simple experimental analysis.
For illumination test, higher recognition rate is
obtained whenever the factor of the IR scores
( a =0.55) is higher than the factor of the visible scores
( b =0.45). This is contradict with the eyeglasses test,
where the higher strength is given to visible spectrum
( a =0.4, b =0.6). These two results indicate that (i)
the final decision scores in IR domain are contributed
more by IR scores component, and (ii) the final
decision scores are mostly denoted by the visible part.
However, for expression and pose tests the domination
between IR and visible spectrums on the strength of
weighted factors which giving considerable results are
uncertain. The ranges of a for these two tests are
between 0.45-0.6 and 0.45-0.5, respectively. We take
the average value where a is equal to 0.525 and
0.475, respectively. This simple experiment
recommends that the consideration on the strength of
different image spectrums during the fusion process
could contributes to the higher recognition rate.

Fig. 2: Weighted factor for different experiments

The next experiment is to determine a good metric
for measuring the distance between probe and gallery
images. Fig. 3(a-c) show performance of the four
distance metrics (Euclidean, Manhattan, Euclidean
angle and Mahalanobis) by using single recognizers
and avgWD fusion. From the figure it is clear that
Mahalanobis distance is superior to all other distance
metrics regardless of type of recognizers. This is due to
the fact that when using Mahalanobis distance, we take
into account the face data structure by considering the
covariance matrix for the observed samples. Our result

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

show that Manhattan and Euclidean distances perform
worst. The intermediate performance results are
obtained by using the Euclidean angle distance.
Seven different decision fusion schemes as well as
IR and visible recognizers have been employed. The
results of the experiments are shown in Table 2-3. For
the rest of the discussion, the decision fusion schemes
are defined as follows: visible individual recognizer
(V), IR individual recognizer (IR), average weighted
distance fusion (avgWD), minimum distance fusion
(minD), product distance fusion (prodD), average
distance fusion (avgD), minimum ranking fusion
(minR), product ranking fusion (prodR), average
ranking fusion (avgR). Referring to Table 2, the
matching results on normal condition (frontal view,
normal lighting and normal expression) are
significantly superior. avgWD performs consistently
better compared to other schemes. This is also shown
in 3 where avgWD always achieve higher recognition
rate compared to other fusion schemes as well as
individual recognizers.
Looking at the results of illumination test in both
tables, we see indication of the difficulty of
recognition in visible spectrum. For example in Table
2, the recognition rate in IR spectrum is 81.25% while
in visible spectrum the recognition rate drops to
67.5%. In Table 3, the matching rate in test no.1 by the
visible imageries is 84.45% compared to recognition
by using IR images is 89.49%. For expression test,
there is no significant performance differences
observed between IR and visible spectrums. The
recognition rates for expression test in Table 2 for IR
and visible spectrums are the same. The expression test
in Table 3 shows that the IR result is slightly higher
than visible result. For the pose test in Table 2, the
result in IR spectrum is slightly lower than visible
spectrum and under the influence of pose and
illumination the result drops to 44.17%. For the test
under the influence of eyeglasses appearance, our
experimental result in Table 3 show that IR performs
inadequately compared to visible where the IR
recognizer attains less by 6% compared to visible
recognizer. The results reported in Table 2 and Table 3
are further illustrated in Fig. 4 and Fig. 5, respectively.
Generally, we find that recognition rate by using
fusion scheme exceeds the individual recognizers. The
choice of fusion rule, however, should be carefully
considered as some fusion rules defeated by the
individual recognizers. This is shown in Fig. 4 and 5
where sometimes individual recognizers do better than
some fusion rules such as minD, avgD, minR, prodR
and avgR.
Fig. 6 shows that the avgWD decision fusion
improves recognition performance for all tests and

classification algorithms (Fisherfaces and Eigenfaces
classification). This observation also indicates the

(a)

known observation that the Fisherfaces is superior to
the Eigenfaces technique [5].

(b)

(c)

Fig. 3: Comparison of distance metrics on different schemes: (a) visible, (b) IR, (c) avgWD fusion
Test
no.
1
2
3
4
5

Probe set

Normal
Illumination
Expression
Pose
Pose, illumination
average matching

Matching rate (%) by using different decision fusion based on Fisherfaces
V
IR
avgWD
minD
prodD
avgD
minR
prodR
avgR
100
100
100
100
100
100
100
100
100
67.5
81.25
90
72.5
87.5
87.5
81.25
85
83.75
97
97
99
97
99
99
97
98
96
86.25
70
92.5
86.25
90
92.5
70
88.75
87.5
44.17
70.83
81.67
53.33
81.67
80.83 70.83
71.67
67.5
79.89
88.37
92.63
81.82
91.63
91.97 83.82
88.68
86.95

Table 2: Matching rate by using OTCBVS dataset.
No

Probe set

1
Illumination
2
Expression
3
Eyeglasses
average matching

Matching rate (%) by using different decision fusion based on Fisherfaces
V
IR
avgWD
minD
prodD
avgD
minR
prodR
avgR
84.45
89.49
91.33
89.49
91.24
89.03
89.49
90.34
90.4
88.98
90.68
92.37
90.68
91.53
88.98
90.68
92.37
92.37
86.03
79.95
90.6
79.95
90.41
88.03
79.95
89.35
89.54
86.49
86.71
91.43
86.71
91.06
88.68
86.71
90.69
90.77

Table 3: Matching rate by using Equinox dataset.

Fig. 4: Performance of fusion methods on OTCBVS
datasets. Probe sets: (1) Illumination, (2) Expression,
(3) Pose and (4) Pose & Illumination probe sets.

(a) Illumination test (Equinox)

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Fig. 5: Performance of fusion methods on Equinox
datasets. Probe sets: (1) Illumination, (2) Expression,
and (3) Eyeglasses.

(b) Expression test (Equinox)

(c) Eyeglasses test (Equinox)
(d) Pose test (OTCBVS)
Fig. 6: Fisherfaces vs. Eigenfaces. Decision fusion strategies: (1) visible, (2) IR, (3) avgWD, (4) minD, (5) prodD, (6) avgD,
(7) minR, (8) prodR and (9) avgR

7. Conclusion

8. References

Based on the results, we state that decision fusion is
a very useful tool to improve recognition. The quality
of fusion, however, is certainly dependent on the
choice of the combiner. We have shown that weighted
distance averaging achieves better results compared to
other conventional methods. However, the weighted
factor varies over various conditions. Thus, our future
work will recommend dynamic weighted factor where
the factor should depends on the occurrence of
eyeglasses and the changes of lighting conditions.

[1] M. Kirby, L. Sirovich, "Application of the KarhunenLoeve Procedure for the Characterization of Human Faces",
IEEE Trans. on Pattern Analysis and Machine Intelligence,
Vol. 12, No. 1, 1990, pp. 103-108.
[2] M. Turk and A. Pentland, "Eigenfaces for Recognition",
Journal of Cognitive Neuroscience, Vol. 3, 1991, pp.71-86.
[3] T. K. Ho, J. J. Hull and S. N. Srihari, "Decision
Combination in Multiple Classifier Systems", IEEE
Transactions on Pattern Analysis and Machine Intelligence,
Vol. 16, No. 1, 1994, pp. 66-75.
[4] D.L. Swets and J. Weng, "Using Discriminant
Eigenfeatures for Image Retrieval", IEEE Transactions on
Pattern Analysis and Machine Intelligence, Vol. 18, No. 8,
1996, pp. 831-836.
[5] P. N. Belhumeur, J. P. Hespanha, David J. Kriegman,
"Eigenfaces vs Fisherfaces: Recognition Using Class
Specific Linear Projection", IEEE Trans. on Pattern Analysis
and Machine Intelligence, Vol.19, No. 7, July 1997, pp. 711720.
[6] A. Saranli, M. Demirekler, "Rank-based Multipleclassifier Decision Combination: A Theoretical Study", Proc.
of the IEEE International Workshop on Intelligent Signal
Processing, September 1999, pp. 51-56.
[7] F.J. Prokoski, "History, Current Status, and Future of
Infrared Identification", Proceedings IEEE Workshop on
Computer Vision Beyond the Visible Spectrum: Methods
and Applications, 2000, pp. 5–14.
[8] D. M. Tax, Martijn van Breukelen, R. Duin, and J.
Kittler, "Combining Multiple Classifiers by Averaging or by
Multiplying?", Pattern Recognition, Vol. 33, No. 9,
September 2000, pp.1475-1485.
[9] D. Socolinsky, L. Wolff, J. Neuheisel, C. Eveland,
"Illumination Invariant Face Recognition Using Thermal
Infrared Imagery", Computer Vision and Pattern
Recognition, Vol. 1, 2001, pp. 527–534.
[10] D. Socolinsky, A. Selinger, J. Neuheisel, "Face
Recognition with Visible and Thermal Infrared Imagery",
Computer Vision and Image Understanding, Vol. 91, No.1-2,
July 2003, pp.72–114.
[11] A. Gyaourova, G. Bebis, I. Pavlidis, "Fusion of Infrared
and Visible Images for Face Recognition", European

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Conference on Computer Vision, Vol. IV, 2004, pp.456-468.
[12] X. Chen, Z. Jing, S. Sun, and G. Xiao, "Fusion of
Visible and Infrared Imagery for Face Recognition", Chinese
Optical Letters, Vol.2, No. 12, December 2004, pp. 694-697.
[13] J. Heo, S. Kong, B. Abidi, and M. Abidi, "Fusion of
Visual and Thermal Signatures with Eyeglass Removal for
Robust Face Recognition," Conference on Computer Vision
and Pattern Recognition Workshop, 2004, pp. 122-127.
[14] Xuerong Chen, Zhongliang Jing, Gang Xiao, "Fuzzy
Fusion for Face Recognition", 2nd International Conference
on Fuzzy Systems and Knowledge Discovery, China, 2005,
pp. 672-675.
[15] X. Chen, P.J. Flynn, K.W. Bowyer, "IR and Visible
Light Face Recognition", Computer Vision and Image

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Understanding, Vol. 99, no 3, Sept 2005, pp. 332–358.
[16] H. Hariharan, A. Gribok, M. Abidi and A. Koschan,
"Image Fusion and Enhancement via Empirical Mode
Decomposition," Journal of Pattern Recognition Research,
Vol. 1, No.1, January 2006, pp. 16-32.
[17] G. Bebis, A. Gyaourova, S. Singh, I. Pavlidis, "Face
Recognition by Fusing Thermal Infrared and Visible
Imagery", Image and Vision Computing, Vol. 24, 2006, pp.
727-742.
[18]http://www.equinoxsensors.com/products/HID.html
[19]http://www.cse.ohio-state.edu/OTCBVSBENCH/bench.html
[20]http://www.cs.colostate.edu/evalfacerec/algorithms5.htm

