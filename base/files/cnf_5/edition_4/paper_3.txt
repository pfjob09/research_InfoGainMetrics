Cache­Oblivious Scanline Algorithm Design 
Md Mizanur Rahman 
London South Bank University, UK 
rahmanml@lsbu.ac.uk 
Abstract 
This paper introduces the concept of cache­oblivious 
scanline algorithms and their design issues to overcome 
the limitations of cache size and cache line­length within 
existing  workstations,  making  them  suitable  for  the 
visualisation  of  very  large  3D  data  sets.  Unlike  the 
standard  RAM  model,  the  cache­oblivious  model  can 
tune  cache  parameters  without  knowing  them  to 
minimize  cache  misses  and  data  movement  among 
multiple levels of caches. 
Keywords­­­  scanline  algorithms,  cache­oblivious 
model, RAM model and divide and conquer approach. 

1. Introduction 
Scanline algorithms have a long history in computer 
graphics [7], [8], [10], [19]. For the visualisation of large 
3D data sets, visibility computations should be as fast as 
possible.  An  efficient  approach  is  using  scanline 
algorithms  [8],  [19],  rather  than  implementing  exact 
visibility algorithms. A scanline is a row of pixels of the 
image. A system using scanline algorithms is based on a 
simple  client­server  architecture,  where  a  number  of 
servers  support  typically  one  client.  The  client  is 
responsible  for  the  display  of  image,  and  each  server is 
responsible  for  the  calculation  of a  couple  of  scanlines, 
and  the  delivery  of  the  results  back  to  the  client. Using 
the  cache  memory  of  the  server  machines  effectively 
with  scanline  algorithms  and  ensuring  that  frequently 
accessed  data  has  the  opportunity  to  reside  in  cache 
memory,  the  running  time  of  client  processes  can  be 
reduced significantly. 
In  the  cache­oblivious  model,  data  moves  among 
multiple levels of the cache effectively without knowing 
the  characteristics  of  the  memory  system.  If  the  data 
movement  is  minimal  between  the  CPU  and  the  cache 
during  processing,  the  system  performance  increases 
minimizing the idle time while the CPU waiting for the 
memory system. 
The  random  access  model  (RAM)  is  one  of  the 
popular computational models, but the model considers a 
‘flat’ memory of unlimited size and uniform access times 

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

to all memory locations. The model is unable to capture 
the increasing impact that memory systems have on the 
behaviour  of  present  computer  programs.  Rahman  and 
Raman  [17]  pointed  out  that  the  RAM  model  does  not 
take into account the memory hierarchy found in modern 
computers.  They  argue  that  the  cost  of  accessing  main 
memory  can  be  60  times  greater  than  the  cost  of 
operation on two operands in CPU registers. Considering 
arithmetic  and  logic  operations  taken  on  input  data  are 
inadequate  models  for  processors  with  instruction­level 
parallelism  (ILP):  a  program  with  more  arithmetic 
operations  but  more  predictable  branches  or  less  cache 
misses  can  be  faster  than  another  program  with  less 
arithmetic  operations  but  also  less  predictable  branches 
or  more  cache  misses.  As  traditional  RAM  model  and 
any  other  models  of  computation  cannot  capture  ILP 
concepts including branch predictability and other issues 
such  as  locality  of  cache  reference  and  multiple  cache 
hierarchy,  we  need  another  model.  For  addressing  the 
above issues, an asymptotic analysis is also inevitable, in 
particular  for  fundamental  graphics  algorithms  and 
applications,  such  as  line  segment  generation,  clipping 
and visibility scanline algorithms [9], [10], [19]. 
This  paper  introduces  cache­oblivious  scanline 
algorithms  and  techniques  for  the  visualisation  of  very 
large  data  sets.  Unfortunately,  most  commonly  used 
visibility  algorithms  were  not  designed  with  multiple 
level of cache in mind. The main difference between the 
work in this paper and previous work related to visibility 
scanline  algorithms  is  that  we  focus  on  cache 
performance  as  well  as  design  issues of cache­oblivious 
scanline algorithms. This paper is organized as follows: 
Section  2  introduces  the  concept  of  the  cache­oblivious 
model,  Section  3  introduces  cache­oblivious  scanline 
algorithms  including  tools  and  techniques.  Section  4 
discusses asymptotic analysis of cache­oblivious scanline 
algorithms  for  the  visualisation  of  very  large  data  sets. 
Finally,  in  Section  5  the  importance  of  cache­oblivious 
scanline  algorithms  is  discussed,  and  directions  for 
further work are suggested. 

2. Cache­oblivious model 
In 1999 Frigo et. al. [11] presented a new model for 
memory  systems, introduced cache­oblivious algorithms

and  showed  that  the  model  is  faster  than  traditional 
memory  approaches  due  to  the  better  memory  usage. 
Cache­oblivious algorithms can work on most machines 
with  multi­level  cache  hierarchies.  The  principle  of  the 
algorithms is the optimal use of internal cache memory, 
increasing  cache  hit  and  reduction  of  latency caused by 
memory  access.  Using  cache  memory  effectively  could 
reduce visibility computation’s running time. It is a two­ 
level model with the assumption that parameters M and B 
are unknown to the algorithm, where M is the size of the 
cache  at  the  faster  level  and  B  is  the  size  of  a  block  of 
data that transferred from slower to faster level. Without 
any  memory  specific  parameterisation  or  without 
specifying  the  parameters  M  and  B,  a  cache­oblivious 
algorithm  can  be  efficient  for  the  whole  memory 
hierarchy. The model is built on some basic assumptions 
such  as  optimal  replacement  policy,  locality  of  memory 
reference,  inclusion  and  coherence  property  and  cache 
associativity (described below). An ideal cache­oblivious 
model has been illustrated in Figure 1. 
Faster 
but smaller size 

Slower 
but bigger size 

Cache 
misses  Level 2 
Level 1 
CPU 
Cache 
Cache
Size=M  Size = B 
Cache lines 
block transfer 

Main 
Memory 

Figure 1: The ideal (2 levels) cache­oblivious model 
Due  to  the  huge  improvement of technology, CPUs 
are  rapidly  getting  faster  and  faster  and  memories  are 
getting larger and larger [13]. However, as memory gets 
larger,  it  is  still  slower  relative  to  the  faster  CPU.  In 
order  to  make  the  memory  faster,  to  keep  up  with  the 
speed  of  CPU,  few  levels  of  cache  memory  are inserted 
between  the  main  memory  and  the  CPU  as  cache 
memory (or simply called cache). The first level of cache 
(level 1) is placed closest to the CPU and performs faster 
than the second level (level 2) of cache. The second level 
of cache is large enough to capture many of the accesses 
to store most of the data needed during the execution of 
the processor’s program. The number of level of cache or 
memory hierarchy depends on the computer architecture; 
for  example,  Intel  Itanium  has  7  levels  in  its  memory 
hierarchy.  The  level  of  cache  places further and further 
away from the CPU, the size of the storage increases as 
well as the access time or memory latency. 
Due to having several cache levels, memory latency 
could  be  a  drawback  for  some  high­end  systems.  If  the 
requested  data  is  not  available  at  a  level  that  causes 
cache  misses,  all  the  way  through  the  hierarchy  incurs 
latency  corresponding  to  the  sum  of  latencies  at  all 
levels.  Depending  on  the  degree  of  ILP,  modern  CPUs 
could  minimize  memory  latency  by  reading  data  from 
several  levels  simultaneously  during  CPU  waiting  time 
[22]. 
Memory  words  are  stored  in  a  cache  and  are 
grouped into small pages, called cache blocks or lines. A 

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

cache block is transferred between two adjacent levels of 
cache.  When  data  that  is  needed  by  a  process  is  in  the 
cache, a cache hit occurs. When data is not available in 
the cache, a cache miss occurs. In this case, the requested 
data is brought into cache and some existing data in the 
cache is removed to make space where necessary and the 
scheme is called caching. The cache can store up to M/B 
blocks data, for a total size of M elements. The cache is 
assumed to be tall, that is, the size of a cache line is no 
wider than the number of lines the cache can contain in 
total  [22].  A  cache  memory  is  called  tall  when  M  =  B 2 
and in practice caches are usually tall. 
In  case  of  a  cache  miss,  which  block  will  be 
replaced  is  determined  by  the  associativity  and  the 
replacement  policy  of  the  cache  level.  The  associativity 
restricts the number of cache lines in which a particular 
block can reside. There are three general formats for the 
mapping  of  a  block  to  the  cache  based  on associativity: 
direct  mapped,  fully  associative  and  set  associative. 
Different  algorithms  have  been  proposed  to  bring  the 
block  that  is  not  available  in  the  cache  as  replacement 
policies.  Some  of  these  are  random  replacement,  least 
recent used (LRU), first in first out (FIFO) and modified 
bit [13], [18]. 
A  cache  miss  is  very  expensive  in  terms  of  speed 
and  can  be  reduced  by  designing  algorithms  that  use 
locality  of  memory  reference,  inclusion  and  coherence. 
An  application  does  not  access  all  of  its  data  or  large 
amounts  of  data  at  once  with  equal  probability.  The 
locality  of  reference  [14],  [22],  predicts  the  availability 
of  data  and  logical  memory  addresses,  which  is  very 
important to speed up the process. It can be classified as 
temporal and spatial locality of reference. If some data or 
memory  locations  are  accessed,  then  temporal  locality 
ensures  that  there  is  a  high  probability  of accessing the 
same memory locations or data within a short interval of 
time. On the other hand, the spatial locality ensures that 
there is a high probability of accessing memory locations 
or  data  close  to  one  that  has  recently  been  accessed 
within  a  short  interval  of  time.  The  inclusion  property 
indicates data probability is less at the level next to CPU 
than  to  the  next  level  due  to  the  limitation  of size. The 
coherence property ensures the consistency of data at the 
higher  level  of  the  memory  hierarchy.  If  a  word  is 
modified  in  the  cache,  copies  of  that  word  must  be 
updated at all higher levels of the memory. 
Cache­oblivious  algorithms  are  becoming  popular 
among  researchers  of  external  memory  algorithms, 
parallel  algorithms,  data  structures  and  other  related 
topics. If an algorithm fails to use the memory properties 
effectively, the algorithm will be slow no matter what the 
real  world  computer  environment  is.  The  memory 
hierarchy  will  be  bigger  and cache miss penalty will be 
higher in the future. As a result, algorithm designers and 
programmers need several design issues keeping in mind 
and  more  intelligent  algorithms  to  use  cache  memory 
effectively  to  ensure  that  frequent accessed data has the 
opportunity  to  reside  in  cache  and  programs  run  faster 
than just directly coding an algorithm. 

3. Design of cache­oblivious scanline 
algorithms 
Increasing  cache  hit  is  very  important  in  terms  of 
speed.  After  receiving  user  instructions,  cache  memory 
or  cache  blocks  can  be  updated  and  marked  by 
displaying  flag  bit  to  be  benefited  from  the  cache 
property, called locality of reference. Cache memory can 
use such properties to increase cache hit or speed up the 
visibility computation process [18], as it deals with only 
limited number of allocated scanlines. The data needs to 
be executed many times, such as a point or pixel of a line 
segment that intersects a scanline rather than all pixels of 
a  scanline,  can  be  kept  in  the  cache  memory,  possibly 
faster level of the hierarchy. If data, e.g., endpoints of a 
line segment, are available at faster level i, that data must 
also  be  available  at  slower  level  i  +  1  of  the  cache 
hierarchy. 
Scanline algorithms are suitable for the visualisation 
of  medium­sized  data  sets.  Considering  the  concepts  of 
cache­oblivious algorithms, scanline algorithms could be 
suitable  to  handle  very  large  or  massive  data  sets 
ensuring to increase cache hit than they do in the present 
form.  So  increasing  the  number  of  cache  hit  is  very 
important  for  the  visualisation  of  very  large  data  sets. 
Algorithm  designers,  who  want  to  design  practical 
algorithms  and  write  programs  for  this  purpose,  it  is 
necessary  to  implement  cache­oblivious  algorithms 
carefully  and  precisely  at  the  algorithm  design  and 
programming  levels.  They  should  keep  the  following 
issues  in  mind  during  design  and  implementation  of 
cache­oblivious scanline algorithms: 
(1)  Breaking  down  a  problem  into  several  simple 
and smaller sub­problems for visibility computation 
to  reduce  the  data  movement  between  different 
levels of the cache hierarchy. 
(2)  All  the  pixels  associated  with  all  line  segments 
along a scanline can be kept in cache memory rather 
than  keeping  a  part  of  the  whole  model,  if  the 
machine  is  responsible  for  the  processing  of  one 
scanline.  The  most  frequently  used  data  can  be 
brought possibly to the level 1 cache to increase the 
chance of cache hit. 
(3)  If  data  is  not  available  in  the  cache  next  to  the 
CPU,  blocks  of  data  associated  with  line  segments 
or  visibility  computation,  will  be  replaced  by 
optimal  and  sophisticated  replacement  algorithms, 
e.g., least recent used (LRU) and modified bit [18]. 
An optimal replacement algorithm can look into its 
sequence  of  future  requests  by  analyzing  past  data 
patterns  used  and  evict  the  cache  line  that  is 
accessed furthest in the future. 
(4) Real world caches are either directed­mapped or 
2­way  associative  [6].  The  limited  associativity  in 
the mapping from main memory addresses to cache 
sets can significantly degrade running time. 
(5) TLB and BTB caches are usually small with 128 
to 256 entries [14] and can be implemented as fully 
associative.  The  limited  number  of  entries  could 
incur high penalty and easily lead to thrashing. 

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

(6) Consider the ratio between the data set size and 
the cache size and possibly other factors. 
Many scanline algorithms use some kind of sorting 
methods  to  solve  the  visibility  problem.  Implementing 
efficient sorting algorithms (such as tiled mergesort and 
multi­mergesort  with  TLB  padding)  to  exploit  cache 
locality  could  be  an  efficient  approach  to  reduce  cache 
misses on high­end systems [25]. 
The  algorithms  should  determine  how  many  main 
memory  words  are  mapped  into  cache  memory  words 
effectively.  The  mapping  between  cache  memory  and 
main memory plays an important role to increase system 
performance  [18].  Direct  mapping  could  be suitable  for 
increasing cache hit while each machine deals with only 
fixed  number  of  scanlines.  Finding  a  block  within  the 
cache, only one block needs to be checked. The block set 
associative  or  combinational  form  of  direct  and 
associative  mapping  could  be  also  suitable  to  use 
memory effectively [18]. In a fully associative cache, all 
the  blocks  in  the  cache must be checked to determine a 
block  that  contains  the  information  requested. If two or 
more main memory blocks map to a cache block and are 
referenced  in  temporal  proximity,  the  accesses  will 
become costlier than they are assumed in the model (also 
known as cache interference problem [23]. 
Translation  look­aside  buffers  (TLB)  contain  the 
most recent address translations from virtual to physical 
addresses. Due to temporal locality of reference, the TLB 
translates the virtual address into a physical address and 
hands  the  physical  address  over  to  the  level  1  cache 
quickly. If the branch address is not in the branch target 
buffer (BTB) cache (because the program is executed for 
the first time, or the branch was executed long time ago, 
and  hence  dropped  from  the  BTB)  then  static  branch 
prediction  will  take  place.  In  static  branch  prediction 
unconditional  and  backward  conditional  branches  are 
predicted  as  taken,  and  forward  conditional  branches 
predicted  as  not  taken.  Some  branches  are  more 
predictable  than  others,  and  different  processors  have 
different branch prediction abilities even within the same 
family. For example, a branch at the end of simple loop 
is easily predictable by any processor. The inner loop of 
a  scanline  algorithm  can  take some of its branches in a 
regular  pattern,  which  is  still  well  predictable  on  the 
Pentium  II.  Branches  based  on  random  input,  however, 
cannot reasonably be predicted. 
Unlike the cache­oblivious model, external­memory 
algorithms  are  based  on  the  knowledge  of  the  memory 
structure  and  size  can  be  used  for  the  increasing  of  the 
cache  hit  purposes,  which  makes  it  difficult  to  move 
applications from one system to another [16]. The basis 
of  the  theory  of  cache­oblivious  algorithm  is  that  the 
algorithms do not need any knowledge of the underlying 
memory  structure  or  internal  main  memory.  When  a 
cache­oblivious  algorithm  takes  the underlying memory 
structure or cache parameters in to account, it is called a 
cache­aware algorithm.

2.2. Algorithm design tools and techniques 
In  order  to  exploit  multi­level  of  cache  hierarchy 
fully and design cache­oblivious scanline algorithms, we 
should  design  algorithms  that  do  not  depend  on  the 
parameters of the hierarchy such as cache size and cache­ 
line  length  but  could  tune  the  parameters  for  optimal 
performance.  One  of  the  advantages  for  algorithm 
designer is that, if the algorithm works well between two 
levels  of  memory  hierarchy,  then  it  must  automatically 
work  well  between  any  two  adjacent  levels  of  the 
memory  hierarchy.  The  algorithm  can  scale  to  take 
advantage of larger cache line sizes. 
Designing a cache­oblivious algorithm where tuning 
cache  parameters  without  knowing  them  is  not  a 
straightforward task. Suppose we need to examine all the 
pixels  of  a  scanline  in  a  set.  On  a  traditional  RAM 
model,  a  procedure  requires  O(N)  time  for  N  elements. 
On an external memory model, it requires N/B blocks of 
size  B  where  the  number  of  block  transfers  is  N/B.  In 
order  to  achieve  a  similar  bound  in  the  cache­oblivious 
model,  we  can  store  the  elements  of  the  set  in  a 
contiguous level of cache in any order and implement the 
traversal  by  scanning  the  elements  one­by­one  in  the 
ordered  they  are  stored  where  the  procedure  doesn’t 
require the knowledge of B. 
The  divide  and  conquer  approach  works  well  with 
cache  hierarchies  and  parallel  computers.  On  the  other 
hand,  most  of  the  scanline  algorithms  use  divide  and 
conquer  and  recursive  approaches  to  solve  the visibility 
computation problems [7], [8], [9], [19], [20]. The divide 
and conquer approach is to split the line segments along 
a  scanline  into  several  sub­problems  until  each  sub­ 
problem  can  be  solved  easily,  independently  and 
efficiently.  The  same  approaches  can  be  used  to  design 
cache  memory  because  they  adhere  to  the  principle  of 
locality of reference, inclusion and coherence properties. 
The  algorithm  divides  and  conquers  a  problem  into 
several  sub­problems,  at  some  point  of  time,  the  sub­ 
problems fit inside faster cache level, M and subsequent 
recursion,  fits  the  sub­problems  into  a  block,  B.  The 
frequent  used data of the sub­problems can be kept into 
faster  level  or  level  1  so  that  data  transfer  can  be 
minimized between level 2 and level 1 cache hierarchy. 
We  need  an  optimal  strategy  to  divide  a  problem 
into  several  small  sub­problems  to  reduce  processing 
cost.  For  example,  a  problem  size P, cache size M, and 
block  size  B  where  P  is  bigger  than  M  or  B.  For 
simplicity,  the  problem size P is divided into P1  and P2 
to solve it easily and independently. Assume that P1  and 
P2  are  smaller  than  P  or  cache  size  M  but  bigger  than 
block size B. For further simplicity, P1  is divided into P12 
and P22  where each ones is smaller than B. Now assume 
that P2, P12  and P22  are suitable size for processing. For 
an  optimal  dividing  case,  we  need  less  number  of 
recursions  to  fit  data  into  cache  for  processing,  and the 
capacity  of  the  problem  is  usually  larger  than  a  block 
size but smaller than cache capacity. Here P2  is smaller 
than  M  but  suitable  to  fit  in  the  cache.  It  needs  one 
recursion only. With more data in the cache like P2, the 

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

chances  of  the  data  required  residing  in  the  cache 
increases,  thus  decreasing  the  cache  miss  rate.  Data 
occupying more blocks in the cache might increase cache 
misses  in  future  if  the  probability  of  using  the  newly 
fetched  data  in  the  newer  future  becomes  less  than  the 
probability  of  using  the  data  that  was  just  evicted  from 
the  cache  to  make  room  for  the  new  data.  Transferring 
big  size  of  data  in  to  level  1  could  make  cache  slower, 
thus  jeopardizing  the  CPU  clock  rate.  In  worst  case,  a 
problem size may be equal or less than the block size but 
it takes more recursions to fit in the cache for processing. 
Here P12 or P22 is smaller than B and suitable to fit in the 
cache.  Both  of  these  sub­problems  take  two  recursions 
that  is  comparatively  expensive.  In  this  case,  data 
transfer is faster, taking advantage of the small block size 
and fast RAM access methods [12]. So problem size and 
number  of  recursion  are  important  consideration  issues 
for the design of cache­oblivious scanline algorithms for 
high performance systems. 
For the Z­buffer scanline algorithm [10], [19], [20], 
all the pixels of a scanline can be placed into an array of 
N  elements.  In  the  ideal  cache  model,  the  visibility 
computation causes at worst N/B + 1 cache misses where 
B is the block size or each cache line contains B words. 
In  order  to  minimize  the  cache  misses,  the  algorithm 
repeatedly divides the array of N elements into multiple 
sub­arrays until each sub­array can be solved in the level 
1  cache  easily  and  independently  to  exploit  temporal 
locality. 
For the Merge scanline algorithm [2], [19], [20], an 
array of line segments is repeatedly scanned and divided 
into two sub­arrays each of half the size until each sub­ 
array  is  optimal  to  transfer  level  1  cache  to  exploit 
temporal locality. It is important to reuse such sub­arrays 
in  the  cache  as  much  as  possible.  The  base  case  is 
reached  when  the  sub­arrays  only  contain  one  line 
segment  each,  and  therefore  can  be  merged  and 
computed visibility easily. The same technique could be 
employed  for  Warnock  scanline  algorithm  [19],  [20], 
[24], where an array of pixels for a scanline is used. 
For the Random and Trapezoid scanline algorithms 
[9],  [19],  [20],  as  a  problem  the  visualisation  area  is 
divided  into  three  smaller  areas  and  examined  them 
whether  they  can  be  fit  in  the  cache  and  solved 
independently until the visibility computation is solved. 
The  Z­tree  scanline  algorithm  [7],  [19],  [20], 
represents  all  the  pixels  of  the  whole  scanline  in  the 
visualisation area as hierarchical or binary tree structure. 
The  parent  node  should  be  available  in  the  level  1  to 
traverse the whole tree. In worst case, each leaf node can 
store  into  one  block.  For  optimal  performance,  all 
parents’ children at a level fit in a single cache line and 
align them with cache lines depending on the size of the 
tree. 
The  priority­queue  and  BSP  scanline  algorithms 
[10], [19], [20], use a heap data structure that provide the 
insert  and  remove  operations  efficiently.  The  N  heap 
elements  are  stored  in  array  elements  0,  1,  2,  …,  N  ­1 
where the root is in position 0. The tree has depth log (N 
+1) and all levels of the heap are accessed independently.

The  cache  is  divided  into  regions  based  on  the  heap’s 
structure.  All  parents’  children  at  a  level  fit  in  a single 
cache  line  to  reuse  them  as  much  as  possible.  A  heap 
could  keep  cache  misses  minimum  or  does  not  predict 
any cache misses until the size of the heap grows larger 
than the cache size. The size of the heap also depends on 
cache line and input size. Using a heap with a fanout of 
A/r where A is the size of the cache line and r is the size 
of each node of the tree would result in maximum cache 
line utilization. For example, the fanout of a heap would 
be four if the cache line has 32 bytes and each node has 8 
bytes.  Increasing  the  heap  fanout  from  two  to  four 
provides a large reduction in cache misses and number of 
page  accesses  or  execution  times,  and  increasing  from 
four  to  eight  reduces  the  misses  and  execution  times 
further  [15].  Figure  2  illustrates  the  layout  of  a  4­heap 
when 4 heap elements fit per cache line and the array is 
padded to cache­align the heap. 
Following  the  divide and conquer approach, cache­ 
oblivious  algorithms  will  often  exhibit  cache 
complexities  that  are optimal within a constant factor  – 
specially when divide and conquer cost is dominated by 
the  leaf  cost  [6].  Cache  parameters  tuning  could  make 
code  portability  difficult.  Due  to  lack  of  such  tuning, 
cache­oblivious  algorithms  may  not  work  well.  For 
example, if trimming the base case of a recursion is not 
appropriate,  high  performance  will not be achieved.  In 
order  to  overcome  the  problem,  it  is  necessary  to 
determine  an  effective  compiler  strategy  for  trimming 
base cases automatically. 
0 
1 
5 

2 

3 

4 

Cache 
line 

6    7   8  9   10   11   12 

Cache line 

Cache line

block 0            block 1           block 2            block 3 
0 

1  2   3   4 

5  6   7   8 

9  10 11 12 

Siblings 
Figure  2:  A  4­heap  where  four  elements  fit  per 
cache  line  and  the  array  is  padded  to  cache  align 
the heap. 
Padding  technique  can  be  used  to  reduce  or 
eliminate  cache  misses  [25].  Using  padding  at  the 
algorithm design level, the data layout modification can 
be  done  at  run­time  by  system  software  [4],  [26]  or 
compile­time by compiler optimisation [21]. Scanning an 
array  of  N  elements  stored  in  a  contiguous  segment  of 
memory  arbitrarily  aligned  with  memory  blocks  may 
cost  more  memory  transfer  than  N/B.  Bentley’s  array­ 
reversal  algorithm  [3]  can  be  used  to  make  optimal 
blocks transfer. The algorithm makes two parallel scans, 

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

one  from  each  end  of  the  array  and  at  each  step  swaps 
the two elements under consideration. 

4. Asymptotic analysis 
A  prototype  scanline­based  distributed  system  has  been 
demonstrated  where  several  servers  support  a  client 
processing  specific  number  of  scanlines  [5],  [19].  A 
server does not need to scan sequentially and repeatedly 
a  large  data  set  that  can  not  be  stored  in  the  cache 
entirely,  and  could  avoid  capacity  cache  misses. 
Allocating  limited  number  of  scanlines,  based  on  the 
standard  cache  size  (not  knowing  the  actual  cache  size 
and  cache  line  length)  to  localize  the  cache  used  by  a 
stage  in  execution  is  effective  for  the  visualisation  of 
very large data sets. 
To  measure  the  complexity  of  a  cache­oblivious 
algorithm where input size is n, we can measure running 
time  or  work  complexity  W(n)  that  is  conventional 
running time in a RAM model [1]. We can use the cache 
complexity  Q(n;  Z,  L)  function  to  count  the  number  of 
cache misses where Z is the size and L is the line length 
of  the  idle  cache.  The  goal  for  creating  a  good  cache­ 
oblivious  algorithm  is  to  match  the  work  complexity  of 
optimal  RAM  model  with  cache  complexity  Q(n) 
minimizing from Q(n; Z, L) where Z and L are clear from 
context. 
For  more  precise  predictions  of  running  times  for 
visibility computation, we need asymptotic analysis that 
resembles  contemporary  memory systems  more  closely. 
Using  elapsed­time  measurement  tool  with  just­in­time 
compiler (and ignoring limitations on Pentium family of 
processors),  we  measured  clock  cycles  for  an  empty 
loop, such as 
for (i = 0; i < 1000000; i++); 
takes  only  2  or  2.5 clock cycles per iteration depending 
on  its  memory  location.  Tuning  cache  parameters  and 
looking  into  the  sequences  of  future  requests  of  data 
carefully  in  advance,  the  cache­oblivious  model  could 
minimize  data  movement  between  CPU  and  cache  and 
reduce  running  time  significantly.  For  this  purpose, 
experimental  scanline  algorithms  evaluation  is 
inevitable. 
Though  the  RAM  model  and  its  variants  are 
successfully used even in parallel computation (e.g., the 
CREW  PRAM)  it  is  often  needed  to  compare  scanline 
algorithms  with  the  cache­oblivious  asymptotic  time 
complexity,  in which case the RAM model is no longer 
adequate. 

5. Conclusions 
Deeper  understanding  of  how  cache  behaviour 
affects  the  execution  performance  is  an  important  task 
for designing cache­oblivious scanline algorithms for the 
visualisation  of  very  large  data  sets.  We have discussed 
issues  and  techniques  related  to  the  cache­oblivious 
model  to  design  asymptotically  optimal  cache­oblivious 
scanline  algorithms  for  the  visualisation  of  very  large 
data sets in a modern memory system. 

3D visibility problems have already been popular in 
both computer graphics and computation geometry. The 
scanline approach solves visibility computation problems 
in  addition  to  the  load  balancing  and  communication 
bottleneck  problems.  Adding  the  cache­oblivious  model 
with  scanline  algorithms  could  speed  up  the  visibility 
computation process, and can handle very large data sets 
for  the  visualisation  efficiently  and  with  low  cost.  For 
better  performance,  cache­oblivious  algorithms  can  be 
combined with cache­aware algorithms. 
The  most  promising  directions  for  further  research 
are  in  the  applications  of  cache­oblivious  scanline 
algorithms,  performance  measurement  between  cache­ 
oblivious  and  cache­aware  algorithms,  performance 
measurement of scanline algorithms to compute visibility 
problems,  hardware  and  software  shared­memory 
systems and grid computing. 

Acknowledgements 

[9] 

[10] 

[11] 

[12] 
[13] 

[14] 
[15] 

The author thanks Dr Frank Dévai and Dr Martin Bush 
for the improvement of the presentation of the paper. 

[16] 

References 

[17] 

[1] 

[2] 

[3] 
[4] 

[5] 

[6] 

[7] 

[8] 

L.  Arge,  M.  A.  Bender,  E.  D.  Demaine,  B.  Holland­ 
Minkley  and  J.  I.  Munro.  Cache­Oblivious  Priority 
Queue  and  Graph  Algorithm  Applications. Proceedings 
of  the  34 th  ACM  Symposium  on  Theory  of  Computing. 
ACM Press. 268 – 276. 2002. 
M.  J.  Atallah,  R.  Cole  and  M.  T.  Goodrich.  Cascading 
divide­and­conquer  –  a technique for designing parallel 
algorithms. SIAM J. Computing. Vol.  18, No. 3, 499 – 
532. 1989. 
J.  Bentley.  Programming  Pearls.  Addision­Wesley.  Inc. 
Second Edition, 2000. 
B.  Bershad,  D.  Lee,  T.  Romer  and  B.  Chen.  Avoiding 
conflict  misses  dynamically  in  large  direct­mapped 
caches. Proceedings of the 6 th  International Conference 
on  Architectural  Support  for  Programming  Languages 
and Operating Systems (ASPLOS­VI). October 1994. 
S.  Dalal  and  M.  M.  Rahman.  An  Efficient  Scanline­ 
Based  System  for  the Visualisation of Large Data Sets. 
Proceedings  of  the  International  Conference  on 
Computer  Graphics,  Imaging  and  Vision  (CGIV05). 
Beijing, China, 26 – 29. July 2005. 
E.  D.  Demaine.  Cache  Oblivious  Algorithms  and  Data 
Structures. Preliminary lecture notes – handed out at the 
EEF  Summer  School  on  Massive  Data  Sets,  BRICS, 
University of Aarhus, June 2001. 
F.  Dévai.  Approximation  algorithms for high­resolution 
display.  In  Proc.  PIXIM’88,  First  International 
Conference on Computer Graphics in Paris, B. Péroche 
(ed.), France, 121 – 210. 1988. 
F.  Dévai.  Scan­line  methods  for  parallel  rendering.  In 
High­Performance  Computing  for  Computer  Graphics 
and Visualisation. M. Chen, P. Townsend, and J. A. 
Vince (eds.), Springer Verlag. 88 – 98. 1996. 

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

[18] 
[19] 

[20] 

[21] 

[22] 

[23] 

[24] 

[25] 

[26] 

F. Dévai. On the Computational Requirements of Virtual 
Reality  Systems.  In  State  of  the  Art  Reports. 
Eurographics'97. Budapest, Hungary, 59 – 92. September 
1997. 
J. D. Foley, A. van Dam, S. K. Feiner and J. F. Hughes. 
Computer  Graphics:  Principles  and  Practice. Addison­ 
Wesley. Second Edition in C, Reading, MA, USA, 1996. 
M.  Frigo,  C.  E.  Leiserson,  H.  Prokop  and  S. 
Ramachandran. 
Cache­oblivious 
algorithms. 
Proceedings  of  the  40 th  Annual  Symposium  on 
Foundations  of  Computer  Science.  IEEE  Computer 
Society Press. 285 – 297. 1997. 
J.  P.  Hayes.  Computer  Architecture  &  Organization. 
Second Edition, 1988. 
J.  L.  Hennessy  and  D.  A.  Patterson.  Computer 
Architecture:  A  Quantitative  Approach.  The  Morgan 
Kaufmann series in Computer Architecture and Design. 
Third Edition, 2002. 
P. Kumar. Algorithms for Memory Hierarchies. Springer 
Berlin/Heidelberg. 193 – 212. 2003. 
A.  LaMarca  and  R.  E.  Ladner.  The influence of caches 
on  the  performance  of  sorting.  Proceedings  of  the  8 th 
Annual ACM­SIAM Symposium on Discrete Algorithms. 
370 – 379. January 1997. 
J. H. Olsen and S. C. Skov. Cache­Oblivious Algorithms 
in Practice. Department of Computer Science, University 
of Copenhagen, December 2002. 
N.  Rahman  and  R.  Raman.  Adapting  radix  sort  to  the 
memory  hierarchy.  In  ALENEX00  2nd  Workshop  on 
Algorithm Engineering and Experiments. San Francisco, 
CA, USA, 131–146. January 2000. 
M. M. Rahman. Microprocessor and Peripherals. Systech 
Publications. Dhaka, 249 – 255. 2002. 
M. M. Rahman. A scanline­based distributed system for 
the visualization of large data sets. SIAM Conference on 
Geometric Design and Computing. Seattle, Washington, 
USA, 469 – 480. 2003. 
M.  M.  Rahman.  Scanline  Algorithms  for  the 
Visualisation  of  Large  Data  Sets.  SIAM  Conference  on 
Geometric  Design  and  Computing.  Phoenix,  Arizona, 
USA, 2005. 
C.  Rivera  and  C.  W.  Tseng.  Data  transformations  for 
eliminating  conflict  misses.  Proceedings  of  the 
SIGPLAN’98  Conference  on  Programming  Language 
Design and Implementation. July 1998. 
F.  RÆnn.  Cache­Oblivious  Searching  and  Sorting. 
Master’s  Thesis,  Department  of  Computer  Science, 
University of Copenhagen, July 2003. 
O. Temam, C. Fricker and W Jalby. Cache Interference 
Phenomena. In Measurement and Modeling of Computer 
Systems. 261­ 271. 1985. 
J. E. Warnock. A hidden­surface algorithm for computer­ 
generated  halftone  pictures.  Computer  Science 
Department, University of Utah, USA, TR, 4 – 15. 1969. 
L.  Xiao,  X.  Zhang  and  S.  A.  Kubricht.  Improving 
Memory  Performance  of  Sorting  Algorithms.  ACM 
Journal on Experimental Algorithmics, Vol. 5, 2000. 
Y. Yan, X. Zhang and X. Zhang. Cacheminer: a runtime 
approach  to  exploit  cache  locality  on  SMP.  IEEE 
Transactions on Parallel and Distributed Systems. Vol. 
11, No. 4, 357 – 374. 2000.

