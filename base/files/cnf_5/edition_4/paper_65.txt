Constructing Accurate Fuzzy Classification Systems:
A New Approach Using Weighted Fuzzy Rules
S.M. Fakhr Ahmad a, M. Zolghadri Jahromi b
Department of Computer Engineering, School of Engineering,
Islamic Azad University of Shiraz, Shiraz, Iran
b
Department of Computer Science &Engineering, School of Engineering,
Shiraz University, Shiraz, Iran
mfakhrahmad@cse.shirazu.ac.ir , zjahromi@shirazu.ac.ir
a

Abstract
Different approaches to design fuzzy rule-based
classification systems can be grouped into two main
categories: descriptive and accurate. In the descriptive
category, the emphasis is on the interpretability of the
resulting classifier. The classifier is usually represented
by a set of short fuzzy rules (i.e., with a few number of
antecedent conditions) that make it a suitable tool for
knowledge representation. In the accurate category, the
generalization ability of the classifier is the main target
in the design process and no attempt is made to use
understandable fuzzy rules in constructing the rule base.
In this paper, we propose a simple and efficient method
to construct an accurate fuzzy classification system. We
use rule-weight as a simple mechanism to tune the
classifier and propose a new method of rule-weight
specification for this purpose. Through computer
simulations on some data sets from UCI repository, we
show that the proposed scheme achieves better
prediction accuracy compared with other fuzzy and nonfuzzy rule-based classification systems proposed in the
past.
Keywords--- Fuzzy systems, Classification, Ruleweight, Generalization Accuracy

1. Introduction
Fuzzy rule-based systems have been widely applied
to various application areas such as classification. A
Fuzzy Rule-Based Classification System (FRBCS) is a
special case of fuzzy Rule-Based systems where the
output of the system is crisp and discrete. Different
classifiers can be grouped into two main categories:
descriptive and accurate. The main advantage of
descriptive classification systems in comparison with
their counterparts is their interpretability. The classifier
is usually represented by a set of short fuzzy rules (i.e.,
with a few number of antecedent conditions) that make it
a suitable tool for knowledge representation. However,
the main goal of many designers, who want to develop

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

accurate classification systems (either fuzzy or nonfuzzy), is to maximize the generalization accuracy of the
classifier. In these approaches no attempt is made to
improve the understandability of the system.
The fuzzy if-then rules used for classification
problems have commonly the following form:
Rule Rj: If x1 is Aj1 and ... and xn is Ajn then Class Cj
with CFj , j Є 1,2,...,N ,
(1)
where x = (x1,..., xn ) is an n-dimensional pattern vector,
Aji is an antecedent linguistic value such as young and
old ( i Є 1,2,..., n) , Cj is a consequent class (i.e., one of
the given c classes), N is the number of fuzzy if-then
rules, and CFj is the certainty grade of the rule Rj which
usually has a real value in the unit interval [0, 1] (i.e., 0 ≤
CFj ≤1 ).
It has already been shown that various methods of
rule weighting have a significant effect on the
classification performance of fuzzy and non-fuzzy
systems [1,2].
Among non-fuzzy approaches, the k-nearest
neighbor (K-NN) rule is a simple and accurate pattern
classification algorithm. However, the problems emerge
in cases that patterns of different classes overlap in some
regions in the feature space. Many researchers have
already developed various adaptive or discriminate
metrics to improve the performance of the K-NN. In [3],
Wang et al. showed that a simple adaptive distance
measure significantly improves the performance of the kNN.
In this paper, we propose a simple and efficient
method to construct an accurate fuzzy classification
system. We use rule-weight as a simple mechanism to
tune the classifier and propose a new method of ruleweight specification for this purpose. Having an initial
rule-base for a problem, our learning method assigns a
weight to each rule in the rule-base. The base of the
learning algorithm is a process that finds the optimal
decision area for each rule according to the target classes
of training patterns. In other words, it moves the
boundaries of decision areas so as to the majority of the

training patterns within an area be of the same class. This
leads to the minimal error rate of the classifier over
training data. Using this learning mechanism, the
generalization accuracy of the classifier will also be
improved, significantly.
In this approach, the number of rules does not
grow exponentially as the number of features and fuzzy
sets increase. The number of rules at most equals the
number of training patterns and thus, no process for rule
subset selection is needed.
Rule weighting is rarely used in research works on
FRBCSs (e.g. [2]). Instead, in many cases, they modify
membership functions of antecedent fuzzy sets using
numerical data, which involves learning a number of
parameter values for each membership function. Thus,
rule weighting is a much easier approach that improves
the performance of the classifier without changing the
position of fuzzy sets given by domain experts.
The rest of this paper is organized as follows. In
Section 2, a brief introduction for FRBCS is given. In
Section 3, the rule weight learning mechanism is
discussed. Section 4 is devoted to the method of finding
the best boundaries for the decision area of a typical rule.
The experimental results are presented in Section 5.
Finally, a conclusion is given at the end of the paper.

2. Fuzzy Rule-Based Classification Systems
(FRBCS)
A Fuzzy Rule-Based Classification System
(FRBCS) is composed of three main conceptual
components: database, rule-base and reasoning method.
The database describes the semantic of fuzzy sets
associated to linguistic labels. Each rule in the rule-base
specifies a subspace of pattern space using the fuzzy sets
in the antecedent part of the rule. The reasoning method
provides the mechanism to classify a pattern using the
information from the rule-base and database. Different
rule types have been used for pattern classification
problems [3]. We use fuzzy rules of the following type
for an n-dimensional problem.
Rule Rj: If x1 is Aj1 and … and xn is Ajn then class h with
(2)
CFj
where, X=[x1, x2, …, xn] is the input feature vector,
h ∈ [C1, C2 …, CM] is the label of the consequent class,
Ajk is the fuzzy set associated to xk , CFj is the certainty
grade (i.e. rule weight) of rule Rj and N is the number of
fuzzy rules in the rule-base.
In order to classify an input pattern Xt = [xt1, xt2, …,
xtn], the degree of compatibility of the pattern with each
rule is calculated (i.e., using a T-norm to model the
“and” connectives in the rule antecedent). In case of
using product as T-norm, the compatibility grade of rule
Rj with the input pattern Xt can be calculated as:

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

n

µ j (X t ) = ∏ µ A (x ti )
i =1

(3)

ji

In the case of using single winner reasoning method,
the pattern is classified according to consequent class of
the winner rule Rw. With the rules of form (1), the winner
rule is specified using:

w = arg max{µ j (X t ) CFj , j = 1,..., N }

(4)

Note that the classification of a pattern not covered
by any rule in the rule-base is rejected. The classification
of a pattern Xt is also rejected if two rules with different
consequent classes have the same value of µ(Xt).CF in
equation (3).
In case of using weighted vote [4] as the reasoning
mechanism, each fuzzy rule gives a vote for its
consequent class. The strength of the vote given by each
rule can be defined as the product of compatibility grade
and certainty grade. The total strength of the vote for
each class can be calculated as follows.

N



σClass T (X t ) = ∑µj (X t )CFj | R j ∈S , Consequent (R j ) =ClassT 
 j =1


(5)

Where, S represents the rule-base. In this case, a test
pattern Xt is classified as the class having maximum total
strength.

3. Rule-base construction
For an M-class problem in an n-dimensional feature
space, assume that m labeled patterns Xp=[xp1, xp2, …,
xpn], p=1, 2, …, m from M classes are given. A simple
approach for generating fuzzy rules is to partition the
domain interval of each input attribute using a prespecified number of fuzzy sets (i.e., grid partitioning),
denoted by k. Some examples of this partitioning (using
triangular membership functions) are shown in Fig. 1.
Given a partitioning of pattern space, one approach
is to consider all possible combination of antecedents to
generate the fuzzy rules. The selection of the consequent
class for an antecedent combination (i.e. a fuzzy rule)
can be easily expressed in terms of confidence of an
association rule from the field of data mining [5]. A
fuzzy classification rule can be viewed as an association
rule of the form A j ⇒ class C j , where, Aj is a multidimensional fuzzy set representing the antecedent
conditions and Cj is a class label. Confidence (denoted
by C) of a fuzzy association rule Rj is defined as [6]:

∑

C (A j ⇒ class C j ) =

µ j (X p )

X p ∈ class C j

(6)

m

∑µ

j

(X p )

p =1

Where, µj(Xp) is the compatibility grade of pattern Xp
with the antecedent of the rule Rj, m is the number of
training patterns and Cj is a class label. The consequent
class Cq of an antecedent combination Aj is specified by
finding the class with maximum confidence. This can be
expressed as:

q = arg max{C ( A j ⇒ class h | h = 1,2,..., M } (7)
Note that, when the consequent class Cq can not be
uniquely determined, the fuzzy rule is not generated.
The problem with grid partitioning is that for an ndimensional problem, kn antecedent combinations should
be considered. It is impractical to consider such a huge
number of antecedent combinations when dealing with
high dimensional problems.
In order to prevent from exponential growth of the
rule-base, we just generate rules that have at least one
training pattern in their decision areas. Since the decision
areas of different rules do not overlap, the number of
rules will be at most equal to the number of training
patterns.

3.1. The proposed method for rule weighting
Initially, all rules are assumed to have a weight of
one (i.e. CFk=1, K=1,2,...,N). In this section we propose
an algorithm to assign some real numbers (in the interval
[1,∞)) as the rule weights using the training patterns. The
rule-weighting process for a typical rule, Ri, can be
organized into the following steps:
1. Specify the center of the rule's covering area, sort
the training patterns within this area in ascending
order of their distance from the center (The first
pattern in the sorted list is the most compatible one
with the rule).
2. Scan the patterns through the sorted list until a
pattern (Xn) from the negative class (any class
except the rule's target class) is met (The enemy
pattern with maximum compatibility degree is
found).
3. Call the pattern just before the enemy pattern in the
list X* and Find its compatibility with the rule Ri
(µi(X*)).
4. Compute the rule's weight (CFi ) using the following
equation:
CFi = 1/ µi (X*)

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

(8)

This algorithm obtains a real number in the interval
[1,∞) as the weight of each rule. However, in this issue,
two exceptional cases may occur:
1. The first pattern in the sorted list is an enemy pattern.
For this case, we set the value of 1 to µi(X*) and thus
the rule's weight will not change from 1.
2. There is no enemy pattern in the covering area of the
rule (i.e., an interesting case). For this case, we chose
the compatibility degree of the last pattern in the
sorted list for µi (X*), i.e., µi(X*) = µi(last pattern).
Since the last pattern has the minimum compatibility
with the rule, a higher weight is given to such rules.
In this method, no rule is given a weight of 0. Thus,
the number of rules does not change through this
weighting process. As the number of partitions of each
feature increases, the performance of the system
approaches the performance of weighted K-NN method,
while having the extra advantage of interpretability,
especially for low-dimensional data sets.

4. Finding the optimal decision boundaries
The method proposed in Section 3, increases the
classification accuracy of each fuzzy rule over training
data up to 100%. This is accomplished by tuning the
boundaries for the decision area of each rule through
assigning a weight to it. It can be predicted that the
generalization ability of the classifier will be improved,
too. However, there is really no reason that we will get
the optimal results for generalization accuracy, through
this issue. The main reason refers to some probable noisy
or exceptional patterns or in case of data sets with highly
ovelapped classes. Our proposed method can easily
become more flexible (for noisy data) by making a small
change to it. After determining a threshold for the rule
accuracy, we do not stop the scanning of the sorted list
when meeting the first enemy pattern. Instead, we extend
the decision boundary of the rule until we reach an
enemy pattern that makes the rule's accuracy become less
than the specified threshold. In other words we let a few
number of enemy patterns to exist in the decision area of
each rule. This can be more effective for noisy-nature
data sets.
To find the best accuracy threshold for a typical data
set, different threshold values can be tested through the
discussed method and the optimal value will be obtained
by comparing the generalization accuracies of different
cases.

5. Experimental results
In order to evaluate the performance of the proposed
scheme, we used the data sets shown in Table 1 available
from UCI ML repository. To construct an initial rulebase for a specific data set, a number of equi-length rules
(number of antecedents equaling to the number of
features), having at least one training pattern in their
decision areas were generated. In order to assess the
effect of the proposed scheme in comparison with its

alternatives, we used 10CV technique which is a case of
n-fold cross validation.
In the first part of the experiment, the generalization
accuracy of the initial rule-base (before rule weighting)
was measured.
In the second part, our rule-weighting method was
evaluated without considering any accuracy threshold, as
discussed in Section 4. In other words, the threshold was
set to 1. The results, shown in Table 2 narrate from a
positive effect for the weighting method over the
generalization ability.
Finally, in the third part, for each data set, we tried
different values of the accuracy threshold by changing it
from 1 down to 0.4 (by the step size of 0.05). Using the
LVO (Leave One Out) technique, in each case, the error
rate was measured using training data. The best threshold
(leading to the best result) was then selected to evaluate

the generalization ability over that data set using the
10CV technique. The results of this part are shown in
Figures 1.a through 1.f. These figures indicate the error
rates of the classifier for each data set through different
values of the threshold (The threshold is denoted by θ).
The straight line in each figure represents the error
rate of the classifier without rule weighting and has been
used to easily see the effect of rule weights in different
cases. The error rates of the classifier using the optimal
value of θ for each data set are presented in Table 2. In
this table, our proposed method is also compared with
another successful rule-based method as benchmark
results called C4.5 reported by Elomaa and Rousu [7].
As shown in Table 2, except in one case, the proposed
classifier in this paper results in better classification
rates, compared to the best results already achieved by
C4.5.

Table 1 Some statistics of the data sets used in our computer simulations

Number of
attributes
4
13
5
60
6
8
9

Data set
Iris
Wine
Thyroid
Sonar
Bupa
Pima
Glass

Number of patterns

Number of
classes
3
3
3
2
2
2
6

150
178
215
208
345
768
214

7
Error rate (%)

6
5
4

Weighted

3

Non-w eighted

2
1
0
0.4

0.5

0.6

0.7

0.8

0.9

1

Threshold (θ)

a) Iris
8

Error rate (%)

7
6
5

Welghted

4

Non-w eighted

3
2
1
0
0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

Threshold (θ)

b) Wine

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

0.8

0.85

0.9

0.95

1

36
Error rate (%)

35.5
35
Weighted

34.5

Non-w eighted

34
33.5
33
0.4

0.5

0.6

0.7

0.8

0.9

1

Threshold (θ)

c) Glass
40
39
Error rate (%)

38
37

Weighted

36

Non-w eighted

35
34
33
0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

1

Threshold (θ)

d) Bupa
28

Error rate (%)

27.5
27
26.5

Weighted

26

Non-w eighted

25.5
25
24.5
24
0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

1

Threshold (θ)

e) Pima

Error rate (%)

12
10
8

Weighted

6

Non-w eighted

4
2
0
0.4

0.45

0.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

1

Threshold (θ)

f) Sonar
Figure 1 Error rates of the proposed classifier using different threshold values and
comparison with non-weighted rule-base classifier for data sets of Table 1

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Table 2 Classification Error rates of the proposed classifier using the optimal threshold
values and comparison with threshold of 1, non-weighted rule-base classifier and the C4.5
method for data sets of Table 1

Data sets
Iris
Wine
Pima
Bupa
Thyroid
Glass
Sonar

No. Weight
5
6.1
27.8
39
8.5
35
11.2

Error Rates (%)
Weighted rules
θ=1
Optimal θ
3.6
6.3, 5.2
25.7
37.8
4.1
34.9
5

Conclusions
In this paper, we proposed a simple and efficient
method to construct an accurate FRBCS. We proposed a
new method of rule-weight specification in order to tune
the classifier. In this method, the number of generated
rules at most equals the number of training patterns and
thus, no process for rule subset selection is needed. This
number does not change through the weighting process,
since no rule is given the weight of zero. As the number
of partitions of each feature increases, the generalization
ability of the system competes and even precedes the
weighted K-NN method. Moreover, the proposed scheme
is a FRBCS and has the advantage of interpretability,
especially for low-dimensional data sets.
We also proposed a mechanism to find the optimal
rule weights, which is much more useful in case of noisy
or highly overlapped data sets, in order to prevent from
overfitting of the learned classifier.
We used seven data sets from UCI-ML repository to
assess the performance of the learning scheme.
Simulation results on thsese data sets showed that the
method can be used to construct a rule-base with a good
generalization ability. The effect of rule weights could be
seen, clearly, through this experiment. We also showed
that the proposed method is more effective in reducing
the error rate of the classifier in comparison with C4.5 as
a successful rule-based method.

References
[1]
[2]
[3]

S. Abe, R. Thawonmas, A fuzzy classifier with
ellipsoidal regions, IEEE Trans. on Fuzzy Systems 5 (3)
(1997) 358-368.
D. Nauck, R. Kruse, A neuro-fuzzy method to learn
fuzzy classification rules from data, Fuzzy Sets and
Systems 89 (3) (1997) 277-288.
O. Cordon, M. J. del Jesus, F. Herrera, A proposal on
reasoning methods in fuzzy rule- based classification
systems, International Journal of Approximate Reasoning
20 (1999) 21-45.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

3.6
5.1
24.3
37.8
4.1
33.3
4.1

[4]
[5]
[6]

[7]
[8]
[9]

[10]

[11]
[12]
[13]
[14]

[15]

[16]

C4.5
(best results)
5.1
5.6
25
38.2
6.7
27.3
23.3

Ishibuchi, H., Nakashima, T., and Morisawa, T.: Voting
in Fuzzy Rule-Based Systems for Pattern Classification
problems, Fuzzy Sets and Systems 103 (1999) 223-238.
R. Agrawal, R. Srikant, Fast algorithms for mining
association rules, in: Proc. 20th International Conference
on Very large Databases, 1994, pp. 487-499.
H. Ishibuchi, T. Yamamoto, Fuzzy rule selection by
multi-objective genetic local search algorithms and rule
evaluation measures in data mining, Fuzzy Sets and
Systems 141 (1) (2004) 59-88.
Elomaa, T. and J. Rousu (1999). “General and efficient
multisplitting of numerical Attributes,” machine
Learning 36, 201-244.
Arima, M., Hara, E.H., Katzberg, J.D., "A fuzzy logic and
rough sets controller for HVAC systems", Proceedings of
the IEEE WESCANEX, New York, pp. 133–138, 1995.
Cordón, O., Herrera, F., Peregrín, A., "Applicability of
the fuzzy operators in the design of fuzzy logic
controllers", Fuzzy Sets and Systems, Vol. 86, No. 1, pp.
15–41, 1997.
Glorennec, P.Y., "Application of fuzzy control for
building energy management. In: Building Simulation":
International
Building
Performance
Simulation
Association 1. Sophia Antipolis, France, pp. 197–201,
1991.
Bárdossy, A., Duckstein, L., "Fuzzy rule-based modeling
with applications to geophysical", biological and
engineering systems, CRC Press,1995.
Bezdek, J.C., Pal, S.K., "Fuzzy Models for Pattern
Recognition, Methods that Search for Structures in
Data", IEEE Press, Boca Raton, 1992.
Chi, Z., Yan, H., Pham, T., "Fuzzy algorithms with
applications to image processing and pattern
recognition", World Scientific, New York, 1996.
S. Abe, M. S. Lan, A method for fuzzy rules extraction
directly from numerical data and its application to pattern
classification, IEEE Trans. on Fuzzy Systems 3 (1995)
18-28.
H. Ishibuchi, K. Nozaki, H. Tanaka, Distributed
representation of fuzzy rules and its application to pattern
classification, Fuzzy Sets and Systems 52 (1) (1992) 2132.
S. Mitra, L. I. Kuncheva, Improving classification
performance using fuzzy MLP and two-level selective
partitioning of the feature space, Fuzzy Sets and Systems
70 (1) (1995) 1-13.

