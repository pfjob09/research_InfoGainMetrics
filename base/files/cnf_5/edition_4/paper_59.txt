Enhancement of Neuro-eigenspace Face Recognition Using Photometric
Normalization
Shahrin Azuan Nazeer1, Marzuki Khalid2, Nazaruddin Omar3, Mat Kamil Awang4
1,3,4
Telekom Research & Development Sdn Bhd, 2Universiti Teknologi Malaysia
{1shahrin,3nazar, 4kamil}@tmrnd.com.my, 2marzuki@citycampus.utm.my
Abstract
A face recognition system based on recent method
which concerned with both representation and
recognition using learning algorithm is presented. The
learning algorithm, artificial neural network is used as
a classifier for face recognition and face verification
whereas the features are extracted using linear subspace techniques. This paper initially provides the
overview of the proposed face recognition system, and
explains the methodology used. It then explains the
performance evaluation of the proposed system by
applying two photometric normalization techniques:
Histogram equalization and Homomorphic filtering,
and comparing with Euclidean Distance and
Normalized Correlation classifiers. The system
produces promising results for face verification and
face recognition where it achieved False Acceptance
Rate (FAR) of 2.98% and False Rejection Rate (FRR)
of 2.59% using ANN classifier with PCA feature
extraction using homomorphic filtering, and 94.4% for
recognition.

1. Introduction
Face recognition systems (FRS) are still in their
infancy and many types of algorithms and techniques
have been proposed to improve the ability of these
systems. Artificial neural networks (ANNs) have been
commonly used as the classifiers for FRS whereas
principal component analysis and linear discrimination
analysis have been used widely as the feature
extractors. However, many current FRS still
experiencing low accuracy rates using these techniques
due to factors such as occlusion, illumination and other
disturbances.
In this paper, we similarly used the popular backpropagation neural network (BPNN) as the classifier
for our FRS as it has proven to be simple for
implementation. In addition we use the extractors of
the face features based on the well known PCA and

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

LDA techniques. In order to increase our accuracy
rate, we apply photometric normalization techniques
consisting of histogram equalization and homomorphic
filtering as the pre-processing module of our FRS.
The objective of this paper is to present the
performance evaluation of a FRS that we have
developed based on the above techniques compared
with the template based matching techniques. Our
proposed FRS is referred to as “KenalMuka” and is
shown in Figure 1. By comparing the results, we found
that using the ANN based classifier and the
photometric normalization techniques, the performance
of “KenalMuka” improved significantly. This paper is
organized as follows. Section 2 describes the system
process flow and the modules of the proposed face
recognition system. Section 3 elaborates the
methodology used for the preprocessing, feature
extraction, and classification of the proposed system.
Section 4 presents and discusses the experimental
results and the conclusions are drawn in section 5.

2. System Overview
The proposed face recognition system consists of
two (2) phases which are the enrollment and the
recognition/verification phases as depicted in Figure 2.
It consists of several modules which are Image
Acquisition, Face Detection, Training, Recognition and
Verification.

2.1. Enrollment phase
The image is acquired using a web camera and
stored in a database. Next, the face image is detected
and trained. During training, the face image is
preprocessed using geometric and photometric
normalization. The features of the face image are
extracted using several feature extraction techniques.
The features data is then stored together with the user
identity in a database.

comprises of several modules which are image
acquisition, face detection, and face recognition or
verification.

Figure 1. User interface for “KenalMuka”

Enrollment

Recognition / Verification

Image Acquisition

Face Detection

Training
Preprocessing
Feature
Extraction

Recognition

Verification

Preprocessing

Preprocessing

Feature
Extraction

Feature
Extraction

Classification

Classification

Stored
Template

Threshold

Client ID

Pass/Fail

Figure 2. Block diagram for “KenalMuka”

2.2. Recognition/verification phase
A user’s face biometric data is once again acquired
and the system uses this to either identify who the user
is, or verify the claimed identity of the user. While
identification involves comparing the acquired
biometric information against templates corresponding
to all users in the database, verification involves
comparison with only those templates corresponding to
claimed identity. Thus, identification and verification
are two distinct problems having their own inherent
complexities. The recognition/verification phase

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

2.2.1. Image Acquisition/Face Detection Module. A
real-time face detection system [1] is used to detect
face and to extract the pertinent information related to
facial features from the acquired image. The detected
face image will then be resized and corrected
geometrically for recognition/verification where the
scenes unrelated to face will be eliminated. It is also
robust against illumination variance and works well
with different skin color and occlusions such as beards,
moustache and with head cover. The outputs of the
system are the rectangle which contains face features,
and image which contains the extraction of the
detected face features.
2.2.2. Face Recognition/Verification Module. The
face recognition module comprises of preprocessing,
feature extraction, and classification sub-modules. The
input to the face recognition/verification module is the
face image, which is derived from two sources: from
the camera and from the database. From these sources,
each image is preprocessed to get the geometric and
photometric normalized form of the face image.
During feature extraction, the normalized image is
represented as feature vectors. The result of the
classification for the recognition purpose is determined
by matching the client index with the client identity in
the database.

3. Approaches
3.1 Image Preprocessing
The purpose of the pre-processing module is to
reduce or eliminate some of the variations in face due
to illumination [5,10,12]. It normalized and enhanced
the face image to improve the recognition performance
of the system. The preprocessing is crucial as the
robustness of a face recognition system greatly
depends on it. By performing explicit normalization
processes, system robustness against scaling, posture,
facial expression and illumination is increased. The
photometric normalization consists of removing the
mean of the geometrically normalized image and
scaling the pixel values by their standard deviation,
estimated over the whole cropped image. The
photometric normalization techniques applied are
histogram equalization, and homomorphic filtering.
3.1.1
Histogram
Equalization.
Histogram
equalization is the most common histogram
normalization or gray level transform, which purpose

is to produce an image with equally distributed
brightness levels over the whole brightness scale. It is
usually done on too dark or too bright images in order
to enhance image quality and to improve face
recognition performance. It modifies the dynamic
range (contrast range) of the image and as a result,
some important facial features become more apparent.
The steps to perform histogram equalization are as
follow:

very close to the original reflectance. That is, color
constancy results as the color of the surface is not
affected much by the color illumination. The steps of
this algorithm are as follow:

Step 1: For an N x M image of G gray-levels, create
two arrays H and T of length G initialized
with 0 values.

Step 2: Carry 2D Fourier transform of the signal :

Step 2: Form the image histogram: scan every pixel
and increment the relevant member of H-- if
pixel X has intensity p, perform

L(u, v)∆FL' ( x, y ) = FR' ( x, y ) + FI ' ( x, y)∆R(u, v) + I (u, v)

H [ p] = H [ p] + 1

(1)

Step 3: Form the cumulative image histogram Hc, use
the same array H to store the result.

(2)

for p = 1, …, G -1.
Step 4: Set
T [ p] =

G −1
H [ p]
MN

L ' ( x, y )∆ log L ( x, y ) = log[ R( x, y ) I {x, y )]

(3)

L'(x,y)=R'(x,y) + I'(x,y)
(5)

where R(u,v), I(u,v) and L(u,v) are the Fourier spectra
of the corresponding spatial signals R'(x,y), I'(x,y) and
L'(x,y), respectively.
Step 3: Suppress low frequency components in Fourier
domain
(6)

where H(u,v) is a filter in the frequency domain whose
entries corresponding to the low frequencies are
smaller than 1 (suppression of low-frequency
components, the illumination) while the rest entries are
1 to keep the high-frequency components in the signal
(mostly the reflectance) unchanged.
Step 4: Take inverse Fourier transform

Rescan the image and write an output image with graylevels q, setting q = T[p].

L'(x, y)∆F −1[H(u, v)L(u,v)]= F−1[H(u, v)R(u, v)]+ F −1[H(u,v)I (u, v)]

3.1.2 Homomorphic Filtering. Homomorphic
filtering algorithm is similar to that of Horn's algorithm
except the low spatial frequency illumination is
separated from the high frequency reflectance by
Fourier high-pass filtering. In general a high-pass filter
is used to separate and suppress low frequency
components while still passing high frequency
components in the signal, if the two types of signals
are additive, i.e., the actual signal is the sum of the two
types
of
signals.
However,
in
this
illumination/reflection
problem
low-frequency
illumination is multiplied, instead of added, to the
high-frequency reflectance. To still be able to use the
usual high-pass filter, the logarithm operation is
needed to convert the multiplication to addition. After
the homomorphic filtering process, I(x,y), the
processed illumination should be drastically reduced
due to the high-pass filtering effect, while the
reflectance R(x,y) after this procedure should still be

Step 5: Take exponential operation

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

(4)

= log R ( x, y) + log I ( x, y) ∆R ' ( x, y ) + I ' ( x, y )

H(u,v)L(u,v)=H(u,v)R(u,v)+H(u,v)I(u,v)

H [0] = H [0]

H [ p ] = H [ p − 1] + H [ p]

Step 1: Take logarithm of the input light signal:

(7)

∆R'(x, y) + I '(x, y)

L ( x , y ) ∆ exp[ L ' ( x , y )] = exp[ R ' ( x , y ) + I ' ( x , y )]

(8)

= exp[ R ' ( x , y )] exp[ I ' ( x , y )] ∆ R ( x , y ) I ( x , y )

3.2 Feature Extraction
The purpose of the feature extraction is to extract
the feature vectors or information which represents the
face. The feature extraction algorithms used are
Principal Component Analysis (PCA), and Linear
Discriminant Analysis (LDA).
3.1.1 Principal Component Analysis (PCA). PCA for
face recognition is used in [2,3,4,6] is based on the
information theory approach. It extracted the relevant
information in a face image and encoded as efficiently
as possible. It identifies the subspace of the image
space spanned by the training face image data and de-

correlates the pixel values. The classical representation
of a face image is obtained by projecting it to the
coordinate system defined by the principal
components. The projection of face images into the
principal component subspace achieves information
compression, de-correlation and dimensionality
reduction to facilitate decision making. In
mathematical terms, the principal components of the
distribution of faces or the eigenvectors of the
covariance matrix of the set of face images, is sought
by treating an image as a vector in a very high
dimensional face space. The detailed explanation is
provided in [7,13,16].
3.1.2. Linear Discriminant Analysis (LDA). LDA is
used in machine learning to find the linear combination
of features which best separate two or more classes of
object or event, where the resulting combinations are
used as a linear classifier [8,11,14]. It is also
considered as feature reduction, mapping a
multidimensional space into a space of fewer
dimensions, prior to later classification. LDA is used
in a number of classification related applications. One
of these is face recognition where each face, which
consists of a large number of pixels, is reduced to a
smaller set of linear combinations prior to
classification. The linear combinations obtained using
LDA are referred to as Fisherfaces. Linear Disriminant
Analysis (LDA) is used for face recognition in [9],
where face image retrieval is based on discriminant
analysis of eigenfeatures. The LDA is the projection
of a face image into the system of fisherfaces
associated with nonzero eigenvalues, which will yield
a representation which emphasize the discriminatory
content of the image. LDA selects the linear subspace
Φ which maximizes the ratio:

Φ T SbΦ

(9)

Φ T SW Φ
Sb =

1 c
( µ k − µ )( µ k − µ ) T
∑
c k =1

where c is the number of clients, M is number of
training face images, xi , µ is the grand mean, and

µ k is the mean of class Ck .
Intuitively, LDA finds the projection of the data in
which the classes are most linearly separable.

3.3 Classification
The purpose of the classification sub-module is to
map the feature space of a test data to a discrete set of
label data that serves as template. The classification
techniques used are, Artificial Neural Network,
Euclidean Distance and Normalized Correlation.
3.3.1 Artificial Neural Networks (ANN). ANN is a
machine learning algorithm that has been used for
various pattern classification problems such as gender
classification, face recognition [9], and classification of
facial expression. ANN classifier has advantages for
classification such as incredible generalization and
good learning ability. The ANN paradigm that is used
in this application is BPNN. BPNN is a form of nonlinear network consisting of a set of inputs (forming
the input layer), followed by one or more hidden layers
of non-linear neurons and an output layer of non-linear
neurons as shown in Figure 3. BPNN is an ideal
means of tackling a whole range of difficult tasks in
pattern recognition and regression because of its highly
adaptable non-linear structure. In order to train the
network to perform a given tasks the individual

(w )

weights ij for each neuron are set using a supervised
learning algorithm known as the error-correction backpropagation algorithm, which involves repeatedly
presenting the network with samples from a training
set and adjusting the neural weights in order to achieve
the required output by using momentum and adaptive
learning as in equation (13).

(10)

is the between-class scatter matrix, and

Sw =

1
M

c

∑ ∑ (x

i

− µ k ) ( xi − µ k ) T

(11)

k =1 i| xi ∈Ck

the within-class scatter matrix,
Figure 3. BPNN Architecture

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

The momentum will help us to go through any small
local minima and speed up convergence.
Standard:

∆w(t ) = −η∇E (t )

Momentum: ∆w(t ) = −η∇E (t ) + α∆w(t − 1)

(12)
(13)

where α is the momentum term.
The adaptive learning rate is used to prevent
overshooting by adjusting the learning rate
dynamically.
3.3.2 Euclidean Distance(ED). The Euclidean
distance is the nearest mean classifier which is
commonly used for decision rule is denoted as [15]:

d E ( x, wk ) = ( x − wk )T ( x − wk )

(14)

where the claimed client is accepted if d E ( x, wk ) is
below the threshold τ Ek and rejected otherwise.
3.3.3 Normalized Correlation (N.C.). The normalized
correlation decision rule based on the correlation score
denoted as:

d C ( x, wk ) =

x T wk
x wk

(15)

where the claimed identity is accepted if d C ( x, wk )
exceeds the threshold τ Ck .

4. Experiments
The purpose of the experiments are to evaluate the
performance of the face recognition system by
applying two (2) photometric normalization
techniques, which are homomorphic filtering and
histogram equalization to the face images.
The local dataset consists of 400 images which
consist of 20 persons each with 20 images with 50 by
50 pixels. The images are well-aligned frontal view,
and are divided into two parts: training set and testing
set. 5 images per person are used for training and 15
images per person are used for testing.
The performance measures are based on verification
and recognition rates. For verification, two measures
are used, which are the false acceptance rate (FAR)
and false rejection rate (FAR). FAR is the case when
an impostor, claiming the identity of a client, is

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

accepted, whilst FRR is the case when a client
claiming his true identity is rejected. The FAR and
FRR are given by:
FAR = IA/I,

FRR = CR/C

(16)

where IA is the number of impostor accepted, I is the
number of impostor’s trials, CR is the number of client
rejected and C is the number of client’s trials. For
recognition, the measure is based on the recognition
rate (RR):
RR = RI/TI

(17)

where RI is the number of image recognized, TI is the
number of image’s trials.
The experiments for face verification and face
recognition are conducted based on five cases: case 1
using original images, case 2 applying histogram
equalization to face images, case 3 applying
homomorphic filtering to face images, case 4 applying
combination of histogram equalization and
homomorphic filtering to face images, and case 5
applying combination of homomorphic filtering and
histogram equalization to face images.
For face verification, the experimental results for
each of the cases are tabulated in Table 1, Table 2,
Table 3, Table 4, and Table 5, respectively. From the
results, the ANN classifier with PCA feature extractors
using homomorphic filtering produces the highest
verification rates with FAR of 2.98% and FRR of
2.59%.
Meanwhile for face recognition, the results for each
of the cases are tabulated in Table 6, Table 7, Table 8,
Table 9, and Table 10, respectively. From the
experimental results, we found that the recognition rate
using the ANN based classifier with PCA feature
extractor and homomorphic filtering is 94.44%
improves significantly as compared to that of using the
original images with 90.0%. Thus, applying
photometric normalization improves the verification
and recognition rates for the ANN based classifier.

5. Conclusions
The paper has presented an enhancement of neuroeigenspace face recognition system using photometric
normalization. By comparing the experimental
results, we found that using photometric
normalization techniques, the performance of the
proposed neuro-eigenspace face recognition system
improved significantly. Thus, applying photometric
normalization techniques on the face images give

much impact to the performance of the proposed face
recognition system.

Table 1. Verification result using original
image
Feature
Extractor

Classifier

PCA

E.D.
N.C.
N.N
E.D.
N.C.
N.N

LDA

FAR = FRR (%)
FAR
7.250
14.440
5.820
3.700
10.920
4.550

HTER
(%)

FRR
7.410
15.560
5.560
3.330
10.370
5.190

7.330
15.000
5.690
3.515
10.645
4.870

Table 2. Verification Results using Histogram
Equalization
Feature
Extractor

Classifier

PCA

E.D.
N.C.
N.N
E.D.
N.C.
N.N

LDA

FAR = FRR (%)
FAR
6.970
6.210
4.200
8.890
6.560
5.510

FRR
12.960
5.930
4.440
8.910
6.670
7.040

LDA

Classifier
E.D.
N.C.
N.N
E.D.
N.C.
N.N

FAR = FRR (%)
FAR
FRR
6.580
6.300
7.800
7.410
2.980
2.590
4.200
4.070
6.600
5.930
3.400
3.700

Classifier

PCA

E.D.
N.C.
N.N
E.D.
N.C.
N.N

LDA

FAR = FRR (%)
FAR
9.320
5.750
7.340
6.580
5.250
6.080

FRR
11.850
6.300
7.780
6.670
6.300
6.300

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

LDA

Classifier
E.D.
N.C.
N.N
E.D.
N.C.
N.N

6.070
4.320
8.900
6.615
6.275

Feature Extractor
PCA
LDA

HTER
(%)
10.555
6.025
7.560
6.625
5.775
6.190

HTER
(%)
9.565
5.625
3.920
6.165
4.235
4.925

Classifier
E.D.
N.C.
N.N
E.D.
N.C.
N.N

Recognition (%)
95.19
94.07
90.0
97.78
97.04
84.44

Table 7. Recognition Results using Histogram
Equalization
Feature Extractor
PCA
LDA

HTER
(%)
6.440
7.605
2.785
4.135
6.265
3.550

FAR = FRR (%)
FAR
FRR
6.540
12.590
5.690
5.560
4.140
3.700
6.030
6.300
3.660
4.810
4.660
5.190

Table 6. Recognition Results using Original
image

9.965

Table 4. Verification Results using Histogram
Equalization and Homomorphic Filtering
Feature
Extractor

Feature
Extractor
PCA

HTER
(%)

Table 3. Verification Results using
Homomorphic Filtering
Feature
Extractor
PCA

Table 5. Verification Results using
Homomorphic Filtering and Histogram
Equalization

Classifier
E.D.
N.C.
N.N
E.D.
N.C.
N.N

Recognition (%)
91.48
91.48
91.48
92.96
90.74
87.04

Table 8. Recognition Results using
Homomorphic Filtering
Feature Extractor
PCA
LDA

Classifier
E.D.
N.C.
N.N
E.D.
N.C.
N.N

Recognition (%)
95.19
94.07
94.44
94.44
92.96
89.63

Table 9. Recognition Results using Histogram
Equalization and Homomorphic Filtering
Feature Extractor
PCA
LDA

Classifier
E.D.
N.C.
N.N
E.D.
N.C.
N.N

Recognition (%)
90.74
90.00
87.78
92.96
91.11
88.89

Table 10. Recognition Results using
Homomorphic Filtering and Histogram
Equalization
Feature Extractor
PCA
LDA

Classifier
E.D.
N.C.
N.N
E.D.
N.C.
N.N

Recognition (%)
91.85
91.85
92.59
90.00
92.22
85.56

6. Acknowledgments
This work has been supported by Telekom Research
& Development Sdn Bhd under project number R060658.

7. References
[1] Shahrin Azuan Nazeer, Nazaruddin Omar, Khairol
Faisal Jumari, Marzuki Khalid, “Face Detection Using
Artificial Neural Network Approach’’, Proceedings of the
First Asia International Conference on Modelling &
Simulation (AMS'07),2007.
[2] Stefano Arca, Paola Campadelli, Elena Casiraghi,
Raaella Lanzarotti, “An Automatic Feature Based Face
Authentication System”, 16th Italian Workshop on Neural
Nets(WIRN), 2005, pp.120-126.
[3] Kyungim Baek, Bruce A. Draper, J. Ross Beveridge,
Kai She, “PCA vs. ICA: A Comparison on the FERET Data
Set”, Proceedings of the 6th Joint Conference on
Information Science (JCIS), 2002, pp.824-827.
[4] L. S. Balasuriya, N. D. Kodikara, “Frontal View Human
Face Detection and Recognition”, Proceedings of the
International Information Technology Conference (IITC),
2001.
[5] T. Chen, W. Yin, X.-S. Zhou, D. Comaniciu, T. S.
Huang, "Total Variation Models for Variable Lighting Face
Recognition and Uneven Background Correction", IEEE
Transactions on Pattern Analysis and Machine Intelligence,
vol. 28(9), 2006, pp.1519-1524.
[6] Bruce A. Draper, Kyungim Baek, Marian Stewart
Bartlett, J. Ross Beveridge, “Recognizing faces with PCA
and ICA.” Computer Vision and Image Understanding, vol.
91(1-2), 2003, pp.115-137.
[7] P. J. B. Hancock, V. Bruce and A. M. Burton, "Testing
Principal Component Representations for Faces", Proc. of
4th Neural Computation and Psychology Workshop, 1997.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

[8] Seung-Jean Kim, Alessandro Magnani Stephen P. Boyd,
“Robust Fisher Discriminant Analysis”, Neural Information
Processing Systems (NIPS), 2005.
[9] S. Lawrence, C. L. Giles, A. Tsoi, and A. Back, "Face
recognition: A convolutional neural-network approach,"
IEEE Trans. on Neural Networks, vol. 8, January 1997,
pp.98-113.
[10] Longin Jan Latecki, Venugopal Rajagopal, Ari Gross,
“Image
Retrieval
and
Reversible
Illumination
Normalization”, SPIE/IS&T Internet Imaging VI, vol. 5670,
2005.
[11] Johnny Ng, Humphrey Cheung, “Dynamic Local
Feature Analysis for Face Recognition”, International
Conference Biometric Authentication, (ICBA), 2004, pp.
234-240.
[12] M. Villegas and R. Paredes. “Comparison of
illumination normalization methods for face recognition.”,
In Mauro Falcone Aladdin Ariyaeeinia and Andrea Paoloni,
editors, Third COST 275 Workshop - Biometrics on the
Internet, 2005, pp. 27-30.
[13] Jonathon Shlens, “A Tutorial on Principal Component
Analysis”, Systems Neurobiology Laboratory, Ver.2, 2005.
[14] Javier Ruiz-del-Solar, Pablo Navarrete, “Eigenspacebased Face Recognition: A comparative study of different
approaches”, IEEE Trans. on Sys., Man. & Cyb. C., vol.
16(7), pp.817-830.
[15] Kilian Q. Weinberger, John Blitzer and Lawrence K.
Saul, “Distance Metric Learning for Large Margin Nearest
Neighbor Classification”, Neural Information Processing
Systems (NIPS), 2005.
[16] Wendy S Yambor, Bruce A. Draper J. Ross Beveridge,
“Analyzing PCA-based Face Recognition Algorithms:
Eigenvector Selection and Distance Measures”, Proc. 2nd
Workshop on Empirical Evaluation in Computer Vision,
2000.

