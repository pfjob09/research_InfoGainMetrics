Hopfield Neural Network (HNN) Improvement for Color Image Recognition
Using Multi-Bitplane and Multi-Connect Architecture
Kussay Nugamesh Mutter
Zubir Mat Jafri
Azlan Abdul Aziz
School of Physics,
Universiti Sains Malaysia,
Malaysia
kussaynm@yahoo.com
Abstract
A new approach of using HNN with multi-connect
architecture in color image recognition has been
produced in this work. HNN consists of a single layer
of fully connected processing elements, which is
described as an associative memory. However, HNN is
useless in dealing with data not in bipolar
representation. As such, HNN failed to work directly
with color images, unless, another way is produced in
order to pave the way for expected right recognition.
In RGB bands each represents different values of
brightness, still it is possible to assume for 8-bit RGB
image consists of 8-layers of binaries, or bipolar. In
such way, each layer is as a single binary image for
HNN. The results have shown the possibility and
usefulness of HNN in RGB image recognition. Besides,
the possibility of using wide number of RGB images
stored in the net memory without sensed affection on
the final results.

1. Introduction
Inspired by the structure of the human brain,
artificial neural networks have been widely applied to
fields such as pattern recognition, optimization, coding,
control, etc., because of their ability to solve
cumbersome or intractable problems by learning
directly from data. An artificial neural network usually
consists of a large number of simple processing units,
i.e., neurons, via mutual interconnection. It learns to
solve problems by adequately adjusting the strength of
the interconnections according to input data. Moreover,
the neural network adapts easily to new environments
by learning, and it can deal with information that is
noisy, inconsistent, vague, or probable. These features
have motivated extensive research and developments

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

in artificial neural networks. However, a large number
of neural network paradigms have been developed and
used in the last four decades. One of these widely used
paradigms is HNN, and the HNN is nearest match
finding network. This network alters the input patterns
through successive iterations until a learned pattern
evolves at the output, which is no longer changes on
successive iterations [2], [10].
HNN is described as associative memory which is a
simple single-layer neural network that can learn a set
of pattern pairs (or associations). Still, an efficient
associative memory can store a large set of patterns as
memories. Moreover, during recall, the memory is
excited with a key pattern (also called the search
argument) containing a portion of information about a
particular member of a stored pattern set. This
particular stored prototype can be recalled through
association of the key pattern and the information
memorized [5], [9]. In other words, associative
memories provide one approach to the computerengineering problem of storing and retrieving data
based on content rather than storage address. Since
information storage in a neural net is distributed
throughout the system (in the net’s weights), a pattern
does not have a storage address in the same sense that
if it was stored in a traditional computer. An
associative memory net may serve as a highly
simplified model of human memory. Before training an
associative memory neural net, the original patterns
must be converted to an appropriate representation for
computation. However, not all representations of the
same pattern are equally powerful or efficient. In a
simple example, the original pattern might consist of
“on” and “off” signals, and the conversion could be
“on”Æ +1, “off”Æ 0 (binary representation) or “on”Æ
+1, “off”Æ -1 (bipolar representation) [9]. The main
application of HNN is in pattern recognition that is
able to recognize correctly unclear (noisy) pictures
[14]. A multi-connect architecture of HNN is shown in

figure (1). All the processing elements (neurons or
nodes) are connected in feedback architecture with the
connection weights specified in a certain way.
Moreover, this net is fully interconnected, that is to
say, each unit (node) is connected to every other unit,
and has feedback connections between the units [4].

Figure 1. HNN with multi-path
(multi-connect) architecture

The net consists of n neurons having threshold
values Ti. The feedback input to the i’th neuron is
equal to the weighted sum of neuron outputs vj, where
j=1,2…n. Denoting wij as the weight value connecting
the output of the j’th neuron with the input of the i’th
neuron. The total input neti of the i’th can be expressed
by:
n

neti =

n

∑∑ w

ij

v j + i i − Ti

For i=1,2, … n …(1)

i =1 j=1

The external input to i’th neuron has been
denoted here as ii, and by Introducing the vector
notation for synaptic weights and neuron output,
equation (1) can be rewritten as:
For i=1,2…n ………………...(2)
neti = wiv+ii-Ti
Matrix W, sometimes called the connectivity
matrix, is an n×n matrix containing network weights
arranged in rows of vectors. The weight matrix W in
this model is symmetric, i.e., wij= wji, and with
diagonal entries equal explicitly to zero, i.e., wii= 0. In
other words, no connection exists from any neuron
back to itself. Physically, this condition is equivalent to
the lack of the self-feedback in the nonlinear
dynamical system. If the diagonal elements were not 0,
the net would tend to reproduce the input vector rather
than a stored vector [4], [5]. If an HNN is given by
weights and limiting values, then the network will be
in dynamic equilibrium when one creates a pattern. A
network can define various patterns; one can find them
by different start vectors in the iteration.
Corresponding to the spinglass theory of solid state
physics, such equilibrium functions in HNNs are
characterized by the fact, that the total energy
(Hamilton function) becomes minimum. This leads
here to a “Lyapunov function or Energy function”,
which becomes exactly minimum when one creates a
pattern. This energy function can be defined as follows
[14]:

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

n

1 n n
E = −  ∑ ∑ w ij v i v j + ∑ t k v k  ……..….. (3)
2  i =1
k =1


Where n: the number of elements in the vector v.
wij: the weight from the output of neuron i to
the input of neuron j.
t: limiting (threshold) value, which equal to
zero in Hopfield network.
In the vector form the energy function is written as:

1
E = − ⋅ v ⋅ ( W ⋅ v) + t ⋅ v
or with t=0 we have:
2
1
E = − ⋅ v ⋅ (W ⋅ v) ………………………….…. (4)
2

One can calculate the energy function E for every
input vector, which can be created in the network. If
one calculates the function E for all the possible input
vectors, an energy landscape with maximums and
minimums can be obtained. The point is that the
minimum is taken when the input is a pattern.

2. HNN and Color Image representation
In this work we will try to adapt HNN for ColorLevel images recognition. However, before going on
further into details we need to show some concepts
about image representation. It is concerned with the
representation of input data, which can be measured
from the objects that can be recognized. The input data
is acquired by digitizing images, which are obtained
through standard scanner devices [7], [13]. The digital
image is represented as two dimensional array of data
I(x, y), where each pixel (picture element) value
corresponds to the brightness of the image at the point
(x, y). In linear algebra terms, a two-dimensional array
is referred to as a matrix, and one row (or column) is
called a vector. This image model is for monochrome,
(one-color, this is what we normally refer to as black
and white), image data or Binary images. These images
are the simplest type of images and can take one of two
values, typically black and white, or ‘0’ and ‘1’. A
binary image is referred to as 1 bit/pixel image because
it takes only 1 binary digit to represent each pixel.
These types of images are most frequently used in
computer vision applications where the only
information required for the task is general shape, or
outline information. For color images, one can consider
the Red, Green, and Blue bands as gray representation
consists of 8-bit layers. The digital image is
represented as a two-dimensional array of data I(x, y),
where each pixel (picture element) value corresponds
to the brightness of the image at the point (x, y). Such
Images can be modeled as three-band monochrome

image data, where each band of data corresponds to a
different color. The actual information stored in the
digital image data is the brightness information in each
spectral band. Typical color images are represented as
red, green, and blue or RGB images, and by using the
8-bit monochrome standard as a model, the
corresponding color image would have 24 bits/pixel,
i.e. 8-bit for each of the three color bands (red, green,
and blue). Figure (2) illustrates that, in addition to refer
to a row or column as a vector, we can refer to a single
pixel’s red, green, and blue values as a color pixel
vector (R, G, B). For our purpose, RGB color
information is transformed into mathematical space
that decouples the brightness information from the
color information. After this has been done, the image
information consists of one-dimensional brightness, or
luminance, space and a two-dimensional color space
[6],[13].
Original

Red

Red band layers

Green

Green band layers

Blue

Blue band layers

Figure 2. Bitplanes of 8-bit gray level image.

As mentioned previously, HNN deals with the
bipolar system (i. e. -1 and +1) for direct input data, so
it is useful for binary images, but not useful for graylevel or color images. One can assume that 8-bit RGB
image consists of 8-bitplanes of binaries for each of R,
G, and B bands, and then can be represented as bipolar
data. In this way it is possible to express each bitplane
as single binary image for HNN, see figure(2). Hence,
the 8-bit image consists of 8-bitplanes or 8-subimages
arranged systematically in its proper index in the net
memory (i.e. the net data base) [8].
The general form of equation (1) for Gray images is
as it is seen in [9]:
n

net ik = ∑ w ikjv j + i i

……………………….(5)

j=1

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

here the net for bitplane k consists of 3 neurons. The
feedback input to the i’th neuron is equal to the
weighted sum of neuron outputs vj. wij is the weight
value connecting the output of the j’th neuron with the
input of the i’th neuron. The Energy function of GrayLevel images is then[8]:


1 n n n
E = −  ∑∑∑ w ij v i v j 
2  k =1 i=1 j=1


……………….(6)

In our case, and for RGB images in c-bit bitplane of
pixel brightness depth, equation (5) takes the following
general form:

net ci ,k =

n

∑w

ikj

v j + ii

……………………(7)

j=1

Therefore, energy function ERGB for the new RGB
form of HNN can be rewritten as follows:


1 b n n n
E RGB = −  ∑∑∑∑ w ij vi v j  …………..(8)
2  c=1 k=1 i=1 j=1

Where b: the number of bitplanes in each of R, G,
and B bands.
n: the number of elements in the vector v.
wij: the weight from the output of neuron i to
the input of neuron j.
k: the number of bits in the gray-level image.
The values of the energy function ERGB for all the
possible input vectors can be calculated here for each
bitplane, and then an energy landscape with maximums
and minimums can be obtained.

3. Results and Discussion
The empirical tests of a number of 8-bit gray images
show very good results indicate the range of possibility
of using HNN in b-bit Gray image recognition. A lot
of different 8-bit gray-level images are applied for
learning and converging by the new HNN algorithm.
Figures (3) and (4) show two chosen samples of RGB
8-bit images and their curves of converging ratio
against noising ratio acted randomly on images canvas.
For all of original, Red band, Green band, and Blue
band images, the solid curves represent the converging
ratio for direct pixel to pixel matching, while the
circled curves are for HNN converging. From each

figure, one may notice that the two curves in each
figure start by meeting for low noise ratio then they
diverged for each increase in noising ratio. However,
this depends on the way the HNN works in finding the
nearest vector when a pattern is created in the
minimum energy according to the energy function of
equation (8). Still, the closest fitting equation that
represents these curves is as the following form:

y = ae − bx ……………. (9)
where a and b are constants. We notice from this that
all curves are having the same exponential shape in the
converging process.
120

Original

120
100

Original

100

80

80
60

60
40

40
20

20
0
0

0
0

20

40

60

80

20

40

60

80

100

80

100

100
120

120

Red band

100

Red band

100

80

80

60

60

40

40

20

20

0
0

20

40

60

0
0

20

40

60

80

100
120

100

Green band

100

Green band

90

80

80

60

70
60

40

50

20

40
30

0
0

20

20

40

60

80

100

80

100

10

120

0
0

20

40

60

80

Blue band

100

100
120

80

Blue band

100

60

80

40

60

20

40

0
0

20

40

60

20
0
0

20

40

60

80

100

Figure 3. Converging Ratio vs Noise Ratio (image1)
pixel-pixel matching
HNN Converging

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Figure 4. Converging Ratio vs Noise Ratio (image2)
pixel-pixel matching
HNN Converging

4. Conclusions
Many conclusions are found according to the
results noted bellow:
1. According to relations (7 and 8), one can use HNN
for multiresolution image recognition with multibands
which gives good results.
2. No limitation to the number of 8-bit gray level
images which can be stored in the net memory with the
same efficient results.
3. Using multi-bitplane RGB images never affects the
net efficiency because each image is considered as
binary sub images (bitplanes) stored as weights by
learning process in the net database memory.
4. The possibility of implementing this approach in a
lot of applications, such as, fingerprint identification,
face recognition, voice recognition, multiresolution
environmental images … etc.

5. References
[1]Colin Molter, Utku Salihoglu and Hugues Bersini,
“Storing static and cyclic patterns in an Hopfield
neural network”, Laboratory of artificial intelligence
IRIDIA cp194/6, Universit´e Libre de Bruxelles, 50 av
F.Roosevelt, 1050 Brussels, Belgium, 2003.
[2] Cornelius T. Leondes, “Algorithms and
Architectures, Neural networks systems techniques and
applications”, Academic press, 1998.
[3] Domagoj Kovacevic and Sven Loncaric, "Ct Image
Labeling Using Hopfield Neural Network",
Department of Electronic Systems and Information
Processing, Faculty of Electrical Engineering and
Computing, University of Zagreb, Croatia, 1998.
[4] Imad Issa Abd Al-Kaream, “Hopfield Neural
Network Using Genetic Algorithm”, M.Sc. thesis, High
studies institute for computer and information, Iraq,
2000.
[5] Jacek M. Zurada, “Introduction to Artificial Neural
Systems”, Jaico publishing house, 1996.
[6] Julius T. Tou and Rafael C. Gonzalez, “Pattern
Recognition Principles”, Addison-Wesley publishing
company, 1974.
[7] Kussay N. Mutter, “Hopfield Neural Network and
Genetic algorithm Techniques for Fingerprint Image
Identification and Reconstruction”, M.Sc. thesis, AlMustansiryah University, College of Education,
Department of Physics, Iraq, 2002.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

[8] Kussay N. Mutter, Imad I. Abdul Kaream, and
Hussein A. Moussa, “Gray Image Recognition Using
Hopfield Neural Network With Multi-Bitplane and
Multi-Connect Architecture”, IEEE, Proceedings of the
International Conference on Computer Graphics,
Imaging and Visualisation (CGIV'06), July-2006.
[9] Laurence Fausett, “Fundamental of Neural
Networks,
Architectures,
Algorithms
and
Applications”, Prentice-Hall, 1994.
[10] L. C. Jain and R. K. Jain, “Hybrid Intelligent
Engineering Systems”, World Scientific publishing Co.
Pte. Ltd., 1997.
[11] Rafael C. Gonzalez, and Richard E. Woods,
“Digital Image Processing”, Addison-Wesley
publishing company, 1992.
[12] R. Ahmed, “Wavelet-based Image Compression
using Support Vector Machine Learning and Encoding
Techniques”, ACTA Press, Australia, 2005.
[13] Scott E. Umbaugh, “Computer Vision and Image
Processing: A Practical Approach Using CVIP Tools”,
Prentice Hall PTR, 1998.
[14] Werner Kinnebrock,
Fundamentals, Applications,
publications, Pvt. Ltd., 1995.

“Neural Network,
Examples”, Galotia

