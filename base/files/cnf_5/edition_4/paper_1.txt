Random Projection with Robust Linear Discriminant Analysis Model in Face
Recognition
Pang Ying Han, Andrew Teoh Beng Jin
Faculty of Information Science and Technology, Multimedia University
{yhpang@mmu.edu.my, bjteoh@mmu.edu.my }
Abstract
This paper presents a face recognition technique
with two techniques: random projection (RP) and
Robust linear Discriminant analysis Model (RDM).
RDM is an enhanced version of Fisher’s Linear
Discriminant with energy-adaptive regularization
criteria. It is able to yield better discrimination
performance. Same as Fisher’s Linear Discriminant, it
also faces the singularity problem of within-class
scatter. Thus, a dimensionality reduction technique,
such as Principal Component Analsys (PCA), is
needed to deal with this problem. In this paper, RP is
used as an alternative to PCA in RDM in the
application of face recognition. Unlike PCA, RP is
training data independent and the random subspace
computation is relatively simple. The experimental
results illustrate that the proposed algorithm is able to
attain better recognition performance (error rate is
approximately 5% lower) compared to Fisherfaces.

1. Introduction
Eigenfaces, proposed by Turk and Pentland, has
become a benchmark in face recognition technology
[1]. It is a linear transformation based on Principal
Component Analysis (PCA) that computes a new
subspace with the optimization objective of
maximizing the variation of the face data. PCA is an
optimal tool for data representation due to its perfect
job in data reconstruction from a low dimensional
basis; but, it may not be optimal for discrimination.
The reason is PCA produces projection vectors that
maximize the overall class scatter [2]. Consequently,
Belhumeur introduced a combination of PCA and
Fisher’s Linear Discrimiant (PCA+FLD), known as
Fisherfaces, in the application of face recognition. PCA
is applied for dimensionality reduction to resolve the
singularity problem of within-class scatter of FLD.
Recently, Hu et al. has introduced an improved FLD,
namely Discriminative Semantic Feature (DSF) as a
feature descriptor for text categorization [3]. Similar to

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Fisherfaces, DSF utilizes PCA for dimensionality
reduction to deal with the within-class scatter
singularity problem. However, DFS includes an
energy-adaptive regularization criterion into FLD. This
modified discriminant analysis technique is known as
Robust linear Discriminant analysis Model (RDM).
DSF and Fisherfaces suffer from high computation
due to their dimensionality reduction technique-- PCA.
If the data matrix is a pixel-wise covariance matrix
with size of n dimensional data, the computational
3

complexity is O( n ) . However, from matrix theory, if
the number of training images, N, is smaller than n, the
3

computational complexity will be reduced to O ( N ) .
Even though the complexity is reduced, PCA
computation still suffers from heavy computational
load in cubic order if N is increasing [4]. If we use a
small training dataset to compute the eigen-based
projection subspace by limiting N, there is insufficient
data to generate informative subspace.
This paper presents a face recognition technique
with two contributions: 1. the introduction of RDM to
analysis face data, 2. Random Projection (RP) is used
for dimensionality reduction as an alternative to PCA.
RDM has been reported to be a superior feature
extractor in text categorization [3]. Here, we exploit
RDM in face recognition. RP is a promising
dimensionality reduction technique and no training
image is needed for random matrix computation.
Besides, random matrix computation is simple. Its
complexity is only O ( nd ) , where n is the number of
pixels of each image and d is the projected dimension,
also called as intermediate length, d << n. Therefore,
RP is used as an alternative to PCA for dimensionality
reduction in face recognition system.
The overview of the proposed approach (RP+RDM)
is demonstrated in Figure 1. RP and RDM processes
are discussed in more detail in the following sections.
Experimental results illustrate that RP+RDM is able to
obtain about 5% lower error rate compared to
Fisherfaces. Besides, DSF and RP+RDM accomplish
competitive performance. However, PCA in DSF
suffers from high computation complexity for subspace

construction; but, the generation of random subspace in

RP is computationally simple.

Random
Projection for
dimensionality
reduction

Robust linear
Discriminant
analysis Model

Feature
template

Face Data

Figure 1. The overview of the proposed face recognition system (RP+RDM)

2. Random projection

RT R = I + δ ij

where δ ij = ri T r j ; ri , r j ∈ R, δ ii = 0 for ∀i and I is

Let face data is represented in a set of vectors
Γ i , i = 1, 2,...., N , in a n-dimensional space and N is
the total image. RP plays a role as a transformation
matrix to Γ i to produce Γ i in a lower dimension d,
with d << n. The transformation is described as:
Γ i = RT Γ i
(1)

an identity matrix.
If the random vectors in R is orthogonal, then
δ ij = 0 , R T R = I , so that Γ i = Γ i (i.e. the data
structure is preserved). Unfortunately, it is
computationally heavy in the orthogonalization of R
(step 2). However, Johnson-Linderstrauss lemma states
that if the points in a vector space are projected into a
randomly selected subspace of sufficiently highdimensionality, the distances between the points are
approximately preserved as there are many nearlyorthogonal vectors in a high-dimensional space, in
which the orthogonality of R is approximated [7][8].

where R is nxd random matrix such that each entry
rij of R is orthonormal and i.i.d (independent and
identically distributed) zero-mean normal variables,
scaled to have unit length.
The entries of matrix R can be computed based on
the following steps [5]:
1. Set each entry of the matrix to an
independent and identically distributed N
(0,1) value.
2. Orthogonalize the d rows of the matrix to
preserve the pairwise distances between the
feature vectors in the random subspace.
3. Normalize each column vector of the matrix
to unit length.
Given two raw image feature vectors, Γ i and Γ j , of

3. Robust Linear Discriminant Analysis
Model (RDM)
RDM is an enhanced version of FLD that is
introduced by Hu et al. for text categorization [3]. It
decomposes the FLD procedure into a simultaneous
diagonalization of both the within- and between-class
scatter matrices and also calculates an approximation
−1

the same user and let the resulting projected feature
vectors be Γ i = RT Γ i and Γ j = R T Γ j where
|| Γ i ||=|| Γ j ||= 1 , hence || Γ i − Γ j ||2 =|| Γ i − Γ j ||2 [6].

Proof:

(

|| Γ i − Γ j ||2 =|| Γ i ||2 + || Γ j ||2 −2Γ iT Γ j = 2 1 − Γ iT Γ j

Let

α

= 1 − Γ i T Γ j
= 1 − Γ i T R T RΓ j

0 <α <1

(2)

α is known as correlation and it will be used as the

classification metric.
The matrix R T R can be decomposed as follows:

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

(3)

)

of the inverse of within-class scatter matrix, S w , by
an energy-adaptive regularization criterion. The
within-class scatter matrix, S w , is expressed in the
form of:
S w = ΦΛΦ T , with Λ = diag{τ1 ,τ 2 ,...,τ N } (4)
where Φ ∈ℜ NxN is an orthogonal eigenvector matrix
and Λ is a set of generalized eigenvalues in
decreasing order τ1 ≥ τ 2 ≥ ... ≥ τ N . The difference
between FLD and RDM is the latter method is using
ˆ = Λ + σ I for σ > 0 , instead of Λ . The
Λ
determination of σ is not a trivial problem: when

σ → ∞ , the information on S w is ignored; when
σ → 0 (very small value), it may not be sufficient

effective. Thus, both the spectral energy and the
magnitude requirement are considered. RDM
determines the optimal σ as
m


τi
 i =1
*
≥ E (θ ) , where
σ = τ m , with J (m) = min m N

τi

 i =1
J(m) determines the value of m according to the
proportion of energy captured in the first m
eigenvectors, E (m) . In this paper, E (m) = 0.98 is
fixed for all the experiments.
The inverse of within-class scatter matrix can be
estimated as
ˆ −1ΦT , with Λ
ˆ = Λ + σ *I
Sˆ −1 = ΦΛ
(5)

∑
∑

elevation. In the proposed method, RP does not need
training image set to generate random subspace.
However, a training image set is required for
Fisherfaces and DSF to construct eigen-based
subspace. The training set used is a combination of
Essex Face94 (Face94) and Olivetti face database
(ORL) [10][11]. The size of the training dataset of
Face94 is 1000 images (100 classes * 10 images),
while for ORL, the size is 400 images (40 classes * 10
images). A few samples of training and testing datasets
are shown in Figure 2.

w

With the decent estimate in Eq (4), FLD procedure
can be in the form of:

( ΦΛ ) S ( ΦΛ )( ΦΛ )
ˆ −1/2

(

T

ˆ −1/2

b

ˆ −1/2

Let Kb = ΦΛ

ˆ −1/2

) S ( ΦΛ ) ,
T

b

ˆ −1/2

−1

(

ˆ −1/2

v = ΦΛ

(

ˆ −1/2

Y = ΦΛ

)

)

−1

−1

vλ

(6)
v ,

then the simplified version of the above equation is as
follow;
K bY = Y λ
(7)
Finally, the projection matrix of RDM is
ˆ −1/ 2Y
w = ΦΛ
(8)

4. Experimental results and discussions
We address a comparison for the performance of the
proposed
algorithm
(RP+RDM),
Fisherfaces
(PCA+FLD) and DSF (PCA+RDM). Basically, there
are two criteria for the comparison: (1) the amount of
distortion and (2) the computational complexity. The
second criterion has been discussed at Section 1. For
the first criterion, the amount of data distortion can be
measured by the data representation capability, in
which the recognition performance (similarity
matching) is evaluated. Since RP+RDM involves
random data matrix, we repeat the same process 10
times and the results are averaged to reduce the
statistical frustration caused by the different random
numbers.
In the experiments, a combination of Yale Face
Databases (with 10 classes, 28 images per class) and
extended Yale Face Database B (with 28 classes, 28
images per class) is used as testing image set [9]. These
images are frontal images and possess significant
illumination variations with the settings of ± 35 degree
for light source from camera and ± 40 degree

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

(a)

(b)
Figure 2. Samples of (a) training and (b)
testing datasets.
Table 1 shows the similarity matching of Random

Projection ( Γ i = RT Γ i ) with different random matrix
sizes: 100%, 50% and 10% of the original image size.
We can see that the Equal Error Rates (EER) obtained
are similar. This empirical result shows that data
dimensionality reduction by RP does not distort the
data structure.
Table 1. The recognition error of different
random matrix size for RP
Percentage of
random matrix to
original image
size (%)
100
50
10

FAR (%)

FRR (%)

EER (%)

32.20
33.86
33.73

32.56
33.74
33.38

32.68
33.80
33.56

As discussed in the previous section, the main
different between RDM and FLD is that the former
−1

calculates an approximation of S w based on energyadaptive regularization criterion. [3] reported that the

RDM is able to attain superior performance to other
feature extraction algorithms, such as PCA, PCA+FLD
and etc, in text categorization. Figure 3 presents the
error rates of Fisherfaces (PCA+FLD) and DSF
(PCA+RDM). The result demonstrates that RDM
achieves superior performance to FLD; this justifies
that RDM has better discrimination power.
Next, the performance comparison between PCA
and RP as a dimensionality reduction technique in
RDM is presented in Figure 4. We can observe that
PCA and RP are able to attain comparable performance
for dimensionality reduction purpose to deal with the
singularity problem of RDM at higher final feature

length. Both techniques acquire about 10% EER. Even
through both methods accomplish similar performance;
PCA suffers from high computation complexity. PCA
needs a set of sufficient training images to construct a
lower-dimensional subspace by optimizing certain
criteria (e.g. finding a projection subspace that
maximizes the variance in the data). But, RP does not
implement such optimization criteria and it is data
independent. It does not need any training image for
establishing the random subspace. Furthermore, the
generation of random subspace in RP is
computationally simple and the random subspace is
able to preserve the data structure.
Fisherfaces with d=500
Fisherfaces with d=1000
DSF (PCA+RDM) with d=500
DSF (PCA+RDM) with d=1000

Error rate (%)

30

20

10

0
50

100

150

200

250

300

350

400

450

500

Final feature length

Figure 3. Error rates of Fisherfaces (PCA+FLD) and DSF (PCA+RDM). Note that d is the intermediate length.

DSF (PCA+RDM) with d=500
DSF (PCA+RDM) with d=1000
RP+RDM with d=500
RP+RDM with d=1000

Error rate (%)

30

20

10

0
50

100

150

200

250

300

350

400

450

Final feature length

Figure 4. Error rates of DSF (PCA+RDM) and RP+RDM

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

500

Conclusions

References

We have presented a face recognition method, a
combination of random projection and Robust linear
Discriminant analysis Model, known as RP+RDM. This
proposed method is actually an enhanced-version of DSF.
In this approach, RP is selected as an alternative to PCA
for dimensionality reduction to solve the singularity issue
of the within-class scatter of RDM. Raw face images are
projected into lower dimension, via random matrix. Then
the projected images are input into RDM for data analysis.
The empirical result shows that both RP and PCA perform
equal recognition performance. However, PCA suffers
from high computation complexity. Besides, a set of
training database is needed for PCA to construct a number
of eigenvectors to form a lower-dimensional subspace. The
number of eigenvectors is limited by the size of the
training dataset. This implies that PCA might not be useful
when the training dataset if too small. Moreover, the
establishment of PCA-based subspace has to be based on
an optimizing certain criteria (e.g. finding a projection
subspace that maximizes the variance in the data). But, RP
does not implement such optimization criteria and it is data
independent. It does not need any training image for
establishing the random subspace. Furthermore, the
generation of random subspace in RP is computationally
simple and the random subspace is able to preserve the
data structure. Thus, RP is better in overall performance.

[1] M. Turk, and A. Pentland, “Face Recognition using
Eigenfaces”, In Proceeding IEEE Conference on Computer Vision
and Pattern Recognition, 1991, pp. 586-591.
[2] P.N. Belhumeur, J.P. Hespanha, and D.J. Kriegman,
“Eigenfaces vs. Fisherfaces: Recognition using Class Specific
Linear Projection”, In IEEE Transactions on Pattern Analysis
and Machine Intelligence, 19, 1997, pp. 711-720.
[3] J. Hu, W. Deng, and J. Guo, “Robust Discriminant Analysis
of Latent Semantic Feature for Test Categorization”, In Lecture
Notes of Artificial Intelligence (LNAI) 4223, L. Wang et. al.
(Eds.), Springer-Verlag: FSKD, 2006, pp. 400-409.
[4] G.C. Feng, P.C. Yuen, and D.Q. Dai, “Human Face
Recognition using PCA on Wavelet Subband”, In SPIE Journal
of Electronic Imaging 9 (2), 2000, pp. 226–233.
[5] Navin Goel, George Bebis, and Ara Nefian, “Face
Recognition Experiments with Random Projection”, In
proceeding SPIE Conference on Biometric Technology for
Human Identification, 2005, pp. 426-437.
[6] T.B.J. Andrew, “Cancellable Biometrics and Multispace
Random Projections”, In Proceedings 2006 Conference on
Computer Vision and Pattern Recognition, ISBN: 0-7695-2646-2,
2006, pp. 164- 164.
[7] R. Hecht-Nielsen, “Context Vectors: General Purpose
Approximate Meaning Representations Self-organized from Raw
Data”, In Computational Intelligence: Imitating Life (Zurada et.
al. Eds.), 1994, pp. 43-56.
[8] W.B. Johnson, and J. Linderstrauss, “Extensions to Lipshitz
Mapping into Hilbert Space”, In Contemporary Mathematics, 26,
1984.
[9] G. Athinodoros, P. Belhumeur, and D. Kriegman, D, “From
Few to Many: Illumination Cone Model for Face Recognition
under Variable Lighting and Pose”, In IEEE Transaction of
Pattern Analysis and Machine Intelligence, vol.23, no.6, 2001,
pp. 643-660.
[10] Vision Group of Essex University- Face database url:
http://cswww.essex.ac.uk/allfaces/index.html
[11] ORL face database url:
http://www.uk.research.att.com/facedatabase.html

Acknowledgements
Authors thank Yale University, Cambridge University,
Vision Group of Essex University for providing face
databases.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

