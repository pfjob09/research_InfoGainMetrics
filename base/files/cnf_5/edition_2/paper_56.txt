2009 Sixth International Conference on Computer Graphics, Imaging and Visualization

Human Age Estimation by Metric Learning for Regression Problems
Yangjing Long, Student Member, IEEE

Abstract—The estimation of human age from face images has
many real-world applications. However, how to discover the
intrinsic aging trend is still a challenging problem. We proposed
a general distance metric learning scheme for regression
problems, which utilizes not only data themselves, but also their
corresponding labels to strengthen the credibility of distances.
This metric could be learned by solving an optimization
problem. Via the learned metric, it is easy to find the intrinsic
variation trend of data by a relative small amount of samples
without any prior knowledge of the structure or distribution of
data. Furthermore, the test data could be projected to this
metric by a simple linear transformation and it is easy to be
combined with manifold learning algorithms to improve the
performance. Experiments are conducted on the public
FG-NET database by Gaussian process regression in the
learned metric to validate our framework, which shows that its
performance is improved over traditional regression methods.

I. INTRODUCTION
ACE-based biometric systems such as Human-Computer
Interaction have great potential for many real-world
applications. As an important hint for human
communication, facial images comprehend lots of useful
information including gender, expression, age, pose, etc.
Unfortunately, compared with other cognition problems, age
estimation from face images is still very challenging. This is
mainly due to the fact that, aging progress is influenced by not
only personal gene but also many external factors. Physical
condition, living style and plenty of other things may
accelerate or slower aging process. Besides, since aging
process is slow and with long duration, collecting sufficient
data for training is a fairly strenuous work.
[10,17] formulated human ages as a quadratic function.
Yan et al. [27,28] modeled the age value as the square norm
of a matrix where age labels were treated as a nonnegative
interval instead of a certain fixed value. However, all of them
regarded age estimation as a regression problem without
special concern about the own characteristics of aging
variation. As Deffenbacher [8] stated, the aging factor has its
own essential sequential patterns. For example, aging is
irreversible, which is expressed as a trend of growing older
along the time axis. Such general evolution of aging course is
beneficial to age estimation, especially when training data are
limited and distributed unbalanced over each age range.
Geng et al. [13,12] firstly made some pioneer research on
seeking for the underlying aging patterns by projecting each
face in their aging pattern subspace (AGES). Guo et al. [16]
proposed a scheme based on Orthogonal Locality Preserving
Projections (OLPP) [5] for aging manifold learning and get

F

Manuscript received April 9, 2009. Yangjing Long is with the Max Planck
Institute for Mathematics in the Sciences, Leipzig, Germany. (Phone: +49341-9959565; fax: +49-341-9959565; e-mail: long.yangjing@hotmail.com)

978-0-7695-3789-4/09 $25.00 © 2009 IEEE
DOI 10.1109/CGIV.2009.91

343

the state-of-art results. In [16], SVR (Support Vector
Regression) is used to estimate ages on such a manifold and
the result is locally adjusted by SVM. However, they only
tested their OLPP-based method on a private large database
consisting of only Japanese people, and no dimension
reduction work was done to exact the so-called aging trend on
the public available FG-NET database [1]. A possible reason
is that, FG-NET database may not supply enough samples to
recover the intrinsic structure of data. The lack of sufficient
data is a prominent barrier in age estimation.
Therefore, how to dig out the underlying variation trend of
data within a limited amount of samples is well worth
investigation. From a generalized standpoint, manifold
learning algorithm is unsupervised distance metric learning,
which attempt to preserve the geometric relationships
between most of the observed data. The starting point is the
input data, while labels are always not taken into
consideration. But labels indeed provide important cues about
similarities among samples, which is crucial to construct the
structure of data, especially under a small given dataset. To
take full advantage of labels, a family of supervised metric
learning algorithms [3,14,25,26] are developed, which adds
label information as a weight to entice samples pertaining to
the same class to go nearer by learning a special metric. Yet,
almost all of these methods are specially designed for
classification problem. For regression problems such as age
estimation, there are naturally infinite classes, where the
constraints in previous literatures are not practical.
We propose a new framework aiming to learn a special
metric for regression problems. Age is predicted based on the
learned metric rather than the traditional Euclidean distance.
We accomplish this idea by formulating an optimization
problem, which approximates a special designed distance that
scaled by a factor determined according to the labels of data.
In this way, the metric measuring the similarity of samples is
strengthened. More importantly, since labels are incorporated
to depict the underlying sample distribution tendency, which
signifies the inclusion of more information, a smaller amount
of training data is required. Unlike the nonlinear manifold
learning where it is repeated to find its low dimensional
embedding, a merit of our framework is that, a full metric
over the input space is learned and expressed as a linear
transformation, and it is easy to project a novel data into this
metric. Moreover, the proposed framework may also be used
as a pre-processing step to assist those unsupervised manifold
learning algorithms to find a better solution.
The rest of the manuscript is arranged as follows: Section II
gives the details of the metric learning formulation for
regression problems based on labels of training data. Section
III takes Gaussian Process Regression (GPR) as an example
to explain how to make use of the learned metric. Section IV
demonstrates the experimental results of the performance of

the proposed framework on FG-NET Aging Database.
Section V comments on conclusions.
II. METRIC LEARNING FOR REGRESSION
Let S = (Xi, yi) (1≤i≤N) denotes a training set of N
observations with inputs Xi ∈ Rd and their corresponding non
negative labels yi. Our goal is to rearrange these data in
high-dimensional space with a distinct trend as what their
labels characterize. In other words, we hope to find a linear
transformation T: Rd→Rd, after applying which, the distances
between each pair-wise observation may be measured as:

dˆ ( X i , X j ) =|| T ( X i − X j ) ||2
(1)
The distance dˆ ( X i , X j ) should be reliable to measure the
difference as what their labels indicate.
A. Problem Formulation
Metrics is a general concept, as a function giving a
generalized scalar distance between two argument patterns
[11]. Straightforwardly, different distances are also possible
to depict the tendency of a data set. Similar to Weinberger et
al. [25] and Xing et al. [26], we consider learning a distance
metric of the form

d A ( X i , X j ) = ( X i − X j )T A( X i − X j )

(2)

But unlike their works for classification problems, in
regression problems, every two observations are of different
classes. Better metrics over their inputs are expected and a
new metric learning strategy ought to be established.

dˆij =dˆ(Xi , Xj )

Suppose given certain well-defined distance

ideally delineating the data trend, our target is to approximate

dˆij by d A ( X i , X j ) minimizing the energy function

(

ε ( A) = ∑ d A ( X i , X j ) p − (dˆij ) p
i, j

)

2

(3)

To promise that A is a metric, A is restricted to be symmetric
and positive semi-definite. For simplicity, p is assigned to be
2. This metric learning task is formulated as an optimization
problem with the form below

(

min ∑ ( X i − X j )T A( X i − X j ) − (dˆij )2
i, j

)

2

(4)

satisfying the matrix A is symmetric and positive semidefinite. And there exists a unique lower triangular L with
positive diagonal entries such that A=LLT [15]. Hence
learning the distance metric A is equivalent to finding a linear
transform LT projecting observation data from the original
Euclidean metric to a new one by

X = LT X

(5)

B. Distance with Label Information
In practical application, Euclidean distance is not always
capable to guarantee the rational relationship among input
data. Although manifold learning algorithms may discover
the intrinsic low-dimensional parameterizations of the high
dimensional data space, at the outset, it also requires
Euclidean distance to apply kNN (k-Nearest Neighbors) to
344

know the local structure of the original space. On the other
hand, manifold learning demands a large amount of samples,
which is not available in some circumstances. Figure 1
visualizes the age manifolds of the FG-NET Aging Database
learned by Isomap [24], Locally Linear Embedding (LLE)
[21] and OLPP [5] respectively. Data points of age from 0 to
69 are colored from blue to red. From the 2-D view, none of
them can detect a distinctive aging trend. A possible reason is
that, FG-NET database only have 1002 images, and each
person only have a few images that span from 0 to 69,
inadequate to approximate its underlying manifold correctly.
For many regression and classification problems, it is in
fact a waste of information if only data Xi is utilized but with
their associated labels yi ignored in the training stage.
Balasubramanian et al. [2] proposed a biased manifold
embedding framework to estimate head poses. In their work,
the distance between data is modified by a factor of the
dissimilarities fetched from labels. The basic form of this
modified distance is

d '(i, j ) =

β × P (i, j )

max m ,n P(m, n) − P(i, j )

× d (i, j )

(6)

where d(i, j) is the Euclidean distance between two samples
Xi and Xj . P(i, j) is the difference of poses between Xi and Xj.
Through incorporating the label information to adjust
Euclidean distance, the modified distances are prone to give
rise to the true tendency of data variation i.e. if the distance of
two observations is large, then the distance of their labels is
also large, vice versa. Hence it is intuitively that the biased
distance is a good choice for

dˆij in Eq.(3):
p

⎛ β × | L(i, j ) | ⎞
dˆ (i, j ) = ⎜
⎟ × d (i, j )
⎝ C − L(i, j ) ⎠

(7)

Analogously, L(i, j) is the label difference between two
data. C is a constant greater than any label value in a train set
and p is selected to make data easier to discriminate. d(i, j) is
the Euclidean distance between two samples Xi and Xj.
C. Optimization Strategy
Since the energy function is not convex, it is a non-convex
optimization and consequently it is impossible to find a
closed form solution. The metric A is with the property to be
symmetric and positive semi-definite, so it is natural to
compute a numerical solution to Eq.(4) using the Newton’s
method. Similar to [26], in each iteration, a gradient descent
step is employed to update A. The iteration algorithm is
summarized as follows:
1. Initialize A and step length α;
2. Enforce A to be symmetric by A←(A+AT)/2;
3. The Singular Value Decomposition of A=LT∆L, where the
diagonal matrix ∆ consists of the eigenvalues λ1,…,λn of A
and columns of L contains the corresponding eigenvectors;
4. Ensure A to be positive semi-definite by A←LT∆'L, where
∆'=diag(max(λ1, 0),…,max(λn, 0));
5. Update A'←A − α∇ Aε ( A) , where ∇ Aε ( A) is the
gradient of the energy function in Eq.(3) w.r.t. A;

Isomap age manifold

LLE age manifold

OLPP age manifold

Figure 1. The 2 dimensional embedding of FG-NET Aging Database by Isomap, LLE and OLPP methods, based on the
Euclidean distance. Points of age from 0 to 69 are marked from blue to red as prescribed in the gradient ruler rightward.
6. Compare the energy function ε(A) with ε(A') in Eq.(3), if
ε(A)<ε(A'), then augment the step length α with a momentum
to accelerate the optimization process; otherwise, shrink α to
assure a local minimum is not overpassed.
7. If A has converged or the maximum iteration times are
reached, terminate; otherwise go back to Step 2.
III. GAUSSIAN PROCESS REGRESSION
GPR from the traditional Minkowski metric is extended to
the learned metric. It should be clarified that, the learned
metric is designed for regression problems, especially kernel
based method such as GPR [20] and SVR (Support Vector
Regression) [22]. GPR is preferred here because it can
determine the hyper-parameters of the kernel automatically
based on Bayesian model selection criterion such as
Maximum A Posteriori, Markov Chain Monte Carlo method
[18] etc. rather than SVR where parameters is often chosen by
cross-validation. Hereby it is not necessary to partition the
training set into two parts to get an extra validation set.
For target prediction such as human ages and head poses,
designing an appropriate regressor is a key point to model the
problem. SVR is one of the most popular and powerful tools,
which adopts a Radial Basis Function (RBF) kernel computed
in Euclidean space. In recent years, predictor variables in
extended versions of SVR are assumed to be in the proximity
of a low dimensional manifold embedded in a high
dimensional input space.
Sugiyama et al. [23] modified the RBF kernels by
substituting the Euclidean distance with the geodesic distance.
But it has been proven that for many practical problems,
geodesic distance fails to discover the intrinsic structure of
data [9]. Figure 1 is just such an example. More importantly,
geodesic distance is also not reliable to construct kernels even
if in a proper case. Since such distance is approximated by
searching the shortest path on a k nearest neighbor graph [24],
and there is no guarantee that the kernel matrix is positive
semi-definite. Hence it is possible that a local optimum can
not be arrived and the inverse matrix could not be computed.
Many existing methods of regression on manifolds
measure distance in Minkowski metric [4,19]. The evaluation
of similarity by this metric is based on the assumption that,
the similar point should be close to the query point in all
dimensions. If the attributes of data are many enough,
345

Euclidean distance is not credible. In [2], a biased Euclidean
distance scaled by label difference is used to find the k nearest
neighbors of each data and then applied to manifold learning
algorithms. To construct the nonlinear relationship between
the high and low dimensional space, [2] takes a Generalized
Regression Neural Network to learn this nonlinear mapping.
Yet, the training of a Neural Network is time consuming, and
the curse of dimensionality is inevitable since the input
dimension (typically raw images) is high.
Given a training set S = (Xi, yi) (1≤i≤N) as described in
Section II and a sample X* for query, GPR predicts its output
y* by putting a Gaussian process prior on this function f(·),
assuming that all sample points evaluated from the function
have a multivariate Gaussian density [20].
Let X=[X1,…,XN] and Y=[y1,…,yN]T, the Gaussian predictive
distribution of y* is derived of the form
p(y*|X*,X,Y,Θ)~N(μ(X*),V(X*))
(8)
The mean prediction and covariance matrix in Eq.(8) are
μ(X*)=k(X*,X)[K+σ2I]-1Y
(9)
V(X*)=k(X*,X*)-k(X*,X)T[K+σ2I]-1k(X*,X) (10)
where k(·,·) is the covariance function, K is the covariance
matrix of X and σ2 is the variance of noise.
Another way to perceive and thus rewrite Eq.(9) is to treat the
mean prediction as a linear combination of N kernel functions:
N

μ ( X * ) = ∑ α C k( X * , xc )

(11)

c =1

where α=(K+σ2I)-1Y
Gaussian kernel is a good choice for the covariance function
k(Xi,Xj)=v2exp(-||Xi-Xj||2/2l2+σ2σXiXj)
(12)
In respect that the proposed learned metric encodes label
information implicitly, it is bestowed as the similarity
measure and Eq.(12) becomes
k(Xi,Xj)=v2exp(-(Xi-Xj)TA(Xi-Xj)/2l2+σ2σXiXj)
(13)
IV. EXPERIMENTAL RESULTS
Age estimation is carried on the public FGNET Aging
Database [1] by the regression strategy on the basis of the
proposed metric. The database contains totally 1002 color or
gray images from 82 people. Each person has around 10 face
images with the ranges from 0 to 69 with labeled ground
truth. These images are taken under varying lighting
condition, poses and expressions. Each image is labeled by 68

other recent methods. As compensation, an outstanding
improvement is achieved in the larger age range. This trait is
fairly attractive considering the fact that, people over 30 years
old account for less than 15% of the whole FG-NET database.
Even if there are only a few samples (for example, there are
only 8 images out of 1002 over 60 years old), a relatively
acceptable age prediction can be obtained.

points characterizing its shape features. Similar to
[13,16,27,28], input features are selected to be the parameters
of AAMs [6]. Figure 2 presents some typical face images and
their reconstructed faces by AAM.
Firstly we hope to testify that the proposed metric is able to
disinter some internal patterns of human’s aging progression.
We randomly choose 300 images out of all the 1002 images in
FG-NET Database as training samples, and the rest as test
samples. The parameters in Eq.(7) are chosen as C=100, β=1
and p=1. The energy function is converged after 50 iterations
or so. Figure 3(a) and 3(b) portrays the positional relationship
among training samples in the hyper-space measured by
Euclidean distance and the learned metric A. The 2D view is
acquired by Multi-Dimensional Scaling (MDS) [7]. Figure 4
plots the relative position of the remaining 702 image samples
for test. Contrast to Figure 1, manifold learning algorithms
like Isomap, LLE and OLPP fails to predicate the aging trend
sometimes. Furthermore, though only 30% of the entire data
set is directed for learning the aging trend is effectually set up.
As in Eq.(5), the original parameters from AAMs can be
linearly transformed into a hyper-space based on our learned
metric, by multiplying LT satisfying A=LLT. Figure 5 draws
the 2D aging manifold inputted with the transformed data.
Compared to Figure 1, the linear transform LT is salutary for
other manifold algorithms to find an improved aging trend.
Then, age estimation of our methodology is compared with
the performance of some state-of-art approaches. The LeaveOne-Person-Out mode [13,16,27,28] is the mechanism for
experimentation, i.e. each time we choose one person for
testing and all others for training. The same as in [13,16,27,
28], two criteria are adopted for performance evaluation. One
is the Mean Absolute Value (MAE), which is defined as

Figure 2. Typical sample images of FG-NET Aging Database
and their AAM synthetic faces
MDS based on the Euclidean distance MDS based on the learned metric

(a)

N

k − age | / N
MAE = ∑ | age
i
i

(14)

i =1

where for each Xi,

(b)

Figure 3. 2-D view of the clustering effects of the 300 training
samples by metric learning. It illustrates the 2 dimensional
embedding of the training data sampled from FG-NET Aging
Database by MDS. Points of age from 0 to 69 are marked
from blue to red. It is seen that, the distance calculated based
on our learned metric in Figure (b) preserves local proximity
of samples with close labels better than that based on the
traditional Euclidean distance in Figure (a).

k is its labeled ground truth and age
age
i
i

is the estimated age. N is the number of testing images.
Another widely acknowledged criterion is the cumulative
score at error level l [13]
CumScore(l)=Nerror ≤ l /N×100%
(15)
In respect that, when a face image is labeled as O years old,
the person is customarily thought to be [O,O+1) years old
[27], thus the error less than a specified number of years is by
and large neglectable in practical application. Eq.(15) is an
indicator of the algorithmic correct rate.
The parameters in Eq.(7) are rectified to be C=80, β=1 and
p=0.6. Table 1 lists the MAE of different approaches. The
MAE of the proposed method is almost the same as the best
one [16]. However, unlike their LARR, we simply predict
ages in a new metric by regression without any local
refinement. LARR slides the estimated age up and down by
checking different age values to see if it can come up with a
better prediction [16]. The parameters defining the search
range is determined manually, which is at least not
convenient and automatic enough, and may be laborious and
not feasible in some real-world applications. Table 2 details
Table 1 with separate MAEs over different age range. The
MAE of our method in younger people is slightly higher than

MDS based on the Euclidean distance MDS based on the learned metric

(a)

(b)

Figure 4. 2-D view of the clustering effects of the 702 testing
samples by metric learning, corresponding to Figure 3. It is
obvious that, the actual aging trend is, to some extended,
manifested in the hyper-space based on our learned metric.

346

Isomap age manifold on learned metric

LLE age manifold on learned metric

OLPP age manifold on learned metric

Figure 5. 2D age manifolds. This figure illustrates the 2 dimensional embedding of FG-NET Aging Database by Isomap, LLE
and OLPP algorithms based on our learned metric.
Reference
Method
MAE
[13]
AGES
6.77
[12]
KAGES
6.18
[27]
RUN1
5.78
[28]
RUN2
5.33
[16]
LARR
5.07
Proposed
Metric learning+GPR
5.08
Table 1. MAE comparison of different methods

V. CONCLUSIONS

Age Range
Proposed
GPR
RUN[27]
0-9(37.0%)
2.99
3.55
2.51
10-19(33.8%)
4.19
4.34
3.76
20-29(14.4%)
5.34
5.09
6.38
30-39(7.9%)
9.28
9.04
12.51
40-49(4.6%)
13.52
14.65
20.09
50-59(1.5%)
17.79
19.77
28.07
60-69(0.8%)
22.68
31.76
42.50
Average
5.08
5.45
5.78
Table 2. MAEs over various age ranges on FG-NET Database
for the proposed method, GPR and RUN. In the first column,
the value in the parenthesis stands for the proportion
(percentage) for each age group out of the whole database.

In this paper, a new metric learning framework is proposed
to resolve regression problems. It is feasible to be applied to
many other problems in machine learning or computer vision.
No assumptions about the structure or distribution of the
samples are made, and a relatively small quantity of training
samples is required to learn their underlying variation trend.
Experiments shows the effectiveness of the learned metric to
restore the intrinsic infrastructure of input sample data and
encouraging performance is acquired on a widely used public
face aging database.
REFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]

Figure 6. Cumulative scores of our method and GPR at error
levels [0,10].
Figure 6 displays the cumulative scores of our method and
GPR. It can be found that at each level, our method is better
than GP regression to some degree.

347

[13]
[14]
[15]

FG-NET Aging Database. http://www.fgnet.rsunit,com
V. N. Balasubramanian, J. Ye, and S. Panchanathan. Biased manifold
embedding: A framework for person-independent head pose
estimation. In IEEE Conf. CVPR, pp. 1–7, 2007.
A. Bar-Hillel and D. Weinshall. Learning distance function by coding
similarity. Proc. ICML, pp. 65-72, 2007.
M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: a
geometric framework for learing from labeled and unlabeled examples.
Journal of Machine Learning Research, 7:2399–2434, 2006.
D. Cai, X. He, J. Han, and H. J. Zhang. Orthogonal laplacianfaces for
face recognition. IEEE Trans. Image Processing, 15:3608–3614, 2006.
T. Cootes, G. Edwards, and C. Taylar. Active appearance models. IEEE
Trans. Pattern Analysis & Machine Intelligence, 23(6):681–685, 2001.
T. Cox and M. Cox. Multidimensional Scaling. Chapman & Hall,
Lodon, 1994.
K. A. Deffenbacher, T. Vetter, J. Johanson, and A. J. O’Toole. Facial
aging, attractiveness, and aistinctiveness. Perception, 27, 1998.
D. L. Donoho and C. E. Grimes. When does geodesic distance recover
the true hidden parametrization of families of articulated images? Proc.
European Symposium on Artificial Neural Networks, 2002.
A. L. C. Draganova and C. Christodoulou. Comparing different
classifiers for automatic age estimation. IEEE Trans. Systems, Man,
and Cybernetics, 34(1):621–628, 2004.
R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification, 2nd ed.
John Wiley & Sons, Inc., New York, 2001.
X. Geng, K. Smith-Miles, and Z.-Z. Zhou. Facial age estimation by
nonlinear aging pattern subspace. Proc. ACM Conf. Multimedia, 2008.
X. Geng, Z. H. Zhou, Y. Zhang, G. Li, and H. Dai. Learning from facial
aging patterns for automatic age estimation. Proc. ACM Conf.
Multimedia, pp.307–316, 2006.
J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov.
Neighbourhood components analysis. NIPS, 2005.
G. H. Golub and C. F. V. Loan. Matrix Computations. Johns Hopkins
Univ. Press, 1996.

[16] G. Guo, Y. Fu, C. Dyer, and T. S. Huang. Image-based human age
estimation by manifold learning and locally adjusted robust regression.
IEEE Trans. on Image Processing, 17:1178–1188, 2008.
[17] A. Lanitis, C. J. Taylor, and T. Cootes. Toward automatic simulation of
aging effects on face images. IEEE Trans. Pattern Analysis and
Machine Intelligence, 24(4):442–455, 2002.
[18] R. M. Neal. Monte carlo implementation of gaussian process models
for bayesian regression and classification. Technical Report
CRG-TR-97-2.
[19] J. Nilsson, F. Sha, and M. I. Jordan. Regression on manifolds using
kernel dimension reduction. IEEE Conf. ICML, pp. 265–272, 2007.
[20] C. E. Raumussen and C. K. Williams. Gaussian Processes for Machine
Learning. MIT press, 2006.
[21] S. T. Roweis, and L. K. Saul. Nonlinear dimensionality reduction by
locally linear embedding. Science, 290(5500):2323–2326, 2000.
[22] B. Scholkopf and A. J. Smola. Learning with Kernels: Support Vector
Machines, Regularization, Optimization, and Beyond. MIT Press,
Cambridge, MA, 2002.

[23] M. Sugiyama, H. Hachiya, C. Towell, and S. Vijayakumar. Geodesic
gaussian kernels for value function approximation. Autonomous
Robots, 25:287–304, 2008.
[24] J. B. Tenebaum, V. de. Silva, and J. C. Langford. A global geometric
framework for nonlinear dimensionally reduction. Science, 290(5500):
2319–2323, 2000.
[25] K.Weinberger, J. Blitzer, and L. Saul. Distance metric learning for
large margin nearest neighbor classification. Proc. NIPS, pp.
1475–1482, 2006.
[26] E. Xing, A. Ng, M. I. Jordan, and S. Russell. Distance metric learning
with application to clustering with side-information. Proc. NIPS, 2002.
[27] S. Yan, H. W. ang T. S. Huang, and X. Tang. Ranking with uncertain
labels. IEEE Conf. Mulitimedia and Expo, pp. 96–99, 2007.
[28] S. Yan, H. Wang, X. Tang, and T. S. Huang. Learning autostructured
regressor from uncertain nonnegative labels. IEEE Conf. ICCV, pp.
1–8, 2007.

348

