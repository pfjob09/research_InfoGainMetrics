2009 Sixth International Conference on Computer Graphics, Imaging and Visualization

Facial Expression Representation Using A Quadratic Deformation Model
1

M. Sagar

1

M. Obaid, 2R. Mukundan, 1M. Billinghurst

Weta Digital
Wellington, New Zealand
marks@wetafx.co.nz

HITLab NZ, 2Computer Science and Software
Engineering Department
University of Canterbury
Christchurch, New Zealand
{mohammad.obaid, mark.billingurst}@hitlabnz.org,
mukund@cosc.canterbury.ac.nz

sheet transformations. Section IV explains how the proposed
approach uses the rubber-sheet transformation for
representing facial expressions. Section V presents the
analysis and results for constructing the Facial Deformation
Tables for facial expression representations. Finally, section
VI concludes the paper and outlines some future research
directions.

Abstract - In this paper we propose a novel approach
for representing facial expressions based on a quadratic
deformation model applied to muscle regions. The nonlinear nature of muscle deformations can be captured for
each expression, by subdividing the face into 16 facial
regions and using the most general rubber-sheet
transformation of second degree.
The deformation parameters are derived using a leastsquare minimization technique, and used to construct a
Facial Deformation Table (FDT) to mathematically
represent each expression. The generalized nature of the
transformations allows us to easily map expressions from
one model to another, and employ the FDTs in facial
expression applications such as facial recognition and
animations. The paper presents experimental results using
the smile expression.

II.

Several earlier approaches have attempted to analyze and
represent facial expressions. These include, image-based
approaches (Gabor wavelets, principle component analysis
and neural networks), model-based approaches (active
appearance models, 3D geometric face models), and motionbased approaches (dense optical flow, 3D motion models).
Full survey details are given in [6] and [9].
One of the most significant descriptions of facial
expressions is the Facial Action Coding System (FACS),
developed by Paul Ekman and Wallice Friesden [2]. This
defines facial expressions as a combination of facial actions
corresponding to movements of particular muscle groups.
Ekman and Friesden defined facial muscle actions as Action
Units (AU). Originally, FACS used forty four AUs to define
all facial muscular actions. Recently, a new version of FACS
was published by Ekman, Friesen, and Hager [3] where
facial expressions are categorized into seventy two AUs. One
of the limitations of FACS is that the facial expressions are
described based on local information, making it difficult to
define and integrate them in related applications.
Essa and Pentland [4] described an extension to FACS
called FACS++, which addresses the lack of emotional
spatiotemporal information by using vision-based
observations with the dynamics of facial expressions. They
used an optical flow method to estimate muscle actuation
and to generate flow on the face model.
Matsumura, Nakamura, and Matsui [8] described a
method to extract a mathematical function which can be used
to reconstruct facial expressions from face portraits. Their
method extracts the facial function from a 2D face image by
using the metamorphosis by thin-plate splines.
Another common description of facial expression is the
MPEG-4 standard [7] which supports the definition,
encoding, transmission, of facial animation. The facial
features specified by the MPEG-4 standard, are a
representation of the human facial structure in a way that the

Keywords --- Rubber Sheet Transformations, Facial
Expression Representations, Quadratic Deformation Model.
I.

INTRODUCTION

Recently, with the exponential growth in the power of
computer graphics hardware, facial animation has become
increasingly important in various application areas such as
games, animation, teleconferencing and multimedia
education. The main application areas for facial animation
are in the film industry, gaming, and advertising, where the
focus is on the modeling and animation of Virtual Humans
and 3D-digital characters. In this context, facial animation
has proven to be one of the most challenging parts of
building and animating a virtual character model. The
complexity of the human face and the high sensitivity
humans have in identifying facial expressions make the
representation of facial expressions complicated and
challenging to produce.
This paper presents a novel approach to representing
facial expressions in terms of transformation functions. The
main advantage of our approach is the generic representation
of facial expressions that can be employed in facial
expression applications such as facial animations and
recognition.
This paper is organized as follows. Section II outlines the
previous research and background in the area of facial
expression representations. Section III presents the rubber978-0-7695-3789-4/09 $25.00 © 2009 IEEE
DOI 10.1109/CGIV.2009.29

BACKGROUND

44

facial expressions allow the recognition of the speaker’s
mood, and also the support of speech reproduction. The
MPEG-4 facial animation standard is defined by 84 feature
points (FPs). These points are used to define the 68 Facial
Animation Parameters (FAPs). The MPEG-4 standard also
defines Face Animation Parameter Units (FAPU), which
define the distance ratios between key facial feature points.
A detailed description about MPEG-4 Facial animation is in
[7].
One feature of most facial representation standards, such
as FACS and MPEG-4, is their very general description of
muscle movements as shown in Fig 1. Our approach
attempts to represent each expression as a collection of
transform coefficients that are model independent.
Figure 2.

Deformation of a set of points.

A. Transformation Parameters
To find the 12 parameters representing a transformation,
the minimum of the sum of squares (or least square
minimization) [13] is carried out on the pair of equations
(Equation (1) and (2)) representing the x and y rubber-sheet
transformations. The process of finding the transformation
parameters is illustrated in Fig. 3.
Input
Output
Figure 1. An example of the FACS description for the fear expression
[17] .

III.

xi
yi

RUBBER-SHEET TRANSFORMATION

i=1…,n

Rubber-sheet transformations are higher-order (nonlinear) polynomial transformations [13] [14]. The name
comes from the logical analogy of overlaying an elastic piece
of rubber to fit over a surface of some shape. Geographical
Information Systems (GIS) extensively use rubber-sheet
transformations for operations such as converting map
coordinates, map alignments and geometrical correction
between maps [10].
In
the
two-dimensional
space,
rubber-sheet
transformations are defined by a pair of equations:
x i' = a 1 x i2 + a 2 x i y i + a 3 y i2 + a 4 x i + a 5 y i + a 6 (1)

xi’
yi’

Least Square
Minimization

aj
bj
j=1,..,6

i=1,..,n
Figure 3. Derivation of deformation parameters.

For example, the minimum of the sum of squares is
achieved when the gradient of the sum-of-error-squares for
equations (1) and (2) is zero. The following equations
represent the sum-of-error-squares for equations (1) and (2)
respectively.

y i' = b1 x i2 + b 2 x i y i + b 3 y i2 + b 4 x i + b 5 y i + b 6 (2)
i = 1,..., n

[
= ∑ [y

]
)]

G x = ∑ x i' − ( a1 x i2 + a 2 x i y i + a 3 y i2 + a 4 x i + a 5 y i + a 6 )
Gy

Where n is the number of transformed points and ai , bi
are the transformation parameters, or coefficients.
Generally, the above 12 transformation parameters are
not known, but the coordinate points before and after the

'
i

− (b1 x i2 + b 2 x i y i + b3 y i2 + b 4 x i + b5 y i + b6

2

2

The minimum of the sum of squares is found by setting
the gradient of Gx and Gy to zero with respect to all unknown
coefficients as follows
∂G y
∂G x
=0;
=0
j = 1,...6.
∂a j
∂b j
Differentiating Gx and Gy with respect to all unknown
variables and equating to zero gives the conditions for the
minimum error in the transform coefficients in the least-

transformation are known (i.e. ( xi' y i' ) and ( xi y i ) from
equations (1) and (2)) as shown in Fig. 2. With enough
coordinate points, the solution for the 12 parameters that best
fit the transformation can be worked out by solving from the
known coordinate points. The process of finding the solution
is described in the following sections.

45

square sense. For example, differentiating Gx with respect to
∂G
a1 ( x = 0) is carried out as follows
∂a1
n

[

on the muscular anatomy of the face. We define facial
regions based on individual muscles or a small group of
muscles that elastically deform the facial skin shape. At the
facial expression representation step, we use the rubber-sheet
transformation method (described in section III) to the
represent each of the defined facial regions in terms of
deformation parameters. To find those parameters, face
markers were captured and tracked for the six main
expressions (happy, sad, fear, surprise, anger and disgust) to
obtain a set of coordinate data for each of the facial regions.
The data is normalized before applying the rubber-sheet
transformation method to find the deformation parameters.
The following two sections describe in more detail the
process for the steps used to represent facial expressions
mathematically.

]

2 ∑ x i' − (a1 x i2 + a 2 x i y i + a 3 y i2 + a 4 x i + a 5 y i + a 6 x i2 = 0
i =1

n

[

]

∑ xi' xi2 − a1xi4 − a2 xi3 yi − a3 xi2 yi2 + a4 xi3 − a5 xi2 yi − a6 xi2 = 0

i =1

n

n

n

n

n

n

n

i=1

i=1

i=1

i=1

i =1

i=1

i=1

3
2 2
3
2
2
' 2
4
∑xi xi − ∑a1xi −∑a2xi yi −∑a3xi yi −∑a4xi −∑a5xi yi − ∑a6xi = 0

This can also be written as

x i' n1 − a1 m11 − a 2 m12 − a3 m13 + a 4 m14 − a 5 m15 − a 6 m16 = 0
Where
n

n

n

i =1

i =1

i =1

A. Defining Facial Regions
To define facial regions that can be used to represent
facial expressions mathematically, we looked at the FACS
description of facial expressions [2]. FACS defines facial
expressions based on an anatomical analysis of facial
behavior and on all visually distinguishable facial
movements, in which every facial movement is a result of a
muscular movement [16].
Using the FACS definition and the anatomy of the facial
muscle system from [1], we defined sixteen facial regions
that represent the deformation of the facial expressions. The
sixteen regions form the minimum number for defining
independent facial muscle groups. Fig. 4 illustrates the
defined facial regions.

n1 = ∑ x i' x i2 ; m 11 = ∑ x i4 ; … ; m 16 = ∑ x i2
For the derivative of Gy with respect to the other unknown
coefficients, the same process is applied to obtain the values
for ni and mij , where i = 1,... 6 ; j = 1,... 6 .
Having found those values we can now solve for the
parameter values Pa (a1 ,..., a 6 ) and Pb (b1 ,..., b6 ) by solving
the following Matrix setup
P M = N , P = M −1 N
To illustrate, the matrix elements to solve for the
parameter values Pa (a1 ,..., a 6 ) are as follows

⎡ a1 ⎤ ⎡ m11
⎢a ⎥ ⎢ m
⎢ 2 ⎥ ⎢ 21
⎢ a3 ⎥ ⎢ m31
⎢ ⎥=⎢
⎢a 4 ⎥ ⎢ m41
⎢ a5 ⎥ ⎢ m51
⎢ ⎥ ⎢
⎢⎣a 6 ⎥⎦ ⎢⎣ m61

m12
m22
m32

m13
m23
m33

m14
m42
m43

m15
m25
m35

m42
m52
m62

m43
m53
m63

m44
m54
m64

m45
m55
m65

m16 ⎤
m26 ⎥⎥
m36 ⎥
⎥
m46 ⎥
m56 ⎥
⎥
m66 ⎥⎦

−1

⎡ n1 ⎤
⎢n ⎥
⎢ 2⎥
⎢ n3 ⎥
⎢ ⎥
⎢n 4 ⎥
⎢ n5 ⎥
⎢ ⎥
⎢⎣n6 ⎥⎦

The solution to the set of parameters can now be used to
'

'

obtain the transformed coordinate ( xi , y i ) of any point by

Figure 4.

substituting the ( xi , y i ) coordinates into equations (1) and
(2).
In the following two sections we show how the rubbersheet transformation parameters can be used to represent
facial expressions.
IV.

Facial Regions.

B. Facial Expression Representations
In this section, we describe how we represent facial
expressions using the rubber-sheet transformations and the
defined facial regions from sections III and IV (A)
respectively. The main question we are answering is: “How
does a region deform for a particular expression?” (Fig. 5).
To answer this question, the defined facial regions are
represented as non-linear shapes that can deform into any
other shape depending on the facial expression. To find how
each of regions deforms, several face markers were placed
on an actors face and used to track how each region was

METHOD

The method for representing facial expressions
mathematically consists of two steps: (1) defining the facial
muscle regions, and (2) facial expression representation. At
the first step, the aim is to divide the face into regions based

46

deformed from the neutral facial expression to any
performed expression (happy, sad, fear, surprise, anger and
disgust).

points ( xi , yi ) k into the right hand-side of equations (1) and
(2) from section III.
Therefore, applying this process to all facial regions will
result in deforming the facial regions from the neutral facial
expression to the desired facial expression.
C.

Normalization Process
To analyze the details of the facial regions, global head
movements are eliminated by pose- and scalenormalizations. The normalization is done based on two
global facial locations (eye pupils) p1 and p 2 as shown in
Fig. 6, where m is the middle point between p1 and p 2 .

How does region 10 deform in an
expression (happy, sad, fear, surprise,
anger and disgust)?

?
Figure 6. Global facial locations for normalising.

Coordinate points

(xi , y i )

+Y

To find the region deformation parameters using the
rubber-transformations (described in section III), we
collected, from 2D images, the (x, y ) coordinates for the
neutral expression and then collected the corresponding
x' , y'
for the preformed expression
coordinates

(

p1

p1

Figure 5. Example to demonstrate region deformations.

xi' ; y i

+Y

X

m

p2
-Y

)

(i)

y i'

⇒
i = 1,..., n , where n represent the
(i.e. xi ⇒
number of points in a region). The data was normalized to
eliminate all global head movements by pose- and scalenormalizations, as described in section IV (C). Using the
normalized data, we computed the parameter values for each
facial region to obtain the following set of parameters for an
expression
(a1 , b1 ,…, f1 , a2 , b2 ,…, f 2 ) k
(k = 1,...,16)
where k is the number of facial regions.
Using the computed deformation parameters for region k,
we can obtain the transformed coordinates ( xi' , y i' ) k of any
point within a region by substituting the parameter values
(a1 , b1 ,…, f1 , a2 , b2 ,…, f 2 ) k and the original coordinate

θ

m

p2

X

-Y
(ii)

Y

p1 =(-1,0) m

p2 =(1,0)
X
n=(0,-1)
(iii)

Figure 7. Normalisation operations: (i) translating the points about the
origin (ii) eliminating the XY rotations (iii) scaling operation.

The line (m, n) is the perpendicular bisector of the line
( p1 , p 2 ). All of the points were normalized through the
following three transforms

47

•
•

Translate point m to the origin, and then translate all
other points about the origin by (-mx,-my,-mz) as
illustrated in Fig. 7 (i).
Rotate all points about the origin by θ around the Zaxis, where θ is the angle between vector ( p1 , p 2 ) and
the X-axis (see Fig. 7 (ii)).

•

⎛
⎞
Scale all the point to ⎜ xi , y i ⎟ as shown in Fig. 7 (iii).
⎜ xp
⎝ 2

A.

Analysis and Results
The results from the above procedure gave a set of
FDTE that represent the deformation parameters for each of
the participants’ facial expressions.
To compute a generic set of FDTE that can be used to
represent expressions, the mean ( μ ) of the FDTE parameter
values for expression E are computed. The computed results
represent each of the six main expressions in a FDTEμ . For

x p2 ⎟⎠

This operation will scale points p1 and p 2 to (-1, 0) and
(1, 0) respectively, and point n to (0,-1).
DERIVATION OF FACIAL DEFORMATION TABLES
(FDTS)
The aim of our work was to develop a new method for
representing facial expressions in terms of deformation
parameters, where those parameters can be employed in
facial expression applications such as facial animations,
recognition and interpretations.
To achieve the facial expression representations, we
defined a Facial Deformation Table ( FDTE ), for an
expression E, that represents the deformation parameters for
each facial region of expression E. The following describe
the procedure to compute a FDTE for each of the six main
expressions (happy, sad, fear, surprise, anger and disgust).
V.

•

•

•
•

Figure 8. Example of using Table 1 to transform (a) the neutral expression
to (b) the smile expression.

Facial expression data acquisition: We analyzed the
facial expressions of twelve model subjects. Each model
subject was asked to perform the six main facial
expressions (happy, sad, anger, fear, disgust and
surprise) from the neutral facial expression state. A
series of frontal photographs of the face were taken to
capture the each of the expressions. The model subject
had several markers positioned on their face for the
duration of the expression photography. The twelve
models participated in the data acquisition stage are
aged from 20-50 years.
Collecting Coordinate points: The image data was
analyzed for each facial expression by collecting the
coordinate points for the neutral expression and the
performed expression (before and after the expression).
The collected coordinate points were grouped into their
facial regions as described in section IV(A).
Normalization: All of the facial expression coordinate
data were normalized as described in section IV (C).
Computing the deformation parameters: For each of
the facial expression coordinate data sets, the
deformation parameter values were computed as
described in section IV (B). This computed result forms
the FDTE for expression E.

μ
example, TABLE I shows the FDTsmile
containing the
deformation parameters for the smile expression. Fig. 8
shows the results from using the deformation parameters to
transform facial regions.
Using the standard deviation ( σ ) and the mean, we can
compute and represent the acceptable range for each of the
deformation parameters as μ ± σ .

VI.

CONCLUSION AND FUTURE WORK

This paper proposed a novel approach in representing
facial expressions based on a quadratic deformation model
applied to muscle regions. The rubber-sheet transformation
functions are used to compute the deformation parameters
for facial regions, where we defined sixteen facial regions
based on the anatomy of the facial muscle system. The
results form a set of FDTs, which we generalized by taking
the average of the FDTs for each expression. The FDTs can
be used and employed in facial expression applications such
as facial recognition and facial animations.
Future research will focus on the following:
• Compress the size of the FDT representations using
suitable data structures.
• Improve the method so a broader range of facial
expressions can be generated using FDTs.

48

TABLE I: DEFORMATION PARAMETERS FOR THE SMILE FACIAL EXPRESSION

Facial Region Number

Deformation Parameters

•
•

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

a1
-0.003
-0.002
-0.016
-0.009
0.001
0.040
-0.050
0.064
-0.024
0.157
-0.107
0.013
0.021
0.318
-0.321
0.044

a2
0.009
0.000
-0.119
-0.019
-0.005
-0.041
-0.088
-0.035
-0.207
-0.007
-0.008
0.050
0.065
0.061
0.066
0.153

a3
-0.002
-0.007
0.093
-0.104
0.104
-0.026
-0.031
-0.022
0.010
0.022
-0.048
-0.006
0.005
0.003
0.003
0.005

a4
0.991
1.005
1.070
0.977
1.001
1.054
1.035
0.978
0.803
1.387
1.222
1.308
1.326
1.609
1.673
1.632

a5
0.007
0.015
-0.045
-0.024
0.026
-0.012
-0.048
-0.023
0.008
-0.012
-0.112
-0.012
0.015
-0.014
0.042
0.015

a6
0.000
0.000
-0.001
0.000
-0.001
0.000
0.000
-0.001
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
[8]

Look at how the FDT representations can be applied
to the MPEG-4 standard applications, such as facial
animations.
Evaluate the proposed facial representation approach
by integrating the FDTs into a facial animation
system.

[9]

ACKNOWLEDGMENT

[10]

The authors would like to thank Karin Fitz for her help
and time with collecting the facial expression data.
REFERENCES
[1]

[2]
[3]
[4]

[5]

[6]
[7]

V. C. Flores. ARTNATOMY (anatomical basis of facial
expression interactive learning tool). In ACM SIGGRAPH
2006 Educators Program (Boston, Massachusetts, July 30 August 03, 2006).
Ekman P and Friesen W, "Manual for the Facial Action
Coding System", Consulting Psychologist 1977, Press Palo
Alto California.
DataFace: Psychology, Appearance, and Behavior of the
Human Face. Retrieved June 6th, 2007, from:
http://www.face-and-emotion.com.
I. Essa and A. Pentland. Coding, Analysis, Interpretation and
Recognition of Facial Expression. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 19(7):757-763,
1997.
M.J. Black and Y. Yacoob, “Tracking and Recognizing
Rigid and Nonrigid Facial Motions Using Local Parametric
Models of Image Motion,” Proc. Int"l Conf. Computer
Vision, pp. 374-381, 1997.
B. Fasel and J. Luettin, "Automatic facial expression
analysis: A survey," Pattern Recognition, vol. 36, no. 2,
2003, pp. 259-275.
I. S. Pandzic and R. Forchheimer, Eds. 2003 Mpeg-4 Facial
Animation: the Standard, Implementation and Applications.
John Wiley & Sons, Inc.

[11]
[12]
[13]

[14]
[15]

[16]
[17]

49

b1
-0.007
0.005
0.009
-0.009
-0.012
-0.078
-0.082
0.132
0.247
-0.113
-0.132
0.107
0.306
-0.173
-0.218
0.003

b2
0.015
0.005
-0.037
0.086
-0.126
0.022
-0.030
-0.021
-0.059
-0.028
0.022
0.018
0.057
-0.322
0.146
0.036

b3
0.011
0.004
0.073
0.101
0.089
0.003
-0.015
0.057
0.010
-0.020
-0.028
0.007
-0.023
0.073
0.038
0.023

b4
-0.018
-0.021
-0.009
-0.015
0.021
-0.194
0.197
-0.022
-0.088
-0.286
0.314
0.047
0.133
-1.521
1.052
0.042

b5
0.987
0.998
0.951
0.994
1.037
1.012
0.992
1.072
1.048
0.931
0.911
0.961
0.972
1.315
1.222
1.110

b6
0.000
0.000
0.001
0.001
0.001
0.000
0.000
0.004
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000

K. Matsumura, Y. Nakamura, and K. Matsui, “Mathematical
Representation and Image Generation of Human Faces by
Metamorphosis”, Electronics and Comm. in Japan-3, vol.
80, no. 1, pp. 36-46, 1997.
M. Pantic and L.J.M. Rothkrantz, “Automatic Analysis of
Facial Expressions: The State of the Art,” IEEE Trans.
Pattern Analysis and Machine Intelligence, vol. 22, no. 12,
pp. 1424-1445, Dec. 1996.
Shimizu, E. and Fuse, T. 2003. Rubber Sheeting of
historical maps in GIS and its application to landscape
visualization of old-time cities: focusing on Tokyo of the
past. University of Tokyo. Proceedings of the 8th
International Conference on Computers in Urban Planning
and Urban Management, Reviewed Papers, CD-ROM, 2003.
http://planner.t.utokyo.ac.jp/member/fuse/rubber_sheeting.pdf.
Wootton, R., Springall, D. R., Polak, J. M. (eds.) (1995).
Image Analysis inHistology—Conventional and Confocal
Microscopy. Cambridge UniversityPress, London.
Goerge Wolberg. Digital Image Warping. IEEE Computer
Society Press, Los Alamitos, California, 1990.
Weisstein, Eric W. "Least Squares Fitting--Polynomial."
From
MathWorld--A
Wolfram
Web
Resource.
http://mathworld.wolfram.com/LeastSquaresFittingPolynom
ial.html.
Rafael C. Gonzalez, Richard E. Woods. Digital Image
Processing. Third Edition, Prentice-Hall, 2007.
British Ordnance Survey. Features of transformations
options.
Retrieved on February 1st,, 2009, from
http://www.ordnancesurvey.co.uk/oswebsite/pai/pdfs/transfo
rmation_descriptions.pdf
Facial Action Coding System. Retrieved June 2007 from
ttp://web.cs.wpi.edu/~matt/courses/cs563/talks/face_anim/ek
man.html.
Ekman, P. & Friesen, W. V. (1975). Unmasking the face. A
guide to recognizing emotions from facial clues. Englewood
Cliffs,
New
Jersey:
Prentice-Hall.

