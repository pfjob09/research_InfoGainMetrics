2009 Sixth International Conference on Computer Graphics, Imaging and Visualization

Real-time Accurate Facial Feature Corner Localization by Rule-Based Selection
Jiaqi Tong, Lizhuang Ma, Zhiliang Xu
Department of Computer Science and Engineering
Shanghai Jiaotong University
Shanghai, P. R. China
Email: tong.jiaqi@sjtu.edu.cn, ma-lz@cs.sjtu.edu.cn, xuzhiliang@sjtu.edu.cn

used by modeling the distribution of these points. Since
gray value is used as the most important information,
they are also not robust enough to occlusion and illconditioned intensity.
In this paper, we try to avoid the sensitivity to image
intensity bias and gains by using SUSAN corner detector
[15] and NCC template matching. This approach is tolerant
to face tilt and rotation because Active Shape Model (ASM)
is used to generate the initial corner positions. What’s
more, the NCC templates in the refinement are transformed
beforehand according to the rotation and scale extent. Realtime processing is achieved by applying hierarchical technologies to get robust rough initialization on down-sampled
images and use original size only in the refinement stage
for accuracy purpose. We have verified the new approach
on the database containing over 5000 faces with various
poses, expressions, occlusions and light conditions. Results
show that robust and accurate facial feature corners can be
obtained in most cases, and the processing time can achieve
20PFS.
In the following, we will first introduce the whole picture
of our facial feature corner detection in Section 2. Then,
in Section 3, the detailed process of the corner detection
and selection is described, followed by the corner refinement
process in Section 4. We show our experimental results and
analysis in Section 5, and finally, the paper is ended with a
conclusion and discussion in Section 6.

Abstract—We propose a coarse-to-fine approach for localizing eye and lip corners, which is accurate and robust, and
can be executed in real-time. Given an image, we first detect
the face and the rough initial positions of eyes and lip in the
coarsest level. In the middle level, SUSAN corner detection is
applied to obtain candidate corners, from which we select a
best one as the facial feature corner. Finally, in the finest level,
the corner is further refined by Normalized Cross-Correlation
(NCC) template matching. We have tested our algorithm on
the database containing over 5000 faces with various poses,
expressions, occlusions and light conditions. The experimental
results show that our method can successfully localize the facial
feature corners with more than 20FPS, which is very appealing.
Keywords-Pattern recognition; Facial features localization;
SUSAN corner detection; Coarse to fine framework

I. I NTRODUCTION
Localization of facial feature corners, such as eyes and lip
corners plays an important role in the various applications
such as expression recognition [1] [2], human computer
interaction [3] [4], facial animation [5] [6], low bit-rate
video coding [7], etc. Although much work has been done
in this area, it is hard to get an algorithm with both desirable
accuracy and speed. Robustness is another key issue since
most of facial feature extraction methods are sensitive to
various non-idealities, such as variations in illumination,
camera noise, orientation and occlusion.
Previous work can be mainly categorized into three
classes.
• Color-based approaches [8] [9] [10]: They build models for human skin and lips, and utilize intensity information to locate the facial features. These approaches
are inherently sensitive to environment distributions
such as different light conditions and camera settings.
• Geometry-based approaches
[11] [12]: These approaches are mainly based on computation of geometric
relationship among facial features. Facial features are
selected so that they best conform to the average
geometric face model. They can detect facial features
efficiently, but are not robust enough to large face tilt.
• Appearance-based approaches [13] [14]: These approaches map the pixel intensities in an area to a point
in high dimensional feature spaces. Classifiers such
as neural networks, SVM, or Bayes classifier can be
978-0-7695-3789-4/09 $25.00 © 2009 IEEE
DOI 10.1109/CGIV.2009.7

II. OVERVIEW OF OUR APPROACH
The input of the whole system is an image with human
faces; the outputs are 6 groups of image coordinates for
the eye corners (both inner and outer corners for two eyes)
and the left and right mouth corners. As is shown in Fig.
1, our approach mainly has three stages, from the coarse
level to the fine level: face detection, rough facial corner
initialization and corner refinement. As the image resolution
is increasing in progress, the search region is more restricted.
This coarse-to-fine framework makes our approach achieve
real-time processing.
In the rough corner initialization stage, the accuracy of
corner positions is not the key concern. So, for the sake of
speed, we first down-sample the whole input image to 1/4 in
both width and height. We apply robust face detection [13]
331

Figure 1.

Flowchart of our facial feature corner localization approach

Figure 3. An overview of corner selection. (a) Refining the eye and
mouth centers, yellow points are the rough initial corners, and the red ones
are refined facial feature centers. (b) SUSAN corner detection results in
the areas where facial feature corners have occurrence potential. (c) Best
corner candidates selected.

Figure 2. Two examples of facial feature initialization given by ASM.
Frontal face results in satisfactory localization (a), while rotated face results
in poor localization (b).

on the down-sampled image, and get rectangles containing
the human faces. For images with multiple faces, we select
the rectangle with the largest area. Active Shape Model
(ASM) [16] matching is then applied to obtain the initial
positions of facial features, still on the down-sampled image.
The improved ASM proposed in [16] converges quickly
since the image size has been scaled down and the initialization positions are reasonable with the help of the face
rectangle. However, since the training set of ASM does not
contain every face pose and lighting condition, sometimes
the corner positions may be different from the real ones. Fig.
2 gives two examples of face detection and facial feature
initialization, in which (a) is quite satisfactory while (b) is
poor. So we need some more steps to get the accurate facial
corners.
After the face detection, we can just process on the Region
of Interest (ROI) of the image containing the face. Then
we normalize the face region and get the facial feature
corner candidates via SUSAN corner detection [15]. The
best corner candidates are selected according to intensity

and geometric rules of the facial features, which will be
discussed in detail in Section 3. The idea is similar to Hess
and Martinez’s framework [17]. However, their approach depends much on the corners and edges around the true facial
feature corners, which may be influenced by eye glasses,
bangs, mustaches and beards. Moreover, their approach is
limited to near frontal faces, since the geometric constraints
they used is based on the mean frontal face. Our approach
is more robust due to the use of robust features such as eye
pupils and a dark path in the center of the mouth.
Since all the previous steps consider down-sampled or
normalized images to enhance the localization accuracy, we
finally refine the corner positions within a neighborhood near
the results of corner candidate selection. This is done by
matching with an adaptively transformed NCC template.
Besides the coarse-to-fine framework, the contributions
of our work lie in the corner candidate selection and corner
position refinement stages. So, next, we will discuss them
in detail.

332

is d = 13 D , where D is the distance between two eye
centers. Formally, we select
pcenter = argminp∈Atriangle (pinit ) s(p),

(1)

where s(p) =
q∈WN ×N (p) I(q), Atriangle (pinit ) is the triangular search area below the initial eye center, WN ×N (p)
is the N × N local square area centered at point p, and I(q)
is the pixel intensity at point q.
If the initial eye center is below the real eye center (i.e.,
the center of pupil), the above approach will wrongly result
in a point even below the initial center since the real pupil
has no chance to be considered. We detect such a case
by the following rule. We first calculate the minimum and
maximum sums, Smin and Smax , as follows:

Figure 4. Two examples of eye center refinement. (a) Search within a
triangular region first, (b) if the initial eye center is below the real eye
center, search in a rectangular region again.

III. C ORNER CANDIDATE SELECTION
Through the face detection and ASM matching on downsampled image, we can get the rough initial facial feature
corner positions efficiently. However, the initial positions are
sometimes not satisfactory (as shown in Fig. 2). To ensure
that the facial feature corners are real corners on the image,
we resort to SUSAN corner detection and selecting a corner
candidate near the expected corner locations.
See Fig. 3 for overview of what is accomplished in
this step. First, in Fig. 3(a), the centers of the eyes and
the mouth are localized according to the rough positions
obtained in the previous step and the image features.
Then, in Fig. 3(b), corners near the eyes and the mouth
are detected by SUSAN corner detector. Finally, the best
candidates are selected as the desired facial feature corners
according to the geometric a priori knowledge, as shown
in Fig. 3(c). In the following, we will explain how to
determine the expected corner locations and the candidate
selection.

Smin = min{S(p)|p ∈ Wd/2×d/2 (pinit )}

(2)

Smax = max{S(p)|p ∈ Wd/2×d/2 (pinit )},

(3)

where Wd×d (pinit ) is the d × d rectangle searching area
centered at the initial eye center. Then we let the threshold
T be T = αSmin − (1 − α)Smax .
When some hair is within the rectangle search area, it
may be detected as the wrong eye center as it may have less
intensity than the pupil. So, set α less than 1 (typically 0.8)
to avoid the influence of hair. If S(pcenter ), then we consider
it as the wrong eye center, and we will re-localize the eye
center as the position with lower local sum of intensity, and
at the same time closer to the initial eye center, formally,
pcenter = argminp∈Wd×d (pinit ) S(p) × e

p−pinit

2

/σ2

, (4)

where S(p), Wd ×d (pinit ) are the same as defined before,
and σ = d4 . We add the geometric restriction term to avoid
wrongly localized eye centers in the hair region.
Having the refined eye centers, we can determine the
two areas in which SUSAN corner detector is applied. We
use two rectangles centered at the eye centers with size
0.75D × 0.35D for both left and right eyes as the corner
detection regions, as the yellow rectangles in Fig. 4(b).
We use the 37-pixel mask in SUSAN corner detector and
select the brightness threshold T adaptively according to the
contrast of the face image.
Having these SUSAN corners detected, for the candidate
selection, we define the expected position of the inner corner
of the left eye as

A. Eye corner candidate selection
Since we select the corner candidates according to the
geometric a priori knowledge, the initial positions of eyes
and mouth centers are very crucial for the accuracy and
robustness of the selection. However, ASM matching does
not always give satisfactory initial center positions. So, we
have to first refine the center positions according to the initial
positions.
The eye center is defined as the center of the pupil.
From the observation that pupils are usually darker than eye
white, we choose the center of the small local area with
the least sum of pixel intensity as the refined pupil position.
To avoid the wrong detection of black hair, we first search
in a triangular area and then in a rectangular area near the
initial eye center (the center of the initial inner and outer
eye corners). Since the face image has been normalized, the
summed area can be fixed as the N × N square region.
We set N to 5 in our experiments empirically. As is shown
in Fig. 4(a), the red triangle is the search region, and the
farthest vertical searching distance from the initial eye center

pexpected = pcenter + (0.2D, 0)T .

(5)

The best inner corner candidate is the point closest to
Pexpected among the SUSAN corner candidates. The outer
corner of the left eye and the corners of the right eye are
selected in the same manner.

333

choose pcorner = argminp CN CC (p), where

B. Mouth corner candidate selection

¯
(I(q) − I(p))
· (T (q − p) − T¯)

It is hard to fix a distance from mouth center to mouth
corners for the different rotation angles. However, we notice
that there is a dark path connecting two mouth corners, no
matter the mouth is shut or open. So we only find a center for
a fixed horizontal image coordinate instead of the accurate
mouth center, and use a path tracing technique to find the
mouth boundary. The fixed horizontal coordinate is given by
the center of the initial mouth corners obtained from ASM
matching. We define the vertical center as the pixel with
least intensity within a search scope of 10 pixels (which is
learned from the training face set) from the initial mouth
center. This is illustrated in Fig. 5(a).
Then we can trace from the vertical center to localize the
expected left / right corner positions. Let’s take the expected
right mouth corner for illustration. From the mouth center,
we go to right one pixel by one pixel in x coordinate.
Suppose in the current step, the pixel coordinate is (x, y).
Then, for the next step, we choose to go to the one with least
intensity among (x + 1, y − 1), (x + 1, y), (x + 1, y + 1).
This process continues until we get two continuous pixels
whose intensity is larger than the threshold T. This can be
defined as
T = Tinit · e−

p−pcenter

2

/σ2

,

CN CC (p) =

q∈Wp
2
¯
(I(q) − I(p))
·
q∈Wp

2
(T (q − p) − T¯)
q∈Wp

(7)
Wp is the local NCC matching window centered at pixel
p, I is the original input image, and T is the NCC template,
whose center is defined at coordinate (0, 0). Since the face
in the original image may have different scales and different
rotation angles, a fixed NCC template cannot deal with such
cases ideally. So we first calculate the scale coefficient s
and the in-plane rotation angle θ relative to the learning face
dataset from the estimated eyes and mouth centers, and then
transform the NCC template accordingly: T = fs ,θ (T ).
The size of the searching neighborhood is also adapted to
the scale coefficient as 3s × 3s pixels.
V. E XPERIMENTAL RESULTS AND ANALYSIS
To evaluate the accuracy and robustness of our approach,
we have collected three groups of faces. The first group is the
CMU PIE (Carnegie Mellon University, Pose, Illumination
and Expressions) database [18]. We label the facial feature
corners by hand, use them as the training set for ASM model
and calculate the standard NCC templates. The second
dataset contains 35 groups of photos of totally 80 people
with a neutral expression. About 30% of them wear glasses,
and about 30% of them have long hairs. Each group has a
different face pose, ranging from −30◦ to −30◦ of right /
left rotation, and from −20◦ to 20◦ of up / down rotation.
This dataset is used to evaluate the robustness to non-frontal
faces of our approach. The last dataset is 500 face images
collected from Internet, such as Flicker [19], Google Image
Search [20] and personal photos collected by our colleagues.
This dataset is applied to test the robustness to different
image quality, such as resolution, lighting effect, different
expressions, blur, etc.
For evaluation purpose, we manually label all the facial
feature corners for all the datasets. We define the successful
localization of eye corners as within 6% of D (as defined
in Sect. 3.1), and that of eye corners as within 10% of D
from the true corners. The success rate for each group in the
second group is listed in Table I. We can see that the success
rate is quite good for the frontal faces, and deteriorates
slowly as the rotation angle increases.
A comparison of our approach with Hess and Martinez’s
[17] is shown in Fig. 6. Our results are shown in Fig. 6(a)
and (c), while Hess and Martinez’s are shown in Fig. 6(b)
and (d). Their approach fails to detect the correct eye corners
mainly due to the thick border of eye glasses and the large
head rotation respectively. All the six facial feature corners
have been detected correctly with our approach.
Fig. 7 lists some successful cases as well as the failed
ones. Our approach works well for most of faces, but in the

(6)

where pcenter is the refined mouth corner, σ = D
2 , and Tinit
is the mean intensity of the mouth region. As is shown in
Fig. 5(b) and (c), this approach is robust enough for both
shut and open situations. Even though the teeth are visible,
there is, for most of the time, still a path with the lowest
intensity. The expected position for the left mouth corner is
obtained in the same manner.
The path tracing approach can give us more robust estimation than ASM matching does for the expected mouth
corners. But the expected corner positions may still be
different from the actual mouth corners, e.g., due to special
lighting conditions. So, similar to eye corners, the best left
/ right mouth corner candidates are selected, which are the
ones closest to the expected corners among SUSAN corner
candidates.
IV. C ORNER POSITION REFINEMENT
Usually, the normalized images are down-sampled compared to the original one. In such a case, the selected
facial feature corners obtained in the last step need further
refinement. To do so, we first learn six templates for the
six feature corners by averaging the 15 × 15 pixels areas
centered at the corners of the labeled face database. Then,
NCC is adopted in the small neighborhood centered at the
selected corner candidates and the refined corners are the
ones which minimize the NCC matching cost. Formally, we

334

,

Table I
S UCCESS RATES FOR FACES WITH DIFFERENT ROTATION ANGLES

Up 20◦
Up 10◦
Center
Down 10◦
Down 20◦

Left 30◦
0.66
0.76
0.81
0.77
0.72

Left 20◦
0.70
0.84
0.86
0.79
0.78

Left 10◦
0.79
0.92
0.89
0.85
0.79

Center
0.86
0.90
0.93
0.87
0.84

Right 10◦
0.81
0.94
0.91
0.87
0.78

Right 20◦
0.72
0.81
0.88
0.81
0.75

Right 30◦
0.65
0.74
0.79
0.80
0.71

Figure 5. Finding the expected mouth corners. (a) Finding the pixel with least intensity as the start point for path tracing. (b) Path tracing for shut mouth.
(c) Path tracing for open mouth

Figure 6. Comparison of our results with Hess and Martinez’s [17]. (a) and (c) are obtained using our approach, while (b) and (d) are obtained using
Hess and Martinez’s

Figure 7.

More successful (a, b) and failed (c, d) examples obtained by our approach

335

case of highlight reflection on the eye glasses (Fig. 7(c)) and
the very large head rotation (Fig. 7(d)), the approach fails
to present the correct corners. The cause is that SUSAN
corner detection is based on pixel intensities and the corner
candidate selection rule is based on geometry.

[5] Ming Shing Su, Ming Tat Ko, Kuo-Young Cheng.Control of
Feature-Point-Driven Facial Animation Using a Hypothetical
Face. Computer Graphics Forum, 2001, also presented in
Proceedings of Pacific Graphics 2000, 359-368, 2000.
[6] Ludovic Dutreve, Alexandre Meyer, Sa¨ıda Bouakaz. Feature
points based facial animation retargeting. 15th ACM Symposium on Virtual Reality Software and Technology, pp. 197200, Bordeaux, France, 2008

VI. C ONCLUSION AND DISCUSSION
In this paper, we propose an accurate facial corner localization approach. The approach can achieve real-time
processing since it utilizes a coarse-to-fine framework.
Global matching - the ASM algorithm - is adopted in the
coarsest level to obtain rough initial corner positions. In the
middle level, the corner candidate selection is done near the
rough initial positions. And in the finest level, dense NCC
matching is adopted in a very small neighborhood around
the selected corner candidates.
To select the real corners in the face image, we adopt
SUSAN corner detector in small areas near the face features,
and use geometry a priori knowledge to select the best
candidates from the SUSAN corners. To refine the corner
positions in the finest level, we transform the NCC templates
according to the rotation angle and scale obtained from the
feature centers, and search for the best matches in the small
neighborhoods of the SUSAN corner candidates.
The experiments show that our approach can detect facial
feature corners both accurately and robustly, except for the
cases with the large face rotation or very ill conditions, such
as highlight reflection on the eye glasses. To deal with these
cases is a challenging future work.

[7] K. Aizawa, T. S. Huang. Model-based image coding: Advanced video coding techniques for very low bit-rate applications. In Proc. IEEE, vol. 83, pp. 259-271, Feb. 1995.
[8] D. Chai and K. N. Ngan. Locating facial region of a headand-shoulders color image. International Conference on Automatic Face and Gesture Recognition, 124-129, 1998.
[9] R.L. Hsu, M. Abdel-Mottaleb, A.K. Jain. Face detection in
color images. IEEE Trans. Pattern Anal. Machine Intelligence
24(5), pp. 696-706, 2002.
[10] Z. Liu, J. Yang, N.S. Peng. An efficient face segmentation
algorithm based on binary partition tree. Signal Processing:
Image Communication 20(4) 295-314, 2005.
[11] A. Al-Qayedi and A. F. Clark. An algorithm for face and
facial-feature location based on grey-scale information and
facial geometry. In Proc. IEEE International Conference on
Image Processing and its Applications, vol. 2, pp. 625-629,
Manchester, UK, July 1999.
[12] H. Sako, A.V.W. Smith. Real-time facial expression recognition based on features’ positions and dimensions. In Proceedings of the 13th International Conference on Pattern
Recognition, Vol. 3, pp. 643-648, 1996.

ACKNOWLEDGMENT
This work is partly supported by the National Grand
Fundamental Research 973 Program of China under grant
No.2006CB303105, the National Natural Science Foundation of China under Grant No.60873136, and the collaboration research project with Omron Corporation, Japan.

[13] P. Viola, M.J. Jones. Robust Real-Time Face Detection. International Journal of Computer Vision 57(2): 137-154, 2004.

R EFERENCES

[15] S. M. Smith, J. M. Brady. SUSAN-A New Approach to Low
Level Image Processing. Int’l J. Computer Vision, vol. 23, no.
1, pp. 45-78, 1997.

[14] C. Liu. A Bayesian discriminating features method for face
detection. IEEE Transactions on Pattern Analysis and Machine Intelligence 25(6) pp. 725-740, 2003.

[1] G. Littlewort, I. Fasel, J. R. Movellan. Real time face detection and facial expression recognition: Development and
applications to human computer interaction. In Proceedings
of the International Conference on Computer Vision and
Pattern Recognition Workshop, Madison, WI, vol. 5, pp. 5358, 2003.

[16] Stephen Milborrow, Fred Nicolls. Locating Facial Features
with an Extended Active Shape Model. In Proc. European.
Conference on Computer Vision, pp. 504-513, 2008.
[17] M. Hess, G. Martinez. Facial feature extraction based on
the smallest univalue segment assimilating nucleus (SUSAN)
algorithm. In Proceedings of the Picture Coding Symposium
(PCS ’04), San Francisco, Calif, USA, December 2004.

[2] Y. Zhang, Q. Ji. Active and dynamic information fusion for
facial expression understanding from image sequences. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
vol. 27, no. 5, pp. 699-714, 2004.

[18] T. Sim, S. Baker, M. Bsat. The CMU Pose, Illumination, and
Expression (PIE) Database of Human Faces. CMU-RI-TR01-02, pp. 1-17, USA, 2002.

[3] Alejandro Jaimes, Nicu Sebe.Multimodal human-computer
interaction: A survey. Computer Vision and Image Understanding, vol.108 no.1-2, p.116-134, October, 2007.

[19] Flicker. http://www.flickr.com

[4] Xiaozhou Wei, Lijun Yin, Zhiwei Zhu, Qiang Ji. Avatarmediated face tracking and lip reading for human computer
interaction. ACM Multimedia 2004: 500-503.

[20] Google Image Search. http://images.google.com

336

