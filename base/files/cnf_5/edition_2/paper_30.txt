2009 Sixth International Conference on Computer Graphics, Imaging and Visualization

A Framework for Evaluating Human Action Detection via
Multidimensional Approach
Lili N.A.
Dept of Multimedia, Faculty Computer Science and Information Technology, 43400 UPM
Serdang, Selangor, Malaysia
liyana@fsktm.upm.edu.my

Abstract

When either audio or visual information alone is not
sufficient, combining audio and visual features may
resolve the ambiguities and to help to obtain more
accurate answers Unlike the traditional methods that
analyze audio and video data separately, this research
presents a method which able to integrate audio and
visual information for action scene analysis. The
approach is top-down for determining and extract
action scenes in video by analyzing both audio and
visual data. A multidimensional layer framework was
proposed to detect action scene automatically. The
first level extracts low level features such as motion,
edge and colour to detect video shots and next we use
Hidden Markov model(HMM) to detect the action.
An audio feature vector consisting of n audio features
which is computed over each short audio clip for the
purpose of audio segmentation was used too. Then it
is time to decide the fusion process according to the
correspondences between the audio and the video
scene boundaries using an HMM-based statistical
approach. Results are provided which prove the
validity of the approach. The approach consists of
two stages: audiovisual event and semantic context
detections. HMMs are used to model basic audio
events, and event detection is performed. Then
semantic context detection is achieved based on
Gaussian mixture models, which model the
correlations among several action events temporally.
With this framework, the gaps between low-level
features and the semantic contexts that last in a time
series was bridged. The experimental evaluations
indicate that the approach is effective in detecting
high-level semantics such as action scene.

This work discusses the application of an Artificial
Intelligence technique called data extraction and a
process-based ontology in constructing experimental
qualitative models for video retrieval and detection.
We present a framework architecture that uses
multimodality
features
as
the
knowledge
representation scheme to model the behaviors of a
number of human actions in the video scenes. The
main focus of this paper placed on the design of two
main components (model classifier and inference
engine) for a tool abbreviated as VASD (Video
Action Scene Detector) for retrieving and detecting
human actions from video scenes. The discussion
starts by presenting the workflow of the retrieving
and detection process and the automated model
classifier construction logic. We then move on to
demonstrate how the constructed classifiers can be
used with multimodality features for detecting
human actions. Finally, behavioral explanation
manifestation is discussed. The simulator is
implemented in bilingual; Math Lab and C++ are at
the backend supplying data and theories while Java
handles all front-end GUI and action pattern
updating.
Keywords: audio feature, visual feature, hidden
Markov model, human action detection
1. Introduction
Action is the key content of all other contents in the
video. Action recognition is a new technology with
many potential applications. Action recognition can
be described as the analysis and recognition of
human motion patterns. Understanding activities of
objects, especially humans, moving in a scene by the
use of video is both a challenging scientific problem
and a very fertile domain with many promising
applications. Use of both audio and visual
information to recognize actions of human present
might help to extract information that would improve
the recognition results. What to argue is that action is
the key content of all other contents in the video. Just
imagine describing video content effectively without
using a verb. A verb is just a description (or
expression) of actions. Action recognition will
provide new methods to generate video retrieval and
categorization in terms of high-level semantics.

978-0-7695-3789-4/09 $25.00 © 2009 IEEE
DOI 10.1109/CGIV.2009.48

2. Related Works
The problem of human action recognition is
complicated by the complexity and variability of
shape and movement of the human body, which can
be modelled as an articulated rigid body. Moreover,
two actions can occur simultaneously, e.g., walk and
wave. Most work on action recognition involving the
full human body is concerned with actions
completely described by motion of the human body,
i.e., without considering interactions with objects.
Previous works on audio visual content analysis were
quite limited and still at a preliminary stage. Most of
the approaches are focused on visual information
such as colour histogram differences, motion vectors
and key frames [3,4,5]. Colour histogram difference
186

anymore, but it can directly query on concept level.
The user can for example query an action in which a
man is punching another man, which might cause a
violent later on. As a result a more advanced search
engine is created that can be used for human action
purposes.

and motion vector between video frames or objects
are the most common features in the scene
recognition algorithms. Although such features are
quite successful in the video shot segmentation,
scene detection based on such visual features alone
poses many problems. The extraction of relevant
visual features from an images sequence, and
interpretation of this information for the purpose of
recognition and learning is a challenging task. An
adequate choice of visual features is very important
for the success of action recognition systems.

Video
Acquisit
ion

Because of the different sets of genre classes and the
different collections of video clips these previous
works chose, it is difficult to compare the
performance of the different features and approaches
they used.

Binary Level
lConcept Level

Action
Recognit
ion and
Inferenc
e

Feature
Extracti
on

Feature

Presenta
tion
Concept
Level
Informati
on

Leve

Figure 1.0 Metadata Generation Process
Automatic interpretation of human action in videos
has been actively done for the past years. The
investigation of human motion has drawn a great
attention from researches in computer vision or
computer graphics recently. Fruitful results can be
found in many applications such as visual
surveillance. Features at different levels have been
proposed for human activity analysis. Basic image
features based on motion histogram of objects are
simple and reliable to compute (Efros et al. 2003). In
(Stauffer & Grimson 2000), a stable, real-time
outdoor tracker is proposed, and high-level
classifications are based on blobs and trajectories
output from this tracking system. In (Zelnik-Manor
& Irani 2001), dynamic actions are regarded as longterm temporal objects, and spatio-temporal features
at multiple temporal. There is a huge body of
literature on the topics of visual tracking, motion
computation, and action detection. As tools and
systems for producing and disseminating action data
improve significantly, the amount of human action
detection system grows rapidly. Therefore, an
efficient approach to search and retrieve human
action data is needed.

In this paper, the feature extraction process (Figure
2.0) is discussed: converting the binary level
information coming from the video acquisition
system into feature level information suited for
further processing. From the flowchart (Figure 3.0),
we have four levels of processes:
•
•
•
•

Pixel level represents the average percent of
the changed pixels between frames within a
shot
Histogram indicates the mean value of the
histogram difference between frames within
a shot
Segmentation level to indicate background
and foreground areas
Object tracking based on observation:
flames, explosion, gun

3. Methodology
The process of video action detection is depicted in
Figure 1.0.
There are three classes of information. The main type
of information, which serves as the input of the
complete system, is the binary level information,
which comprises the raw video files. This binary
level information is analyzed which results in the so
called feature level information, i.e. features
interesting for detecting certain action like the
measurements of action speeds. Finally, from this
feature level information, actions are detected that are
regarded as the concept level information. When a
user searches information from the multimedia, it
does not have to browse the binary level video files

Figure 2.0 The overview of the system

187

Classification is a pattern recognition (PR) problem
of assigning an object to a class. Thus the output of
the PR system is an integer label. The task of the
classifier is to partition the feature space into classlabeled decision regions. Basically, classifiers can be
divided into parametric and non-parametric systems
depending on whether they use statistical knowledge
of the observation and the corresponding class. A
typical parametric system is the combination of
Hidden Markov Model (HMM) and Gaussian
Mixture Model (GMM), which assumes Gaussian
distribution of each feature in the feature vector.

Figure 3.0 Flow chart of feature extraction process
3.1 Segmentation

3.4 Posture and Gesture

The segmentation and detection stages are often
combined and simply called object detection.
Basically the task of the segmentation is to split the
image into several regions based on color, motion or
texture information, whereas the detection stage has
to choose relevant regions and assign objects for
further processing.

Based on our overall system approach we treat the
human body posture recognition as a basic
classification task. Given a novel binary object mask
to be classified, and a database of samples labeled
with possible body postures, the previously described
shape descriptors are extracted and the image is
classified with a chosen classifier. This can be
interpreted as a database query: given a query image,
extract suitable descriptors and retrieve the best
matching human body posture label. Other similar
queries are possible, resulting in a number of useful
applications, such as skeleton transfer, body posture
synthesis and figure correction.

We follow the assumption that the video sequence is
acquired using a stationary camera and there is only
very little background clutter. Based on this we use
background differencing followed by threshold to
obtain a binary mask of the foreground region. In
order to remove noise median filtering and
morphological operations are used. Regions of
interest (ROI) are detected using boundary extraction
and a simple criteria based on the length of the
boundary. Boundary filling is applied to each ROI
and the resulting binary object masks are given to the
description stage.

3.5 HMM and GMM
An approach for action recognition by using Hidden
Markov Model (HMM) and Gaussian Mixture
Modelling (GMM) to model the video and audio
streams respectively will be proposed. HMM will be
used to merge audio-visual information and to
represent the hierarchical structure of the particular
violence activity. The visual features are used to
characterize the type of shot view. The audio features
describe the audio events within a shot (scream,
blast, gun shots). The edge, motion (orientation and
trajectory) information is then input to a HMM for
recognition of the action. For each type of violent
activity (punch and kick), a HMM will be build to
characterize the action and interaction processes as
observation vectors. A HMM for each action is
trained with the corresponding training MPEG video
sequences (kick, punch, run, walk, stand, etc.). The
expectation maximization (EM) and GMM
approaches are then used to classify testing data
using the trained models. Once the mean and
covariance of the Gaussian model of the training
audio data are obtained, the likelihood ratio between
the input audio track and the sound classes, is
computed to determine which class the associated
sound belong to.

3.2 Object Descriptor
The object description refers to a set of features that
describe the detected object in terms of color, shape,
texture, motion, size etc. The goal of the feature
extraction process is to reduce the existing
information in the image into a manageable amount
of relevant properties. This leads to a lower
complexity and a more robust description.
Additionally, the spatial arrangement of the objects
within the video frame and as related to each other is
characterized by a topology.
A very important issue for the performance of a
subsequent classification task is to select a suitable
descriptor that expresses both the similarity within a
class and the distinctions between different classes.
Since the classifier strongly depends on the
information provided by the descriptor it is necessary
to know its properties and limitations relating to this
specific task. In case of human body posture
recognition it is obvious that shape descriptors are
needed to extract useful information.
3.3 Classification

188

using the image pre-processing operations described
below:
a) preparing background
b) blurring the images with a low pass filter
c) extracting human area
d) binarized the extracted images so that the
white and black pixels corresponded to
background and human areas

4. Experimental Discussion
By using these processes for action detection, the
result of successful detected action is depicted in
Table 1. As is seen, sitting is characterized as the
most significant motion with 80% of success rate.
This is due to less motion activities involved.

Table 2: Action detection results by audio only,
audiovisual combination

TABLE 1 Classification of the individual action
sequences
Type
of
Sequence

Total
Number

Correctly
Classified

%
Success

Standing

4

3

75

Sitting

5

4

80

Walking

3

2

67

Punching

5

3

60

Falling

4

3

75

Kicking

6

3

50

Running

2

1

50

By audio only
Feature
Video
Clip
A
B

These actions were taken from the dataset itself. For
example, most of the standing and walking scenes
were collected from movie I, Robot. Most of the
falling scenes were captured from Rush Hour, sitting
and punching from Charlie’s Angel, and kicking and
running from Matrix. Figure 4 shows some scenes
that demonstrate these actions for classification

Precision

Recall

Precision

Recall

15/22
= 68.2%
14/16
= 87.5%

15/2
0 = 75%
14/1
5
=
93.3%
55/8
4
=
65.5%
51/7
3
=
69.9%
11/2
8
=
39.3%
75/9
8
=
76.5%
40/9
3
=
43.0%
66.07%

20/27
= 74.1%
15/16
= 93.8%

20/2
0 = 100%
15/1
5 = 100%

84/89
= 94.4%

84/8
4 = 100%

70/79
= 88.6%

70/7
3
=
95.9%
27/2
8
=
96.4%
93/9
8
=
94.9%
92/9
3
=
98.9%
98.01%

C

55/77
= 71.4%

D

51/87
= 58.6%

E

11/15
= 73.3%

F

75/10
9 = 68.8%

G

40/49
= 81.6%

AVE
RAGE

By
audio/visual
combination

72.77%

27/30
= 90.0%
93/10
6 = 87.7%
92/98
= 93.9%
88.93%

5. Conclusion
The proposed algorithm is implemented in C++ and
it works on an Intel Pentium 2.56GHz processor. As
described above HMMs are trained from falling,
walking, and walking and talking video clips. A total
of 64 video clips having 15,823 image frames are
used. Some image frames from the video clips are
shown in Figure 4. In all of the clips, only one
moving object exists in the scene.
In summary, the main contribution of this work is the
use of both audio and video tracks to decide a fall in
video. The audio information is essential to
distinguish a falling person from a person simply
sitting down or sitting on a floor. To prove the
usefulness of the proposed method, the experiments
were performed to evaluate the detection
performance with several video genres. The
experimental results show that the proposed method
to detect action scenes gives high detection rate and
reasonable processing time.

Figure 4.0 Some examples of video clips
We will be testing our algorithm on action movies.
The original frames sequences captured contain
complicated background and the positions of the
actor/actresses moves during sequence. So, human
area extraction and tracking are needed. We will be

6. References

189

1.

2.

3.

4.

5.

L. Zelnik-Manor and M. Irani, “Event-based
Analysis of Video”. Proceedings of IEEE
Conference Computer Vision and Pattern
Recognition, 2001.
C. Stauffer and W.E.L. Grimson, “Learning
Patterns of Activities using Real-Time
Tracking”. Journal of IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol
(22), no. (8), pp. 747 – 757, 2001.
A.A. Efros, A.C. Berg, G. Mori and J. Malik,
“Recognizing
Action
at
a
Distance”.
Proceedings of International Conference on
Computer Vision, 2003.
S. Fischer, R. Lienhart and W. Effelsberg,
“Automatic Recognition of Film Genres”,
Proceedings of ACM Multimedia, pp. 295 – 304,
2003.
G. Tzanetakis and P. Cook, “Musical Genre
Classification of Audio Signals”, IEEE Trans.
On Speech and Audio Processing, vol. 10, no. 5,
pp. 293 – 302, 2002.

190

