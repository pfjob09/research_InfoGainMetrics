2009 Sixth International Conference on Computer Graphics, Imaging and Visualization

Two Dimensional Compressive Classifier for Sparse Images
1

Armin Eftekhari , Hamid Abrishami Moghaddam1, Massoud Babaie-Zadeh2
1
K.N. Toosi University of Technology, Tehran, Iran
2
Sharif University of Technology, Tehran, Iran
{a.eftekhari@ee.kntu.ac.ir, moghadam@eetd.kntu.ac.ir, mbzadeh@ ee.sharif.edu}
compressive detection for arbitrary signals has been
considered [1, 2]. Extending these results, [2] considered
the problem of multiple hypothesis testing in compressed
domain and [1] proposed a compressive classifier dubbed
smashed filter, which requires knowledge of class
manifolds for successful operation.
However, as shown later in this paper, direct
extension of these results to image domain (2D) is
computationally prohibitive, which strongly hinders the
application of conventional compressive classifier (1DCC) in real-world scenarios. To overcome this major
drawback, the idea of 2D compressive classification is
developed in this paper. First, 2D random projection
scheme is introduced in Section 2 and associated
concentration properties are studied. It is then observed
that Gaussian random matrices, as the most common
choice in 1D compressive framework, are not
appropriate for our 2D random projection scheme. Then,
by adding an assumption of so-called 2D sparsity (in
some basis) to images, desirable concentration properties
are proved for the same set of admissible random
matrices as in 1D framework. This assumption is not
restrictive, as most images become sparse under wellknown transformations, like DCT, or have a sparse edge
map. In Section 3, these findings are exploited to develop
a 2D compressive classifier (2D-CC) for sparse images,
along with derivation of error bound for an important
special case. Finally, 2D-CC is applied to retinal
identification within a realistic setting. It is observed
that, at worst, 2D-CC provides significant saving on
computational load and memory requirements compared
to 1D-CC, at the cost of negligible loss in performance.
This performance loss, however, can be avoided by
“wise” choice of parameters. We note that, to emphasize
on the main result of the article, some intermediate steps
of the derivations have been omitted, and the interested
reader is referred to [5] for details.

Abstract
The theory of compressive sampling involves making
random linear projections of a signal. Provided signal is
sparse in some basis, small number of such
measurements preserves the information in the signal,
with high probability. Following the success in signal
reconstruction, compressive framework has recently
proved useful in classification, particularly hypothesis
testing. In this paper, conventional random projection
scheme is first extended to the image domain and the key
notion of concentration of measure is closely studied.
Findings are then employed to develop a 2D compressive
classifier (2D-CC) for sparse images. Finally,
theoretical results are validated within a realistic
experimental framework.
Keywords--- Compressive sampling,
projections, retinal identification

random

1. Introduction
The recently developed theory of compressive
sampling (CS) involves making random linear
projections of a signal by multiplying a random matrix.
Provided the signal is sparse in some basis, few random
projections preserve the information in the signal, with
high probability [1]. This remarkable result is rooted in
the concentration of measure phenomenon [1], which
implies that the Euclidean length of a vector is uniformly
“shrunk” under a variety of random projection matrices,
with high probability. Due to tangible advantages, CS
framework has found many promising applications in
signal and image acquisition, compression, and medical
image processing [1, 2, 4]. The CS community, however,
has mainly focused on the signal reconstruction problem
from random projections to date [3], and other
applications of CS have yet remained unexplored. In
particular, few recent studies showed that classification
can be accurately accomplished using random
projections, which suggests random projections as an
effective and reliable, yet universal feature extraction
and dimension reduction tool. In this context,
978-0-7695-3789-4/09 $25.00 © 2009 IEEE
DOI 10.1109/CGIV.2009.68

2. Concentration of Measure for 2D Random
Projection Scheme
In order to linearly project a given image  ‫א‬
Թ௡భൈ௡మ to lower dimensional space, columns of  are
conventionally stacked into a vector  ൌ ሺሻ. This
402

corresponding to the indices in ܴ௜ . Obviously, rows of
ோ೔ are independent. Now, we may write ԡԡଶଶ ൌ
௤
σ௜ୀଵԡோ೔ ԡଶଶ , where entries of ோ೔ are zero-mean rv’s
with variance ͳȀ݊ଵ ݊ଶ . It is then straightforward to show
that strong concentration of ோ೔ for ݅ ൌ ͳǡ ǥ ǡ ‫ݍ‬, implies
that of  [5]. This follows easily from the following fact:
݉ଵ ݉ଶ
݉ଵ ݉ଶ
ԡԡଶଶ ฬ ൒
 ൤ฬԡԡଶଶ െ
Ԗ൨ ൑
݊ଵ ݊ଶ
݊ଵ ݊ଶ
௤
(2)
ȁܴ௜ ȁ
ȁܴ௜ ȁ
ԡԡଶଶ ቤ ൒
෍  ቈቤԡோ೔ ԡଶଶ െ
߳቉
݊ଵ ݊ଶ
݊ଵ ݊ଶ

process, however, ignores the intrinsic row/column-wise
structure of the image and, even for moderately sized
matrices, involves prohibitive computational load and
memory requirements for generation and manipulation of
the projection matrix. As a remedy to these drawbacks,
one may use so-called 2D projection scheme, i.e.
 ൌ  ୘ , in which  ‫ א‬Թ௠భ ൈ௠మ ,  ‫ א‬Թ௠భൈ௡భ ,
 ‫ א‬Թ௠మ ൈ௡మ , with ݉ଵ ൏ ݊ଵ and ݉ଶ ൏ ݊ଶ . Note that, in
terms of storage requirements, 2D projection scheme
requires only ݉ଵ ݊ଵ ൅ ݉ଶ ݊ଶ memory units for projection
matrices, whereas conventional projection scheme to
Թ௠భ ௠మ requires ݉ଵ ݉ଶ ݊ଵ ݊ଶ memory units. Similarly,
using a naive matrix multiplication procedure,
computational complexity of 2D projection scheme is an
order less than conventional projection scheme to
Թ௠భ ௠మ . In this section, properties of 2D projection
scheme in the compressive framework are studied.
Clearly, successful inference in this scenario
depends on preservation of the structure of samples after
projection [1]. For 1D signals, this has received
extensive treatment. Particularly, it has been shown that,
given a 1D signal, random matrices whose entries are
i.i.d. random variables (rv’s) with proper tail bounds, e.g.
Gaussian rv, uniformly “shrink” the signal length after
projection, with high probability. Using the union bound,
one may then show that, with high probability, such
random matrices preserve the structure of a set of
samples after projection, by uniformly shrinking the pairwise Euclidean distances. Focusing on the Gaussian
random matrices, as the most common choice in CS
framework, a similar strategy is developed in the
following. Consider random Gaussian matrices  ‫א‬
Թ௠భ ൈ௡భ and  ‫ א‬Թ௠మ ൈ௡మ , whose entries are i.i.d.
ࣨሺͲǡͳȀ݊ଵ ሻ and ࣨሺͲǡͳȀ݊ଶ ሻ rv’s, respectively. Given
 ‫ א‬Թ௡భൈ௡మ , let us define  ൌ ሺሻ and  ൌ ۪,
where ۪ denotes the Kronecker product. Thus,  ୘ ൌ
, where each entry of  has a Bessel-like distribution
with zero mean and variance ͳȀ݊ଵ ݊ଶ [5]. We are
interested in studying the concentration of rv ԡԡଶଶ
about its expected value ሺ݉ଵ ݉ଶ Τ݊ଵ ݊ଶ ሻԡԡଶଶ by finding
an exponentially-fast decreasing bound on:
݉ଵ ݉ଶ
݉ଵ ݉ଶ
ԡԡଶଶ ฬ ൒
 ൤ฬԡԡଶଶ െ
߳൨
݊ଵ ݊ଶ
݊ଵ ݊ଶ
(1)

௜ୀଵ

For ease of notation, let  ൌ ோ೔ and ܴ ൌ ȁܴ௜ ȁ.
Now, it suffices to find an exponentially-fast decreasing
bound for ሼȁԡԡଶଶ െ ܴΤ݊ଵ ȁ ൒ ܴ߳ Τ݊ଵ ݊ଶ ሽ, where, due
to linearity, each column of  is assumed to have unit
length. Consider the probability of ԡԡଶଶ െ ܴΤ݊ଵ ൒
ܴ߳ Τ݊ଵ ݊ଶ . Then, for any ݄ ൐ Ͳ, invoking the Chernoff
bounding technique and using the inherent symmetries in
 [5] gives:
 ൤ԡԡଶଶ െ
൑ ॱ ቈ݁

ܴ
ܴ
൒
߳൨
݊ଵ ݊ଵ ݊ଶ
మ ோ

೙ ೙

భ మெ ௫ ቁ
௛ቀσೕసభ
భǡೕ ೕ

቉ ݁

ோ
ି
௛ሺ௡మ ାఢሻ
௡భ ௡మ

(3)

where, without loss of generality, first row of  is used
in (3). Further simplifications and exploiting the
moments of Gaussian and Chi-square distributions [5]
leads to:
ॱ ቈ݁

೙ ೙

మ

భ మெ ௫ ቁ
௛ቀσೕసభ
భǡೕ ೕ

቉

݊ଵ
(4)
ʹ݄ ௞ ʹ݇Ǩ ī ቀ݇ ൅ ʹ ቁ
൰

݊
݊ଵ ݊ଶ ሺ݇Ǩሻଶ
ī ቀ ଵቁ
௞ୀ଴
ʹ
in which īሺȉሻ denotes the Gamma function. The sum on
the right hand side of (4) apparently fails to converge.
Consequently, the probability of ԡԡଶଶ െ ܴΤ݊ଵ ൒
ܴ߳ Τ݊ଵ ݊ଶ fails to decrease exponentially fast, in general;
not letting us to extend the random projection scheme to
image domain for random Gaussian matrices. In the
following, we prefer to insert an additional constraint on
, which would allow for successful operation of 2D
random projection scheme with the same set of
admissible random matrices as in 1D case. Let ȭ௞ denote
the set of all signals of length  with at most  nonzero
entries. We say that  ‫ א‬Թ௠ൈ௡ satisfies restricted
isometry property (RIP) of order ݇, if for every  ‫ א‬ȭ௞ ,
the following holds for some ߳௞ ‫ א‬ሾͲǡͳሻ.
݉
݉
ሺͳ െ ߳௞ ሻ ԡԡଶଶ ൑ ԡԡଶଶ ൑ ԡԡଶଶ ሺͳ ൅ ߳௞ ሻ
(5)
݊
݊
ஶ

൑ ෍൬

where probability is taken over all matrices  and .
Note that (in contrast to 1D counterpart), entries of  are
no more independently distributed. Furthermore, each
entry of  is a product of two i.i.d. Gaussian rv’s.
Adopting the idea presented in [4], let us construct an
undirected graph ‫ ܩ‬ൌ ሺܸǡ ‫ܧ‬ሻ with ܸ ൌ ሼͳǡ ǥ ǡ ݉ଵ ݉ଶ ሽ,
where any two pair of vertices are connected by an edge
iff corresponding rows in  are dependent. Clearly, each
row of  is dependent with exactly ‫ ݍ‬െ ͳ ൌ ݉ଵ ൅ ݉ଶ െ
ʹ other rows in . Thus, maximum degree of ‫ ܩ‬is ‫ ݍ‬െ ͳ,
and we can partition ‫ ܩ‬into ‫( ݍ‬or more) nonoverlapping
௤
partitions ሼܴ௜ ሽ௜ୀଵ , such that vertices in each partition do
not share any edges [4, 12]. Next, let ோ೔ be the ȁܴ௜ ȁ ൈ
݊ଵ ݊ଶ submatrix obtained by retaining the rows of 

Using arguments on tail bounds, it is shown that
many random matrices satisfy RIP condition with high
probability [1, 4]. For instance, ݉ ൈ ݊ matrices whose
entries are i.i.d. ࣨሺͲǡͳȀ݊ሻ rv’s satisfy RIP of order ݇
with probability exceeding ͳ െ ʹ݁ ି௖మሺఢೖሻ௠ , provided
݇ ൑ ܿଵ ሺ߳௞ ሻ݉ ሺ݊Τ݇ ሻ, where ܿଵ ሺ߳௞ ሻ and ܿଶ ሺ߳௞ ሻ depend
403

only on ߳௞ [1, 4]. Also random orthoprojectors, i.e.
matrices with random orthonormal rows, satisfy the RIP
condition similarly. Extending these ideas to image
domain, let ȭ௞ೝ ǡ௞೎ denote the set of all images  ‫א‬
Թ௡భൈ௡మ , whose nonzero entries are distributed in at most
݇௥ rows and ݇௖ columns. A matrix with this property will
be called 2D sparse matrix. Now, we can make the
following observation [5].
Observation 1. Suppose projection matrices  ‫א‬
Թ௠భ ൈ௡భ and  ‫ א‬Թ௠మ ൈ௡మ , respectively, satisfy the RIP
conditions of orders ʹ݇௥ and ʹ݇௖ , for some ߳ଶ௞ೝ ǡ ߳ଶ௞೎ ‫א‬
ሾͲǡͳሻ. Then, for any  ‫ א‬ȭଶ௞ೝ ǡଶ௞೎ , we have:
݉ଵ ݉ଶ
ԡԡଶଶ ൑ ԡ ୘ ԡଶଶ 
݊ଵ ݊ଶ
݉ଵ ݉ଶ
൫ͳ ൅ ߳ଶ௞ೝ ൯൫ͳ ൅ ߳ଶ௞೎ ൯
൑ ԡԡଶଶ
݊ଵ ݊ଶ

ͳ
ሺǡ ሻ ൌ ͳ െ න ሼ‫݌‬௟ ሺሻሽ 
‫ ୷ ܮ‬௟

where
‫݌‬௟ ሺሻ ൌ ࣨ൫௟ ǡ ߪ ଶ ௡భ௡మ ൯ is the conditional
distribution of  given ௟ . Note that, for any nonnegative
sets ሼܽ௟ ሽ and ሼ‫ݏ‬௟ ሽ with σ௅௟ୀଵ ‫ݏ‬௟ ൌ ͳ, we have ௟ ሼܽ௟ ሽ ൒
ς௅௟ୀଵ ܽ௟௦೗ . Consequently, we may write:
௅

ሺǡ ሻ ൑ ͳ െ

ሺǡ ሻ ൑ ͳ െ  ቆെ

σ௟ஷ௟ᇲ ԡሺ௟ െ ௟ᇲ ሻԡଶଶ
ቇ
Ͷߪ ଶ ‫ܮ‬ଶ

(10)

Assuming that  and  satisfy the RIP conditions similar
ଶ
‫ ؜‬௟ஷ௟ᇲ ԡ௟ െ
to Observation 1, and defining ݀௠௜௡
ଶ
௟ᇲ ԡଶ , Observation 1 implies the following bound for
classification error [5]:

(6)

ͳ െ  ቆെ൫ͳ ൅ ߳ଶ௞ೝ ൯൫ͳ ൅ ߳ଶ௞೎ ൯

݉ଵ ݉ଶ ሺ‫ ܮ‬െ ͳሻ ଶ
݀ ቇ
݊ଵ ݊ଶ ͺߪ ଶ ‫ ܮ‬௠௜௡
(11)

Particularly, if  and  are random orthoprojectors, then
above bound holds with conditions noted right after
Observation 1. It is observed that the classification error
decays exponentially fast as the number of random
observations ݉ଵ ݉ଶ increases.

3. Two Dimensional Compressive Classifier
for Sparse Images

4. Experiments

In many applications, images are either sparse in the
pixel domain or have a sparse representation in some
basis, such that their nonzero entries are concentrated in
a small number of rows/columns. Examples include a
images with sparse edge map or DCT transform of
natural images [10]. For this class of images, obtained
results in previous section are applicable; enabling us to
develop 2D-CC. Formally, assume that ࣲ ൌ ሼଵ ǡ ǥ ǡ ௅ ሽ
denotes a set of ݊ଵ ൈ ݊ଶ known 2D sparse images, i.e.
௟ ‫ א‬ȭ௞ೝ ǡ௞೎ , ݈ ൌ ͳǡ ǥ ǡ ‫ܮ‬, for some integers ݇௥ ൏ ݊ଵ ,
݇௖ ൏ ݊ଶ . The (possibly noisy) “true” image  ் ‫ࣲ א‬
undergoes 2D random projection to obtain  ൌ ሺ ் ൅
ሻ୘ , where  ‫ א‬Թ௡భൈ௡మ represents the noise. Now, we
will be concerned with discrimination among the
members of ࣲ, given only low-dimensional random
projectionsǤ Given  and , failure will be quantified in
terms of expected error. Let  ൌ ሺሻ,  ൌ ሺሻ,
 ൌ ሺሻ and  ൌ ۪. For simplicity of analysis,
we further assume ̱ࣨ൫Ͳǡ ߪ ଶ ௡భ௡మ ൯, where ௔ denotes
the ܽ ൈ ܽ identity matrix. Also,  and  are selected to
be random orthoprojectors, which implies that the
distribution of noise remains unchanged under
projection. Now, provided ௟ ’s happen equally likely, the
Bayes decision rule and the associated expected error
would be [11]:
ଡ଼೗ ‫ࣲא‬

(9)

Further simplification and setting ‫ݏ‬௟ ൌ ͳȀ‫ܮ‬, gives the
following bound [5]:

In particular, if  and  are admissible random
matrices in the 1D compressive framework, (6) holds
with
probability
exceeding
ͳ െ ʹ݁ ି௖మሺఢమೖೝ ሻ௠భ െ
ି௖మ ሺఢమೖ೎ ሻ௠మ
, provided ʹ݇௥ ൑ ܿଵ ሺ߳ଶ௞ೝ ሻ݉ଵ ሺ݊ଵ Τʹ݇௥ ሻ
ʹ݁
and ʹ݇௖ ൑ ܿଵ ሺ߳ଶ௞೎ ሻ݉ଶ ሺ݊ଶ Τʹ݇௖ ሻ. This means that, if
 ‫ א‬ȭଶ௞ೝ ǡଶ௞೎ , then ԡ ୘ ԡଶଶ Ȁԡԡଶଶ is strongly
concentrated about its expected value for variety of
random matrices; hence establishing concentration of
measure in 2D random projection scheme.

୶೗ ‫୴א‬ୣୡሺࣲሻ

ͳ
௦೗
න ෑ൫‫݌‬௟ ሺሻ൯ 
‫୷ ܮ‬
௟ୀଵ

൫ͳ െ ߳ଶ௞ೝ ൯൫ͳ െ ߳ଶ௞೎ ൯

ො௟ ൌ  ԡ െ ௟ ԡଶ ൌ ԡ െ ௟ ୘ ԡଶ

(8)

In this section, the efficacy of the proposed 2D-CC
is examined in retinal identification problem. Retinal
biometrics refers to identity verification of individuals
based on their retinal images. Salient anatomical features
of retina are depicted in Figure 1.a, among which vessel
tree pattern is a superior biometric trait, as it is unique,
time invariant and almost impossible to forge. Our
experiments are conducted on VARIA database
containing 153 (multiple) retinal images of 59
individuals [7]. To compensate for the variations in the
location of optic disc (OD) in retinal images and to
exploit the larger diameter of vessels near OD, a circular
region of interest (ROI) in the vicinity of OD is used to
construct the feature matrix. To extract the ROI, center
of OD is first localized using template matching as in [6],
followed by simple analysis of area. Then, similar to [6],
vessel tree is extracted by a combination of local contrast
enhancement and histogram thresholding. Then, a ringshaped mask with proper radii centered at OD is used to
form the feature matrix  ‫ א‬Թ௡భൈ௡మ by collecting the
pixels along ݊ଶ ൌ ʹͲͲ beams of length ݊ଵ ൌ ͳͲͲ
originating from OD (Figure 1.c). Note that feature
matrices readily satisfy the requirements of Observation
1 in pixel domain, as ݇௥ ൏ ݊ଵ and ݇௖ ‫݊ ا‬ଶ . Due to small
number of images per subject (~2) and approximate
invariance of feature matrix to the location of OD,
feature matrices of the same subject are modeled as noisy

(7)

404

deviations from corresponding mean feature matrix.
Hence, once all images are processed, ࣲ ൌ ሼଵ ǡ ǥ ǡ ௅ ሽ is
formed, where each ௟ is the mean feature matrix of ݈th
subject. Dimension reduction and classification of a test
feature matrix is then performed in Թ௠భൈ௠మ by the
proposed 2D-CC, and in Թ௠భ ௠మ by 1D-CC. Error is
measured using the leave-one-out scheme and the
average results over 100 independent repetitions are
depicted in Figure 2 for a wide range of ݉ଵ and ݉ଶ .
Although explicit calculation of the bound in (11) is
intractable [1], we note that the exponential nature of
error is in accordance with our findings. Also, due to
highly redundant nature of feature matrices along their
columns, “wise” choices for ݉ଵ and ݉ଶ , which consider
this redundancy, exhibit good performance especially for
small values of ݉ଵ and ݉ଶ . In contrast, “careless”
choices for ݉ଵ and ݉ଶ , degrade the performance (Figure
3). Using an Intel Core 2 Duo, 2.67 GHz processor with
3.24 GB of memory, we found that each repetition of
2D-CC approximately took ͲǤͲͲͻͺ݉ଵ ݉ଶ seconds,
whereas this number was roughly ͲǤ͸ͷͷ݉ଵ ݉ଶ for 1DCC, in MATLAB7 environment. Note that this
difference is significant even with our small-sized feature
matrices. In sum, for typical choices of ݉ଵ and ݉ଶ , 2DCC runs much faster than 1D-CC, yet producing results
with negligible loss in performance. This loss, however,
disappears with proper choice for ݉ଵ and ݉ଶ which
takes the prior knowledge into account. In addition, 2DCC enjoys significantly less memory requirements.

(a)

(b)

(c)
Figure 1. (a) Retinal image; bright area is OD. (b) Vessel tree
(in white) and mask (in blue). (c) Feature matrix for ݊ଵ ൌ ͳͲͲ,
݊ଶ ൌ ͵ͲͲ (images (a) and (b) are cropped).

Figure 2. Average classification error of 2D-CC (red surface)
and 1D-CC (blue surface) for a wide range of ݉ଵ and ݉ଶ .

Conclusions
In this paper, the idea of random projections is
extended to image domain and associated concentration
properties are studied. Our findings are then used to
develop 2D-CC, along with error bound for an important
special case. Finally, results are validated in a real-world
application.

Figure 3. Two examples of “wise” choices which consider the
redundancy along columns: ݉ଵ ൌ ͳ (left) and ݉ଵ ൌ ͵ (right)

References

[6] H. Farzin, H. Abrishami, “A novel retinal identification
system,” EURASIP Jr. on Advances in Signal Proc., 2008
[7] VARIA
database,
available
online
at
http://www.varpa.es/varia.html
[8] A. Ghaffari, M. Babaie-Zadeh, C. Jutten, “Sparse
decomposition of two dimensional signals,” Int. Conf.
Acoustics, Speech, and Signal Proc., (ICASSP’09),
Accepted.
[9] K.B. Peterson, M.S. Pederson, “Matrix Cookbook,”
Available online at http://matrixcookbook.com
[10] J.-L. Starck, M. Elad, D.L. Donoho, “Image
decomposition via the combination of sparse
representations and a variational approach,” IEEE
Transactions on Image Processing, 2006.
[11] R.O. Duda, P.E. Hart, D.G. Stork, “Pattern Classification,”
John Wiley, 2nd edition, 2001.
[12] A. Hajnal, and E. Szemeredi, “Proof of a Conjecture of P.
Erdos,” in Combinatorial Theory and its Application, P.
Erdos, A. R´enyi, and V. T. S´os, Eds., pp. 601–623.
North-Holland, Amsterdam, 1970.

[1] M.A. Davenport, M.F. Duarte, M.B. Wakin, J.N. Laska,
“The smashed filter for compressive classification and
target recognition,” Proc. SPIE Computational Imaging V,
2007.
[2] J. Haupt, R. Castro, R. Nowak, G. Fudge, A. Yeh,
“Compressive sampling for signal classification,” Fortieth
Asilomar Conf. on Signals, Systems and Computers, 2006.
[3] A. Eftekhari, M. Babaie-Zadeh, C. Jutten, H. Abrishami
Moghaddam, “Robust-SL0 for stable sparse representation
in noisy settings,” IEEE International Conference on
Acoustics, Speech, and Signal Processing, (ICASSP’09),
Accepted.
[4] J. Haupt, W.U. Bajwa, G. Raz, “Toeplitz compressed
sensing matrices with applications to channel sensing,”
Submitted, 2008.
[5] A. Eftekhari, H. Abrishami Moghaddam, M. BabaieZadeh, “Two dimensional compressive classifier for
sparse images,” Technical Report, available online at:
http://www.ee.kntu.ac.ir/pow/detlbiog.asp?bioguser=mogh
adam&bioglastn=Abrishami Moghaddam&group=*

405

