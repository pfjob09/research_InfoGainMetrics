Comparative High Contrast Area Extraction in Image
Based on Spatial-Contrast Feature

1,3

Xiaomiao Zhang1, Xiaolin Liu2, Qifeng Yu3
College of Aerospace and Material Engineering, 2College of Electromechanical Engineering and
Automation, 1,2,3National University of Defense Technology, Hunan 410073, P. R. China
{sharron_nudt@xinhuanet.com, yuqifeng@vip.sina.com}
Abstract

Referring to the fact that people recognize object
often by the contrast difference between the object and
the background, this paper proposes two new features
named the absolute-contrast and the spatial-contrast,
while presents a new method of extracting comparative
high contrast area in image based on the spatial-contrast.
First, the absolute-contrast is defined as the longest
distance between peaks of the image’s histogram, which
is used to measure the whole contrast of the image.
Second, aiming to simulate the perceive mechanism of
human being, the spatial-contrast feature is proposed to
describe the contrast difference between the area
including the object and the entire image. Third, the
algorithm of comparative high contrast area extraction
based on spatial-contrast feature is developed. Finally,
the method is verified by the text caption area extraction
in video. The result shows that the new method is feasible
and efficieQW. Because the spatial-contrast focuses on the
feature related with regional contrast, it is qualified as a
measure of salience in object recognition, which is free
of the object shape and geometric transformation. So,
such new features and method can provide a good basis
for recognition following.
Keywords---

absolute-contrast
histogram

spatial-contrast

1. Introduction
Along with the booming of multimedia and
convenience of image acquisition, image comprehension
and object recognition interest more and more
researchers. We never give up to find an object
recognition method resembling the perceive mechanism
of human being. In this paper, the area including the
object to recognize is called the object area.
In order to accelerate the digital image processing,
the qualities of image were made excessive demands. As

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

the meanwhile, people can distinguish the object by the
little differences between gray levels distribution of
object and whole image even the image is a little blurry,
and people do not need to enhance the low contrast of
image or object area. In addition, object features are
always relevant to the adjacent region. So, according to
the Purkinje effect, we assume that if the contrast of the
entire image is not of significant meaning to object
recognition, the contrast difference between the object
area and the image may be more valuable. In this paper,
we propose an absolute-contrast and a spatial-contrast to
represent the two cases before. The spatial-contrast
feature can be used to extract the comparative high
contrast area in the whole image. Furthermore, we
present a procedure using the spatial-contrast feature to
extract the object area of comparative high contrast,
which provides a good basis for recognition following.
Predictably, the object recognition can be more efficient
on the object area than on the whole image while does
not sacrifice the completeness and accuracy.
Chua [1] took notices of high contrast area for easier
object recognition also. However, he focused on the high
frequency the high contrast area. He save the computing
time by selecting limited number of the DCT (Discrete
Cosine Transform) coefficients to compute the gradient
energy for extraction, so it is unavoidable that some
valuable information is missing. In order to keep the
image being of information missing brought by image
transformation, we emphasize on the gray levels property
of image. On this condition, the absolute-contrast and the
spatial-contrast are represented and a better extraction
result is showed.
The outline of the paper is as follows. Section 2
represents the two new features gotten out of classical
histogram and spatial information in order to highlight
the contrast feature domain the object area belongs to.
Section 3 explains the algorithm to extract the
comparative high contrast areas using spatial-contrast
feature. Section 4 describes text caption area extraction
experiment and analyzes the result. Section 5 concludes
the paper.

2. Spatial-Contrast Feature
The image contrast goes the same way with the gray
levels. When the gray levels occupy a broad range of
gray scale, the image is of high contrast. When an 8-bit
gray image is taken into account, its gray-level content
always contain considerable information and its graylevel content can be represented by the gray-level
histogram [4] , which shows for each gray level, the
number of pixels in the image that have that gray level.
The gray-level histogram is a classical feature, which is
one of the simplest and most convenient tools in digital
image processing, but it has an obvious shortcoming that
is the missing of spatial information, therefore, it was
often transformed to many other kinds of histograms for
more complicate tasks.
There have been some meaningful studies on the
histogram modification and application. Sablak[2]
proposed the use of a vector of color histogram peaks as
an efficient and effective way for many image-indexing
problems. It shows that histogram peaks are more stable
than general histogram bins when there are variations of
scale and/or scale. Ferman[3] introduced a set of
histogram-based descriptors defined for a group-offrames (GoF) or a group-of-pictures (GoP) instead of
features of selected key-frame of key-image. This
descriptor is of high reliability and has been accepted to
the Working Draft of MPEG-7, the evolving ISO
standard for multimedia content description.
The more unrelated information in images is
reduced, the less time the object recognition cost. Being
a preprocessing for recognition, the strategy of reducing
useless information should not occupy too much
compute resource. While the histogram of any image
contains considerable information and certain types of
images are completely specified by their histograms,
another apparent merit is that the computation of the
histogram is simple and may be done at little cost when
an image is copied from one place to another [4] . On the
other hand, People always recognize object by the gray
level difference between object and background.
According to the Purkinje effect, the key point of human
perceives mechanism is the gray level difference. This
paper emphasize on the contrast of adjacent region, what
is called spatial-contrast feature.
In this section, we represent absolute-contrast and
spatial-contrast that are gotten out of classical histogram
and spatial information in order to highlight the contrast
feature domain the object area belongs to.



When an image is taken into account, its contrast is
the gray-level range occupied and can be represented by
classical gray-level histogram. The global contrast of
image I is defined as

GC I  Di  Di , H Di  z 0
min

Formula 1

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

max



means the minimum gray level.
Under most circumstances, the gray levels that
salience object and background covers both have enough
pixels to form a peak in the histogram. On the other hand,
using histogram peaks instead of gray level extremums
can decrease the effect of random noises to some extent.
Although the correspondence between peaks and objects
is unknown, using the longest distance between peaks to
illustrate the largest possible gray level difference
between object and background is feasible.
We propose to use the location of two histogram
peaks instead of Di and Di . Now, the absolutemax

min

contrast of image I is defined as

AC I 

Pimax  Pimin

Formula 2

In which, Pi is the gray level where H Pi  is the
peak of histogram.
When there exists only one peak in the histogram,
the representation of AC I  will be defined as



AC I  max P  Di , P  Di
max

min



Formula 3

We obtain an image (the left image of 1-a in Figure
1) from [1] , the right one is the histogram of the left.
From the histogram, we can get that Di is 183 and
max

Di

min

is 12, while Pi

max

is 135 and Pi

min

is 20. So, GC I 

gets 171 and AC I  gets 115.

2.2. Spatial-contrast
One of the useful properties of histogram is the
histogram of the entire image is the sum of the regional
histograms of disjoint segments. So, the global contrast
the entire image has is the upper limit of each optional
part’s global contrast. According to the formula 2 and
formula 3, one part of the image may have the absolutecontrast close to or even larger than the entire image, we
call it the comparative high contrast area, and the spatialcontrast of a connected region R k in image I is defined
as
SC R k , I 

2.1. Absolute-contrast

max

In which, Di means the gray level, Di  >0,255@ , so,
GC I   >0,255@ ; H Di  means the number of pixels
occupying the gray level Di ; i max means the maximum
gray level on which H Di is not equal to 0, while i min

AC R k 
, Rk  I
AC I 

Formula 4

For traditional gray level histograms, it is difficult to
maintain stability for information while changing
resolution, scaling, and illumination. And it is impossible
to label the contrast with area position and size. We do
illumination transformation, histogram equalization and
median filtering on the left image of 1-a in Figure 1,

which are showed in 2-a, 3-a and 4-a of Figure 1. Then,
on the same position of the whole images, four regions of
the same size are picked out. Surely, the four blocks
contain the same content for men’s eyes, which are
showed in 1-b, 2-b, 3-b and 4-b. now, each of the
SC Rk , I  is computed and showed in Table 1. It is clear
that the spatial-contrast of the same area is almost
invariable. Using this measure, the spatial-contrast can
be treated as irrelevant to the global gray level
transformation on the entire image.

Figure 1: 1-a, 2-a, 3-a and 4-a are entire images
and their histograms respectively. 1-a is the
original image obtained from [1] ; 2-a is the
result of illumination transformation of 1-a; 3-a
is the histogram equalization of 1-a; 4-a is the
result of filtering the 1-a by the median filter. As
the meanwhile 1-b, 2-b, 3-b and 4-b are small
parts of the left corresponding entire images
and their histograms respectively, and the small
parts are of the same size and on the same
position in the whole images.

Table 1: the spatial-contrast of four part images
showed in Figure 1.

Pi

Image
No.1
No.2
No.3
No.4

Rk
I
Rk
I
Rk
I
Rk
I

max

105
135
130
162
205
236
104
135

Pi

min

33
20
75
73
80
36
33
20

SC Rk , I 

0.62608696
0.61797753
0.62500000
0.61739130

Next section will go into particulars on the algorithm
of extracting comparative high contrast area where the
object to recognize may be locating.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

3. Algorithm of Comparative High Contrast
Area Extraction
As what have been stated in section 1, object
features are always relevant to the adjacent region
closely. If a part area is of the desired spatial-contrast,
that is to say, where the object to recognize may be
locating, aiming to embrace a region covering the whole
object, it is essential to enlarge the area and maintain the
spatial-contrast as while. According to which, this paper
emphasize on the contrast of adjacent region, and the
spatial-contrast features are used to extract the region
covering the whole object.
Firstly, noise reduction is optional. When the
median filter takes effect on the entire image, the spatialcontrast is subjected to little variation (as showed in
Table 1). The goal of noise reduction is to decrease the
bad effect on the object to recognition, so such process
can be done after the object area has been located by the
spatial-contrast feature, in another word, reducing noises
only on the object area may be more efficient.
Secondly, compute the absolute-contrast AC I  of
the image I according to Formula 2 and Formula 3.
In succession, segment the entire image to disjoint
blocks of same size, while the size of blocks is arbitrary.
As the meantime, the spatial-contrast SC Rk , I  of each
block is calculated according to Formula 4. It should be
noticed that although the size of blocks can be appointed
at will, it is suggested to use minor one in order to reduce
more useless information. Because if the size of the
block is same to that of the entire image, surely it
contains the object to recognize but it means nothing for
future recognition.
Then, a desired comparative high contrast is
determined and the blocks those which SC Rk , I  of are
equal to or higher than the comparative high contrast
threshold are labeled. Traditionally, the comparative high
contrast threshold is appointed to 0.5.
Finally, merge the adjacent labeled blocks together
and treat them as the object area on which the
recognition algorithm is going to act. If the region united
has holes, the holes should be filled in by original image
at the same area, because it can be the case that blocks
cover the edge of the object to recognize.
In addition, there can be iteration in the object area
locating. When the object area is merged at the first time,
the area may be still too large. Reducing the size of block
appointed in step 3 and doing step 2 to step 5 on the
located area until AC R k  is stable.
So far, although all the features described in section
2 are transformation of gray-level contrast based on the
classical histogram, the property of local area where the
object may be locating is emphasized and the spatial
information, such as the size and the position of the
object area, is added to the contrast organically.

4. Experiment Result of Text Caption Area
Extraction and Discussion
The method of comparative high contrast area
extraction using the spatial-contrast feature has been
used to reduce vast irrelevant information before
recognition. In this section, the experiment of text
caption area extraction in video is represented, and
excellent result has been observed.
Chua [1] suggested that the high contrast between the
object and its background should be reflected as large
gradient energy of the appropriate DCT blocks. A variant
of DCT coefficients are used to calculate the gradient
energy, while several coefficients of lower and higher
frequency are ignored, and some valuable information in
the object areas are discarded unavoidable.

Figure 2: (a) original video frame [1] ; (b) contrast
feature domain extracted by the way described
in [1] ; (c) comparative high contrast area
extracted using the new spatial-contrast feature.

For the sake of providing a good basis to detect text
caption area in video, we use the spatial-contrast feature
to extra text caption area. Since the gray level of the text
caption is always of high contrast to the background in
video, we suggest the SC Rk , I  of the text caption area
has a relative high value. In the experiment, the
comparative high contrast threshold is determined as 0.7
and the size of block is appointed to 7 u 7 , and the result
is showed in (c) of Figure 2. With the compare to the
result of (b) [1] of Figure 2, (c) covers less irrelevant area
than (b) while includes the entire text caption in the area.
Comparatively, our experiment reduces more non
text caption area, while the object area appears in
sufficiently large size and is sufficient for the purpose
text detection and segmentation.
In addition, because the spatial-contrast focuses on
the feature related with regional contrast, it is qualified
as a measure of salience in object recognition, which is
free of the object shape and geometric transformation. So,

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

such new features and method can provide a good basis
for recognition following.

Conclusions
With more convenience of image acquisition and
flood of vast amount information, it becomes more
critical to extract valuable information efficiently. People
recognize object by difference between it and the
background. In order to simulate such perceive
mechanism, this paper proposed two new features and
related algorithm that can be used to extract the area
including the object. Thus, the recognition can be run on
the object area, which will be more efficient.
This paper assumes that the object area is of
comparative high contrast to the entire image. First, the
absolute-contrast is defined as the longest distance
between peaks of the image’s histogram, which is used
to measure the whole contrast of the image. Second, the
spatial-contrast feature is defined to describe the
difference between the area including the object and the
image. Third, the algorithm of comparative high contrast
area extraction using spatial-contrast feature is developed.
Finally, the method is verified by the text caption area
extraction in video and the result shows that the new
method is feasible and efficient. Being a contrast-related
and shape-free feature of the object area, the spatialcontrast is qualified as a measure of salience in object
recognition, which can provide a good basis for
recognition following.

Acknowledgements
This work is financially supported by the National
Nature Science Foundation of China (NSFC).

References
[1]

[2]

[3]

[4]

Tat-Seng Chua, Yunlong Zhao, Yi Zhang, Detection of
Objects in Video in Contrast Feature Domain,
Proceedings of IEEE Pacific–Rim Conference on
Multimedia (PCM2000), Sydney, Australia, Dec 2000.
Sezai Sablak, Terrance E. Boult, Multilevel Color
Histogram Representation of Color Images by Peaks for
Omni-Camera, Proceedings of the IASTED International
Conference Signal and Image Processing October 18-21,
1999, Nassau, Bahamas.
A. Mufit Ferman, S. Krishnamachari, A. Murat Tekalp,
M. Abdel-Mottaleb, R. Mehrotra, Group-Frame/Picture
Color
Histogram
Descriptors
for
Multimedia
Applications, In the Proceedings of International
Conference on Image Processing - 2000, Vancouver, Sep
2000.
Kenneth R. Castleman, Digital Image Processing,
Prentice-Hall International, Inc., 1996.

