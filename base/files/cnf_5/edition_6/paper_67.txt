3D Object Layout by Voice Commands Based on Contact Constraints
Hiromichi Fukutake1, Yoshiaki Akazawa1, Yoshihiro Okada1,2 and Koichi Niijima1
1

Graduate School of Information Science and Electrical Engineering, Kyushu University,
6-1 Kasuga-koen, Kasuga, Fukuoka, 816-8580, JAPAN
{h-fuku, y-aka, okada, niijima}@i.kyushu-u.ac.jp
2
Intelligent Cooperation and Control, PRESTO, JST

Abstract
There is a well-known problem that it is very difficult to
accurately make a 3D object move/rotate to a specific
position/orientation in a virtual 3D space by the direct
manipulation of a mouse device on a 2D computer display
screen. To deal with this problem, the authors have
already proposed an automatic 3D object layout method
based on contact constraints.
After automatically
generating a 3D scene using this method, the user often
wants to modify it manually. For this, the authors
propose voice commands for 3D object layout based on
contact constraints in this paper. With the contact
constraints used in the 3D object layout method, it
becomes easier to layout 3D objects by voice commands.
Voice commands are intuitive interface and very efficient
in the situation when the user can not use a mouse device.
KEYWORDS
3D Scene Generation, 3D Object Layout, Voice commands,
IntelligentBox.

1. Introduction
This paper treats voice interface for the manipulation of 3D
objects in a 3D graphics application. Especially, we
propose voice commands for 3D object layout system that
we proposed based on contact constraints. The layout of
3D objects for creating a 3D scene takes a long time
because 3D objects have six degrees of freedom (DOF) and
are difficult to be positioned by using a 2D pointing device
on a standard 2D computer display screen. To deal with
this problem, we have already proposed an automatic 3D
object layout method [1]. This method works with
mainly contact constraints specified in a semantic database
of 3D objects. The proposed layout system randomly
places 3D objects in keeping with the semantic constraints.
Although the semantic database is very simple, our layout
system can generate realistic 3D scenes in a few seconds
even if there are many 3D objects existing in the scene.
After the layout system generates a 3D scene, the user
often wants to modify the 3D scene if it does not satisfy the
user. For this modification process, we propose voice

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

commands as an intuitive interface and incorporate them in
the layout system. In keeping with contact constraints
specified in the semantic database, the manipulations like
move/rotate 3D objects become simple because such
constraints reduce the number of DOF of 3D objects.
Therefore, even using voice commands, it is possible to
layout 3D objects. Voice commands are intuitive and
very efficient especially in the situation when the user can
not use a 2D pointing device like a mouse device.
[Related work]
For object placement, semantic information has been
employed. Coyne and Sproat [2] proposed the WordsEye
system, which generates a 3D scene composed of 3D
objects according to text description. This method allows
a few kinds of 3D scenes to be generated, based on a few
lines of text. Although our semantic database is very
simple rather than theirs, automatically generated 3D
scenes appear to be natural.
To control a high DOF object with the use of a common
2D input device, there are researches on reducing the
number of DOF. Most researchers take the approach of
mapping input vectors to higher dimensional vectors [3, 4].
These methods make the assumption that surfaces are
planar, and tend to restrict object motion along those
surfaces. Our voice commands also work efficiently by
reducing DOF of 3D objects based on their contact
constraints.
As for voice input interface, there are many researches
and commercial products for voice recognition/dictation,
Julius [5], Microsoft Speech API [6], IBM ViaVoice [7]
NEC SmartVoice [8], Toshiba LaLaVoice [9] etc. There
are also many researches for general purpose voice input
interfaces [10-15].
However, there are very few
researches about voice commands dedicated for layout
operations of 3D objects [16]. One of our research
purposes is to propose efficient voice commands. In this
paper, we propose prospective examples of such voice
commands.
The remainder of this paper is organized as follows.
First of all, Section 2 introduces automatic 3D object
layout method based on contact constraints. Section 3
introduces voice commands for 3D object layout. We

also describe component based voice input interface briefly.
Finally we conclude the paper in Section 4.

2. Automatic 3D objects layout based on
contact constraints
Before explaining our voice commands based on contact
constraints, we introduce our automatic 3D object layout
method. This is realized by semantic database, bounding
box, parent-child relationship, contact constraints and
placement algorithm.

2.1 Semantic database
To define the layout of 3D objects, our 3D scene
generation system uses a semantic database whose each
record called “object info” means placement constraints of
the corresponding object. Table 1 shows two records of
the semantic database, related to a bookshelf and a TV.
In the following subsections, we describe such placement
constraints.
Table 1: Semantic database
ot4 (bookshelf)
Face
Occupancy
Parent
Contact
constraint
distance
1
1.0
2
0
3
0
4
0
ot1-3
X
5
0
6
0
ot2-1
ot5 (TV)
Face
Occupancy
Parent
Contact
constraint
distance
1
1.0
2
0
3
0
4
0
ot4-3, ot6-3
X
5
0
6
0
OT(Object type) No.- Face No.:
{ot1(floor), ot2(wall), ot3(ceiling),.. ot6(desk),…}

2.2 Bounding box and occupancy space
In the real world, every object exists without any collisions.
When laying out 3D objects, we have to detect collisions
for every object. Detecting accurately collisions for all
3D objects in a scene takes a very long time because each
object has its own complex shape. One of the solutions is
to simplify such a complex shape. To maximally simplify
the layout process, we decided to employ the bounding box
of each 3D object instead of its original 3D shape as shown

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

Figure 1: Bounding box and occupancy space
of a bookshelf
in Figure 1. In the real world, every object is in contact
with another object due to the gravity, e.g., a desk rests on
a floor and a painting is hung on a wall. While detecting
collisions of one object using its bounding box, it is enough
to consider only the surface on which the corresponding
object lies. In this way, by using bounding boxes, the
calculation cost of detecting collisions is drastically
reduced. Our prototype system can generate a 3D scene
consisting of many 3D objects in an acceptable calculation
time.
While laying out a 3D object by avoiding its collisions
with other objects, we have to consider the extra space
necessitated by the object to play its given role. For
example, as shown in Figure 1, we can place any object
around a bookshelf physically. However, a bookshelf
needs a vacant space in its front in order to make possible
taking a book out. The front face of a bookshelf has to be
kept away from any face of other objects. In this way,
some faces of a 3D object must have the minimum distance
not to touch other objects. We call it “occupancy
distance”. We define occupancy distances for all faces of
a bounding box. A value, zero is specified for the face
that does not need its occupancy space. Using the
bounding box and the occupancy distance, a 3D object
comes to have its own space to avoid other objects. We
call it “occupancy space”. By laying out 3D objects by
avoiding collisions using their occupancy spaces, the
system can generate a natural, semantically correct scene.

2.3 Parent-child
constraints

relationship

and

contact

As previously explained, every object in the real world has
to touch other objects because of the gravity. At least,
one of the faces of a bounding box is defined to be a
contact face. For example, for the bookshelf, face 4 of its
bounding box must touch the face of the floor as shown in
Figure 2. If the user moves the floor object, the bookshelf

Figure 3: Parent-child relationship graph

Figure 2: Parent-child relationship and
contact constraint of a bookshelf
should move with it. Usually this is treated as a parentchild relationship in 3D applications. Every 3D object
has information indicating which of the other 3D objects is
allowed to be its parent, and indicating which face of that
3D object and which face of its parent object touch each
other. In the semantic database, this information is
specified as the parent attribute of the face which is in
contact with the parent object, see Table 1. In addition to
the above information, we have to specify contact
constraints for each face of a 3D object. This indicates
whether a face should touch a certain face of other object
or not. For example, the bookshelf often touches a wall in
addition to the floor. That is, face 6 of the bookshelf in
Figure 2 has to touch the wall object. We describe this
constraint in the constraint attribute of each face of each
record in the semantic database as shown in Table 1.
Recently, a number of researchers have addressed the issue
of object placement method using semantics [17]. In this
paper, we apply semantics such as contact constraints
mainly to the faces of the bounding box of a 3D object.
Using the parent-child relationship and the contact
constraint explained above, although these constraints are
very simple, the system can generate natural 3D scenes by
laying out 3D objects randomly in keeping with those
constraints.

2.4 Placement algorithm
The system lays out 3D objects according to the placement
constraints using the semantic database. First of all, the
system generates a parent-child relationship graph from the
semantic database as shown in Figure 3. The system
places objects one by one randomly in keeping with the
contact constraints written in the semantic database. The
concrete placement algorithm is as follows.
1. Select one object type according to the distance from
the root object type in the parent-child relationship
graph. This distance is the priority of the selection.
If there are multiple object types in the shortest distance
from the root object type, select one object type that has
the maximum contact constraints.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

Figure 4: Four steps of the placement.
2. Choose one 3D object, which belongs to the selected
object type, from all the 3D objects the user prepared.
3. Randomly choose another 3D object as the parent of
the 3D object chosen in the step 2 according to the
graph.
4. Place the object chosen in the step 2 at random position
on the object chosen in the step 3 as shown in Figure 4
(a).
5. Rotate and move the object to the position that satisfies
all contact constraints as shown in Figure 4 (b), (c) and
(d) according to the semantic database record.
6. This random placement in the steps 4 and 5 is repeated
until the object does not interpenetrate other objects.
7. Go to the step 2 unless the steps 3 through 6 were
applied to all 3D objects that belong to the object type
selected in the step 1.
8. Go to the step 1 unless the steps 2 through 7 were
applied to all object types in the graph.
9. Repeat the above steps until the placement result
satisfies the user.

2.5 Placement examples
A prototype system is developed using IntelligentBox [18],
which is a constructive visual 3D software development
system. Figure 5 shows four placement results generated
by the system. The 3D objects of each result are laid out
like those in the real world. The user selects desirable
one of the results. After the user obtains his/her desirable
layout, he/she can interactively move and rotate any 3D

text string and sends it to the layout system by a slotconnection. Slot-connection is a dynamic data-linkage
mechanism employed in IntelligentBox.

3.2 Voice commands
There are two types of voice commands. One type does
not use contact constraints. The commands among this
are “move”, “move to”, “rotate” and “rotate to” commands.
The other type uses contact constraints represented as
semantic database shown in Table 1. The commands
among this are “put on” and “connect to” commands.

Figure 5: Four layout results
object by voice commands according to the contact
constraints specified in the semantic database. As for the
performance, the generation time is a few seconds when
the number of 3D objects is around 40.

3. Voice commands for 3D object layout
After the layout system generates a 3D scene, the user
often wants to modify the 3D scene if it does not satisfy the
user. To make this modification process easier, we are
going to introduce voice commands into the layout system.

3.1 Component based voice input interface
IntelligentBox is a component based 3D graphics software
development system. All functionalities are provided as
software components called boxes. For voice input
interface, a particular box called VoiceInputBox exist [19].
Since IntelligentBox does not have any voice recognition
functionality, we employed Microsoft Speech API [6] and
developed one server program called VoiceServer for that
as shown in Figure 6. VoiceInputBox connects to the
server using a standard TCP/IP socket communication by
specifying the IP address and port number of the server.
VoiceInputBox asks the server to send a current dictated

Figure 6: Component composition for voice
input

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

3.2.1 Voice command for object selection
When applying one of the above commands, first of all, the
user has to select one target object to be applied that
command. Figure 7 shows how to select a target object
using voice inputs. As already explained, each object
belongs to one of the object types. For example, if the
user wants to select a desk object, he/she should say “desk”.
Then, as shown in the upper left part of Figure 7, all
objects belong to the desk type will be highlighted by a red
color and dashed wire-frame, and one of them will be
highlighted in a red color and solid wire-frame as a current
selected object. If the current selected object is not the
target object that the user wants to apply the command, the
user should say “no”. Then, as shown in the upper right
part of Figure 7, a different desk closest to the current
selected desk will be highlighted as a newly selected object.
If the user says “no” again, as shown in the lower left part
of Figure 7, a new different desk will be highlighted.
When the current selected object becomes the target object
that the user wants to apply a certain command, then he/she
should say that voice command, e.g., “move”, “rotate to”.
However, if there are many desks in a scene, voice input
“no” is not enough for the rapidly selecting a target object.
For this, we prepare “right”, “left”, “far” and “near” voice
inputs. In the case shown in the upper right part of Figure 7,

Figure 7: Object selection by voice inputs

Figure 8: “move” and “move to” commands

Figure 10: “rotate” and “rotate to” commands

Figure 9: “more” or “less” voice inputs

Figure 11: “connect to” command

if the user says “right”, a new different desk just located in
the right side of the current selected desk will be
highlighted as a newly selected object shown in the lower
right part of Figure 7.
3.2.2 Voice commands not using contact constraints
Figure 8 shows executions of “move” and “move to”
commands. The “move” command makes a target object
move to a destination position by specifying its direction
and distance using additional voice inputs.
After
selecting a target object, if the user says “move”, first of all,
the dial plate of a clock will appear as shown in the upper
left part of Figure 8. Using this dial plate, the user can
specify the moving direction of a target object by voice
input. This is a popular method frequently used to specify a
direction. This method is also used in [8]. For example, if
the user says “three o’clock”, the target object starts to
move in the corresponding direction as shown in the upper
right part of Figure 8.
On the other hand, the "move to" command makes a
target object move to the position of the object chosen by
the user as a destination object of the verb "move to".
After selecting a target object, if the user says "move to",

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

first of all, an arrow symbol will appear to indicate the
moving direction of the target object. The lower left part
of Figure 8 shows its initial direction. Next the user
selects a destination object by the same way as the object
selection explained in 3.2.1. After that, the arrow will
rotate and point to the destination object, and then the
target object starts to move along the arrow towards the
destination object as shown in the lower right part of
Figure 8.
In this case, the destination object is
highlighted by a blue color wire-frame.
In the both cases of “move” and “move to” commands,
the user has to specify the stop position of the target object
by voice inputs. For this, we prepare “more” and “less”
voice inputs additionally. The “more” and “less” voice
inputs are used for the gain and the loss of the moving
distance from the initial position respectively. As shown
in Figure 9, using these two voice inputs, the user can
move a target object to his/her required position efficiently.
Besides translation operations, i.e., “move” and “move
to” voice commands, there are “rotate” and “rotate to”
voice commands as shown in Figure 10. After selecting a
target object, if the user says “rotate”, first of all, the dial
plate of a clock will appear as shown in the upper left part

of Figure 10. Using this dial plate, the user can specify
the rotation angle of a target object by voice input. The
upper right part of Figure 10 shows the case when the user
said “seven o’clock”.
Finally, the “rotate to” command makes a target object
turn to a destination object as shown in the lower parts of
Figure 10. The selection of the destination object in this
case is performed by the same way as the object selection
explained in 3.2.1.
3.2.3 Voice commands using contact constraints.
There are two voice commands, “connect to” and “put on”,
those use contact constraints. Figure 11 shows the
execution of the “connect to” voice command. After
selecting a target object, if the user says “connect to”, first
of all, all of the contactable destination objects will be
highlighted in a blue color and dashed wire-frame as
shown in the upper left part of Figure 11. In this case, the
information to which objects the target object can connect
is derived from the semantic database. One of the above
highlighted objects is highlighted by a blue color and solid
wire-frame as a current selected object. If the user wants
to select another object different from the current selected
object, he/she should say “no” similarly to the object
selection process explained in 3.2.1. If the user says “ok”,
contactable faces of the current selected object will become
highlighted and the target object will move and connect to
one of the highlighted faces as shown in the lower left part
of Figure 11. If the user wants to connect the target
object to a different face, he/she should say “no”. And
then the target object will move and connect to another
face as shown in the lower right part of Figure 11. Finally,
if the user says “ok”, the “connect to” command will be
completed similarly to the termination of the other voice
commands.
Due to the space limitations, we cannot explain the
detail of the “put on” command. Basically it is almost the
same as the “connect to” command.

5. References
[1]

[2]
[3]
[4]
[5]

[6]
[7]
[8]
[9]
[10]

[11]

[12]

[13]
[14]

[15]

4. Concluding Remarks
This paper proposed one solution for the well-known
problem that it is very difficult to lay out 3D objects to
create 3D scenes using a 2D mouse device on a 2D
computer display screen. In this paper, we proposed
contact constraints based voice commands for 3D object
layout operations performed after a 3D scene has been
generated by our already proposed automatic 3D object
layout system. We showed experimental results of our
prototype system and how the voice commands work.
We have to evaluate the usefulness of our prototype
system by consulting the actual users of the system. And
then we will propose more voice commands suited to their
needs. These are our future works.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

[16]

[17]

[18]

[19]

Akazawa, Y., Okada, Y. and Niijima, K., Automatic 3D
object placement for 3D scene generation, The European
Simulation and Modeling Conference 2003 (ESMC2003),
pp. 316-318, 2003.
Coyne, B. and Sproat, R., WordsEye: an automatic text-toscene conversion system, SIGGRAPH 2001, pp. 487-496,
2001.
Bier, E. A., Snap Dragging in Three Dimensions, ACM
SIGGRAPH, pp. 193-204, 1990.
Venolia, D., Facile 3D Direct Manipulation, Proceedings of
INTERCHI '93: ACM Conference on Human Factors in
Computing Systems, pp. 31-36, 1993.
Lee, A., Kawahara, T. and Shikano, K., Julius --- an open
source real-time large vocabulary recognition engine, In
Proc. European Conference on Speech Communication and
Technology (EUROSPEECH), pp. 1691-1694, 2001.
http://www.microsoft.com/speech/download/sdk51/
http://www-306.ibm.com/software/voice/viavoice/
http://121ware.com/product/software/smartvoice_4/
http://www3.toshiba.co.jp/pc/lalavoice/
Igarash, T. and Hughes, J. F., Voice as Sound: Using Nonverbal Voice Input for Interactive Control, 14th Annual
Symposium on User Interface Software and Technology,
ACM UIST'01, Orlando, FL, pp.155-156, 2001.
Manaris, B. and Harkreader, A., SUITEKeys: A Speech
Understanding Interface for the Motor-Control Challenged,
Proc. of The Third International ACM Conference on
Assistive Technologies (ASSETS '98), pp. 108-115, 1998.
Manaris, B., McCauley, R. and MacGyvers, V., An
Intelligent Interface for Keyboard and Mouse Control,
Providing Full Access to PC Functionality via Speech,
Proceedings of 14th International Florida AI Research
Symposium (FLAIRS-01), pp. 182-188, 2001.
Azfar S. Karimullah and Andrew Sears, Speech-Based
Cursor Control, Proc. of ACM ASSETS 2002, pp. 178-185.
Frankie James and Jeff Roelands, Voice over Workplace
(VoWP): Voice Navigation in a Complex Business GUI,
Proc. of ACM ASSETS 2002, pp. 197-204.
Stephen C. Arnold, Leo Mark and John Goldthwaite,
Programming by Voice, VocalProgramming, Prof. of ACM
ASSETS 2000, pp. 149-155.
Richard A. Bolt, “Put-That-There”: Voice and Gesture at the
Graphics Interface, Proc. of the 7th annual conference on
Computer graphics and interactive techniques, ACM, pp.
262-270, 1980.
Zeng, X., Mehdi, Q. H. and Gough, N. E., Shape of the
Story: Story Visualization Techniques, Proc. of Information
Visualization 2003 (IV03), pp. 144-149, 2003.
Okada, Y. and Tanaka, Y. : IntelligentBox: A Constructive
Visual Software Development System for Interactive 3D
Graphic Applications, Proc. of Computer Animation '95,
IEEE Computer Society Press, pp.114-125,1995.
Fukutake, H., Okada, Y. and Niijima, K. : 3D Visual
Component Based Voice Input/Output Interfaces for
Interactive 3D Graphics Applications, Proc. of CGAIDE
2004, pp. 216-220, 2004.

