Head Nod and Shake Recognition Based on Multi-View Model and Hidden
Markov Model
Peng Lu, Mandun Zhang, Xinshan Zhu, Yangsheng Wang
Institute of Automation, Chinese Academy of Sciences
Beijing 100080, People’s Republic of China
Email: {plu,wys}@mail.pattek.com.cn
Abstract
Head gestures such as nodding and shaking are often
used as one of human body languages for communication
with each other, and their recognition plays an important
role in the advancement of human-computer interaction. As
head gesture is the continuous motion on the sequential time
series, the key problems of recognition are to track multiview head and understand the head pose switch. This paper
presents a novel approach to recognize the nod and shake
in the interactive computer environment. First, head poses
are detected by multi-view model(MVM) and then Hidden
Markov Model(HMM) are used as head gesture statistic inference model for gesture recognition. Experimental results
show that the approach is effective and real time.

1. Introduction
With the ever increasing role of computers in society,
HCI has become an increasingly important part of our daily
lives. One long-term goal in HCI has been to migrate the
“natural” means that humans employ to communicate with
each other into HCI. Gesture recognition plays an important role in the advancement of human-computer interaction
since it provides a natural and efﬁcient interface to computers. Among gestures, head nod and shake are very common
and used often, so their detection is basic to a visual understanding of human responses.
As head gesture is the continuous motion on the sequential time series, tracking multi-view head is important and
fundamental to gesture recognition. Up to now, many methods for gesture recognition have been proposed such as syntactical analysis, neural based approach, Hidden Markov
Model(HMM) based recognition, especially in head gestures recognition [2] [3] [1]. In [1], a real-time head gesture
detector was presented, but in order to track head an extra
hardware was needed. In [3], there is no extra hardware

needed and some results are got. But just using color information the tracking maybe fail in bad illumination scenes.
In [2], the pattern between eyes was used for tracking. But it
maybe fail when the pattern doesn’t exist in the image, such
as a bigger angles moving of subjects. In order to improve
the robust of head pose tracking the features, which have
more statistical signiﬁcance, should be added to the system.
Meanwhile the gesture can be described by a set of head
poses, thus it is also necessary for us to revealing head pose
switch in gesture sequences. HMM based methods have so
far been the most effective recognition tool for sequential
time series. In this paper, we proposed an MVM for head
pose tracking, which fuses appearance, color and motion
information for head tracking. And then HMM is applied
as head gesture statistic inference model for head gesture
recognition. The proposed approach differs other methods
in two aspects. First, the color information is used in a new
way in MVM. Second, the appearance feature, which has
more statistical signiﬁcance,the color feature and the motion information are fused into the MVM.
The process of our gesture recognition system is shown
in Figure 1. It consists of two parts: multi-view model and
gesture inference model.
The rest of this paper is organized as follows: In Section
2, an MVM based head pose tracking system is proposed.
Section 3 presents the gesture recognition by HMM. Some
experimental results are shown in Section 4.

2. Head Pose Tracking
2.1. Maximum Likelihood Detector
The probability of input pattern X which belongs to class
Ω can be modelled by a multidimensional Gaussian probability density function:

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

P (X|Ω) =

exp[− 12 (X − X)T Σ−1 (X − X)]
(2π)N/2 |Σ|1/2

(1)

Y = Tθ X
Tθ = [

cos θ
− sin θ

(5)
sin θ
]
cos θ

(6)

where θ is rotation of the [I(i + r, j + c) : (r, c) ∈ R]. Substituting (5) into (3), we have
S (i, j, k, θ|Ω) = Pˆ (Y |Ω)

Therefore the rotation invariant face detector is represented
as
(i, j, k, θ)ML = arg max Pˆ (Y |Ω)
(8)
= arg max Pˆ (Tθ X|Ω)

Figure 1. The process of our system
where X is the mean vector of class Ω. Σ is the covariance matrix of class Ω. The complete likelihood estimate
can be written as the product of two independent marginal
Gaussian densities by using PCA [6].

Pˆ (X|Ω) = [

exp[− 12

M
i=1
M

(2π)M/2

y2
i
λi

]

1/2

λi

(2)

i=1

= PF (X|Ω)PˆF (X|Ω)
where PF (X|Ω) is the true marginal density in principal
subspace and PˆF (X|Ω) is the estimated marginal density
in the orthogonal complement space, 2 (x) is the residual
error, λi is eigenvalue of Σ, M is the dimensional of principal subspace, N is the dimension of total subspaces.
The density estimate Pˆ (X|Ω) can be used to compute
a local measure of target saliency at each spatial position
(i, j) in an input image based on the vector X obtained
by the lexicographic ordering of the pixel values in a local
neighborhood R
S(i, j, k|Ω) = Pˆ (X|Ω)

(3)

for X =↓ [I(i + r, j + c) : (r, c) ∈ R] where ↓ (•) is the
operator which converts a subimage into a vector. The likelihood computation is performed (in parallel) on linearly
scaled versions of the input image I k for a predetermined
set of scales. The maximum likelihood(ML) estimate of the
spatial and scale indices is then deﬁned by
(i, j, k)ML = arg max S(i, j, k|Ω)

2.3. The Usage of Color and Motion Information
It is not enough to just consider the appearance information of human’s face. For a robust algorithm it is necessary
to fuse multiple information such as color and motion.
2.3.1 Motion Cue

2 (x)

exp[− 2ρ ]
][ (2πρ)(N −M
)/2 ]

(7)

(4)

2.2. Rotation Invariant Face Detector
Applying the rotation transform matrix Tθ to X, the rotated subimage Y is represented as

The smooth changing constraint of the position of head, we
assume a Gaussian distribution Pm (i, j) around the head
position pt of t frame as proposal distribution of position of
the head in t+1 frame.
S (i, j, k, θ|Ω, M ) = S (i, j, k, θ|Ω) × Pm (i, j)
= S (i, j, k, θ|Ω) × N (pt , Σm )
(9)
where the covariance Σm is set by experience and M denotes the motion cue .
2.3.2 Color Cue
In our scheme, we use the region of face in actual frame,
which is detected by our Haar-Sobel-like boosting algorithm [4] and aligned by active shape model (ASM) algorithm [7], to create skin color model. At detection stage,
Haar and sobel features are used as feature space. GentleBoost is used to select simple classiﬁers. Haar features are
used to train the ﬁrst ﬁfteen stages. And then sobel features are used to train the rest fourteen stages. At alignment
stage, ASM is used for face alignment. At color model creating stage,we change the input image from Red, Green,
Blue(RGB)color space to HSV space, which separates out
hue (color) from saturation (how concentrated the color is)
and from brightness. And the color models are created by
taking 1D histograms from the H (hue) channel in HSV
space. Through this model the input image can be convert
into a corresponding head position probability map Pc (i, j),

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

which represents the possibility of point(i, j) belonging to
head.
In practice, we assume the skin color distributes as normal distribution N (µc , σc ). µ is the mean value of the H
value in face region and σ is the variance of H value. Thus
we have

As show in Fig.
2, the observation sequence
E1 , E2 · · · , ET and head gesture G are connected through
HMM model λ = (A, B, Π).

2

Pc (i, j) = P (h(i, j)|skin) =

]
exp[ (h(i,j)−u)
2
√ 2σ
;
2πσ

Figure 2. A left-to right state transition diagram for a 4-state HMM. For head nod, 1,2,3,4
denote frontal, up ,frontal, down respectively.
For head shake 1,2,3,4 denote frontal, left,
frontal, right respectively.

(10)

where h(i, j) is the H value at point(i, j).
Then equation (9) can be expressed as
S (i, j, k, θ|Ω, M, C) = S (i, j, k, θ|Ω, M ) × Pc (i, j)
(11)
where C is skin color cue.
Substituting (9) (11) into(8), the ML estimation equation
can be represented as
ML

= arg max(S (i, j, k, θ|Ω, M, C))
(i, j, k, θ|Ω, M, C)
ˆ
= arg max(P (Tθ X|Ω) × N (pt , Σm ) × Pc )
(12)

2.4. Multi-View Model

P (E1 , E2 , · · · , ET |G)
=
P (E1 , E2 , · · · , ET |Q, G)P (Q|G)
=

allQ

q1 ,q2 ,···,qT

πq1 bq1 (E1 )aq1 q2 bq2 (E2 ) · · · aqT −1 qT bqT (ET )

(14)
In practice,the P (E1 , E2 , · · · , ET |G) can be computed
by Forward-Backward algorithm [5] as follows:
we deﬁne αt (i) as follows:
αt (i) = P (E1 , E2 , · · · , ET , qt = Si |G)

Previous researches show that pose changes of a continuous face rotation in depth form a smooth curve in pose
eigenspace. The manifold could be modelled probabilistically by a set of covariance matrices at different poses, then
we can project novel face images into the eigenspace and
determine their positions along the pose distribution manifold by simply measuring likelihood to the manifold[?]. So
for recognizing different head poses, a set of detectors will
be constructed. For each pose detector, we can get an output of (i, j, k, θ|Ω, M, C)ML . Decision is made based on
the output of each detector.

(15)

1) Initialization:
α1 (i) = πi bi (E1 )1 ≤ i ≤ N.

(16)

2)Induction:
N

αt+1 (j) = [

αt (i)aij ]bj (Et+1 )

(17)

i=1

3)Termination:
Et =

arg max((i, j, k, θ|Ω, M, C)ML
n )
n

= arg max(arg max(Pˆ (Tθ X|Ω) × N (pt−1 , Σm ) × Pc )n )
n
(13)
where n denotes the number of detectors.

3. Head Gesture Recognition
Given the observation sequence E1 , E2 · · · , ET , the likelihood P (E1 , E2 · · · , ET |G) can be derived from discrete
HMM model, which is represented by λ = (A, B, Π),
where A = (aij)N ×N is the state transition probability distribution matrix , B = (bjk )N ×T is the observation symbol
probability distribution matrix and Ek represents discrete
observation symbol, Π = (π1 , · · · , πN ) is the initial state
distribution.

N

P (E1 , · · · , ET |G) =

αT (j)

(18)

i=1

3.1. Learning the Gesture Likelihood Model Parameters
In our experiment ﬁfty labelled gesture sequences were
used for training. The parameters (A, B, Π) can be learned
by Baum-Welch algorithm [5] as follows.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

N

γt (i) =

ξt (i, j)

(19)

j=1

π i = γ1 (i)

(20)

T −1

aij =

ξt (i, j)

t=1
T −1
t=1

γt (i)

T

bj (k) =

Table 1. Recognition results for training set
Head Nods Head Shake Miss

(21)

t=1
T
t=1

γt (j)

Head Nods

19

0

2

Head Shake

0

22

0

Average Recognition ratio: 95.1%
(22)

γt (j)

Table 2. Recognition results for testing set
Head Nods Head Shake Miss

where
ξt (i, j) = P (qt = Si , qt+1 = Sj |E1 , E2 , · · · , ET , G).

4. Experimental Result and Discussion
We implemented the algorithm on a P4 machine with
1.3G CPU and the database collection of our experiment
was similar to that in [1]. A program agent will ask 5 questions, and the subjects are asked to answer with head nod or
head shake.
As an example, Figure 3 shows the recognition process
of head shake. Our algorithm is initialized by the output
of Haar-like feature based boosted cascade face detector.
Once a frontal face is detected, the MVM will be created
for tracking head pose and HMM will be applied to infer
the head gesture.

Head Nods

27

0

4

Head Shake

0

32

3

Average Recognition ratio: 88.1%
to the system to deal with this problem. This is our future
work.

5. Conclusions
We have described an approach to recognize head nod
and shake. First, an MVM is proposed for head tracking.
And then the tracking results are used by HMMs to recognize head nod and shake. The main advantages of our algorithm are described as follows: 1)The proposed approach
reduces the request of extra hardware, and a cheap web
camera is enough. (2)More features are used for head pose
tracking, which increase the robust of the system. (3)A new
way of using color information are presented in this paper.

References

Figure 3. A recognition process of head
shake.
Table 1 and Table 2 show the experimental results. From
table we observe that there are no false positives. However,
most of the head nods and head shakes that go undetected
are the slight head movements. This is because, the base
of Multi-view Model is appearance feature, which is dull to
those small poses. So more local features must be added

[1] A. Kapoor and R. Picard. A real-time head nod and shake
detector. Proceedings of the Workshop on Perspective User
Interfaces, Nov. 2001.
[2] A. Kawato and J. Ohya. Real-time detection of nodding and
head-shaking by directly detecting and tracking the ’betweeneyes. Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition, 2000.
[3] P. C. Ng and L. D. Silva. Head gestures recognition. Proceedings of International Conference on Image Processing 2001,
3:266–269, Oct. 2001.
[4] Y. P.Lu, X.S.Huang. A new framework for hand-free navigation in 3d game. Proceedings of the International Conference
on CGIV04.
[5] L. Rabiner. A tutorial on hidden markov models and selected
applications in speech recognition. Proc. IEEE, 77(2):257–
286, 1989.
[6] G. Shakhnarovich and B. Moghaddam. Face recognition in
subspaces. Handbook of Face Recognition.
[7] C. T.F. Cootes. Statistical models of appearance for computer
vision. Imaging Science and Biomedical Engineering, University of Manchester, Mar. 2004.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

