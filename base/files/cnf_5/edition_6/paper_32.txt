Structural Models and Topology Design of Rapid Neural Networks
Mamoun S. Al Rababaa
Computer Science Department
Al al-Bayt University,
Al mafraq- Jordan
marababaa@aabu.edu.jo

Abstract

we suffer from workloads of more and more raw

It is examined structural and topological models

data and a highly competitive, global economic

of multilayered fast Neural Networks (FNN). The

climate. According to [Kara93], the “neural

mathematics of numerical sets reflections and

network industry is expected to grow to some

theory of linear presentations are used. The

billions of dollars in annual U.S. sales by the year

algorithm of FNN design is offered.

2000 and then sustain an average annual growth
rate of 40% each year.”

Keywords: neural networks, rapid algorithm,
network

topology,

partial

reflection,

substitution, structural model.

Rapid neural networks [Dor00] are the
variety of multilayered neural networks of direct
distribution. FNN can be compared with ordinary
neural networks approximately in that level as

1. Introduction
A Neural Network is a massively parallel
distributed processor that has a natural propensity
for storing experiential knowledge and making it
available for use. It resembles the brain in two
respects [Hay94, Che94, Arbib95]:
1. Knowledge is acquired by the network
through a learning process.
2. Interneuron connection strengths known as
synaptic weights are used to store the
knowledge.
Technological advances enable us to
collect far more information than we can possibly
analyze, while our time to accomplish our goals
seems to be vanishing in this fast-paced world. We
need help from intelligent assistants, expert
systems, data mining, and complex information
retrieval strategies to make sense of it all.
Intelligent applications, including those using
neural networks, will be increasingly in demand as

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

algorithms of Fourier’s fast transformation (FFT)
with direct Fourier’s discrete transformation. High
calculation efficiency of FNN is achieved due to
reasonable limitations on structural organization of
neuron network.
Structure of FNN in some way repeats the
neural networks for which there always are
limitations on the dimension of the receptors fields
and connections between neurons. It was shown in
the [2] that it is necessary to examine the questions
of structural organization of FNN on two levels:
that of structural model of FNN and level of
topology realization of FNN. If the level of
structural model determines commons properties of
FNN on dimensions of receptors fields and
structure of interlayered connections, the level of
topology determines concrete hardware or software
representation of neuron network. Both these levels
of consideration are tied-up unbreakably between
themselves.

2. Algorithm of fast neural networks

layer’s neurons. Connections between layers are set

On fig.1 the example of FNN is shown in

by designing operators which preserve the terms of

classical presentation where every apex corresponds

calibrating. The grade of designing operator

to one neuron, and arcs determine connections

determines the weight of structural model’s arc.

between neurons. The given presentation in future

Independence of the receptors fields of kernels is

we will call topology realization of the structural

the principle feature of FNN construction, that is

model. On fig.2 the structural model built for this

the receptors fields of neuron kernels do not

network is resulted. The group of neurons having

intersect.

the common receptor field corresponds to every

The

structural

model

is

essentially

apex of structural model. These groups of neurons

description of whole class of equivalent topologies

were adopted by neural kernels. At structural level

FNN realization. It can be stated that the set of

every neural kernel is characterized by dimension

topologies forms the orbit of structural model on

of the receptor field and number of neurons

the set of matrix representations. Topology of FNN

incoming into it. This pair of numbers sets the

is set by the numerical partial reflections (by partial

weight of structural model’s apex. The neuron

substitutions) [Mal03]:

kernel j in the layer m is described by the operator
Amj, which transforms the input vector of the own

Vm
j

§ u1
¨
©1

u2

Pm
j

§ v1
¨
©1

v2

receptor field:
Ymj = (Xmj)Amj.

2

... u Nxmj ·
¸,
... Nxmj ¹

(1)

2

... v Nymj ·
¸,
... Nymj ¹

(4,5)
where the Nxmj is the size of the receptor
field of kernel j in the layer m, Nymj - number of
neural in the kernel j of layer m. u is a number of
base vector in the space of receptors, v is number of
base vector in the space of neurons.

Fig.1

Fig.2

Action of partial substitutions is set on the
input and output vectors of neuron layer, partial

In the matrix representation the operator of
neuron kernel can be written down as following:
Smj = XmjWmj, Ymj = F(Smj),

(2,3)

substitution Vmj selects a receptor fields of kernel,
and substitution the Pmjneuron field. Formally the
action of partial substitutions is written down as
following:

where Wmj is a matrix of kernel synopsys
weights, F( ) is the multidimensional function of

Xmj = Xm*Vmj, Ymj = YmPmj.

(6)

activation. Its components are the functions of
activating of separate neurons of kernel.
The action of operator of neuron kernel is

Combinations of symbols *Vmj and Pmj

determined on the pair of the calibrated [Far03]

can be regarded as well as symbolic denotation of

vector spaces: space of receptors and space of

designing operators, however a record in the formal

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

way gives a number of advantages at the

presented on fig.1 corresponds to the product of

mathematical

matrixes:

calculations.

According

to

the

principle of FNN construction designing operators
do not intersect, therefore

VmD



Vmj

= 0,

PmD



Pmj

= 0 for any v z j.

1 1 1

1 1 1

1 1 1
1 1 1

1 1 1

Contiguous neural layers are connected
between themselves by permutable transition

1 1 1
1 1 1

operators, action of which is certain by substitutions

1 1 1

.

1 1
1 1 1
1 1
1 1

of qm:
Xm+1 = Ym*qm.

(10)

(7)

We will name such matrixes topologies.
Positions of nonzero elements in these matrixes are
Interlayered transition operators induce the

determined by partial substitutions Vmj and Pmj. It is

local operators of connection between neuron

not difficult to notice that under arbitrary

kernels, so that

transposition of elements in any line of partial

¦

X m+1, j

substitutions Vmj , Pmjthe topologies matrixes do

Ymi  U ijm ,

(8)

i

not change. The ambiguousness in the choice of
partial substitutions can be removed if to limit in a
number of substitutions in which there no

where

Umij

-

partial

substitutions

inversions [Kos04]. The elements of lines will be
always organized by ascending.

corresponding to the local operator, ¦ is symbol of
direct sum of vectors.

In the absence of inversions there is any
necessity in the reflection of the second lines of

At the level of structural model every local
connection is characterized by the grade of
designing operator. The numerical value of the
grade determines the weight of the proper arc of
structural model (fig 2). From equations (6) - (8)
follows:

substitutions as they always will be organized by
ascending. Thus the location of neuron kernel in a
topology matrix could be set by the pair of
organized sets Umj,Vmj, which form the upper lines
of partial substitutions of Vmj, Pmj. We will call the
introduced sets the topological ones. Taking into

1
Vm
j

m 1

(q )

¦

P imU ijm

i

consideration abovementioned thoughts we will
build the algorithm of FNN topology design
comfortable for software realization. Let’s present
the description of the algorithm and then we will

This expression determines the recurrent

show that it satisfies to common correlation (9).

algorithm of neuron network topology construction.
The topology of neural network can be

Lets for apexes Amj the topology is set by
pair

(Umj,Vmj

), and for the apexes of layer m+1 by

m+1

m+1

represented in a compact way of product of

the pair (U

contiguity matrix of graph of topology realization.

layers

For example, topology of the neural network

connections in the way of rank matrix:

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

we

j,V

will

j).

Between two contiguous

write

down

the

structural

A

m
T11

m
T12

...

T1ml

T1A

m
T21

m
T22

.

T1ml

T2A

.. .

...

..

...

...

...

m
T kl
TlB
B

T kA

T km1
T1B
B

T km2
T2B
B

...

pq

pq

...

U 1m  1

U 2m  1

...

Rm

q
 o
A
q
 o
A
q
 o
A
q
 o

V1m
V 2m
V km

:
r km1

Graphically

an

algorithm

can

be

represented with the chart presented on fig.3.
Let’s show that this algorithm satisfies the

Ɋɢɫ.3
Fig.3

pq

m
r12
m
r 22

(12)

...

U lm  1

m
r11
m
r 21

1
Um
TjB q B
j

Vim TiA q A ,

:

...
...
...

r1ml
r 2ml

r km2

...

m
r kl

correlation (9). >From the chart of fig. 3 follows:

U mj 1

¦V

m

i

q 

A 1

q m , where,q m

q B . (13)

i

As
(11)

:

Vim

-1

^1,2,..., N ymj` Umij  P mj 

-1

,

and

also
1
Um
j

where rij are ranks of designing operators
of interleyered connections. The line of rank matrix
corresponds to every apex of layer m and every
apex of layer m+1 to column of matrix. The sum of

^1,2,..., N ymj` V mj 1  1 ,

from (13) we will get

V mj 1 1

¦ Uijm 

1

i

P mj  1q m

. (14)

elements of line i is equal to the order of set Vmi,
and column j to order of set Um+1j. Let’s enter into
consideration the numerical set of Tm={1,2,...,Nm},
where

Nm=card(Tm) =

¦ ¦ rijm .
i

(It is not

j

difficult to check out that the value of Nm is equal to
the number of neurons in the layer of m and
coincides with the dimension of the receptor field of
layer m+1. We will place the elements of set Tm as
a matrix (fig 3.), similar to the structure of matrix
Rm. Thus we will make the placing so that a
m
condition will be executed: card ( Tij )

The transformation of the last expression,
obviously, results in (9). The use of two
substitutions qA and qB instead of one qm makes an
algorithm

the

symmetrical

one

and

more

comfortable during realization.
If

qA

q B ɬɨ q m

q A  1 q B

e, where

ɟ is identical substitution.
Let’s call topology for which the last

rijm . Let’s

equality is executed the compact one and extended

designate the subset of elements belonging to the

one otherwise. For the extended topology between

A
B
line i as Ti , and the column j as Tj . Let’s

the topologies matrixes of contiguous layers it is

introduce two arbitrary substitutions on the set Tm

substitution of qm. In general case for the task of

A

which we designate as q

B

necessary to enter a permutable matrix proper to

and q . Then the

topology it is necessary to fix substitution of qA , qB

algorithm of construction of topologies sets will be

and placing of set Tm. While storing the topology it

determined with a rule:

is possible without the loss of information to
shorten the volume of information, if to make

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

preliminary so called “normalization” of placing of

a)

T*ma TmqA,

q*Aa e,

q*Ba

A 1 B

q 

4

,

123 o V11
 56  456 o V21
3

p
p
124  356

b)

T*mb TmqB,

q*Bb e,

q*Ab

qB 1qA qm 1.
(15)

after normalization, and characters “a,b” the
variants of normalization. If topology is compact
normalization

A
equality q *

q *B

of

any

type

results

U12

in

e . Thus for a compact

4
p qB

m

substitution q . Let’s show that both variants of
normalization are equivalent. For this purpose we

,

U 22

p qB

 456 123
U12

U 22
U1i , V j2 can

Subsets

topology it is enough to store placing of set Tm
only, and for the extended one - placing Tm and

.

3 123 o V11
 56  456 o V21

12

Here the character “*” specifies the value

then

,

1 2

12

qm

q

2 1

R

set Tm, following one of the rules presented below:

arbitrarily.
1
equal U1

We

will

U12

123 ,

chosen

choose

them

 456 ,

will make normalization over the normalized

V12

presentation of the type of “*a” as a type “*b”.

topologies matrixes of the first variant exactly

T*ma q *Ba

  1 q B

T mq A q A

T mq B

T*mb ,

1

Thus,

we

come

to

the

q*Ab

qm 1.

normalized

 45 .

In this case the

correspond to presentation (10). For the second
variant

qB

q*Ba  1q*Aa q*Ba  1e §¨©qA 1qB·¸¹ qB 1qA

123 ,

V22

be

we

qA

choose

=

e,

§ 1 2 3 4 5 6·
¨
¸ and the previous
© 5 6 1 4 2 3¹

values of

U1i , V j2 are stored. The extended

presentation of “*b” type, determined by a rule

topology for this case has following matrix

(15). The reiteration of normalization as “a” type

presentation:

returns us to the initial state.

1 1 1

1

1 1 1
1 1 1

3. Trial study
As an example we will consider the
construction of topology for the structural model
presented on fig.2. The rank matrix and charts of
construction of two variants of topology are
presented below:

1 1
1

1 1
1 1

1
1 1 1
1 1 1
1 1 1

1
1
1

1 1 1
1 1 1
1 1 1

>From the extended topology it is always
possible to pass to compact one by multiplying the
left or right contiguous topology matrix on
permutable one. Obviously the information about
substitution qm is lost and the reverse transition will
be indefinite.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

Applications. Boston: Kluwer Academic Publishers,

4. Conclusion
In the given paper two levels of structural
description of FNN are presented: structural model

1993.
3.

[Che94] Cheung, John Y. Scheduling in

and topological realization. It is presented that the

Artificial

body

Manufacturing, Dagli, Cihan H., ed. London:

of

mathematics

of

numerical

partial

reflections is the adequate instrument of topology
design. On the basis of this body of mathematics the

Neural

Networks

for

Intelligent

Chapman & Hall, 1994, pps. 159-193.
4. [Arbib95] Arbib M. A., editor. The Handbook

computer-oriented algorithm of FNN construction

of

Brain

Theory

and

Neural

Networks.

of is offered. The topology design allows by

MIT Press, Cambridge, MA, 1995.

optimized way to choose hardware or software

5. [Dor00] Dorogov A.Y. Structure Synthesis of

representation of FNN from the class of equivalent

Fast Neural Networks. Neurocomputers Design and

ones.

Application. (New York).- Vol.1, Issue 1. 2000.pps. 1-18.
6. [Far03] Farin G., Hansford D. Linear Algebra:

References
1.

[Hay94] Haykin, S., Neural Networks: A

Comprehensive Foundation, NY: Macmillan, 1994,

Verlag, Heidelberg, 2003, pps.372-381.
7.

p. 2.
2.

Ein geometrischer Zugang. Published by Springer

[Kara93] Karayiannis, N.B. and A.N.

Venetsanopoulos.

Artificial

Neural

Networks:

Learning Algorithms, Performance Evaluation, and

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

[Mal03] Maltzew A.I. Algebraic systems.-

Moscow.: Science, 2003, pps.214-225.
8.

[Kos04] Kostrikin A.I. Modern algebra. -

Moscow.: Science. 2004, pps. 134-148.

