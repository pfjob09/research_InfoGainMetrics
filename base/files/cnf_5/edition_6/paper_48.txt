Fast Individual Facial Animation
Jian Yao, Mandun Zhang, Xiangyong Zeng, Yangsheng Wang
Institute of Automation, Chinese Academy of Sciences
Email: jyao@mail.pattek.com.cn

Abstract
We present a fast approach to generate individual face
model and to produce relevant facial animation. Firstly, a
frontal face image is captured with an ordinary video camera, and 85 key points are automatically detected. Then the
points are used for RBF network to deform a generic model
to an individual model. The texture is generated from the
same image and texture coordinates are interpolated. Finally we transfer the original vertex motion vectors to the
individual model in real time. All the procedure can be
implemented with minimal or even without manual tuning.
The result shows that our method is fast enough for common
applications.

1 Introduction
Facial modeling and animation, especially for individual face, is always a challenging and interesting task since
Parker’s pioneer work [1]. And applications of individual facial animation are highly desired feature for computer
games, virtual reality, medicine imaging, and so on.
Individual facial animation is mainly divided into two
step, individual face modeling and facial animation generating. There are various methods for individual face modeling, including laser scanner [2][3], stereoscopic camera [6],
active light stripper [5] etc. These methods generally need
expensive devices or are time consuming for realistic modeling. So it is difﬁcult for them to be used in generating
animation in real time. Also there are many methods for
animation generating. The anatomy method [4] uses physical models of muscle, fatty and bone, additional spring elements are needed to link different models. Performancedriven facial animation (PDFA) [7] uses local deformation
and often need manual tuning. In order to reuse existing
data, Jun-yong Noh presents a method called expression
cloning (EC). Generally, these methods seldom combine
well with individual modeling. Our aim is a fast combination of individual modeling and facial animation with an
ordinary video camera.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

In this paper, we present an approach for reconstructing
individual facial model from single frontal facial image and
generate animation in real time. Firstly, 79 MPEG-4 key
points [8] are automatically detected from the camera video
frame, and 6 forehead key points are added to maintain the
facial aspect ratio. With these key points, we can use Radial
Basis Function (RBF) to deform a generic model to individual face model. Then the texture image is generated with
the same image and texture coordinates are interpolated. Finally we use a similar method in expression cloning [11] to
reuse existing motion data on individual model.
The rest of the paper is organized as follows. Section 2
describes how to generate individual face model. Including
deformation of the generic model with detected key points,
and generation of texture image. Section 3 describes how
to transform the original motion vector to individual face
model. Then Section 4 shows some examples and draws a
conclusion.

2 Individual Face Modeling
2.1 Geometry Generation
We use one camera to capture frontal face image and automatically align the position of key points (Figure 1, 2).
Then the generic face model with needed key points can be
deformed to generate individual face model. From MPEG4 Face Deﬁnition Parameters (FDP), 79 key points are selected as the feature points of RBF. The rule of selection is
that whether a point can be obtained from frontal face. A
boosting method [9] is used to detect face existence, which
can prevent inﬂuence of skin color and illumination difference. And an ASM (Active Shape Model) method is used to
align all the feature points. Six additional key points (black
points in Figure 1) of the forehead are added to generate
proper facial aspect ratio. Detecting key points within each
single frame will bring wobble, so we interpolate the key
points’ position between frames for stabilization, which can
also smooth the alignment result.
After automatic alignment above, some manual adjustment can be applied optionally. The detected key points

only have x and y coordinates, so we use the depth information of the generic model to approximate z coordinate of
key points. Then all the 85 key points can be used in RBF
deformation.

3*

+2

+3
3,

,*
,,

,+

02

23
22

3+

+1

,-

,.

-

,

,/

-/

*

3

+
1

0

03

-0 .1
-1

-3

1+

.3

2
.0

-. -+*

+/

+.

++

+,

.+ .,
/*

/+

20

+-

/.
//

/3
/2

/1

face

(c) Individual face
model with texture

Figure 2. RBF deformation from generic
model to individual model

2-

/-

.2

1-

(b) Individual
model mesh

2/

2.

./,

(a) Generic face
model mesh with
key points (red)

-+

..
.*

1,

21

-*
-,

./

-2

1*

,3

,0

+0

.
/

,2

,1

2,

/0
2+

1.

2*
1/
10

11

12

13

(a) Selected MPEG-4 key points
(white) and added forehead key
points (black)

2.2 Texture Image Generation

(b) Automatically detected key
points (up) and manually tuned
key points (down)

Except geometry information, we also need a texture image for individual model. As mentioned before, the feature
points have been detected, so the region of face can be determined. We crop the face from the original captured image,
and then merge it with a background to generate the texture image. The background is ﬁlled with mean face color,
which is calculated from predeﬁned cheek regions. For example, the left cheek region is deﬁned as follows.

xmin = (x71 + x39 )/2



xmax = x39
(4)
ymin = x71



ymax = x68

Figure 1. Deﬁnition of key points

RBF is a supervised neural network. And it is a powerful tool for geometry smoothing and deformation [13][14].
RBF could perform deformation between any two meshes
in theory. To achieve fast implementation, we use the detected key points as RBF key points directly. The RBF
equation is
n

f (pi ) =

ωj · hj (pi )

(1)

j=1

where pi = (xi , yi , zi )T and f (pi ) = (xi , yi , zi )T is input
and output coordinates respectively. ωj is weight coefﬁcient
to be computed. And here is the basis function
hj (pi ) =

pi − pj

2

+ s2j

(2)

where sj = minj=i pi − pj . A regularization parameter
λ is used to minimize the cost function
C(ω) = eT e + λ · ω T ω

(3)
(a) Cropped face image and cheek region

where e = f (p) − H · ω is the error vector between the
training input and output coordinates, and H = [Hij =
hj (pi )]n×n . Here we use Generalized Cross-Validation
(GCV) [15] to measure prediction error, and iteratively
compute until GCV converges or reaches maximum iteration times.
Once the iteration has been done, RBF network smoothly
interpolates the remaining non-key points, thus deforms the
generic model to an individual model (Figure 2).

(b) Generated texture image

Figure 3. Texture generation

In order to avoid abrupt transition, a 10×10 sine template
is applied to feather the face border. Then the image size is
scaled to the power of 2 for hardware acceleration. Figure 3
shows the procedure of texture generation. After geometry
2

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

deformation, the vertices have different texture coordinates
with the original model. So the texture coordinates need
to be interpolated, which is also implemented by RBF. Figure 2 shows the ﬁnal individual face model with individual
texture.

R = RSG · RGD

To preserve the animation for models with different
scale, the motion vector also needs magnitude adjustment.
The width, height and length of the tow bounding boxes
(Figure 5) are respectively used to scale the x, y and z value
in local coordinate systems. This adjustment can be presented by a scale matrix S (Equation 8), which is a diagonal
matrix. Combination of rotation and scale matrix could produce the ﬁnal transformation matrix T (Equation 9). During
animation, the motion vector for each vertex is obtained by
m = T · m. Where m represents the source motion vector
and m is the destination motion vector.

3 Facial Animation for Individual Model
In general, a set of animation data is available for the
generic model. The individual model has different shape
with it, so these motion data can not be used directly. Similar with expression cloning, we use local bounding box to
establish the transformation matrix from the original normal vector to the deformed vertex’s normal vector. Then
the matrix can be used to adjust the original motion vector.
With direction and magnitude transformation, the original
motion data set can be reused and generate individual facial
animation in real time. The bounding box is deﬁned as a
directional bounding box, which involves all the neighbor
faces (Figure 4). And the bounding box has the same direction in edges with the local coordinate system. We deﬁned
axes of local coordinate system as follows. Vertex normal
vector, the average of the neighbor faces’ normal vector, is
the y axis. Second, the x axis is deﬁned as a horizontal line,
which lies in the plane whose normal is y axis. Lastly, z axis
is the cross product of x and y axes. With the deﬁnition of
local coordinates system, the rotation matrix between global
coordinate system and local coordinate systems, RSG and
RGD can be respectively determined with Equation 5 and
6. Then we can use Equation 7 to calculate the rotation matrix R from the source motion vector m to the destination
motion vector m .
NW[XQS MWQOU G
JWWXRTVOZS
H

@

E
F

G=

,09:0<
7896-5

@ A?
+0423.89 1-/0

.

-

/

+
,

*

Figure 5. Size of bounding box in local coordinate system


sx
S =0
0

0
sy
0


0
0
sz

where


sx = dw /sw
sy = dh /sh

sz = dl /sl

T =S·R

KSYZTVOZTWV MWQOU
JWWXRTVOZS
F=

(8)

(9)

4 Conclusion and Future Work

E=
H=

We implement our method with C++ and OpenGL, and
use a generic face model of 988 vertices and 1954 triangle
faces. Figure 6 shows some examples. On an average, it
takes less than 2 seconds to generate individual model and
the animation has an FPS above 60.
Our approach of individual facial animation can be implemented in real time. Although it is an ill problem to
generate model based on one image, this approach still satisﬁes common application such as games and communication. And possible ways to improve it include:

C
D

+0423.89 ;09:0<

@?>

(7)

B
LUWPOU
JWWXRTVOZS

Figure 4. Bounding box and local coordinate
system


xS
RSG =  yS  · xG yG zG
zS
 
xG
RGD =  yG  · xD yD zD
zG


• Use two or more generic models to adapt different
types of people.

(5)

• Use illumination model to approximate depth information.
(6)

• Add models and animation of hair, teeth and eye balls.
3

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

actions on Pattern Analysis and Machine Intelligence
(PAMI), Vol.22, No.10, p.1179 - 1185, 2000
[7] L. Williams, Performance-Driven Facial Animation,
SIGGRAPH Computer Graphics, Vol.24, p.235 - 242,
1990
[8] M. Escher, I. Pandzic, N. Thalmann, Facial Deformation for MPEG-4, IEEE Computer Animation, p.56-62,
1998
[9] Huang Xiangsheng, Xu Bin, Wang Yangsheng. Shape
Localization by Statistical Learning in the Gabor Feature Space, ICSP04, Sep. 2004
[10] V. Blanz , T. Vetter, A morphable model for the synthesis of 3D faces, Proceedings of the 26th annual
conference on Computer graphics and interactive techniques, p.187-194, July 1999
[11] J.-Y. Noh, U. Neumann, Expression Cloning, Proceedings of the 28th annual conference on Computer graphics and interactive techniques, August 2001
Figure 6. Individual facial animation. Top row
is generic model and animation. Left column
is neutral face

[12] J.-Y. Noh, U. Neumann, A Survey of Facial Modeling
and Animation Techniques, Technical Report 99-705,
University of Southern California, 1998.
[13] F. Pighin, J. Hecker, D. Lischinski, R. Szeliski, D. H.
Salesin, Synthesizing realistic facial expressions from
photographs, Proceedings of the 25th annual conference on Computer graphics and interactive techniques,
p.75-84, July 1998

References
[1] F. I. Parke, Computer Generated Animation of Faces.
Proc. ACM annual conf., 1972
[2] Y.C. Lee, D. Terzopoulos, and K. Waters, Constructing Physics-based Facial Models of Individuals, In Proceedings of Graphics Interface, p. 1-8, 1993

[14] F. Ulgen, A Step Toward Universal Facial Animation
via Volume Morphing, 6th IEEE International Workshop on Robot and Human communication, p.358-363,
1997

[3] Y.C. Lee, D. Terzopoulos, and K. Waters, Realistic Face
Modeling for Animation, Siggraph proceedings, p.5562, 1995

[15] G. Golub, M. Heath, and G. Wahba, Generalized cross
validation as a method for choosing a good ridge parameter, Technometrics, 21(2), p.215-224, 1979

[4] Yu Zhang, Edmond C. Prakashb, and Eric Sunga, Constructing a Realistic Face Model of an Individual for
Expression Animation, Animation International Journal of Information Technology Vol. 8, No. 2, p.10-25,
September 2002
[5] C. Beumier, and M. Acheroy, 3D Facial Surface Acquisition by Structured Light, In International Workshop
on Synthetic-Natural Hybrid Coding and Three Dimensional Imaging, Santorini, Greece, 15-17 Sep, p.103106, 1999
[6] O.Faugeras, L.Quan, and P.Sturn, Self-calibration of a
1D Projective Camera and its Application to the Selfcalibration of a 2D Projective Camera, IEEE Trans4

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV’05)
0-7695-2392-7/05 $20.00 © 2005 IEEE

