Poster: Translating Cross-Filtered Queries into Questions
Maryam Nafari∗

Chris Weaver†

School of Computer Science and Center for Spatial Analysis
The University of Oklahoma

A BSTRACT
Complex combinations of coordinated multiple views are increasingly used to design tools for highly interactive visual exploration
and analysis of multidimensional data. While complex coordination patterns provide substantial utility through expressive querying, they also exhibit usability problems for users when learning
required interaction sequences, recalling past queries, and interpreting visual states. As visual analysis tools grow more sophisticated, there is a growing need to make them more understandable as well. Our long-term goal is to exploit natural language familiarity and literacy to directly facilitate individual and collaborative use of visual analysis tools. In this poster, we present work
in progress on an automatically generated query-to-question user
interface to translate interactive states during visual analysis into
an accompanying visual log of formatted text. Our effort currently
focuses on a symmetric and thus relatively simple coordination pattern: cross-filtered views. We describe our current thinking about
query-to-question translation in a typical cross-filtered visualization
of movies, people, and genres in the Internet Movie Database.
Keywords: Coordinated multiple views, cross-filtered queries, interaction states, natural language generation, visual provenance.
Index Terms: D.2.2 [Software Engineering]: Design Tools and
Techniques—[User Interfaces]; H.5.2 [Information Systems]: Information Interfaces and Presentation—[User Interfaces]
1

I NTRODUCTION

Visual analysis tools are rapidly becoming indispensable for making sense of complex data. As these tools become more sophisticated in response to increasing analytical requirements (and good
design know-how), there is a corresponding need to make their usage easier to understand. In particular, the importance of producing,
presenting and disseminating analysis results [6] calls for strategies
to extend visual tools beyond local and immediate interaction.
“What is going on in this visualization?” is a common question
when users are interacting with a complicated visual analysis tool
consisting of multiple tables, plots, graphs, and other views. Moreover, the meaning and provenance of visual states visited during an
analysis session [2] are often forgotten soon after the session ends,
if not mere moments after each state is left behind. In cross-filtered
view designs [8], for instance, an out of sight, out of mind effect
occurs because users have trouble remembering more than the 2-3
most recent states in their querying process, despite the fact that selection of data items and toggling of checkboxes are the only interactions required to compose queries individually and in sequence.
Most visual analysis tools are structurally even more complicated.
Natural language generation is a well-established subfield of
computational linguistics with broad applications in information
∗ e-mail:
† e-mail:

maryam.nafari@ou.edu
weaver@cs.ou.edu

IEEE Symposium on Visual Analytics Science and Technology
October 24 - 29, Salt Lake City, Utah, USA
978-1-4244-9487-3/10/$26.00 ©2010 IEEE

processing, such as for description of data structures and relationships [1]. In expert systems and relational database systems, the
generation of questions can reflect the data query operations performed [5]. These questions are typically predefined questions suggested by the system designer to be relatively close to what users
expect. Natural language generation has also been applied in parallel with software visualization to explore program execution [4].
The relatively simple, symmetric coordination pattern in crossfiltered visualizations makes them well-suited for our initial exploration of techniques for translating sequences of visual analytic states into formatted text. We use a simplified version of
Cinegraph, a visualization for exploring and analyzing the Internet
Movie Database (IMDB) [7], to develop query-to-question techniques; movies, people, and genres have familiar ground truth that
facilities comparing generated text against visualization states.
It is important to note that our work focuses not on natural language processing, such as for entity extraction from text documents,
but rather on the reverse problem of expressing the entities and
structures involved in multidimensional querying as natural language. Our immediate aim is to provide an ancillary visualization
of query sequences as text that mimics semi-structured but natural
language forms like journals or meeting minutes. We also aim to reproduce query semantics as literally as possible within the limits of
visual versus textual forms. Toward this aim, we adopt constraints
on text generation in how we order questions, incorporate data values (entities) into questions, and update by expanding, collapsing,
and marking up existing text in response to new interactions.
2

T HE Q UESTION

OF

Q UESTION G ENERATION

Figure 1 shows an intermediate state of the visualization. The section on the right appends and scrolls as the user interacts with the
visualization. Two columns reflect sequential visualization states
by showing salient visualization interactions and the corresponding text translation in the form of questions. Alignment of corresponding elements in the two columns allows users to revisit the
interactions they have performed and review the resulting queries
as questions. A help button shows a brief description of formatting.
Exploring different formats, and the corresponding translation
rules for expressing them, is a major thrust of our work. Based on
different analytical tasks, users performing different tasks in various roles may prefer different formats and levels of abstraction of
questions. At present, we provide two formatting options (“simple” and “complex”), each of which has advantages and disadvantages as a function of user preferences and specific analytical tasks.
Simple questions concatenate using “AND”: “What movies did win
{Best Picture} Oscar? AND What movies are {Action,...,or Crime}
genre? AND What movies were {Affleck, Anderson, or Black} involved with?” Basic concatenation is simple and easy to follow, but
is less natural and is less expressive for showing compound logic
across multiple questions. Complex questions are closer to natural
language in terms of expressiveness but also inherit the linguistic
conjunctive-disjunctive ambiguity of “and” and “or”, as in “What
movies did win {Best Picture} Oscar and are {Action,...,or Crime}
genre and were {Affleck, Anderson, or Black} involved with?”
Selecting various options and cross-filtering different tables on
different variables can confuse users while they are performing an-

245

Figure 1: A simplified version of the Cinegraph visualization, showing popular recent movies, people, and genres in the Internet Movie Database,
alongside a hand-formatted execution of our current question generator algorithm. The event and question views (rightmost pair of columns)
show correspondences between interactions (selections and cross-filterings) and natural language fragments that describe visual query states.

alytical tasks. Users can be divided into two main groups: those
familiar with the tool and those who are not. Providing an interface
that displays visual states in the form of text visualization may be
beneficial to both kind of users. Natural text might help new users
find out how a particular tool can be used for their analytic tasks,
and what kind of information those tasks produce. For professional
users, it might facilitate and accelerate the analysis process by allowing analysts and decision-makers to focus on sensemaking at
the level of abstraction of text summaries rather than spending time
(re)foraging at the level of individual data items in the visualization.
Various extensions could fulfill other analysis needs. Users could
recall interactions with the visualization and restore visualization
states to see the full context of summarized information. They
could cooperate within and across roles by sharing visualizations,
text, or both; performing different analytical tasks in terms of same
time same space and different time same space are thus possible [3].
A major challenge lies in accounting for the idiosyncrasies of
each data attribute that is being cross-filtered. For instance, the
genre attribute calls for a “what” question but the people attribute
calls for a “who” question. We are discovering that there is a tradeoff between syntactic symmetry and linguistic quality, necessitating
choices that often pit implementational expediency against anticipated readability of the resulting questions. But we are also finding,
as we had hoped early on, that additional visual encoding of the text
may provide an escape from this tradeoff. In figure 1, even a basic
visual encoding of interaction events and key sentence structures
appears to improve readability dramatically, by highlighting data
entities (blue), emphasizing possibly confusing interactive conditions (red), and copyediting rather than replacing defunct questions
using strikeout. We handle arbitrarily long lists of attribute values
using an ellipsis as a graphical placeholder; future options include
popup menus (for long lists of nominal values) and sparklines.
As we move forward, we plan to support design-time (and perhaps even use-time) specification of this grammatical plus structural
encoding as a part of the data transformation pipeline, using a formbased interface like many visualization builder tools. Generation of
text with sentence, paragraph, section, and page formatting happens at the visualization transformation stage rather than the data
transformation stage, lest we need to perform natural language processing on our own generated text. An important part of this will

246

be maintaining the sequence of questions as structured data with
appropriate back references to interactive states and events.
3 C ONCLUSION
We anticipate that query-to-question techniques will greatly enhance visual analysis by capturing interaction and visual representation states as natural language. Moreover, ancillary text summaries may make interaction both more useful and usable by integrating multiple levels of perceptual, cognitive, and informational
abstraction into tool interfaces. In developing these techniques, we
have initially focused on basic visual forms like cross-filtering in
a simplified Cinegraph visualization. Moving forward, we will expand these techniques to support general tabular data, particularly
temporal and spatial data across multiple scales. Finally, we are
exploring how multiple levels of translation along a visual-to-text
spectrum might be used to support a succession of increasingly abstract user-manipulated summaries that enable a full range of visual
analysis tasks from basic data foraging to high-level sensemaking.
R EFERENCES
[1] P. S. Jacobs. Knowledge-intensive natural language generation. Artificial Intelligence, 33(3):325–378, 1987.
[2] T. J. Jankun-Kelly, M. Kwan-Liu, and M. Gertz. A model and framework for visualization exploration. IEEE Transactions on Visualization
and Computer Graphics, 13(2):357–369, March-April 2007.
[3] R. Johansen. Groupware: Computer Support for Business Teams. Free
Press, New York, 1988.
[4] N. Myller. Automatic generation of prediction questions during program visualization. Electronic Notes in Theoretical Computer Science,
178(4):43–49, 2007.
[5] W. A. Perkins. Generation of natural language from information in a
frame structure. Data & Knowledge Engineering, 4(2):101–114, 1989.
[6] J. J. Thomas and K. A. Cook, editors. Illuminating the Path: The Research and Development Agenda for Visual Analytics. IEEE Computer
Society, August 2005.
[7] C. Weaver. InfoVis 2007 contest entry: Cinegraph. In Proceedings
of the IEEE Symposium on Information Visualization (InfoVis) (Compendium), Sacramento, CA, October 2007.
[8] C. Weaver. Cross-filtered views for multidimensional visual analysis. IEEE Transactions on Visualization and Computer Graphics,
16(2):192–204, March-April 2010.

