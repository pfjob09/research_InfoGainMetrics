ALIDA: Using Machine Learning for Intent Discernment in Visual Analytics
Interfaces
Tera Marie Green∗
∗ School

Ross Maciejewski†

Steve DiPaola∗

of Interactive Arts + Technology - Simon Fraser University
Visual Analytics Center - Purdue University

† Purdue

A BSTRACT
In this paper, we introduce ALIDA, an Active Learning Intent Discerning Agent for visual analytics interfaces. As users interact with
and explore data in a visual analytics environment they are each
developing their own unique analytic process. The goal of ALIDA
is to observe and record the human-computer interactions and utilize these observations as a means of supporting user exploration;
ALIDA does this by using interaction to make decision about user
interest. As such, ALIDA is designed to track the decision history
(interactions) of a user. This history is then utilized to enhance the
user’s decision-making process by allowing the user to return to
previously visited search states, as well as providing suggestions of
other search states that may be of interest based on past exploration
modalities. The agent passes these suggestions (or decisions) back
to an interactive visualization prototype, and these suggestions are
used to guide the user, either by suggesting searches or changes to
the visualization view. Current work has tested ALIDA under the
exploration of homonyms for users wishing to explore word linkages within a dictionary. Ongoing work includes using ALIDA to
guide users in transfer function design for volume rendering within
scientific gateways.

cognition model through use of an intelligent, autonomous agent
that sits underneath a simple data visualization prototype.
2 R ELATED W ORK
Work in determining a users interest during interaction has largely
focused on web interfaces. Recent examples include Godoy and
Amandis [1] personal agents for web experiences, which tracks user
behaviour and provides a user profile to aid in future web navigation. Another example, MASHA is a multi-agent architecture for
multi-modal web interaction [5]. In visualization related research,
machine learning has been used on user annotations to attempt to
classify user goals [6]. Our work focuses on adapting this concept
of a personal agent into the visual analytics pipeline.
3 ALIDA: ACTIVE L EARNING I NTENT D ISCERNING AGENT
This section describes our personal agent (ALIDA) and the current
application domain. The current ALIDA prototype was developed
in the Processing prototyping language. Processing does not easily
support highly interactive visualizations (such as direct interaction),
but does support the rule based decision function ALIDA uses.

Keywords: artificial intelligence, cognition, intent discernment,
volume rendering.
1

I NTRODUCTION

Intuitive interactive visualizations are designed to scaffold human
cognition. However, cognition, especially the higher processes such
as reasoning, tend to be combinatorial and dynamic. As such, these
processes are difficult to standardize. Furthermore, there are no operational models of human reasoning, thus such interactive visual
interfaces often encounter difficulties when trying scaffold what is
not known or understood. One effort to define an intuitive visualization system that would incorporate an ongoing comprehension of
holistic cognition is the Human Cognition Model (HCM) [3]. The
HCM outlines the primary interactive processes during interfaceenabled analysis.
One of the agenda items that the HCM outlined is the need to
make visual analytics more intuitive by creating interfaces that pay
attention to what information or data the user is exploring in order
to provide clues to what related information the user might be interested in [3]. This fits well with the definition of visual analytics in
general as the use of interactive interfaces to support analytical reasoning. By capturing the analyst’s focus, interfaces are able to be
automatically tailored to a user’s needs, enabling the data analysis
and visualizations to have enhanced contextual information directly
related to the analysts needs. This paper outlines ongoing research
toward the interface-human collaboration envisioned in the human

IEEE Symposium on Visual Analytics Science and Technology
October 24 - 29, Salt Lake City, Utah, USA
978-1-4244-9487-3/10/$26.00 ©2010 IEEE

Figure 1: The flow of intent discernment with ALIDA, from user interaction until ALIDA provides feedback.

ALIDA sits underneath the interactive visual interface, continuously recording and analyzing user behaviour in real-time. The
agent uses low-level interaction information, such as mouse clicks
and movements, to make decisions about current and future intent.
During this observation ALIDA does not disturb the user or ask for
direct feedback. Instead, ALIDA is designed to be autonomous; every 60 seconds, the agent makes a decision by using the captured
interactions to calculate a decision function. The results of ALIDA’s decisions are provided as feedback to the user in a variety of
ways and stored in ALIDAs memory for future use.
The decision function currently returns three categories of decision: area of interest, browsing, and inaction. (Please see Figure 1.) To return a decision about an area of interest, a comparison

223

of means of interaction behaviors and decision history are both used
to determine the current base category of interest. If no single category stands out (interaction is scattered in a statistically insignificant way), the agent decides the user is browsing. In the last case,
the user has ceased (or almost ceased) all interaction with the visualized glyphs. In this way, ALIDA uses these decisions as part of
ongoing research to make interfaces aware of what the user is exploring in order to direct the interface which context or related data
to display.
ALIDA uses a MySQL database to store a history of intent decisions for each user. This history is currently used in two ways. First,
the history is incorporated into the decision function. If past intent
discernment decisions are similar to previous decisions, the probability of accuracy is increased. Second, it allows the user to take
advantage of interface capacity to return to previous states the user
has visited. This is also part of an emerging capacity for user profiling as ALIDA becomes a learning agent. Feedback, for testing
purposes, is done through direct decision reporting in an ambient
overlay. (See Figure 2.) Other forms of feedback are less direct,
such as making areas of user intent more visible in the view. All of
ALIDA’s features are currently being user tested for accuracy and
ease of use by comparing ALIDA’s decision to the self-reported intent of the users during interaction.
4

A PPLYING ALIDA

FOR

L EXICAL O NTOLOGY

The testing and evaluation phase of ALIDA is being performed using interactive visualization tool for exploring lexical ontologies.
The interface prototype visualizes a multiple hierarchical dataset
where each level, or cluster, is visualized as a sphere within the
spatial context of the base node (or categorization) in the hierarchy,
Currently, we utilize a lexical ontology of homonyms as the test
dataset. This dataset was chosen to support more advanced semantic testing in future work. The name of each category is used to
label each spherical glyph. All category labels had fewer than three
words, and the glyphs were organized such that there was sufficient
space between glyphs to make the labels easily legible.
Interaction with the glyphs is direct, that is, users click directly
on the glyph of interest. Drill down is discrete. The clicked-on
glyph disappears to reveal all subcategory glyphs. Right-clicking
supports a discrete drilling up.
To support the “reset” and “go back” functionality, associated
icons float on the periphery of a highlighted glyph. Reset allows
the user to return to initial view. Go back takes the user back to the
glyph, or area, of last interest based on ALIDA s memory of the last
intent decision.
5

O NGOING

AND

F UTURE W ORK

This agent is only successful insofar as it “understands” and responds to the human user. To some degree, this agent will allow
us to test certain research questions that have arisen during our research of human complex cognition during analytical reasoning in
interface interaction, such as the impact of human individual differences on use of visually enabled interfaces (e.g. [2]). We will use
what we have learned about complex cognition during the agent development, and use the agent as another method of evaluating the
human cognitive models such we are currently developing, such as
aspects of the HCM.
In addition, as part of future research, we plan to extend ALIDA to be used in volumetric rendering. Multi-dimensional transfer functions have long been established as a powerful means for
interacting with volumetric data [4]. However, efficiently designing transfer functions has turned out to be a non-trivial problem.
Typically, multi-dimensional transfer functions are shown as a twodimensional (2D) scatterplot, the axes of which represent two variables of the feature space of the data. Users interact in this space,
assigning optical properties (color and opacity) to the voxel data. In

224

Figure 2: ALIDA direct reporting through ambient overlay.

this dual-domain interaction (volume data space and transfer function feature space), users are able to select data properties in the feature space and see the resultant rendering in the volume space. This
interaction improves the user’s ability to understand how the feature
space of the data is reflected in the volumetric data. However, much
of the user interaction tends to be a hunt-and- peck approach as they
modify and refine the transfer function based on their knowledge of
the dataset or general intuition. Through the addition of an intent
discerning agent where the agent unobtrusively observes the overt
behavior of the user and generates transfer functions across combinations of feature space, suggesting potential areas of exploration
to the user based upon the user’s own interactions. The goal of the
agent would be to direct the visualization with respect to underlying statistical properties that the user may or may not be utilizing
in their exploration. Thus, the agent becomes part of a continuous
feedback loop which is fed by continuing human interaction (or
lack thereof).
R EFERENCES
[1] D. Godoy and A. Amandi. User profiling for web page filtering. In
Proceedings of Internet Computing, volume 9, pages 56–64, 2005.
[2] T. M. Green, D. H. Jeong, and B. Fisher. Using personality factors to
predict interface learning performance. In Proceedings of the Hawaii
International Conference on System Sciences, volume 43, pages 1–11,
2010.
[3] T. M. Green, W. Ribarsky, and B. Fisher. Determining and building a
human cognition model for visual analytics. Information Visualization,
8(1):1–13, 2009.
[4] G. Kindlmann and J. W. Durkin. Semi-automatic generation of transfer functions for direct volume rendering. In Proceedings of the IEEE
Symposium on Volume Visualization, pages 79–86, 1998.
[5] D. Rosaci and G. M. L. Sarne. Masha: A multi-agent system handling user and device adaptivity of web sites. User Modeling and UserAdapted Interaction, 16(5):435–462, 2006.
[6] O’Brien, T.M., Laidlaw, D.H.: Toward a machine learning approach for
classifying user goals from user interactions In: Proceedings of Symposium on Interactive 3D Graphics, 2010 ACM SIGGRAPH, (2010).

