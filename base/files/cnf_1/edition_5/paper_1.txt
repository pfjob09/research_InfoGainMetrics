Activity Analysis Using Spatio-Temporal Trajectory Volumes in
Surveillance Applications
Firdaus Janoos

Shantanu Singh
Okan Irfanoglu
Richard Parent ∗

Raghu Machiraju

Dept. of Computer Science and Engineering ,Ohio State University

A BSTRACT
In this paper, we present a system to analyze activities and detect
anomalies in a surveillance application, which exploits the intuition
and experience of security and surveillance experts through an easyto-use visual feedback loop. The multi-scale and location speciﬁc
nature of behavior patterns in space and time is captured using a
wavelet-based feature descriptor. The system learns the fundamental descriptions of the behavior patterns in a semi-supervised fashion by the higher order singular value decomposition of the space
described by the training data. This training process is guided and
reﬁned by the users in an intuitive fashion. Anomalies are detected
by projecting the test data into this multi-linear space and are visualized by the system to direct the attention of the user to potential
problem spots. We tested our system on real-world surveillance
data, and it satisﬁed the security concerns of the environment.
Keywords: wavelets, HOSVD, surveillance, anomaly detection,
trajectory
Index Terms:
I.4.7 [Computing Methodologies]: Image
Proc. and Comp. Vision—Feature Measurement; I.5.4 [Computing
Methodologies]: Pat. Rec. —Applications; I.4.8 [Scene Analysis]:
Tracking—
1 I NTRODUCTION
In today’s world, monitoring and surveillance are increasingly critical parts in wide-spread homeland security systems. However, one
of the main difﬁculties in building such systems lies in discriminating unusual and threatening behavior against the backdrop of the
range of normal or typical behaviors of people in the environment.
What is informative in surveillance includes recognizing activities,
knowing typical and threatening patterns of behavior, inferring intent, and seeing through attempts to mask threats with normal behavior. Yet, ﬁnding any of this information in the increasing sea of
low level video data (and other sensor modalities) is a complex task
that goes well beyond simply providing more and different sensors
over wider areas. Advances in remote sensing systems have provided human operators access to more data, but, in a typical surveillance application the amount of video feed captured is so large that
it quickly overwhelms the ability of security personnel to analyze
and respond to data in a time critical fashion. The current challenge
is to help extract relevant patterns and direct the attention of the human operators to acute situations and longer term trends (i.e. seeing
new patterns of typical behavior and identifying emerging threats.)
Our proposed system is part of a larger Strategic Surveillance
System built on the concept of Collaborative Autonomy, in which
machine agents are designed to collaborate with human operators to
overcome the brittleness in algorithms, to cope with the false alarm
problem, to re-direct human attention to changing events, and to be
re-directable as people bring contextual information to bear. This
∗ e-mail:

{janoos,singhsh,irfanogl,raghu,parent}@cse.ohio-state.edu

IEEE Symposium on Visual Analytics Science and Technology 2007
October 30 - November 1, Sacramento, CA, USA
978-1-4244-1659-2/07/$25.00 ©2007 IEEE

collaboration must be enabled for different time scales which will
require different tactics of human-automata coordination. These
principles require that the human operators be able to interact with
and guide the system to ensure that it operates congruent to their
experience and intuition for human behavior patterns.
In computer vision, the problem of determining patterns of behavior in an environment has been extensively studied (see Sec. 2
for a brief survey). The traditional approach involves tracking
people in the environment, extracting the spatio-temporal trajectories of each individual, and then analyzing these trajectories
separately and collectively to identify the behavior of the group,
in a semantic sense (e.g. people gathering/dispering,
or person A following person B or a traffic jam
at intersection, etc. ). However, one of the main drawbacks
of this approach has been the generation of the spatio-temporal trajectories themselves. It is a major challenge to acquire a clean, error free, uninterrupted trajectory of every individual in the environment. Unreliability in the low-level person detection algorithms is
one factor, but the main problem is due to tracking issues like fragmentation of the trajectory (due to occlusions), sensor gap (when
an individual temporarily moves out of camera coverage) and ambiguity (when two or more individuals come together and then move
apart). We deal with these problems of fragmentation, sensor gap
and ambiguity, by looking for behavior patterns in the ensemble of
the trajectories rather than individually. We build a spatio-temporal
trajectory volume (STTV) of all the (perhaps fragmented) trajectories occurring in the environment over an extended period of time,
and analyze this volume for known patterns and anomalies without
having to construct the exact trajectory for each individual. Also,
the combination of machine analysis and user interpretation makes
our method robust to the levels of noise usually encountered in
surveillance systems.
Behavior patterns occur across multiple scales, and are location
dependent (space and time). Different behaviors may be composed
of the same set of primitive events that differ only in their timespan, and their time of occurrence. Similarly, the geographic location and extent affects the identiﬁcation of the behavior. Additionally, the trajectory volumes are constructed from tracking data
obtained from multiple independent cameras, and therefore suffer
from problems of exactly determining trajectory correspondences
across cameras, and from general errors due to occlusions, illumination changes, etc. Therefore, the multi-scale, location sensitive nature of our task, and the localized nature of the noise in the
model, suggest that wavelets are an appropriate basis for the feature
descriptor.
Our system is designed to be trained online by security personnel, who identify segments of video feed as normal behavior patterns. The training instances in a single class of activities exhibit
similar patterns at characteristic scales and translations in the trajectory volume. Our system learns these recurring themes from the
instances of a class in an automated fashion, and uses this information to identify whether a new trajectory volume (testing data point)
exhibits a similar activity pattern. A PCA-like method (Fukunaga
[16]) is ideal for such an application. PCA itself requires the feature be presented as a vector, which blurs the separation between

3

the two spatial scales, the temporal scale, the two spatial locations
and the temporal location in the wavelet description. To preserve
the information independence between these modes, we use Higher
Order Singular Value Decomposition (HOSVD) to perform such an
analysis. We perform the HOSVD on the wavelet derived feature
space, rather than the trajectory volume space itself, because our
feature descriptor captures the spatio-temporal proximity of the activity in the scene, which is not captured by voxels of the original
volume.

Figure 1: Schematic overview of system showing the different modules of the systems. The Low Level Tracking Module interfaces with
the surveillance cameras and supplies our system with trajectory
data, which is converted to a representation called a Spatio Temporal Trajectory Volume (STTV). The Analysis Module builds a wavelet
based description of the behaviors and performs anomaly detection,
in conjunction with the Training Subsystem. The Visualization Module and User Feedback Loop allows the user to interact with and
control the system.

Fig. 1 shows a schematic overview of our proposed system. The
Low Level Tracking Module interfaces with the surveillance cameras and supplies our system with trajectory data, which is converted to a representation called a Spatio Temporal Trajectory Volume (STTV) as explained in Sec. 3. The Feature Descriptor Extraction sub-system builds a wavelet based description of the behaviours captured by the STTV (Sec. 3.1). These feature descriptors are used to train the system, as well as perform anomaly detection as described in Sec. 4. The user (security expert) can view
the video feed, the STTV representation and anomalies detected by
the system through the User Interface, and can guide the system
through the Visual Feedback Loop described in Sec. 5.
In Sec. 6 we discuss the experience of the users with our system and show user validation of the analysis algorithm. Finally, in
Sec. 7, we conclude by presenting a summary of our methods and
directions for future work.
2 R ELATED W ORK
The problem of human activity recognition has been extensively
studied in computer vision (Bobick [3] and Nagel [25]) and has the
following four salient categories:
I. Human Action Analysis: The high level reasoning about the
activity of individuals by analyzing the motion of their body parts.

4

Aggarwal and Cai [1], and Gavrila [17] give a plenary review of the
current research in this area.
II. Motion Based Recognition: The recognition of behavior of
individual humans from their motion information, by analyzing the
spatio-temporal trajectories. See Niu et al. [26], Brand et al. [5],
Oliver [27], for more detail on these methods.
III. Interactive Behavior Recognition: It consists of understanding human behaviors involving interactions between two or
more people. These methods are usually restricted to a very small
number of people, and over relatively short durations of time.
Oliver [28], [27] gives a comprehensive survey of the work in this
ﬁeld.
IV. Group Behavior Analysis: This involves analyzing the behavior of large and varying numbers of people over varying lengths
of time, and geographical areas of varying sizes, in order to determine the patterns of human behavior in the environment. Whereas
we are interested in the problem of labeling group behavior patterns as normal or anomalous, and in highlighting the region of the
anomaly, much of existing literature is devoted to extracting semantic (higher level) inferences and cataloguing the behavior patterns
(Buxton and Mukerjee [8] ). Below we give a brief survey of the
representative research in this domain.
Buxton [7] presents a Bayesian belief revision approach for active behavioural analysis which exploits simple correlations in the
spatio-temporal data to infer more complex behaviours. Cupillard
et al. [10], [11] discuss a method for recognizing behavior using
multiple cameras with overlapping ﬁelds of view. They deﬁne a
formal language to describe behavioural patterns. The surveillance
data from multiple cameras is combined by grouping objects with
similar values of position, size, motion patterns, and semantic labels. Hongeng et al. [18], [19] propose a more general method
of activity recognition using a Bayesian framework based analysis of the trajectories of the individuals in the scene. Br´emond and
Medioni [6] specify a model to describe scenarios of human activity
as a combination of sub-scenarios and properties of the mobile objects involved in the scenario. To perform scenario recognition, this
method requires an a priori speciﬁcation of the mission context (the
speciﬁc methods to recognize the scenarios,) in a formal language.
The class of methods listed so far, require an a priori speciﬁcation of the behavior patterns in some type of formal language or as
ﬁnite automata, as compared to our method in which security personnel can train the system simply by a ”point-and-click” type of
operation, without having to analyze the semantics of the patterns
themselves.
Makris and Ellis [21] propose a combination of spatial and probabilistic models for reasoning about pedestrian behavior, using unsupervised learning techniques to label trajectories and identify
atypical behaviours. In [22] they investigate an activity-based semantic model for a scene under visual surveillance, that identiﬁes
regions where particular types of motion activity are located. They
illustrate methods that allow unsupervised learning of the model
from trajectory data. The main drawback of such methods is that
they build a probabilistic model of typical pedestrian trajectories
and ﬂag anomalies as large deviations from this but do not enforce
user deﬁned or context specﬁc behavior classes, (e.g. day time activity vs. night time activity).
In addition to requiring clean motion trajectories of all the individuals in the environment, the methods cited above, in general,
require other modalities of information such as the size and color
histograms of the moving regions, etc., which makes them very sensitive to noise in the lower-level vision systems. Also, this affects
scalability of these methods with respect to group sizes and behavior patterns, in terms of the computational complexity of the methods and the descriptive capability of the behavior models.
Brand and Kettnaker [4], use an entropy minimized HMM to
annotate ofﬁce activity, monitor trafﬁc intersections and infer 3D

motion from video. In the trafﬁc monitoring application, they use
optical ﬂow to perform abnormality detection. The main difference
is that trafﬁc motion is very ordered (restricted to lanes), and the
number of behavior types is small as compared to pedestrian behaviours. Furthermore, this method does not distinguish between
different behavior types, rather it learns a single model of normal
behavior and tests all input against this model. Also, it does not address the issue of behavior patterns occurring over multiple spatiotemporal scales.
A signiﬁcant difference between the methods mentioned so far
and ours is that they do not speciﬁcally incorporate visual and userinteractive elements into their design and formulation. We feel that
this prevents these systems from being used to their maximum potential (See Sec. 5).
Davis et al. [13] introduced the concept of activity maps, as a 2D
spatial and scene-speciﬁc model of the local activity (motion) occurring in the area under surveillance, over a speciﬁc time interval.
Human activity is deﬁned as any human generated motion and is
recognized by characteristic translating motion patterns. They then
use the activity maps to determine the optimal scan paths for the
PTZ cameras in the environment.
The problem of visualizing video so as to convey maximum information to users in appropriate visual representations has been
studied in the context of volume visualization. Chen et al. [9] give
a broad treatement of this subject, with emphasis on the visualization of motion events in videos, and a comparison of the different
abstract visual representations.
3 S PATIO -T EMPORAL T RAJECTORY VOLUMES (STTV)
Trajectory fragments of pedestrian motion from video are used to
generate a volumetric representation called the Spatio-Temporal
Trajectory Volume (STTV). An STTV summarizes the results of
tracking human activity within a spatio-temporal extent of constant
duration. An implementation of the Lucas-Kanade tracker [20] is
used to extract trajectories from a video sequence. It generates several fragmented trajectories of the motion in that temporal extent.
We then embed these trajectories in the STTV. Next, we smooth
the volume with a Gaussian ﬁlter to mitigate the effects of tracking
noise and trajectory fragmentation. Fig. 2 shows the STTV constructed for one such video sequence in our environment.
3.1 Wavelet Based Feature Descriptors
The wavelet transform provides a rich and descriptive means of analyzing functions by providing time-frequency localization and they
form an unconditional basis for a large class of signals (Donoho
[15]). Also most wavelet systems satisfy the multi-resolution condition (Daubechies [12]), and allow for a multi-scale analysis of
signals.
If F(x, y,t) is the STTV to be transformed, the 3D scaling coefﬁcient ci jk (u, v, w) and corresponding wavelet coefﬁcient di jk (u, v, w)
are deﬁned by:
ci jk (u, v, w) =
di jk (u, v, w) =

x y t
x y t

F(x, y,t)φi jk,uvw (x, y,t)dxdydt

(1)

F(x, y,t)ψi jk,uvw (x, y,t)dxdydt

(2)

where φi jk,uvw (x, y,t) is the 3D scaling function and ψi jk,uvw (x, y,t)
is the 3D wavelet function at the log2 scale factors i, j, k and with
the translations factors as 2i u, 2 j v, 2k w respectively.
The reconstruction formula is:
F(x, y,t) = ∑ ci0 j0 k0 (k)φi0 j0 k0 ,uvw (x, y,t)
uvw

i0

+∑

j0

k0

∑ ∑ ∑ di jk (u, v, w)ψi jk,uvw (x, y,t)

i=1 j=1 k=1 uvw

(3)

The 3D wavelet transform (and its inverse) are computed by the sequential application of the 1D Discrete Wavelet Transform (Inverse
DWT respectively) in O(UVW ) time (Mallat [23]), where U,V,W
are the extents of F(x, y,t) along the x, y,t axes respectively.
Since we are interested in studying the multi-scale structure
present in the STTV F(x, y,t), and not in reconstruction or compression per se, we use the scaling coefﬁcients of the wavelet transform
to create our feature descriptor. The scaling coefﬁcients capture
the structure present in the signal, and their magnitude is maximum
when the size of the structure matches the size of the scaling function (Mallat and Zhong [24], Baranuik [2]). The 1D form of the
feature descriptor γ j (u) at scale level i and translation 2i u is deﬁned
as:

γi (u) = ci (u) − ri (u)

j = j0 ...1

(4)

where, ri (u) = (ci+1 ↑ 2) h0 ,a is the reconstruction of the scaling
coefﬁcients ci+1 (u) up by one level. It is so deﬁned to remove
the redundancy of information that exists in the scaling coefﬁcients
across scales. Using this descriptor reconstruction can be achieved
only up to the ﬁnest but one scale. That is, while the coefﬁcients
c0 represent the sampling resolution, reconstruction of only up to
coefﬁcients c1 can be performed recursively using Eqn. 5
ci (u) = γi (u) + ri (u)

i = i0 ...1

(5)

The 3D form of the descriptor γi jk (u, v, w) is obtained similarly
from the 3D scaling coefﬁcients. We use the orthogonal 3D Haar
wavelet system to generate the feature descriptors, because the
compact, non-overlapping support of the scaling function in each
scale gives good separation of the information at that scale.
4 A NOMALY D ETECTION
4.1 Behavior Classes
In our application, the behavior patterns are grouped together
into day-and-time speciﬁc classes, like early-morning,
start-of-school-day, lecture-period,
weekend-night, etc. Therefore the candidate class, against
which to test a video sequence for anomalies, is known a priori
based on when (day and time) the video sequence was recorded.
As a result, the anomaly detection problem becomes (a) to see
if the test data point belongs to the candidate class, (b) in what
salient ways does it differ from the class. We learn a fundamental
description of each class by decomposing the space spanned by
its training samples using Higher Order Singular Value Decomposition (HOSVD) (Sec. 4.2) and then eliminating the natural
variations and background noise in the training samples through
dimensionality reduction. The process of dimensionality reduction
is guided by the human operators of the system who adjust the
parameters of the anomaly detection module to bring it in line with
their expectations (Sec. 4.4).
4.2 Higher Order Singular Value Decomposition
HOSVD is an extension of the classical PCA/SVD on matrices to
higher order tensors (Lathauwer et al. [14]). The feature descriptor
γi jk (u, v, w) of the STTV is essentially a tensor, in which the two
spatial scales, the temporal scale, the two spatial translations and
the temporal translation form six independent modes. Ideally, we
would like to preserve this independence of the modes when ﬁtting
a model to each behavior class, however the size of this six mode
tensor is prohibitively large (log2 U × log2 V × log2 W ×U ×V ×W
where U,V,W are the extents of the STTV along the x, y and t axes
respectively). In order reduce the size of this tensor, we assume the
two spatial axes x and y to be isotropic and identical, and collapse
a ↑ denotes upsampling by a factor or 2, denotes convolution, h is the
0
low-pass quadrature ﬁlter of the wavelet system

5

0
10
20

t−Axis

40

30
20
40
50

0
0

10

20

30

40

50

60

70

x−Axis

60

y−Axis

(a) Image of scene with trajectories

(c) STTV after Gaussian smoothing

(b) Trajectories in 3D

Figure 2: Construction of Spatio-Temporal Trajectory Volume (STTV). Fig. (a) shows an image of the scene with the trajectories over a ﬁxed
temporal duration overlaid on it. Fig. (b) shows these trajectories laid out in 3D (x,y,t). Fig. (c) shows the Gaussian smoothed STTV for this
duration.

the two spatial scales into one mode (similarly for spatial translations). Thus the four mode tensor form is γ[i j]k ([uv], w) and has size
log2 U × log2 W ×U ×W . We decompose the tensor using HOSVD
(Vasilescu and Terzopoulos [29]). This allows us to control the
variance in the observed data and to localize the error of a test data
point in each mode independently, by reducing the dimensions of
each mode separately. Intuitively, this means that by adjusting the
number of dimensions in, say, the temporal translation mode one
can control how sensitive the model is to differences of temporal
translation between the test data point and the space spanned by the
class. If N is the number of training instances of a behavior class,
we construct a 5 mode tensor T by concatenating the training instances along the ﬁfth mode. We then learn the linear space spanned
by the modes of that behavior class by decomposing the tensor as:

multi-linear space, as:

If
then

C = S ×1 x ×2 U(i j) ×3 U(k) ×4 U(uv) ×5 U(w)

(7)

M = S ×2 U(i j) ×3 U(k) ×4 U(uv) ×5 U(w)

(8)

C = M ×1 x

(9)

Eqn. 9 is a multi-linear operation which maps the coefﬁcients of the
space spanned by the training instance mode to feature descriptor
tensor C. In order to ﬁnd the best coefﬁcient vector x for an unseen
test instance, we minimize the reconstruction error as:
N

E = ∑ ∑ ∑ ∑(C pqrs − (∑ Mnpqrs × xn ))2
p q

T = S ×1 U(trg) ×2 U(i j) ×3 U(k) ×4 U(uv) ×5 U(w)

(6)

The tensor S is called the core tensor and is analogous to the diagonal singular value matrix in traditional SVD. However, in HOSVD,
S is not a diagonal tensor. The operation ×n indicates a tensormatrix multiplication with respect to mode n. The U matrices contain the eigenvectors giving the principal directions in the respective
mode. Here U(trg) is the space of training instances, U(i j) the space
spanned by the spatial scales, U(k) the space spanned by the temporal scales, and U(uv) and U(w) are the spaces spanned by the spatial
and temporal translations respectively.
With this decomposition, the sources of variation and noise are
identiﬁed in each mode independently. The underlying behavior
class is then modeled by reducing the dimensions in each mode
separately as explained in Sec. 4.4.

r

s

(10)

n

This minimization problem can be recast as the linear system:
Ax = b
where

Al,m = ∑ ∑ ∑ ∑(Ml pqrs × Mmpqrs )

(12)

bl = ∑ ∑ ∑ ∑(Ml pqrs × C pqrs )

(13)

p q

and

(11)

p q

r
r

s
s

The solution x to this equation is the optimum representation for the
test feature tensor C. The rows of U(trg) are the fundamental feature
vectors for that mode, and can be used to detect any abnormalities
in the test case by ﬁnding the minimum distance d of the test case
from the behavior class, as:
dn =

N

∑ (xl − U(trg) (n, l))2

(14)

l=1

4.3 Detecting Anomalies
The anomaly in a new STTV with respect to a behavior
class is detected and localized by reconstructing the STTV in
the reduced dimension multi-linear space describing that class,
and then measuring the difference with the original STTV.
Let S be the core tensor of this reduced multi-linear space,

where d = min(dn ). If this distance d is within a given tolerance
interval, the test instance is assumed to be normal as deﬁned by
that particular behavior class (with minor variations), else the test
instance is assumed to not at all exhibit that behavior pattern.
If the test instance belongs to the class (within acceptable tolerances), then the anomalies are computed as:

U(trg) , U(i j) , U(k) , U(k) , U(uv) , U(w) be the corresponding eigenvector matrices, and let x be the coefﬁcients for a test data point in

E = C−C

U(trg) .

the space spanned by
If C = γ[i j]k ([uv], w) , ∀[i j], k, [uv], w
is the 4-mode feature descriptor tensor for this test instance, then a
corresponding tensor C is reconstructed in the reduced dimension

6

(15)

This error tensor E is then converted to the STTV space and is
visualized in the user interface as anomalous behaviours in the test
data point.

4.4

Dimensionality Reduction

The accuracy of the anomaly detection algorithm is quantiﬁed by
its sensitivityb (MT P /(MT P + MFN )) and speciﬁcity (MT N /(MT N +
MFP )). Empirically, we ﬁnd that the sensitivity and speciﬁcity
depend monotonically on the number of dimensions retained, as
would be expected. If a large number of dimensions are retained
then the speciﬁcity increases but the sensitivity reduces, because of
over-ﬁtting. By contrast, if too many dimensions are removed then
the sensitivity increases while the speciﬁcity reduces because now
the model is too general.
However, because the concept of an anomaly is subjective, it is
hard to mathematically determine what the correct number of dimensions should be. For this purpose we provide the user with four
parameters to control the number of dimensions in each mode: (i)
Γi j for the spatial scale mode, (ii) Γk for the temporal scale mode,
(iii) Γuv for the spatial translation mode, and (iv) Γw for the temporal translation mode. A value 0 results in all dimensions being
removed, while a value of 1 results in no dimensions being reduced.
We found that reducing the U(trg) matrix to 5 dimensions gives
good results across all behavior classes. From our experiments
(Sec. 4.5, Sec. 6), we also noticed that the minimum number of dimensions required to perform accurate anomaly detections depends
upon the subjective complexity of the behavior class. More complex classes required more number of dimensions to be retained.
4.5 Algorithm Veriﬁcation
To quantify the correctness of our algorithms against known ground
truth, we simulated the trajectories for three behavior patterns of
increasing subjective complexity, and built synthetic STTVs from
them. We generated the training set by adding perturbations to the
base STTV in the form of random trajectories. The massb of the
perturbations varied from 1% to 50% of the total mass. Fig. 3(a)
shows the base STTVs for pattern 1. Pattern 3 is similar to the
STTV in Fig. 2, and pattern 2 has intermediate complexity.
We expect our algorithm to identify the perturbations as anomalies. In Fig. 3(b) we show the sensitivity and speciﬁcity curves
with respect to Γi j for the 20% noise case, with other modes ﬁxed.
From these curves we observe that as the complexity of the behavior class increases, the number of dimensions required to reach
the same level of speciﬁcity increases. The sensitivity and speciﬁcity curves with respect to the other modes exhibit similar trends.
Fig. 3(c) shows the average sensitivity and speciﬁcity curves (for all
patterns) with respect to the percentage of perturbation mass, keeping the number of dimensions ﬁxed. Here we see that the algorithm
is robustly capable of identifying the salient patterns in a behavior
class even with a large amount of variation in the training data set.
5

and to understand the spatio-temporal relationships of normal behavior patterns and anomalies, (iii) to be able to train the system to
detect new behavior patterns, (iv) to be able to update the behavior patterns detected by the system, (vi) to ﬁne tune the algorithms
to bring it inline with the user’s expectations and intuition, (vii) to
keep the interface simple yet functional in order to minimize the
training required to use the system. In the following sections we
describe the user interface which was designed to meet these requirements.
5.1 GUI Layout
The main window of the GUI is shown in Fig. 4. It consists of three
types of visualizations and provides controls through which the user
interacts with the analysis engine.
The Video Feed view displays the video sequence being analyzed
frame by frame, with the regions of anomaly overlaid on it, thereby
guiding the attention of the user to interesting or suspicious regions.
The user can load a video sequence and control the play back using
a Play, Fast-Forward, Rewind paradigm. The Current STTV view
displays the STTV of the current window of 32 × 15s duration. The
Anomaly STTV view displays the anomalies in the current STTV.
The anomalies are color coded according to their intensity as per the
Alert Code color bar, which also indicates the cumulative anomaly
level of the current STTV.
The Add To Training Database panel enables the user to train
the system by adding a new instance to an existing behavior class,
or initializing a new class. The Adjust Analysis Parameters panel
allows the user modify the parameters of the analysis module
(Sec. 4.4) and visualize the results. The Review Training Database
button brings up a dialog box that lets the user view the training
STTVs in a behavior class and delete an instance if it is unsatisfactory.
5.2 User Interaction
The typical user interaction loop is shown in Fig. 5 Also, see the

U SER I NTERFACE

Our system was developed to meet a need of campus security of
analysis of macro-behavior patterns for the purpose crowd management and for determining the optimal deployment of security personnel. It was important that the system leverage the users’ intuition
and experience for normal vs. abnormal behavior without subsuming, hampering or overwhelming their decision making capabilities.
The speciﬁc user requirements were: (i) to display the video feed
from the scene, overlaid with cues to possible locations of anomalies (direction-of-attention), (ii) to provide a spatio-temporal representation of the activity allowing the users develop an intuition for
b Mass

of

a

spatio-temporal

F F(x, y,t)dxdydt.
MT P : mass of the volume

volume

F

is

deﬁned

as

marked as true positive anomalies, MT N : mass
of the volume marked as true negative anomalies, MFP : mass of the volume
marked as false positive anomalies, MFN : mass of the volume marked as
false negative anomalies.

Figure 5: Typical user interaction loop

accompanying video for a demonstration of the GUIc . Typical user
interaction would be as follows: (i) the user loads a video sequence
and speciﬁes its date and time, (ii) the GUI displays the video frame
by frame in the Video Feed view, while the Current STTV view
shows the STTV spanning 32 × 15s time windows, with 120s steps,
c www.cse.ohio-state.edu\

˜singhsh\itr\gui.avi

7

1

1

0.9

0.9

0.8

0.8

pattern 1

sensitivity/specificity

0.7

0.7

0.6

0.6

pattern 2
0.5

0.5

0.4

0.4

pattern 3

0.3

0.3

0.2

0.2

0.1
0
0

(a) STTV for pattern 1

0.1
0.1

0.2

0.3

0.4

0.5

Γij

0.6

0.7

0.8

0.9

1

0

Sensitivity
Specficitity
5

10

15

20

25

30

35

40

45

50

Perturbation %

(b) Γi j sensitivity and speciﬁcity

(c) Perturbation sensitivity and speciﬁcity

Figure 3: Quantitative veriﬁcation of the algorithm . Fig. (a) shows one synthetic STTV. Fig. (b) shows the sensitivity (full lines) and speciﬁcity
(dashed lines) curves with respect to Γi j for patterns 1 (in blue), 2 (in red) and 3 (in green). Fig. (c) shows the average sensitivity and speciﬁcity
curves with respect to the percentage of perturbation mass.

and the Anomaly STTV view shows the anomaly with respect to
the day-and-time speciﬁc behavior class, (iii) the user can adjust
playback speed, pause, fast forward and rewind the video stream,
or step through the STTV windows, (iv) the user can select another
behavior class to analyze the current STTV against, (v) the user can
examine the views in more detail by zooming, panning and rotating
(for the volume views), (vi) the user can adjust the Γi j , Γk , Γuv , Γw
parameters for the current behavior class and reanalyze the STTV,
(vii) if the current STTV is a good example of an existing class, the
user can add it to the training database, (viii) if the STTV represents
a new behavior class, the user can initialize the new class with this
instance, and specify the day and time of the class occurence, (ix)
the user can review the STTVs in the training database for any class
and can remove an instance if it is no longer a suitable example for
that class.
6

R ESULTS

Our system was trained and tested on real surveillance video in a
busy area of a university campus of size ≈ 200m2 as shown in the
Video Feed display of Fig. 4. The spatio-temporal trajectories were
embedded in STTVs of size 64 × 64 × 32 corresponding to physical dimensions of 15m × 15m × (32 × 15s), built on a sliding timewindow basis (of 120s intervals).
6.1 User Experience
We polled the users of our system regarding the intuitiveness of
the STTV representation and the functionality of the system. Initially, most of the users found the STTV representation of spatiotemporal activity hard to understand, and had trouble relating a 3D
volume with something that they understood as a temporally changing 2D phenomenon. However, because of the parallel layout of
the Video Feed view and the corresponding STTV view, they soon
(< 5 video sequences) learnt to correlate the dynamic 2D activity
with the static spatio-temporal representation. The Current STTV
and Anomaly STTV views by themselves remained difﬁcult to understand, but, when coupled with the Video Feed view most users
could map activity in a region of the STTV to activity in the environment. They were also able to map their intuition for normal
vs. abnormal behavior to the STTV representation, and could readily distinguish STTVs of different behavior classes. After getting
acclimatized to the STTV representation, the users could train the
system for different behavior patterns with increasing ease.
It was observed that manually identifying the speciﬁc anomalies
in the STTV was a time-consuming and difﬁcult task. Also, the

8

Table 1: Behavior Patterns
Behavior
Class
early-morning
start-of-school-day
lecture-period
inter-lecture-period
after-hours
late-evening
weekday-night
weekend-day
weekend-night

Training
Insts.
15
11
12
17
11
10
14
10
11

Sens.
(%)
87
82
83
88
84
86
82
83
85

Spec.
(%)
80
81
84
83
82
81
79
85
83

σΓ
0.20
0.12
0.22
0.16
0.16
0.19
0.21
0.16
0.17

anomalies detected by the algorithm matched those identiﬁed manually with a high degree of conﬁdence (Sec. 6.2). This conﬁrmed
our initial belief that while users have an intuitive understanding
of when a certain behavior pattern is unexpected or abnormal, they
cannot pinpoint the exact cause of the abnormality with equal ease.
Therefore, an automatic system like ours that uses their understanding to aid them identify anomalies would be invaluable for crowd
behavior analysis.
The users found the adjustment of the Γ parameters fairly intuitive without having to understand their operation. As the result of
changing the parameters could be observed visually, the user could
simply use visual feedback to guide the selection of these parameters.
6.2 User Validation
We were able to identify and train the system for nine distinctive behavior classes, as shown in Table 1, where each class corresponds to
a distinct behavior pattern. Each user was allowed to independently
select the number of dimensions in each mode, and then subjectively validate the accuracy of the anomaly detection system. The
sensitivity and speciﬁcity were determined by polling the users for
their estimates and then averaging the results across the users. Fig. 6
and 7 show the results for two different cases.
Table 1 also shows the standard deviation σΓ in the Γ parameters
across the users. It can be seen that, despite the fact the users were
training the system to match their individual intuition, the variation
was relatively low.

Figure 4: The Graphical User Interface

7

C ONCLUSION

In this paper, we presented a user-guided visual system for surveillance applications to detect anomalies in crowd behavior patterns.
Our system was built on the concept of collaborative autonomy,
in which we leverage the domain expertise of users in an intuitive
manner, through a visual feedback loop, to identify abnormal behaviors and improve the performance of the system. The system in
turn directs the attention of the users to anomalous events occurring
in the environment. We proposed a wavelet based feature descriptor
that captured the multi-scale, location sensitive nature of behavior
patterns. The multi-linear decomposition method that was used to
learn the underlying model for each behavior class gives the user a
high degree of control on the operation of the analysis and anomaly
detection algorithms, and lets the user ﬁne-tune the system so that
it reﬂects his/her intuitive notion of anomalous behaviors. We presented a quantitative evaluation of our algorithm, and a qualitative
evaluation of the entire system from the end user’s point of view.
We also demonstrated how the system is an invaluable aid in a
modern surveillance setup. Currently we are working on making
this system completely real-time and deploying it across the entire
campus, in which security personnel can monitor and respond to
events as they happen. We also looking at methods to improve the
accuracy of the algorithms and providing more advanced analysis
and visualization capabilities (e.g. monitoring a speciﬁc group of
people over time and identifying their participation in anomalies,
etc.)
ACKNOWLEDGEMENTS
The authors wish to thank Prof. James W. Davis and Patrick
Maughn. This material is based upon work supported by the National Science Foundation under Grant No. 0428249.
R EFERENCES
[1] J. K. Aggarwal and Q. Cai. Human motion analysis: A review. Computer Vision and Image Understanding, 73(3):428–440, 1999.

[2] R. Baraniuk. Optimal Tree Approximation using Wavelets. SPIE
Technical Conference on Wavelet Applications in Signal Processing,
3813, July 1999.
[3] A. Bobick. Computers seeing action. British Machine Vision Conference, 1:13–22, 1996.
[4] M. Brand and V. Kettnaker. Discovery and segmentation of activities
in video. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22:844–851, Aug 2000.
[5] M. Brand, N. Oliver, and A. Pentland. Coupled hidden markov models
for complex action recognition. IEEE Conference on Computer Vision
and Pattern Recognition, 1996.
[6] F. Br´emond and G. Medioni. Scenario recognition in airborne video
imagery. DARPA Image Understanding Workshop, pages 211–216,
1998.
[7] H. Buxton. Advanced visual surveillance using bayesian networks.
IEEE Colloquium on Image Processing for Security Applications,
9(074):1–5, March 1997.
[8] H. Buxton and A. Mukerjee. Conceptualizing images. Image and
Vision Computing, 18(2):79, Jan 2000.
[9] M. Chen, R. Botchen, R. Hashim, and I. Thornton. Visual signatures
in video visualization. IEEE Transactions on Visualization and Computer Graphics, 12(5):1093–1100, 2006.
[10] F. Cupillard, F. Br´emond, and M. Thonnat. Tracking groups of people
for video surveillance. 2nd European Workshop on Advanced Videobased Surveillance Systems, Kingston, UK, pages 88–100, Sept 2001.
[11] F. Cupillard, F. Br´emond, and M. Thonnat. Group behavior recognition with multiple cameras. Sixth IEEE Workshop on Applications of
Computer Vision, pages 177–183, 2002.
[12] I. Daubechies. Ten lectures on wavelets. Society for Industrial and
Applied Mathematics, Philadelphia, PA, USA, 1992.
[13] J. Davis, A. Morison, and D. Woods. An adaptive focus-of-attention
model for video surveillance and monitoring. Machine Vision and
Applications, 2007.
[14] L. de Lathauwer, B. de Moor, and J. Vandewalle. A multilinear singular value decomposition. SIAM Journal of Matrix Analysis and Applications, 21(4):1253–1278, 2000.
[15] D. L. Donoho. Unconditional bases are optimal bases for data compression and for statistical estimation. Applied and Computational
Harmonic Analysis, 1(1):100–115, 1993.

9

(a) Test STTV

(b) Reconstruction of test STTV in
class HOSVD space

(c) Anomalies in test STTV

Figure 6: Results for an inter-lecture-period test case, showing a high density of spatio-temporal trajectories along all walk-ways. Fig. (a)
Test STTV with the anomalies manually highlighted Fig. (b) shows the reconstruction of the test-case in the reduced dimension multi-linear space
spanned by the behavior class Fig. (c) shows the anomalies identiﬁed by the analysis module

(a) Test STTV

(b) Reconstruction of test STTV in class
HOSVD space

(c) Anomalies in test STTV

Figure 7: Results for an after-hours test case, showing scattered trajectories along the walk-ways. Fig. (a) Test STTV with the anomalies
manually highlighted Fig. (b) shows the reconstruction of the test-case in the reduced dimension multi-linear space spanned by the behavior
class Fig. (c) shows the anomalies identiﬁed by the analysis module

[16] K. Fukunaga. Introduction to Statistical Pattern Recognition. Elsevier
Science, Massachusetts, USA, 2 edition, 1990.
[17] D. M. Gavrila. The visual analysis of human movement: A survey.
Computer Vision and Image Understanding, 73(1):82–98, 1999.
[18] S. Hongeng, F. Br´emond, and R. Nevatia. Bayesian framework for
video surveillance application. 15th International Conference on Pattern Recognition, 1:164–170, 2000.
[19] S. Hongeng, F. Br´emond, and R. Nevatia. Representation and optimal
recognition of human activities. IEEE Conference on Computer Vision
and Pattern Recognition, pages 818–825, 2000.
[20] B. Lucas and T. Kanade. An iterative image registration technique
with an application to stereo vision. International Journal of Computers And Their Applications, pages 674–679, 1981.
[21] D. Makris and T. Ellis. Spatial and probabilistic modelling of pedestrian behaviour. British Machine Vision Conference, 2:557–566, Sept
2002.
[22] D. Makris and T. Ellis. Automatic learning of an activity-based semantic scene model. IEEE Conference on Advanced Video and Signal
Based Surveillance, pages 183–188, 2003.
[23] S. Mallat. Multiresolution approximation and wavelet orthonormal
bases of l2 . Transactions of the American Mathematical Society,
315:69–87, July 1989.
[24] S. Mallat and S. Zhong. Characterization of signals from multiscale
edges. IEEE Transactions on Pattern Analysis and Machine Intelli-

10

gence, 14(7):710–732, July 1992.
[25] H.-H. Nagel. From image sequences towards conceptual descriptions.
Image Vision Computing, 6(2):59–74, 1988.
[26] W. Niu and J. Long. Human activity detection and recognition for
video surveillance. IEEE Multimedia and Expo Conference, 2004.
[27] N. Oliver. Towards Perceptual Intelligence: Statistical Modeling
of Human Individual and Interactive Behaviors. PhD thesis, Massachusetts Institute of Technology (MIT), Media Lab, Cambridge,
MA, 2000.
[28] N. Oliver, B. Rosario, and A. Pentland. Statistical modeling of human
interactions. IEEE CVPR Workshop on the Interpretation of Visual
Motion, 1998.
[29] M. A. O. Vasilescu and D. Terzopoulos. Multilinear subspace analysis for image ensembles. IEEE Conference on Computer Vision and
Pattern Recognition, 2:93–99, June 2003.

