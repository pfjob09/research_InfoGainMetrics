Working Memory Load as a Novel Tool for Evaluating Visual Analytics
Courtney C. Dornburg *

Laura E. Matzen

†

Travis L. Bauer

‡

Laura A. McNamara

§

Sandia National Laboratories

ABSTRACT
The current visual analytics literature highlights design and
evaluation processes that are highly variable and situation
dependent, which raises at least two broad challenges. First, lack
of a standardized evaluation criterion leads to costly re-designs for
each task and specific user community. Second, this inadequacy
in criterion validation raises significant uncertainty regarding
visualization outputs and their related decisions, which may be
especially troubling in high consequence environments like those
of the Intelligence Community. As an attempt to standardize the
"apples and oranges" of the extant situation, we propose the
creation of standardized evaluation tools using general principles
of human cognition. Theoretically, visual analytics enables the
user to see information in a way that should attenuate the user's
memory load and increase the user's task-available cognitive
resources. By using general cognitive abilities like available
working memory resources as our dependent measures, we
propose to develop standardized evaluative capabilities that can be
generalized across contexts, tasks, and user communities.
KEYWORDS: Visual analytics, evaluation, cognitive load.
INDEX TERMS: H.5.2 [Information interfaces and presentation
(e.g., HCI)]: User Interfaces—Evaluation/Methodology
1

INTRODUCTION

To be effective, software for visual analytics must do several
things well. It must be able to consume non-graphical
information, transform that non-graphical information in one or
more ways, display that information to the user, and be responsive
to the user’s interactions with the data. The field of visual
analytics is not focused on a particular domain or type of data.
Experts in this field have split the kinds of things that users need
to do with visualization technology into a set of “Knowledge
Tasks” that include exposing uncertainty, concretizing
relationships, formulating cause and effect, determining domain
parameters, multivariate explanation, testing hypothesis,
answering previously unforeseen questions, looking at data from
different perspectives, and discovering patterns [1].
A wide variety of user communities can benefit from
visualization technology that supports these knowledge tasks. In
particular, visual analytics tools stand to benefit to the Intelligence
Community (IC). Intelligence analysts are faced with massive
amounts of inconsistent, unreliable, and heterogeneous data.
They often work under time pressure, with poorly specified tasks,
_____________________________________
* Email: ccdornb@sandia.gov
† Email: lematze@sandia.gov
‡ Email: tlbauer sandia.gov
§ Email: lamcnam@sandia.gov
IEEE Symposium on Visual Analytics Science and Technology
October 12 - 13, Atlantic City, New Jersey, USA
978-1-4244-5283-5/09/$25.00 ©2009 IEEE

and receive little or no feedback. All of these factors place a high
burden on the analysts’ cognitive resources. Effective
visualization tools should help to alleviate this burden, enabling
analysts to generate better analyses from more data, in less time.
While many visual analytics tools have been developed for the
IC, only a few of those tools have been formally validated. Given
the demands that intelligence analysis and other complex analytic
tasks place on cognitive resources, we propose developing a
measure of cognitive workload that can be used as a standardized
evaluation metric for visual analytics tools.
2

WHY IS EVALUATION DIFFICULT?

Formal validation studies are necessary to demonstrate the
effectiveness of visualization tools, yet conducting these studies is
difficult for a variety of reasons.
First, there is little ground truth available for validation. By
design, visualization tools aim to improve users’ ability to make
sense of complex data. Yet collecting historical user effectiveness
data requires accessing and understanding complex data sets that
are often sensitive and proprietary. Additionally, some data sets
lack ground truth by their very nature. For example, in the IC,
threat analysis accuracy may be indeterminate if the threat is
never realized.
Secondly, each analysis problem and data set includes many
variables which must be controlled for valid experimental testing.
Even when data sets are controlled, comparison across tools is
still difficult if the tasks involved in using the tools are different.
Because of these problems, a large variety of methodologies
have been developed to study visualization technologies. While
many researchers have noted the need for standardized evaluation
metrics, no standard has emerged. Unified evaluation techniques,
applicable across tasks and data sets, would be useful in at least
two respects. First, more unified validation techniques would
attenuate uncertainty in information visualization outputs and
related decisions. Better transparency in these results is especially
important in high consequence environments like the IC. A
second advantage that more unified evaluation would confer is
that of more cost-effective design. As discussed by Plaisant et al.
(2008), current usability studies and controlled experiment
methodologies related to information visualization evaluation are
“helpful but take significant time and resources,” [5] and
moreover, do not generalize across conditions and contexts
leading to costly re-designs for each project and specific user
community.
3

COGNITIVE LOAD EVALUATION

Intelligence analysis involves many challenges that relate to
human cognition. Those challenges include manipulation and
comparison of information, remembering relevant information,
discrimination of threat from non-threat, and avoiding
confirmation bias. Effective visual analytics tools should help to
reduce the cognitive demands stemming from finding and
manipulating the data, freeing up the analyst’s cognitive resources
for making sense of the information.

217

Because the mechanisms of human cognitive processing can be
viewed (somewhat) universally, and because one commonality
across high-level data analysis is cognitive demand, we propose
that measuring the cognitive processing demands associated with
interacting with a visualization tool might offer an evaluation
methodology that could be validated and generalized across the IC
and potentially across other work contexts. This approach can
provide a basis for quantitative comparisons of visualization tools
while simultaneously addressing the larger questions of how the
tool affects job performance.
Cognitive load is defined as “the amount of cognitive resources
needed to perform a given task” [2]. As argued by Huang et al.
(2008), typical visualization performance measures, which
compare performance differences in response time and accuracy,
don’t adequately capture the amount of mental effort that might be
required to compensate for a poor visualization tool. Thus, while
performance measures might be equivocal across two
visualizations, users may have to expend greater mental effort to
compensate for a bad visualization.
Cognitive load measures have been used to test performance in
a variety of domains. In general, there are four types of cognitive
load measures: primary task measures, such as measures of a
person’s speed and accuracy when completing their main task;
secondary task measures, which measure performance on a
concurrent task; physiological measures, where physiological
markers of stress or effort are recorded; and subjective measures,
such as questionnaires. After an extensive literature review, we
have found that there is some precedent for using measures of
cognitive load to assess software designed for the IC. However, to
our knowledge, the prior uses of cognitive load measures in this
domain have been limited to subjective questionnaires [3, 6].
We are developing secondary task measures of cognitive load
that are based on working memory theory. Working memory is
the “theoretical construct that has come to be used in cognitive
psychology to refer to the system or mechanism underlying the
maintenance of task-relevant information during the performance
of a cognitive task” [7]. While considered a central construct
essential to complex cognition, working memory can be defined
and evaluated in several different ways. For current purposes, we
are interested in working memory as it relates to the difficulty of
sustaining attentional resources during a visual analytics task.
Whereas Huang et al., have suggested that visualization
effectiveness relates to how well a person can maintain
concentration when working on a complicated task, we suggest an
alternative view whereby the need to maintain concentration, or
working memory resources, might be negatively related to the
usability of the visualization tools. In other words, the former
viewpoint is driven by measuring the distractive nature of the
visualization and the latter by the amount of resources required to
perform the task. Both approaches are similar in that they espouse
the importance of measuring general-purpose processing
resources.
In either case, working memory measures seem to map well
onto what North (2006) has called for in terms of a new
evaluation method to measure visualization “insight” [4]. North
describes insight as a process that is complex, deep, qualitative,
unexpected, and gives relevance to the data by connecting it to
existing domain knowledge. If an analyst is devoting more
cognitive resources to navigating a difficult interface, he or she
will have fewer resources available for analyzing and
understanding the data. This could decrease the likelihood of
gaining insight into a visualized data set and increase the analyst’s
chances of making errors or missing important information.

218

We believe that metrics based on working memory will address
the core issue of the effectiveness of visual analytics tools and
will have reliability and validity across temporal, contextual, and
user-related variances. In addition, by using a secondary task to
assess working memory load, we will create a general tool that
can be used to evaluate any type of visual analytics software
without requiring the costly and time-consuming development of
application-specific evaluation metrics.
4

CONCLUSION

Through an extensive literature review of evaluation methods,
we have concluded that although information visualization
technologies might be extremely useful in fields such as
intelligence analysis, where analysts must make sense of massive
and complex data sets, fully evaluating the impact of such
visualization tools remains difficult. This difficulty is due to a
variety of factors, including a lack of ground truth, myriad
dependent variables, and vast differences between different
analytical tasks, different data sets, and even between the methods
of different analysts completing the same task.
Although the differences across analysis tasks present a
daunting challenge for developing a broadly-applicable metric for
evaluating visualization tools, one commonality across analysis
tasks is that they are cognitively demanding and impose a high
load on working memory. Additionally, the cognitive resources
of human users are finite, and when more of these resources are
consumed by interactions with software tools, fewer resources are
available for gaining insight and making sense of the data. Based
on these commonalities, we propose developing a metric for
software evaluation that uses secondary task measures of working
memory load. An effective tool should demand few of the user’s
cognitive resources, enabling the user to perform high-level
analysis and sensemaking tasks more effectively. We think this
novel approach to evaluating visualization software will allow for:
1) effective comparisons across different users, data sets, and
analysis tasks and 2) informative evaluations of visualization tools
without the need to develop costly tool- and application-specific
evaluation metrics and design.
REFERENCES
[1]

[2]

[3]

[4]
[5]

[6]

[7]

Bauer, T., Dornburg, C., and Matzen, L. Human factors literature
review. Talk presented at the Networks Grand Challenge Seminar
Series, Sandia National Laboratories, Albuquerque, NM, 2008.
Huang, W., Eades, P., and Hong, S. Beyond time and error: A
cognitive approach to the evaluation of graph drawings. In Proc.
2008 Conference on Beyond Time and Errors: Novel Evaluation
Methods for Information Visualization, 2008.
Morse, E., Steves, M.P., and Scholtz, J. Metrics and methodologies
for evaluating technologies for intelligence analysts. In Proc.
Conference on Intelligence Analysis, 2005.
North, C. Toward measuring visualization insight. IEEE Computer
Graphics and Applications 26, 3 (2006), 6-9.
Plaisant, C., Fekete, J., and Grinstein, G. Promoting insight-based
evaluation of visualizations: From contest to benchmark repository.
IEEE Transactions on Visualization and Computer Graphics 14, 1
(2008), 120-134.
Scholtz, J., Morse, E., Steves, M.P. Evaluation metrics and
methodologies for user-centered evaluation of intelligent systems.
Interacting with Computers 18 (2006), 1186-1214.
Shah, P. and Miyake, A. Models of working memory: An
introduction. In A. Miyake and P. Shah (eds.). Models of Working
Memory: Mechanisms of Active Maintenance and Executive Control,
Cambridge University Press, Cambridge, UK, 1999, pp. 1-27.

