Guiding Feature Subset Selection with an Interactive Visualization
Thorsten May∗

Andreas Bannach†

James Davey‡

Tobias Ruppert§

¨ Kohlhammer¶
Jorn

Fraunhofer Institute for Computer Graphics Research, Darmstadt, Germany

A BSTRACT
We propose a method for the semi-automated refinement of the results of feature subset selection algorithms. Feature subset selection is a preliminary step in data analysis which identifies the most
useful subset of features (columns) in a data table. So-called filter techniques use statistical ranking measures for the correlation of
features. Usually a measure is applied to all entities (rows) of a data
table. However, the differing contributions of subsets of data entities are masked by statistical aggregation. Feature and entity subset
selection are, thus, highly interdependent. Due to the difficulty in
visualizing a high-dimensional data table, most feature subset selection algorithms are applied as a black box at the outset of an
analysis. Our visualization technique, SmartStripes, allows users to
step into the feature subset selection process. It enables the investigation of dependencies and interdependencies between different
feature and entity subsets. A user may even choose to control the
iterations manually, taking into account the ranking measures, the
contributions of different entity subsets, as well as the semantics of
the features.
Index Terms: G.3 [Mathematics of Computing]: Probability and
Statistics—Contingency Table Analysis; H.5.2 [Information Interfaces and Presentation]: User Interfaces—Graphical User Interfaces; I.5.2 [Pattern Recognition]: Design Methodology—Feature
Evaluation and Selection
1

I NTRODUCTION

Bethany is preparing for the analysis of a data table. We will call the
columns of the table features and the rows entities. Bethany’s data
table contains data collected about patients in a clinic and has more
than 100 features. Her task is to classify the entities (i.e. patients)
with respect to a chosen target feature. In her case, the target feature
is an indicator feature for a particular medical condition with the
values positive or negative.
Due to the statistical dependencies between the target feature and
the other features in the data table, we can say that the other features
contain information about the target feature. Classification makes
use of this information to produce a predictor for the values of the
target feature. So Bethany’s goal is to find a function or rule which
takes the other features (e.g. temperature, blood pressure, weight,
height, etc.) as input and outputs a prediction for the target feature
- i.e. positive or negative.
It makes no sense for Bethany to use all of the available features
for the classification. On the one hand, she must include the features which contain as much information about the target feature
as possible (relevance). On the other hand, she must ensure that
the included features show as little redundancy as possible. When
∗ e-mail:

thorsten.may@igd.fraunhofer.de

† e-mail:andreas.bannach@igd.fraunhofer.de
‡ e-mail:james.davey@igd.fraunhofer.de
§ e-mail:tobias.ruppert@igd.fraunhofer.de
¶ e-mail:joern.kohlhammer@igd.fraunhofer.de

IEEE Symposium on Visual Analytics Science and Technology
October 23 - 28, Providence, RI, USA
978-1-4673-0014-8/11/$26.00 ©2011 IEEE

redundant features are included the weighting of these features is
effectively increased. This could lead to good features for the classification being masked by less good features.
Bethany needs to choose a subset of the available features which
is optimized with respect to the above criteria. Due to the sheer
number of features, she knows that she will have to resort to using
automated methods to make the choice. Without automated methods she might have to test all of the more than 2100 feature subsets
to find the best candidate. Feature subset selection algorithms will
help her overcome this problem. The subclass of algorithms she
considers is known as filters. These iterative algorithms consist of
heuristics, which make use of quality measures to construct candidate subsets.
The search for useful candidate feature subsets is dependent on
the entity subset to which the algorithms are applied. Practically all
the automated methods Bethany can draw on carry out their optimization of the feature subset on a fixed, predefined entity subset.
This is usually the whole data table. Statistical correlations which
only affect subsets of the entities can contribute in very different
ways to the results of feature subset selection algorithms.
The contributing factors to the medical condition being analysed
by Bethany need to be evaluated differently for men and women.
Without some form of guidance, the feature subset selection algorithm would determine which features are most useful for classification in general. If two different models for prediction are required
(one for men and one for women) then it would make more sense
to execute the feature subset selection algorithm separately for each
subset. This would ensure a better separation of the prediction models in the classification step of the analysis. For this and other trivial
distinctions Bethany can easily preselect entity subsets. However,
if Bethany were to consider correlations which are not self evident
then an apportionment of the data into subsets is generally not possible before feature subset selection. If Bethany was able to apportion the data based on the common information of more than one
feature then the classification could be improved. The risk of generating a prediction model which simply represents a compromise
between several very different entity subsets would be reduced.
For better control over feature subset selection in the future,
Bethany wishes to visualize those measures of feature redundance
and relevance which are used by her chosen algorithm. In particular, she wants to interactively monitor changes in the feature subsets during iterative generation with the help of automated methods.
This would allow her to guide the algorithm.
1.1

Overview

We illustrate our basic idea with a simple example (see Figure 1).
The scatter plots show features A1 and At (the target feature) of a
data table. The analyst is interested in a dependency of At on A1 .
Figure 1(a) shows the bivariate distribution of A1 and At . For small
values, A1 would in fact be sufficient to create a predictor for At .
The problem is, that A1 might be only one of hundreds or even
thousands of potential candidate features. In this case, visual inspection of all features is not feasible. Hence, automatic methods
are used for the ranking of candidates (see Section 2). Most automatic methods, however, use all entities of a data table to estimate
the usefulness of candidate features for prediction. In this case, the
strong local dependency of At on A1 might go unnoticed: Instead,

111

correlation measures for interactively selectable and adjustable subsets of data entities. The contribution of each subset to the quality
measures is visualized separately. Thus, the construction of feature
subsets can be analyzed with a particular focus on entity subsets.
SmartStripes provides users with the ability to interrupt the filter
algorithms at any time. In addition, the user can execute individual
iterations manually and examine the quality measures for each iteration. Features can be interactively selected or excluded in each
step. A manual initialization, in which the user selects the first features before continuing with the algorithm is also possible.
In addition to the tight coupling of automated and interactive
feature selection, we have defined the following requirements for
our software program:
• The visualization must provide an overview over a large, in
principal unlimited, number of features in a data table
• It must be scalable with respect to the number of features and
the number of entities.
• It must be applicable to a variety of filter algorithms. A prerequisite here is the ability to break down quality measures
with respect to any given partition of the table into entity subsets.
Figure 1: These scatter plots illustrate our basic idea for finding candidate features A1 to build a predictor for At . Most automatic feature
selection methods compute dependency measures based on all entities in the data table. Thus, local features can be masked by the
global distribution, even if they would make for a good local predictor
(a). To monitor automatic feature selection, we use a partition of the
data and we show the local details of the global quality measures
(b+c+d). (Example modified from [12])

weaker candidates than A1 may be selected for the feature subset.
We propose a visual-interactive method to display the quality
ranking measures computed with existing automatic techniques.
The measures are decomposed to different subsets of the dataset.
Figure 1(b) shows a useful partition of A1 . We compute and show
the contribution of every subset to the quality ranking measure (see
Section 4.3). The local dependency in subsets A11 and A12 would
be highlighted. Figure 1(c) shows a less suitable partition of A1 .
We have to note, that the partition has a great influence on the quality of the results. In Section 4.2 we show how the partitions can be
computed and how they can be modified interactively.
In principle, the detailed inspection of the quality measures can
be used with any partition of entities. Hence, we allow other features than A1 or At to be used for the decomposition. Figure 1(d)
shows a partition using a third feature, which can be freely chosen.
In Section 5 we show, how the visualization can be used for the
interactive initialization, monitoring and steering of the automatic
feature subset selection methods.
1.2

Contribution

In this paper we introduce a software program called SmartStripes,
which couples automated data analysis with interactive visualization for the purpose of feature subset selection. In particular, SmartStripes was developed for use with filter algorithms. These are supported by visualizing the quality measures which are used for the
assessment of feature subsets. The quality measures used are normally measures of statistical correlation.
Measures of correlation are commonly displayed with the help of
a correlation matrix [5]. Correlation matrices display one value for
each pair of features – i.e. (N 2 − N)/2 values for N features – in a
matrix display. In contrast to correlation matrices, we calculate the

112

• Experts must be able to bring their knowledge of the data set
into the analysis process.
In the next section we will provide a technical overview of feature subset selection. We will then summarize the academic work
related to the visual support of feature subset selection in Section
3. We will provide a detailed description of the SmartStripes approach in Section 4. This will be followed by a description of the
SmartStripes work flow in Section 5. Finally, we will conclude our
paper and give an outlook for future work in Sections 7 and 8.
2

OVERVIEW

OF

F EATURE S UBSET S ELECTION

Our contribution is a technique for the evaluation and guidance of
feature subset selection for the purposes of data mining. It is therefore based on existing automated processing methods. In the following overview we will first classify the different algorithms. Secondly, we will characterize those classes of algorithms which can
be used together with SmartStripes. Thirdly, we will illustrate the
points at which we enable an interactive intervention in the feature
subset selection process.
Automated methods for feature subset selection are often used
for the exploratory analysis of data tables with a large number of
features. The problem can be formulated as the search for an appropriate feature subset, where the search space contains all possible subsets of features (i.e. 2N − 1 subsets for N features). Even
after cleaning the data, the number of possible feature subsets can
still be so large that a manual search cannot be carried out systematically. Guyon and Elisseeff [7] describe the general problem as
a search for a minimal subset of features, which together are most
useful for the following steps of the analysis. In their opinion, one
cannot expect to find a single method which is the best method for
all cases.
Considering their goal, feature subset selection methods are very
similar to dimension reduction methods. They differ, in that no new
summarizing features are constructed in the data table. Dimension
reduction methods are generally not applicable in all situations. Almost all methods of this class require that the base set of features
is numerical (PCA, ICA, SOM, etc.) or that a distance function is
defined for non-numerical features (MDS and variants; see Fodor
[4] for a survey on these methods). We assume the more general
case, in which the relationships between nominal features are also
considered.

The quality of a feature subset can be measured in a number
of different ways. Kohavi and John [11] differentiate between socalled wrapper methods and filter methods.
Wrapper methods consider the results of data mining models
which are constructed using the chosen features. In this case the
quality of a generated predictor may be carried over to measure the
quality of a selected feature subset. These methods involve a high
computational effort, because a new predictor must be constructed
for each test of a candidate feature subset.
Filter methods use statistical measures of quality which can be
applied before choosing any particular data mining algorithm. The
quality measures provide a generalised description of the level of
dependency between the candidate features and a target feature.
One can distinguish two cases; the features are evaluated independently of one another (feature ranking) or the measure takes a set
of features as input (feature set selection). In the first case, the subset containing the features which were ranked the highest is simply
chosen (max-relevance method). In the second case, a heuristic is
used in order to improve the feature subset based on an evaluation
of its contents. These methods require more computational effort,
however they have the advantage that they also take the dependencies inside the feature subset into account. Thus, redundant features
can be eliminated from the selection.
Our method allows the examination and interactive modification of exactly these heuristics, in which a chosen feature subset
is changed by adding or removing a feature in each step of the algorithm (see Section 5). More complex heuristics, such as those
used in genetic algorithms [21] are not directly supported by our
method.
Without consideration of the heuristics, filter methods can be distinguished with the help of their quality measures. One of the most
common measures for numerical data is Pearson’s correlation coefficient (see, for example Sirkin [19]), which describes the level
of linear dependence between two features. Our approach does not
make use of numerical coefficients because these cannot be applied
to all - in particular not to nominal - features. For nominal features
only those quality measures can be used for which the statistical
distributions of values are estimated and compared. An example of
such a measure is the mutual information measure [7]. This measure is given by the following formula:
n

IMI (A1 , A2 ) =

m

∑ ∑ p(A1i , A2 j )log

j=1 i=1

p(A1i , A2 j )
p(A1i )p(A2 j )

(1)

:=K(A1i ,A2 j )

IMI denotes a quality measure, describing the mutual dependence of the two features A1 and A2 . The subsets A1i and A2 j for
i = 1, . . . , m and j = 1, . . . , n represent the partitions of A1 and A2
respectively (compare with Figure 1). The cardinality of a given
subset relative to the size of a dataset is given by p, which is used
as an estimator of the discrete density function.
Brown [2] introduces a system by which the quality measures
of different automated methods can be compared and classified.
Brown’s general formulation of an important class of feature subset selection methods is of particular relevance for our work. The
formula for the quality measure Q of feature As , in which t denotes
the index of the target feature, is as follows:
s−1

Q(As ) = IMI (As , At ) − β
Relevance QRel

s−1

∑ IMI (As , Ai ) + γ ∑ IMI (As , Ai |At )

i=1

Redundance QRed

(2)

i=1

Conditionality QCond

According to this formulation, most methods only differ in the
way that the redundance measure between an evaluated feature As

and the already selected features Ai is calculated. The difference is
encapsulated in the weights β and γ. Also important is the fact that
the formula for the relevance IMI can also be used for the redundance.
Molina et al. [15] describe, in addition to entropy measures,
a number of other possible quality measures. Almost all of these
quality measures are sum or integral formulae whose summands
each refer to particular intervals or value subsets of the features
considered. This is an important factor for the SmartStripes technique. In many cases the sums can be reordered so that the quality
measures are broken down to the level of the summands as shown
below. These summands each refer to particular entity subsets. In
this way the influence of different partitions of the data table can be
compared.
Kriegel et al. [12] suggest a systematic approach for the categorization of methods for the clustering of high-dimensional data.
They identify local feature relevance as one of four problems which
these methods face. This means that different clusters are manifested in different feature subsets. Even though this problem was
formulated for clustering methods, it is also relevant for the selection of features for the purposes of classification.
In conclusion, we have identified three points at which we can
connect our visual interactive approach with automated filter methods.
1. Breaking down and visualizing the contributions of different
partitions of the data table with respect to the quality measure.
Restriction of the feature subset selection algorithm (and the
subsequent classification) to particular entity subsets of the
data table (see Section 4.3).
2. Selection of quality measures from a pool of measures and
sorting of the features with respect to the chosen measure (see
Section 4.4).
3. Switching between automated and interactive heuristics
(mixed initiative). The heuristic can be stopped manually to
add or remove features. An initialization of the algorithm is
also possible in this way (see Section 5).
The requirement that numerical, ordinal or nominal data should
be handled in the same way restricts, on the one hand, the types of
methods which can be used with SmartStripes. On the other hand,
one is forced to accept a certain information loss, due to the fact that
numerical features need to be discretized. Because all further calculations depend on this discretization, it makes no sense to choose
a simple discretization a priori. Neither is it wise to allow the discretization of possibly hundreds of features by hand. We therefore
initialize the discretization automatically. In addition, we provide
users with the ability to refine this discretization freely (see Sections
4.1 and 4.2).
3

R ELATED W ORK

In the previous section we presented a categorization of the automated methods for feature subset selection. In this section the
state of the art with respect to visual analytics is presented. Our
work is compared to other approaches which combine automated
and graphical interactive methods for the selection of features.
The idea of splitting up quality measures for a detailed inspection has been presented by May et al. in [14]. However, the idea
was illustrated with single quality measure only and it did not cover
feature interdependency. One contribution of our approach we use
the generic formula 2 to apply this technique for a broader class
of feature subset selection methods. In addition, our improved approach covers the mixed initiative steering of the selection heuristics, which is necessary whenever feature interdependency is considered.

113

Guo [6] argues that human intervention is necessary to evaluate
and guide the procedure of feature subset selection. To our knowledge, the visualizations most commonly used for interactive feature
selection are correlation matrices. As described in Section 1, correlation matrices visualize statistics for all correlations between pairs
of features in a given table. The relationships between feature pairs
are condensed to a single value. Friendly [5] presents an in-depth
exploration of the design of correlation matrices.
Visual support of the task of feature subset selection must display
information on a coarse level of detail. Highly detailed views are
not suitable for feature subset selection, because they do not scale
well with the number of features. Hence, many approaches implement an overview & detail strategy, where the overview component
is used as a feature selection method. Two examples for using a correlation matrix as an overview are given by MacEachren et al. [13]
and Ingram et al. [8]. Each presents a framework which includes
an interactive feature selection step in its respective analytical process. Ingram et al. focus on the analysis of data sets with numerical
features. They use the correlation matrix as a means to supervise
filter and dimension reduction operations on the basis of Pearson’s
correlation coefficient. MacEachren et al. use the correlation matrix as a feature selection tool to steer other methods directly. They
use the maximum conditional entropy as a measure of correlation.
Like other measures that apply to the distribution of values rather
than the values themselves, entropy measures have the advantage
of being applicable to nominal, ordinal and discretized numerical
features.
Feature sorting is a major concern in a number of visual approaches to feature selection. Guo [6] enhances the usability value
of the correlation matrix by proposing a sorting scheme for the features. This scheme involves sorting the features with respect to
their similarity. Our sorting scheme uses the ranking measures derived from the automatic methods instead of mutual similarity. This
makes more sense for our goal of reduced redundancy in the feature
subset.
Approaches which do not use a correlation matrix as an overview
component have been presented by Elmqvist et al. [3] and Yang
et al. [22]. Elmqvist et al. use an ordered scatterplot matrix as
an overview for the selection of features and to navigate the space
of all 2D axis-aligned scatterplot projections. Scatterplot matrices
provide a detailed overview of the data table, but they lose this advantage as the number of features increase. Yang et al. propose a
visualization technique, which combines a detailed display of the
values of features and the relations between different features. The
detailed display shows glyphs which represent univariate or bivariate feature values. The similarity relation is represented by the layout of the glyphs on the canvas. In terms of the number of dimensions that can be displayed and compared, it is one of the most scalable techniques. However, using the similarity between different
features or other numerical measures requires appropriate features.
Yang et al. [23] present an approach for semi-automatic feature selection. Like our approach it is derived from an automatic
method, which can be interactively controlled in order to identify a
meaningful and useful feature subset. It computes a agglomerative
hierarchical clustering of the features. Their approach, however, relies on the definition of similarity measures for all pairs of features.
This requires numerical features to be commensurable. Without
preparation it can not be applied to nominal or ordinal features.
Another example for the integration of automatic and interactive
heuristics are presented by Ankerst et al. [1]. Their goal is the optimization of a decision tree. Similar to our approach, their heuristic
combines the optimization of feature partitions (i.e. split-points of
the tree) and the search for relevant features. The difference is, that
our approach does not prescribe the analytical model for the predictor.
Johansson et al. [9] present an approach tackling with a problem

114

related to ours. Competing structures in a dataset are emphasized
or masked depending on the quality metrics chosen. To alleviate
this problem, a mixture of different statistics may be chosen to the
control the dimension reduction process. In a sense, this approach
is orthogonal to ours: Instead of treating quality metrics as degrees
of freedom, we use different entity subsets to emphasize local dependencies in the data.
Piringer et al. [17] present a concept which also allows the
comparison of candidate features based on statistical measures for
the uni- and bivariate value distribution. In contrast to most other
approaches, their measures are not only applied to the complete
dataset, but they are applied to any subset generated by brushing.
When considering a brushed subset as a binary partition of the entity set (selected vs. unselected entities), this approach is very similar to ours. While their approach allows more flexible definition
of partitions, our approach allows the comparison of more than two
partitions.
Mixed initiative approaches for Visual Analytics have also been
suggested for other parts of the analytical process. Kapoor et al.
[10] present a method to steer the classification process from its
end: By changing preferences to accept or avoid specific kinds of
errors, they provide a user-centered way to guide an automatic optimization of the classification model. Another user-centered approach is presented by Tan et al. [20]. They use visualization for
the interactive steering of the combination of classifier ensembles.
In summary, we feel that there is still a gap between detail and overview techniques in multidimensional visual analysis.
Overview techniques like correlation matrices provide a space efficient overview due to a high aggregation of values. Detail techniques like scatterplot matrices show the plain data values. While
correlation matrices provide a space-efficient overview, we feel that
they limit the analyst’s options for problem diagnosis and interactive refinement. Scatterplot matrices do not scale well with the
number of features. SmartStripes is settled between the coarse
overview provided by a correlation matrix and the details visible
in scatterplot matrices.
4

T HE S MART S TRIPES A PPROACH

In this section we will describe the components of the SmartStripes
approach in detail. Due to the close coupling of data processing and
visualization components, a basic understanding of the background
processes is necessary for the user to interact with the visualization
effectively. Thus, we will begin our presentation with a description of how we partition the features. This will be followed by a
description of the feature partition view, in which the partitions of
features can be manipulated by the user. In the third subsection we
will describe the decomposition of measures for relevance and redundance in order to increase the granularity of the overview. This
will be followed by a description of the dependency view, in which
the measures can be visualized and interactively explored. Finally,
we will describe how the two views are linked to empower the user
in guiding the process of feature selection. A screen shot of the
SmartStripes user interface can be seen in Figure 2.
4.1

Partitioning of Features

We begin by emphasising the distinction between the values of a
feature and the entities in the data table. When we talk of the value
range of a feature we mean the values that can occur in the feature
column. The entities are simply the rows of the data table. A small
subset of the value range can correspond to a large entity subset and
vice versa. Put in another way, many entities can contain the same
value for a particular feature and there may be valid values which do
not occur in any entities in the data table. The value ranges considered in this paper consist of simple data types, such as floating point
numbers, integers and strings. The same concepts can, however, be
applied to more complex data types.

4
1
5

2
3

Figure 2: Overview of the SmartStripes interface. (1) The feature partition view. (2) The dependency view. (3) Value subset labels. (4) Feature
labels and feature relevance visualized in the form of a bar chart. The target feature label is black, the hue and saturation of the other labels is
determined by their quality.

Since they will play a major role in the following discussion,
we include a general definition of partitions, followed by an elaboration for numerical and nominal value ranges. Given a set As ,
a partition of As is a series of sets As1 , As2 , . . . , Asm , such that
each Asi is a subset of As , no two sets contain the same elements
(Asi ∩ As j is empty if i = j) and the union of all sets make As
(As1 ∪ As2 ∪ . . . ∪ Asm = As ). For numerical value ranges the sets
of the partition are intervals. In this case each set continues where
its predecessor ends. Manual modifications of the partition involve
simply shifting the boundaries of the intervals or unifying pairs of
neighbouring intervals. For nominal value ranges the sets of the
partition do not have a natural ordering. Manual modifications in
this case involve moving values from one set to another, unifying
pairs of sets and changing their display order.
As described in Section 2 numerical features have to be discretized to make them comparable with nominal features. Since
we use histograms to represent the value ranges we begin by binning the numerical values. For this step we make use of the method
described by Shimazaki and Shinomoto [18], which produces a binning optimized for histogram displays. In general, this initial binning is not suitable for the comparison of features using statistical
methods. We therefore cluster the bins using the k-medoids algorithm in a second preprocessing step. We chose the k-mediods algorithm because of its simplicity and its tolerance of outliers.
Automated preprocessing is necessary, especially in cases where
a large number of features has to be prepared for analysis. We cannot expect the initial partition to be optimal in all cases. Thus, to
remain flexible, we allow for interactive modification of the partitions in the feature partition view. We assume that more advanced
methods for the automatic creation of useful partitions - like, for example, clustering or brushing - can be integrated into our approach.
We will take this up in future work (see Section 8).

4.2

Feature Partition View

The feature partition view (see Figure 2(1)) visualizes the value
range of each feature in the form of a vertical histogram. The bars
of the histograms represent bins (in the case of numerical features)
or individual values (in the case of nominal features). The length
of the bars represents the number of entities in the corresponding
entity subset. In the case of nominal features, the bars can be reordered interactively by drag-and-drop.
The left (base) axis of each histogram is made up of a series
of buttons. The buttons represent the clusters described above (in
the case of numerical features) or interactively defined groups of
values. By dragging the top or bottom edges of the buttons, cluster
or group boundaries can be modified and whole clusters or groups
can be added or removed. Clicking on a button will add or remove
the corresponding entities from the current analysis. By default, all
entities are visible and, thus, all the buttons are activated.
4.3

Decomposition of sums

In this section, we show how the quality measure (see Equation 2)
of a specific feature As can be decomposed according to the partitions given. The quality measures, introduced in Section 2, only
describe the relationship between features with respect to all entities in the data table. The most important component of the visualization is therefore the detailed representation of the quality measures regarding the partition of the features. The idea is to separate
the corresponding sum formulae in an appropriate way into components and to represent their contributions to the overall quality
individually.
Every sub sum represents a subset of entities. To break the quality measure down with respect to different subsets we use the partitions which are defined in the feature partition view. The partitions
of different features are almost always different. The entity subsets
of each feature partition most often strongly differ regarding other

115

4.4

Figure 3: The decomposition of the sums is distributed among the
cells of the column.

features. Thus, the selection of appropriate feature partitions for
defining the sub sums plays an important role. When we combine
the equations 1 and 2 we can show which part of the quality measure can be decomposed by which feature: The relevance term QRel
consists of two nested sums and the nesting can be exchanged:
m

QRel (As ) =

n

n

∑ ∑ K(Asi , At j )

j=1 i=1

Decomposition by At

=

m

∑ ∑ K(Asi , At j )

(3)

i=1 j=1

Decomposition by As

The redundance term QRed consists of three nested sums, but only
the feature As does appear in all terms.
o

QRed (As ) = −β

n

m

m

o

n

∑ ∑ ∑ K(Asi , Ak j ) = ∑ −β ∑ ∑ K(Asi , Ak j )

k=1 j=1 i=1

i=1

k=1 j=1

Decomposition by As

(4)
Since we can not expect, that a common partition exist for all features Ak , only the feature As may be used for the decomposition of
the redundancy part. For the same reason, the conditional mutual
information QCond can be decomposed by the partitions of As or At .
Now we are able to define, which part of the quality measure can
be applied to which decomposition. The user will be able to select
between two decomposition strategies:
1. The partition of the reference feature Ar is used for all features. The advantage of this strategy is that the subsets of the
partition can be compared across all features. The drawback
is, that it can be applied to QRel (see Figure 4(a)) and QCond
(see Figure 4(b)) only. In particular it can not be applied to
the overall measure Q(As ).
2. The quality measure for every feature As is partitioned with its
own partition (see Figure 4(c)). The advantage of this strategy
is that it can be applied to the overall measure. The drawback
is, that the partitions of different features can not be compared
directly, because they may be unrelated.
Due to the decomposition we retain a number of values in addition to the overall quality measure for every feature As . Two
features may have identical overall quality measures; their detailed
measures defined by the partial sums can be different. In some
cases a single partial sum contains the only significant share of the
overall measure. However, strong dependencies which span only a
part of the dataset may be interesting for classification. Local patterns, which might be worth a detailed inspection are displayed in
the dependency view.

116

Dependency View

The dependency view (see Figure 2(2)) is the visualization central
to our approach. This visualization shows the quality measures,
which were calculated with respect to the chosen feature ranking or
feature subset selection algorithm. We use a matrix-based layout,
in which each column represents a feature. In the first decomposition strategy (see previous subsection), the number and size of the
cells in every column is determined by the reference feature and
its partition (see Figure 3). In the second decomposition strategy,
the number and size of the cells is determined by the feature represented by the column. The height of a cell is proportional to the
size of its entity subset; this is also an indication of its influence in
the overall quality measure. Aside from the decomposition strategy, the user may also choose to inspect a specific component QRel ,
QRed or QCond , if suitable.
The color saturation of a cell is defined by the normed value of
the partial sum. Low values are shown in white, high dependency
values are shown in dark blue. The visualization also shows the
overall quality measure as a bar-chart on top of the columns (see
Figure 2(4)). The quality measure is determined by the feature subset selection method chosen. The height of the bars is normed to
the range between the minimum and maximum values. The result
is not skewed by norming in this way, since the absolute value of
the quality measure is not relevant for the heuristic.
The reference feature used for the calculation of the statistics is
initialized with the target feature. While the target feature always
remains the same, the reference feature can be changed during analysis (by right clicking on the respective column). This is useful to
check for interdependencies between potential candidates which are
not necessarily affected by the target feature. In the next section, we
will describe in detail how we support the investigation strategy.
The dependency view also includes tooltip-visualizations, which
show details on the quality measures of entity subsets (see Figure
2(5)). Since the quality measure is basically a comparison of two
value distributions, we show these distribution with two bar-charts.
One bar-chart shows the overall distribution of the feature values
(light grey), while the second bar-chart shows the distribution of
the values in the regarded entity subset only (dark grey). Because
the conditional component QCond is defined by three features, we
display the distributions as a scatterplot, with the corresponding entity subset marked red (see Figure 4(c))
The result of the selection process is a subset of the set of features. We establish a cooperation of human and machine to create
this set. In our proposed mixed initiative, the human may select features by clicking on the corresponding columns. She may also filter
specific entity subsets by clicking in the corresponding columns.
Any selection or removal causes an automatic recalculation of the
statistics and an update of the quality measures for every feature
and every cell. Because of the interdependencies of the features, all
quality metrics may change after selection. Hence, in order to find
the best features for the next iteration, the feature may need to be
reordered by their quality measure. The sorting is triggered by the
user, to leave the chance to inspect the data with different quality
metrics.
We consider sorting to be the part of the technique, which makes
the visualization scalable in terms of the number of features. The
default view covers the left boundary of the virtual screen space
and the features will be sorted by their quality from left to right.
The features, which are likely to be irrelevant are placed to the right
side and will be shown by default. However, those features and
their measures can be investigated by scrolling.
In addition, the dependency view is connected to the feature partition view in both directions. On the one hand changing the partition of a feature in the feature partition view, causes a recalculation of the statistics and the current quality measure. Splitting and
joining entity subset has an immediate effect. On the other hand,

the histograms can be colored by selecting a cell in the matrix and
its corresponding subset. The coloring can be used to find suitable splitting points for manual a partition. Therefore, we calculate
the correlation between the value distribution of all entities and the
value distribution within the subset.
5

W ORKING

WITH

S MART S TRIPES

In order to work with SmartStripes the user has to define the dependent feature to be used as the target feature for the classification. The dependent feature (or target feature) is the most important
reference feature throughout the entire feature subset selection and
most calculations will refer to it. In the following we will first describe a straightforward work flow, in which the reference feature is
not changed. Secondly, we will describe how we deal with changing the reference feature for the inspection of interdependencies.
SmartStripes can be used in two basic modes: An automated
heuristic mode and a mixed initiative mode. In automated mode,
feature subset selection works ”as usual”: All iterations of the
heuristic are calculated in a single run. The process terminates
based on the maximum number of features or a threshold of the
quality measure. The selected features are highlighted. In this
mode, SmartStripes can be used to inspect and modify the final
result.
In a mixed initiative mode, the automated method is used to calculate the measures for one iteration at a time. After one iteration,
the calculation stops. The quality measures are displayed in the dependency view to suggest the next choice of a feature. The actual
choice, however, is made by the user. The user may consider the
overall quality measures, the name and semantics of the features
and the detailed quality measures regarding the entity subsets defined by the partition (see 5.1). The selection of a feature starts
the next iteration: The quality measures are recalculated and a new
feature is suggested for the next selection (see Figure 4(d)+(e)).
Each selected feature will be added or removed from the subset
of selected features depending on its previous state. That way it is
possible to combine heuristics with forward and backward propagation. In addition the two modes can be used interchangeably: It
is possible to initialize a feature subset in the mixed initiative mode
and switch to the automated heuristic after a few iterations. It is
also possible to use the automated heuristic as an initialization of a
mixed initiative backward propagation.
Because of this step-by-step approach, the mixed initiative mode
allows branching from the straightforward work flow. Sometimes,
it is interesting to inspect the interdependency between features
other than the target feature in greater detail (see Figure 4(f)). As
opposed to a correlation matrix, our technique displays a one-tomany relationship between features. This means the user must
change the reference feature (initially identical to the target feature) to check the details of the interdependency. This is especially
useful when evaluating features suggested by the automated feature subset selection. For example, the first feature is suggested by
the automated method for its large relevance. Before accepting this
suggestion it makes sense to check whether the new feature is relevant in its own right or whether it is dependant on two or more
features, which represents even more fundamental information.
Because the interdependencies between features may form a
complete graph, searching this graph is not linear. Apart from connecting our technique to a view of such a graph - which is beyond
the focus of this paper - the arrangement of the feature columns can
help to organize the search. Firstly, sorting by the quality measures
is automatic, but it is never triggered automatically. Often, the user
needs to keep track of the changes while focussing on specific features. Secondly, the sorting is done separately for three groups of
features. The first ”group” consists of the dependent feature only.
This feature is always displayed at the leftmost column and acts as
an anchor-point to get back to the ”main-branch” of analysis. Next

to the right we arrange the features, which are currently selected for
the resulting subset in descending order. The rest of the features are
arranged on the right-end side.
In summary the work flow for the feature selection consists of
a main branch, which is pursued whenever the target feature is the
reference feature and a number of side branches, which are pursued
for the detailed evaluation of potential candidate features for selection or removal. Usually the user re-enters the main branch, whenever the subset has been changed. The quality measures shown in
the visualization can be changed at any time.
5.1

Reading the Dependency View

At its core, our approach juxtaposes information detailing entity
subsets and information describing whole features. In general, the
dependency view shows where (automatic) feature subset selection
will be sensitive to the selection of entities used for subsequent analysis or vice versa. Wherever the dependency measures are unevenly
distributed along a feature column, the exclusion of an entity subset
or a different data sample is likely to change results.
Some visual artifacts stand out and may indicate a need for a detailed inspection. The best candidate features are displayed as an
unbroken dark blue column, indicating that the candidate may be
useful for the predictor of the complete dataset. However, there may
be different degrees of dependency along the same column. In the
reference feature based views, every row corresponds to the same
subset of the reference feature. Sometimes, rows can be clearly distinguished as a darker area spanning multiple columns (see Figures
4(a) and 2(2)). Their corresponding entity subsets display an unusual distribution in a large number of features. Depending on the
strength of the relationship, it makes sense to investigate whether
this subset can be predicted with a separate classification model.
Subsets, which can be distinguished as dark-blue or white rows
can also be used for an optimization of the partition of the reference
feature. The best partition would minimize the number of subsets
while retaining most of the information in the dataset. By merging
subsets, the resulting subset often becomes more similar to the overall distribution, resulting in a weaker dependency measure. Thus
subsets indicating a strong dependency may be merged until their
corresponding rows become white. Likewise, subsets which appear
white may simply be too big to expose any features distinguishing
it from the overall distribution. The user may split these subsets to
see if the resulting partition reveals more details.
In the candidate feature based view every column is partitioned
like its feature. Usually no rows can be identified in this view,
because every feature has its own partition. However, this view
allows to spot those feature/entity subset combinations which are
most suitable for prediction. Instead of focussing on candidate features that display a ”solid” dependency on average, it is necessary
to trade them off with features that display a high dependency yet only in a specific entity subset. Because most quality measures
for automatic selection methods move the ”solid-on-average” features into focus, we implemented an additional measure which uses
the weighted maximum measure per column to compare features
(weighted by the relative size of its entity subset). This decreases
risk of overlooking potentially interesting local features.
One caveat is a tendency to over interpret the color-contrasts
within different parts of a column. The corresponding subsets do
not necessarily represent the best partitions of an optimal model of
the data: Subsets (i.e. patterns) that manifest in more than one feature are usually not represented accurately by univariate partitions.
However, our technique relies on the fact that it is not necessary to
use optimal partitions in order to expose dependencies between features. Except in the cases where the features are truly independent,
a less-then-optimal partition can be compensated, if the number of
entities is sufficiently high.

117

6

E VALUATION

We used real-world datasets like the US public micro census for a
first evaluation. Many of the features in this household dataset are
redundant. Documented relationships are used as ”ground-truth”
data. Strong global dependencies can easily be detected with our
method and with automated methods. Also, a number of local dependencies which are easily detectable in the visualization could be
verified in the documentation. Some of them, like the ”abnormal”
behaviour of disbanded households might appear trivial in retrospect. Features with a hybrid encoding - basically a numerical feature combined with nominal values like not available - also leave
a distinct trace in the visualization. Other dependencies, like a redundant encoding of some household characteristics like household
type and working experience would require more experience with
the data. We believe that an exposition of quality measures may
keep the analyst alert to avoid potential pitfalls.
In the following we were interested in the possibility of detecting multivariate dependencies. Our technique calculates statistics
for bi- and trivariate data; in this sense it can be compared to a projection to low-dimensional space, which most often results in a loss
of information. In a preliminary study, we created artificial local
dependencies by choosing a random subset of data. One to five features are chosen randomly to create clusters or manifolds in their
subspace. The rest of the entities and features contained noisy data.
Test persons were required to identify the dependent subspaces in
the dataset. In this study, the dependencies were easy to spot with
our visualization. However, this may be due to the fact, that plain
noise simply is not sufficient to mask the artificial dependencies.
In a follow-up study we modified subsets of real-world data in
the same way. We expected that real-world data is a more efficient
distractor, which in fact proved to be true. Naturally, the attention is drawn to the strongest - most often bivariate - relations. In
some cases these dependencies were clearly associated with an entity subset of the data. They were eliminated by removing this subset from the calculation. Thus, weaker dependencies were easier
to spot. However, users stated that they were not able to effectively
eliminate subsets, because of the limited flexibility of the univariate
feature partitions. Our lesson from this study is the need for a more
flexible definition of the subsets.
7

L IMITATIONS

An important limitation of virtually all techniques using statistical
measures is their instability when applied to a too small number
of samples; and SmartStripes is no exception. We have to note that
many freely available test data sets with less than a thousand entities
are unlikely to produce valid results. After bivariate or even trivariate decomposition of the test statistic, a single term may be based
on a few samples only. This is in fact one reason, why feature subset selection methods are rarely used with multivariate dependency
measures [16].
Because we require interactive frame rates, the effect of having
too many samples has to be considered, too. Fortunately, many
of the computations can be cached. The most extensive updates
are the change of the target feature and the modification of feature
partitions. Without any extensive database support, we are able to
process tables with about 4 million entries with interactive performance.
SmartStripes itself is very flexible, which can both be a chance
and a burden to the user. Feature partitions and reference feature
may be freely chosen at any time, but usually the user does not
consider all these information at once. Thus, the interactive control
of the automatic feature set optimization covers only the one aspect
of the parameter space for interaction. It may be possible that the
optimization of the feature partitions can be done in a similar semiautomatic way. This would narrow the degrees of freedom to a
more concise user interface.

118

8 C ONCLUSION & F UTURE W ORK
We presented SmartStripes, a visualization technique for the monitoring and control of automatic filter methods for feature subset
selection. The visualization displays the quality measures computed for every feature of a data table and broken down to a selected grouping of data entities in a matrix layout. The statistical
aggregations of the quality measures are decomposed to compare
the individual contribution of different data groups to the quality
measure.
While feature ranking measures are displayed in existing visualization techniques, SmartStripes also supports feature subset selection methods, in which the inclusion or exclusion of a single
feature influences the measures of all other features. The user is
able to step into the heuristic to interactively modify the automatic
optimization of the feature subset. The subset may be changed by
adding or removing features from the current configuration. Automatic and interactive selection can be used alternately. In addition,
the user may include or exclude specific data partitions depending
on the detailed inspection of the measures.
All features can be sorted according to their current quality rank
as a candidate. While by default the virtual screen displays only
the most important candidate features, the user is able to view the
whole table by scrolling. Hence, the number of features that can be
processed and displayed in a meaningful way is virtually infinite.
Compared to other visualizations, SmartStripes presents an intermediate level of aggregation. We propose a trade-off between the
high aggregation of a correlation matrix and detailed visualization
of actual data values.
SmartStripes is very strictly intended to be an preliminary
overview for analysis, which does not prefer or defer specific types
of data or specific types of relations (e.g. similarity). Hence, we deliberately restricted the technique to generic statistical dependency
measures. The task of finding specific patterns in the data has to
be covered by the following data-mining steps. SmartStripes minimizes the risk to miss an opportunity for the generation of a better
feature subset.
In the future we will extend SmartStripes to include synthetic
features. These features are the result of clustering and dimension
reduction methods or of data transformations. Because they combine information from multiple features, they potentially make up
for stronger feature partitions. However, this requires alternating
between the feature selection methods and data-mining methods,
which are, of course, interdependent. Both processes could be used
for an incremental refinement of the other and our technique can be
used to control this loop.

(
d)

(
a)

(
e)

(
b)

(
f
)

(
c
)
(
g)
Figure 4: The SmartStripes work flow. Each column is broken down with respect to the value range partition of (a) the target feature, (b) each
feature individually, and (c) a chosen reference feature.(d) Shows a first glance at the data. In (e) the data has been sorted by relevance and the
first feature chosen. (f) Shows the examination of the relevance and redundance with respect to a reference feature. In (g) A candidate feature
subset has been selected.

119

R EFERENCES
[1] M. Ankerst, M. Ester, and H.-P. Kriegel. Towards an effective cooperation of the user and the computer for classification. In Proceedings of
the 6th International Conference on Knowledge Discovery and Data
Mining, pages 179–188. ACM Press, 2000.
[2] G. Brown. A new perspective for information theoretic feature selection. In Proceedings of the 12th International Conference on Artificial
Intelligence and Statistics (AISTATS), volume 5, 2009.
[3] N. Elmqvist, P. Dragicevic, and J.-D. Fekete. Rolling the dice: Multidimensional visual exploration using scatterplot matrix navigation. In
In Proceedings of IEEE Transactions on Visualization and Computer
Graphics, pages 1141–1148, 2008.
[4] I. Fodor. A survey of dimension reduction techniques. Technical
Report UCRL-ID-148494, Lawrence Livermore National Laboratory,
Center for Applied Scientific Computing, 2002.
[5] M. Friendly. Corrgrams: Exploratory displays for correlation matrices. The American Statistician, 56:316–324, November 2002.
[6] D. Guo. Coordinating computational and visual approaches for interactive feature selection and multivariate clustering. Information Visualization, 2:232–246, 2003.
[7] I. Guyon and A. Elisseeff. An introduction to variable and feature
selection. Journal of Machine Learning Resarch, 3:1157–1182, 2003.
[8] S. Ingram, T. Munzner, V. Irvine, M. Tory, S. Bergner, and T. M¨oller.
Dimstiller: Workflows for dimensional analysis and reduction. In Proceedings of the 5th IEEE Conference on Visual Analytics in Science
and Technology (VAST), Florida, USA, October 2010. IEEE Computer
Society.
[9] S. Johansson and J. Johansson. Interactive dimensionality reduction
through user-defined combinations of quality metrics. IEEE Transactions on Visualization and Computer Graphics, 15:993–1000, November 2009.
[10] A. Kapoor, B. Lee, D. Tan, and E. Horvitz. Interactive optimization
for steering machine classification. In Proceedings of the 28th international conference on Human factors in computing systems, CHI ’10,
pages 1343–1352, New York, NY, USA, 2010. ACM.
[11] R. Kohavi and G. H. John. Wrappers for feature subset selection.
Artificial Intelligence, 97:273–324, 1997.
[12] H.-P. Kriegel, P. Kr¨oger, and A. Zimek. Clustering high-dimensional
data: A survey on subspace clustering, pattern-based clustering, and
correlation clustering. ACM Transactions on Knowledge Discovery
from Data, 3:1:1–1:58, March 2009.
[13] A. M. MacEachren, X. Dai, F. Hardisty, D. Guo, and E. Lengerich.
Exploring high-d spaces with multiform matrices and small multiples.
In Proceedings of the IEEE Symposium on Information Visualization
(InfoVis). IEEE Computer Society, 2003.
[14] T. May, J. Davey, and T. Ruppert. Smartstripes: Looking under the
hood of feature subset selection methods. In S. Miksch and G. Santucci, editors, Proceedings of the 2nd International Workshop on Visual Analytics (EuroVA 2011), pages 13 – 16, 2011.
[15] L. C. Molina, L. Belanche, and A. Nebot. Feature selection algorithms: a survey and experimental evaluation. In IEEE International
Conference on Data Mining (ICDM), pages 306 – 313, 2002.
[16] H. Peng, F. Long, and C. Ding. Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(8):1226 –1238, Aug. 2005.
[17] H. Piringer, W. Berger, and H. Hauser. Quantifying and comparing
features in high-dimensional datasets. In Proceedings of the 2008 12th
International Conference Information Visualisation, pages 240–245,
Washington, DC, USA, 2008. IEEE Computer Society.
[18] H. Shimazaki and S. Shinomoto. A method for selecting the bin size
of a time histogram. Neural Computation, 19(6):1503–1527, 2007.
[19] M. Sirkin. Statistics for the Social Sciences, 3rd Edition. Sage Publication Inc., Thousand Oaks, CA, 2006.
[20] J. Talbot, B. Lee, A. Kapoor, and D. Tan. Ensemblematrix: Interactive
visualization to support machine learning with multiple classifiers. In
ACM Human Factors in Computing Systems (CHI), 2009.
[21] J. Yang and V. Honavar. Feature subset selection using a genetic
algorithm. IEEE Intelligent Systems and their Applications, 13:44,

120

Mar/Apr 1998.
[22] J. Yang, D. Hubball, M. O. Ward, E. A. Rundensteiner, and W. Ribarsky. Value and relation display: Interactive visual exploration of
large data sets with hundreds of dimensions. IEEE Transactions on
Visualization and Computer Graphics, 13:494–507, 2007.
[23] J. Yang, M. O. Ward, and E. A. Rundensteiner. Visual hierarchical
dimension reduction for exploration of high dimensional datasets. In
Proceedings of the symposium on Data visualisation (VisSym), pages
19–28, Aire-la-Ville, Switzerland, Switzerland, 2003. Eurographics
Association.

