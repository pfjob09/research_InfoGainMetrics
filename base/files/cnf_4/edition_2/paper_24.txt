Interactive Volumetric Lighting Simulating Scattering and Shadowing
Timo Ropinski∗

†
¨
Christian Doring

Christof Rezk-Salama‡

University of Munster
¨

European Institue of Molecular Imaging (EIMI)

Mediadesign University of
Applied Sciences,
Dusseldorf
¨

Figure 1: A CT scan of a mouse (360 × 270 × 550 voxels) rendered with the proposed illumination model. The closeups on the right show, that
semi-transparent, diffuse as well as specular materials are captured realistically.

A BSTRACT
In this paper we present a volumetric lighting model, which simulates scattering as well as shadowing in order to generate high
quality volume renderings. By approximating light transport in inhomogeneous participating media, we are able to come up with an
efficient GPU implementation, in order to achieve the desired effects at interactive frame rates. Moreover, in many cases the frame
rates are even higher as those achieved with conventional gradientbased shading. To evaluate the impact of the proposed illumination model on the spatial comprehension of volumetric objects, we
have conducted a user study, in which the participants had to perform depth perception tasks. The results of this study show, that
depth perception is significantly improved when comparing our illumination model to conventional gradient-based volume shading.
Additionally, since our volumetric illumination model is not based
on gradient calculation, it is also less sensitive to noise and therefore also applicable to imaging modalities incorporating a higher
degree of noise, as for instance magnet resonance tomography or
3D ultrasound.
Index Terms: I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism—Color, shading, shadowing, and texture.
1

I NTRODUCTION

Today, volume rendering can be performed in real-time on consumer graphics hardware, thanks to a variety of acceleration techniques developed in the past. In practice, the shading in most
∗ e-mail:

ropinski@math.uni-muenster.de

† e-mail:christian.doering@uni-muenster.de
‡ e-mail:c.rezk-salama@mediadesign-fh.de

IEEE Pacific Visualisation Symposium 2010
2 - 5 March, Taipei, Taiwan
978-1-4244-6686-3/10/$26.00 ©2010 IEEE

cases is rather simple, i. e., gradient-based local Phong illumination,
which comes with two major drawbacks. First, since the Phong illumination model has been originally developed for surface lighting, it relies on a well-defined gradient as substitute for the surface
normal when applied to volume data. Second, except for the voxels used for the gradient estimation, neighborhood information is
not taken into account. This restricts the model to the simulation
of local illumination phenomena, although advanced illumination
models would significantly improve spatial comprehension [12].
While normalized gradients are usually a good approximation
for the normal vectors of isosurfaces in CT data, modalities with a
significantly lower signal-to-noise ratio will lead to severe degradation of the gradient quality. This is not only the case for more recently emerging modalities as 3D ultrasound (US) or positron emission tomography, but also for very well established modalities such
as magnet resonance tomography (MRT). While gradient filtering is
frequently applied when shading such modalities, its application is
limited due to the a loss of information and the performance penalty
induced by on the fly gradient estimation. It is thus desirable to develop a volumetric illumination model providing good perceptual
qualities without relying on gradient computation.
A gradient vector can be thought of as a very rough approximation of the vicinity of a voxel, where only 6 to 26 direct neighbors
are taken into account. Due to this small number of contributing
voxels, noise has a rather strong impact. Incorporating a larger
neighborhood around the current voxel would improve the estimate.
In this sense, the extreme case would be to consider the entire data
set, which would pave the way for the integration of global illumination effects. The approach of incorporating a larger neighborhood into the shading computation is not new and has been previously exploited by other shading models [11, 20, 23]. In this paper,
we follow this idea and present a physically plausible volumetric
illumination model with a flexibility which meets the perceptual
requirements of the user. Figure 1 shows the application of the proposed volumetric illumination model to a CT scan of a mouse. As

169

it can be seen, semi-transparent as well as diffuse and specular materials are rendered realistically.
The contributions of this paper are as follows. In order to generate a realistic representation of participating media for the human
observer, the integration of spatially varying scattering effects is
supported. Since interactive high-quality volume rendering is desired, we aim at an integration of these effects into GPU-based volume ray-casting. A perceptional problem of scattering effects for
visualization is the vanishing of hard shadow borders, which are
important for the spatial comprehension of a scene [26]. In our
model the shadow computation is decoupled from the scattering,
which allows substantially harder shadow borders. Many previous
solutions to volumetric scattering lead to unrealistic appearance of
surfaces within the volume data. Bone structures in CT often look
like marble when scattering is applied. Our model allows us to
easily control the amount of surface-like reflection and volumetric scattering. While the approximative nature of our model leads
to physically plausible results, the solution to the light transport
equation cannot be considered as physically correct. Therefore, we
have conducted a user study to assess the perceptual qualities of
our volumetric illumination model. The obtained results indicate,
that our model significantly improves depth perception, i. e., leading to faster and less error prone depth decisions when compared
to gradient-based Phong illumination. On top of that, our performance measurements show that in many cases even an increase in
performance can be achieved compared to gradient-based lighting.
To the best of our knowledge, the developed illumination model
is the first one, which meets all the described requirements at the
same time while preserving full interactivity and providing good
perceptual qualities.
In the remainder of this paper, we first discuss related work in
Section 2, before deriving the interactive volumetric illumination
model supporting scattering as well as shadowing in Section 3. The
technical realization allowing interactive frame rates is described in
the following two sections, whereby Section 4 explains the lighting
computation and Section 5 includes details about the actual rendering. Section 6 demonstrates some applications of the developed
illumination model and discusses the results of the conducted user
study, before the paper concludes in Section 7.
2

R ELATED W ORK

In his seminal paper on optical models for volume rendering,
Max [16] emphasizes the importance of global illumination effects.
Accurate solutions of the physical equations of light scattering in
participating media, however, are still too expensive for real-time
applications.
Originally developed for surface lighting, ambient occlusion [29]
and obscurance shading [17] have become popular in volume rendering. These techniques have been applied successfully to enhance the perception of spatial relationships for isosurface rendering [1, 24, 18]. Several techniques exist to calculate ambient occlusion for direct volume rendering. Hernell et al. [7, 8] and Ljung et
al. [14] propose local ambient occlusion as a fast way to calculate
soft shadows for volumetric data with varying transfer functions.
Ropinski et al. [20] calculate dynamic updates for both ambient occlusion and scattering effects by utilizing local histograms. Schott
et al. [23] propose a different, but also inexpensive implementation
which restricts ambient occlusion computation to a user-specified
cone angle by assuming the light position coincides with the camera position (head light). The vicinity occlusion map proposed by
Diaz et al. [9] is an adoption of screen-space ambient occlusion for
direct volume rendering. A significant drawback of this postprocessing technique is its dependency on a valid depth buffer, which
restrict itself to rather opaque renditions. The obscurance shading
technique proposed by Ruiz et al. [21] is a generalization of ambient occlusion for illustrative purposes instead of soft shadows.

170

Since ambient occlusion techniques simulate the appearance of soft
shadows assuming ambient light only, they cannot account for external light sources.
To incorporate external light sources, different approaches have
been proposed. A texture-slicing techniques which allows for shadows caused by attenuation of one distant light source has been proposed by Behrens and Ratering [2], who superimpose a separate
shadow volume on top of the scalar data set. While our approach
is similar in spirit, it also supports scattering and can be applied to
point light sources without posing constraints on the light position.
This allows an interactive changing of the light position, for instance in order to achieve a top-left lighting with supports resolving
convex-concave ambiguities and is therefore a preference in illustrations [3, 25]. Zhang et al. [27, 28] have proposed methods to
incorporate shadows into sheet-based splatting techniques, which
are less efficient compared to texture slicing or GPU-based volume ray-casting. Desgranges et al. [4] propose a gradient-free technique for shading and shadowing which produces visually pleasing
shadowing effects. In the strict sense, however, it is accurate only
for a head light, but in this case no shadows would be visible at
all. For GPU-based volume raycasting, Ropinski et al. [19] give an
overview of different possibilities to incorporate shadows from external light sources. The most prominent implementation is shown
by Hadwiger et al. [6] who demonstrate a GPU-based implementation of deep shadow maps with real-time capabilities.
In comparison to shadowing algorithms, much less work has
been done in order to support scattering. Dobashi et al. [5] have
proposed a hardware-accelerated rendering technique to account for
atmospheric scattering effects in homogeneous participating media. To account for scattering of light in volume data, Kniss et
al. [10, 11] have proposed a half-angle slicing technique which allows textured slices to be rendered from both light and viewing direction simultaneously. They account for scattering effects in slicebased volume rendering by sampling the incident light from multiple directions while updating the light’s attenuation map. Similar to
our implementation, scattering is restricted to a user-specified cone
which represents a special type of phase function. We emphasize
the importance of this publiaction, however, while the results show
convincing light transport effects, their implementation is inconsistent in the strict sense: It accounts for light scattered around the
dominant direction of incident light, but neglects scattering of outgoing light on its way towards the eye, which can be incorporated
in our model. Furthermore, the technique can be only applied to
slice-based volume rendering. Rezk-Salama [22] has proposed a
fast Monte-Carlo-based algorithm for volumetric rendering, however, with scattering events restricted to a limited set of isosurfaces.
To these ends we propose a flexible optical model which incorporates scattering, shadowing and specular reflections, and we
demonstrate its efficient implementation based on GPU-based volume ray-casting. Our technique is flexible in the sense that it may
be used to model scattering more physically accurate compared to
previous work, but also allows us to ease the restrictions of physical
laws by decoupling shadow calculation from scattering.
3 VOLUMETRIC I LLUMINATION M ODEL
Scattering and shadowing have a great influence on the quality of
volume rendered images. The radiance Ls (x, ωo ) which is scattered
from position x inside the volume in direction ωo can be defined as:
Ls (x, ωo ) = s(x, ωi , ωo ) · Li (x, ωi ) + Le (x),
where Li (x, ωi ) is the incident radiance reaching point x from direction ωi , Le (x) is the emissive radiance and s(x, ωi , ωo ) describes
the actual shading, which is dependent on both the incident light direction ωi as well as the outgoing light direction ωo . Furthermore,
s(x, ωi , ωo ) is dependent on parameters, which may vary based on
x, as for instance the optical properties assigned through the transfer

function. In the context of volume rendering, s(x, ωi , ωo ) is often
written as:

where Ω is the unit sphere centered around x. In our model,
p(x, ωi , ωo ) is a strongly forward peaked phase function:

s(x, ωi , ωo ) = τ(x) · p(x, ωi , ωo ),
where τ(x) represents the extinction coefficient at position x and
p(x, ωi , ωo ) is the phase function describing the scattering characteristics of the participating medium at the position x. When considering shadowing in this model, where the attenuation of external
light traveling through the volume is incorporated, the incident radiance Li (x, ωi ) can be defined as:
Li (x, ωi ) = Ll · T (xl , x).
This definition is based on the standard volume rendering integral, where the light source is located at xl and has the radiance Ll
and T (xl , x) is the corresponding transparency between xl and x.
According to Max, the standard volume rendering integral simulating absorption and emission can be extended with these definitions to support single scattering and shadowing:
L(x, ωo ) = L0 · T (x0 , x)
x

+
x0

(Le (x) + (s(x , ωi , ωo ) · Li (x , ωi ))) · T (x , x)dx ,

with ωi = xl − x , L0 being the background intensity and x0 a
point behind the volume. When extending this equation to support multiple scattering, it is necessary to integrate the scattering
of light coming from all possible directions ωi on the unit sphere.
However, Max has stated that this would be overkill for most scientific visualization applications [16]. Similar to Kniss et al. [11] we
incorporate indirect lighting by blurring the incoming light within
a given cone centered about the incoming light direction. In contrast to their technique, our approach can be easily extended, to also
account for scattering that happens when the light travels towards
the eye. To achieve this, we use an additional scattering pass on
the light volume, which blurs the outgoing light within a cone facing away from the viewing direction. Another important difference
compared to [11] is that we use separate blurring for the chromaticity and the intensity of the light (luminance). Although such an
approach is not physically correct, the decoupling of shadow computation from color bleeding provides us with additional flexibility
to optimize the perception of spatial structures, which is known to
be improved, when hard shadow borders are present [26], since humans are very sensitive in perceiving small changes in luminance,
but not in hue [13]. To guarantee hard shadow borders, we omit the
blurring of the intensity part and just apply standard linear interpolation. Thus, our illumination model can be specified as follows:
x

L(x, ωo ) = L0 · T (x0 , x) +

x0

(Le (x) + q(x, ωi , ωo )) · T (x , x)dx ,

(1)
where the radiance q(x, ωi , ωo ) is calculated as the transport
color t(x), as assigned through the transfer function, multiplied by
the chromaticity c(x, ωo ) of the in-scattered light and modulated by
the attenuated luminance li (x, ωi ):
q(x, ωi , ωo ) = t(x) · c(x, ωo ) · li (x, ωi ).
The transport color t(x) as well as the chromaticity c(x, ωo ) are
of course wave length dependent, while the scalar li (x, ωi ) describes
the incident (achromatic) luminance. Multiplying all together will
result in the incident radiance. Chromaticity c(x, ωo ) is computed
from the in-scattered light
τ(x) · p(x, ωi , ωo ) · c(x, ωi )d ωi ,

c(x, ωo ) =
Ω

p(x, ωi , ωo ) =

C · (ωi · ωo )β
0

if ωi · ωo < θ (x)
otherwise.

Similar to the work of Kniss et al. [11], the cone angle θ is used to
control the amount of scattering and depends on the scalar data set
at position x. It is higher for less opaque voxels and can be maximal
π
4 . The phase function is a Phong lobe whose extent is controlled
by the exponent β , restricted to the cone angle θ . The constant C
must be chosen with respect to β to ensure energy conservation. If
the gradient magnitude at x is high with respect to a user-specified
threshold, we have a well-defined gradient and may as well compute specular reflections. This allows us to account for both surface
like reflection and volumetric scattering.
4 L IGHT VOLUME C ALCULATION
To efficiently approximate the volumetric illumination model described in the previous section, we propose an algorithm, which
exploits the features of current GPUs. While in standard DVR,
the final color is obtained by incorporating emission and absorption along a viewing ray, we exploit an alternative concept in order
to independently compute the scattering and the shadowing contribution. When considering just shadowing, a straight forward extension to a GPU-based volume ray-caster already requires a substantial overhead during rendering. It would require at least 1 shadow
ray per sample. Since the number of samples directly influences
the image quality it is often rather high. When for instance considering a given image resolution of 10242 and a volumetric data
set having a resolution of 5123 voxels, due to the Nyquist theorem
O((512 · 2) · 10242 = 10243 = nsr ) shadow rays would be required.
When taking into account, that each of these nsr shadow rays needs
to be sampled sufficiently high, it becomes clear that this shadowing
approach results in a high performance penalty. When additionally
simulating scattering by computing different ways of light through
the medium, it would become even more expensive. Therefore, we
have decided to realize the derived volumetric illumination model
not in image space, where a high number of sampling operations is
necessary, but directly in volume space.
To compute the chromaticity c(x, ωo ), we process the volume in
front-to-back order from the light position and blur the chromaticity
of the incident light within a cone centered around the light direction, with the tip of the cone pointing away from the light source,
while we directly accumulate the opacity in order to compute the luminance li (x, ωi ). To also simulate the scattering, that occurs when
light travels towards the eye, we can process the light volume again
in back-to-front order with respect to the viewing position and blur
the outgoing light within a cone centered around the viewing direction, with the tip pointing towards the eye position. By integrating
this additional pass, pitch black shadows can be avoided in some
areas resulting in a more vivid image.
Thus, the additional performance impact becomes independent
of the sampling rate, and shadowing as well as scattering can be realized with only O(v · a) sampling operations, where v is the number of voxels and a the size of the desired neighborhood used for
blurring the indirect achromatic light contribution. In the following subsections, we will explain our voxel space realization of the
described volumetric illumination model.
4.1 Light Propagation
The proposed realization is based on an illumination volume. Alternatively, one could think about using a deep shadow map approach [15]. However, while deep shadow maps would allow a
similar shadow impression and do not need to store a volumetric
data set, they do not allow an easy integration of scattering and

171

Figure 2: Light is propagated through the volume slice by slice starting at the cube face F0 being closest to the light source located in
xl .

have the drawback, that they incorporate a data set dependent user
defined error value. In contrast our technique does not need any
additional user assigned parameters.
Our voxel space light propagation model is similar in spirit to the
shadowing approach presented by Behrens and Ratering [2]. However, in contrast their approach is constrained to simulate shadows
with one distant light source hitting the volume at an angle of 45 ◦ .
Our technique is not confined in this way and therefore allows us
to capture shadowing of arbitrary directional or external point light
sources as well as scattering effects, leading to more realistic rendering results. In this subsection, we will explain our technique for
an arbitrary point light source.
In order to exploit the features of current GPUs, we approximate
the described model by processing the volumetric data set slice by
slice. Thus, we compute an illumination volume, which is updated
on the fly whenever the transfer function and/or the light position
xl has been changed. Since the shadowing as well as the scattering effects at each position x depend only on those voxels lying in
between x an xl , it is sufficient to process the volumetric data set
starting near xl and proceeding in a direction facing away from the
light source to compute c(x, ωo ) and li (x, ωi ). We start at the cube
face F0 being closest to the light source and propagate illumination information along one of the major axes of the volume, i. e.,
x, y or z. As illustrated in Figure 2, the used axis is determined by
the principle processing direction dirmax . Determining dirmax can
be thought of choosing the Fj of all cube faces F, which has the
largest projection as seen from xl . Since, we want to propagate the
lighting information from xl through the volume, not all Fj come
into account, but only those which are visible from xl and thus fulfill the equation nFj · (xl − cFj ) < 0, where nFj is the normal and cFj
is the center of Fj . Based on this observation, we sort the three cube
faces being visible from xl , such that F0 minimizes nFj · (xl − cFj )
(see Figure 2). Thus F0 is the cube face with the largest and F2 is
the cube face with the smallest projection as seen from xl . Since nF0
is the normal of F0 , the principle light propagation direction dirmax
can be set to dirmax = −nF0 .
Now, that we know in which direction we have to propagate the
light information through the volume, we can start at the slice S0
which is closest to xl along the axis dirmax . During the light propagation we step through all slices Si until we get to the last slice Smax .
This propagation is performed on the fly by exploiting framebuffer
objects and render-to-texture functionality. Since rendering into a
3D texture only allows to render along the z−direction, we have to
apply an axis permutation during the light propagation, in order to
support arbitrary light positions xl . However, for explanatory rea-

172

sons, we first describe the simple case, where dirmax is along the
positive or negative z-axis, before briefly describing the permutation, which is used to extend the technique also for dirmax pointing
along the x- or y-axis.
To exploit render-to-texture functionality along the z-axis, we
render each slice from S0 to Smax with texture coordinates reaching from (0.0, 0.0, zi ) to (1.0, 1.0, zi ), where zi is the z coordinate of
i
. For each slice except S0 , we need to access
slice Si with zi = max
the previously computed slice. Thus, during rendering each fragment corresponds to one voxel in the original data set and we can
exploit a GLSL fragment shader to compute the lighting information as given by Equation 1. This is done based on the given cone
angle described in Section 3. To compute both the scattering color
as well as the shadowing intensity we apply back to front compositing to Si and Si−1 , as known from standard DVR. When assuming
that the current voxel’s color and opacity are stored as voxel.rgb
resp. voxel.a, the scattering color c(xi , ω) as well as the attenuated
intensity li (xi , ω) at xi can be computed in the shader as follows:
c(xi , ω) = (1.0 − voxel.a) ∗ c(xi−1 , ω) + voxel.a ∗ voxel.rgb,
li (xi , ω) = (1.0 − voxel.a) ∗ li (xi−1 , ω) + voxel.a,
whereby c(xi−1 , ω) and li (xi−1 , ω) are read around the intersection pi of the current voxel’s light vector dirl = xl − xi with the
previous slice. Since in this step, we want to simulate the light
traveling from the light source through the volume, the incident direction of li (xi−1 , ω) and the outgoing direction for c(xi , ω) are the
same. Thus, we can approximate the light traveling by considering
the emission absorption model. While li (xi−1 , ω) may be obtained
by performing a single texture fetch at pi if we want to guarantee hard shadows, c(xi−1 , ω) is obtained by blending voxels in the
neighborhood of pi . The amount of voxels and the blending factors
are dependent on the cone angle θ . We have implemented a Phong
lobe phase function as shown in Section 3, where each contributing
voxel is weighted with cos(α)β , where α is the angle between the
light vector from x and the vector towards the contributing voxel,
and β is a user specified fall-off coefficient. To achieve energy
preservation, we further divide the computed contribution by the
number of considered neighbors, using the previously introduced
constant C.
In order to incorporate the scattering of the outgoing light, we
can proceed similarly in a second pass. But instead of using xl to
determine F0 , we use the current camera position and set dirmax to
nF0 before propagating illumination from back to front. During this
propagation we blend the outgoing chromaticity with the previously
computed incoming chromaticity and choose the minimum of the
scalar luminance. While this proceeding is not physical correct, it
leads to plausible results as discussed in Section 6.
Implementing the described light propagation scheme brings up
some technical issues. First, as already mentioned, it is only possible to render into a 3D texture along the z-axis. Therefore, the
texture coordinates as assigned to the rendered slices are permutated based on a permutation vector, which stores for each axis in
image space, to which axis it is mapped in volume space. Since
this permutation is applied when generating the illumination volume, we have to apply its inverse to the texture coordinates when
accessing this volume later on during rendering. Furthermore, for
all texture fetches during the light propagation, it has to be ensured,
that the information on the current slice Si is not taken into account.
This can be either achieved by manually filtering in the xy-plane
only, or more easily by exploiting texture arrays, which allow to
filter only within one texture slice. Finally, when applying the described light propagation algorithm in cases, where xl is located
inside the volume, the lighting information cannot be propagated
in a single direction. There are two alternative solutions for this

problem. First, the lighting information can be propagated into both
directions starting at xl . However, this approach may lead to noticeable discontinuities within the illumination volume. Therefore, we
have decided to simply project xl onto F0 and apply the algorithm
as described above in order to propagate lighting information into
direction dirmax . In our experiments this lead to results of sufficient
quality.
While the illumination propagation can be done rapidly, since
only basic operations are required, the four channel illumination
volume might be a limitation when dealing with large data sets.
When taking into account the perceptual properties of the human
visual sense, we are able to reduce the memory footprint of the
illumination volume. Therefore, we have exploited the fact, that
chromaticity variations are less perceived than variations in luminance [13]. Keeping in mind, that the scattering is propagated
through the RGB channels and the attenuation through the A channel, we are able to downscale the scattering volume by remaining the original size of the attenuation part. This is a technique,
which has also been incorporated in the JPEG image compression
technique, and it is referred to as chroma subsampling. In fact,
instead of downsampling a previously generated data set, we directly modify the light propagation in order to also achieve a performance gain. However, since this increases the distance between
the slices to process, the chromasubsampling factor gcs is used
to modify the opacity of the current voxel prior to compositing:
color.a = color.a ∗ gcs . This avoids luminance variations when enabling chroma subsampling. The technique has been applied when
rendering the salamander data set shown in Figure 4.
4.2

Changing Light Position

While the light propagation technique described in the previous
subsection, would work well for arbitrary but fixed light positions
xl , a dynamic change may introduce problems. This is due to the
fact, that dirmax runs always along one of the major volume axis.
When xl changes in a way that dirmax is also changed, the illumination is propagated along a different direction through the volume,
which might lead to noticeable popping artifacts.
To deal with this problem, we introduce an additional light propagation direction dirblend . While dirmax is the light propagation direction based on the cube face F0 with the largest projection size as
seen from xl , dirblend = −nF1 is based on F1 , the cube face with the
second largest projection size (see Figure 3). When blending the results of the light propagation along these two principle axes, we can
avoid popping effects, since the contribution of the possibly new
dirmax has been already taken into account, before the the princi-

ple light propagation direction has changed. Thus, the light volume
generation is done, whereby the first propagation is along dirmax
as described in Subsection 4.1, and the second is along dirblend .
Therefore, we process the volume along dirblend and compute the
scattering and shadowing along this axis. But instead of using two
different illumination volumes during rendering, we blend the computed illumination results I1 and I0 : S = α0 · I0 + α1 · I1 .
As depicted in Figure 3, α0 and α1 are dependent on the angle
β between the light vector dirl and dirmax resp. γ between dirl
and dirblend . To achieve an appropriate blending, we have to ensure
that the sum of α0 and α1 always equals 1. Furthermore, α1 must
be 0 around the center area at cF0 , because otherwise popping would
occur, when moving the light source over cF0 . Choosing α0 = 1 −
2·acos(β )
2·acos(γ)
and α1 = 1 −
fulfills these requirements.
π
π
5

R ENDERING

Using current GPUs for rendering with our model is straight forward. While in principle the proposed lighting computation is applicable to any volume rendering technique, we use GPU-based volume ray-casting, whereby the shading is done with GLSL shaders.
In the following, we assume the light volume has been generated
as described in the previous section and is present as an RGBA data
set, where the RGB channels represent the chromaticity c(x, ω) and
the A channel the attenuated light intensity li (x, ω), both at the current voxel’s position x. Such light volumes are shown in column
(a) in Figure 4. Based on the values fetched from these data sets,
we solve the front-to-back compositing volume rendering integral.
Thus, shading of x is a multiplication of the transport color t(x),
as assigned through the transfer function, and c(x, ω) to which we
apply an intensity adaption based on li (x, ω). In order to support
emission, the emissive color Le (x) can be added. The compositing
along the viewing ray is not altered.
Figure 4 shows the results, whereby (a) shows a volume rendering of the generated illumination volumes. From (b) to (d), the
following techniques are shown. In (b) only shadowing is applied,
whereby the voxel is assumed to be initially white. While we have
used an RGBA light volume for all cases shown in this section, a
single channel shadow volume would have been sufficient for this
shadowing only case. In (c), we only show the scattering color
c(x, ω) as computed during our volume processing. The figures labeled with (d) show the application of the entire illumination model
combined with a specular term. Therefore, we multiply the transport color t(x) and c(x, ω), which is modulated based on li (x, ω) as
shown in Equation 1. While this rendering technique generates realistic results for diffuse and scattering materials, it is not possible
to capture materials having a high degree of specularity. Therefore,
we have combined the technique with an achromatic specular intensity cs (x) as calculated with the gradient-based Phong model. Since
we assume, that the specular intensity is not wave length dependent
we are able to multiply cs (x) and li (x, ω), and are able to modulate
the result by the gradient magnitude | τ(x)| in order to achieve
the desired intensity:
I=|

Figure 3: To avoid popping artifacts, when the main propagation axis
dirmax is changed, we employ a blending between the two principal
light directions given by F0 and F1 .

τ(x)| · cs (x) · li (x, ω).

As it can be seen, surfaces appear to be vivid having a specular component, while diffuse elements are also present. Instead of
modulating cs (x) with the gradient magnitude, it could also be modulated by the signal to noise ratio of the computed gradients. Thus,
it would become possible to emphasize surfaces in an otherwise
noisy data set.
It should be pointed out, that while we can use a single texture
fetch in order to obtain c(x, ω) as well as li (x, ω) in the general case,
we need to apply two texture fetches when exploiting the chroma
subsampling introduced in Section 4. One in the original shadow

173

(a) illumination volume

(b) li (x, ω)

(c) c(x, ω)

(d) full shading

Figure 4: The generated illumination volume (a) can be used to achieve different rendering effects: rendering with the attenuated light intensity
li (x, ω) (b), with the scattering chromaticity c(x, ω) (c) and with full shading as defined by our illumination model (d).

Figure 5: Comparison of an MRT scan of a human head (256 × 256 ×
256 voxels) rendered with gradient-based Phong illumination (left)
and with our technique (right). The scattering contribution results in a
more realistic appearance of the skin, while the shadowing supports
the spatial comprehension.

volume and one being subject to a texture coordinate transformation
into the scattering volume having a lower resolution.
6 R ESULTS AND D ISCUSSION
Besides the application to CT data sets as shown in Figure 1 and
Figure 4, we have also applied our algorithm to other modalities
having a lower signal to noise ratio. Figure 5 shows the application to an MRT scan of a human head having a resolution of
256 × 256 × 256 voxels. Compared to the gradient-based shading
(left), our algorithm (right) results in a more realistic representation
of the human skin, since scattering effects are taken into account.
Furthermore, the shadowing gives the image more depth, which is
especially visible below the nose and in the region around the eyes.
We would like to point out the rather hard shadow borders, which
would not be present when just applying scattering. While high
quality shading of MRT data with conventional shading techniques
is already challenging, it becomes nearly impossible when applying
it to 3D US data, which suffers from an even lower signal to noise

174

ratio (see Figure 6 (left)). When instead applying our technique to
the same data set (see Figure 6 (right)), structures start to emerge
and the spatial comprehension becomes more easy.
To analyze the performance of our algorithm, we have measured
the fps over 1000 frames at a resolution of 1024 × 1024, where the
rendered object is rotated around its up-vector. With this scenario,
we have compared three different cases: Phong shading, and our
technique with and without light source movement. For the Phong
shading we have exploited forward difference gradients computed
on the fly and have considered ambient, diffuse and specular illumination. Table 1 shows the results, we could achieve on a standard desktop computer, having an Intel Core2 Duo CPU E8400 running at 3.00 GHz, 4 GB of main memory, and an nVidia GeForce
GTX285 graphics board. As it can be seen, for most cases our technique results in higher frame rates than conventional shading. Only
when recomputing the illumination volume, the frame rates drop
below those achieved with gradient-based shading. The only case,
where our technique was slower in both cases is the torso data set.
Since our volumetric illumination technique only approximates
scattering and shadowing, we have decided to conduct a user study
in order to assess its usefulness. To realize this, we have implemented an applet, with which each participant had to accomplish
three different tests. Since the usefulness of a rendering algorithm
is always dependent on the application case in which it is utilized
and therefore hard to judge in general, we have decided to focus primarily on the facilitation of depth perception. This decision is based
on the assumption, that depth perception is essential for most 3D
viewing tasks. In each of the three tests, the participants have been
confronted with numerous rendering results generated either with
the technique described in this paper or alternatively with gradientbased Phong illumination. To achieve better comparability, each
image has been incorporated twice into each test series, once rendered with our model and once with Phong. Besides from the rendering technique, all other parameters, e. g., transfer function as
well as camera and light position, have been left unchanged. In the
first test, the depth comparison test, 38 images have been shown
to each participant. The goal of this test was to analyze relative

depth perception. Therefore, two regions have been emphasized in
each image by using circular markers and the user has been asked
to click on the region, which is closer to the viewer. During this test
we have measured the time used for each image as well as whether
the correct region has been selected. In the second test, the depth
estimation test, we wanted to judge absolute depth perception. To
do so, we have emphasized only one region in each of the 14 shown
images by using a spherical marker. In this test, the participants
had to estimate the depth of the emphasized region in percentage
of the overall depth extend of the shown object. The participants
could input the estimated depth by using a slider as shown in Figure 7. In this test we have measured the difference between the real
depth as computed via volume ray-casting and the estimated depth
as transmitted using the slider. Additionally, we have also measured
the time used for each image. As a third test, we have conducted
a very subjective image comparison test, where we have shown 19
pairs of images to each participant. Again, both images where identical despite the used rendering technique. The subjects have been
asked to select that image from the pair, which they find more visually pleasing. While the order of the three tests has been fixed for
each participant (depth comparison, depth estimation, image comparison), the order of the displayed images has been randomized in
order to reduce order-dependent side effects. In addition, each test
started with a simple example in order to make the participants familiar with it. Since these examples where unambiguous, we have
decided to omit the results for one test, if the participant failed doing the example correctly. This was never the case.
16 (age 22-33, ∅ : 27) subjects participated in the study. Most
subjects were students or members of the departments (computer
science, mathematics, geoinformatics, and physics). All had normal or corrected to normal vision; 5 wear glasses or contact lenses.
The time per subject including a short pre-questionnaire, reading
the instructions, training, and experiment took approximately 8
minutes, where in total 94 images have been processed in the three
different tests. The results of the depth comparison test are shown
in Figure 8. In the top left bar chart, the average time used for
conventional shading (3935.4 ms) as compared to our technique
(3250.5 ms) is shown. The lower left plot shows the percentage of
correct depth decisions as based on conventional shading (71.4%)
and our technique (94.2%). For both these results, we have performed a t-test and we found a significant (ρ < 0.01) increase in
accuracy as well as speed, when using our technique for the relative depth perception in the tested cases. This is also reflected in
the scatter plot shown in Figure 8 (right), where the average time
needed and the percentage of correct depth decisions are plotted for
each image. It can be clearly seen, that the images generated with
our technique form a cluster in the top left region. Additionally, in

Figure 7: During the depth estimation test, the users had to estimate
the depth of the highlighted region by adapting the slider, representing the entire depth extend of the shown object, accordingly.

the depth estimation test the average depth estimation was more accurate and it was less time used. However, based on the rather low
number of subjects and images, we could not show a high significance. Finally, the results of the third test have shown, that our volumetric illumination model is perceived as more aesthetic in 72.4%
of all shown cases.
7 C ONCLUSIONS AND F UTURE W ORK
In this paper we have proposed a volumetric lighting model supporting scattering as well as shadowing effects, which allows interactive frame rates when implemented on current graphics hardware. We have derived the model and described our implementation based on illumination propagation. While the model is similar in spirit to the model proposed by Kniss et al. [11], it requires
fewer sampling operations for the illumination calculation, since
this is performed in volume space. This allows an easy integration
into current state-of-the-art GPU-based volume ray-casters, which
is not possible with the half-angle technique proposed by Kniss et
al.. Based on our implementation, we have shown several successful application examples also including modalities with a low signal to noise ratio, as 3D US and MRT. Since the presented model
and its implementation are physically-motivated but not physicallycorrect, we have conducted a user study, with which we have compared our technique to conventional gradient-based shading. Based
on the result of three different tests, we were able to show, that the
proposed model is able to improve the depth perception of volume
renderings. Furthermore, the technique is very easy to implement,
and can be easily combined with clipping planes and arbitrary classification techniques, while providing in many cases higher frame
rates than gradient-based shading. This allows full control over all
relevant rendering parameters as transfer function, lighting parameters and camera view. Therefore, we believe it has the potential to
be integrated in other volume rendering frameworks.
Since we could show that the approximative nature of the model

Table 1: Performance of our technique with and without light volume
regeneration as compared to local Phong illumination.

data set

Figure 6: A 3D US scan of a human heart (148 × 203 × 149 voxels)
rendered with gradient-based Phong illumination (left) and with our
technique (right). Due to the low signal to noise ratio, spatial comprehension is difficult when using gradient-based shading. Instead,
when using our technique, structures emerge.

mouse
torso
salamander
MRT head
3D US heart

size
(voxel3 )
360 × 270 × 550
378 × 346 × 445
512 × 512 × 202
256 × 256 × 256
148 × 203 × 149

Phong
12.6 fps
11.3 fps
9.4 fps
22.6 fps
24.0 fps

our technique
no regen.
regen.
16.5 fps
7.2 fps
9.5 fps
4.2 fps
17.3 fps
5.5 fps
25.0 fps
12.2 fps
31.4 fps
14.3 fps

175

100
90

[8]

average % of correct responses

80
70
60

[9]

50
40

[10]

30
20
10
0
2000

conventional shading
our technique
2500

3000
3500
4000
average elapsed time (in ms)

[11]
4500

5000

[12]
Figure 8: The results of the conducted depth comparison test. The
plots on the left show the average time needed and the accuracy
of depth decisions, while the scatter plot shows all images plotted
based on their average time and accuracy. As it can be seen, images
generated with our technique form a cluster towards more quick and
accurate depth decisions.

still allows improved spatial comprehension, the major drawback
is the usage of an additional illumination volume. However, by
exploiting chroma subsampling, the situation can be improved. In
addition to that, we are positive, that with the increasing memory
size of graphics hardware, this will become even less important.
In the future, we would like to extend our technique in order to be
able to integrate area light sources. Since our algorithm propagates
illumination information from the outside of the volume inwards, it
could be possible to project area light sources onto the border of the
volume in order to achieve the desired effect. Additionally, it would
be interesting to investigate the influence of multiple light sources.
On the evaluation side, we would like to compare the shadowing to
the ground truth and evaluate the influence of chroma subsampling.
ACKNOWLEDGEMENTS

[14]

[15]

[16]
[17]

[18]

[19]

[20]

This work was partly supported by grants from the Deutsche
Forschungsgemeinschaft (DFG), SFB 656 MoBil M¨unster, Germany (project Z1). The presented concepts have been integrated
into the Voreen volume rendering engine (www.voreen.org).

[21]

R EFERENCES

[22]

[1] K. M. Beason, J. Grant, D. C. Banks, B. Futch, and M. Y. Hussaini.
Pre-computed illumination for isosurfaces. In Proceedings of the Conference on Visualization and Data Analysis (VDA), pages 1–11, 2006.
[2] U. Behrens and R. Ratering. Adding shadows to a texture-based volume renderer. In Proceedings of the IEEE International Symposium
on Volume Visualization, pages 39–46, 1998.
[3] D. Dalby. Biological Illustration : A Guide to Drawing for Reproduction. Field Studies Council, 1980.
[4] P. Desgranges, K. Engel, and G. Paladini. Gradient-free shading: A
new method for realistic interactive volume rendering. In Proceedings
of the International Fall Workshop on Vision, Modeling, and Visualization (VMV), pages 209–216, 2005.
[5] Y. Dobashi, T. Yamamoto, and T. Nishita. Interactive rendering of atmospheric scattering effects using graphics hardware. In Proceedings
of the ACM SIGGRAPH/EG Conference on Graphics Hardware (GH),
pages 99–107, 2002.
[6] M. Hadwiger, A. Kratz, C. Sigg, and K. B¨uhler. GPU-accelerated
deep shadow maps for direct volume rendering. In Proceedings of
the ACM SIGGRAPH/EG Conference on Graphics Hardware (GH),
pages 49–52, 2006.
[7] F. Hernell, P. Ljung, and A. Ynnerman. Efficient ambient and emissive tissue illumination using local occlusion in multiresolution vol-

176

[13]

[23]

[24]
[25]
[26]

[27]
[28]

[29]

ume rendering. In Proceedings of the IEEE/EG International Symposium on Volume Graphics (VG), pages 1–8, 2007.
F. Hernell, P. Ljung, and A. Ynnerman. Interactive global light propagation in direct volume rendering using local piecewise integration.
In Proceedings of the IEEE/EG International Symposium on Volume
and Point-Based Graphics (VG), pages 105–112, 2008.
P. V. J. Daz, H. Yela. Vicinity occlusion maps - enhanced depth perception of volumetric models. In Proceedings of Computer Graphics
International (CGI), pages 56–63, 2008.
J. Kniss, S. Premoze, C. Hansen, and D. Ebert. Interactive translucent
volume rendering and procedural modeling. In Proceedings of IEEE
Visualization 2002, pages 109–116, 2002.
J. Kniss, S. Premoze, C. Hansen, P. Shirley, and A. McPherson. A
model for volume lighting and modeling. IEEE Transactions on Visualization and Computer Graphics, 9(2):150–162, 2003.
M. S. Langer and H. H. B¨ulthoff. Depth discrimination from shading
under diffuse lighting. Perception, 29(6):649–660, 2000.
M. Livingston. Vision and Art: The Biology of Seeing. Harry N.
Abrams, New York, 2002.
P. Ljung, F. Hernell, and A. Ynnerman. Local ambient occlusion in direct volume rendering. IEEE Transactions on Visualization and Computer Graphics, 15(2), 2009.
T. Lokovic and E. Veach. Deep shadow maps. In SIGGRAPH ’00:
Proceedings of the 27th annual conference on Computer graphics and
interactive techniques, pages 385–392, New York, NY, USA, 2000.
ACM Press/Addison-Wesley Publishing Co.
N. Max. Optical models for direct volume rendering. IEEE Transactions on Visualization and Computer Graphics, 1(2):99–108, 1995.
A. Mendez, M. Sbert, and J. Cata. Real-time obscurances with color
bleeding. In Proceedings of the Spring Conference on Computer
Graphics (SCCG), pages 171–176, 2003.
E. Penner and R. Mitchell. Isosurface ambient occlusion and
soft shadows with filterable occlusion maps. In Proceedings of
the IEEE/EG International Symposium on Volume and Point-Based
Graphics (VG), pages 57–64, 2008.
T. Ropinski, J. Kasten, and K. H. Hinrichs. Efficient shadows for
GPU-based volume raycasting. In Proceedings of the International
Conference in Central Europe on Computer Graphics, Visualization
and Computer Vision (WSCG), pages 17–24, 2008.
T. Ropinski, J. Meyer-Spradow, S. Diepenbrock, J. Mensmann, and
K. H. Hinrichs. Interactive volume rendering with dynamic ambient
occlusion and color bleeding. Computer Graphics Forum (Eurographics 2008), 27(2):567–576, 2008.
M. Ruiz, I. Boada, I. Viola, S. Bruckner, M. Feixas, and M. Sbert.
Obscurance-based volume rendering framework. In Proceedings of
the IEEE/EG International Symposium on Volume and Point-Based
Graphics (VG), pages 113–120, 2008.
C. R. Salama. GPU-based monte-carlo volume raycasting. In Proceedings of Pacific Graphics (PG), 2007.
M. Schott, V. Pegoraro, C. Hansen, K. Boulanger, and K. Bouatouch.
A directional occlusion shading model for interactive direct volume
rendering. In Computer Graphics Forum (Proceedings of Eurographics/IEEE VGTC Symposium on Visualization 2009), pages 855–862,
2009.
A. J. Stewart. Vicinity shading for enhanced perception of volumetric
data. In Proceedings of IEEE Visualization (VIS), page 47, 2003.
J. Sun and P. Perona. Where is the sun? Nature Neuroscience,
(1):183–184, 1998.
L. Wanger. The effect of shadow quality on the perception of spatial
relationships in computer generated imagery. In SI3D ’92: Proceedings of the 1992 symposium on Interactive 3D graphics, pages 39–42,
1992.
C. Zhang and R. Crawfis. Volumetric shadows using splatting. In
Proceedings of IEEE Visualization (Vis), pages 85–92, 2002.
C. Zhang and R. Crawfis. Shadows and soft shadows with participating media using splatting. IEEE Transactions on Visualization and
Computer Graphics, 9(2):139–149, 2003.
S. Zhukov, A. Iones, and G. Kronin. An ambient light illumination
model. In Proceedings of the EG Workshop on Rendering (EGRW),
pages 45–55, 1998.

