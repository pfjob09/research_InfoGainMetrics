A Model of Symbol Lightness Discrimination in Sparse Scatterplots
*

Jing Li

Jarke J. van Wijk†

Jean-Bernard Martens‡

Eindhoven University of Technology
ABSTRACT
Symbols are used in scatterplots to encode data in a way that is
appropriate for perception through human visual channels. Color
is believed to be the most dominant channel with lightness
regarded as the most important one of three color dimensions. We
study lightness perception in scatterplots in the context of analytic
tasks requiring symbol discrimination. More specifically, we
performed an experiment to measure human performance in three
visual analytic tasks. Outlined circles and unframed spots, equally
sized, with a uniform luminance that was varied at ten or eleven
equispaced levels between black and white were used as symbols
and displayed on a uniform white background. Sixteen subjects
divided in two groups, participated in the experiment and their
task performance times were recorded. We propose a model to
describe the process. The perception of lightness is assumed to be
an early step in the complex cognitive process to mediate
discrimination, and psychophysical laws are used to describe this
perceptual mapping. Different mapping schemes are compared by
regression on the experimental data. The results show that
approximate homogeneity of lightness perception exists in our
complex tasks and can be closely described by a blended
combination of two opposite power functions assuming either the
light end or the dark end of the lightness scale as the starting point.
The model further yields discriminability scales of lightness for
sparse scatterplots with a white background.
KEYWORDS: Lightness, Gray Scale, Symbol, Scatterplots,
Contrast Model, Color Perception, Graphical Encoding, Visual
Analytic Task, Quantitative Model, User Experiment.
1

INTRODUCTION

Symbols are used in scatterplots to denote objects of interest.
Besides their position, other visual attributes can be used to
represent multivariate information. Color is believed to be the
most dominant channel among all the visual channels, with
lightness (achromatic perception scale) regarded as the most
important one of three color dimensions [1]. The lightness channel
has an intuitive association with many analytic features of data,
such as order, proportion and density, and also can convey high
spatial frequency information. Therefore, it supports information
decoding and interpretation quite well. However, lightness is very
sensitive to the applied context, such as the environment
illumination and the color of the background. There are few
quantitative results on the perception of symbol lightness in visual
analytic tasks. Our question here is what scale to use to support a
visual analysis as well as possible.
Existing psychophysical research provides us relevant clues to
design graphical encoding schemes. Among them, Stevens’s
*e-Mail: j.li@tue.nl
†e-Mail: vanwijk@win.tue.nl
‡e-Mail: j.b.o.s.martens@tue.nl
IEEE Pacific Visualisation Symposium 2010
2 - 5 March, Taipei, Taiwan
978-1-4244-6686-3/10/$26.00 ©2010 IEEE

Power Law depicts the relationship between the physical
magnitude of a stimulus and the corresponding experienced
magnitude [2]. The perceived magnitude is a power function of the
stimulus, where the power coefficient depends on the tested
physical channel of the stimulus. However, for lightness, it is not
straightforward to select the appropriate value of the power. This
is due to a number of reasons. First, experiments in psychophysics
are set up in a context quite different from practical user tasks.
Second, these experiments require the user to make judgments on
an explicit scale, for instance the lightness of a gray toned paper or
the perceived darkness of an inked area [3,4]. However, lightness
is not simply the perception of target luminance or reflectance, and
it adapts to the luminance of the surround, affected by the
simultaneous contrast of the background [5]. We are not sure yet
how those factors might be involved in symbol discrimination
tasks, especially when a self-luminous display such as a PC screen
is used. Third, individual variance has often been ignored in the
analysis as only average data are reported, while visualization
potentially needs to be optimized for specific users.
We consider lightness encoding in applications of information
visualization. Our aim is to pick luminance levels such that
analytic tasks, such as distinguishing sets and counting outliers,
are as easy as possible. We use perception models based on
psychophysical laws as starting point. Based on the Guided Search
theory of visual attention [8], we hypothesize that the perceived
contrast between symbol sets is negatively correlated with task
difficulty. Or, put simply, we assume that the more different two
sets of symbols look, the easier it is to discriminate them, thereby
enabling fast and precise pattern discovery.
Cartographic experiments revealed different equal-value gray
scales with non-linear patterns for printed maps. In this paper, we
aim to model the relation between the computer displayed gray
tones, i.e., symbol lightness and discriminability in analytic tasks
of scatterplots visualization. We take the circular symbol, as the
most commonly used symbol; use eleven equispaced luminance
levels for outlined circles including black and white ends, and ten
equispaced levels for unframed spots without white end; test with
three visual analytic tasks; and measure user task performance.
We fixed the symbol size and the spatial frequency, since these
two channels were found to be influential [5,6] and we leave the
study of channel interaction to the next stage. A quantitative
model is proposed for describing the relation between stimulus
luminance levels and task performance, and the model parameters
are estimated from the experimental data. Adopting the principle
of Homogeneity of Perception [9], we also test the assumption of a
uniform perceived lightness scale across users. The resulting
scales suggest possible encoding schemes for visualization
designers.
In the following, we construct our quantitative model based on
related work, report our user study, analyze data on alternative
models, and discuss the implications of our work.
2

RELATED WORK

Many different research fields study how humans process visual
information. In psychophysics, the aim is to derive quantitative
models for perception via vision and other modalities. Generic

105

laws have been developed to describe perceptual mappings.
Meanwhile, the fields of visualization and cartography focus on
visual features of graphical objects and analytic tasks. The goal is
to provide guidelines for the design of better displays of
information in terms of easy discovery of patterns, and veracious
interpretation.
2.1
Laws and Models in Psychophysics
Psychophysics deals with the relation between physical stimuli
and subjective percepts, and therefore measures the human
sensation of various physical stimuli quantitatively. The
continuum nature of perception is assumed pre-conditionally in
most of the studies [10]. Pioneers in this field have developed
general relationships, such as the Weber-Fechner Law: P = log X
and Stevens’s Power Law: P = Xβ, where P presents the sensed or
perceived magnitude and X is the physical magnitude of the
stimulus. Stevens’s power law was empirically verified for many
perception channels, across different modalities and even for the
overall statistical properties such as the mean size or mean
brightness of sets of symbols [11,12]. However, the common
criticisms in the field are as follows.
Stevens’s method averages data in different observations. It had
been seriously questioned if averaging can be correctly applied to
the sensation variability [10]. We believe that a more appropriate
model can be achieved by modeling the individual perception and
performance of different subjects in distinct test conditions.
The judgment of a single stimulus was used in Stevens’s
experiments, i.e., subjects were required to make judgments
without an explicit reference. However, only relations between
stimuli might provide a basis for judgment [10]. We believe that a
discriminability scale is more meaningful in real use cases.
Stevens’s experiments require an agreement on what to judge.
However, achromatic appearance is perceptually complex.
Different descriptions are possible by assuming either white or
black as the base of perceptual increments [4], especially when a
white background is used. Moreover, the references of both ends
depend on the context. Therefore we are not sure what criteria
people use when they perform tasks that involve symbol lightness
discrimination.
A widely accepted standard scale of lightness, CIE L* is the
power law mapping from luminance [1,7]:
Y/Yn > 0.008856
(1)
L* = 116 (Y/Yn)1/3 – 16
*
where L presents the perceptually uniform scale of lightness and
Y is the luminance variable with Yn as the luminance of a reference
white. When Y/Yn is below 0.008856, we should replace it with a
linear transformation or correct the exponent to be approximately
1/2.43 [7]. In our experiment, Ymin/Yn is always ≥0.02 (Figure 7),
thus (1) can be used. However, this model does not account for the
influence of the surroundings [5], particularly the contrast
distortion.
There are different luminance contrast models which consider
the modulation of achromatic appearance via the eye adaptation to
the background [6]. Weber’s contrast model is often used for
larger uniform backgrounds:
(2)
△L = CW = (YS – YB) / YB ;
where the subscript S denotes stimulus, and B denotes background.
Michelson’s contrast model is often used for simple but highfrequency patterns without a clear uniform background:
(3)
△L = CM = (Ymax – Ymin) / (Ymax + Ymin) .
In this paper, we focus on how to improve visual analytic work
in practical applications, and argue that lightness judgment thus
cannot be isolated from the working context. We use sparse
scatterplots with a uniformly white background in the analysis.
With “sparse”, we mean here a clearly recognizable background.

106

Moreover, we propose an alternative way to analyze lightness
perception in such a more complex context.
2.2
Gray scales in Cartography
In choropleth, dasymetric, monochromatic “layer-tinting”, and
other mapping techniques, lightness differences encode classes or
progressions of quantitative data. Cartographers aim at maximum
distinguishability in maps to be produced by an equal perceived
contrast between all pairs of adjacent gray tones. Many
experiments have been done to obtain an optimal equal-value gray
scale, and with quite different results.
A first difference concerns the type of measurement. Stevens
used magnitude estimation where subjects were asked to assign
numbers to different shades of gray paper in proportion to the
perceived reflectance [3]. However, subjects might use white
paper as the comparison base and judge on the perceived darkness.
Munsell used a partition method where subjects were asked to
divide the gray spectrum (with white and black ends) into evenly
spaced tones [13]. The former produced a ratio scale with
exponent 1.2 and the latter produced an empirical curve that could
be closely approximated by a power law with exponent 1/3 as the
CIE L* [14]. The difference between the obtained values for the
exponents was explained by Kimerling [14] by the fact that the
second method used explicit references for the extreme luminance
levels, while the former method didn’t.
Another difference exists within the experiments that use the
partition method. An empirical curve, i.e., Williams’ curve,
achieved by using patterned areas and a partition method, gave
results that deviated significantly from the Munsell curve where
solid tints were used [14]. More specifically, a certain amount of
recurvature is apparent near 70 percent reflectance, leading to an
S-shaped curve [15], as shown in Figure 1. This was explained as
being due to the visible patterns, i.e., the contrast between white
and a patterned gray tone was higher than between white and a
finely screened gray tone.

The plot was
reproduced by
the author
according to
Kimerling 1975
[13], 1985 [14],
and Leonard
1989 [15].

Figure 1. S-shaped curves in literature with reflectance measure.

A similar pattern of recurvature was also observed in other
studies. Kimerling studied the influence of background reflectance,
for a screened area (solid tint) on a white background using a
partition method [13]; while Stoessel used finely screened squares
and a measurement method that mixed magnitude estimation, the
partition method and the just-noticeable-difference method [4,13];
and Leonard used a laser printer, printed chips on a white
background with precisely measured values of dpi for different
gray tones, and a partition method [15]. The recurvature from
Kimerling’s study using a white background was found to be
significantly different from the results with black and gray

backgrounds that fit the Munsell curve quite well. However, he
concluded by using a general curve averaged across different
backgrounds which resembles closely the Munsell scale. Although
acknowledged by other cartographers as interesting and relevant
[4,13], the recurvature found by Stoessel is obviously a complex
phenomenon (neither logarithmic nor exponential) that lacks a
sound theoretical understanding. Leonard explained his result as
similar to Williams’ that the recurvature was caused by the printed
dot pattern that became obvious at high reflectance levels.
In summary, the recurvature at high reflectance level has either
been ignored, or explained as being a consequence of the use of
patterns. However, we want to put forward an alternative
explanation. The recurvature primarily occurs in experiments
where gray tones have to be judged against a white background.
As explained before, lightness may be a combined construct. One
gray tone is not universally judged to be “more” or “less” than
another gray tone. When a white background is used, it is
reasonable to assume that this constitutes the reference and that
objects are discriminated from their background using perceived
darkness. On the other hand, the surface reflectance (from the
print material) or the luminance in the working context (from the
computer screen) has its own influence on human perception, so
that the perceived lightness may also influence the human
judgement. We propose that the ambiguity in what end constitutes
the reference for the judgment is possibly causing the recurvature.
This is going to be addressed later on when we model our
experimental data. Our proposal doesn’t exclude that the pattern
might be a factor in Williams’ curve. When the viewing
background is black, this ambiguity disappears since detecting a
gray object from the black background is in the same manner of
perceiving lightness. This explains why the gray and black
backgrounds used in Kimerling’s study fit with the Munsell curve
very well.
From another point of view, the perceived darkness is closely
related with the simultaneous contrast to the white background.
Thus in our case, the Weber contrast |△L|=−Cw in (2) could be
used to describe the perceived darkness of symbols. Meanwhile
the CIE L* in (1) is a good candidate to model the perceived
lightness of symbols L. The final achromatic perception should be
a combined effect of (L, △L). This hypothesis accords with
Carter’s general idea [5] that the gray-scale appearance of a
stimulus is determined by both the stimulus luminance and the
contrast from the surround. Referring to the recurvature
phenomenon, we later on propose a novel way of blending the
effects of (L, △L) and modeling the cognitive processing.
2.3
Perceptually Uniform Space in Visualization
Because of the strong association with data features such as order,
proportion and density, lightness is heavily used in visualization to
encode ordinal, interval and ratio data. This is known as a
sequential encoding scheme in Brewer’s work [16]. In a study by
Healey [17], the color distance in the CIELUV color space was
used as one of the three most important effects to develop a multicolor selection technique. Ware studied linear gray sequences and
the CIE L* perceptual gray sequence as well as other chromatic
sequences for relative encoding [18]. In a recent study [19],
Wijffelaars had adopted a logarithmic scale of lightness to
calculate distances between different colors, which produce equalcontrast sequential palettes for data encoding. He demonstrated
that the resulting palettes look more uniformly spaced than those
generated from CIE standards on a grey background. However, as
Ware pointed out [1], the gray scales are perceptually altered by
background lightness and the current standard should be taken as

no more than a useful approximation.
Another important issue to be considered by visualization
designers is the user-task context. We are not sure if a perceptually
uniform color space can also be used to describe uniform
separability in a specific task context. In a previous study on
symbol discrimination [20], the existence of a separation space for
symbols was assumed. A separation space is a space in which
different symbols can be positioned and where distances between
them are proportional to their discriminability. In this study, 32
symbols were tested with four linearly varying sizes and eight
different shapes in three visual analytic tasks. A 3D separation
space was established to position the symbols according to the
task performance when discriminating between them. We propose
that such a separation space generated through user tests in the
task context could result in more practically relevant design rules.
By using the same technique, the equal-contrast lightness scale
might also be produced in such a separation space.
3

SYMBOL DISCRIMINATION MODEL IN THE TASK CONTEXT

The contrast in symbol lightness can be expected to influence task
performance by altering the task difficulty. We aim to model the
overall mappings, with lightness perception as an intermediate
step.
In Guided Search theory [8], visual attention involves top-down
and bottom-up processes, and the bottom-up process is determined
by how different a target is from its context. Also, according to
Ware [1], whether something stands out preattentively is
determined by the degree of difference of the target from the nontargets and the degree of difference of the non-targets from each
other. We make the assumption here that a measure for the
difference between targets and distractors should be based on their
perceived strengths, instead of their direct physical difference. In
other words, it is how they look different which determines the
successive processing, rather than their difference in physical
quantities. This enables us to model the visual analytic task
process as two consequent steps, as shown in Figure 2. A function
G is assumed to transform the original stimulus onto a perceived
scale, such that equal distances between stimuli on this scale
denote equal separability.

Figure 2. Model of lightness discrimination in visual analytic tasks: li,
lj are two different luminance levels; function G models the
lightness perception; function H models the lightness discrimination
in an analytic task and maps to the task performance measure.

Further, we consider judgment in the context of analytic tasks.
Typical visual analytic tasks are to compare symbol sets and to
distinguish patterns from random clutter. In the simplest situation,
just two different sets of symbols are used. If the perceived
strengths of the two sets are very different, the task becomes easy,
and can be finished quickly and precisely. More generically, we
assume a monotonically increasing function H that maps the
contrast of the perceived lightness |G(lj)–G(li)| into the measured
task performance Mij, i.e., the larger the contrast, the easier and
quicker the task can be performed.
Function G describes the relation between a physical measure
and a value P on the perceptual scale (also known as lightness).
Taking the luminance level l of the circular symbol as a physical
measure, it is given by P = G(l). Given a certain G, we can
construct perceptually uniform scales. The luminance levels
follow from li = G-1(Pi) (i = 0,…,N), where Pi is a linear

107

interpolation on the perceptual scale between the black and white
ends. Using different instances of G, we can generate different
sequences. Some candidates are:
− Fechner’s logarithmic function: P = G(l) = log l;
− Stevens’s magnitude estimation: P = G(l) = α·lβ, β = 1.2;
− Assuming linear relationship between luminance and lightness:
P = G(l) = α·l, thus β = 1;
− Munsell’s scale by partition method and fitted with a power
function: P = G(l) = α· lβ, β = 1/3 ≈ 0.33, refer to (1).
The resulting sequences are shown in Figure 8 (in section 6.1).
One issue in working with luminance is that the medium used has
its own influence, also the size and surround. Hence this printed
version cannot be expected to match perfectly. The following
remarks are based on judging the sequences on an LCD computer
monitor. Stevens’s scale provides a better contrast at the dark end,
but worse at the darker grays. Fechner’s logarithmic scale
provides the best contrast at the lighter side but is the worst at the
darker side. Munsell’s scale seems to be better than the
logarithmic scale, but still bad for darker side. A linear scale
balances well for both ends, but not yet produces optimally equal
discriminability considering the middle grays. We are not sure if
one of these scales will yield the optimal discriminability in an
analytic task context, or whether yet still a different scale is
required. In the following we describe our experiments to obtain
relevant measurements, followed by an analysis.
4

EXPERIMENT

For an intensive study on lightness, we selected one standard
symbol shape, the circle, and drew it in two commonly used ways,
i.e., with an outline and without. The lightness was specified via
physically measurable luminance levels with the white and black
ends determined by the capacity of the PC monitor used in the test.
A colorimeter was used to obtain the empirical mapping from the
parameters that we can manipulate via the PC graphics card and
the real luminance of symbols displayed on the PC screen in the
test setup. Three relevant visualization tasks were selected and
task performance in terms of accomplishing time was evaluated.
4.1
Stimuli and Apparatus
The experiment was carried out in a room without windows. This
was done to avoid variation of ambient illumination due to
sunlight change during a day. Instead of testing in an artificial
darkness, the room was illuminated by standard fluorescent lamps
installed in the ceiling. We think such an environment is closer to
the real working space and is more likely to produce practical
results.
A Dell 17 inch LCD monitor was used to display symbols at
different luminance levels. Before configuring the stimulus, a
colorimeter was used to evaluate the relationship between the
parameter that controls the displayed lightness and the luminance
values in the test environment. The lightness control parameter
used in the test application is ranging from 0−1. We test the whole
range including 0 and 1 with equal steps spacing 0.005. A square
covering 90% of the screen area was configured by the lightness
parameter in every sampled step; next it was displayed and
measured with the colorimeter. The empirical curve depicting the
relationship is shown in Figure 3. The nonlinearity is quite
obvious at the dark side. The highest luminance level measured
was 193 cd/m2, which corresponds to the white on the monitor, the
lowest level measured was 4 cd/m2, which corresponds to black.
Including the white and black ends, eleven luminance levels
were sampled by linear interpolation between the two ends:
4+i·(193−4)/10, i = 0,1,…,10, which then were mapped to eleven
values of the lightness control parameter including 0 and 1.

108

Circles were tinted with these values and sized 0.573○ in the visual
angle of diameters. Usually, tinted symbols can be drawn with
outlines or without. Here we use both: tinted circles with a black
outline (a black line with a thickness of 5% of the total length of
diameter) and consistent with the previous study [20], and tinted
circles without outlines and consistent with another study [12]. For
the circles without outlines (also called unframed spots), the
highest luminance level cannot be used, hence we used 10
sampled levels (i = 0,1,…,9).

Figure 3. Empirical mapping curve between the lightness control
parameter and luminance.

We displayed 2D scatterplots on the PC monitor in a white
plotting area of size 25×25 cm2. In each plot, two sets of circles
(either outlined or not outlined) were used with different lightness
and only differing in lightness. One of the two sets was the target
set, while the other was the distractor set. The total number of
circles displayed in each plot was fixed to 50, occupying 10%25% of the plotting area, in order to keep the plot density and
viewing surround more or less the same. The plotting area was
divided into 20×20 cells and each cell contained only one symbol
to avoid unwanted overlap. Furthermore, the area of a cell was
slightly larger than a specified symbol, which allowed for some
random shift of the symbol shown inside the cell. This prevents
symbols to assemble into lines. For each user task, 11×(11−1) =
110 randomly generated plots were used for outlined circles and
10×(10−1) = 90 were used for unframed spots. Subjects
performing the same task viewed either the 110 plots or the 90
plots in different random orders.
4.2

Analytic Tasks and Measures

Figure 4. Examples of the plots used in different tasks.

To be consistent with previous work, we used the same analytic
tasks as before. These are:
Task 1 (T1): Visual Segmentation and Quantity Comparison
Instruction: Select the symbol that is presented most frequently in
the plot.
Task 2 (T2): Outlier Detection and Subitizing
Instruction: Locate and count the symbols of the type that is least
presented (less than 5 times).
Task 3 (T3): Distribution Characterization and Cluster Detection.
Instruction: Select the symbol that is distributed around the center
of the plot with the smaller variance. (Or, select the symbol that
has higher chance to appear in the center of the plot and converges
more into a cluster.)
Examples of plots for these tasks are shown in Figure 4. These
three tasks occur frequently in routine analytic cases. They
represent four fundamental tasks in a ten-task list of a recent
taxonomy of low-level analytic activities [21], i.e., computing
derived value, characterizing distribution, finding anomalies, and
clustering. Particularly, T1 is the same as the quantity estimation
task used by Tremmel [22]; T2 is similar to the identification task
in Nowell’s work [23]; and T3 is comparable to the correlation
judgment task from Lewandowsky and Spence [24].
These tasks and their instructions were presented to the subjects
in such a way that no prediction of the target is possible before the
display of the plot. This design is in order to prevent the influence
of a top-down attention process as indicated by Guided Search
theory [8].
The two standard measures for task performance are the time
needed and the number of errors. Since we aim at the perceived
scale in a separation space, we selected time as measure, as it
gives a higher sensitivity in outcomes. We aim at minimizing
errors, to assure the direct link between task accomplishment time
and the symbol discrimination. For the different tasks we took the
following measures to assure this:
T1. The target type was presented 2.3 times more frequently than
the distractor type;
T2. Outliers were constrained to be within a range of 2○ visual
angle to keep the eye gaze at a one location and eliminate extra
effort of eye movement for counting.
T3. The target type had the same number of samples as the
distractor type and had a four times smaller standard deviation
than the distractor type.
These arrangements make the tasks fairly simple and the
difficulty is mainly determined by the contrast between the two
sets of symbols rather than by the complexity of the task itself.
Therefore, a higher contrast makes the task easier, and can be
performed quicker; while a lower contrast makes the task more
difficult, and requires more time. Our pilot studies and the formal
experiment showed that the above task setup indeed yielded very
few errors (0−5%). As performance measure in the data
processing, we use Mij=1/tij (t denotes the measured time,
i,j=1,2,…,8 and i≠j), such that a higher sensitivity corresponds to a
shorter time performance.
4.3
Subjects and Procedure
Sixteen subjects were recruited from different departments of the
Eindhoven University of Technology, all students or researchers.
They all had experience in using statistical graphs, but in different
fields. All of them had normal or corrected-to-normal vision and
they were aged between 24 and 34 years, and balanced in gender.
Subjects were divided into two groups, with one group assigned
to the outlined circles and the other assigned to the unframed spots.
Every subject performed all the three tasks in three separate

measurement sessions. A training session preceded each test
session, and instructions were given together with the training
session. It was emphasized that the decisions should be made as
quickly as possible, under the precondition that subjects took
sufficient time to perceive the plot clearly and did not guess the
answers.
The test was started by pressing the space bar. A blank screen
was presented for 1.5 seconds to clear the viewport, after which a
plot was displayed and an internal clock was started. As soon as
the subject decided on the answer, she or he hit the space bar to
stop the timing and the plot was replaced by a response interface.
The response screen displayed the two differently tinted circles
that were used in the plot for T1 and T3, or displayed “0”,
“1”…, ”5” as the number of outliers for T2. Next, the subject could
select the answer. For T1 and T3 the lighter circle was always at the
left side and the darker circle at the right side on the response
screen, to reduce cognitive error of answer inputting. Next, the test
continued with the next plot by pressing the space bar again.
5

DATA EXPLORATION

We have specified our model in Figure 2 in terms of functions G
and H. Before we fit specific functions, we first consider some
underlying assumptions of this model.
5.1
Symmetry of Symbol Discrimination
One underlying assumption is that how symbol A differs from
symbol B is the same as how symbol B differs from symbol A. In
other words, the output of H should be constant when the target
set and distractor set are swapped. We verify this as follows.
For each subject the plots viewed in a session lead to a 11×11 or
a 10×10 dissimilarity matrix, filled with Mij (i,j=0–10 or 0–9, and
i≠j) and with an empty diagonal. The assumption made is valid if
this matrix is symmetric. We use the following method to test the
matrix symmetry. A matrix of a symmetric predictor is created by
averaging the data of the same symbol pair tested in the
observations of role swapping for each subject. Then an ANOVA
test can be performed between the symmetric predictor and the
experimental samples [25]. For all the 16 subjects in the three task
conditions, totally 16×3=48 matrices, we found no significant
evidence of asymmetry, which indicates that swapping the role of
the target and distracter symbol has no significant effect on the
experimental data.
5.2

Visualize the Perceived Lightness Scale

Figure 5. Curve fitting with 3D MDS results: the distance between li
and lj (i,j=0,1,…, 9,10) represents the lightness separability and the
spiral curve represents the continuous scale of the perceived
lightness.

One straightforward approach to deal with dissimilarity data is
multi-dimensional scaling (MDS) [22,25]. We use it here to
observe how the symbols with different luminance levels are
arranged in an internal separation space under perception [20]. A

109

3D space gives the best balance between model complexity and
prediction error among 1D to 5D spaces. Figure 5 shows 2D views
of the 3D generic separation space for outlined circles and
unframed spots across subjects and tasks, taking Mij=1/tij as the
measure for dissimilarity.
Assuming the continuous nature of perception, if we sample as
many as possible different luminance levels, the projected points
of these samples into the perceptual separation space should form
a continuous scale. Therefore, we used a 3D curve fitting to
observe the status of the perceived scale of lightness. We have
three observations as follows.
Firstly, the points are located on a conical spiral curve, in order
of increasing lightness. This indicates that both G and H increase
monotonically. Secondly, the spacing between the points along the
curve (on-path length) is largest at the black and the white end,
and lower in between. This indicates a similar recurvature pattern
as the equal-value gray scales in cartographic studies. Thirdly, the
3D curving indicates a saturation effect that if three points m, n
and o stand for luminance levels with lm<ln<lo, and Dij stands for
the separation distance between points i and j, then Dmo<Dmn+Dno.
As shown in Figure 5, the saturation effect is strongest between
the highest and the lowest level, e.g., D08 is almost equal to D09.
This indicates that the derivative of H decreases monotonically.
5.3
Switching Feature and Blended Perception
As mentioned before, when judging lightness with a white
viewing background two different reference points can be
assumed. It is reasonable to assume that for lighter symbols, the
white end is more likely to be the starting point, and for darker
symbols, the black end gets a higher chance.
We use luminance as the physical specification that is Yi =
4+i·(193−4)/10, i = 0, 1,…, 10, which can be normalized into L =
0.02+li·(1−0.02), li = 0, 0.1, 0.2,…,1, with the normalized
luminance li (defined by (Y−Ymin) / (Ymax−Ymin)) modulating the
perceived lightness of symbols (0 corresponds to black and 1
corresponds to white). Variable di = 1−li modulates the perceived
darkness from the white background (0 corresponds to white and 1
corresponds to black). If both follow a psychophysical power law,
then perception of the symbol lightness L should fit with the
widely accepted CIE L* scale with an exponent β≈0.33, while d
might have another exponent ρ that is probably smaller than 1.
Furthermore, the perceived darkness can also be viewed as
reversely contributing to the perceived lightness by 1−(1−l)ρ. Thus
the two power functions can be added to model lightness
perception simultaneously as:
(4)
P = G(l; β, ρ, u) = (1 – u)·B(l) + u·W(l)
where B(l) = (0.02+l·(1−0.02))β, W(l) = 1−(1−l)ρ, and u controls
the blending weights. Noticing that value 1 corresponds to the
normalized luminance of the white background, the perceived
darkness can thus be transformed into a function of Weber
contrast: W(l) = 1−((1−l)/1)ρ = 1−((lB−lS)/lB)ρ = 1−(−Cw) ρ =
1−|△L|ρ. Therefore the final perception P is described by both the
symbol lightness L~L* and the contrast to the background △L~CW.
It is also possible that the blending weights are not constant
over the whole range. We found high linear positive correlation
between Mij and B(lj)−B(li) when li, lj <0.5 and high linear positive
correlation between Mij and W(lj)−W(lj) when li, lj >0.5. This
indicates that for lighter symbols, B(l) might constitute the main
influence on the perceived lightness; while for darker symbols,
W(l) affects more. In other words, L* works fine when target
luminance is well below the background luminance, and Cw works
when it is close to the background luminance. Thus the two power
functions could be blended with linearly varying weights: 1−l and

110

l, or nonlinearly varying weights: 1−lr and lr where r>1. Therefore
the generic form the blending function with varying weights is:
(5)
P = G(l; β, ρ, r) = (1−lr)· B(l) + lr· W(l)
Both functions allow us to model the recurvature pattern.
5.4

Map the Perceived Difference to the Performance Data

One of our assumptions is that the measured times are negatively
associated with the perceived contrast by subjects. Independent of
the specific form of H, this indicates that the rank order of time
should correlate negatively with the rank order of perceived
lightness contrast. Using task performance Mij = 1/tij, we expect a
highly positive correlation for the rank orders.
Based on the different hypotheses of G in (4) and (5), the
Spearman’s rank order correlation R can be treated as a function of
power-law parameters β and ρ and also the blending weights
parameter u or r:
6 ⋅ ∑ Dij2
Rk = 1 −
(6)
N ( N 2 − 1)
where N=110 (for circles) or 90 (for spots),
k=1,2,…,24 denotes the index of test session per person per task,
and Dij = Rank | G (lj) – G (li) | – RankMij with G(lj) and G(li) follow the
form either in (4) as G(l; β, ρ, u) or in (5) as G(l; β, ρ, r).

Figure 6. Plots of relationship between the perceived lightness
determined by hypothesized G and the task performance M.

For both outlined-circle data and unframed-spot data, we found
that using G(l; β, ρ, u) generally resulted in higher correlation
across all the subjects and tasks than using G(l; β, ρ, r). This
indicates that the fixed blending weights in (4) should be preferred.
We also found that the correlation values are generally far higher
than the critical values of R and approaching the maximum value
of 1, which suggests that our model assumption is sound. At the
same time, we can observe that the ρ values which produce the
high Rk are largely in the range of 0.6–0.9 for both circles and
spots. These results indicate that all the subjects may share the
same exponent ρ, which would confirm the principle of
Homogeneity of Perception [9]. To explore this, we add up all the
Rk calculated on an individual basis. The optimal sum ΣRk is
roughly produced by (β, ρ, u) = (0.33, 0.6, 0.47) for circles, and by
(β, ρ, u)) = (0.33, 0.8, 0.66) for spots.
Figure 7 shows the relation between |G(lj) – G(li)| and Mij in two
different test sessions with G determined by (β, ρ, u) = (0.33, 0.6,
0.47) for circle and (0.33, 0.8, 0.66) for spot. Although there is
quite some variation, the overall pattern suggests that H can
indeed be described by a power-like relationship.
6

MODEL FITTING AND DISCUSSION

The exploration of the data has verified our main assumptions and
given us suggestions for the functions to use. We use nonlinear
regression models to fit the data and to perform inferences [26]. H
is assumed to be in the following power-like [25] form:
(7)
H = a · (( |G(lj) – G(li)| + d )b – db )
where the free parameters b and d denote the nonlinear effect of
the user responses in the discrimination tasks and the threshold of

lightness contrast perception respectively. Meanwhile, G is given
by formula (4) as suggested in our data exploration. The parameter
a is a linear regression parameter, and the complete nonlinear
regression model is:
(8)
Mij = H + εij
where εij is assumed to be normally distributed with zero means
and unknown standard deviation σ.
6.1
Perception Homogeneity and Estimation of G
We can make different assumptions for the free parameters,
dependent on how we share cases, i.e., one shared value for β and
ρ or individual values per subject per task. According to
Maximum Likelihood Theory [25,26], the most appropriate model
should be selected by balancing between the model fit in terms of
the log likelihood and the model complexity in terms of the total
number of free parameters. The AIC index is the most widely
adopted measure for this purpose [27]. We calculate AIC indices
for different models: (βk, ρk, uk, ak, bk, dk, σk), (β, ρ, uk, ak, bk, dk,
σk), (β, ρ, uk, ak, bk, dk, σ), (β, ρ, u, ak, bk, dk, σk) and (β, ρ, u, ak, bk,
dk, σ) where k=1,2, …,24 denotes the individual parameters per
subject per task. The model selection is based on △AIC. The
comparison shows that models with shared β, ρ, and u are
preferred over models with individual βk, ρk, and uk. This indicates
that the internal homogeneity of lightness perception is a sound
assumption. The parameter estimates are as follows.
For circle data, β = 0.3290 with 95% confidence interval (CI)
[0.2986, 0.3624], and for spot data, β = 0.3270 with 95% CI
[0.2257, 0.4198] which completely contains the CI for the circle
data. The estimate coincides with the widely accepted exponent
0.33 for the perceived lightness. It also indicates that the same
lightness perception is involved in the discrimination tasks
regarding the both types of symbols.
For circle data, ρ = 0.6540 with 95% CI [0.6036, 0.7082], and
for spot data, ρ = 0.7810 with 95% CI [0.7211, 0.8463] which is
significantly different from that of circle data since there is no
overlap between the CIs. This indicates that the darkness
perception (also the contrast perception) is different for the two
types of symbols: circles are more difficult to discriminate. The
circles with outlines are much easier to detect from the
background, but it seems that this makes discrimination of subtle
variations in their interior more difficult.
For circle data, u = 0.4263 with 95% CI [0.3901, 0.4633] and
for spot data, u = 0.7422 with 95% CI [0.6922, 0.8243]. This
indicates that the black outline might also influence the way of
blending, i.e., where the recurvature happens. Specifically, the
perception of darkness has larger impact to perceive spots than
circles. This is reasonable since without the black outline, the
object detection relies only on the darkness of symbol fill.
These results largely coincide with what has been observed in
the previous data exploration. Figure 7 and Figure 8 compare our
results with other choices of G in a function plot and sequential
symbol encoding plot. It is worth to remark that for spots, the plot
of G with estimated parameters resembles closely a linear
relationship. Since subjects commented that it is more difficult to
detect objects without outlines, this coincides with a study for
detection purposes [18] that a physically linear gray scale may be
optimal.
The individual variance in task performance is modeled by ak,
bk, dk and σk. Modeling the individual variance aims at a more
precise model approximation to the data. Due to the page limit, we
are not going to present individual estimates. As a remark, the
fluctuation of the estimated values across individual parameters is
limited which suggests that a model with shared a, b and d might

be acceptable. The analysis for such a shared H is presented in the
next section.

Figure 7. The plot of function G in different proposed forms.

Figure 8. Sequences of outlined circles and unframed spots
configured by linear interpolation on different scales assumed. (The
printed colors do not match exactly the colors specified)

6.2
Task Effects and Estimation of H
In our model, H models the performance in the cognitive part of a
particular task. Different tasks do not necessarily share the same
cognitive process, which means that the response behavior
expressed by b and d is expected to vary across tasks. Therefore, it
is reasonable to assume task associated values for the free
parameters of H. Again, different models with shared a, b, and d
across a specific task are compared based on △AIC. Figure 9
presents the estimates for b and d in the case of shared β, ρ, and u.

Figure 9. Estimates of shared b and d in different tasks with their
95% confidence intervals for circle (C) and spot (S).

We can observe that b is not significantly different in T1 and T2
for both circle and spot. However, it is significantly larger for
circle than for spot in T3, which indicates the least extent of the
saturation effect. With the black outlined circles on a white
background, i.e., with both black and white ends of the lightness

111

scale presented as reference, users’ responses might be linearly
regulated. Besides this, there is significant difference of the
parameter d between different tasks. For both types of symbols, d
produces the highest threshold in T2 and the lowest threshold in T3
for lightness discrimination, which indicates that in T2 a
performance limit was approached in case of high-perceived
contrast; while in T3, that was approached in case of low-perceived
contrast. Due to the clusters of the same type of symbols, the task
might become easy for low contrast sets.
Although the analysis of models with task associated parameters
gives a direct insight in the influence of the tasks, the models with
individualized parameters still yield much better model fits. This
indicates that individual variation is strong and cannot be
neglected.
7

CONCLUSIONS

We found a way to model the optimal lightness scale with respect
to equal perceptual separation of symbols on a white screen
background. It is used to generate a blended combination of two
opposite power functions with one modulating the perceived
lightness and the other modulating the perceived darkness (symbol
contrast to the background). For the perceived lightness, the
estimated exponent is the same as the widely accepted 0.33; while
for the perceived darkness, the estimated exponent varies with the
painted circles with a black outline (0.65) and without (0.78).
Moreover, the blending weights show the higher impact of the
darkness perception when discriminating the unframed spots (0.43)
than the outlined circles (0.74). The model successfully combines
the two major determinants of achromatic appearance, namely the
stimulus lightness and contrast to the background. For different
colored backgrounds, the perceived darkness could be extended to
the Weber contrast; for dense plots, the contrast to background
could be modeled by the Michelson model. Meanwhile, the
unframed spots can be viewed as an extreme case of with outlines,
thus the corresponding parameter values might be viewed as
boundaries. The resulting scales might be used to guide the data
encoding of sparse scatterplots with a white background. Further
studies are required to model the interaction between different
visual channels, for instance the size effect on color perception.
In order to obtain this discriminability scale, we aim at models
that depict the internal homogeneity well and handle the
individual variance correctly, which is missing in Stevens’s
method. Individual variance has been modelled and estimated by
using individual cognitive functions H. No evidence was found to
support the more complex model with an individually varied
perception function G.
8

FUTURE WORK

The methodology of modeling presented in this paper can be
extended to other visual channels of symbols and also to other
graphical contexts such as maps, where symbols are heavily used.
It is also interesting to know how the other visual channels of
symbols will affect lightness perception as well as the background
color. Moreover, the idea of blending power functions proposes a
way to understand other types of switching between visual
channels in the perception of symbols, for instance the thickness
of outlines bounded by the sizes of inner parts and the whole.
These could be addressed in future work.
9

ACKNOWLEDGEMENTS

We thank all the participants in our experiment from different
departments of TU/e. The project is supported by the VIEW
program of the Netherlands Organization for Scientific Research
(NWO) under research grant no. 643.100.502.

112

REFERENCES
[1]
[2]
[3]

[4]
[5]
[6]
[7]
[8]
[9]

[10]
[11]
[12]
[13]

[14]
[15]
[16]
[17]
[18]

[19]

[20]
[21]

[22]

[23]

[24]

[25]
[26]

[27]

C. Ware. Information Visualization: Perception for Design, 2nd
Edition. Morgan Kaufmann, SF, USA, 2004.
S.S. Stevens. The psychophysics of sensory function. Amer. Scientist,
48: 226-253, 1960.
S.S. Stevens and E.H. Galanter. Ratio scales and category scales for a
dozen perceptual continua. Journal of Experimental Psychology, 54:
377-411, 1957.
G.R. Williamson. The equal contrast gray scale. The American
Cartographer, 9(2): 131-139, 1982.
R. Carter. Gray scale and achromatic color difference. Journal of
Optical Society of America, 10(6): 1380-1391, 1993.
M. Stone and L. Bartram. Alpha, contrast and the perception of
visual metadata. Proc. Color Imaging Conference, pp. 355-359, 2008.
M. Stone. A field guide to digital color. A K Peters, Canada, 2003.
J.M. Wolfe. Guided Search 2.0: A revised model of visual search.
Psychonomic Bulletin & Review, 1(2): 202-238, 1994.
P.E. Green, Jr.F.J. Carmone and S.M. Smith. Multidimensional
Scaling, concepts and Applications. Allyn & Bacon, Massachusetts,
USA, 1989.
D. Laming. The Measurement of Sensation. Oxford University Press,
NY, USA, 1997.
D.J. Weiss. Averaging: An empirical validity criterion for magnitude
estimation. Perception and Psychophysics, 12: 385-388, 1972.
D. Ariely. Seeing sets: representation by statistical properties.
Psychological Science, 12(2):157-162, 2001.
A.J. Kimerling. A cartographic study of equal value gray scales for
use with screened gray areas. The American Cartographer, 2: 119127, 1975.
A.J. Kimerling. The comparison of equal-value gray scales. The
American Cartographer, 12(2):132-142, 1985.
J.J. Leonard and B.P. Buttenfield. An equal value gray scale for laser
printer mapping. The American Cartographer, 16(2):97-107, 1989.
C. Brewer. Color use guidelines for data representation. Proc.
Section on Statistical Graphics, pp. 55-60, 1999.
C.G. Healey. Choosing effective colors for data visualization. In
Proceedings IEEE Visualization’96, pp. 263-270, SF, USA, 1996.
C. Ware. Color sequences for univariate maps: theory, experiments,
and principles. IEEE Computer Graphics and Applications, pp. 4149, 1988.
M. Wijffelaars, R.Vliegen, J.J. van Wijk, and E.J. van der Linden.
Generating color palettes using intuitive parameters. Computer
Graphics Forum, 27(4): 743-750, 2008.
J. Li, J.J. van Wijk and J.B. Martens. Evaluation of symbol contrast
in scatterplots. Proc. PacificVis2009, pp. 97-104, 2009.
R. Amar, J. Eagan and J. Stasko. Low-level components of analytic
activity in Information Visualization. Proc. IEEE InfoVis, pp. 112119, 2005.
L. Tremmel. The visual separability of plotting symbols in
scatterplots. Journal of Computational and Graphical Statistics,
4(2):101-112, 1995.
L.T. Nowell. Graphical Encoding for Information Visualization:
using icon color, shape and size to convey nominal and quantitative
data. PhD Dissertation, Dep. of Computer Science, Faculty of the
Virginia Polytechnic Institute and State University, 1997.
S. Lewandowsky and I. Spence. Discriminating strata in scatterplots.
Journal of the American Statistical Association, 84(407):682-699,
1989.
J.B. Martens. Image Technology Design: A Perceptual Approach.
Kluwer Academic Publishers: Dordrecht, NL, 2003.
E. Uusipaikka. Confidence Intervals in Generalized Regression
Models. STATISTICS: Textbooks and Monographs. CRC Press:
Finland, 2009.
K.P. Burnham and D.R. Anderson. Multimodel Inference –
Understanding AIC and BIC in Model Selection. Sociological
Methods & Research, 33(2): 261-304, 2004.

