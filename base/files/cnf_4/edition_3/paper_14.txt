Evaluation of Symbol Contrast in Scatterplots
Jing Li*

Jarke J. van Wijk†

Jean-Bernard Martens‡

Eindhoven University of Technology

ABSTRACT
Symbols are frequently used to represent data objects in
visualization. An appropriate contrast between symbols is a
precondition that determines the efficiency of a visual analysis
process. We study the contrast between different types of symbols
in the context of scatterplots, based on user testing and a
quantitative model for symbol contrast. In total, 32 different
symbols were generated by using four sizes, two classes (polygonand asterisk shaped), and four categories of rotational symmetry;
and used three different tasks. From the user test results an
internal separation space is established for the symbol types under
study. In this space, every symbol is represented by a point, and
the visual contrasts defined by task performance between the
symbols are represented by the distances between the points. The
positions of the points in the space, obtained by Multidimensional
Scaling (MDS), reveal the effects of different visual feature
scales. Also, larger distances imply better symbol separation for
visual tasks, and therefore indicate appropriate choices for
symbols. The resulting configurations are discussed, and a number
of patterns in the relation between properties of the symbols and
the resulting contrast are identified. In short we found that the size
effect in the space is not linear and more dominant than shape
effect.
KEYWORDS: Symbol Contrast, Visual Feature Encoding, Symbol
Separation, MDS, Scatterplots, Size Perception.
INDEX TERMS: G.3 [Probability And Statistics]: Experimental
Design; H.1.2 [User/Machine Systems]: Human Factors and
Human Information Processing; H.5.2 [User Interfaces]:
Evaluation/Methodology and Theory And Methods; I.2.10 [Vision
And Scene Understanding]: Perceptual Reasoning; J.4 [Social
And Behavioral Sciences]: Psychology.
1

INTRODUCTION

Information visualization aims to support visual analytic
processes for users. Under limited time and effort, a large amount
of visual stimuli have to be scanned and interesting patterns need
to be discovered and located interactively. This requires an
encoding method such that interesting patterns stand out and that
the contrast between stimuli is proportional to the contrast in the
underlying data.
The scatterplot is a classic and still very effective method for
visualizing multi-dimensional data. Data objects are represented
by symbols, which position is governed by the two spatial axes,
representing two continuous or ordinal data attributes.
Furthermore, different data attributes of objects can be encoded by
*e-Mail: j.li@tue.nl
†

e-Mail: vanwijk@win.tue.nl
e-Mail: j.b.o.s.martens@tue.nl

‡

IEEE Pacific Visualization Symposium 2009
April 20 - 23, Beijing, China
978-1-4244-4404-5/09/$25.00 ©2009 IEEE

different visual features of the symbols, for instance, nationalities
can be indicated by color, weights can be indicated by size, etc.
According to visual perception theories [1, 2], the perceived
contrast of a target relative to the visual context determines
whether the target stands out or not. Therefore, the perceived
contrasts between different symbol types, i.e., the symbol
separation, determines how easy sets of such objects can be
perceived as a whole and discriminated from the other sets of a
different type [8].
We aim at quantitatively modelling the perception of symbol
separation. Our model simulates a human perception process [16],
mapping symbols such that distances between symbols represent
contrasts between these symbols. The contrasts are experimentally
established in the context of basic information visualization tasks,
and based on user test results.
1.1
Visual Attention
Visual attention involves complex perceptual processes.
Traditionally, it is considered to consist of two different
successive sub-processes [20, 21, 24], which are referred to as
“preattentive” and “attentive”. A preattentive process is
characterized by parallel search instead of serial search, and is
therefore more efficient. However, the strict dichotomy of parallel
search and serial search had been doubted and was revised
according to the assumption that search ability varies continuously
[2, 3, 4]. Thus, whether a search process is more parallel or more
serial is determined by the dissimilarities or contrasts between the
target and distracters and between the different distracters [1, 2].
Guided search is a widely accepted theory of early visual
processing. Beyond preattentive features, it studies the triggers of
visual attention. Guided search suggests that an activation map
based on both bottom-up and top-down information is constructed
during visual search [2, 26]. The bottom-up activation measures
how different a presented element is from its neighbours while the
top-down activation is defined by subjective requests. Hills in the
activation map can be intensified or reduced by manipulating
dissimilarities or contrasts between stimuli as well as by
modifying subjective requests. If the hills of target regions stand
out, then a more parallel search is triggered, otherwise a more
serial search is triggered. Summarizing, the theory of guided
search teaches us that contrasts between stimuli are important (as
they determine the bottom-up activation), but also that subjective
requests (for instance, the task to be done) play an important role
as well.
1.2
Related Work
A number of studies on symbol perception have used perceptual
experiments with visual search tasks [5, 22, 25-31]. Visual search
implies perceptual tasks requiring attention. These experiments,
heavily used in research on early visual perception, usually
involve two kinds of tasks: a search task and an identification
task. A search task is to determine the presence of one or more
symbols meeting certain visual criteria and to locate them if
present. An identification task is to report semantic data of the
symbols represented, typically by noting some facts about the

97

Figure 1. Model of the process of testing symbol contrast

encoded data. For a search task, the search time is the standard
measure and for an identification task, accuracy is usually
measured [25].
In the research field of information visualization, the work of
Jock Mackinlay in 1987 [6] is based on the accuracy measure of
perceptual tasks by Cleveland and McGill [7]. The accuracy
measure defines a rank of elementary perceptual tasks for
graphical design. At the same time, Mackinlay stated that since an
empirically verified theory of human perceptual capabilities is
lacking, there are other possible alternative measures beyond
accuracy, such as speed, visual impact or cost. Later on, the time
measure was formally brought forward as a complement to the
accuracy measure [5, 11]. According to Lewandowsky and
Spence [11], both have to be considered to decide on task
performance, because of subject individual differences, i.e.,
different strategies or expertise, which can lead to a different
trade-off between speed and accuracy. This dual-measure issue
might lead to two rank orders, for instance the rank order between
size and shape of Nowell [25], which makes the interpretation
more difficult.
Other studies on symbol perception have concentrated on
symbol structure, particularly perceptual discriminability of
structures. Cleveland and McGill summarized a list of standard
symbol types in statistical graphs [8] and proposed a tentative
rank order for them for scatterplots [9]. Colors are assumed to
provide optimal discriminability, followed by amounts of fill, then
geometric shapes, and finally letters. Two sets of symbol types
were suggested for plots with little overlap and more overlap.
Some evidence to support this rank order may be found in
psychophysical work with respect to topological structure of
symbols [10]. The order was further tested and improved by a
comprehensive analysis based on both accuracy and time
measures [11] of visual analytic tasks. However, only one task
(judging correlation) was used. In a study by Tremmel [12],
accuracy was ignored under certain constraints on the task. The
logarithm of time was input to a two-dimensional MDS analysis.
Symbol contrasts were represented by 2D-distances. A
combination of shape and fill contrast (using redundant coding)
yielded the optimal separation, followed by fill-only contrast, then
by shape-only contrast. Terminators (the number of line ends for
instance, in an asterisk) are a relevant feature to facilitate symbol
separation, next to brightness. However, only one analytic task
(judging symbol amount) was performed and no structural
configuration, for instance clear visual feature scales, was used.
1.3
Conjecture of a Quantitative Model
Our conjecture is that an internal perception space for symbol
exists with an associated scalar distance metric, representing the
contrast or separation between symbols. In other words, each
symbol is represented by a point in this space and the perceived
contrasts between symbols would be represented by the distances
between the points. A large distance between a target point and a

98

distractor point indicates a large perceived contrast, therefore
supporting more accurate and faster task completion. For multitype distractors or targets, the synthesized contrast might be
defined, i.e., for instance a weighted distance between elements
from distractors and targets in the internal separation space, where
the weights are based on frequencies. This might explain the fact
that higher contrast distractors would degrade the visual assembly
of the target group [1].
Since the perceived contrast is evoked by visual feature
differences of the stimulus configuration, a mapping from the
feature configuration space to the internal separation space must
exist. Our ultimate aim is to describe and understand such a
mapping, and if possible, quantify it for further prediction of the
mapping results by other configurations.
2

INTERNAL SEPARATION SPACE

In order to establish the mapping from the visual feature space to
the internal perception space, a quantitative model describing the
perception process is needed (Figure 1). Here, we assume for
simplicity that a visualization is shown to a user with two
different sets of symbols (I and J), which symbols are defined by
their values in the configuration space constructed by the visual
feature scales (f1 × f2 × …× fn). After being processed by the
human perception system, it is assumed that the stimuli are
projected onto an internal continuous space as specific points (xi,
xj), such that the symbol contrast is correspondingly mapped onto
distance dij (= ||xi – xj||). A linear relationship is pursued between
the interstimulus distance dij and the perceived contrast ck,ij. such
that the individual scaling factor sk of sensation amplitude can be
modelled for either different subjects or different test sessions.
For different times of repetition on the kth subject or session, some
noise (σk) will be added into the final perceived contrast strength
and we could assumed it following Gaussian N(dk,ij, σk). A
schematic overview of this mimic process of visual perception is
in Figure1 the bottom line, which is similar as what was used in a
study on visual correlation analysis [16]. To investigate the
internal space, subjects are assigned with tasks and enforced to
give responses on a certain measure scale (refer to Figure 1, top
line). In our case, we aim to use visual analysis tasks that are
representative and that rely on measuring contrasts between
stimuli. If the response scale is continuous, we can assume that
the responses measure the contrasts between stimuli. This will be
further discussed in the rest of the paper.
Our construction of the internal separation space is based on a
quantitative model of image quality evaluation [13]. This model is
built on top of the principle of Homogeneity of Perception (HP)
[14], the Maximum Likelihood theory (ML), and the
Multidimensional Scaling (MDS) approach. The assumption of
HP is that a single multidimensional configuration representing
visual stimuli underlies the perceived strength on every feature
scale for all subjects [14]. In the MDS approach, the relative

positions of object points can be optimized with given dimensions
of the space and fitted with the response data; while the space can
be rotated and scaled in an arbitrarily way. By means of ML
method, the most likely (optimal) values of parameters in a
quantitative model can be estimated. Errors of these optimizations
can be estimated as well.
2.1
Separation Space in the Context of Scatterplots
We consider the basic case for standard scatterplots: symbols on a
white background and within a fixed size area. Both targets and
distractors are symbols and we use only two different types of
symbols (symbol I and symbol J). Two points xi and xj in the
internal separation space represent the two types of symbols,
while the distance ||xi – xj|| between the two points represents the
perceived contrast between symbol I and J. The visual features
under study are those frequently used in current statistical and
business graphics, and in visualization tools (such as Excel,
Matlab, and SPSS), and are discussed in the next subsection.
One assumption underlying the above model is the (as yet
unconfirmed) feature symmetry. Suppose that the number of
symbols of one type is different from the number of symbols of
the other type in some analytic tasks. This might influence the
measured contrast, and hence, the contrast when group I is larger
than group J might be different from the contrast when group J is
larger than group I, which relates to the well-known asymmetry
discussion in psychology [22, 23]. As a result, the distance
between point xi and xj in the internal separation space would have
two lengths corresponding to the two directions, and such a space
cannot be modelled by a normal meaningful space. This has
therefore to be verified before further analysis is done (refer to the
section on data analysis).
2.2
Interesting Features
Shape is a frequently used feature to construct different symbol
types in scatterplots. In for instance Microsoft Excel, there are
basically two families of shapes: convex polygons and asterisks
(also called sun-flower symbols [9] or terminators [12, 17, 18]).
One choice to characterize the elements of the families is the
number of sides (polygons) or line segments (asterisks). We prefer
to use angle of rotational symmetry here, since this can be used
for both families, and also because the difference in shape is better
represented on this scale. To limit the number of different
symbols, we do not take the rotation angle itself into account yet,
hence we for instance only consider squares and not diamonds.
A very important scale is the size of symbols, since size is
believed to be the second dominant feature after color [9, 25]. We
did not use color in the current stage, because color is believed to
be the most dominant and might block out interaction effects in
the internal space. Size can be either denoted by the radius or the
area of the enclosing circle (the minimum circle containing the
symbol), so the scale could be the radius scale or its square.
However, for the same enclosing circle, a triangle occupies much
less area than a circle. If the perceived contrast is determined by
the real occupation instead of the enclosing circle, then the
perceived contrast between different polygons is a compound
result of the symmetric rotation angle and the enclosing circle
size. In other words, there is interaction between the two scales.
2.3
Practical Measures in Visual Analytics
Once the symbol contrast is perceived by human mind, it can be
reflected by either direct sensation judgment or by task
performance. In other words, humans can express a judgment as
to what extent two types of symbols look different for them, and

they can also perform a visual analysis task given the two types of
symbols. We can measure their perceived symbol separation
subjectively or measure how well it affects their task
performances. Both can be used as input for the description of the
internal distance between the symbol points (or say the relative
positions of these points). Since we aim at improving the process
of visual analytic tasks, the task performance is the preferred
measure here. Matching with the assumption on the continuity of
internal space, we use time to construct the continuous response
scale under the control of task accuracy (see section 3.2).
Therefore, a quickly accomplished task indicates a high perceived
contrast and therefore a large distance and vice versa.
One concern here is the influence of different individual
capabilities and strategies on time, a well-known issue in
perception experiments and crucial to our model. Different people
might have different skills in judging graphical configurations,
and they might employ different strategies to finish the tasks: a
subject might be faster than others but make more errors. Some
previous experiments [12] handled the strategy variance by telling
subjects to keep error rate as close as possible to a standard. We
aim to resolve this issue by normalizing the data per person to
remove skill and strategy variances. This still requires an
assumption that the skills and the strategy of an individual user do
not change during one test session. This will be further expatiated
in section4 on data analysis.
Once the response scale is defined, the MDS approach can be
used to approximate the internal separation space via relative
positions of every pair of symbol types. Next we analyze the
influences of individual feature scales on the perceived contrast,
their interactions.
2.4
Power-Like Transformation
There is a long discussion in psychology about the relationship
between the physical magnitudes of stimuli and the perceived
intensity of the stimuli. The classic Weber–Fechner law [15]
models a logarithmic relation. Steven rebutted this and
recommended the use of power functions, and he has provided a
large number of experiments to support this [15]. A revised
version of the power function is the power-like function [13],
which has been proved to be very sound in practice. The powerlike function models two truths of perception: firstly, there is a
threshold of physical magnitude that can be perceived; secondly,
the capability of human perceiving stimuli is limited. It
furthermore assumes a monotonic relationship. The power-like
function is defined by
( x + xt ) q − xtq
(1)
P( x ; q, xt ) = sgn( x) ⋅
q
where P denotes the perceived intensity of a stimulus x, xt stands
for the threshold, and the exponent q is similar to Steven’s powerlaw. To preserve the same range [m, M] as the input data, we
apply a linear transformation and use
T ( x ; q, xt , b) =

( M − m) ⋅ P ( x − b ) − M ⋅ P ( m − b) + m ⋅ P ( M − b )
P ( M − b) − P ( m − b )

(2)

where b within [m, M] is the origin.
After being transformed, the difference between the response
data and the model predictions should have an approximately
Gaussian distribution. The transformation is embedded into our
quantitative model as a step of the whole process (see Fiture1),
and xt, q and b will be optimized together with other parameters.
In our study, response time is used for measurement instead of
sensation judgment. The experiment data show that the

99

Figure 2. Some examples of configurations shown to participants for the three different tasks

power-like transformation applies to the time response as well,
because the optimized after-transformation distribution is close to
Gaussian. This supports the assumption that the response time is
determined by the perceived contrast strength. The MDS approach
can be applied to the data after they are power-like transformed.
3

EXPERIMENT

The general set-up of the viewing context is based on the practical
use of scatterplots. Normal printed scatterplots have symbols
sized between 1-3 mm (for normal healthy eyes, the visual acuity
is about 0.4 minarc of the visual field, therefore the visible
boundary of the symbol size for 100cm reading distance is
0.12mm). The normal reading distance is 35cm. Now on the PC
screen, because of the resolution constraints, symbol size requires
to be larger than paper prints. At the same time, the reading
distance is also normally further away. In our set-up, we keep a
reading distance 100cm, therefore, the symbol size range should
be within [2.86, 8.57]mm. The scatterplots are displayed at the
center-to-left part of an LCD PC screen (Dell 17’’) in an area
sized 25×25cm2, and symbols are randomly positioned in the
plotting area. Every plot only contains two types of symbols.
30 participants joined the tests including 8 females. They were
all university Ph.D. students or faculty from different
departments, between 23 and 49 years old (the average age was
30) and with varied nationalities (7 different nationalities).
3.1
Stimulus Configuration
As we discussed in section 2.2, we selected symbol size and
symbol shape as interesting feature scales, where within symbol
shape we distinguish between rotational symmetry and family
(polygon and asterisk).
Four levels of size were selected on the radius scale r of the
enclosing circle: r =1.25, 2.5, 3.75, and 5 mm. We denote them by
t(iny), s(mall), m(edium), and l(arge).This ratio scale is believed
to be the best to describe the magnitude of human sensation [15].
We use four levels here as a compromise between accuracy and
keeping the total number of symbols within reasonable bounds.
For polygons, a pilot study showed that a hexagon (a 6 sided
polygon) at 5mm cannot be reliably distinguished from a circle.
Therefore, we selected the triangle, square, pentagon and circle
(n=3, 4, 5, infinite, n denotes the number of sides). In terms of the
angle of rotational symmetry, the selected stimuli correspond to
120, 90, 72, and 0 degrees.
For asterisks, we found that the difference between 6-ends and
7-ends could not be distinguished reliably by users in pilot tests
and higher number of ends brings in brightness effect obviously.

100

Therefore, we selected four types of symbols (n’=3, 4, 5, 6, n’
denotes the number of ends), or, with rotational symmetry angles
of 120, 90, 72 and 60 degrees.

Figure 3. Symbols used, arranged by size and shape (rotational
symmetry and family). Printed in the real experiment size.

3.2
Three Visual Analytic Tasks
We selected three visual analysis tasks, based on the literature
[32]. These tasks are fundamental and frequently involved in more
advanced and complex visual analysis. In all cases fifty symbols
are shown, using two different types of symbols, but different
tasks have to be carried out (Figure 2):
Task1. Determine which type of symbol is most often used;
Task2. Search and count the number of outliers;
Task3. Determine which type of symbol is more clustered.
The last task can also be interpreted as identification of the
symbol distribution with the smaller standard deviation. These
tasks are set up such that no top-down process can occur, since the
target types are not known before seeing the plot. All the three
tasks are intended to be accomplished in a very short time, i.e.,
they must be performed as quickly as possible while keeping the
same strategy throughout a test session.
We fixed the total number of symbols in each plot to 50, such
that about 10%-25% of the plotting area is occupied, a number we
think to be representative for routine cases. To this end, the
plotting area is first divided into a uniform grid with 20×20 cells,
labelled with integers. Next, the symbols are drawn into a
randomly selected cell, giving a pseudo uniform distribution. In
this way, we avoid symbol overlap. Within a cell, the symbol is
not always positioned centrally and noise is randomly added to 8
directions radiating from the centre of the grid, in order to avoid
that the regular grid shows up and that symbols are positioned on
lines. The distribution of symbol types over these fifty symbols is
different for the three tasks. For Task1, the larger group contains

35 symbols, the smaller group contains 15. This is a similar
choice as was made by [12], and indeed, such a large gap yields
quite small and stable error rates in our pilots, and therefore yields
relevant time measurements. Both groups of symbols are
distributed randomly over the plot. For Task2, the outlier counting
task, the outlier group only contains 1-5 symbols (following
subitizing theory), and the main group contains the remaining
symbols. To eliminate the influence of Fitts’s Law on eye
scanning, we constrain the distances between outliers to be within
[1, 4] cm but still randomize the relative positions in different
directions. For Task3, the cluster degree comparing task, both
groups contain 25 symbols, and both are plotted randomly, using a
pseudo Gaussian 2D distribution. For the standard deviations we
used 2.5cm and 10cm (1:4) respective to the two sets of symbols.
As we verified with a pilot, this large difference led to a
reasonably small and stable error rate.
3.3
Test Sessions and Test Procedure
The configuration choices produce 32 different symbol types: 4
size levels × 8 shapes (4 for polygons, 4 for asterisks). These 32
types produce 32 × (32 – 1) / 2 = 496 different pair combinations.
Considering the asymmetry issue (section 2.1), we also have to
swap the roles of the two symbol types in each of the three user
tasks. This produces 496 × 2 = 992 plots per task (Figure 4),
which is too much for one test session. Hence we divide them into

session, for a particular plot group, the display order of plots is
also randomized.
Pilot tests showed that subjects can finish one session within 15
minutes, but with a large variance among individuals (session
with 120 plots: 3~8 minutes and session with 256 plots: 6~12
minutes). This indicates that different persons do use different
strategies to perform the task. No breaks are allowed in one test
session to avoid strategy shifting, and subjects are also instructed
before the test to keep the same strategy and trained in a trial
session with all the tasks but different data sets. Later on, the data
per subject are normalized per test session. However, breaks are
required between trials to relax the eyes. During a test session, a
plot is displayed when the subject presses the space button on the
keyboard. Once the subject knows the answer, he or she should
press the space button again immediately. Next, the plot is
removed and the user gets access to the input interface. After
inputting the answer, he or she can view the next plot until all the
plots have been viewed. The response time between the two times
of pressing the space button is recorded.
4

ANALYSIS

The data of the response times cannot be directly used for
analysis. It requires inversion, since shorter time indicates better
discriminability and larger distance. Also, normalization is
required to cope with differences in skills and strategy between
subjects. Therefore, we use the following pre-transformation (see
Figure 1):

Ct = C (t k , ij ) =

t k , ave

(3)

t k , ave + t k , ij

where tk, ave is the average time per subject per session. The value
of C(t) is monotonically decreasing from 1 to 0 exclusively and
transforms the average time value right to 0.5. These values are
used as input for the XGMS Tool [13], the tool we used for model
analysis, which supports power-like transformation and MDS.

Figure 4. Stimulus matrix: each cell represents a testing plot with
one symbol type indicated by the row and the other type by the
column (excluding the diagonal cells)

six groups: the first half of polygons configured by size and angle
of rotational symmetry (SP1: 120 plots) and the second half of
them (SP2: 120 plots); the first half of asterisks configured by size
and angle of rotational symmetry (SA1: 120 plots) and the second
half them (SA2: 120 plots); the first half of combinations of
polygons and asterisks with varied size and angle of rotational
symmetry (SPA1: 256 plots), and the second half of them (SPA2:
256 plots). Every subject performs all the three tasks, but only
picks up one plot group for that particular task. The plot groups
picked are also varied. Hence, we designed eight codes for the test
sessions. Subjects are randomly assigned to one of the eight
codes, and the session order is randomized as well. In every

4.1
Symmetry
Before MDS estimation, we need to verify if the role swap of
symbol pairs results in symmetric time responses. A symmetric
model is firstly constructed by averaging the sum of all the data of
the same symbol pair in repeated observations. Then we test the
goodness-of-fit of the symmetric model. The correlation
coefficients between the symmetric model predictions and the
original data of all the observations are 0.93.
The high correlation between the symmetric model and Ct
shows that the symmetric model fits the data quite well and
therefore we can use MDS for further analysis, using the
symmetric model as input.
Table1 Correlation Coefficients between the Model and Ct
Task
Model
Task1
Task2
Task3
All-tasks
symmetric model

0.93

0.96

0.93

0.93

4.2
Three Dimensional Scaling
A lower dimensional model has fewer parameters (freedom) and
is easier to interpret, however gives larger errors. As a corollary, a
higher dimensional model has larger freedom and leads to smaller
errors, but can be very complex and difficult to interpret. The
dataset itself could be taken as the model with highest freedom
and is a perfect description with no error, but tells nothing of the
general pattern. An appropriate model should balance its freedom

101

with estimation error. To find an optimal model we have applied
MDS with 1 to 9 dimensions. The resulting errors of the n-D
models for Ct estimation are shown in Figure 5.
We see that that the error drops significantly when going from 2
to 3 dimensions. Increasing the number of dimensions leads to a
further reduction of the error, but not as dramatic as compared
with the error drop from 2 to 3 dimensions. Since 4 dimensions
are much more complex, we use the results of 3D MDS for our
analysis. The 3D model has an average correlation coefficient of
0.91, which indicates a fairly good fit with the data.

distance between any two points is smaller than adding up the
distances from a middle point to both of them. On the other hand,
the shortened segments of the polyline might imply that size is not
mapped (either linearly or non-linearly) to a single scale, but that
in judging the perceptual difference between symbols, both of
their sizes must be taking into account.

Figure 5. Estimation Errors of the n-D model (T1 – Task1, T2 –
Task2, T3 – Task3, and all – all the tasks)

For each task, a separate analysis shows similar patterns. Since
we aim at a general model, we therefore used a combined
analysis. In Figure 6, results of the 3D MDS are shown, with a
close view of polygon symbols – (a), a close view of asterisk
symbols – (b) and an overview of all symbols simultaneously –
(c). Symbols of the same type and with varying sizes are given the
same color and are connected by lines. The results are intriguing.
On one hand, the resulting structure is complex, and hard to
understand, especially from 2D projections of the 3D positions.
On the other hand, structures and patterns do appear. In the
following we consider the various feature scales separately.
4.2.1
Size Effect
The 3D MDS results show that the mapping of symbol types into
the internal separation space is rather complex (not linear for size
and shape). Further, a strong pattern can be observed for the size
scale for all different shapes: All polylines seem to be 2D curves.
We have performed separate MDS estimations for each kind of
symbol type and varying sizes. It turns out that there is a strong
decrease in error when going from 1D models to 2D models, but
that the step from a 2D models to a 3D models does not lead to
significantly smaller errors. Therefore, for different shapes, we
conclude that the size scale produces a 2D pattern. Figure 7 shows
superimposed projections of such 2D patterns for the polygon and
asterisk families.
It can be observed that the 2D pattern of size is uniform within
the polygon family and within the asterisk family, but changes
somewhat between the two families. This could be interpreted as
no interaction between the size and the angle of rotational
symmetry, and some interaction between the size and the shape
family (polygon or asterisk). The order of the symbols along the
polyline is according to the linear size of the symbols. However,
the polyline is bending in 2D to the same side (clockwise or
counterclockwise). At the same time, the segments along the
polyline seem to be shortened gradually to the larger sizes. On one
hand, the bending effect shows that the perceived contrast of
symbols with increased size difference is impaired since the

102

Figure 6. 3D MDS Results. (a) rotations of polygons (b) rotations of
asterisks (c) rotations of all symbols simultaneously

Figure 7. Optimal superposition of size scale for different shapes:
(a) polygons, (b) asterisks.

To analyze this effect, we have calculated the average
perceptual distances of symbol combinations with the same pairs
of size levels. Next, we distinguish the increasing steps between
these size levels: 1 (t-s, s-m, m-l), 2 (t-m, s-l), and 3 (t-l), and
calculated the average distances of each category. The average
distance increases with increasing steps, which is the dominant
tendency. But also, the average distance usually decreases within

the same number of steps when the absolute size level is
increasing. Thus the size separation of symbols is influenced by
two factors, the size difference between the symbols and the
absolute size level. Moreover, the size difference has a strong
positive impact, and the absolute size level has a weakly negative
impact.

that the distance between the circle and the pentagon is almost the
same as the distance between the pentagon and the square,
although circle has an infinitely small symmetric rotation angle
(which is expected to be farther away). For asterisks, the distance
between 4-ends and 5-ends is almost the same as distance between
3-ends and 6-ends.

Figure 8. Perceptual distances between symbols for different size
steps and families

One could argue that a similar effect would be obtained if for
instance the perceptual distance would be modeled by D(r1, r2) =
|log(r2/r1)| = |log(r2) – log(r1)|. But by using this, the sizes are
mapped to one scale (the logarithmic scale), and this should lead
to straight lines in the MDS-plots (with the points non-uniformly
distributed), and they are clearly not true.
A first step to a more appropriate model is to use a function g(r)
for the absolute size effect, and s( |r1 – r2| ) for the size difference,
where g should be monotonically decreasing and s should be
monotonically increasing. Notice that s controls the main
tendency. Therefore the mapping function for size might be given
by D(r1, r2) = s( |x2·g(x2) – x1·g(x1)| ). However, we have only
four levels for the size in our experiment. To further depict the
mapping function more test data is needed. This will be addressed
in our future work.

To explore the third hypothesis, we applied K-means clustering
to all symbols at the same size levels, requesting two clusters. The
result is that polygons and asterisks indeed are divided over two
clusters. This indicates that the distances between the polygon
family and asterisk family are larger than those within both
families.

4.2.2
Shape Effect
The patterns for shape are less strong than for size. However, we
can make three observations:
1. Symbols with the same size but different angles of rotational
symmetry lead to a tetrahedron-like object within the polygon
family or asterisk family, and there is no standard order for
different angles;
2. Particularly for the asterisk family, the object is more likely to
be a 2D quadrilateral;
3. Polygons and asterisks are two clear clusters especially for
larger sizes.
To explore the first two observations further, we have taken the
distance data of all the different angles (within polygon family or
asterisk family) for every level of symbol size and run MDS
optimizations. This showed that a 2D model suffices for the
asterisk family. For the polygon family, a 3D model does reduce
the error, but with quite a large p-value (0.04). Therefore, we
could conclude that for both families, the symmetric rotation
angle scale produces a 2D effect in the perceptual separation
space.
The optimal positions for both shape families are presented in
Figure 9. It shows that the symbols are not always aligned along a
curve, but zigzagging. If we look back to Figure 7, we see that the
square and the plus-asterisk behave quite different from the other
symbols. Therefore, the zigzag patterns might be also due to these
two outliers. When these two outliers are removed, the order
retains the same as the angle order. Another interesting result is

Figure 10. Average size contrast of each shape (red), average
polygon contrast and average asterisk contrast of each size (blue),
and average between-family contrast of each size (yellow)

Figure 9. 2D MDS results for symbols per size

4.2.3

Size Dominance

In the previous analysis, we found that the shape has little
influence on the size. However, the inverse is not true. In Figure
10 we show the average size contrast of each shape in red, and we
see that it is quite constant for all shapes. This indicates that the
perceived size contrast is barely influenced by shape difference.
The blue part presents the average contrast of all rotational
symmetry angles (either in polygon family, denoted by P or
asterisk family denoted by A) for each size level. A clear positive
correlation with size can be observed. This indicates that the
perceived contrast between polygons or between asterisks is
positively influenced by size increase. Furthermore, the yellow
part shows the average contrast between the two families for each

103

size level. We see that the positive impact of size remains between
the two families.
The interaction between size and shape seems to be
asymmetric. This is probably because size has a larger weight
than shape in perception of the differences. A simple multiple
regression analysis produces effect coefficients on symbol
contrast: 0.8215 for the size effect and 0.0048 for rotational
symmetry angle effect. In practice, we always judge first whether
something is presented or not, and then characterize the details if
it is perceived. When the size is small, we might omit it. When the
size is large, we start to scan it for further categorization.
However, there is also a tendency that the size impact fades out
with increasing size.
5

CONCLUSIONS

The uniform bending pattern of size indicates that size difference
is not linearly perceived. Further, the size separation is impaired
with size increasing. However, the order on the size scale is kept.
For different shapes, a better separation exists between the
polygon family and asterisk family than within the families. The
distance between symbols does not follow the order of the
rotational symmetry angle. The square and the plus-sign behave
singularly.
Shape has little influence on the size separation. However, size
affects shape separation positively like an amplifier but the impact
fades out quickly when the size increases.
We view the intuitive interpretation of the current results as a
confirmation that our methodology is sound. The next step will be
to carry out more experiments to generate more data as input for a
refined model of the perception of differences in size as well as
task effect. After this, we will aim at an extended and integrated
quantitative model for more symbol features.
REFERENCES
[1]

C. Ware. “Visual Attention and Information that Pops Out”,
Information Visualization: Perception for Design, 2nd Edition, , pages
151-199, Morgan Kaufmann, San Francisco, 2004.
[2] J.M. Wolfe. Guided Search 2.0: A revised model of visual search.
Psychonomic Bulletin & Review, 1(2): 202-238, 1994.
[3] J.M. Wolfe, K.R. Cave and S.L. Franzel. Guided Search: An
alternative to the feature integration model for visual search. Journal
of Experimental Psychology: Human Perception & Performance
15(3): 419-433, 1989.
[4] J.M. Wolfe and K.R. Cave. Deploying visual attention: The Guided
Search model. In AI and the Eye, T. Troscianko and A. Blake, Eds,
pages 79-103. John Eiley & Sons, Inc. Chichester, United Kingdom,
1989.
[5] J.M. Wolfe. Visual Search. Attention, J. Pashler Eds. London, UK:
University College London Press, 1998.
[6] J. Mackinlay. Automating the Design of Graphical Presentations of
Relational Information. ACM Transactions on Graphics, Vol. 5, No.
2, pages 110-141, 1986.
[7] W.S. Cleveland and R. McGill. Graphical Perception: Theory,
Experimentation, and Application to the Development of Graphical
Methods. Journal of the American Statistical Association, Vol. 79,
No. 387, pages 531-554, 1984.
[8] W.S. Cleveland. The Elements of Graphing Data. Pages 160-164,
AT&T Bell Laboratories, Murray Hill, New Jersey, 1985.
[9] W.S. Cleveland and R. McGill. The Many Faces of a Scatterplot.
Journal of the American Statistical Association, Vol. 79, No. 388,
pages 807-822, 1984.
[10] L. Chen. Topological Structure in Visual Perception. Science, 218,
pages 699-700, 1982.

104

[11] S. Lewandowsky and I. Spence. Discriminating Strata in
Scatterplots. Journal of the American Statistical Association, Vol.
84, No. 407, pages 682-699, 1989.
[12] L. Tremmel. The visual separability of plotting symbols in
scatterplots. Journal of Computational and Graphical statistics, Vol.
4, No. 2, pages 101-112, 1995.
[13] J.B. Martens. “Psychophysical Measurement and Modelling of
Image Quality”, Image Technology Design: A Perceptual Approach,
, pages 193-286. The International Series in Engineering and
Computer Science. Kluwer Academic Publishers: Dordrecht, 2003.
[14] P.E. Green, F.J. Carmone Jr. and S.M. Smith. Multidimensional
Scaling, Concepts and Applications. Allyn & Bacon, Boston,
Massachusetts, 1989.
[15] D. Laming. The Measurement of Sensation. Oxford University
Press, 1997.
[16] J. Li, J.B. Martens and J.J. van Wijk. Judging Correlation from
Scatterplots and Parallel Coordinate Plots. Palgrave Journal:
Information Visualization: advance online publication. doi:
10.1057/palgrave.ivs.9500179, 1 May 2008.
[17] B. Julesz. Textons, the Elements of Texture Perception and Their
Interaction. Nature, 290, pages 91-97, 1981.
[18] B. Julesz. A theory of Preattentive Texture Discrimination Based on
First-Order Statistics of Textons. Biological Cybernetics, 41, pages
131-138, 1981.
[19] B. Julesz. A Brief Outline of the Texton Theory of Human Vision.
Trends in Neuroscience, 7(Feb.), pages 41-45.
[20] A. Treisman and G. Gelade. A Feature-Integration Theory of
Attention. Cognitive psychology 12, pages 97-136, 1980.
[21] A. Treisman. Preattentive processing in vision. Computer vision,
Graphics and Image processing 31, pages 156-177, 1985.
[22] A. Treisman and S. Gormican. Feature Analysis in Early Vision:
Evidence from Search Asymmetries. Psychological Review. Vol. 95,
No. 1, pages 15-48, 1988.
[23] R. Rosenholtz. Search Asymmetries? What Search Asymmetries?
Perception & Psychophysics, Vol. 63, No. 3, pages 476-489(14),
2001.
[24] C.G. Healey, K.S. Booth and J.T. Enns. High-Speed Visual
Estimation Using Preattentive Processing. ACM Transactions on
Human Computer Interaction, 3(2): 107-135, 1996.
[25] L.T. Nowell. Graphical Encoding for Information Visualization:
using icon color, shape and size to convey nominal and quantitative
data. PhD thesis. Faculty of the Virginia Polytechnic Institute and
State University.
[26] M.J. Proulx. Bottom-Up Guidance in Visual Search for
Conjunctions. Journal of Experimental Psychology: Human
Perception and Performance, Vol. 33, No. 1, pages 48-56, 2007.
[27] R.E. Christ. Research for evaluating visual display codes: an
emphasis on color coding. In Information Design: The design and
evaluation of signs and printed material. R. Easterby & H. Zwaga
Eds, pages 209-228. John Wiley and Sons Ltd., New York, 1984.
[28] R.M.T. Jubis. Coding effects on performance in a process control
task with uniparameter and multiparameter displays. Human Factors,
32(3): 287-297, 1990.
[29] L. Smith and D. Thomas. Color versus shape coding in information
displays. Journal of Applied Psychology, 48(3): 137-146, 1964
[30] J. Theeuwes. Perceptual selectivity for color and form. Perception &
Psychophysics. 51(6): 599-606, 1992.
[31] R. Rauschenberger and S. Yantis. Perceptual Encoding Efficiency in
Visual Search. Journal of Experimental Psychology: General. Vol.
135, No. 1, pages 116-131, 2006.
[32] Amar R, Eagan J, Stasko J. Low-level Components of Analytic
Activity in Information Visualization. IEEE Symposium
on Information Visualization 2005 (Minneapolis, USA), IEEE,
Computer Society Press: Chicago, pages 111–117, 2005.

