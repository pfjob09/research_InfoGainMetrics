Structure-Aware Viewpoint Selection for Volume Visualization
Yubo Tao1∗

Hai Lin1†
1 State

Hujun Bao1‡

Gordon Clapworthy2¶

Key Lab of CAD&CG, Zhejiang University, P. R. China
2
University of Bedfordshire, UK

A BSTRACT
Viewpoint selection is becoming a useful part in the volume visualization pipeline, as it further improves the efficiency of data understanding by providing representative viewpoints. We present two
structure-aware view descriptors, which are the shape view descriptor and the detail view descriptor, to select the optimal viewpoint
with the maximum amount of the structural information. These
two proposed structure-aware view descriptors are both based on
the gradient direction, as the gradient is a well-defined measurement of boundary structures, which have been proved as features
of interest in many applications. The shape view descriptor is designed to evaluate the overall orientation of features of interest. For
estimating local details, we employ the bilateral filter to construct
the shape volume. The bilateral filter is very effective in smoothing local details and preserving strong boundary structures at the
same time. Therefore, large-scale global structures are in the shape
volume, while small-scale local details still remain in the original
volume. The detail view descriptor measures the amount of visible details on boundary structures in terms of variances in the local
structure between the shape volume and the original volume. These
two view descriptors can be integrated into a viewpoint selection
framework, and this framework can emphasize global structures or
local details with flexibility tailored to the user’s specific situations.
We performed experiments on various types of volume datasets.
These experiments verify the effectiveness of our proposed view
descriptors, and the proposed viewpoint selection framework actually locates the optimal viewpoints that show the maximum amount
of the structural information.
Index Terms:
I.3.3 [Computer Graphics]:
Generation—Viewing algorithms
1

Feng Dong2§

Picture/Image

I NTRODUCTION

In recent years, GPU-accelerated direct volume rendering has become the major technique in volume visualization, and a large number of classification methods[10], using domain-specific knowledge
and the information extracted from the volume itself, have also been
proposed to identify and analyze features of interest. These developments make volume visualization more effective in acquiring
complex structural information inside the volume. However, information acquired from the 2D rendered image is not only dependent
on the classification, but it also relies on the viewpoint. This is
because structures inside the volume may be occluded by others
from some viewpoints and this clearly influences the information
amount presented in the 2D rendered image. Although the user can
interactively rotate the volume to gain more information, a user, not
∗ e-mail:

taoyubo@cad.zju.edu.cn
lin@cad.zju.edu.cn (Corresponding Author)
‡ e-mail: bao@cad.zju.edu.cn
§ e-mail: Feng.Dong@beds.ac.uk
¶ e-mail: Gordon.Clapworthy@beds.ac.uk
† e-mail:

IEEE Pacific Visualization Symposium 2009
April 20 - 23, Beijing, China
978-1-4244-4404-5/09/$25.00 ©2009 IEEE

familiar with computer graphics, may be lost in 3D viewpoint interaction due to the great degree of freedom compared with 2D image interaction. Moreover, viewpoint selection for the time-varying
volume dataset is more pressing, since it is labor-intensive to find
a representative view with the maximum amount of information at
each time step. It is necessary, therefore, to suggest optimal viewpoints for the initial exploration and overview of volume datasets
in the volume visualization pipeline for general users. The suggestion of the optimal viewpoint could avoid the non-intuitive trialand-error viewpoint search process, and provide the representative
view for fast browsing through a large number of volume datasets.
The integration of viewpoint selection into the volume visualization
pipeline also follows good human-centered interactive design.
Although the evaluation of the optimal viewpoint is dependent
on the objective, we study this issue from the point of view of information theory. Hence, the goal of viewpoint selection in this
paper is to maximize the information amount of features of interest
embedded in the 2D rendered image. In most cases, the information about features of interest is equivalent to the boundary information, especially boundary structures. This results from the fact that
the user is usually interested in boundary regions between homogeneous materials, which has been accepted in data classification[11].
The well-defined property to detect boundary regions is the gradient. The magnitude of the gradient is usually used to identify
boundary regions, and the gradient direction is also helpful for the
specification of transfer functions[17, 27]. The gradient direction
as the normal is also used for computing shading, as it reflects the
orientation of local boundary structures. As a result, the gradient
direction is a representative property for evaluating information of
boundary structures.
In the area of volume visualization, several evaluation metrics of
viewpoint quality have been proposed to quantitatively measure the
information displayed on the rendered image, such as surface area
entropy[23], voxel entropy[2], opacity entropy[9], etc. While these
previous view descriptors could only capture global features of interest, this paper proposes the shape view descriptor and the detail
view descriptor to estimate global structures and local details, respectively. This is because volume datasets are not only a simple
aggregate of smoothed parts, but also they may contain local details on boundary structures, especially the highly detailed volume
dataset. For these volume datasets, the optimal viewpoint should
also take local details into account.
As global structures and local details are evaluated separately,
we first employ the bilateral filter[20] to construct the shape volume, separating global structures from local details. The bilateral
filter can smooth local details on the boundary structures while preserving strong boundary structures. Therefore, large-scale global
structures are in the shape volume, while small-scale local details
still remain in the original volume. This decomposition is inspired
by edge-preserving image smoothing in computational photography. It is a valuable tool to decompose an image into a smooth base
layer and a residual detail layer for many applications, such as HDR
tone mapping[6]. Although there are many smoothing techniques
for this purpose, such as PDE-based anisotropic diffusion[21],
weighted least squares[15], and the bilateral filter[20], we make use
of the bilateral filter to construct the shape volume, as the bilateral

193

filter is very simple and fast enough, and more importantly it could
extract details at different scales.
We use the concept of the viewing sphere[9], namely all viewpoints are located on the surface of the sphere with the center coincide with the volume center. Our shape view descriptor measures
the distribution of the relative angle between the viewing direction
and the gradient direction of the shape volume. The assumption under this evaluation metric is that the optimal viewpoints are always
at the normal direction[22, 3]. The reason is that if more normals
are towards the same direction, more boundary structures are visible from that direction. Hence, the shape view descriptor takes
the viewpoint with larger visible boundary structures and even relative orientation to the viewing direction as the optimal viewpoint.
On the other hand, our detail view descriptor is designed to deal
with the visible detail amount in terms of variances in corresponding gradient directions between the shape volume and the original
volume. As the elimination of local details in the shape volume
results in great changes in orientation of local structures, large variances between these corresponding gradient directions imply the
existence of many local details. The detail view descriptor prefers
larger amount of detailed local features.
The viewpoint selection framework integrates the shape view descriptor with the detail view descriptor to locate the optimal viewpoint with the maximum amount of information about global structures and local details of features of interest for detailed volume
datasets. In addition, the shape view descriptor based on the original volume alone is also valid for smoothed volume datasets.
The rest of the paper is structured as follows: we characterize
the novelty of the proposed view descriptor through a review of
related previous work in section 2. The shape view descriptor is
first introduced in detail in section 3. In section 4, the separation of
large-scale global structures and small-scale local details based on
the bilateral filter is first discussed, and then we present the detail
view descriptor, and the integration of the shape view descriptor
and the detail view descriptor is described finally. In section 5, we
show and discuss several examples of volume datasets as well as
their optimal viewpoints using these proposed view descriptors.
2

R ELATED W ORK

Viewpoint selection is a widely investigated research area in many
fields. The aspect graph describes the relationship between the
topological equivalent viewpoints [12], and has been used for automatic object recognition in computer vision [5]. The “canonical
views” defines a small number of user-preferred viewpoints by the
psychophysical experiments of Blanz et al.[1].
Viewpoint selection is also a research topic in computer graphics,
and a large number of view descriptors have been proposed. A recent comprehensive exploration of viewpoint selection for polygon
mesh can be found in [22], and three principles (view-independent,
view-dependent, and semantic meaning) are employed to classify
these view descriptors. The surface area entropy was first proposed
by V´azquez et al. [25]. Each face is assigned with a probability,
which is the ratio of projected area of the face to total projected
area, and the optimal viewpoint is the one in the case that all visible faces have equal area. Besides the surface area entropy, curvature entropy [19] and surface entropy of semantic parts [22] have
also been proposed to find the optimal viewpoint for polygon mesh.
Lee et al. [16] introduced mesh saliency to measure the regional
importance for polygon mesh, i.e., the scale-dependent Gaussianweighted average of mean curvature, and the optimal viewpoint is
the one with the maximum sum of mesh saliency. Recently, an information channel between a set of viewpoints and polygons was
suggested by Feixas et al. [7] to cope with viewpoint and mesh
saliency in a unified information-theoretic framework.
The idea of viewpoint selection for polygon mesh has already
been applied to the volume data, and most of them estimate the

194

optional viewpoint based on information theory[26]. Takahashi et
al. [23] presented a feature-driven approach to locate the optimal
viewpoint. The volume is decomposed into feature components,
and the locally optimal viewpoint for each feature component can
be found through the surface area entropy. These locally optimal
viewpoints are weighted by opacities to get the global optimal viewpoint. The voxel-entropy approach was proposed to identify a minimal set of representative views by Bordoloi and Shen [2]. Each
voxel is assigned a noteworthiness value, such as the opacity and
domain-knowledge based importance value, and the optimal viewpoint is defined as the one with voxel visibilities proportional to
their noteworthiness. While voxel entropy is an object-based viewpoint evaluation metrics, image-based evaluation metrics have also
been proposed by Ji and Shen [9] to select the optimal viewpoint,
such as opacity entropy, color entropy, and curvature information.
These image-based evaluation metrics prefer an even opacity and
color distribution with a larger projected area and more perceived
curvatures. Our proposed view descriptors both belong to imagebased evaluation metrics, and the shape view descriptor is very
similar to the opacity view descriptor. However, the shape view
descriptor does not take homogeneous regions into account due
to the zero gradients in these regions and only uses the opacity
to weight boundary structures, while the opacity view descriptor
includes these regions if the opacity in these regions is not zero.
Moreover, our detail view descriptor takes local details into consideration separately, while previous image-based view descriptors
consider features of interest as a whole to evaluate viewpoint quality.
As the Introduction pointed out, the optimal viewpoint is dependent on the objective. For medical applications, the user is usually
interested in small critical part of whole features, such as the vessel near the tumor and a particular vessel in the head. Several researches have been performed on this topic. A viewpoint selection
framework for angiographic volumes was discussed by Chan et al.
[3]. View descriptors for visibility, coverage and self-occlusion of
features of interest are designed to search the optimal viewpoint in
the solution space. The LiveSync interaction metaphor synchronizing the 2D slice view with the volumetric view of medical datasets
was studied by Kohlmann et al. [13, 14]. Viewing spheres are
deformed to encode view qualities for the component specified by
the user in 2D slice view, and these deformed viewing spheres are
combined to identify the optimal viewpoint. With the different objective, our view descriptors seek the representative view of whole
features defined by transfer functions. Additionally, our view descriptors can be integrated with the visibility factor to locate the
optimal viewpoint of partly critical features for these medical applications.
Filtering has many applications in volume visualization,
such as denoising, multiresolution data decomposition[28], and
classification[18]. The most commonly used filtering may be
the Gaussian filter. However, edges/boundary structures are also
blurred by the Gaussian filter in the result image/volume. To overcome this drawback, various non-linear edge-preserving smoothing filters have been proposed, such as PDE-based anisotropic
diffusion[21], weighted least squares[15], the bilateral filter[24],
etc. Among these filters, the bilateral filter as a valuable tool is
widely used in computational photography, due to its ability to extract details at different scales[20]. As the volume is very similar
to the image, the bilateral filter is a desirable tool to extract global
structures without local details for the shape view descriptor.
3

S HAPE V IEW D ESCRIPTOR

Features of interest are specified by transfer functions using
domain-specific knowledge and the information extracted from the
volume, and all these features are the components that we need to
take into consideration in viewpoint selection. Moreover, features

(a)

(b)

(c)

(d)

Figure 2: The figure shows viewpoint selection results of the hydrogen atom around the Y axis based on the shape view descriptor. (a) the
deformed viewing circle. The radius of each viewpoint is proportional to normalized viewpoint quality and the color mapping from blue to red
corresponds to viewpoint quality from low to high. (b) the rendered image from the optimal viewpoint. (c) the corresponding intensity image from
the optimal viewpoint. (d) the rendered image from the worst viewpoint for comparison.

wP
Vie

With this mapping of the deviation angle and the emitted intensity, the relative orientations to the viewing direction of visible
boundary structures are recorded in the 2D intensity image. We
compute the information amount of the intensity image with n pixels by use of the Shannon entropy function[4], which is a useful
measurement of the average information in the random sequence.
Assuming the intensity values are I0 , I1 , I2 , , In−1 , the probability pi
of the pixel i is defined as

Featue
Boundary

e
lan

gt
Ray
t

0

v

Figure 1: The relation between the normalized gradient direction gt
and the viewing direction v.

pi =

Ii
.
n−1
∑ j=0 I j

(3)

The shape entropy can be calculated as follows
may have various shapes, such as the blob, sheet, and line shape,
and it is mainly reflected on boundary structures of features. As a
result, our shape view descriptor evaluates boundary structures of
features. This is achieved by measuring the distribution of the relative angle between the viewing direction and the gradient direction
on the 2D rendered image.
In direct volume rendering, the rendering equation for the ray
usually has the form as follows
∞

c(t) ·V (t)dt,

C=

(1)

0

where c(t) is the emitted intensity at the position t, and V (t) is the
visibility from the origin to the position t along the ray. The final
intensity takes into account the emitted intensity from all possible
positions t along the ray.
Often, c(t) is specified in color transfer functions, which map the
scalar value into the emitted intensity (color). For our shape view
descriptor, the emitted intensity c(t) is determined by
c(t) = gt · v,

(2)

where gt is the normalized gradient direction (the opposite direction of the normal) at the position t, and v is the viewing direction.
Figure 1 illustrates the relation of gt and v on the feature boundary.
In the manner, the emitted intensity c(t) only exists in the boundary regions, as zero gradients in homogenous regions correspond to
zero emitted intensity. Only visible boundaries are considered in
this evaluation (we ignore the negative value in the back boundary).
Moreover, the closer the gradient direction and the viewing direction is, the higher the emitted intensity is. The emitted intensity is
still weighted by the visibility V (t) according to Equation 1, which
depends on opacity transfer functions.

n−1

H(v) = − ∑ pi logpi .

(4)

i=0

The Shannon entropy function reaches the maximum value when
probabilities have the same value, which implies that visible boundary structures are nearly all towards the viewing direction with the
same deviation angle. As the opacity view descriptor [9] pointed
out, background pixels do not contribute to the shape entropy
(0log0 = 0) and the shape entropy is only determined by the foreground area. Therefore, the maximum value of the shape entropy
is proportional to the number of pixels on the foreground area. The
optimal viewpoint of the shape view descriptor corresponds to the
viewpoint with larger visible boundary structures and even relative
orientation to the viewing direction.
Figure 2 shows viewpoint selection results of the hydrogen atom
around the Y axis according to the shape view descriptor based on
the original volume. As seen clearly from the deformed viewing
circle in Figure 2(a), the shape view descriptor is very effective in
locating the optimal viewpoint for the perception of global shape
structures. In all our deformed viewing circles or spheres, the radius R(v) for the viewpoint v is proportional to normalized viewpoint quality H ′ (v) (R(v) ≈ 0.5(1 + H ′ (v)) and the color mapping
from blue to red corresponds to viewpoint quality from low to high.
The concept of deformed viewing spheres here is only used for visualization similar to [16], and it differs from deformed viewing
spheres in LiveSync[13, 14], which are used to encode and combine viewpoint qualities for different components.
4 D ETAIL V IEW D ESCRIPTOR
Besides the global shape structure, the detail gives more information about features of interest, especially for these highly detailed
volume datasets. To estimate these detail information, we should

195

(a)

(b)

(c)

(d)

(e)

Figure 3: Effects of the Gaussian filter and the bilateral filter with different parameters on the engine dataset. The detail is highlighted through
the lighting. The intensity values is normalized in the range [0,1]. (a) the original volume. (b) the filtered result using the Gaussian filter with
σ = 2. (c) the filtered result using the Bilateral filter with σs = 2 and σr = 0.1. (d) the filtered result using the Bilateral filter with σs = 2 and σr = 0.2.
(e) the filtered result using the Bilateral filter with σs = 4 and σr = 0.2.

first separate global structures from local details. This could be
achieved through constructing the shape volume that contains only
global boundary structures. Local details still remain in the original
volume.
The Gaussian filter is widely used in volume smoothing. Unfortunately, boundary structures are also blurred by the Gaussian filter
in the filtered volume, and appearance variances may be further aggravated by transfer functions. The output of the Gaussian filter
with σ = 2 for the engine dataset is shown in Figure 3(b). Many
large-scale features are blurred compared with the original volume
and it is undesirable to take these blurred features as local details.
The underlying reason is that the Gaussian filter depends only on
the spatial distance between the voxels, and its intensity values are
totally ignored. As a result, a high-intensity voxel in the boundary
structure is greatly influenced by a adjacent low-intensity voxel and
this results in blurred boundary structures.
Similar to the Gaussian filter, the bilateral filter further takes the
variance of voxel intensities into account to preserve strong boundary structures. Only adjacent voxels with similar intensity value
have a strong influence over the considered voxel, i.e., voxels outside boundary regions do not significantly affect the voxel in the
boundary regions even if they are neighbor voxels. As a matter of
fact, the bilateral filter is used frequently in computational photography for edge-preserving image smoothing and image decomposition of a smooth base layer and a residual detail layer[20].
As discussed above, the bilateral filter is an ideal tool to construct the shape volume. From the mathematical point of view, the
bilateral filter is defined as follows
′

Ip =

1
Wp

Wp =

∑ Gσ (||q − p||) · Gσ (Iq − Ip ) · Iq ,
s

r

(5)

q∈S

∑ Gσ (||q − p||) · Gσ (Iq − Ip ),
s

r

(6)

q∈S

where p is the voxel position, q is the neighbor voxel position
′
around the center position p, Ip is the generated intensity value at
voxel position p in the shape volume, Iq is the intensity value at
the voxel position q in the original volume. Gσs is a spatial Gaussian kernel with the width σs , while Gσr is a range Gaussian kernel
with the width σr . The general Gaussian kernel has the form like
Gσ (x) = exp(−x2 /σ 2 ).
The spatial Gaussian kernel Gσs decreases the influence of voxels far away from the processed voxel, while the range Gaussian
kernel Gσr decreases the influence of large differences in intensity.
These two parameters control the effectiveness of the bilateral filter. In Figure 3, the bilateral filter is applied to the engine dataset
with different parameters. As seen clearly from rendered images of
Figure 3(b) and Figure 3(c), the results of the Gaussian filter and

196

the bilateral filter with the same range parameter show that the bilateral filter preserves more boundary structures information than
the Gaussian filter. With the range parameter doubled, more local
details are smoothed and large-scale structures are displayed in Figure 3(d). On the other hand, with the spatial parameter doubled, the
amplitude of boundary structures larger than the range parameter
have no influence, as shown in Figure 3(e). These two parameters
can be modulated according to the user’s definition of the detail.
For example, if the middle-scale structures are taken as the detail
in some situation, it can be archived by increasing both the spatial
parameter and the range parameter. In our experiments, we define
the spatial parameter σs = 2 and the range parameter σr = 0.2. The
intensity value of the volume data is normalized in [0,1].
In this way, the shape volume can be constructed by the bilateral
filter. The local details lie in the variance between the shape volume and the original volume. The differences in intensity between
the shape volume and the original volume do not expose the exact
differences on the rendered 2D images, which is actually similar to
the screen-error in the multiresolution volume rendering[28]. As is
known to us, besides the difference in intensity, the screen-error is
also reverent to transfer functions. This is also the reason why there
is no requirement to construct the detail volume (the difference of
the original volume and the shape volume). It seems that the screenerror is a good candidate to measure the local detail information.
However, although the bilateral filter can preserve strong boundary
structures, the intensity value may still have a small change even
for smoothed boundary regions without any details. These intensity differences may be amplified by transfer functions. Therefore,
color differences (screen-error) is not fully equivalent to local details on boundary structures.
The gradient direction implies the orientation of local structures
in boundary regions, and the elimination of local details on boundary structures leads to great changes in the gradient direction. On
the other hand, the intensity change in the smoothed boundary regions due to the limitation of the bilateral filter, would not make
the orientation of local boundary structures significantly different.
Therefore, variances of local structures between the shape volume
and the original volume stand for the local detail information, and
our detail view descriptor maps local structure variances to the
emitted intensity in Equation 1 as follows
c(t) =

1 − gt · gts ,
0,

gt = 0
,
other

(7)

where gts and gt are normalized gradient directions at the position t along the ray in the shape volume and the original volume.
This mapping only considers detailed boundary structures, as in
smoothed boundary regions, gradient directions between the shape
volume and the original volume are nearly the same, and subsequently smoothed boundary regions have no contribution to the

(a)

(b)

(c)

(d)

Figure 5: The figure shows the viewpoint selection results of a synthesis cube data with the random details on the -Z face based on the hybrid
view descriptor. The viewing sphere is deformed similar to the deformed circle in Figure 2. The radius of each viewpoint is proportional to
normalized viewpoint quality and the color mapping from blue to red corresponds to viewpoint quality from low to high. (a) the deformed viewing
sphere of the shape view descriptor. (b) the deformed viewing sphere of the detail view descriptor. (c) the deformed viewing sphere of the
combined view descriptor with α = 0.5. (d) the rendered image from the optimal viewpoint of the combined view descriptor with α = 0.5.

(a)

(b)

Figure 4: The figure shows the viewpoint selection results of a synthesis cube data with the random details on the -Z face around the
X axis based on the detail view descriptor. (a) the deformed viewing
circle. (b) the rendered image from the optimal viewpoint.

intensity image. Additionally, the larger the variance of the local
structure is, the higher the emitted intensity is.
The emitted intensity is also weighted by the visibility, which is
determined by opacity transfer functions. The rendered intensity
image represents local details perceived from this viewpoint, and
it can be used to evaluate viewpoint quality of local details. The
viewpoint with the maximum amount of the intensity is the optimal
viewpoint with the most perceived details.
Figure 4 shows the viewpoint selection results of a synthesis
cube data with the random details on the -Z face around the X
axis according to the detail view descriptor, and the detail is highlighted through the lighting. From the deformed viewing circle in
Figure 4(a), it is noticeable that view qualities before the detailed
face is obvious larger than others, and the detail view descriptor
actually captures details of the synthesis cube.
For these detailed volume datasets, viewpoint selection should
take both global structures and local details into account. We
present a new viewpoint selection framework, which integrates the
shape view descriptor with the detail view descriptor to trade off
emphasis of global structures and local details. The hybrid view
descriptor for the viewpoint v is defined as
F(v) = α Fshape (v) + (1 − α )Fdetail (v),

(8)

where the parameter α is in the internal range [0,1], Fshape and
Fdetail are the evaluation function of the shape view descriptor and
the detail view descriptor, respectively. The shape evaluation Fshape

is based on the shape volume for detailed volume datasets, and the
detail evaluation Fdetail accounts for local structure variances between the shape volume and the original volume. It is obvious that
Fshape and Fdetail have to be normalized by their maximum value
among all viewpoints before the summation.
The user can adjust the parameter α according to their specific
situations. If we lower the parameter α , more detail information
will be presented in the rendered image of the found optimal viewpoint. Otherwise, if the user concerns the shape structure of the
volume data, it is desirable to increase the parameter α . As for the
simplicity, the trade-off between global structures and local details
of features of interest is simple enough for general users to manipulate.
Figure 5 shows the viewpoint selection results of the same volume data as Figure 4 according to the hybrid view descriptor. The
deformed viewing spheres of the shape view descriptor and the detail view descriptor are illustrated in Figure 5(a) and Figure 5(b).
As can be seen from Figure 5(a), the global structure of the cube is
a very symmetric shape and optimal viewpoints are at the directions
of principal diagonals. As also indicated in Figure 4, the detail view
descriptor well captures these local details in Figure 5(b). The deformed viewing sphere of the hybrid view descriptor with α = 0.5
and the rendered image from the corresponding optimal viewpoint
are shown in Figure 5(c) and Figure 5(d). The optimal viewpoint
based on the hybrid view descriptor provides the maximum information of the global shape structures and local details about the
synthesis cube.
5

R ESULTS AND D ISCUSSION

We have implemented the shape view descriptor and the detail view
descriptor discussed above in a GPU-based ray-casting volume renderer. The size of the rendered intensity image is 512 × 512 for
each dataset, and the format of each pixel in the intensity image
is the floating point using the Framebuffer Object(FBO) technique.
All tests were run on a 3.0GHz Intel Core 2 Dual CPU with an
NVIDIA GeForce 8800 GTX graphics card. The HEALPix package [8] was used to generate evenly distributed viewpoints over the
viewing sphere. In our experiments, 1200 sample viewpoints on the
viewing sphere were used for each dataset. The high-quality gradients for the shape and detail view descriptors were constructed by
the tri-cubic B-spline filtering.
We employed the 64 × 64 × 128 shockwave dataset (the middle part of the original shockware) to compare the opacity view
descriptor[9] and the shape view descriptor. Viewpoint qualities
were both evaluated around the X axis based on the original vol-

197

(a)

(b)

(c)

(d)

Figure 7: The figure shows the viewpoint selection results of the tooth and vortex datasets based on the shape view descriptor. (a) the rendered
image of the tooth dataset from the optimal viewpoint. (b) the rendered image of the tooth dataset from the worst viewpoint. (c) the rendered
image of the vortex dataset from the optimal viewpoint. (d) the rendered image of the vortex dataset from the worst viewpoint.

(a)

(b)

(c)

(d)

Figure 8: The figure shows the viewpoint selection results of the daisy pollen grain dataset based on the detail view descriptor. (a) the original
volume. (b) the shape volume. (c) the deformed viewing sphere of the detail view descriptor. (d) the rendered image from the optimal viewpoint.

(a)

(b)

Figure 6: The figure shows the deformed viewing circles of a shockwave dataset around the X axis for the comparison of the opacity
view descriptor and the shape view descriptor. The shape view descriptor is more effective in the estimation of the structural information. (a) the deformed viewing circle of the opacity view descriptor.
(b) the deformed viewing circle of the shape view descriptor.

ume, and their deformed viewing circles are displayed in Figure 6.
The deformed viewing circle based the opacity view descriptor is
more symmetric due to the similar opacity distribution in rendered
images. However, the shape view descriptor takes structural differences into account to capture the optimal viewpoint with more
information about expressive boundary structures.
The 1283 hydrogen atom dataset is a smoothed volume dataset
and is employed to verify the effectiveness of the shape view descriptor as shown in Figure 2. The shape view descriptor is also

198

validated on the 256 × 256 × 161 tooth dataset and the 1283 vortex
dataset. Their rendered images from the best and worst viewpoints
are shown in Figure 7. The evaluation of the shape view descriptor
is based on the original volume, not the shape volume. Their computation times are approximately 30.6 seconds and 19.3 seconds,
respectively. As can be seen clearly, the shape view descriptor can
find the optimal viewpoint from which more boundary structures
are visible.
Figure 4 shows a 643 synthesis cube data with the random details
on the -Z face to demonstrate the effectiveness of the detail view descriptor. We used the 192 × 180 × 168 daisy pollen grain dataset to
further verify the detail view descriptor. The rendered images of the
original volume and the shape volume are shown in Figure 8(a) and
Figure 8(b), respectively. The bilateral filter eliminates the wavy
local details, especially on the back of the daisy pollen grain, and
preserves large-scale structures of the original volume. As can be
seen clearly, the shape volume is a smoothed version of the original daisy pollen grain. The computation time of the detail view
descriptor for the daisy pollen grain is approximately 27.6 seconds.
The deformed viewing sphere as well as the optimal viewpoint is
shown in Figure 8. Since there are more wavy local details on the
back, the optimal viewpoint for the local detail actually reflects this
fact.
The hybrid view descriptor is very useful for highly detailed volume datasets. It locates the optimal viewpoint that contains both
large-scale global structures and small-scale local details. Note that
the shape view descriptor in the hybrid view descriptor is evaluated
based on the shape volume. The optimal viewpoint of the synthesis
cube data is shown in Figure 5. Viewpoint qualities on eight principal diagonal lines are nearly the same in the evaluation based on

(a) α = 0.0

(b) α = 0.2

(c) α = 0.4

(d) α = 0.6

(e) α = 0.8

(f) α = 1.0

Figure 9: The figure shows the viewpoint selection results of the shockwave dataset based on the hybrid view descriptor with different α settings.

(a)

(b)

(c)

(d)

Figure 10: The figure shows the viewpoint selection results of the shockwave and the daisy pollen grain datasets based on the hybrid view
descriptor. The parameter α is 0.8. (a) the deformed viewing sphere of the shockwave dataset. (b) the rendered image of the shockwave dataset
from the optimal viewpoint. (c) the deformed viewing sphere of the daisy pollen grain dataset. (d) the rendered image of the daisy pollen grain
dataset from the optimal viewpoint.

the shape view descriptor, and viewpoints on the front of the detailed face have higher viewpoint qualities according to the detail
view descriptor. The hybrid view descriptor with α = 0.5 gives the
optimal viewpoint of this synthesis cube data, and the maximum
amount of the shape and detail structure information is shown in
the rendered image from the optimal viewpoint.
We experimented with different settings of the parameter α of
the hybrid view descriptor for the shockwave dataset. The rendered
images from the optimal viewpoints of different α settings from
0.0 to 1.0 are shown in Figure 9. As the parameter α increases, the
hybrid view descriptor puts more emphasis on large-scale global
structures. From the series of optima viewpoints, the user can easily obtain information about both global structures and local details,
and they can also adjust the parameter α to search the optimal viewpoint according to their specific situations.
The final results for the shockwave dataset and the daisy pollen
grain based on the hybrid view descriptor are shown in Figure 10.
The parameter α is 0.8. The optimal viewpoints provide the maximum amount of information about shape structures and local details. The view stability is one of the criteria of the optimal viewpoints in the psychological experiments[1]. The deformed viewing
spheres shown in Figure 10(a) and Figure 10(c) indicate that the optimal viewpoints are usually located on the smoothed regions (red
color), and this satisfies the principal of the view stability. However, the worst viewpoints are at a steep local valley (blue color), as
can be seen from Figure 10(a).
The presented hybrid view descriptor is well suited to the volume
data with clear detailed features and well-defined gradients. As the
shape and detail view descriptors are both dependent on the gradient direction, it is not proper to employ these view descriptors to
evaluate the optimal viewpoint of discontinuous structures with the
undefined gradient, such as unstable flow. It is also unnecessary to
evaluate the details of natural objects with smoothed profiles, such
as the tooth dataset, as these volume datasets contain little detailed

features. However, with the development of high-resolution digital
imaging techniques, it can be expected that more and more highly
detailed volume datasets will be available in the future.

6

C ONCLUSIONS AND F UTURE W ORK

In this paper, we presented two structure-aware view descriptors to
evaluate the viewpoint quality of global structures and local details.
The shape view descriptor measures the distribution of the relative angle between the gradient direction and the viewing direction.
Global structures and local details can be separated by using the bilateral filter, which smoothes local details while preserving strong
boundary structures. The detail view descriptor is designed to deal
with the visible detail amount in terms of variances in the local
structure between the shape volume and the original volume. The
shape view descriptor and the detail view descriptor prefer larger
visible boundary structures and more perceived detail structures,
respectively. The shape view descriptor can be used to evaluate
boundary structures alone for smoothed volume datasets, while the
shape view descriptor and the detail view descriptor can be integrated into a viewpoint selection framework for highly detailed volume datasets. The hybrid view descriptor takes into account both
global structures and local details. The objective is to locate the
optimal viewpoint with the maximum amount of information about
shape and detail structures. We performed experiments on various
types of volume datasets, and showed the effectiveness of the proposed view descriptors. A future direction of structure-aware viewpoint selection is to extend the structure-aware view descriptor to
the time-varying dataset. Beside the information about shape structures at each time step, the optimal viewpoint for the time-varying
volume data should take into account the structure difference between successive time steps.

199

ACKNOWLEDGEMENTS
The authors would like to thank the anonymous reviewers for their
valuable comments. This work was supported by NFS of China
(No.60873122). The datasets are courtesy of SFB382 of the German Research Council (DFG), General Electric, Kwan-Liu Ma,
Michael Meissner, and Olaf Ronneberger.

R EFERENCES
[1] V. Blanz, M. J. Tarr, and H. H. B¨ulthoff. What object attributes determine canonical views? Perception, 28(5):575–599, 1999.
[2] U. D. Bordoloi and H.-W. Shen. View selection for volume rendering.
In Proceedings of the conference on Visualization’05, pages 487–494.
IEEE Computer Society, 2005.
[3] M.-Y. Chan, H.-M. Qu, Y.-C. Wu, and H. Zhou. Viewpoint selection for angiographic volume. In Proceedings of the second International Symposium on Visual Computing’06, pages 528–537. SpringerVerlag, 2006.
[4] T. M. Cover and J. A. Thomas. Elements of Information Theory. Wiley
Series in Telecommunications, 1991.
[5] C. M. Cyr and B. B. Kimia. 3D object recognition using shape
similarity-based aspect graph. In Proceedings of International Conference on Computer Vision’01, pages 254–261. IEEE Computer Society,
2001.
[6] F. Durand and J. Dorsey. Fast bilateral filtering for the display of highdynamic-range images. In Proceedings of the SIGGRAPH conference
2002, pages 257–266. ACM, 2002.
[7] M. Feixas, M. Sbert, and F. Gonz´alez. A unified information-theoretic
framework for viewpoint selection and mesh saliency. ACM Transactions on Applied Perception. to appear.
[8] K. M. G´orski, E. Hivon, A. J. Banday, B. D. Wandelt, F. K. Hansen,
M. Reinecke, and M. Bartelmann. Healpix: A framework for highresolution discretization and fast analysis of data distributed on the
sphere. The Astrophysical Journal, 622:759–771, 2005.
[9] G.-F. Ji and H.-W. Shen. Dynamic view selection for time-varying
volumes. IEEE Transactions on Visualization and Computer Graphics, 12(5):1109–1116, 2006. (Proc. Visualization’06).
[10] G. Kindlmann. Transfer function in direct volume rendering: Design,
interface, interaction. ACM SIGGRAPH 2008 Course Notes, 2002.
[11] G. Kindlmann and J. W. Durkin. Semi-automatic generation of transfer functions for direct volume rendering. In Proceedings of the 1998
IEEE symposium on Volume visualization, pages 79–86. ACM, 1998.
[12] J. J. Koenderink and A. J. van Doorn. The singularities of the visual
mapping. Biological Cybernetics, 24(1):51–59, 1979.
[13] P. Kohlmann, S. Bruckner, A. Kanitsar, and M. E. Gr¨oller. Livesync:
Deformed viewing spheres for knowledge-based navigation. IEEE
Transactions on Visualization and Computer Graphics, 13(6):1544–
1551, 2007. (Proc. Visualization’07).
[14] P. Kohlmann, S. Bruckner, A. Kanitsar, and M. E. Gr¨oller.
Livesync++: Enhancements of an interaction metaphor. In Proceedings of Graphics Interface’08, pages 81–88. Canadian Information
Processing Society, 2008.
[15] R. L. Lagendijk, J. Biemond, and D. E. Boekee. Regularized iterative image restoration with ringing reduction. IEEE Transactions on
Acoustics, Speech and Signal Processing, 36(12):1874–1888, 1988.
[16] C. H. Lee, A. Varshney, and D. Jacobs. Mesh saliency. ACM
Transactions on Graphics, 24(3):659–666, 2005. (Proc. ACM SIGGRAPH’05).
[17] E. B. Lum and K.-L. Ma. Lighting transfer function using gradient
aligned sampling. In Proceedings of the conference on Visualization’04, pages 289–296. IEEE Computer Society, 2004.
[18] E. B. Lum, J. Shearer, and K.-L. Ma. Interactive multi-scale exploration for volume classification. The Visual Computer, 13(6):622–630,
2006. (Proc. Pacific Graphics’06).
[19] D. L. Page, A. Koschan, S. R. Sukumar, B. Roui-Abidi, and M. A.
Abidi. Shape analysis algorithm based on information theory. In
Proceedings of the IEEE International Conference on Image Processing(ICIP03), pages 229–232, 2003.

200

[20] S. Paris, P. Kornprobst, J. Tumblin, and F. Durand. A gentle introduction to bilateral filtering and its applications. ACM SIGGRAPH 2008
Class #1 Notes, August 2008.
[21] P. Perona and J. Malik. Scale-space and edge detection using
anisotropic diffusion. IEEE Transactions Pattern Analysis Machine
Intelligence, 12(7):629–639, July 1990.
[22] O. Polonsky, G. Patan´e, S. Biasotti, C. Gotsman, and M. Spagnuolo.
What’s in an image: Towards the computation of the ”best” view of an
object. The Visuali Computer, 21(8-10):840–847, 2005. (Proc. Pacific
Graphics’05).
[23] S. Takahashi, I. Fujishiro, Y. Takeshima, and T. Nishita. A featuredriven approach to locating optimal viewpoints for volume visualization. In Proceedings of the conference on Visualization’05, pages 495–
502. IEEE Computer Society, 2005.
[24] C. Tomasi and R. Manduchi. Bilateral filtering for gray and color
images. In Proceedings of International Conference on Computer Vision’98, pages 839–846. IEEE Computer Society, 1998.
[25] P.-P. V´azquez, M. Feixas, M. Sbert, and W. Heidrich. Viewpoint selection using view entropy. In Proceedings of Vision Modeling and
Visualization Conference(VMV01), pages 273–280, 2001.
[26] I. Viola. View selection in scientific visualization. Eurographics 2007
Tutorial #8 Applications of Information Theory to Computer Graphics, September 2007.
ˇ
[27] P. Sereda,
A. V. Bartrol´ı, I. W. O. Serlie, and F. A. Gerritsen. Visualization of boundaries in volumetric data sets using LH histograms. IEEE
Transactions on Visualization and Computer Graphics, 12(2):208–
218, 2006.
[28] M. Weiler, R. Westermann, C. Hansen, K. Zimmermann, and T. Ertl.
Level-of-detail volume rendering via 3D textures. In Proceedings of
the 2000 IEEE symposium on Volume visualization’00, pages 7–13.
ACM, 2000.

