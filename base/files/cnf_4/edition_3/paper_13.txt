A Visual Canonical Adjacency Matrix for Graphs
Hongli Li
Pfizer Research Technology Center

Georges Grinstein
University of Massachusetts Lowell

ABSTRACT
Graph data mining algorithms rely on graph canonical forms to
compare different graph structures. These canonical form
definitions depend on node and edge labels. In this paper, we
introduce a unique canonical visual matrix representation that
only depends on a graph’s topological information, so that two
structurally identical graphs will have exactly the same visual
adjacency matrix representation. In this canonical matrix, nodes
are ordered based on a Breadth-First Search spanning tree. Special
rules and filters are designed to guarantee the uniqueness of an
arrangement. Such a unique matrix representation provides
persistence and a stability which can be used and harnessed in
visualization, especially for data exploration and studies.
KEYWORDS: Visual graph mining, Canonical form, Adjacency
matrix visualization.
INDEX TERMS: G.2.2 [DISCRETE MATHEMATICS]: Graph
Theory—Graph algorithms
1

INTRODUCTION

A graph is a natural way to model relational structured
information where nodes represent entities and edges represent
associations between different objects. Typical graph applications
include social networks, biological pathways, internet topologies,
and many more. Most network visualization systems use the nodelink representation[1]. There are different graph layout algorithms
and many graph layout aesthetics, all aiming to display node-link
visualizations reasonably well. An alternative graph
representation is through a matrix. Recent research has shown that
a properly vertex-ordered matrix can reveal graph structure
information and aid in the graph exploration process [2]. However
such matrix representations are often not stable since they rely on
many non-structure related issues, such as the selection of the
starting node and the initial node order, etc.
A key goal in graph data mining is finding interesting patterns
algorithmically. Much of this work has focused on finding
frequent (sub)-graphs. One of the most difficult problems in this
area is the Graph Isomorphism (GI) problem. It has no known
polynomial algorithm solutions nor has it been proven to be an
NP-complete problem [3, 4].
In this paper, we propose a new unique visual canonical matrix
representation that depends only on a graph’s topological
information so that two structurally identical graphs will have
exactly the same visual adjacency matrix representation.
Hongli Li: 620 Memorial Dr. Cambridge, MA 02139. e-mail:
Hongli.Li@pfizer.com
Georges Grinstein and Loura Costello: 1 University Ave.
Lowell,
MA
01854.
e-mail:
grinstein@cs.uml.edu,
loura_costello@student.uml.edu
IEEE Pacific Visualization Symposium 2009
April 20 - 23, Beijing, China
978-1-4244-4404-5/09/$25.00 ©2009 IEEE

Loura Costello
University of Massachusetts Lowell

In section 2, we briefly survey the existing graph matrix
visualizations, vertex ordering algorithms, and graph mining
algorithms. In section 3 we show how to achieve the visual
canonical representation for the graph adjacency matrix. We then
discuss our approach and prove its correctness in section 4. Both
sections are more technical in nature but valuable for
understanding the resulting visualizations. Finally, in section 5 we
show the results of some evaluation tests and discuss future work
in section 6.
2

RELATED WORK

In this section, we review current state of matrix graph
visualization, current vertex ordering algorithms and graph data
mining algorithms.
2.1
Graph Matrix Representations
Matrix visualization has a long history [2]. The most common
approach is to represent the graph as an adjacency matrix where
cell (i,j) is non-zero if node i is connected to node j, with its value
dependent on node information. The matrix may be nonsymmetric and its cell values need not be just 0 or 1. Bertin
introduced such a visual representation to represent networks [5].
Whereas node-link visualizations of a graph need to deal with
overlapping nodes or edge crossings, the matrix representation
needs to deal with readability. In 2004, Ghoniem et al. [6] showed
that except for finding paths, the graph matrix representation
outperformed the node-link graph visualization in several
common graph exploration tasks, such as the identification of the
most connected nodes, getting the number of nodes in the graph,
and discovering all neighbours of a given node. In 2006, Henry et
al. proposed the MatrixExplorer [7], which used both matrix and
node-link representations arranging them side-by-side to assist in
graph exploration. Their approach provides a vertex ordering
strategy allowing users to reorder the vertices in the matrix both
manually and automatically. Two vertex reordering methods have
been used for automatic reordering; a method based on
hierarchical clustering and one based on the Travelling Salesman
problem. The final matrix visualization shows clusters which
correspond to interesting structures in the graph. The result of
these reordering methods is affected by the initial node ordering,
the resulting matrix organizations are not always the same,
although they can be quite similar. NodeTrix [1] goes one step
further by integrating the matrix and node-link representations.
Each “node” in NodeTrix is actually a matrix representing a
substructure. When there is only one node in a substructure, that
matrix becomes a standard node. Moreover, a user can
interactively expand a matrix node to a standard node-link graph
or select any subgraph and compress it into a matrix. NodeTrix
thus provides a way to show the overall structure and at the same
time also show details by using a matrix.
2.2
Matrix Reordering Algorithms
Besides clustering methods, there are some other ways to order
vertices in a matrix. Mueller et al. [2] evaluated several different
matrix reordering algorithms for their interpretability and stability.

89

Their results suggest that sparse matrix reordering algorithms
have the potential to reveal graph structures but all need further
refinement depending on special visualization requirements.
Our vertex ordering is based on Cuthill-McKee algorithm
(CM) [8]. In CM, vertex ordering is a level order traversal of the
Breadth-First Search (BFS) spanning tree [9] (Figure 2) with
special rules for selecting the root or starting node and for
ordering nodes inside a tree. The goal is to reduce the bandwidth
of the input sparse matrix so that computation costs can be
reduced during matrix calculations. The bandwidth of a graph
matrix is determined by the maximum ordering difference
between two endpoints of all the edges in the graph. The ordering
here refers to the sequence order given to each node. The original
CM algorithm selects a set of starting nodes whose degree satisfy
d min < d < fd max , where d min and d max are respectively the
minimum and maximum degrees and f is a coefficient between 0
and 1. CM builds a BFS spanning tree rooted at each qualified
starting node. At each step, it orders nodes in non-decreasing
order by their degrees. CM ignores the order between same degree
nodes. In the final step, CM chooses one ordering with the
minimum bandwidth. Finding the optimal minimum bandwidth
for a matrix is an NP-complete problem [10]. CM algorithm tries
to find a small bandwidth but not necessarily the optimal one.
However the starting node directly affects the final result. Gibbs,
Pool, and Stockmeyer (GPS) [11] proposed a more efficient
method that yields a slightly smaller bandwidth ordering than the
CM procedure with 8 times faster average performance
improvement [11, 12]. The major difference between GPS and
CM is that GPS uses a pseudo-peripheral node as its starting node
where pseudo-peripheral nodes are those spread apart at maximal
or nearly maximal distance. The distance here refers to the path
length between the two nodes. Using a pseudo-peripheral node as
the starting node generally produces a tall spanning BFS tree, and
a taller tree will likely result in a smaller bandwidth matrix.
Minimum degree ordering [13, 14] is another popular matrix
bandwidth reduction algorithm. As the name suggests, it keeps
choosing a minimum degree node and removes it from the graph.
The remaining graph updates itself accordingly by adding
necessary edges. The updating step is expensive. Several
strategies have been proposed to improve its performance since its
original publication. These include the multiple elimination and
partial updating [15].
2.3

Graph Data Mining Algorithms and Canonical
Forms
Many of the graph mining applications try to identify frequent
graph structures. These algorithms usually compare graphs to
decide which structure appears more often. This is not a trivial
task because graph structures tend to be complex. Figure 1 shows
two identical graphs displayed using two different layout
algorithms. Their appearances are very different. Testing if two
graphs are isomorphic is called the Graph Isomorphism (GI)
problem [16]. The GI problem can be solved in a “moderately
exponential time”, which means the running time T is within
n k < T < a n , where k , a > 1 are some arbitrary real numbers [4,
17]. Some GI related algorithms have been proven NP-complete
problems; these include the subgraph isomorphism problem and
the graph symmetry detection problem [18-21]. Practical GI
testing algorithms usually use vertex invariants [4] to prune the
search space in order to achieve better performance. But there is
no known polynomial computable invariant that can solve the GI
problem in polynomial time.

Figure 1: Two identical graphs with alternative layouts can appear
very different.

The first graph mining system, SUBDUE, was introduced in
1994 [22, 23]. It uses a compression-based methodology. It
iteratively identifies sub-structures which are used to compress the
original graph data. SUBDUE allows some degree of mismatch so
that the result might not be a frequent subgraph at all. Inokuchi,
Washio et al. [24] introduced an apriori-based graph mining
algorithm. They defined a canonical form to test if two graphs are
isomorphic. The canonical form only applies to an attribute graph
where nodes and links in the graph need to have labels. The
canonical form is formed in two steps; the first step connects node
and edge labels by following a specific rule, and then chooses
from all possible combinations the lexicographically minimal one
as the canonical form. The canonical form helps in comparing
graphs and also prunes the search space to make the algorithm
efficient. The canonical form could be matrix-based or structurebased, which defines the order of connection descriptions, For
example, the structure-based method might follow either a BFS
ordered tree or a Depth-First Search (DFS) ordered tree. Using a
canonical form has been adopted by several graph mining
algorithms including gSpan [25], CloseGraph [26], FSG [27], and
FFSM [28].

3

UNIQUE VISUAL CANONICAL ADJACENCY MATRIX

Our goal is to design a procedure that produces a vertex
ordering with following properties:

• The visual appearance of the adjacency matrix is unique so
that two graphs are structurally isomorphic if and only if
their canonical matrices are the same;
• The vertex ordering depends only on the topological
information of the graph (connections between vertices);
• The resulting matrix has a small bandwidth;
• The resulting matrix shows a clustering effect similar to CM
algorithm [8].

Figure 2: The node ordering is a level order traversal through a BFS
spanning tree. The root node’s order is set to 0, and the bottom
right node at the last level (level m ) is set to V − 1 where V is the
total number of nodes in the graph.

Our algorithm is based on CM algorithm. Similar to it, the
output node ordering of our algorithm is a level order traversal of
the BFS spanning tree. Starting with the root node (ordered 0), it

90

visits each level successively from top to bottom; and on each
level it orders nodes from left to right (see Figure 2).
Before start, we want introduce several important
terminologies that we use intensively this thesis.
Definition 1: A visual adjacency matrix is an adjacency matrix
without node and edge labels.
Definition 2: For a given tree T in a graph G, p, v ∈ G , if node
p and v are on two consecutive levels of T with node p is at one
level higher than node v, if edge (p,v) is a tree edge, then node p is
called the direct parent of node v, and node v is called a direct
child of node p.
In this paper, the term node, point and vertex are
interchangeable.

3.1
Properties of Cuthill-McKee Ordering
Any BFS spanning tree of an un-weighted, undirected graph is
a shortest path tree [10]. If we follow CM procedure, we observe
the connections between nodes will have the following properties:
•
The root node can only connect to its direct children and
they are linked by tree edges;
•
If a node connects to other nodes on its direct parent’s
level, those nodes must be visited after its direct parent;
similarly, if a node connects with nodes on its direct
children’s level, then those nodes must be visited before
its direct children ;
•
A node could connect to any other nodes on its own level;
It is not hard to prove the first and the third facts. For the
second statement, if a node v connects with node w that is on the
level of its direct parent p and also if node w comes before node p
in this BFS spanning tree, then by CM procedure, node v should
be a direct child of node w instead of being a directed child of its
current parent p. So if there are any connections between node v
and nodes on the level above (except with its direct parent), those
nodes must appear after node v’s direct parent p. The proof is
similar for the connections between a node v and nodes at its
direct children’s level.

(a)

(b)

Figure 3: A graph and one of its BFS spanning trees. (a) A random
graph. (b) One of the BFS spanning trees rooted at node a. The
number next to the node indicates its order. The solid black line
represents the tree edge and the dashed blue line corresponds to
the non-tree edge.

Theorem 1: If a node ordering is the visitation order of a BFS
tree, then the bandwidth of this ordering is determined by one of
the tree edges of this BFS tree.
Proof: we will use the BFS Spanning tree in Figure 3 as an
example to help us explain different scenarios. Non-tree edges can
be put into three categories:
•
Edges between nodes on the same level;
•
Edges between a node and nodes on its direct parent
level;
•
Edges between a node and nodes on the level below.
These three cases cover every scenario. There are no edges
that could jump over one or more tree levels because that would
violate the fact that any BFS tree is a shortest path tree [10]. We

will prove that in each of these three possibilities, there exists a
tree-edge, where the absolute difference of its two endpoints’
order is larger than any of the other non-tree edges that belong to
the same category. Again, the order here refers to the visitation
order of the corresponding BFS tree.
Case 1: The non-tree edges are between nodes on the same
level, for example, the edge (d, e) on level 1 in Figure 3 (b). For
any level except the root level, the largest possible order
difference between two connected nodes is between the first node
vi. first and the last node vi .last (i represents the level). Let p denote
the parent of node vi .last . We know that node p’s order order(p)
must be less than node vi. first 's order order( vi. first ) because node p
is
on
the
level
above,
this
means
order (vi .last ) − order ( p ) > order (vi .last ) − order (vi . first ) , and edge
( p, vi .last ) is an existing tree edge.
Case 2: The non-tree edges are between a node and other
nodes on its direct parent’s level, for example edge (f, e) in Figure
3 (b). As we proved before, if a node v is connected with another
node w on its parent’s level, then node w must appear after node
v’s directed parent p. This means order ( p) < order ( w) , so
order (v) − order ( p ) > order (v ) − order ( w) , and (p, v) is an
existing tree edge.
Case 3: The non-tree edge is between a node and some other
nodes on its direct children’s level. This is similar to case 2. We
omit the proof here.
Theorem 1 means that for any non-tree edge, there exists a tree
edge who shares one endpoint with this non-tree edge and whose
bandwidth is larger than that of the non-tree edge. So, we have
bandwidth ( M ) ≤ 2 × width (T ) , where M is the matrix derived
from the tree T.
Our algorithm is based on CM. We use properties of CM to
order nodes with the same degree. These properties can help us
reduce the total number of possible orderings. Our procedure
includes three major steps which are explained in detail in section
3.2 to 3.4.

3.2
Determining the starting node
Our goal is to define a canonical visual matrix that represents a
unique graph structure. We would like this matrix to have a small
bandwidth so it can show clusters in the visual representation.
Inspired by GPS, we choose our candidate starting node only from
qualified peripheral nodes.
Finding a pseudo-peripheral node is an efficient procedure [11,
29-32]. But if we use this approximation, there are more qualified
starting nodes. Testing each one and filtering them later are
expensive steps. It is also hard to define a clear threshold for how
much of an approximation we could allow. Since peripheral nodes
are very likely to generate a small bandwidth node ordering [30],
we select our candidate only from these peripheral nodes. A graph
has at least two peripheral nodes, trees generated by them have the
same height but not necessarily the same width. So we test every
level structure rooted at each of these peripheral nodes, and select
those with the smallest width. These root nodes will be our
starting nodes. Figure 4 shows a procedure that generates all
candidate starting nodes
Starting nodes finder
1. for each node v in the graph build a BFS spanning tree rooted
on v;
2. identify all the tallest trees;
3. among all the tallest trees, select trees with the smallest width;
4. return all root nodes that generate these slim tallest trees.
Figure 4: The procedure to find all peripheral nodes in a graph.

91

3.3

Ordering nodes at each Level
d

a

b

a

e

c

f

g

a

d

b

e

c

f

g

a

d

d

b

b

e

e

c

c

f

f

g

g

(a)

(a)

(b)

(c)

(d)

(b)

Figure 5: Two adjacency matrices for the graph in Figure 3. Nodes
are ordered by following CM ordering. If we ignore node labels,
these two matrices are the same. (a) The ordering starts with node
d. (b) The ordering starts with node a.

Ordering nodes with the same degree is one of the difficulties
in finding the visual canonical matrix representation. CM
procedure arranges the same degree nodes in any order, but
different orderings give rise to different visual representations. In
most cases, we must choose a unique order for the nodes that
share the same degree. Before we go into the details on how we
order them, we would like to introduce the concept of
indistinguishable nodes. This concept was first introduced by Liu
in [33]. Any two nodes u , v ∈ V are said to be indistinguishable if
they satisfy the condition that adj (u ) U {u} = adj (v) U {v} , where
adj(u) represents nodes adjacent to node u.
Indistinguishable nodes have a very interesting property, that
is, their order is interchangeable in the corresponding BFS
spanning tree, and the visual appearance of the corresponding
adjacency matrix will not change. Also, it won’t change other
nodes’ order. It is like we just exchanged their labels. If we ignore
the label difference, then the tree’s structure and connections are
exactly the same. In Figure 5, nodes a and d are a pair of
indistinguishable nodes. If a graph is a clique, every pair of nodes
is indistinguishable
Inspired by indistinguishable nodes, we discovered that if any
two nodes u and v satisfy the property adj (u ) = adj (v) then their
order is also interchangeable just like indistinguishable nodes. We
call these nodes pseudo-indistinguishable nodes. Indistinguishable
nodes are special pseudo-indistinguishable nodes. Since pseudoindistinguishable nodes share the same neighbours, all non-root
pseudo-indistinguishable nodes are on the same level of the BFS
spanning tree and share the same directed parent.
There are other situations where two nodes’ order can be
interchanged and the visual matrices stay the same. We call nodes
has this property similar nodes. Similar nodes are not necessary
pseudo-indistinguishable. In fact, pseudo-indistinguishable is just
one special case of similar nodes which is very easy to detect.
Others might not. It relates to the graph symmetry testing
problem. One way to break ties is to use node label. This is
similar to those graph mining algorithms we introduced in section
2. We could generate all possible node arrangements and choose
the lexicographically smallest one as our canonical form. But
labels have nothing to do with the graph structure. It is possible
that two graphs having exactly the same structure but different
labels. In that case, we could get two different forms and thus two
different adjacency matrices. This is not what we want and is
counter to our goal of a unique representation allowing us to
harness persistence. We are more interested in the raw graph
structure and a unique visual representation is of prime
importance.

92

Figure 6: Four different situations between nodes u and v. The
graphs shown here are all subgraphs in some larger graph
structure. The blue triangle represents an arbitrary substructure. (a)
Nodes u and v are pseudo-indistinguishable. (b) The order between
nodes u and v can be determined by the order between nodes a
and b. (c) The order between nodes u and v is determined by the
degree sum of those nodes connected to nodes u and v separately.
(d) Node u should be placed ahead of node v because node g’s
parent node c is ordered before node h’s parent b.

Based on the existing properties of CM ordering and the BFS
spanning tree it creates, we designed following strategies to
achieve a canonical visual representation.
CM determines that when we are ordering nodes sharing the
same directed parent, we already know which nodes appear before
this group and their order. Moreover, we can also derive which
nodes should appear after them and probably their partial orders
(determined by their parent nodes). If their degrees are different,
then we just pick the one with smaller degree and place it before
the other. If they have the same degree then there are several
different cases to consider. We define the number of nodes
connected to node u and which also come before node u the predegree of node u.
1. If two nodes are pseudo-indistinguishable then we randomly
choose one node and place it before the other (see Figure 6
(a)).
2. If node u or v has connections with other nodes which come
before them (either nodes on their direct parent’s level or
nodes on their own level), then if the pre-degree (u) > predegree (v), we place node u before node v, and vice versa;
3. If nodes u and v have the same pre-degree, then we compare
the two lowest ordered nodes that connect to only one of
them. If these two nodes are different then we can use their
order to break the tie. For example, in Figure 6 (b) node a is
the lowest ordered node connected to node u but not v, and
node b is the lowest ordered node connecting to node v but
not u, nodes a and b are two different nodes
and order ( a) < order (b) , so we put node u before v, and vice
versa;
4. If nodes u and v connect to the same set of nodes before
them, since they are not pseudo-indistinguishable, they must
connect to different nodes after them.
i. Let S1 be the set that contains all nodes connected to u
but not to v that have not been ordered; let S 2 represent
the similar set of nodes connected to node v. The sizes
of set S1 and S 2 are equal. We add up all nodes’

degrees in S1 and compare it to the sum of the node
degrees in S 2 , and place the one with the smaller sum
before the other (see Figure 6 (c) where node v should
be placed before node u);
ii. If a tie still exits, then we generate adj ( S1 ) , the union of
all adjacencies of nodes in S1 with node u being
removed, and also generate a similar set adj ( S 2 ) . It is
possible that there are ordered nodes in adj ( S1 ) or
adj ( S 2 ) . As we can see in Figure 6 (d), node g and
h belong to set adj ( S1 ) and adj ( S 2 ) respectively.
Nodes c and b have been ordered. We can use their
order to determine nodes g and h’s order, then derive
the order between nodes u and v;
5. It is possible that the tie still exits after all these steps. If that
is the case, we need to generate all possible orderings and
choose only one from them later by using the filters in
section 3.4.

3.4
Dealing With Multiple Candidate Orderings
In section 3.2, we discussed how to select the starting node.
Section 3.3 explained how to determine two nodes’ order when
there is a tie situation. After all that, it is still possible that we end
up with multiple vertex orderings, either because of multiple
starting nodes or because of some unbreakable ties.
Since smaller bandwidths are more likely to generate clusters
in the adjacency matrix, we always select an ordering with the
minimum bandwidth as our first filter (bandwidth filter). But there
might be more than one ordering with the same bandwidth. In this
case, we use a global penalty and a visual matrix matching
strategy we introduce below.
3.4.1

3.4.2
Visual Matrix Matching
After all rules, filters and penalty we introduced, if there is still
a tie, we compare the two corresponding adjacency matrices, rowby-row, and break the tie if there is a mismatch. If two different
orderings still have a tie after the row-by-row matching, we know
these two adjacency matrices’ visual patterns are exactly the same
and we’re done (we just select one of them). Figure 8 shows an
unreal example (for explanation) where two different orderings
generate exactly the same visual matrix.
An undirected graph’s adjacency matrix is symmetric with
respect to the main diagonal. When comparing two matrices, we
only need to consider the upper triangle (or the lower triangle).
For each row, we begin our comparison starting from the diagonal
position; we match all cells one by one from the left to the right.
The only possible mismatch is that one matrix has a non-empty
element while the other matrix has an empty one at the same
position. When this mismatch is first detected, we stop the
procedure and select the first matrix.

(a)

Global Penalty

(b)
Figure 7: Two adjacency matrices from two different orderings. The
visualization of both matrices are very similar, the only difference
being edge (x, j) highlighted in red. The gray grid background is for
ease of visual tracking for columns and rows. The global penalty for
the left matrix is 41, while the global penalty of the right matrix is 40.

The Global Penalty focuses on all edges as a whole, and again
on bandwidth reduction. It is the summation of all edges’
bandwidth in a graph. Figure 7 shows an example of how the
global penalty could help select one ordering over another. The
two patterns in Figure 7 are very similar although the two vertex
orderings are different. If we use the global penalty, the ordering
on the right will win because node x and j are closer to each other
than the ordering shown on the left.
Whenever we need to compare two orderings with the same
bandwidth, we calculate their global penalties and choose the
smaller one.
It is still possible that there exists two orderings having exactly
the same global penalty value. In such a case, we further refine
our testing by a visual matrix matching strategy.

Figure 8: Two different orderings of the same graph having exactly
the same visual representations for their adjacency matrix. On the
left are two BFS spanning tree orderings with their corresponding
adjacency matrices shown on the right (note that this is an unreal
situation as the starting node a is not a peripheral node).

This matrix matching strategy might match two rows that
represent two different nodes. For example, in Figure 8 (a) the
node ordering is (a, b, u, v, g. h, x, y, i, j) while the ordering in
Figure 8 (b) is (a, b, v, u, h, g, y, x, i, j). To compare these two
matrices, we start from the first row and go down to the second.
The third row in Figure 8 (a) corresponds to node u while the third
row in Figure 8 (b) to node v. We ignore the fact that these are
two different nodes and only focus on two matrices. If these two
rows have the same pattern we consider it is a match and go on to
the next row.
Both global penalty and visual matrix matching filters tend to
select matrices that have patterns close to the main diagonal. Both
our experience and others’ suggest that diagonal patterns are
related to interesting structures of the graph. For example, a fully
filled diagonal block represents a clique graph structure.

93

B
B
B
R
R
R

5.3

30
106
206
30
110
210

200
600
1200
200
500
2000

3
0
0
0
0
0

2
1
1
7
2
1

8
24
94
5
31
58

0
10
49
2
7
7

0
6
4
9
2
3

0
0
0
0
0
0

running time
(ms)

permutation

Symmetric graph data could present the worst case scenario to
our algorithm. But it won’t necessary always result in a
| V |! running time since there are often clues to limit the number
of possible orderings. Figure 9 shows a cubic symmetric graph of
size 8 (totally 8 nodes). In a cubic graph, every node’s degree is 3.
We can see from Figure 9 that there are several pairs of similar
nodes. For example,(a, e), (d, h) and (b, c) etc. are all similar node
pairs. For this particular example, every node is a peripheral node,
so they are all qualified starting nodes. Figure 9 (b) and (c) are
two possible spanning trees rooted on node a. On the second
level, we have nodes d, b and c; their degrees are the same. It is
not hard to find out that node d should be put before the other two
because node d connects to node b and c (two unordered nodes)

test-4i

(c)

Figure 9: (a): A cubic symmetric graph of size 8. (b) and (c) are two
of its BFS spanning trees rooted at node a.

94

Table 1: Several computation factors of applying our procedure to
several synthetic graphs. Where there is more than one qualified
starting node, we only show the average of multiple cases. For the
“running time” column, numbers shown in parentheses represent
the time taken by finding the candidate starting nodes. The
synthetic graph data is generated by using a Barabasi Graph
Generator (http://www.cs.ucr.edu/~ddreier/barabasi.html) [35]. In
the type column, “B” is for the Barabasi graph and “R” is for the
random graph. “test-2”, “test-2a”, and “test-2b” refers to procedure
2, 2a, and 2b in section 3.3.

test-3

(b)

5.2
Different Graph Data
We also tested our algorithm on different type of graphs. As
our analysis in section 4.1, the undecidable cases are related to
difficult to detect symmetric or approximately symmetric graph
structures. In the worst case where there are many unbroken ties,
we have to generate every possible orderings for each possibility,
resulting in an exponential running time algorithm. This worst
case running time is comparable to graph isomorphism testing
algorithms [4, 17, 33, 34]. Table 1 shows some tests of our
procedure applied to two types of synthetic graph data. The
graphs have different sizes. There are no undecidable ties in all
these tests and the first three tests broke most of the tie situations.

edges

(a)

5.1
Initial Node Ordering
We tested the influence of the initial vertex ordering to our
algorithm by adding a randomize function to reshuffle the initial
vertex ordering every time before the starting of the application.
No matter how many times we tested, the visual representation
stayed the same. The actual node ordering might change because
of the existing of similar nodes.

test-2

Symmetric Graph Structures

RESULTS

We implemented a prototype of matrix reordering application
for experimentation and testing. We tested our algorithm for its
sensitivity to the initial layout and also its sensitivity on several
different graph data sets.. We also tested its ability to assist graph
exploration.

nodes

4.1

5

starting
nodes

Our goal is to find a visual canonical representation for the
adjacency matrix, where the vertex reordering depended only on
the graph structure and where two graphs have the same visual
representation if and only if they are structurally isomorphic.
Theorem 2: Two graphs are structurally isomorphic if and only
if their canonical visual adjacency matrix representations are the
same.
Proof: we only need to prove that for any input graph, there is
only one canonical visual adjacency matrix. There are three steps
in our procedure and we show that the result only depends on the
input graph structure.
In the first step, we check every node in the input graph, if
there is a tie, we keep both nodes as our candidates. The search is
complete so the returning starting node set is also complete.
The second step is to order nodes on each level of the BFS
tree. For a fixed starting node, nodes on each level are also fixed.
We order nodes by CM procedure, and use a set of well defined
heuristics to break ties. These heuristics are based on connectivity
information and have no ambiguity. If there is an undecidable
case after these tests, we form all possible orderings for each of
tie. This choice is expensive, rare, but necessary. It ensures the
completeness.
The last step is to select the unique visual matrix from all these
possible results. We use two filters, the bandwidth and the global
penalty, to help us get the canonical visual representation. These
filters are based only on the matrix properties; they can further
reduce the size of our result set. The visual matrix matching
strategy compares two matrices cell-by-cell and selects only one
visual adjacency matrix. This matching guarantees that there is
only one visual matrix selected after each comparison. No matter
which comparison order we choose, the result visual adjacency
matrix should always the same.
Fortin states in his report that “it is almost always trivial to
check two random graphs for isomorphism” [4]. This applies to
our procedure for generating canonical visual adjacency matrices.
General random graphs are not likely to have many undecidable
cases because they are not likely to be symmetric or
approximately symmetric.

and they are on a level higher than f and g, which are neighbours
of node b and c. Our heuristics in section 3.3 cannot decide an
order between node b and c, so we need to get all possible
orderings result from it. But once we choose an order for them,
the rest of nodes’ order is fixed. Node e and h are two pseudoindistinguishable nodes. So in this example, for a fixed starting
node, there is only one undecidable case. For 8 starting nodes,
there are totally 16 possible orderings. Comparing to 8!, 16 is a
small number.

pseudo-indis.
nodes (pair)

DISCUSSION

type

4

16 (16)
78 (78)
562 (547)
16 (16)
78 (78)
890 (875)

IMDB Data

We test our algorithm on IMDB data.

Figure 10 shows all movies released in 2006. There are totally
3953 movies. Each movie is a node in our graph. Two movies

(nodes) are linked together if they share some crew members.
There are totally 12957 edges. The whole graph contains 867
subgroups (subgraphs). Each of them is a connected graph and
there are no connections between different groups. The largest
subgraph contains 2964 movies (nodes) and 12396 connections
(edges). The majority of the rest of the graphs contain only one
node. As showed in Figure 10, the largest subgraph takes most of
the matrix display area. These matrices are in their canonical
forms. The largest one shows a “leaf” shape while the rest 866
subgraphs form the “petiole.” The bounding line of the “leaf” is
formed by all tree edges.
For the movie data of year 2006, finding the canonical
ordering for the first subgraph (the largest one) takes most of the
computation time. Other year’s movie data set also showed
similar patterns. Table 2 shows some computation parameters for
each of the largest component of the corresponding year’s graph.
If there are more than one qualified starting nodes, the number
shown here is the average of all cases. The running time relates to
the size of the graph (both number of nodes and also number of
edges) and also the structure of the graph. As we can see that the
graph of year 2006 is denser when comparing to the graph of year
2004. The running time increased dramatically. Another
interesting fact is that finding the candidate starting nodes takes
most of the computation time. The time showed here reflects the
result of single thread computation. Parallel computing should be
able to reduce the cost.

5.4

Graph Structure Changes

(a)

(b)

(c)

Figure 10: Movies released 2006. The whole graph contains 3953
nodes (movies) and 12957 edges. There are totally 867 connected
subgraphs. The largest subgraph matrix is showed on the upper left
portion of the matrix visualization area. In which, the upper left area
has slightly more connections than the lower right part.
Table 2: Several computation factors our procedure applying to the
IMDB dataset. The column “Running time” shows both the total
running time and also the time used for finding the candidate
starting nodes (in parentheses).

Figure 11: An example of how canonical visual matrix changes
corresponding to graph structure changes. We use the same
circular layout for all three node-link graphs in (a), (b), and (c).
Patterns are highlighted in both displays by using colored boxes or
red cells. Their correspondences are indicated by both colors and
arrows. (a) The original graph that has three clear patterns. (b)
Several new edges have been added between those original
clusters. (c) The graph changes even more, both representation
changes dramatically. The red color in the matrix highlight two
patterns, a high degree node (node 2) and a cluster formed by
node 10, 11, 12,13, 6 and 21.

We tested how well the canonical visual matrix is sensitive to
graph structure changes. We create a graph with the following
known patterns; several highly connected nodes, a highly
connected subgraph, and some low degree nodes (see Figure 11
(a)). The canonical visual matrix also shows these patterns. These
patterns are very clean in a way that each cluster only has a few
connections with nodes in another cluster. We then randomly
added several edges between nodes in different clusters. Figure 11
(b) shows the result. The original patterns are still identifiable.
We went even further by adding more edges between those
original clusters. The original patterns are now hard to see in both
representations (see Figure 11 (c)). Instead, new patterns are
formed. Conclusion and Future Work
We introduced a visual canonical representation for the graph
adjacency matrix. Each canonical matrix uniquely represents a
graph structure. The algorithm only uses the graph’s topological
information and is not affected by either node or edge labels nor
by its initial node ordering. The representation tends to cluster
non-empty elements in the adjacency matrix closer to the

95

diagonal, and experimental results indicate it can help explore the
graph data.
Our strategy to achieve the canonical ordering does not take
advantage of the symmetric data structure and parallel computing.
We believe they both will speed up the process if used
appropriately. Although general symmetric testing is a hard
problem, some simple ones might not be difficult to identify. This
information could help reduce some of the computational costs by
eliminating unnecessary permutations.
We are applying our approach to graph mining in order to help
identify frequent graph structures in graph databases, especially
those in compound databases for drug discovery and evaluating
and measuring social networks.
The strategy we used to achieve the canonical visual
representation actually is not limited to the vertex ordering
algorithm based on a BFS spanning tree. A modified approach can
be applied to the DFS ordering. As Mueller [2] mentioned in his
result, the DFS spanning tree-based vertex ordering could also
prove to be valuable for graph visualization and exploration.

REFERENCES
[1]

[2]

[3]
[4]

[5]
[6]

[7]

[8]

[9]
[10]
[11]

[12]

[13]

[14]
[15]

96

Henry, N., J.-D. Fekete, and M.J. McGuffin, “NodeTrix: A Hybrid
Visualization of Social Networks”, IEEE Transactions on
Visualization and Computer Graphics, 13(6): pp. 1302-1309. 2007.
Corneil, D.G. and D.G. Kirkpatric, “A Theoretical Analysis of
Various Heuristics for The Graph Isomorphism Problem”, Society
for Industrial and Applied Mathematics, 9(2): pp. 281-297, 1980.
Fortin, S., “The Graph Isomorphism Problem”, in Technical Report
TR 96-20, University of Alberta: Edmonton, Alberta, Canada. 1996.
Mueller, C., B. Martin, and A. Lumsdaine, “A Comparison of Vertex
Ordering Algorithms for Large Graph Visualization”, 6th
International Asia-Pacific Symposium on Visualization. Sydney,
NSW, Australia, 2007.
Bertin, J., Semiology of Graphics: Diagrams, Networks, Maps. The
University of Wisconsin Press, 1984.
Ghoniem, M., J. Fekete, and P. Castagliola, “A Comparison of
Readability of Graphs Using Node-Link and Matrix
Representations”, IEEE symposium on information visualization
2004. Austin, TX, 2004.
Henry, N. and J.-D. Fekete, “MatrixExplorer: a Dual-Representation
System to Explore Social Networks”, IEEE Transactions on
Visualization and Computer Graphics, 12(5): pp. 677-684, 2006.
Cuthill, E. and J. McKee, “Reducing the Bandwidth of Sparse
Symmetric Matrices”, The 1969 24th national conference 1969,
ACM. pp. 157-172, 1969.
Hubbard, J.R., “Schaum's Outline of Data Structures with Java”,
McGraw-Hill, pp. 369.
Gross, J.L. and J. Yellen, eds. Handbook of Graph Theory, CRC
Press, 2004.
Gibbs, N.E., W.G. Poole, and P.K. Stockmeyer, “An algorithm for
reducing the bandwidth and profile of a sparse matrix”, SIAM
Journal on Numerical Analysis, 13(2): pp. 236-250, 1976.
Martía, R., et al., “Reducing the Bandwidth of a Sparse Matrix with
Tabu Search”. European Journal of Operational Research, 135(2):
pp. 450-459, 2001.
Markowitz, H.M., “The elimination form of the inverse and its
application to linear programming”, Management Sci, 3: pp. 255269, 1957.
George, A. and J.W.H. Liu, “The Evolution of The Minimum
Degree Ordering Algorithm”, SIAM Review, 31: pp. 1-19. 1989.
Liu, J.W.H., “Modification of the minimum-degree algorithm by
multiple elimination”, ACM Transactions on Mathematical Software
(TOMS), 11(2): pp. 141-153, 1985.

[16] Skiena, S., “Graph Isomorphism”, Implementing Discrete
Mathematics: Combinatorics and Graph Theory With Mathematica,
Addison-Wesley: Reading, MA. pp. 181-187, 1990.
[17] Babai, L., “Moderately Exponential Bound for Graph Isomorphism”,
Proceedings of the Fundamentals of Computing Science. LNCS 117,
pp. 34-50, 1981.
[18] Fraysseix, H.D., “A heuristic for graph symmetry detection”, The
7th International Symposium on Graph Drawing, pp. 276-285, 1999.
[19] Garey, M.R. and D.S. Johnson, “Computers and intractability: A
guide to the theory of NP-completeness”, W. H. Freeman. 1979.
[20] Manning, J., Geometric Symmetry in Graphs, Department of
Computer Science. 1990, Purdue University.
[21] Ullman, J.D., “An algorithm for subgraph isomorphism”, Journal of
the ACM, 23(1): pp. 31-42, 1976.
[22] Ketkar, N.S., L.B. Holder, and D.J. Cook, “Subdue: CompressionBased Frequent Pattern Discovery in Graph Data”, The 1st
international workshop on open source data mining: frequent pattern
mining implementations, ACM Press: Chicago, Illinois, pp. 71-76,
2005.
[23] Holder, L.B., D.J. Cook, and S. Djoko. Substructure Discovery in the
SUBDUE System. in The Workshop on Knowledge Discovery in
Databases. 1994.
[24] Inokuchi, A., T. Washio, and H. Motoda, “An Apriori-based
Algorithm for Mining Frequent Substructures from Graph Data”,
The 4th European Conf. on Principles and Practice of Knowledge
Discovery in Databases (PKDD), Lyon, France, pp. 13-23, 2000.
[25] Fan, X. and J. Han, “gSpan: Graph-Based Substructure Pattern
Mining”, IEEE Intl. Conf. on Data Mining ICDM, Maebashi City,
Japan, pp. 721-723, 2002.
[26] Yan, X. and J. Han, “CloseGraph: mining closed frequent graph
patterns”, The 9th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, ACM Press: Washington,
D.C. pp. 286-295, 2003.
[27] Kuramochi, M. and G. Karypis, “Frequent Subgraph Discovery, in
1st IEEE Conference on Data Mining”, San Jose, California, USA.
pp. 313-320, 2001
[28] Huan, J., W. Wang, and J. Prins, “Efficient Mining of Frequent
Subgraphs in the Presence of Isomorphism”, The 3rd IEEE
International Conference on Data Mining, IEEE Computer Society
Melbourne, Florida, USA., 2003.
[29] George, A. and J.W.H. Liu, “An automatic nested dissection
algorithm for irregular finite element problems”, SIAM Journal on
Numerical Analysis, 15(5): pp. 1053-1069, 1978.
[30] George, A. and J.W.H. Liu, “An Implementation of a Pseudo
peripheral Node Finder”, ACM Transactions on Mathematical
Software, 5(3): pp. 284-295, 1979.
[31] Smyth, W.F. and I. Arany, Another algorithm for reducing
bandwidth and profile of a sparse matrix, in AFIPS 1976 NCC.
1976, AFIPS Press: Montvale, N.J. p. 987-994.
[32] H. L. Crane, J., et al., “Algorithm 508: Matrix Bandwidth and Profile
Reduction [F1]”, ACM Transactions on Mathematical Software
(TOMS), 2(4): pp. 375-377, 1976.
[33] George, A. e J. W.-H. Liu. Computer Solution of Large Sparse
Positive Definite: Prentice Hall Professional Technical Reference.
1981.

