Eurographics Symposium on Geometry Processing 2009
Marc Alexa and Michael Kazhdan
(Guest Editors)

Volume 28 (2009), Number 5

Approximating Gradients for Meshes and Point Clouds via
Diffusion Metric
Chuanjiang Luo

Issam Safa

Yusu Wang

The Ohio State University, Columbus OH, USA.
{luoc, safa, yusu}@cse.ohio-state.edu

Abstract
The gradient of a function defined on a manifold is perhaps one of the most important differential objects in
data analysis. Most often in practice, the input function is available only at discrete points sampled from the
underlying manifold, and the manifold is approximated by either a mesh or simply a point cloud. While many
methods exist for computing gradients of a function defined over a mesh, computing and simplifying gradients
and related quantities such as critical points, of a function from a point cloud is non-trivial.
In this paper, we initiate the investigation of computing gradients under a different metric on the manifold
from the original natural metric induced from the ambient space. Specifically, we map the input manifold to
the eigenspace spanned by its Laplacian eigenfunctions, and consider the so-called diffusion distance metric
associated with it. We show the relation of gradient under this metric with that under the original metric. It turns
out that once the Laplace operator is constructed, it is easier to approximate gradients in the eigenspace for
discrete inputs (especially point clouds) and it is robust to noises in the input function and in the underlying
manifold. More importantly, we can easily smooth the gradient field at different scales within this eigenspace
framework. We demonstrate the use of our new eigen-gradients with two applications: approximating / simplifying
the critical points of a function, and the Jacobi sets of two input functions (which describe the correlation between
these two functions), from point clouds data.
Categories and Subject Descriptors (according to ACM CCS): I.3.5 [Computer Graphics]: Computational Geometry
and Object Modeling—Geometric algorithms, languages, and systems

1. Introduction
The gradient of a function defined on a Riemannian manifold is one of the most important differential objects in data
analysis. It has been used for a broad range of applications,
from partial differential equations (PDEs) in scientific computing, to modeling deformations and simulations in graphics and visualization, to feature identification in image processing, and to inference problems in machine learning. It is
often the first step before further geometric quantities, such
as high-order derivatives, critical points, and Morse-Smale
complex, can be computed.
Very often in practice, we need to compute gradients
where the input function is only available at discrete points
sampled from the underlying manifold, and the manifold
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

is given by a mesh or simply a point cloud. While many
methods have been used when the input function is defined
over a mesh, computing gradients and related quantities such
as critical points, of a function from a point cloud embedded in Rk is a non-trivial task. Furthermore, even when a
mesh structure is given, being a differential operator, gradient computation is sensitive to noise both in the input function and in the underlying manifold. Hence it is highly desirable to be able to smooth gradient fields at different scales.
In this paper, we aim at developing a unifying framework
for approximating and smoothing gradients from discrete inputs that can be meshes and point clouds. Our framework is
based on a novel view of considering gradients under a different metric space where the computation and smoothing

C.Luo & I.Safa & Y.Wang / Approximating Gradients

1498

are made easier, especially for point cloud inputs. It relies
on the approximation of the so-called Laplace-Beltrami operator — The Laplace operator of a Riemannian manifold
M encodes all of the intrinsic geometry of M. Hence intuitively, once we construct the Laplace operator, it is possible
to retrieve various invariants, including gradients, from its
spectrum and eigenfunctions.
Related work. Given a (finite element) mesh, the simplest
and most common way to compute gradient is to interpolate
the input function linearly (or in higher order) within each
mesh element, based on its values at vertices of the mesh.
This mesh-approximation of gradient is somewhat sensitive
to both the shapes of mesh elements and noise [She02], as its
value at a vertex v depends only on the one-ring neighbors of
v. For functions defined on a d-dimensional domain in Rd ,
there is a rich literature in numerical analysis on the powerful finite element methods (FEMs), which can interpolate
the input function non-linearly based on non-local neighborhoods. The same idea can also be extended to develop
meshless finite elements methods for a point cloud sampled
from a d-dimensional domain in Rd . See [FM03] for a good
survey. However, both FEMs and meshless FEMs are usually computationally inefficient in handling low-dimensional
manifolds (such as surfaces) embedded in high-dimensional
space, as well as changing the resolution at which we want
to approximate the gradient.
In many tasks, the input is simply unstructured point sets.
See [AGP∗ , GP07] for a good introduction of processing
point clouds in graphics. For points inputs, gradients are usually estimated by solving a local optimization problem based
on Taylor expansions. Specifically, let P be the set of discrete
points such that the function values of f are available. The
gradient ∇ f (p) at a point p ∈ P can be estimated as the best
vector g∗ minimizing the following error:

∑

s

w(p, q) f (p) + gT (q − p) − f (q) ,

q∈Neighbor(p)

where Neighbor(p) ⊆ P denotes a certain set of neighboring vertices of p, w(p, q) is a weighting function, and s
is a positive integer, typically 1 or 2. For example, Sibson [Sib81] suggested to use the so-called natural neighbors
of p as Neighbor(p), and choose s = 1 in the above optimization problem. This gradient estimation method is implemented in the Computational Geometry Algorithms Library (CGAL). For points sampled from a Riemannian manifold embedded in a high-dimensional space, Mukherjee et
al. took a regularized version of this optimization and choose
s = 2 [MW06, WGMM08]. They provided some theoretical
guarantees of the accuracy of their estimates for points randomly sampled from probabilistic distribution. To make the
estimation robust to noises in the underlying manifolds, one
can use moving-least-squares (MLS) surfaces to locally approximate the surface [AK04, Lev98]. To estimate gradients
at a coarser scale, one can potentially take a larger neigh-

borhood and solve the optimization problem based on more
points around each point p. This unfortunately means that
the time complexity to estimate gradients is higher when the
resolution is coarser.
In this paper we use a spectral method, deploying the
eigenspace of the Laplace operator. Spectral methods have
been widely used in many research fields including graph
theory, vision, and machine learning, and have recently received great attention in geometric mesh processing, from
shape matching, segmentation, optimization, to parametrization and meshing. See [PG01, ZvKD09] for general surveys
and [Lev06, Sor06] for surveys of such methods using one
of the most popular operator, the Laplace-Beltrami operator. In discrete setting, most previous discrete Laplace operators are either constructed from a graph [BN05] or from
a mesh [BSW08, MDSB02, PP93, RWP06, Xu04], including the very popular Cotangent scheme [Dod78, PP93]. In
[BSW09], Belkin et al. proposed the first discrete Laplace
operator for general point sets that converges to the true
Laplacian of the underlying manifold as the sample points
become denser. See [BSW09, VL08, WMKG08, ZvKD09]
and references within for more thorough discussions on
computing discrete Laplace operators.
Our approach. In this paper, we aim to develop a unified
approach that can both approximate and smooth gradients
field of an input function defined on meshes or simply on
point clouds. The main contributions are as follows:
(1) We initiate the study of gradients under a different metric on the underlying manifold M, instead of the usual
metric induced from the ambient space. While gradient
depends on the metric, certain related quantities such as
critical points are metric independent, and metric may not
be essential for some gradient-related applications (such
as gradient descending to find global minimum).
(2) We compute gradients by mapping the input manifold
M to the space spanned by the eigenfunctions of the socalled Laplace-Beltrami operator of M. We show that
when a certain Gaussian kernel is used to approximate
the Laplace operator, this mapping is an isometry up to
a scaling factor. Hence we can recover the original gradients from the eigen-gradients easily.
(3) Laplacian eigenfunctions provide a natural basis for
functions defined on the manifold M, and the mapping of
M to its Laplacian frequency domain provides a natural
way to smooth eigen-gradients. Contrary to what is typical in the original space, computing gradients at a coarser
level in the eigenspace takes less time than at a higher
resolution, and it is possible to smooth both the function
itself and the underlying manifold simultaneously.
(4) We present two applications of the eigen-gradients:
computing and simplifying critical points of one function
and the so-called Jacobi sets of two functions from point
clouds data.
(5) We present a new discrete Laplace operator for point
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

C.Luo & I.Safa & Y.Wang / Approximating Gradients

clouds data that converges to the true Laplacian when input points satisfy certain sampling conditions. Given the
rich intrinsic geometry encoded in the Laplace operator,
this discrete analog allows us to potentially perform shape
analysis simply from point clouds, similarly to spectral
mesh processing [Lev06, ZvKD09]. While the discrete
Laplacian constructed in [BSW09] provides better theoretical guarantee than what we propose, our construction
performs much better in practice. It also produces realvalued spectrum, which is often necessary in applications
but not guaranteed by the operator proposed in [BSW09].
Our approach can be applied to any d-dimensional compact Riemannian manifold embedded in Rm . In this paper,
we focus on surfaces embedded in R3 .
2. Eigenspace and Diffusion Metric
Consider a smooth d-dimensional, compact Riemannian
manifold M isometrically embedded in Rm . Given a scalar
function f : M → R, when we talk about its gradients, we
usually implicitly assume the natural metric on M induced
from the Euclidean metric in the ambient space Rm . In this
paper, we map M into a spectral domain and compute gradients under the diffusion metric associated with it. (A good
exploration of general diffusion metrics can be found in
[Laf04].) Specifically, we will use the heat diffusion metric
and its variant. Hence we focus on the heat operator below.
Heat operator. The heat (diffusion) operator Ht with respect to t is an operator on L2 (M), the space of square integrable functions on M. It is defined as
Ht f (x) =

M

∑ ρi φi (x)φi (y).

useful for practical applications, and here we emphasize the
following two most relevant to our work: First, two manifolds M and N are isometric if and only if they share the
same eigenfunctions and eigenvalues. Hence the Laplacian
of a manifold completely decides its intrinsic geometry information. Secondly, eigenfunctions of the Laplacian form a
natural orthonormal basis for square integrable functions on
the manifold analogous to Fourier harmonics for functions
on a circle. Eigenvalues of the Laplacian give the frequency
of the corresponding harmonics (eigenfunction). Hence it
presents a natural way for smoothing functions defined on
M (this was first explored by Taubin in [Tau95] for graphics
applications).
Diffusion metric. Assume that λ0 ≤ λ1 ≤ . . . are sorted in
increasing order. Note that λ0 = 0 since the largest eigenvalue of Ht is ρ0 = 1. We now embed M to L2 (M) by the
following diffusion map: for any x ∈ M,
Φ(x) = [e−tλ0 /2 φ0 (x), e−tλ1 /2 φ1 (x), ...]T .
It is easy to show that Φ(M) is indeed an embedding of
M. This embedding map Φ is similar to the diffusion map
in [Laf04] and the GPS embedding in [Rus07], but with different weights for each coordinate, and is a special case of
the heat-kernel embedding studied in [BBG94].
The eigenspace L2 (M) is a vector space, and adapts a natural Euclidean distance. For two points x, y ∈ M, the diffusion distance between them w.r.t to t, denoted by Dt (x, y),
is simply the Euclidean distance between Φ(x) and Φ(y). It
then follows from Eqn (1) and the fact ρi = e−tλi that
Dt2 (x, y) = Φ(x) − Φ(y)

ht (x, y) f (y)dµy ,

2

=

∑ e−tλ

i

φi (x) − φi (y)

2

i≥0

where ht (x, y) is the so-called heat kernel, and dµy is the volume form at y. Intuitively, given two points x, y ∈ M, ht (x, y)
measures the amount of heat, out of unit heat at x, that passes
from x to y within time t. Hence given a function f serving
as the initial heat distribution on M, Ht f is the distribution
of heat at time t. This intuitive physical interpretation makes
the heat operator a popular tool to smooth both the manifold itself and functions defined on it. The heat operator is
compact, self-adjoint, and positive semi-definite. Thus it has
discrete spectrum ρ0 ≥ ρ1 ≥ . . . ≥ 0 with Ht φi = ρi φi , and
by the Spectral theorem, the heat kernel can be written as:
ht (x, y) =

1499

(1)

i≥0

The largest eigenvalue ρ0 is necessarily 1, because heat diffusion is an averaging process with M ht (x, y)dµy = 1.
Laplace-Beltrami operator. The heat operator is connected
to the Laplace-Beltrami operator Δ of M by the following
relation: Ht = e−tΔ . This implies that Ht and Δ share the
same eigenfunctions φi , and their eigenvalues satisfy that
ρi = e−tλi , where Δφi = λi φi . The Laplace-Beltrami operator is a fundamental geometric object with many properties
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

=

−tλi

∑e

φ2i (x) + φ2i (y) − 2φi (x)φi (y)

i≥0

= ht (x, x) + ht (y, y) − 2ht (x, y).

(2)

An intuitive stochastic view of the diffusion distance is as
follows: Dt (x, y) is small if a random walk (Brownian motion) on M from x reaches y within time t with high probability. Compared to the Euclidean or geodesic distances, the
diffusion distance is stable with respect to noise and even
small topological changes.
3. Eigen-gradients
We propose to compute gradients in the eigenspace. Below
we first introduce the new eigen-gradients for smooth manifolds. We then describe how to compute it from meshes and
point clouds.
3.1. Smooth Manifolds
Given an input manifold M embedded in Rm , set Ω = Φ(M).
The map Φ : M → Ω is a homeomorphism [BBG94]. Thus

C.Luo & I.Safa & Y.Wang / Approximating Gradients

1500
V

y)
∇ΩF(˜

T Ωy

3.1.2. Smoothing Gradients

T Ωx
V

x)
∇ΩF(˜

x˜

y˜

Figure 1: The gradient vector at a point on Ω = Φ(M) is the
projection of V onto the tangent space at this point.

given a function f : M → R, its push-forward F is welldefined by F(Φ(x)) = f (x). Now let gM denote the natural Riemannian metric on M induced from Rm , and gΩ the
metric on Ω induced from the Euclidean distance in L2 (M).
Given a point x ∈ M, ∇M f (x) is the gradient of f at x ∈ M
under metric gM , which we also refer to as the original gradient in this paper. The eigen-gradient of f at x, denoted by
∇E f (x), is defined as the gradient of f under the pullback
of the metric gΩ onto M.
Specifically, let T Mx and T Ωx denote the d-dimensional
tangent space of M at x and of Ω at Φ(x), respectively.
Let v = ∇Ω F(Φ(x)) be the gradient of F at Φ(x) in the
eigenspace under metric gΩ , and u ∈ T Mx the pullback of
the vector v under the linear map DΦx : T Mx → T Ωx ; that
v u
is, DΦx (u) = v. The eigen-gradient ∇E f (x) = u is simply the vector in the direction of u with length v .
3.1.1. Computation in the Eigenspace
Since the eigenfunctions φi s of the Laplace operator (or more
generally, of a well-behaved diffusion operator) form a set of
orthonormal basis for the space of square integrable function
L2 (M), given f ∈ L2 (M), we can represent f = ∑i≥0 αi φi as
a linear combination of φi s. By definition, we have that
F(x)
˜ = F(e−

tλ0
2

φ0 (x), e−

tλ1
2

φ1 (x), . . .) =

∑ αi φi (x),

i≥0

where x˜ denotes Φ(x). Hence the partial derivative of F
with respect to any eigenfunction (coordinate in L2 (M)) is
tλi

∂F/∂φi |x˜ = e 2 αi , which is independent of x.
˜ Now construct
a vector V in L2 (M) as
V = [e

tλ0
2

α0 , e

tλ1
2

T

α1 , . . .] .

At any point x˜ ∈ Ω, the gradient vector of F at x˜ is simply
the projection of Vx onto T Ωx , where Vx is the vector V
translated to the base point x.
˜ See Figure 1 for an illustration.
This implies that once the vector V, which only depends
on the input function f and M, is given, the gradient ∇F(x)
˜
can be computed by locally approximating the tangent space
at every point in the eigenspace. ∇E f (x) can then be computed by pulling back ∇F(x)
˜ to T Mx .

Since the eigenfunctions with high eigenvalues correspond
to harmonics with high frequency, a natural way to smooth
the gradients in the eigenspace is by taking the subspace
of L2 (M) spanned by the top K Laplacian eigenfunctions
with lowest eigenvalues. Specifically, consider the truncated
eigenmap ΦK (x) = [φ0 (x), φ1 (x), . . ., φK (x)]T and map M
to ΩK = Φk (M). To compute gradients at a coarser level,
we simply take a smaller K, compute the vector VK =
tλ0

tλ1

tλK

[e 2 α0 , e 2 α1 , . . ., e 2 αK ]T and project it to the tangent
space of ΩK at ΦK (x).
There are two types of smoothing simultaneously involved in the above approach: The truncation of the coefficient vector V corresponds to removing the higher frequency
components of the input function f , thus smoothing f ; while
the projection onto the subspace of L2 (M) spanned by the
first few eigenfunctions corresponds to mapping the manifold to its lowest frequency modes, thus removing details
from M. In this way, gradients estimated are robust to noise
both in the input function and in the underlying manifold. In
practice, one can control the resolutions of two smoothing
operations separately, by taking the top K1 coefficients for
V, while projecting M to ΩK2 , for K1 ≤ K2 .
In order to be able to pullback the gradients in the
eigenspace, it is necessary that the map ΦK is a local homeomorphism; that is, ΩK is an immersion of M into RK . Indeed, it follows from Theorem 2.2.1 in [PMS09] that this
is the case for K ≥ C, where C is some constant depending only on the intrinsic properties of M and it appears to
be rather small in the models we test in practice (Results
in [PMS09] are much stronger. The local homeomorphism
is only a corollary of them).
3.1.3. Gaussian Diffusion
The relation between the diffusion metric and the original
metric depends on which specific diffusion operator that we
use. In this paper, we use the heat diffusion operator due
to its natural physical interpretation and the nice properties
its spectrum and eigenfunctions have. However, the analytical form of the heat operator is only known for very limited
families of manifolds. In practice, the heat kernel ht (x, y)
is usually approximated by the following Gaussian kernel:
x−y 2

gt (x, y) =

e− 4t
t(4πt)d/2

. Indeed, ht = gt when the underlying

manifold is the Euclidean space Rd . For general manifolds,
Belkin and Niyogi [BN05, BN08] gave explicit bounds of
the difference between ht and gt , as well as between the
resulting diffusion operators, and used that to approximate
the Laplace operator with theoretical guarantees for points
randomly and uniformly sampled. In this paper, we also use
the Gaussian diffusion to approximate the heat diffusion process (via our new discrete Laplace operator that we will describe later). The Gaussian diffusion distance relates to the
Euclidean distance in the original space by the following
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

C.Luo & I.Safa & Y.Wang / Approximating Gradients

equations. This in turns leads to an explicit relation between
the eigen-gradient and the original gradient, enabling us to
reconstruct the original gradients based on eigen-gradients.
Lemma 3.1 Let D(x, y) denote the Gaussian diffusion distance between x and y. Then we have that
2t(4πt)d/2 D(x, y) = x − y + O( x − y 3 /t).
Proof : Set r = x − y . By Eqn (2), we have
2
1
r2
1
− r4t
+ O(r4 /t 2 ) .
(2
−
2e
)
=
=
(4πt)d/2
(4πt)d/2 2t

Hence
1 + O(r2 /t) = r + O(r3 /t).

Theorem 3.2 The eigen-gradient of f is related to the gradient in the original metric space by:
∇E f (x) =

Proof : Lemma 3.1 implies that the Riemannian metric gM
and the diffusion metric gD on M are isometric up to a scaling factor C = 2t(4πt)d/2 . To see this, fix a point x in M
and take any smooth curve γ(s) through x with γ(0) = x. Let
γ˜ (s) denote the image of the curve γ(s) under the map Φ; that
is, γ˜ (s) = Φ(γ(s)). Now consider the norm of the tangent of
γ(s) and of γ˜ (s) at s = 0 under metrics gM and gΩ , denoted
by γ (0) M and γ˜ (0) Ω , respectively. First note that by
Lemma 3.2 in [BSW08], given any curve π(s) in Rn , the
length of the curve between two points that are close enough
relates to the Euclidean distance between these two points
by the following inequality:
len(π(s), π(s )) = π(s) − π(s ) + O( π(s) − π(s ) 3 ).
This, combined with Lemma 3.1, implies that for two points
close enough on γ, the length of the curve between them and
that between their images in γ˜ satisfy:
C · lenΩ (˜γ(s), γ˜ (s ))
3
M ),

where the big-O notation hides terms in t which is a constant
when computing gradients. It then follows that
lenM (γ(0), γ(s))
M
s
s→0
lenΩ (˜γ(0), γ˜ (s))
= C γ˜ (0) Ω .
= C · lim
s→0
s
Now, take any two vectors u and v ∈ T Mx with their push˜ v˜ ∈ T Ωx , we have that
forwards u,
γ (0)

= lim

u, v

M

˜ v˜
= C2 u,

u, v

X

= u + v, u + v

X

− u − v, u − v

X

where , X denotes the inner product under metric gX .
Since the constant C remains the same for all points on M,
the map CΦ induces the same inner product as , M and
is thus an isometry. The claim then follows from elementary
differential geometry.

We now consider the discrete settings where the input data
is a mesh K with vertex set P, or simply a point cloud P, and
the function values of f are given at vertices P. Set n = |P|.
The proposed algorithm is summarized as follows, and we
explain the implementation of each step after it.
P REPROCESSING : Construct discrete Laplace operator L
and its top N eigenvalues {λi } and eigenfunctions {φi }.
S TEP 1: Compute the coefficients αi s where f = ∑ αi φi .
λ1 t

Ω,

c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

λN t

Set up the vector V = [e 2 α1 , . . ., e 2 αN ]T .
S TEP 2: Map every vertex x ∈ P to RN by ΦN (x) =
[e−

2t(4πt)d/2 ∇M f (x).

= lenM (γ(s), γ(s )) + O( γ(s) − γ(s )

through polarization, as

3.2. Discrete Settings

D2 (x, y) = g(x, x) + g(y, y) − 2g(x, y)

2t(4πt)d/2 D(x, y) = r

1501

λ1 t
2

−λN t

φ1 (x), . . ., e 2 φN (x)]T .
S TEP 3: For each vertex x, approximate the tangent space
T Ωx at ΦN (x). Project V onto T Ωx to obtain vx . Compute
the pullback of vx to the tangent space T Mx , denoted by
ux
ux . The estimated gradient at x is √ vx d/2
.
2t(4πt)

ux

3.2.1. Pre-processing
Recently, Belkin et al. [BSW09] proposed a discrete Laplacian for general point clouds data with convergence guarantees. Unfortunately, their operator may have complex
eigenvalues and eigenfunctions. Using observations from
[BSW08, LSW09], we modify the approach from [BN05]
to construct the following operator for a set of points P =
{p1 , . . ., pn }. A detailed description and intuition of this
construction, together with it theoretical guarantee, can be
found in Appendix A. If we are given a mesh input, then we
construct the discrete Laplacian proposed in [BSW08].
For simplicity, we assume the underlying manifold is a
surface. For any point pi in P, let Ai denote the Voronoi
weight at pi , as defined and computed in [LSW09], which
roughly accounts for the area of the underlying manifold represented by the sample point pi . Each Ai can be computed
locally by taking points within certain distance to pi . If the
input has a mesh structure, then Ai is simply one-third of the
pi −p j 2

1 −
4h
.
one-ring area [BSW08]. Now set Gh (i, j) = 4πh
2e
The discrete Laplace operator L is an n × n matrix where

L[i][ j] =

Gh (i, j)A j
i= j
n
Gh (i, i)Ai − ∑ j=1 [Gh (i, j)A j ] Otherwise

The matrix L is not symmetric. However, it can be decomposed into L = GD, where D is the diagonal matrix with

1502

C.Luo & I.Safa & Y.Wang / Approximating Gradients

D[i][i] = Ai , and G[i][ j] = Gh (i, j) for i = j, and G[i][i] =
− A1i ∑ j=i Gh (i, j)A j . Since G is symmetric and D is diagonal
positive-definite, L has real eigenvalues and eigenfunctions.
We compute the eigenvalues λ1 , . . ., λn and eigenvectors
φ1 , . . ., φn of L by solving the generalized eigenvalue problem Gρi = βi D−1 ρi , and by observing that λi = βi and
φi = D−1 ρi . Indeed GD(D−1 ρi ) = Gρi = βi (D−1 ρi ) implies that D−1 ρi is an eigenvector of L = GD. (We cannot
compute the straightforward generalized eigenvalue problem
Dφi = λi G−1 φi as G may not be numerically invertible).
The eigenvectors φi s form an orthonormal basis with respect
to the D-inner product, namely, φi , φ j D = φTi Dφ j = δi j ,
where δi j is the Kronecker delta. This is because that
φTi Dφ j = (D−1 ρi )T DD−1 ρ j = ρTi D−1 ρ j = δi j .

(3)

This is an important property because, in the continuous
case, the inner product between two functions is f , g =
M f (x)g(x)dµ(x). In the discrete setting, it is natural to replace the integral with a summation ∑ni=1 f (pi )g(pi )Ai =
f , g D , where Ai corresponds to the volume form dµ(pi )
at point pi . This area weighting is necessary due to the nonuniform sampling of input points, and indeed, it is shown
in [LSW09] that this discrete sum converges to the true integral for points satisfying certain sampling conditions. Hence
Eqn (3) is the discrete analog of the fact that the Laplacian
eigenfunctions are orthonormal. Similar property was established for previous mesh Laplacians as well [ZvKD09].
Remark. We consider the construction of the discrete
Laplace operator as a pre-processing step because first, it is
independent of the input function given. Hence once constructed, it can be used for multiple functions, and for different resolutions of the same function. It can also be used
for the same shape under isometric deformation (as will be
demonstrated in our experiments in the next section). Furthermore, since the Laplace operator encodes all intrinsic geometry information, one can expect a general framework of
estimating various geometric quantities directly from point
clouds based on the Laplace operator constructed (see e.g
[BN05, LSW09, RWP06] for data analysis under this framework). Our approach is simply one application of such spectral point-cloud processing framework.
3.2.2. Steps 1 To 3
Given a square integrable function f on M, it can be represented as a linear combination ∑i αi φi of Laplacian eigenfunctions φi s, where αi = f , φi = M f φi dν in the continuous setting. As mentioned above, this integral can be approximated by the D-inner product f , φi D in the discrete
setting, for both point clouds and meshes, where D is the
area-weight diagonal matrix that we constructed above.
Implementing Step 3. For each vertex x, let knn(x) =
{y1 , . . ., yk } denotes the set of k-nearest neighbors of x for
some constant k (k is typically 10 in our experiments). We

approximate the tangent space at x or at ΦN (x) by finding the
best d-dimensional space fitting knn(x) under least square
error measure. To pull back the gradient vector vx from T Ωx
to T Mx , we need to approximate the linear map DΦ : T Mx →
T Ωx , which is simply a d × d matrix (2 by 2 matrix for surfaces). This is achieved by finding the best d × d square matrix that minimizes the following least square error:
k

∑

DΦ(ui ) − u˜i

2

,

i=1

where ui (resp. u˜i ) is the projection of the vector yi − x (resp.
Φ(yi ) − Φ(x)) onto T Mx (resp. T Ωx ). Specifically, let U
˜ denote the matrix with ui s (resp. u˜i ) as column
(resp. U))
vectors. We have that DΦ = U +U˜ where U + is the MoorePenrose pseudo-inverse of U. Since the eigen-gradient has
the same magnitude as vx , the gradient in the eigenspace.
Hence this pullback operation is not necessary if we only
need the magnitudes of gradients in the applications.
4. Experiments
We present three sets of experiments. The first set compares
the eigen-gradients from point clouds with the ground truth,
as well as with gradients computed by a local quadratic fitting method. We then show how to use the eigen-gradients
to simplify the critical points of a function, and the Jacobi
sets of two functions. All eigen-gradients in this section are
computed directly from point clouds. In general, The choice
of t does not matter much as long as t is much larger than the
square of the average distance (d0 ) of any point to its k nearest neighbors. All experiments in this paper use t = 30d02 .
Appendix B compares the performance of our new discrete
Laplacian for point clouds with previous PCD-Laplacian and
mesh-Laplacian.
4.1. Eigen-gradients
We sample the unit sphere S non-uniformly to produce a set
of about 2000 points, and consider the function f (x, y, z) =
(sin(5x) + 5)(y3 + 5)ez — the choice of the sphere is so that
we can know the ground truth, and the input function is an
arbitrary test function combining polynomials and exponential functions. Results from other test functions are similar.
In the top row of Figure 2, we show a sequence of the gradients computed with decreasing number of eigenvectors. The
plots on the bottom row show the average error between the
angle and the magnitude (length) of our eigen-gradient vectors compared with the true gradient vectors. We can see that
in general, few eigenvectors (with lowest eigenvalues) suffice to reconstruct the gradient faithfully (with small error).
The reconstruction error for uniformly sampled data or for
more densely sampled data is much smaller, and is reported
in Table 3 in Appendix B. In practice, we usually compute
only the top 100 eigenvectors.
In Figure 3, we study the performance of our eigengradients under noise. The eigen-gradients are computed
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

C.Luo & I.Safa & Y.Wang / Approximating Gradients

(b)

(c)

(d)

50

50

40

40

Magnitude error (%)

Angular error (degree)

(a)

30

20

10

0

1503

(e)

30

20

10

1

10

2

10
number of eigenfunctions

3

10

0

1

10

2

10
number of eigenfunctions

(f)

3

10

(g)

Figure 2: from (a) to (e) the gradient field reconstructed from 1500, 50, 20, 10, and 5 eigenvectors respectively. in (f) and (g)
the magnitude and angular error plots with their standard deviation.

with the top 100 eigenvectors. In addition to ground truth, we
also compare our method with a discrete gradient computed
from input points P by a local fitting method. Specifically,
for every point p, we take its k-nearest neighbors (k = 15 in
this experiments), and locally fit a quadratic function based
on these neighboring points. We refer to the gradient computed from the fitting function as Fit-gradient. This method
inherently smoothes both the underlying manifold (via fitting a tangent space) and the input function (via fitting a
quadratic function) but in a local way. Hence it is reasonably robust under small amount of noise. The reconstruction errors of these methods are shown in Figure 3 (b). In
the absence of noise (function noise or surface noise), the
quadratic fitting gives better results as measured by the average angular deviation and average magnitude deviation from
the ground truth, however once noise is introduced the eigengradients method shows improvement over the quadratic fitting method. We have also performed 15 Laplace smoothing
iterations on both the input function and the input surface,
and even though the Fit-gradient field appears smoother afterwards, the reconstruction error does not improve much in
the case of function smoothing (7.7 degree angular, 14.95%
magnitude for 3% function noise), and it gets worse in the
case of surface smoothing for surface noise.
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

(a)

no noise
1% surf-noise
3% func-noise
both noises

Avg. error in gradient vectors
our method
quadratic fitting
angle length angle
length
2.72
4.6%
1.1
1.79%
2.98
8.52%
4.03
5.72%
3.51
6.26% 13.03 19.94 %
3.33
4.58%
8.65
14.40%
(b)

Figure 3: top row from left to right, the ground truth gradient field for f (x, y, z) = (sin(5x) + 5)(y3 + 5)ez , gradient
field by eigen method for 1% surface noise and 2% function
noise, and gradient field by quadratic fit for the 1% surface
noise and 2% function noise. The table shows the angular
and magnitude errors for different noise configurations.

1504

C.Luo & I.Safa & Y.Wang / Approximating Gradients

Our eigen-gradient is not only robust with respect to
noise, but more importantly, it provides a simple way to simplify the gradient field at multiple scales, by using fewer
eigenvectors. An example is shown in Figure 4, where we
have a molecular surface with 8952 sample points. The input function is the so-called Connolly function widely used
to capture protrusions and cavities in molecules [Con86].
Figure 4 (a) shows the eigen-gradient field reconstructed using 1000 and 50 eigenvectors. Note that some small bumps
(low-persistence critical points) are smoothed as we reduce
the number of eigenvectors used. Figure 4 (b) shows the
Fit-gradient fields computed by the quadratic fitting method
(left) and the simplified version computed by first performing 30 iterations of Laplace smoothing operators (right).
To compare these two types of simplifications more quantitatively, we introduce a quantity to measure the smoothness
of an input vector field V = {v(p)} p∈P : given a sample point
p, the local smoothness ρ(p) at p is the average of the dotproduct between v(p) and the vectors associated with each
of its 15-nearest neighbors. The smoothness ρ(V ) of the en1
tire vector field V is ρ(V ) = |P|
∑ p∈P ρ(p). It takes the localfitting method 30 iterations of Laplace smoothing operations
(right image in Figure 4 (b)) to achieve a similar smoothness
as the eigen-gradient field computed from 50 eigenvectors
(right image in Figure 4 (a)). Simplifying it at larger scale
will require even more number of Laplace smoothing iterations. In other words, a coarser-resolution gradient field requires more iterations of smoothing when using quadratic
fitting method (which eventually is equivalent to increasing
the number of neighboring points that we need to consider
for each input point). Hence it requires more time. On the
other hand, smoothing using our method simply means using less eigenvectors, while in the eigenspace, still the same
(small) number of neighboring points are needed to compute the projection (thus the time needed is in fact slightly
smaller).

(a)

(b)

Figure 4: (a) Gradient field computed from 1000 eigenvectors (left) and 50 eigen vectors (right). (Note the critical
point on the leftmost protrusion on the left image disappearing on the right image). (b) Gradient field computed from
quadratic fitting (left), and the same field recomputed after
being smoothed for 30 iterations (right).
Jacobi set. Specifically, for each input point x ∈ P, we
first compute the signed local comparison measure function
κ(x) = ∇E f (x) × ∇E g(x), nx introduced by Natarajan et
al. [SN09], where nx is the normal direction at point x. In the
continuous case, the zero level set of κ is the Jacobi set. For
points input, we check for every x, whether there a change
of sign between x and one of its k-nearest neighbor (k = 10
in our experiments). If the answer is positive, we consider x
to be a potential point in the Jacobi set and add it to J.

4.2. Jacobi Sets
The Jacobi set w.r.t to two scalar function f and g defined
over a manifold is the set of points where the two gradients ∇ f and ∇g are aligned, i.e. ∇ f = λ∇g. In the continuous case the Jacobi set between two functions is a set
of 1-manifolds. Intuitively, the Jacobi set represents points
where f and g co-vary in the same way, and is a good way
to show the correlation between f and g [EHNP04]. Edelsbrunner et al. [EH02] proposed an elegant algorithm to construct the Jacobi set from a mesh in a combinatorial manner.
Given the differential nature of the Jacobi set, it is sensitive
to noise, and Natarajan et al. [SN09] presents a method to
simplify the set for meshes, by using level-set methods. Approximating and simplifying Jacobi sets from point clouds
data remains a challenging problem.
Our framework can be easily extended to compute a
set of points J that are potentially around the underlying

(a)

(b)

(c)

Figure 5: Jacobi set computed by (a) the combinatorial algorithm [EH02] from the mesh, (b) by eigen-gradients from
the point cloud (b) uses 1000 eigenvectors and (c) uses 50
eigenvectors.
In Figure 5, we show the Jacobi set between the Connolly
function and the coordinate function f (p) = x defined on the
molecular surface. Note that reducing the number of eigenfunctions can simplify the Jacobi set as shown in (c).
Finally, since the Laplace-Beltrami operator is isometry
invariant, for a model subjected to isometric deformation, it
is sufficient to compute the Laplacian for one model only and
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

C.Luo & I.Safa & Y.Wang / Approximating Gradients

(a)

(b)

(c)

(d)

(e)

1505

(f)

(g)

Figure 6: isometric deformation: from (b) to (f) is the Jacobi set computed for f (p) = x and f (p) = z by our method. figures
(a) and (g) are Jacobi sets computed from the PL mesh for comparison. note that the Laplacian was computed only once for
one model and used for all five of them producing correct results.
use it for all the other deformed models. To illustrate this, we
compute the contour of a sequence of human poses, roughly
isometrically deformed from each other. The contour can be
computed as the Jacobi set with respect to the two coordinates functions f (p) = x and g(p) = y. In Figure 6, (b) – (f)
show the contours for various poses using the same Laplacian computed from one single model. Figures (a) and (g)
show the Jacobi curves computed from the mesh structure of
the poses shown in (b) and (f), respectively. Other than producing cleaner looking contours, our method is meshless,
and thus one need not to maintain a valid mesh structure as
we deform the models.

Figure 7: Critical points computed from the mesh structure
(left) and from the point cloud with 250 eigenvectors (right).

4.3. Critical Points
Since Φ induces a homeomorphism between M and Ω, DΦ|x
can be considered as a change of coordinate system for the
local chart at x. Hence by the Morse Lemma [Mil63], the
criticality of a point and its index are not affected.

points or are slightly shifted version of them with maximum
shifted distance of 0.14.

Claim 4.1 Given a morse function f : M → R, x ∈ M is a
critical point of f with index i if and only if Φ(x) is a critical
point of F in Ω with index i.
When only point cloud is given, by virtue of the above
claim, we can compute the critical points using solely the
magnitudes of eigen-gradients. Specifically, for every point
p, we say that it is a critical point if its eigen-gradient value
is close to zero, and it has the smallest magnitude among its
k nearest neighbors (k = 10 in our experiments). The latter
criteria is because points close to a critical point may also
have very small gradient value.
In Figure 7, we consider the following function defined on
the unit sphere: f (p) = sin(θ)sin(6θ)sin(2φ), where θ and φ
are the spherical coordinates of p. All critical points computed by using the combinatorial mesh structure (shown in
(a)) were recovered from the point cloud (with 2562 points)
by the eigen-gradient method (shown in (b)). The recovered critical points either coincide exactly with mesh critical
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

Figure 8: Left: critical points of the Connolly function computed from the mesh structure. Right: those computed from
point clouds by the eigen-approach using 200 eigenvectors.
For general surfaces, critical points constructed by this
eigen-gradient method tends to be a smoothed version of
those computed from the mesh. Further reducing the number of eigenfunctions results in a further simplification of
its critical points. Figure 8 shows this effect for a molecular
surface (with 8952 points). In some sense, those less important critical points (w.r.t. the input function which captures
protrusions and cavities) are removed.
Timing. The timing for computing the eigen-gradients
for all models in this paper is summarized in Ta-

C.Luo & I.Safa & Y.Wang / Approximating Gradients

1506

ble 1. All the experiments in this paper were done
in Matlab 7.8 on an Intel CPU Q6600 2.4GHz with
6GB RAM. The code is available on the web via
http://www.cse.ohio-state.edu/∼luoc/eigen-gradient.htm.
The largest model we have tested is the dragon model
with about 100K vertices. The main bottleneck is the
computation of the eigenvectors for the discrete Laplace
operators (the decomposition column in Table 1). Note
that the discrete Laplacian is a sparse matrix. Furthermore,
from the error plots (f) and (g) in Figure 2, it appears that
in general, only a few hundred eigenvectors are necessary
to reconstruct the gradient field faithfully. Table 1 reports
the timing required to compute the first 100 eigenvalues /
eigenvectors of the discrete Laplacian, which is computed
by the sparse eigen solver eigs function from the ARPACK
package. For larger data sets, it will be necessary to use a
more efficient method to either compute or approximate the
top few eigenvectors [VL08]. We leave this as an important
future direction.

sphere # v 2000
sphere # v 8000
molecular # v 8952
human body # v 12500
dragon # v 99926

decomposition
9
67
78
127
1904

knn
0.5
4
4.2
6
263

grad
1.5
6
6.3
10
132

Table 1: Timing in seconds for different models with different number of vertices. The gradient is reconstructed from
100 eigen vectors.
We also remark that the current implementation uses
brute-force to compute the k-nearest neighbors in the
eigenspace, which takes more than half of the timing needed
to compute the eigen-gradients after the eigenvectors are
given. This can be improved by using a space-decomposition
data structure such as octrees or BSP trees.
5. Conclusion and Discussions
In this paper, we proposed to approximate the gradients of
a function defined over a point cloud (or a mesh) by computing it under the so-called diffusion distance metric. One
of the main advantages of this spectral approach is that one
can simplify, at various scales, the gradient field with respect
to both, the input function and the underlying manifold, in
a unified and simple way. We demonstrate that our eigengradients can faithfully reflect true gradients, as well as effectively simplify them. We also present two preliminary applications for applying the eigen-gradients. Our results hold
for points from any smooth and compact d-manifold embedded in Rm .
Similar to previous spectral mesh processing methods
[ZvKD09], our approach is a spectral point-clouds processing method. It will be interesting to see what other types
of information we can compute once the discrete Laplace

operator for the point clouds is constructed. Our new discrete point-cloud-Laplace operator will potentially facilitate
the investigation in this direction.
Our method requires the computation of Laplacian eigenfunctions, which can be costly for large data set. There have
been several nice works for improving the efficiency of such
computation, using the fact that the discrete Laplacian matrices are typically sparse (see e.g [VL08] and [ZvKD09]). We
remark that the Laplacian eigenfunctions seem to be well
approximated even with a small number of sample points.
Hence one potential way to handle large data (such as rangescan data) is to first sub-sample it. An alternative direction
we will investigate is whether one can obtain sufficient information just by local approximation of the Laplace operator.
Finally, we will also explore other procedures / geometric
quantities, whose computation can be made easier by considering a different metric space.
Acknowledgement. The authors thank anonymous reviewers for very helpful suggestions. This work is supported
in part by he Department of Energy (DOE) under grant
DE-FG02-06ER25735, by the National Science Foundation
(NSF) under grants CCF-0747082 and DBI-0750891.
References
[AGP∗ ]

A LEXA M., G ROSS M., PAULY M., P FISTER H., S TAM M., Z WICKER M.: Point-based computer graphics.
ACM SIGGRAPH 2004 Course Notes.
MINGER

[AK04] A MENTA N., K IL Y. J.: Defining point-set surfaces.
ACM Trans. Graph. 23, 3 (2004), 264–270.
[BBG94] B ÉRARD P., B ESSON G., G ALLOT S.: Embedding riemannian manifolds by their heat kernel. Goem. Fun. Anal. 4, 4
(1994), 374–398.
[BN05] B ELKIN M., N IYOGI P.: Towards a theoretical foundation for laplacian-based manifold methods. In COLT (2005),
pp. 486–500.
[BN08] B ELKIN M., N IYOGI P.:
Eigenmaps. Preprint, 2008.

Convergence of Laplacian

[BSW08] B ELKIN M., S UN J., WANG Y.: Discrete laplace operator on meshed surfaces. In SCG ’08: Proceedings of the
twenty-fourth annual symposium on Computational geometry
(New York, NY, USA, 2008), ACM, pp. 278–287.
[BSW09] B ELKIN M., S UN J., WANG Y.: Constructing Laplace
operator from point clouds in Rd . In Proc. 20th ACM-SIAM Sympos. Discrete Algorithms (2009), pp. 1031–1040.
[Con86] C ONNOLLY M. L.: Measurement of protein surface
shape by solid angles. J. Mol. Graphics 4 (1986), 3 – 6.
[Dod78] D ODZIUK J.: Finite-difference approach to the hodge
theory of harmonic forms. American Journal of Mathematics 98,
1 (1978), 79–104.
[EH02] E DELSBRUNNER H., H ARER J.: Jacobi sets of multiple
Morse functions. In Foundations of Computational Mathematics, Minneapolis 2002, Cucker F., DeVore R., Olver P., Sueli E.,
(Eds.). Cambridge Univ. Press, 2002, pp. 37–57.
[EHNP04] E DELSBRUNNER H., H ARER J., N ATARAJAN V.,
PASCUCCI V.: Local and global comparison of continuous functions. In IEEE Conference on Visualization (2004), pp. 275–280.
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

C.Luo & I.Safa & Y.Wang / Approximating Gradients
[FM03] F RIES T., M ATTHIES H.: Classification and Overview
of Meshfree Methods. Tech. rep., Informatikbericht-Nr. 200303, Technische Universität Braunschweig, Brunswick, Germany,
2003.
[GP07] G ROSS M., P FISTER H.: POINT-BASED GRAPHICS.
Elsevier/Morgan Kaufmann, 2007.
[HV02] H AR -P ELED S., VARADARAJAN K. R.: Projective clustering in high dimensions using core-sets. In Proc. 18th Annu.
ACM Sympos. Comput. Geom. (2002), pp. 312–318.
[Laf04] L AFON S.: Diffusion Maps and Geodesic Harmonics.
PhD. Thesis, Yale University, 2004.
[Lev98] L EVIN D.: The approximation power of moving leastsquares. Math. Comput. 67, 224 (1998), 1517–1531.
[Lev06] L EVY B.: Laplace-beltrami eigenfunctions: Towards an
algorithm that understands geometry. In IEEE International Conference on Shape Modeling and Applications, invited talk (2006).
[LSW09] L UO C., S UN J., WANG Y.: Integral estimation from
point cloud in d-dimensional space: A geometric view. In Proc.
25th ACM Sympos. on Comput. Geom., to appear (2009).
[MDSB02] M EYER M., D ESBRUN M., S CHRÖDER P., BARR
A. H.: Discrete differential geometry operators for triangulated
2-manifolds. In Proc. VisMath’02 (Berlin, Germany, 2002).
[Mil63] M ILNOR J.: Morse Theory. Princeton Univ. Press, New
Jersey, 1963.
[MW06] M UKHERJEE S., W U Q.: Estimation of gradients and
coordinate covariation in classification. J. Machine Learning Research 7 (2006), 2481–2514.
[PG01] PAULY M., G ROSS M.: Spectral processing of pointsampled geometry. In SIGGRAPH ’01: Proceedings of the 28th
annual conference on Computer graphics and interactive techniques (New York, NY, USA, 2001), ACM, pp. 379–386.
[PMS09] P.W.J ONES , M AGGIONI M., S CHUL R.: Universal local parametrizations via heat kernels and eigenfunctions of the
Laplacian. Submitted. A short version has been accepted to
PNAS., 2009.
[PP93] P INKALL U., P OLTHIER K.: Computing discrete minimal
surfaces and their conjugates. Experimental Mathematics 2, 1
(1993), 15–36.
[Rus07] RUSTAMOV R. M.: Laplace-beltrami eigenfunctions for
deformation invariant shape representation. In SGP ’07: Proceedings of the fifth Eurographics symposium on Geometry processing (Aire-la-Ville, Switzerland, Switzerland, 2007), Eurographics Association, pp. 225–233.

1507

[VL08] VALLET B., L ÉVY B.: Spectral geometry processing
with manifold harmonics. Computer Graphics Forum (Proceedings Eurographics) 27, 2 (2008), 251–260.
[WGMM08] W U Q., G UINNEY J., M AGGIONI M., M UKHER JEE S.: Learning gradients: predictive models that infer geometry
and dependence. Manuscript, 2008.
[WMKG08] WARDETZKY M., M ATHUR S., K ÄLBERER F.,
G RINSPUN E.: Discrete laplace operators: no free lunch. In SIGGRAPH Asia ’08: ACM SIGGRAPH ASIA 2008 courses (New
York, NY, USA, 2008), ACM, pp. 1–5.
[Xu04] X U G.: Discrete laplace-beltrami operators and their convergence. Comput. Aided Geom. Des. 21, 8 (2004), 767–784.
[ZvKD09] Z HANG H., VAN K AICK O., DYER R.: Spectral mesh
processing. Computer Graphics Forum, accepted (2009).

Appendix A: Discrete Laplace Operator from Point Cloud
Given a d-dimensional compact manifold M isometrically
embedded in Rm , the medial axis of M is the closure of the
set of points in Rm that have at least two closest points in M.
For any point p ∈ M, the local feature size at p, is the closest
distance from p to the medial axis of M. The reach ρ of M
is the infimum of the local feature size at any point in M.
In this paper, we assume that the manifold M has a positive
reach. Let P denote a set of sample points on M. We say that
P is an ε-sampling of M if p ∈ M for any p ∈ P, and for any
point x ∈ M, there exists q ∈ P such that x − q ≤ ερ. P is
an (ε, δ)-sampling of M if P is an ε-sampling of M and that
any two points in P are at least δρ away from each other.
In [BN05], the authors proposed to approximate the
Laplace operator in the discrete setting using its relation to
the heat operator Ht = e−tΔ . Specifically, by Taylor expansion, we have
Δ = lim

t→0

H0 − Ht
I − Ht
= lim
,
t
t
t→0

where H0 , the heat operator at time 0, is the identity operator
I. On the other hand, for any scalar function f on M, it is
well-known that
Ht f (x) =

M

ht (x, y) f (y)dµ(y).

[RWP06] R EUTER M., W OLTER F.-E., P EINECKE N.: Laplacebeltrami spectra as "shape-dna" of surfaces and solids.
Computer-Aided Design 38, 4 (2006), 342–366.

Unfortunately, the analytical expression of the heat kernel
ht (x, y) is not known other than for very few cases. For example, if the underlying manifold is the Euclidean space

[She02] S HEWCHUK J.: What is a good linear finite element?
Interpolation, conditioning, anisotropy, and quality measures.
Preprint. (A short version appeared in IMR 2002.), 2002.

− 4t
1
Rd , then the heat kernel is ht (x, y) = (4πt)
. It
d/2 e
turns out that for general manifold, this Gaussian kernel approximates the heat kernel well enough in the sense that the
Laplace operator can be approximated via the Gaussian kernel. More specifically, it is shown in [BN05] that

[Sib81] S IBSON R.: A brief description of natural neighbour interpolation. In Interpreting Multivariate Data, Barnet V., (Ed.).
John Wiley & Sons, Chichester, 1981, pp. 21–36.
[SN09] S UTHAMBHARA N., N ATARAJAN V.: Simplification of
jacobi sets. In TopoInVis (2009).
[Sor06] S ORKINE O.: Differential representations for mesh processing. Computer Graphics Forum 25, 4 (2006), 789–807.
[Tau95] TAUBIN G.: A signal processing approach to fair surface design. In Proc. 22nd Internat. Conf. Comput. Graphics and
Interactive Techniques (SIGGRAPH) (1995), pp. 351–358.
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

x−y 2

1
t→0 t(4πt)d/2

Δ f (x) = lim

e−
M

x−y 2
4t

( f (x) − f (y))dµ(y).
(4)

In the discrete setting, the task is then to approximate the above integral from point clouds. Set Gt (x, y) =

C.Luo & I.Safa & Y.Wang / Approximating Gradients

1508
1
.
t(4πt)d/2

For a set of points P = {p1 , . . ., pn } randomly
sampled from the uniform distribution, this can be easily
achieved by using Monte Carlo integration, and computing
1 n
n ∑ j=1 Gt (pi , p j )[ f (pi ) − f (p j )]. This approximates Δ f (pi )
as t goes to 0 based on the Law of Large Numbers. However,
in non-statistical setting, it is necessary to augment the summation with certain weights.
On the other hand, given a twice-differential function g :
M → R, it is shown in [LSW09] that one can approximate
n
M g(y)dµ(y) by It g = ∑i=1 g(pi )Ai , where Ai is the Voronoi
weight defined as follows. For each point pi , take the set
of points Q that are within distance cερ for some constant
c (say, c = 10). Locally approximate the tangent space T
at pi by fitting the best plane through Q using the algorithm
from [HV02]. Let Q˜ denote the projection of the set of points
Q onto T . Construct the Voronoi diagram for Q˜ in T and
Ai is the volume (area for 2-manifolds) of the Voronoi cell
containing pi . It is shown in [LSW09] that It g − M g ∞ =
O(ε + ε3 /δ2 ) for an (ε, δ)-sample P with an appropriately
chosen t, where δ ≥ ε3/2−ξ for an arbitrary value ξ > 0. This
implies that It g converges to M g as ε → 0.
Combining these two results, we approximate Lt f (pi ) by
L f (pi ) :=

n

∑ Gt (pi , p j )A j f (p j ).

j=1

Hence the discrete Laplace operator from a set of n points is
an n × n matrix L with
L[i][ j] =

Gt (i, j)A j
i= j
Gt (i, i)Ai − ∑nj=1 [Gt (i, j)A j ] Otherwise.

Using similar techniques from [BSW08, LSW09], it is easy
to show that
Theorem A.1 Let the point set Pε,δ be an (ε, δ)-sampling

mesh-Laplace, and with the theoretically better point-clouds
Laplace operator proposed in [BSW09], denoted by PCDLaplace. As shown in Table 2, the performance of our new
Voronoi-Laplacian is close to that of the mesh-Laplacian,
and much better than the PCD-Laplacian.
f =x
f = x2
f = ex
2.91 / 6.05
2.03 / 3.32
2.64 / 3.25
2.36 / 4.40
1.81 / 2.45
2.20 / 2.73
5.03 / 6.97
4.84 / 5.42
5.00 / 5.29
2562 points uniform sphere
mesh-L
3.00 / 6.89
3.37 / 4.30
3.44 / 4.00
Voronoi-L
3.50 / 9.11
3.59 / 5.38
3.81 / 4.80
PCD-L
15.85/33.21 13.77/19.54 15.22/21.64
5000 points non-uniform sphere
mesh-L
1.04 / 2.94
2.87 / 3.16
2.33 / 2.70
Voronoi-L
1.23 / 3.58
2.90 / 3.26
2.37 / 2.67
PCD-L
11.64/15.18 12.28/14.06 11.96/12.69
10000 points non-uniform sphere
function
mesh-L
Voronoi-L
PCD-L

Table 2: Normalized L2 /L∞ error for different sampling on
S2 , all numbers are scaled in percentage.
Table 3 shows the comparisons of the gradient field estimated by the three discrete Laplace operator. Again, the
gradient fields estimated by Mesh-Laplace and VoronoiLaplace are both very close to ground truth and also much
better than the gradient field estimated from the PCDLaplace operator. Note that our formula to estimate the gradient magnitude does not apply to PCD-Laplace, due to
its complex eigenvalues and non-orthogonal eigenfunctions.
Thus we show ”NA” in the table.
func
f1
f2

1

of M. Set t(ε) = ε 2+α for an arbitrary fixed number α > 0.
Let LtP denote the discrete Laplace operator as constructed
above for point set P with parameter t. Then for any function
f ∈ C2 (M), we have that
P

ε,δ
f −Δf
lim sup Lt(ε)

ε→0 Pε,δ

∞

= 0,

where the supremum is taken over all (ε, δ)-sampling of M
satisfying that δ ≥ ε3/2−ξ for an arbitrary positive value ξ.
Note in 3.2.1 we replace the parameter t with h, this is to
avoid confusion with the parameter t used in the diffusion
metric.

f1
f2
f1
f2

magnitude error
median angle diff. (◦ )
0.31%/0.26%/NA 0.324/0.316/0.634
1.77%/1.70%/NA 0.524/0.519/0.704
2562 points uniform sphere
3.38%/3.39%/NA 0.874/0.810/1.811
5.28%/5.30%/NA 1.337/1.329/1.930
2000 points non-uniform sphere
2.32%/2.32%/NA 0.574/0.546/1.549
3.47%/3.44%/NA 0.917/0.902/1.735
5000 points non-uniform sphere

Table 3: Error of gradient field for Voronoi-Laplace/meshLaplace/PCD-Laplace. Two input functions tested are f1 = x
and f 2 = (sin(2x) + 2)(2 − cos(2y)).

Appendix B: Performance of Discrete Laplace Operator
We now show some experimental results comparing our
new discrete Laplace operator from point clouds, denoted
by Voronoi-Laplace, with the discrete Laplace operator constructed from mesh as proposed in [BSW08], denoted by
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

