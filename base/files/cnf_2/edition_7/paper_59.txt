Volume 28 (2009), Number 2

EUROGRAPHICS 2009 / P. Dutré and M. Stamminger
(Guest Editors)

Compressive Dual Photography
Pradeep Sen and Soheil Darabi
Advanced Graphics Lab
University of New Mexico, New Mexico, USA

Abstract
The accurate measurement of the light transport characteristics of a complex scene is an important goal in computer graphics and has applications in relighting and dual photography. However, since the light transport data
sets are typically very large, much of the previous research has focused on adaptive algorithms that capture them
efficiently. In this work, we propose a novel, non-adaptive algorithm that takes advantage of the compressibility of
the light transport signal in a transform domain to capture it with less acquisitions than with standard approaches.
To do this, we leverage recent work in the area of compressed sensing, where a signal is reconstructed from a few
samples assuming that it is sparse in a transform domain. We demonstrate our approach by performing dual photography and relighting by using a much smaller number of acquisitions than would normally be needed. Because
our algorithm is not adaptive, it is also simpler to implement than many of the current approaches.
Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Picture/Image Generation—
Digitizing and scanning I.4.1 [Image Processing and Computer Vision]: Digitization and Image Capture—Reflectance

1. Introduction
In the quest for photorealism in computer graphics, there
has been growing interest in recent years in making accurate measurements of the light transport characteristics for
complex scenes. This light transport, after all, accounts for
the global illumination effects that give the real world its
appearance and make images look realistic. Although the
measurement of the complete light transport function in the
presence of occluders would involve measuring the 5-D light
field [LH96] for both incident and outgoing illumination, it
is satisfactory for most applications to assume that the scene
is confined to a region of space and measure only the light
going into and out of this region, thereby reducing the function to the mapping of an incident 4-D light field to an outgoing 4-D lightfield, also known as an 8-D reflectance function [DHT∗ 00].
Traditionally, these reflectance functions (or slices of
them) have been measured by point-sampling the signal
with either structured illumination patterns or point light
sources while measuring the outgoing radiance with cameras or sensors. Examples of the active illumination sources
include 2-D displays [ZWCS99], scanning laser illumination [HED05], domes of light sources [WGT∗ 05, ECJ∗ 06],
projector illumination [MPDW03,SNB03] and moving light
sources [DHT∗ 00]. Since the signal can only be measured
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

(a)

(b)

(c)
(d)
Figure 1: Our work significantly accelerates the process of
dual photography, allowing us to transform images such as
the one in (a) into dual images taken from the point-of-view
of the projector (b). Since the light transport between the
camera and projector has been captured, these images can
also be relit in both the primal (c) and dual domain (d). Image (256 × 256) was captured using 600 patterns.
at discrete locations, interpolation is typically used to fill in
the gaps in measurement to reconstruct the entire continuous
signal for use in either image-based rendering or relighting.

610

P. Sen & S. Darabi / Compressive Dual Photography
camera
scene

projector

(a)
(b)
(c)
(d)
Figure 2: Overview of the algorithm. (a) One thousand 256 × 256 Bernoulli binary patterns like the one shown (black is -1
and white is +1) are illuminated by a projector onto a scene (b) and the reflected light is measured by a camera. Images like the
one in (c) are captured by the camera and processed with our compressed-sensing algorithm to reconstruct the complete light
transport, which can be used to generate the dual photograph of the scene (d). The dual image computed by our algorithm is of
resolution 256 × 256 and it shows the scene from the point-of-view of the projector as illuminated by the camera.
The fundamental problem with the acquisition of reflectance
functions is that the size of these signals is extremely large.
For example, if we sample each dimension to 103 resolution, the entire 8-D reflectance function would require 1024
samples to be represented.
However, it is well known that the reflectance functions
are compressible in another domain [MPDW04], so after acquisition they can be compressed by transform-coding techniques such as wavelets. In this work, we ask the fundamental question: is it possible to exploit the compressibility of
the signal in a transform domain to accelerate its acquisition? This is a problem studied in the past, with researchers
typically measuring the signal directly in the transform basis
by illuminating it with the appropriate basis functions and
hoping to measure only the largest k coefficients [PD03].
Not only is this difficult to do because of the dynamic range
and quantization of the illumination device, but it also requires that we know the position of the largest coefficients
of the signal in the transform domain. Without knowing this,
we would have to illuminate all the basis functions, which
is equal to the size of the original data set. Researchers in
the past have attempted to use elaborate adaptive algorithms
(such as Peers et al.’s error tree [PD03]) to try to locate the
positions of the largest coefficients.
In this work, we propose a novel approach to accelerate
the acquisition of light transport by leveraging results from
the growing field of compressed sensing (CS), in which a
signal can be faithfully reconstructed from a small set of
samples by exploiting its sparsity in a transform domain.
Our approach uses a small set of simple binary illumination
patterns followed by a CS-based reconstruction post-process
which automatically finds the largest coefficients of our signal in a transform domain. This results in high quality light
transport matrices from a small number of acquisitions.
To demonstrate the feasibility of our approach, we focus on a specific application of light transport called dual
photography, which was first demonstrated by Sen et al. in

2005. The idea behind dual photography is that by measuring the complete light transport between a projector-camera
pair, we can virtually interchange the two and compute an
image from the point-of-view of the projector as illuminated
by the camera. Because the complete pixel-to-pixel transport
between the projector and camera is also known, we can relight both the primal and dual images with arbitrary light
patterns, as shown in Figure 1. Figure 2 shows an overview
of our approach.
Despite the fact that compressed sensing has been a hot
topic in other areas for a couple of years, the only published
work to date in a graphics-related area using compressed
sensing is the work by Gu et al. [GNG∗ 08], which uses it to
efficiently capture light transport in participating media. Recently, we have also become aware of a new technical report
by Peers et al. which, like our work, uses compressed sensing theory to accelerate the acquisition of reflectance functions [PML∗ 08]. Since their work was done independently
and concurrently to our own, we have formulated the problem in different but equivalent ways, each which offers distinct advantages and tradeoffs. We discuss these differences
in more detail after we present our approach in Section 4.
This paper makes two specific contributions. First, along
with the previously mentioned work by Gu et al. [GNG∗ 08]
and Peers et al. [PML∗ 08], we present one of the first applications of compressed sensing to solve a problem in computer graphics. Specifically, we are the first to demonstrate
how we can use CS theory to accelerate the acquisition of
the light transport between a projector and a camera, and
use this to perform dual photography more efficiently than
with previous approaches. In addition, because we capture
a sparse approximation of the complete 4-D light transport
between pixels in the projector and pixels in the camera, we
can perform image-based relighting in an efficient manner
and project novel patterns onto the scene. Second, our formulation of the relighting problem into the framework of
compressed sensing gives us more flexibility in our choice
of compression basis because it decouples the lighting patc 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

P. Sen & S. Darabi / Compressive Dual Photography

terns from our choice of compression basis. Essentially, we
capture our data sets by illuminating only a simple set of
Bernoulli patterns, without need for projecting the patterns
into a compression basis. To our knowledge, this is the first
work to capture light transport using only random Bernoulli
patterns, and it is one of the fundamental differences from
the related concurrent work of Peers et al. [PML∗ 08], where
the illumination pattern must be projected into the compression basis. Not only does this projection result in more patterns, but it also makes it difficult to utilize more sophisticated compression bases, e.g. the Daubechies-8 wavelet,
which can be more efficient at compressing light transport.
We now begin the paper with a review of previous work in
light transport acquisition.
2. Prior Work in Light Transport Acquisition
The measurement of reflectance functions and light transport has been of interest to the graphics community for some
time now and there has been significant previous research
in this area (e.g. [ZWCS99, DHT∗ 00, MPDW03, GLL∗ 04,
FBLS07]). Previous approaches in reflectance measurement
typically overcome the issue of the large size of the data sets
by measuring only slices of the full 8-D projection function.
For example, it is typical to fix the viewpoint and use only
2-D incident illumination either from a projector, a CRT display, or a dome of diffuse point lights, resulting in a 4-D
slice [ZWCS99,DHT∗ 00]. Other approaches have attempted
to increase the dimensionality of the slices by moving the
projector during acquisition to provide an incident light field
(measuring a 6-D slice) [MPDW03], or by exploiting the
symmetry in the transport matrix to capture an 8-D data
set [GTLL06]. There has also been significant research into
improved acquisition devices such as light stages [DHT∗ 00],
including some that can measure reflectance functions at
high-speed for performance capture [WGT∗ 05, ECJ∗ 06].
Although most of the applications involve scenes of solid
objects, there has also been work in measuring light transport for volumetric media [GNG∗ 08] and for capturing
translucent objects [GLL∗ 04]. In addition, there has been
theoretical work on the basis of illumination multiplexing
[SNB03]. To improve acquisition times, many adaptive algorithms have been proposed, such as Matusik et al. [MLP04],
Peers and Dutré [PD03], Sen et al. [SCG∗ 05], and Fuchs et
al. [FBLS07]. Although our work accelerates acquisition, it
does so in a non-adaptive fashion which makes implementation easier since frames do not need to be processed during
acquisition.
Our approach is inspired by Sen et al.’s work on dual
photography [SCG∗ 05], where Helmholtz reciprocity can be
used to virtually interchange the camera and projector in a
scene. This initial work has been extended to the dual light
stage [HED05] and symmetric photography [GTLL06]. Unlike the adaptive algorithms of Sen et al. and Garg et al.,
our approach is non-adaptive and accelerates the acquisition by directly measuring the light transport into a transc 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

n
m
k
r
x
xˆ
x˜
S
Ψ
ΨT
A

611

size of our original signal (size of per-pixel reflectance)
size of the support of our signal in the transform domain
number of linear measurements taken to estimate our signal
the resolution of the camera in our experiments
n × 1 vector in spatial domain
n × 1 m-sparse vector in transform domain, s.t. xˆ = Ψx
k × 1 “measured” vector of x in the spatial domain
k × n sampling matrix, s.t. x˜ = Sx
n × n linear orthogonal transform “compression” matrix
inverse transform (Ψ−1 = ΨT ), so x = ΨT xˆ
k × n “measurement” matrix, A = SΨT

Table 1: Notation used in this paper
form basis, where it is more compressed. Other work that
has tried to exploit compressibility in a transform basis includes the wavelet matting and wavelet noise work of Peers
and Dutré [PD03, PD05].
Our work differs from these previous approaches in several important respects. First of all, we do not illuminate
wavelet patterns, but rather simple, binary Bernoulli patterns. These patterns are much simpler to produce and are
more conducive to production environments. Furthermore,
because previous approaches illuminated the basis functions,
they were typically limited to relatively inefficient binary
wavelets such as Haar, because of the difficulties in projecting more sophisticated wavelets with a standard projector due to its limited quantization and dynamic range. Our
work, on the other hand, decouples the illumination pattern
from the compression basis, which allows us to choose the
wavelet basis that best compresses the data we are acquiring.
For example, to capture the dual photograph for the single
pixel example in Figure 7, we use the Daubechies-8 wavelet
for compression since it is better suited for image compression than Haar [GKG99]. This results in an image of better
quality with less acquisitions than previously possible.
As in Peers and Dutré’s wavelet noise work, our approach
does not require any real-time processing of the data. For
example, Peers and Dutré’s earlier work [PD03] requires a
computation of an error tree to keep track of which wavelets
should be projected next. Our algorithm is completely nonadaptive, so the patterns are all pre-computed and can be displayed at an extremely fast framerate, without need of any
computational power for run-time processing. This significantly simplifies the acquisition set up as compared, for example, to the sophisticated adaptive algorithms proposed in
the original dual photography paper by Sen et al. [SCG∗ 05].
With the previous work in perspective, we are now ready
to introduce the theory of compressed sensing in order to describe its application for light transport acquisition and dual
photography.
3. Compressed Sensing Theory
The theory of compressed sensing (CS) demonstrates how a
subsampled signal can be faithfully reconstructed through
non-linear optimization techniques [CRT06, Don06]. Suppose that we represent our continuous signal (the scene, re-

612

P. Sen & S. Darabi / Compressive Dual Photography

flectance function, light field, etc.) as an n-dimensional discrete signal x ∈ Rn where n is large. In theory, x can represent any 1-D signal, but for this discussion we assume it to be
an n-element scalar reflectance function which has been converted into an n × 1 vector (with trivial extension to vectorvalued signals, e.g. RGB transport). We want to estimate this
signal by measuring a small number of linear samples x˜ of
size k, where k n. We can write x˜ = Sx, where S is a sampling matrix that performs the linear measurements on x. In
our application to light-transport acquisition, for example,
the goal is to estimate our unknown reflectance function x
from k samples.
This seems like an impossible feat, given that the k samples yield a (n − k)-dimensional subspace of possible solutions for the original x that would match our given observations. How do we know which one of those possible solutions is our original x? This is where a key assumption
of compressed sensing comes in: we assume that the transformed version of the signal, xˆ , is m-sparse under some basis
Ψ, meaning that it has at most m non-zero coefficients in that
basis (e.g. xˆ 0 ≤ m, where · 0 denotes the 0 norm). This
is not an unreasonable assumption, since we know we are acquiring real-world reflectance functions (as opposed to random white noise) which will be compressible in a transform
domain, e.g. wavelet. We can now write our measurement
process as:

3.1. Restricted Isometry Condition (RIC)
We cannot solve x˜ = Aˆx for xˆ with any arbitrary A if k
n, despite k ≥ 2m. However, we can apply the compressed
sensing framework if matrix A meets the Restricted Isometry
Condition (RIC) [NV07]:
(1 − ε)||v||2 ≤ ||Av||2 ≤ (1 + ε)||v||2

(3)

with parameters (z, ε), where ε ∈ (0, 1) for all z-sparse vectors v. Essentially, the RIC states that a measurement matrix will be valid if every possible set of z columns of A
forms an approximate orthogonal set. In effect, we want
the sampling matrix S to be as incoherent to the compression basis Ψ as possible. Examples of matrices that
have been proven to meet RIC include Gaussian matrices
(where the entries are independently sampled from a normal distribution), Bernoulli matrices (binary matrices drawn
from a Bernoulli distribution), and partial Fourier matrices
(randomly selected Fourier basis functions) [CT06]. In this
work, we use Bernoulli matrices as our sampling matrix because they are easy to create and to project because they
make the best use of the limited dynamic range of the projector. Our choice of compression matrix depends on the complexity of the reflectance function per-pixel as discussed in
Section 4.

(2)

3.2. Greedy Reconstruction algorithms
Although the 1 optimization is considerably more efficient
than the 0 , its running time can still be large because there
is no known strongly polynomial-time algorithm for linear
programming [NV07]. For this reason, the CS research community has started to investigate greedy algorithms to solve
Eq. 2. Orthogonal Matching Pursuit (OMP) was one of the
first such algorithms explored [TG07]. Given the measured
vector x˜ and the measurement matrix A, we can find the coefficient of xˆ with the largest magnitude by projecting x˜ onto
each column of A and selecting the largest | x˜ , a j |, where a j
is the jth column of A. Once we have identified the largest
coefficient of xˆ , we then solve a least-squares problem assuming it is the only non-zero coefficient. We can then use
the new estimate for xˆ to compute the estimated signal x and
subtract it from the original measurements. We then iterate
the algorithm, using the residual to solve for the next largest
coefficient of xˆ one at a time. By iterating m times, we find
an m-sparse approximation of the transform domain vector.

The solution to this problem, however, involves a combinatorial algorithm in which every xˆ with xˆ 0 ≤ m is checked
to find the one that results in the measured samples x˜ . This
problem is known to be NP-complete [CRTV05] and is intractable for any reasonably-sized signal. However, recent
results [CRT06] have spurred growing excitement in the area
of compressed sensing by showing that Eq. 1 can be solved
by replacing the 0 with an 1 -norm ( x 1 = ∑ni=1 |xi |).
As long as the number of samples k = O(m log n) and the
matrix√A meets the RIC (described next) with parameters
(2m, 2 − 1), the 1 optimization will solve correctly for
xˆ [Can08].

Although it is simple and fast, OMP has a major drawback because of its weaker guarantee of exact recovery
than the 1 methods [NV07]. To overcome these limitations, a modification to OMP called Regularized Orthogonal Matching Pursuit (ROMP) was proposed which recovers multiple coefficients in each iteration, thereby accelerating the algorithm and making it more robust to meeting the
RIC [NV07]. Specifically, on every iteration ROMP approximates the transform coefficients in the same way as OMP
and then sorts them in non-increasing order. It then selects
the continuous sub-group of coefficients with the largest energy, with the restriction that the largest coefficient in the

x˜ = Sx = SΨT xˆ = Aˆx

(1)

where A = SΨT is a general k × n measurement matrix. If
we measured x˜ and could solve the system for xˆ , we could
then apply the inverse transform ΨT xˆ to get our desired signal x. Unfortunately, traditional techniques for solving for
xˆ (e.g. inversion, least squares) do not work because Eq. 1
is severely under-determined (since k
n). However, recent breakthroughs in compressed sensing have shown that
if k ≥ 2m and A meets certain properties (see Sec. 3.1), then
Eq. 1 can be solved uniquely for xˆ by looking for the sparsest xˆ that satisfies x˜ = Aˆx (see complete proof in [CRT06]).
Therefore, we can solve the following 0 -optimization problem to find the correct xˆ :
min xˆ

0

s.t. x˜ = Aˆx

c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

613

P. Sen & S. Darabi / Compressive Dual Photography

group cannot be more than twice as big as the smallest member. These coefficients are then added to a list of non-zero coefficients and a least-squares problem is then solved to find
the best approximation for these non-zero coefficients. The
approximation error is then computed based on the measured
results and the algorithm iterates again.
In this work, we use the ROMP algorithm for signal reconstruction, with the slight modification that we limit the
number of coefficiens added in each iteration. We found
experimentally that this yielded better results for our reflectance function experiments.
3.3. Compressibility vs. Sparsity
While the theory of compressed sensing deals primarily with
signals that are m-sparse, most real-world signals are not
necessarily sparse (they do not have a large number of zero
coefficients), but rather compressible in that they have a lot
of small coefficients. This fact is exploited in compression
algorithms such as JPEG [Wal91] and JPEG2000 [TM01],
where many of these small coefficients are quantized or
thrown away. Formally, we can say a signal is compressible
if when we order its transform coefficients in decreasing absolute magnitude, its ith coefficient, |ˆx|(i) , follows the power

law: |ˆx|(i) ≤ R · i−1/p . Here, R describes the radius of the
weak- p ball that contains xˆ and p is a coefficient of decay. Candès et al. have shown that compressed sensing can
be applied to compressible signals because the m-sparse approximation of the transform vector xˆ m has an error that also
decay by the power law: |ˆx − xˆ m |2 = c · m1/2−1/p , where c
is a positive constant [Can06]. This is important because the
signals in our applications (reflectance functions) are typically compressible but not sparse. Our results verify that our
compressed sensing framework is viable for these kinds of
signals.

3.4. Applications of Compressed Sensing
Since its inception just a few years ago, compressed sensing
has been applied to problems in video processing [MW08a],
face recognition [WYG∗ 08], medical imaging [LDP07,
TMB08], bio-sensing [SMB07], and compressive imaging
[EFK07, Gan07, MW08b, TLW∗ 06, WLD∗ 06]. Specifically,
the work on the single-pixel camera by the group at Rice
University [TLW∗ 06, WLD∗ 06] is particularly relevant, because our approach can also be thought of as a single pixel
camera. We compare our approach to theirs in more detail
later in the paper. The first author of this paper has also been
working in this field, applying compressed sensing to the
mapping of wireless channels [MS08, MS09]. The reader is
referred to the Rice University repository for an excellent
resource of current work in compressed sensing [Ric].
Within the graphics community, however, compressed
sensing has not yet been fully explored. The only available
work at the time of publication is Gu et al.’s work on compressive structured light [GNG∗ 08] and the recently released
technical report of Peers et al. [PML∗ 08] that builds on their
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

wavelet noise work. Of these, our approach is more similar
to the Peers et al. work in that we both use CS for accelerating the acquisition of light transport. An important difference, however, is the manner in which we pose the problem in terms of Eq. 1, which allows us to illuminate only
Bernoulli patterns while they have to project their illumination patterns into the compression basis. This significantly
limits the kinds of basis functions that can be used when
compressing a signal.
4. Compressed Sensing for Dual Photography
In 2005, Sen et al. introduced the concept of dual photography, which allows us to virtually interchange the projectors
and camera in a scene after the light transport between them
has been measured, thereby allowing us to compute an image from the point-of-view of the projector as illuminated
by the camera [SCG∗ 05]. To understand how this process
works, we begin by examining the linear equation of light
transport between a single projector-camera pair:
c = Tl

(4)

Here, c is an r × 1 vector of the measured camera pixels, l is an n × 1 vector of the projector pixels which provide
the illumination, and T is the r × n transport matrix that describes the transport between pixels of the projector to pixels of the camera. If measured properly, matrix T encodes
all the global illumination processes such as diffuse-diffuse
interreflections, sub-surface scattering, etc., that we are often interested in modeling in computer graphics. Our goal
is to determine the “dual” configuration, where light leaves
the camera and arrives at the projector, which can also be
described by a linear system:
l =T c

(5)

Here, l and c are the dual “camera” and “projector,”
respectively, and T is the unknown, n × r transport matrix
relating the two. The fundamental observation of Sen et al. is
that the dual transport matrix T can be calculated directly
from the original T [SCG∗ 05]. To see how, we examine the
light transport between the jth pixel of the projector, l j , and
the ith pixel of the camera, ci , whose transport between each
other is defined by matrix element Ti j . If we take the same
pair of pixels in the dual configuration (ci and l j ), we see
that these are linked by the transport element T ji . Helmholtz
reciprocity states that the light transport between these two
pixels is symmetric, that is that Ti j = T ji . This leads to the
conclusion that T = TT , which allows us to write the dual
light transport equation as:
l = TT c

(6)

Therefore, dual photography is defined by Sen et al. as the
process of measuring transport matrix T and then transposing it to get the light transport from the camera to the projector [SCG∗ 05]. The challenge of the approach is to capture
the light transport matrix T as efficiently as possible. It can
be done, for example, by scanning a pixel on the projector,

614

P. Sen & S. Darabi / Compressive Dual Photography

effectively applying a series of vectors l that contain a single
1 to extract single columns of T, one at a time. This can be
quite time-consuming since the number of acquisitions required is equal to the number of projector pixels n, which is
on the order of 106 for modern projectors. For this reason,
Sen et al. developed an adaptive hierarchical algorithm that
exploits parallelism and captures the transport matrix much
more efficiently.
In this work, we propose to use the theory of compressed
sensing to capture the transport matrix efficiently and without the need for adaptive algorithms. This means that unlike
Sen et al., we do not require real-time processing of each
frame as we acquire the data, making our capture system
much simpler and more portable. In order for us to apply the
theory of compressed sensing to dual photography, we first
consider the acquisition of k images under k different illumination conditions. Our light transport equation can be now
written in matrix form:
C = TL

(7)

where C is an r × k matrix whose columns represent the individual captured images, and L which is an n × k matrix
whose columns are the individual projected patterns. By taking the transpose of both sides, we get CT = LT TT . The
measurement of a single camera pixel over time can now be
rewritten in a form similar to Eq. 1:
ci = LT ti

(8)
th

where ti is the n×1 reflectance function of the i pixel of the
camera (which we want to estimate), ci is the k × 1 measurement vector representing the ith column of CT , and 1 ≤ i ≤ r.
We observe that if our scene does not have a lot of global
illumination and the camera pixels see a contribution from
only a few projector pixels (on the order of a few hundred
out of the million projector pixels), the reflectance functions
ti for each pixel will be fairly sparse in the spatial domain
and so a compression basis Ψ does not need to be applied.
Therefore, in these situations we can take the light pattern
matrix LT as our measurement matrix A of Eq. 1 and use it
to solve for ti directly. We observe that this situation happens
in most of our test scenes, since global illumination effects
tend to be rather small and localized.
On the other hand, if pixels in the camera get contribution
from many pixels in the projector either through significant
global effects, defocusing the camera, etc., our reflectance
transport ti will not be sparse and we cannot solve Eq. 8
directly using compressed sensing. However, for these cases,
we can assume that ti is compressible in some basis Ψ and
apply our framework by writing Eq. 8 as
ci = LT ΨT ˆti
T

T

(9)

We can then substitute A = L Ψ to get ci = Aˆti , which
is similar to Eq. 1. If A meets the RIC, we will be able to
reconstruct ˆti given the measurements ci using compressed
sensing, and then use the result to recover our ti . However,

note that we still only illuminate patterns LT and that we
never have to project them into the compression basis.
To perform dual photography using this compressed sensing framework, we must select our light patterns LT so
that the measurement matrix A, which will be either LT
or LT ΨT , meets the RIC. For the compression basis Ψ,
we would like to use an efficient wavelet transform such
as Daubechies, which has been shown to be better at compressing image signals than a simpler wavelet like Haar
[GKG99]. For experiments requiring compression such as
that of Fig. 7, we use the Daubechies-8 wavelet basis. For
our illumination patterns LT , we choose a Bernoulli matrix
(a matrix composed of 1’s and -1’s randomly selected with
equal probability). We use these because Bernoulli matrices
not only meet the RIC condition by themselves but they also
do so when they are combined with a wavelet basis [CT06].
As described by Takhar et al. [TLW∗ 06], random matrices
like Bernoulli have a universal property in that they are always be incoherent with a sparse inducing basis such as
wavelet. This means that the combination of our Bernoulli
light patterns and the wavelet compression basis LT ΨT will
also meet the RIC.
Using Bernoulli patterns for our illumination makes it
easy to implement our technique in a practical scenario, because we can illuminate the same simple, binary patterns regardless whether we are going to be wavelet compressing the
signal or not. Furthermore, our binary patterns make good
use of the limited dynamic range and quantization of the projector, thereby improving the SNR of our results. Finally, by
formulating the problem in this manner, we never have to illuminate wavelet basis functions, which can be complicated
(especially when we move to more complex wavelets such
as Daubechies) and could be difficult to illuminate on a standard projection system. This makes our framework better
than previous approaches that have applied basis functions
in illumination in order to efficiently capture light transport,
such as the Hadamard patterns of Wenger et al. [WGT∗ 05]
and the wavelet noise of Peers et al. [PD05], or the projection into Haar wavelets for compressive light transport acquisition of the concurrent work of Peers et al. [PML∗ 08].
5. Implementation and Results
We validate our approach with a series of experiments on
different scenes using a single camera-projector pair. We
use a Plus U4-232h projector for illuminating the Bernoulli
patterns and a PointGrey Grasshopper GRAS-03S3M-C
grayscale camera for acquisition. To compute our illumination patterns, we first set the target resolution of the dual
photograph (256 × 256 for most experiments) and then set
a bounding region in projector space that covers the desired
scene. The size of the “pixels” of the Bernoulli pattern are
then computed to the nearest pixel in projector space. We
compute our Bernoulli patterns so that ±1 have equal probability. In order to illuminate ±1 patterns, we could either
illuminate a single 0/1 pattern and subtract it from an all-on
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

P. Sen & S. Darabi / Compressive Dual Photography

image to acquire the negative elements of the pattern, or we
could shine the positive and negative components separately
and capture two images per pattern. We chose the latter because of the improved noise characteristics. To acquire the
color data sets shown (e.g. in Figs. 1 and 2), we illuminate
the Bernoulli patterns separately as R,G,B images so that we
can acquire the transport per channel.
After the images have been captured, we compute the reflectance function of each pixel independently using ROMP
to solve Equation 8 or 9, depending on whether we use a
compression basis or not. The ROMP algorithm is modified
by fixing the maximum number of coefficients added in each
iteration to 15 and the maximum of iterations of the algorithm to 10. This means that for each of our results (with exception of Fig. 7), we assume a maximum of 150 non-zero
elements. Since this is done in parallel for every pixel, we
accelerate our reconstruction with a 24-node Linux cluster,
with each node containing 2 Xeon 5140 CPU’s running at
2.33GHz. Because we optimized our algorithm to reduce acquisition time, the tradeoff results in longer post-processing
times for the data sets. In practice, reconstruction takes on
the order of three hours for most data sets.
5.1. Dual Photography experiments
With the framework in place, our first experiments demonstrate the ability to compressively measure the light transport matrix to perform dual photography. Figs. 1, 2, and 3
all show examples of dual photography using our technique.
Fig. 3 is particularly interesting because it demonstrates how
dual photography can be used to reveal details not easily visible in the original image. By virtually exchanging the projector and camera, we can compute an image from the pointof-view of the projector and see the text more clearly than
could be seen from the camera.

Figure 3: An example of a primal-dual pair, with the primal
image on the left and the dual on the right. One of the surprising things about dual photography is how certain detail
can be drastically enhanced in the dual image. For example,
the “Pro-Tech” text is unreadable in the primal image, yet
clearly visible in the dual (see electronic version of this paper for clear reproduction). It is almost surprising that we
would be able to reconstruct this text using only images like
those on the left. The images have a resolution of 256 × 256
and were captured using 1,500 patterns.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

615

Figure 4: Relighting example. The dual photograph on the
left is relit with a couple of interesting patterns. The images
have a resolution of 256 × 256 and the virtual projector has
a resolution of 185 × 220. The data set was captured using
only 512 patterns.

Figure 5: Validation experiment showing that our reconstructed light transport matches reality. On the left of each
pair is the ground-truth primal image obtained by projecting a pattern onto the scene shown in Fig. 1. On the right
is the image computed by our technique. The projector pattern is 128 × 128 and the camera image is 145 × 195. We
used 1,990 patterns to acquire the data. The glow under the
kitty’s feet is an example of diffuse-diffuse light transport.
5.2. Image-based relighting experiments
Because we have measured the complete transport between
pixels in the camera and pixels in the projector, we are able
to relight the images as a post-process with arbitrary light
patterns. We demonstrate this effect in Figs. 1 and 4. To validate the correctness of our approach, we compare a primal
image relit as a post-process to a ground-truth image obtained by projecting the same pattern on the projector during acquisition, as can be seen in Fig. 5. We can see that
our approximation of sparsity for the light transport still results in reasonable results. Finally, we test our approach to
ensure that we can capture global illumination effects such
as diffuse-diffuse interreflection. Fig. 6 shows a scene with
such illumination. Our algorithm is able to capture global
effects, although they fall off quicker than the ground truth
images. This difference is due to limitations in our HDR capture configuration.
5.3. Single pixel imaging
If we integrate all the pixels of the camera together and think
of them as a single sensor (either by integrating the pixels in software, removing the main lens of the camera, or
simply using a photosensor as in Sen et al. [SCG∗ 05]), we
transform our system into a single-pixel camera that uses
the projector for imaging through dual photography. This results in a system that is similar to that developed by the Rice
group [WLD∗ 06, TLW∗ 06]. In their setup, the imaging is
performed by a lens and an array of digital micro-mirror devices (DMD) modulates the Bernoulli pattern, while in our

616

P. Sen & S. Darabi / Compressive Dual Photography

Figure 6: Capture of global illumination effects such as
caustics and diffuse-diffuse interreflection. On the left of
each pair is the ground-truth primal image. On the right is
the image rendered from the light transport acquired through
our technique. The projector resolution is 64 × 64 and 1,100
patterns were used to acquire the scene.
case the projector both modulates the Bernoulli pattern and
performs the “imaging.” This setup is similar in spirit to the
experiment shown in Fig. 4 of Sen et al. [SCG∗ 05], where
they perform a pixel scan on a projector while measuring the
reflectance function with a photosensor. Unlike Sen et al.,
however, we use compressed sensing to efficiently acquire
the reflectance function at a fraction of the time it would
take a brute-force scan.
Since we have a single-pixel camera in this case, the reflectance function of this pixel is fairly complex (it is in fact
the dual image), so it is unreasonable to assume that it is
sparse in the spatial domain as we did with the other experiments. Instead we choose to compress it using a Daubechies8 wavelet basis. Note that although our measurement matrix
is now A = LT ΨT , we still only illuminate Bernoulli patterns to acquire our dual image. Figure 7 shows a progression of improving images as we increase the number of patterns during acquisition. To appropriately judge the improvement of our approach to that of previous work, the reader
is encouraged to compare our 40% image to Fig. 2e from
Wakin et al.’s single pixel camera, also taken with 40% of total samples [WLD∗ 06]. If the transport between the projector and camera is fairly localized (as is the case with many
of our scenes), the reflectance function can be considered
sparse in the spatial domain and we can perform efficient

15%, MSE: 35.9

25%, MSE: 27.2

40%, MSE: 13.1

50%, MSE: 8.8

75%, MSE: 7.9

100%

Figure 7: Experiment with single pixel camera, using the
projector to do the “imaging.” The dual photograph is 128 ×
128 and would normally require 16,384 patterns to acquire.
However, compressed sensing allows us to get a reasonable
result with a smaller number of patterns. The fraction of patterns used to reconstruct it is shown under each image, along
with its mean-squared error (MSE) from the 100% image. In
this experiment, we use the DB-8 wavelet to compress the
reflectance function so the results indicate how our sparse
approximation of the full wavelet projection affects image
quality. In each case, we assume that there are 5× less nonzero coefficients than patterns. We see acceptable results at
40% (1/8 non-zero coefficients) and nearly indistinguishable
results at 50% (1/10 non-zero coefficients).
single-pixel imaging by computing the entire transport matrix between pixels of the projector and pixels in the camera.
We discuss this in more detail in the next section.
6. Discussion
6.1. Single pixel imaging
As demonstrated in the last section, our system can be considered a single pixel camera similar to the one proposed by
the group at Rice [WLD∗ 06, TLW∗ 06]. Although ours produces results of better quality, it does require active illumination which could make it difficult to use in uncontrolled environments. For outdoor daylight use, for example, one would
have to be performing the imaging at other wavelengths such
as infrared in order for the contribution from the active light
source to be visible at the camera.
One interesting issue that comes up when performing
single-pixel imaging with our approach is the question of
whether the single-pixel reflectance function should be calculated directly (as in Fig. 7) or if the complete pixel-to-pixel
transport should be computed first and then flood illuminated
to generate the equivalent of a single-pixel image (as in the
leftmost image of Fig. 4). We find that when computing the
single-pixel transport directly the reflectance function is extremely complex and requires both a compression basis and
more samples to acquire correctly. On the other hand, when
the complete pixel-to-pixel light transport is captured, every
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

P. Sen & S. Darabi / Compressive Dual Photography

camera pixel sees only a small part of the scene and so its reflectance function is fairly sparse in the spatial domain. This
eliminates the need for compression and reduces the number
of samples required considerably. For example, the image of
Fig. 4 was acquired with only 0.8% of the samples and yet
has quality better than the 75% image from Fig. 7.
The feasibility of a single-pixel camera opens up possibilities for interesting imaging applications. For example, we
could use a small set of cameras each fitted with a different
filter to capture the reflected light and use them to acquire
multi-spectral images. Such a device could have useful applications in computer vision and medicine.
6.2. Comparison with other approaches
To appreciate the benefits of our proposed approach, we
first compare our algorithm with the adaptive algorithm presented in the original dual photography paper [SCG∗ 05].
For the kinds of images featured in this work, the adaptive scheme would require around the same number of patterns (from a few hundred to a few thousand). However,
the adaptive algorithm requires real-time processing of the
camera images to compute the next pattern to be projected,
which nearly doubles the time for acquisition. The demand
for high-throughput made it necessary in the earlier work to
use a cluster to handle the computation of images in parallel. Our approach, on the other hand, is much simpler since
it uses a fixed set of patterns and only processes the images
after they have been acquired. This makes the system more
robust and practical for real-world acquisition.
As mentioned earlier, Peers et al.’s work on compressive
light transport acquisition [PML∗ 08] is also related to this
approach. One of the biggest differences between the two
is the way that the light transport problem is posed in the
compressed sensing framework. Peers et al. require that the
light patterns be projected into the compression basis, or in
our notation LT = ST ΨT , where S is the Bernoulli sampling
matrix. This subtle difference turns out to be fundamentally
important in that it frees us from the limitation of having to
use a compression basis that can be properly projected. In
their work, they are forced to use the Haar basis in order
to exploit the limited dynamic range of the projector, even
though it is not a basis suitable for compression for things
like light fields or images. On the other hand, we are able to
use the Daubechies-8 wavelet, which is better at compression but would be difficult to project directly onto the scene.
However, Peers et al.’s approach has some advantages
over ours, most notably their elegant idea for exploiting
pixel-to-pixel correlation in the camera by establishing a hierarchical technique. We plan to consider related ideas as we
develop ways to accelerate our reconstruction algorithm.
6.3. Limitations and future work
One of the main problems with our prototype implementation for the reconstruction algorithm is that it is quite slow,
taking up to several hours on a cluster to compute the light
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

617

transport required to generate the dual image. We will need
to explore novel reconstruction algorithms as well as a variation of Peers et al.’s hierarchical technique to accelerate
this process. In addition, it would be interesting to decide
per pixel which ones have sparse reflectance functions in the
spatial domain (those that have little global light transport)
and which ones do not so that we can utilize both of our
schemes together in the same image. Finally, the proposed
work presents us with potential areas of interesting future
work. For example, it would also be interesting to apply our
compressed sensing framework to things like BRDF measurement to accelerate their acquisition.
7. Conclusions
In this paper, we propose a novel approach for accelerating the acquisition of light transport for dual photography
by leveraging recent ideas in the field of compressed sensing. In particular, we demonstrate how we can use simple
Bernoulli binary patterns to capture a sparse approximation
of the light transport and use the results to generate the dual
image from the point-of-view of the projector. Our algorithm
is non-adaptive and uses a small number of fixed patterns,
making it practical and robust for real-world applications.
Acknowledgments The authors would like to acknowledge the
work in compressed sensing done at the UNM Advanced Graphics
Lab over the last two years that led to the development of the presented work. Yi Li developed some of the initial infrastructure for
compressed sensing and conducted preliminary experiments during
a summer internship with the first author in the summer of 2007.
His initial results were presented in September 2007 [SL07] and
helped shape the proposed algorithm. Hao He developed the software infrastructure to control the HDR acquisition of the camera
and administered many late-night experiments. Ruijin Wu helped
write the code to perform the CS-based reconstruction on a cluster to achieve the final results. Yasamin Mostofi provided valuable
feedback and discussion during the course of this research. Finally,
the authors would like to thank Maryam Fazel for stimulating discussion of compressed sensing and its application to light transport
acquisition during her visit to UNM in March of 2007.

References
[Can06] C ANDÈS E. J.: Compressive sampling. In Intl. Congress
of Mathematicians, Madrid, Spain (2006), European Mathematical Society, pp. 1433–1452.
[Can08] C ANDÈS E. J.: The restricted isometry property and
its implications for compressed sensing. Compte Rendus de
˚
l’Academie des Sciences, Paris, Serie I 346 (2008), 589U–592.
[CRT06] C ANDÈS E. J., ROMBERG J., TAO T.: Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information. IEEE Trans. on Information Theory
52, 2 (Feb. 2006), 489–509.
[CRTV05] C ANDÈS E. J., RUDELSON M., TAO T., V ERSHYNIN
R.: Error correction via linear programming. In IEEE Symposium
on Foundations of Computer Science (2005), pp. 295–308.
[CT06] C ANDÈS E. J., TAO T.: Near optimal signal recovery
from random projections: universal encoding strategies? IEEE
Trans. on Information Theory 52, 12 (Dec. 2006), 5406–5425.

618

P. Sen & S. Darabi / Compressive Dual Photography

[DHT∗ 00] D EBEVEC P., H AWKINS T., T CHOU C., D UIKER H.P., S AROKIN W., S AGAR M.: Acquiring the reflectance field of
a human face. In SIGGRAPH ’00 (New York, NY, USA, 2000),
ACM Press/Addison-Wesley Publishing Co., pp. 145–156.

[MW08b] M ARCIA R. F., W ILLETT R. M.: Compressive coded
aperture superresolution image reconstruction. IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP 2008) (Apr. 2008), 833–836.

[Don06] D ONOHO D. L.: Compressed sensing. IEEE Trans. on
Information Theory 52, 4 (Apr. 2006), 1289–1306.

[NV07] N EEDELL D., V ERSHYNIN R.: Uniform uncertainty
principle and signal recovery via regularized orthogonal matching pursuit, 2007. Preprint.

[ECJ∗ 06] E INARSSON P., C HABERT C.-F., J ONES A., M A W.C., L AMOND B., H AWKINS T., B OLAS M., S YLWAN S., D E BEVEC P.: Relighting human locomotion with flowed reflectance
fields. In Proc. of the Eurographics Symposium on Rendering
(2006).
[EFK07] E GIAZARIAN K., F OI A., K ATKOVNIK V.: Compressed sensing image reconstruction via recursive spatially
adaptive filtering. In IEEE Conference on Image Processing
(ICIP) (Sept. 2007).
[FBLS07] F UCHS M., B LANZ V., L ENSCH H., S EIDEL H.-P.:
Adaptive sampling of reflectance fields. ACM Transactions on
Graphics 26, 2 (2007), 10.
[Gan07] G AN L.: Block compressed sensing of natural images.
In Conference on Digital Signal Processing (DSP) (July 2007).

[PD03] P EERS P., D UTRÉ P.: Wavelet environment matting. In
Proc. of the Eurographics Symposium on Rendering (2003).
[PD05] P EERS P., D UTRÉ P.: Inferring reflectance functions from
wavelet noise. In Proc. of the Eurographics Symposium on Rendering (2005).
[PML∗ 08] P EERS P., M AHAJAN D., L AMOND B., G HOSH A.,
M ATUSIK W., R AMAMOORTHI R., D EBEVEC P.: Compressive
Light Transport Sensing. Tech. rep., USC ICT, 2008. Posted Nov.
27, 2008.
[Ric] Rice University Compressive Sensing Resources website.
http://www.dsp.ece.rice.edu/cs/.
[SCG∗ 05] S EN P., C HEN B., G ARG G., M ARSCHNER S. R.,
H OROWITZ M., L EVOY M., L ENSCH H. P. A.: Dual photography. ACM Transactions on Graphics 24, 3 (2005), 745–755.

[GKG99] G RGIC S., K ERS K., G RGIC M.: Image compression
using wavelets. Proc. of the IEEE International Symposium on
Industrial Electronics 1 (1999), 99–104.

[SL07] S EN P., L I Y.: Applications of compressed sensing to
computer graphics, 2007. Lecture at University of New Mexico.

[GLL∗ 04] G OESELE M., L ENSCH H. P. A., L ANG J., F UCHS
C., S EIDEL H.-P.: Disco: acquisition of translucent objects.
ACM Transactions on Graphics 23, 3 (2004), 835–844.

[SMB07] S HEIKH M. A., M ILENKOVIC O., BARANIUK R. G.:
Designing compressive sensing DNA microarrays. IEEE International Workshop on Computational Advances in Multi-Sensor
Adaptive Processing (CAMPSAP 2007) (Dec. 2007), 141–144.

[GNG∗ 08] G U J., NAYAR S., G RINSPUN E., B ELHUMEUR P.,
R AMAMOORTHI R.: Compressive structured light for recovering
inhomogeneous participating media. In European Conference on
Computer Vision (ECCV) (Oct 2008).
[GTLL06] G ARG G., TALVALA E.-V., L EVOY M., L ENSCH
H.: Symmetric photography: Exploiting data-sparseness in reflectance fields. In Proc. of the Eurographics Symposium on Rendering (2006).
[HED05] H AWKINS T., E INARSSON P., D EBEVEC P.: A dual
light stage. In Proc. of the Eurographics Symposium on Rendering (2005).
[LDP07] L USTIG M., D ONOHO D., PAULY J. M.: Sparse MRI:
The application of compressed sensing for rapid MR imaging.
Magnetic Resonance in Medicine 58, 6 (2007), 1182–1195.
[LH96] L EVOY M., H ANRAHAN P.: Light field rendering. In
SIGGRAPH ’96 (New York, NY, USA, 1996), ACM, pp. 31–42.
[MLP04] M ATUSIK W., L OPER M., P FISTER H.: Progressivelyrefined reflectance functions from natural illumination. In Proc.
of the Eurographics Symposium on Rendering (2004).
[MPDW03] M ASSELUS V., P EERS P., D UTRÉ P., W ILLEMS
Y. D.: Relighting with 4D incident light fields. ACM Transactions on Graphics 22, 3 (2003), 613–620.
[MPDW04] M ASSELUS V., P EERS P., D UTRÉ P., W ILLEMS
Y. D.: Smooth reconstruction and compact representation of reflectance functions for image-based relighting. In Proc. of the
Eurographics Symposium on Rendering (2004).
[MS08] M OSTOFI Y., S EN P.: Compressed mapping of communication signal strength. In Proceedings of Milcom (2008).
[MS09] M OSTOFI Y., S EN P.: Compressive cooperative mapping in mobile networks. In American Control Conference (ACC)
(submitted) (2009).
[MW08a] M ARCIA R., W ILLETT R.: Compressive coded aperture video reconstruction. In European Signal Processing Conference (EUSIPCO) (Aug. 2008).

[SNB03] S CHECHNER Y. Y., NAYAR S. K., B ELHUMEUR P. N.:
A theory of multiplexed illumination. In ICCV ’03: Proc. of
the Ninth IEEE International Conference on Computer Vision
(Washington, DC, USA, 2003), IEEE Computer Society, p. 808.
[TG07] T ROPP J. A., G ILBERT A. C.: Signal recovery from
random measurements via orthogonal matching pursuit. IEEE
Trans. on Information Theory 53, 12 (Dec. 2007), 4655–4666.
[TLW∗ 06] TAKHAR D., L ASKA J., WAKIN M., D UARTE M.,
BARON D., S ARVOTHAM S., K ELLY K., BARANIUK R.: A new
compressive imaging camera architecture using optical-domain
compression. In Proc. of Computational Imaging IV at SPIE
Electronic Imaging (Jan. 2006), SPIE, SPIE.
[TM01] TAUBMAN D. S., M ARCELLIN M. W.: JPEG 2000:
Image Compression Fundamentals, Standards and Practice.
Kluwer Academic Publishers, Norwell, MA, USA, 2001.
[TMB08] T RZASKO J., M ANDUCA A., B ORISCH E.: Highly undersampled magnetic resonance image reconstruction via homotopic ell-0-minimization. submitted (2008).
[Wal91] WALLACE G. K.: The JPEG still picture compression
standard. Communications of the ACM 34, 4 (1991), 30–44.
[WGT∗ 05] W ENGER A., G ARDNER A., T CHOU C., U NGER
J., H AWKINS T., D EBEVEC P.: Performance relighting and
reflectance transformation with time-multiplexed illumination.
ACM Transactions on Graphics 24, 3 (2005), 756–764.
[WLD∗ 06] WAKIN M., L ASKA J., D UARTE M., BARON D.,
S ARVOTHAM S., TAKHAR D., K ELLY K., BARANIUK R.: An
architecture for compressive imaging. Image Processing, 2006
IEEE International Conference on (Oct. 2006), 1273–1276.
[WYG∗ 08] W RIGHT J., YANG A., G ANESH A., S ASTRY S.,
M A Y.: Robust face recognition via sparse representation. IEEE
Trans. on Pattern Analysis and Machine Intelligence (2008).
[ZWCS99] Z ONGKER D. E., W ERNER D. M., C URLESS B.,
S ALESIN D. H.: Environment matting and compositing. In SIGGRAPH ’99 (New York, NY, USA, 1999), ACM Press/AddisonWesley Publishing Co., pp. 205–214.

c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

