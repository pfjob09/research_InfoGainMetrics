Eurographics/ IEEE-VGTC Symposium on Visualization 2009
H.-C. Hege, I. Hotz, and T. Munzner
(Guest Editors)

Volume 28 (2009), Number 3

Comparing Parameter Manipulation with
Mouse, Pen, and Slider User Interfaces
Colin Swindells‡, Melanie Tory‡, Rebecca Dreezer*
‡

University of Victoria, Canada
McMaster University, Canada

*

Abstract
Visual fixation on one’s tool(s) takes much attention away from one’s primary task. Following the belief that the
best tools ‘disappear’ and become invisible to the user, we present a study comparing visual fixations (eye gaze
within locations on a graphical display) and performance for mouse, pen, and physical slider user interfaces.
Participants conducted a controlled, yet representative, color matching task that required user interaction
representative of many data exploration tasks such as parameter exploration of medical or fuel cell data. We
demonstrate that users may spend up to 95% fewer visual fixations on physical sliders versus standard mouse and
pen tools without any loss in performance for a generalized visual performance task.
Categories and Subject Descriptors (according to ACM CCS): H5.2. Information interfaces and presentation: User
interfaces interaction styles, input devices and strategies, evaluation/methodology. H.1.2. User/Machine Systems:
Software psychology. I.3.6. Methodology and Techniques: Interaction techniques.

1. Introduction
Exploring data in applications such as medical imaging
often requires a user to manipulate many parameters to see
different portions of the data, see the data from different
perspectives, or to find a desired result. If a final image is
known a priori, there’s typically little need for human
exploration instead of an automated process to perform a
set of data manipulations. However, more often, the desired
views of the data are unknown and the process of exploring
different views through parameters is more important than
obtaining a final image. Moreover, the combination of
parameter settings that will achieve a desired new data
result is rarely obvious. Experimenting with a range of
display options and queries may provide insight not
possible with static images alone [Rhe02]. Adjusting
parameters in these situations requires frequent control
manipulation to enable exploration of alternatives.
We present an experiment comparing mouse, pen, and
physical interfaces (sliders) for a parameter manipulation
task. Prior research (e.g., Crider et al. [CBS*07]) reported
qualitative benefits of physical controls, such as physical
sliders, knobs, and buttons, over graphic controls for
parameter manipulation in some visualization applications.
Our goal was to quantify these differences, if they occur,
and deduce the reasoning behind them.
Compared to physical controls, graphic controls
manipulated through pen and mouse interfaces are less
expensive and more flexible (e.g. mappings of functions to
controls can be changed dynamically, and controls can be
hidden). Furthermore, mouse and pen interfaces are often
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

the best choice where clear mappings are know, such as the
(x, y) position of an object [JSMM94]. However, physical
controls have potential interaction advantages:
• Bimanual interaction: physical controls allow use of two
hands, potentially allowing two parameters to be
manipulated simultaneously [BM86, HYA*08].
• Graspability: physical controls can be easily grasped,
potentially reducing acquisition time [FB97].
• Visual attention: physical controls may be easier to move
with less visual attention [CBS*07, HPGK94].
Because control adjustments take place frequently, these
differences could provide substantial benefit over the
course of a higher-level task.
1.1. Task Choice
Our experiment involved a color matching task. Participants
adjusted four slider controls to alter the appearance of a
color swatch until it matched a target color. Each control
adjusted one parameter in a perceptually linear color space.
We chose this particular color matching task to carefully
meet two objectives. We sought a task that was:
• Representative of parameter manipulation in other high
dimensional data exploration tasks.
• Effective for measuring correctness.
Color matching is representative of data exploration tasks
for several reasons: it involves manipulating several
parameters in no specified order; some parameters have
more intuitive effects than others, and some parameters are
conceptually independent where as others are conceptually
related. In particular, we used the CIE L*a*b* color model.

C. Swindells et al. / Comparing Parameter Manipulation with Mouse, Pen and Slider User Interfaces
920
3. Related Research
In this model, lightness could be easily understood and
independently manipulated, but yellow-blue and red-green
Indirect interface devices such as a mouse require a user to
parameters were less intuitive, and their effects were
perform many sub-steps in order to manipulate a graphic
difficult to mentally separate. Measurable correctness was
control. These include acquiring the mouse with one’s hand,
important for experimental control. In many parameter
moving the mouse cursor to acquire the graphic control, and
manipulation tasks, ‘good’ combinations are identified
then adjusting the control [FB97]. Although these steps are
subjectively by the user. However, we needed an objective
simple, they may require visual and cognitive attention,
way to evaluate correctness, so we could determine when a
breaking the flow of a user’s primary task [CBG04]. Direct
task was complete. The CIE L*a*b* color model provided
physical input devices may reduce the need for some of
clear, consistent data manipulation paths and end points
these interface manipulation steps [UHJ03]. Physical
(i.e., color matched within a specified tolerance). This
interfaces offer potential benefits of graspability, bimanual
perceptually linear color model also enabled us to
control, and visual attention as described above. Previous
determine a distance between any color and the target color.
researchers have focused more on performance than visual
fixations on user controls; so, our related research
2. Application Scenarios
discussion has an emphasis on explicit performance work.
Specialized physical input devices have been shown to
Parameter manipulation is a common task in many
outperform equivalent graphical widgets for several
visualization applications. For example, volume rendering
applications. For example, Fitzmaurice and Buxton [FB97]
of 3D medical image data produces a 2.5D image, where
demonstrated that physical or “graspable” user interfaces
some tissues are displayed semi-transparently so that other
with specialized shapes and dedicated functions were
tissues may be seen underneath. These colors and
superior to a generic input device for a target tracking task.
transparencies, as well as other rendering options, are set
Another example is Arsenault and Ware’s [AW00]
interactively through a series of controls such as sliders.
observation of a 12% performance improvement when
Several visualization interfaces have been designed to
participants contacted objects in a virtual reality Fitts Law
support exploration tasks with medical data, including
tapping task. In a graspable user interface, physical objects
parallel coordinates style [TPM05] and spreadsheet style
are tightly coupled to graphic objects in a virtual scene. We
[JM01] interfaces. Physical user interfaces have been used
distinguish this from our work, where the physical
in the medical domain to adjust 3D parameters such as the
interfaces adjust parameters affecting the scene, rather than
position and orientation of a slicing plane [HPGK94] as
directly representing scene objects.
well as 1D parameters such as pre-determined transparency
Parameter control tasks have also been shown to benefit
functions [CBS*07]. Here we focus on 1D parameters that
from physical interfaces. Hunt and Kirk [HK99] conducted
can be adjusted through physical or graphical sliders.
an experiment comparing physical and virtual sliders for
Similarly, for visualizations of non-spatial data, users
setting sound parameters. Participants achieved better
often want to filter the data through dynamic queries
results on a target sound matching task using physical
[AWS92]. In this method, a user drags query sliders that
sliders. Similarly, Chipman et al. [CBG04] compared a
filter information based on each dimension in a
physical slider, a graphical scrollbar, and the mouse wheel
multidimensional data set. For example, to search for a
for two scrolling tasks. Both physical interfaces performed
house to purchase, a user may set parameters such as the
better than the graphical scrollbar, with the mouse wheel
number of bedrooms desired, desired distance from a
being superior for searching and the physical slider being
location such as a workplace, desired cost, and so on, as
superior for reciprocal tapping.
demonstrated in HomeFinder [WS92]. These parameter
We focus on tasks where a user manipulates a moderately
values are not simply set once and then ignored, but are
sized set of parameters that affect a visual display. Within
dynamically updated to change the query and to understand
this domain, Crider et al. [CBS*07] and Hartmann et al.
the variation in data values (in this example, the range of
[HYA*08] qualitatively suggested that a mixing board
homes that are available).
interface consisting of physical slider controls may offer
Related parameter manipulation tasks also occur in many
advantages over graphic slider controls manipulated with
other applications. In scientific simulations, users need to
the mouse.
adjust input parameters (e.g., finding input parameters to
Our work extends previous research by (i) comparing
optimize a fuel cell’s design). In computer graphics, users
control handling performance and visual fixations, and (ii)
set display parameters (e.g., to adjust the way an object
examining a parameter manipulation task that is more
appears when lit). Similarly, Hartmann et al. [HYA*08]
similar to typical tasks in visualization. In contrast to
presented the idea of “tuning” application variables at
previous work, our experimental task requires users to
runtime, enabling programmers to rapidly compare code
judge a single visual output affected by all controls.
alternatives to facilitate software prototyping.
Moreover, the effect of a control on the output is not always
Prior research in visualization [CBS*07] suggested using
easy to predict. In addition, we employ eye tracking to
physical sliders to manipulate visualization parameters,
examine how different interface devices affect visual
citing potential for increased visual attention on the screen,
attention, since maintaining ones eyes on the screen may be
interaction benefits such as bimanual control, as well as
more important in visualization than in some other
more screen space for the visualization (since physical
computer applications.
controls do not take screen space). However, these
proposed benefits have not been tested empirically.
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

C. Swindells et al. / Comparing Parameter Manipulation with Mouse, Pen and Slider User Interfaces

921

4. Goals
We studied control handling and visual fixation patterns in
the context of a representative parameter manipulation task.
Specifically, our experiment…
• Compared the eye fixations, a measure of visual attention,
that participants took to conduct a challenging task using
mouse, pen, and slider user interfaces.
• Determined how much less participants handle controls
during ‘easy’ tasks vs. ‘difficult’ tasks.
• Compared control handling times and distances moved
when participants work with mouse, pen, and slider
.
5. Experiment Design
This section describes our experimental design. The main
factor of user interaction (UI) had 3 conditions: mouse, pen,
and physical slider. Each of these UIs controlled a fourparameter color space that is described in more detail below.
We sought a non-trivial task requiring exploration and
attention where we could measure eye gaze fixations, task
completion time, and parameter distance traveled.
5.1. Participants
Twelve paid participants (5 female, 7 male) each took
approximately one hour to complete the experiment. Their
ages ranged from 18-25 years (M = 21.3, SD = 2.2). Each
participant had normal vision, or corrected to normal vision
with contact lenses, and was required to pass an Ishihara
color-blindness test [IK77] to qualify as a participant.
5.2. Tasks
Participants performed a color matching task; they
manipulated physical or graphical controls to adjust a color
until it matched a target. The task apparatus is illustrated in
Figures 1 & 2. The left and right quadrants of the color
circle displayed a constant target color for the duration of
the trial; whereas, the top and bottom quadrants of the color
circle changed according to each participant’s actions with
each user interface
(mouse, pen, and slider). The
background was chosen to convey consistent transparency
and relative color information within the color quadrants.
Straight edges of each color quadrant intersected the
diagonals of a 20 x 20 grid checkered grid such that half the
edge overlapped white and half the edge overlapped black.
A black background surrounded the checkered grid.
Color Space
Target colors were randomly generated from a perceptually
linear three-parameter CIE LAB (or, CIE L*a*b*) color
space [McL76] and a one-parameter linear transparency (t)
scale. L*, a*, and b* parameters had low to high values
according to International Commission on Illumination
standards
(i.e., CIE; Commission Internationale de
l’Eclairage). t traversed a typical 8-bit transparency scale.
These parameters are summarized below. L* values ranged
from black to white; a* values ranged from green to
magenta; b* values ranged from blue to yellow; and, t
values ranged from transparent to opaque. Parameter ranges
were restricted such that all parameter combinations
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

Figure 1: Color matching task apparatus. The physical
slider board was removed for Mouse & Pen conditions.
mapped to a viewable color on the visual display.
Specifically, L*  [35, 70], a*  [-15, 20], b*  [-15, 20],
and t  [110, 250]. In other words, instead of the broadest
color space, we sought a combination of (L*, a*, b*, t) that
would (i) be faithfully rendered on the visual display, and
(ii) result in a color that was as perceptually linear to its
neighboring parameters as possible.
CIE LAB colors were processed for display using a twostep conversion process. First, CIE LAB values were
converted to CIE XYZ values. Second, CIE XYZ values
were converted into gamma-corrected RGB values that
could be displayed on our sRGB calibrated display.
Equipment Layout
Participants sat at a desk in a dimmed experiment room.
They interacted with a 21” Wacom Cintiq 21UX interactive
pen display set at a 20° inclination as shown in Figure 1.
The desk height and chair were both adjusted to
comfortable positions for each participant before the
experiment. A mouse was positioned next to the tablet and
could be adjusted to a comfortable position. Four physical
sliders and a ‘next trial’ button were implemented using
Phidgets [GF01]. Each slider was placed within a custom
built acrylic frame wrapped in a black matte vinyl coating.
The sliders and graphical display were updated every 16.7
ms with < .1 ms variation between updates using a PC
running Microsoft Windows Vista. The display was set to a
32 bit sRGB color mode with known CIE LAB
transformation matrices, and operated at 1600 x 1200 pixel
resolution. Eye gaze and fixations were measured with a

Figure 2: Color matching task display layout for all
conditions (Mouse, Pen, and Slider).

C. Swindells et al. / Comparing Parameter Manipulation with Mouse, Pen and Slider User Interfaces
922
2. Eye tracking calibration: Participants were fitted with
Locarna PT-1 eye tracker operating at 30 frames / second
the eye tracker glasses, then asked to successively fixate on
and capable of 1° accuracy. Eye tracking data was recorded
centers of a 3x3 grid rendered on the experimental display.
at 320 x 240 pixel resolution, and we defined an eye
3. Training: The experimenter described the study to the
fixation as 10 frames (333 ms) of eye gaze within a 25 pixel
participant from a pre-defined script and answered any
radius. Figure 2 illustrates the layout and dimensions of the
questions raised by the participant. Next, the participant
color circle and parameter controls. The physical controls
performed six color matching trials – two for each
for the slider had the same layout dimensions, width, and
condition of mouse, pen, and slider. These tuples were
range of motion as their graphical counterparts used for the
presented in the same order as the participant would
mouse and pen conditions. Additionally, off-white stickers
experience during the experiment. The participant was free
were affixed to the tops of the physical sliders to match the
to adjust the sliders in whatever order or manner s/he
‘look’ of their graphical counterparts.
desired until the adjustable color quadrants matched the
target color. Each trial began when the participant pressed a
Study Design
graphical ‘next trial’ button (pen and mouse conditions), or
We used a completely counter-balanced, within-participants
a physical ‘next trial’ button (physical condition). Trials
design for the three conditions of UI: mouse, pen, and
ended automatically once all sliders were set correctly,
slider. Each participant performed 10 trials for each UI
within a tolerance of 1/7 (14.2%). Participants could grasp
condition for a total of 30 trials.
and manipulate each user interface widget as they pleased,
Participants performed six training trials before the
and they could iterate towards a target color using any
experiment. Two trials of each UI condition (Mouse, Pen,
strategic combination of user interface movements.
and Slider) were presented in the same order as the
Participants were not instructed to perform bimanual
participant would experience during the actual experiment.
manipulation when handling the sliders; however, they
In addition to training each participant, these trials plus
were not prevented from using bimanual manipulation for
counter-balanced ordering reduced the possible influence of
the training or actual trials.
learning effects in the subsequent experimental trials.
4. Trial Completion: Participants performed the same
The same set of target colors were used for each UI
color matching technique as the training phase with a new
condition, and the presentation order was randomized.
set of target colors. 10 trials were successively performed
Randomization virtually eliminated the chance that a
for each of the three experimental conditions. Asking
participant would remember a particular color, while re-use
participants to have a short rest between conditions
of the same set of 10 colors eliminated bias towards a
minimized fatigue during the experiment.
particular condition. For example, any slight perceptual
5. Eye tracking drift check: After completing all the
difficulty differences while matching a particular color
experimental trials, participants repeated the calibration
would be equally experienced for each of the three
described in step 2. This calibration was used to check for
conditions (mouse, pen, and slider).
calibration ‘drift’ that could be caused by the eye tracking
A color was considered matched when the L*, a*, b*, and
glasses being bumped or adjusted during the study.
t values were within ±5, ±5, ±5, and ±35 units, respectively.
Intuitively, these tolerances represent bounds that were
6. Results
barely differentiable.
We first performed Q-Q plots to test our data distributions.
The controls were chosen to span a range of parameter
We observed linear Q-Q plots for lognormal transforms of
difficulty. The controls were labeled L, A, B, and
measures tmatch, tcontrol, and dcontrol, and a linear
Transparency, respectively. L*, a*, and b* dimensions were
transform of measure Nfix. Lognormal transformations
foreign to all participants and extremely difficult
were therefore applied to the time and distance measures
conceptually segment; but, the concept of transparency was
before performing the appropriate statistics. For the
well understood by all participants and easy to conceptually
statistical results, M denotes Mean, Mdn denotes Median,
segment compared to the other parameters.
SD denotes Standard Deviation, d denotes Cohen’s d
Refer to our accompanying video for additional
measure of effect size, and p denotes level of significance.
information on our experimental design and apparatus. In
We use the typical d thresholds of d = .2, .5, and .8 for
summary our design had…
small, medium, and large values denoting importance of
• One factor user interface with 3 conditions (UImouse,
effect sizes [Coh88]. Statements of significance take into
UIpen, and UIslider)
account Bonferroni corrections to a p < .05 significance
• Four measures time to match (tmatch), time traversing all
threshold. For ANOVA results, we used the Huynh-Feldt
controls (tcontrol), distance moving all controls
correction when Mauchley’s Test of Sphericity was
(dcontrol), and number of eye fixations (Nfix).
violated.
Participants interacted with four controls for L*, a*, b*,
If a participant moved two (or more) sliders at the same
and t that we define as cL*, ca*, cb*, and ct, respectively.
time (i.e., bimanual manipulation), movement times and
distances for each slider were added together. This
5.3 Procedure
procedure provided conservative slider handling data that is
consistent with time and distance data from the mouse and
Participants were seated at a desk in front of a display as
pen interfaces. Only 107 s out of 2157 s of slider
shown in Figure 1. They conducted the following steps:
movements were recorded simultaneously. Thus, only an
1. Color blindness screening: Participants identified a set
average of 5.2%, with a 95% confidence interval [3.1%,
of 8 Ishihara color samples presented on the graphical
7.2%], of movement time contained bimanual
display to screen for common color vision deficiencies such

as red-green and blue-green color blindness.

c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

923
UI conditions x 4 slider controls) revealed significant main
effects of UI (F(2,22)=49.5, p2=0.82, p<0.001) and control
(F(2.2,24.2)=5.4, p2=0.33, p=0.01), and a significant
interaction between UI and control (F(5.4,58.9)=4.1,
p2=0.27, p=0.002). Pairwise comparisons revealed that
UIslider was significantly different from both UImouse and
UIpen (p<0.001), but UIpen and UImouse were not
significantly different from each other. Significant
differences between specific controls, and the interaction
between controls and the UI condition are discussed in the
next section below.
In summary, highly significant differences were observed
for the total number of visual fixations per participant on
the UI control region for UIslider-UImouse and UIsliderUIpen as shown in Figure 5.
Despite these large visual attention differences we did not
observe significant differences in overall task performance.
Figure 6 shows total times participants took to match the
target color. Although UIslider was faster on average, this
difference was not significant.

C. Swindells et al. / Comparing Parameter Manipulation with Mouse, Pen and Slider User Interfaces

manipulations. Three of the twelve participants chose to not
use bimanual manipulation at all when handling the sliders.
Because of this low incidence of bimanual manipulation,
we focus our results and analyses on user control handling
and visual fixation for each of the three user interface
conditions: mouse, pen, and slider.
6.1 Differences between UI Conditions
We observed large visual attention differences between UI
conditions. Figures 3 & 4 show the number of visual
fixations per participant on the color wheel and each of the
UI controls, respectively. Figure 5 compares the total
number of visual fixations on the UI control region for the
mouse, pen, and slider conditions.

Figure 3: Boxplot Visual Fixations on Color Target.

Figure 6: Boxplot Match Times(s) for each Control
6.2 Control Difficulty
The four color manipulation parameters (L, a*, b*, and t)
were not equally easy for participants to adjust. This
resulted in differences in the number of visual fixations,
control handling time, and control handling distance.
Figure 4: Boxplot Visual Fixations on each UI Control.

Figure 5: Boxplot Visual Fixations on UI Control Region.
We did not observe significant differences between UI
conditions for visual fixations on the color target. However,
large and significant differences were observed for fixations
on the four UI controls. A repeated measures ANOVA (3
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

Visual Fixations
Figure 4 shows that the number of visual fixations on each
of the four slider controls differed, and that the amount of
this difference also varied depending on the UI condition.
Overall, pairwise comparisons revealed that ca* was
significantly different from ct (p=0.008).
For UIpen, significant differences were observed between
ca*-cb* (p=0.03) and ca*-ct (p=0.023). For UImouse,
significant differences were observed for ca*-ct (p=0.046)
and cb*-ct (p=0.032). For UIslider, significant differences
were observed only for ca*-ct (p=0.032).
Total Control Handling Times
Figure 7 shows total times participants took handling each
control for UImouse, UIpen, and UIslider conditions. A
repeated measures ANOVA (3 UI conditions x 4 slider
controls) revealed a significant main effect of control
(F(2.4,26.2)=10.5,
p2=0.49,
p<0.001).
Pairwise
comparisons showed that cL* was significantly different
from ca* (p=0.024), cb* (p=0.015), and ct (p=0.012). Other
pairwise differences were not significant.

924

C. Swindells et al. / Comparing Parameter Manipulation with Mouse, Pen and Slider User Interfaces

As shown in Figure 7 and Figure 8, there is more variation
between the time and distance traversed with the individual
sliders than between UImouse, UIpen, and UIslider
conditions. We define UIslider as ‘Physical’ because the
sliders physically ‘project’ from the display. Conversely,
we define UImouse & UIpen as ‘Graphic’ because the
sliders are represented by flat 2-D graphics on the display.
UImouse & UIpen represent indirect and direct user
interfaces, respectively.

Figure 7: Handling Times(s) for each Control.
Total Control Handling Distances
Figure 8 shows total distances participants took handling
the controls for UImouse, UIpen, and UIslider conditions.
A repeated measures ANOVA (3 UI conditions x 4 slider
controls) revealed only a significant main effect of control
(F(2.1,22.7)=49.6, p2=0.82, p<0.001).
Pairwise
comparisons showed significant differences between cL*ca* (p=0.002), cL*-cb* (p=0.001), cL*-ct (p=0.003), ca*-ct
(p<0.001), and cb*-ct (p<0.001).

Figure 8: Travel Distances (mm) for each Control.
7. Analysis
Table 1 summarizes the key relationships between the
results. Parameter manipulation tasks typically involve
some parameters that are conceptually separable from
others, and therefore intuitively easier to understand than
parameters that are conceptually interdependent. This
difference was modeled in our color matching study. We
conjectured that controls cL*and ct would be easier to deal
with than ca* and cb*. This is because L* and t deal with
dimensions that are easier for participants to mentally
segment – L* represents darkness and t represents
transparency. Thus, these controls manipulate an easy-tochunk dimension. Conversely, a* and b* are multi-colored
elements that are not typically ‘chunked’ along a single
dimension by most people. The green / magenta and blue /
yellow scales associated with a* and b* are much more
difficult to distinguish. Consequently, L* and t represent
‘easy’ controls and a* and b* represent ‘difficult’ controls.
We can gain an intuitive sense of how much the different
UI conditions differ compared to individual slider settings.

Table 1: Summary of Differences between Individual
Controls (‘Easy’: cL*, & ct vs. ‘Difficult’: ca* & cb*) and
UI Conditions (‘Graphic’: UImouse & UIpen vs. ‘Physical’:
UIslider)
‘Easy’ vs.
‘Physical’ vs.
‘Difficult’
‘Graphic’ UI
Control


Handling Time

Handling Distance 

Match Time
N/A

Eye on Target
N/A


Eye on Control
Legend





Similar
Slight Difference
Moderate Difference
Very Large Difference

The following analyses do not discuss differences
between bimanual and unimanual handling because such a
small percentage of slider control handling involved
bimanual manipulation. We observed that participants
appeared to focus attention on one particular dimension at a
time during the color matching tasks. This observation is
consistent with the notion that bimanual manipulation often
involves one hand as a ‘support’ while the second hand
actively performs an action. By contrast, moving two
sliders simultaneously in a parameter manipulation task
would require people to think about the effects of two
parameters at once, which may be very difficult. Our results
suggest that offering bimanual control does not offer much
benefit for parameter manipulation tasks.
7.1 Visual Fixation Differences
We observed large differences in visual attention between
the mouse, slider, and pen conditions. About the same
number of visual fixations were observed on the color
target (slightly less for the slider control, but not
significantly less). Since participants moved similar
distances and took similar times moving controls for each
UI condition, the similar numbers of eye fixations on the
color target for each condition are what we would expect.
Conversely, we observed huge differences in visual
fixations on the slider control vs. the mouse and pen
controls. These data, combined with the similar
performance data, suggest that people needed much less
visual attention when using the slider vs. the other controls.
For example, Figure 5 shows a large 95% difference in the
number of eye fixations between the mouse and slider
conditions, and a similarly large 96% difference in the
number of eye fixations between the pen and slider
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

925
interdependent (‘difficult’) controls. Although the
differences in time and distance in Figure 7 and Figure 8 do
not visually appear large, this is primarily due to the
logarithmic scale required to perform the statistics. Up to
50% differences in median times and distances were
observed between the ‘easy’ and ‘difficult’ controls.
Furthermore, these differences had medium to large effect
sizes ranging from d = .6 to d = 1.59. Thus, our study
results cover a range of user interface control difficulties
that people are likely to experience when viewing and
interacting with visualizations. More important, the
observed handling time, handling distance, and visual
fixation differences between easy and difficult controls
suggest that the intuitiveness of a control impacts overall
performance. Finding an intuitive mapping between input
parameters and their effect on the visualization is therefore
critically important.

C. Swindells et al. / Comparing Parameter Manipulation with Mouse, Pen and Slider User Interfaces

conditions. Furthermore, these differences had large effect
sizes ranging from d = 2.94 to d = 3.60. These effect sizes
represent over 90% non-overlap between the UIslider and
UImouse & UIpen conditions [Coh88]. While one might
intuitively expect a person to visually fixate on a mouse or
pen more than a slider, these data provide quantitative
understanding of the scope of these differences.
These results suggest that physical user interfaces such as
sliders can free visual attention away from tools such that a
user can spend up to 20 times more relative visual fixations
on their primary task. This finding could be important for
visualization tasks in time and safety critical environments
such as vehicles, operating rooms, and military theaters. We
further expect that this effect will be more pronounced for
novice users. For example, Law et al. [LAK*04] found that
novice subjects focused substantially more visual attention
on a laparoscopic surgery tool compared to expert surgeons
in a targeting task.

8. Applications and Extensions
7.2 Impact of UI Device on Task Performance
Despite the large visual attention differences, we observed
similar overall task performance between the UIslider,
UIpen, and UImouse conditions. Participants spent similar
times and distances traversing the controls for each of the
conditions, and took similar amounts of time to perform a
color match. Although a 20 – 30% median improvement in
match times was observed for UIslider compared to
UImouse and UIpen, this difference was not statistically
significant.
These results suggest that, using a physical control such
as sliders, one can maintain more visual attention on the
display without significantly jeopardizing control
manipulation performance. If anything, our results suggest
that performance may slightly improve even though
participants require up to 20 times fewer visual fixations.
This is supported by the observation that participants spent
more of their visual fixations on the color target compared
to the controls (see Figures 3 and 5). Cognitively
understanding how parameters affect the visual display,
plus individual differences, appeared to have a larger
impact on overall performance than the input device.
The color task was designed to model representative
parameter manipulation tasks, and to provide a conservative
range of task difficulty, which was supported by the high
variability in match times both within and between
participants seen in Figure 6. Our results indicate that
physical controls enable users to better maintain visual
attention on the screen during such tasks, but that this is not
a major factor influencing overall performance.
Nonetheless, we conjecture that future studies involving a
greater number of participants (i.e., greater statistical
power), would result in statistically significant
improvements in task performance for physical controls. In
addition, maintaining visual attention on the display may
play a larger role for visualization displays more complex
than our simple color display, and may be especially
important in time and safety critical environments.
7.3 Impact of Easy versus Difficult Controls
Our results support our conjecture that cL* and ct represent
separable (‘easy’) controls whereas ca* and cb* represent
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

The following discussion explicitly summarizes how our
results might be applicable for other user interfaces and
applications.
User interfaces
The pen interface in our study is conceptually between a
mouse and physical slider. The major difference between
the mouse and the other devices is that input with the
mouse is indirect. The major difference between the pen
and physical sliders was that the pen required handling
graphical controls on the display like the mouse rather than
grasping controls that protrude out of the screen. With the
physical slider, the graphic and haptic feedback are more
tightly coupled during parameter manipulation than with
the pen or mouse interfaces. Consequently, we might expect
similar results to those presented in this paper from other
physical controls with similar properties as the sliders.
Such controls include other one-dimensional interfaces such
as buttons and knobs. Also, many two- and threedimensional controls have similar properties (e.g.,
interactive props such as the neurosurgical interface
described by Hinckley et al. [HPGK94]).
These physical controls have typically been used to
directly modify objects in a graphical scene instead of
manipulating more abstract concepts such as our color
matching task’s display parameters.
Applications
We studied a color matching task that is representative of
the high-level concept of interacting with a parameter space.
This includes ‘classic’ areas such as audio mixing and
video games, where physical controls are commonly used,
but also more abstract applications where their use is
currently limited. Examples include adjusting visual display
parameters for graphics and visualization applications (e.g.,
setting lighting or material properties to modify appearance
of objects in an architecture rendering), parameter tuning
for software prototyping and algorithm implementation
(e.g., finding optimal default zoom settings for a geographic
information system), and adjusting input parameters for
scientific simulations (e.g., choosing hydrogen and oxygen
mix ratios to optimize fuel cell performance).

926
9. Future Work

C. Swindells et al. / Comparing Parameter Manipulation with Mouse, Pen and Slider User Interfaces

Future work should be performed in two main areas. First,
comparisons of different physical user interfaces other than
sliders should be performed. Second, future studies should
compare differences within specific applications, then
compare these results to more general, abstract tasks such
as the color matching task described in this paper. These
studies could include situations where the controls were
further away, or the visualization was a different size. Such
studies might influence the relative performance of certain
controls.
In addition, future studies of parameter manipulation
could consider bimanual interaction in greater detail. In our
study, participants rarely moved two physical sliders
simultaneously; however, they may have occasionally held
two sliders at the same time, reducing the need to acquire
them. Future studies could compare these and other
potential advantages of bimanual interaction.
10. Conclusions
We have demonstrated that users may spend up to 95%
fewer visual fixations on physical sliders versus standard
mouse and pen interfaces without any loss in performance
for a representative visual performance task. We observed
levels of significance less than p = .001 and effect sizes
representing over 90% non-overlap between slider vs.
mouse & pen user interfaces. Additionally, users
maintained these results across a large range user interface
control difficulty. These results strongly suggest that users
performing high cognitive load tasks may free much of
their visual attention from mouse or pen tools if they use
slider tools instead. These results are applicable to both
direct manipulation and abstract parameter handling tasks.
Acknowledgements
We thank the Natural Sciences and Engineering Research
Council (NSERC) of Canada for funding this research.
References
[AWS92] AHLBERG, C., WILLIAMSON, C., AND
SHNEIDERMAN. B.: Dynamic queries for information
exploration: an implementation and evaluation. In Proc.
CHI 1992, ACM Press (1992), 619–626.
[AW00] ARSENAULT, R. AND WARE, C.: Eye-hand coordination with force feedback. In Proc. CHI 2000, ACM
Press (2000), 408–414.
[BM86] BUXTON, W. AND MYERS, B.: A study in twohanded input. In Proc. CHI 1986, ACM Press (1986),
321–326.
[CBG04] CHIPMAN, L.E., BEDERSON, B.B., AND GOLBECK.
J.A.: Slidebar: analysis of a linear input device.
Behaviour and Information Technology, 23(1):1–9, 2004.
[Coh88] COHEN, J.: Statistical power analysis for the
behavioral sciences (2nd ed.), Lawrence Erlbaum
Associates, Hillsdale, NJ, USA, 1988.
[CBS*07] CRIDER, M., BERGNER, S., SMYTH, T.N., MÖLLER,
T., TORY, M.K., KIRKPATRICK, A.E., AND WEISKOPF, D.:

A Mixing Board Interface for Graphics and Visualization
Applications, In Proc. Graphics Interface 2007, 87-94.
[FB97] FITZMAURICE, G. W. AND BUXTON, W.: An
empirical evaluation of graspable user interfaces:
towards specialized, space-multiplexed input. In Proc.
CHI 1997, ACM Press (1997), 43–50.
[GF01] GREENBERG, S. AND FITCHETT, C.: Phidgets: easy
development of physical interfaces through physical
widgets. In Proc. UIST 2001, ACM Press (2001), 209218.
[HYA*08] HARTMANN, B., YU, L., ALLISON, A., YANG, Y.,
AND KLEMMER, S.R.: Design As Exploration: Creating
Interface Alternatives through Parallel Authoring and
Runtime Tuning. In Proc. UIST 2008 (to appear).
[HPGK94] HINCKLEY, K., PAUSCH, R., GOBLE, J.C. AND
KASSEL, N.F.: Passive Real-World Interface Props for
Neurosurgical Visualization. In Proc. CHI 1994, ACM
Press (1994), 452-458.
[HK99] HUNT, A. AND KIRK, R.: Radical user interfaces for
real-time control. In Proc. EUROMICRO, IEEE
Computer Society, IEEE Press (1999), 2006–2012.
[IK77] ISHIHARA, S., Kanehara & Co., Ltd., Tests for
Colour-blindness, Kanehara Shuppan, 1977.
[JM01] JANKUN-KELLY, T.J. AND MA, K.-L.: Visualization
exploration and encapsulation via a spreadsheet-like
interface. IEEE Trans. Visualization and Computer
Graphics, 7(3):275–287, 2001.
[JSMM94] JACOB, R. J., SIBERT, L. E., MCFARLANE, D. C.,
AND MULLEN, M. P. Integrality and separability of input
devices. ACM Trans. Comput.-Hum. Interact., 1(1): 3-26,
1994.
[LAK*04] LAW, B., ATKINS, M.S., KIRKPATRICK, A.,
LOMAX, A. AND MACKENZIE, C.L.: Eye Gaze Patterns
Differentiate Skill in a Virtual Laparoscopic Training
Environment. In Proc. Eye Tracking Research and
Applications (ETRA 2004), ACM Press (2004), 41-47.
[McL76] MCLAREN, K.: The development of the CIE 1976
(L*a*b*) uniform colour-space and colour-difference
formula, J. Soc. Dyers and Colourists, 92 (1976), 338341.
[Rhe02] RHEINGANS. P.: Are we there yet? Exploring with
dynamic visualization. IEEE Computer Graphics and
Applications, 22(1):6–10, Jan/Feb 2002.
[TPM05] TORY, M., POTTS, S., AND MÖLLER, T.: A parallel
coordinates style interface for exploratory volume
visualization. IEEE Trans. Visualization and Computer
Graphics, 11(1):71–80, 2005.
[UHJ03] Ullmer, B., Ishii, H., and Jacob, R.J.K. Tangible
Query Interfaces: Physically Constrained Tokens for
Manipulating Database Queries. In Proc. of
INTERACT'03 (2003), 279-286.
[WS92] WILLIAMSON, C., AND SHNEIDERMAN, B.: The
dynamic HomeFinder: evaluating dynamic queries in a
real-estate information exploration system. In Proc. ACM
SIGIR 1992, ACM Press (1992), 338-346.

c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

