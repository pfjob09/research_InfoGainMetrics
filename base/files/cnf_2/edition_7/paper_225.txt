DOI: 10.1111/j.1467-8659.2009.01532.x

COMPUTER GRAPHICS

forum

Volume 28 (2009), number 8 pp. 2291–2301

Wipe-Off: An Intuitive Interface for Exploring Ultra-Large
Multi-Spectral Data Sets for Cultural Heritage Diagnostics

K. Ponto, M. Seracini and F. Kuester
Center of Interdisciplinary Science for Art, Architecture and Archaeology, UC San Diego, La Jolla, CA, USA

Abstract
A visual analytics technique for the intuitive, hands-on analysis of massive, multi-dimensional and multi-variate
data is presented. This multi-touch-based technique introduces a set of metaphors such as wiping, scratching,
sandblasting, squeezing and drilling, which allow for rapid analysis of global and local characteristics in the
data set, accounting for factors such as gesture size, pressure and speed. A case study is provided for the analysis
of multi-spectral image data of cultural artefacts. By aligning multi-spectral layers in a stack, users can apply
different multi-touch metaphors to investigate features across different wavelengths. With this technique, flexibly
definable regions can be interrogated concurrently without affecting surrounding data.
Keywords: cultural heritage, interface, multi-touch, multi-spectral
Categories and Subject Descriptors (according to ACM CCS): User Interfaces [H.5.2]: Input devices and
strategies

1. Introduction
The field of cultural heritage is an important yet all too often
forgotten one. Richard Ready and Stale Navrud argue in
their book Valuing Cultural Heritage that cultural heritage
is a public good, which is ‘non-excludible’ and ‘non-rival’
in consumption [NR02]. By this definition, digitized cultural
artefacts are easily accessible allowing for multiple viewers.
The inherent benefit of digitized cultural heritage comes from
the ability to maintain, interrogate and study these kinds of
artefacts in a way such that their cultural value is not lost
over time.
Projects such as the VASARI project [Mar91] [SC93] have
focused on archiving paintings, creating a persistent, high
resolution digital record. These digital records have several
advantages over the traditional methods of film-based photography recording, as these digital archives do not fade over
time if the risk of ‘bit rot’ is properly addressed. This permanent record holds great promise for the diagnostics of an
artefact and its conservation, while concurrently opening the
door for broad community collaboration in the areas of data
c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and
Blackwell Publishing Ltd. Published by Blackwell Publishing,
9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main
Street, Malden, MA 02148, USA.

sharing, archiving, modelling, synthesis and analysis. These
records can be refined and enriched over time as new imaging
modalities become available. Once this digital record of an
artefact has been created, it can be freely studied, for example, facilitating a virtual microscope to visualize the artefact
at arbitrary spatial as well as temporal scales.
Different multi-spectral imaging techniques can be combined to capture artefact characteristics, such as transmission, reflection, absorption, etc. [EKCB03] [PDMDRP08].
Originally, multi-spectral imaging was used to improve the
colour fidelity of artefacts [MCSP02]. Over time, other
uses of multi-spectral imaging were found. For example, by
analysing data in the UV spectrum, art historians can easily
see which particular areas of paintings have been restored
[Leh97].
While the gathering of these data presents many difficulties, visualizing and interrogating these data sets adds additional challenges. A popular method for interrogating spectral layers is to scroll through them, either flipping through
them like a stack of cards or by interpolating through them

2291

2292

K. Ponto et al. / Wipe-Off
∗

[KVV 04]. These approaches have the disadvantage that the
entire image must be changed, removing spatial orientation
correspondences and restricting layer comparison to a pairwise format.
Another approach to analysing stacked data is to create
arbitrary cutting planes through the data [KVV∗ 04]. This
approach also has the disadvantage of changing large portions
of data all at once. The project Khronos Projector [CI05]
took a different approach, allowing for this cutting plane
to be warped regionally. This ‘push through’ approach was
developed for video data, to study temporal correspondence
and was not intended to view volumetric data or create fine
point inquires.
In this paper, we present a system which interactively creates user-defined transfer-functions allowing colour channels for arbitrary sections of giga-pixel images to be freely
‘wiped’, ‘scratched’, ‘drilled’, and so forth to reveal other
representations of the same artefact. Using this method, targeted analysis of arbitrary regions is possible while leaving
surrounding data undisturbed.
A multi-touch, pressure-sensitive interface was developed
in combination with metaphors for hands-on data exploration. These metaphors, include wiping, scratching, squeezing, sandblasting and drilling concepts considering param-

eters, such as the touch gesture, size, pressure and speed,
combined with more traditional multi-touch techniques allowing data to be resized, rotated and moved.
The goal of this project was to create a system from the
ground-up to provide users with an intuitive way to explore
multi-dimensional large data sets as depicted in Figure 1.
To create this natural system, it was important to have high
interactivity, low latency and pressure sensitivity, which, in
turn, created the need to custom build much of the system
ourselves. Unlike large photo-viewers, an entire data stack
needed to be loaded as opposed to a single image layer that
required sophisticated resource management. The emphasis
was not only on the ability to arbitrarily zoom through largescale image layers, but also to interrogate this data set on
localized regions in visceral ways. By allowing for natural
gestures, even novice users could explore and analyse these
cultural artefacts.
2. Data Layers
As each one of these data layers, as shown in Figure 1,
can easily contain hundreds of millions to billions of pixels, it is important to only load those regions of data, which
are currently needed. Similar to large-scale image viewers
such as Microsoft’s Deep Zoom [Sre08] and other giga-pixel

Figure 1: Image stack of six different multi-spectral layers with modifiable layers. A resulting analysis is shown in the upper
right. The lower left shows the lookup table for a given resolution of the visible layer and its layout in texture memory.
c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

K. Ponto et al. / Wipe-Off

Figure 2: Tiling of Cosme Tura’s St. George. While the image size is quadrupled in each step, the number of tiles and
amount of padding does not increase in the same fashion.

2293

different multi-texture channels and subsequently combined
via a hardware shader. To make the modifiable layer more
efficient, it is important to keep the processing entirely on
the GPU. Frame buffer objects provide a powerful tool for
rendering to textures. By activating the frame buffer for rendering, re-rendering the frame buffer on top of itself, and then
rendering the manipulated data, much like a brush, a “paint
to texture” effect may be achieved. By changing the style of
how the manipulated data are rendered, different interrogation effects can be synthesized. It is then possible to swiftly
explore localized regions of interest across many data layers
all at once, allowing localized investigation without changing
data in surrounding regions.
3.1. Method for generating tiled images

viewers [Fli07] [KUDC07], data layers are broken up into
tiles as shown in Figure 2. This allows sub-sections of the
image to be loaded without massive cache penalties. Tiling
images also allows for pre-generation of tiles containing different resolutions, analogous to the mip-mapping approach
[Wil83]. Load balancing can then be achieved by loading
small sections at finer and large sections at coarser detail.
These data tiles can also be created to be the same pixel
dimensions for all data resolutions. This provides a major
benefit for transferring data between main memory and GPU
texture memory as the GPU texture memory can be allocated
at initial start-up. This allows tiles to simply be swapped in
and out of texture memory on an as-needed basis without
requiring de-allocation and re-allocation when viewpoints
change.

3. Localized Data Interrogation
For each of the original data layers, a second modifiable data
layer containing colour channel transfer functions, is created,
to allow localized data exploration as shown in Figure 3.
This modifiable layer and the data layer are then bound on

A tiled image layout is used to facilitate interactive analysis on large-scale image layers. Image tiles are only loaded
when needed for interaction, following an adaptive and progressive, out-of-core data access model. To store the needed
multi-resolution images for each layer, the TIFF format was
selected because it allows images to be tiled into individually accessible sub-regions and provides a container that
can readily group multiple images within a single file. With
this setup, tiles for any region and level of resolution can be
fetched without the need for other information look-ups. To
create the image data base, the original images are first converted to the VIPS [MC05] image format, and subsequently
converted into a tiled TIFF with tile sizes of 256 × 256 pixels.
Loss less deflate compression is applied to these tiles to minimize data footprint and optimize transmission performance
needed for fast out-of-core lookups.
Systems such as Microsoft’s Deep Zoom [Sre08] which
are catered to server systems generally break each image tile
into a separate file. In Deep Zoom these image files are put
into separate folders depending on the resolution from which
they were generated. While this approach is good for a server
model, on a local machine an added cost for file I/O will be
incurred from opening and closing a multitude of files as
opposed to a single TIFF file.
Unfortunately, generating equal sized tiles does complicate the matter when trying to match data resolutions. Because data may not completely fill all of the tiles, extra
padding must be added to fill the extra space. This amount
of padding will vary for different levels of resolution creating potential problems for subsequent tile alignment. To
address this, a data object is created such that the tiled image
is re-scaled with the extra padding falling outside of the data
object. This method ensures that features between resolutions
remain consistently aligned.
4. Resource Management

Figure 3: A photo of the user interrogating data using our
system.

In this section we will discuss the methods used in our resource management system. With growing data set sizes the

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2294

K. Ponto et al. / Wipe-Off

need for optimized resource management increases and resource management systems, tasked with accessing, partitioning, processing and delivering data, are needed. From
the visualization perspective, the resource manager has to
choose relevant data at the resolution and processing level
appropriate for the current visual, load it into memory, curate and ideally remove it when it is no longer being used.

memory. While attractive for its simplicity, a simple round
robin replacement could cause actively displayed tiles to be
swamped out. To address this issue, tiles are labelled with
the last frame during which they were used. The replacement
algorithm then enforces that only those tiles that are stale are
replaced, thereby avoiding a visual ‘popping’ effect.

4.3. Data stack
4.1. Data loading
For each data object, a table is created which contains texture
memory placement pointers for each tile for each resolution
of the image. Each table entry is defaulted to zero, indicating
that no data for this tile exist in memory or in GPU texture
memory. When data for this tile are requested from disk, a
texture pointer finds an open or stale section of pre-allocated
texture memory while the data are loaded into RAM. Once
data are loaded into memory, the data are transferred to the
GPU via a pixel buffer object for increased transfer performance.
On a periodic interval, the state of the data object is compared to screen space. The current resolution of the loaded
tiles is compared to the resolution at which they are currently being displayed. If the tiles are being under- or oversampled, loading is changed to a higher or lower resolution
tile set accordingly. The threshold was set such that an image
which is over-sampled by 1.5 times the correct resolution or
under-sampled by 0.75 times the correct resolution would be
switched as to avoid hysteresis.
Next, for the given target resolution, each tile within the
viewing frustum is checked to verify whether it has already
been uploaded to texture memory. If not, it is loaded from
disk into RAM and then pushed into GPU texture memory.
Because smooth interaction is desired and disk access may
be a slow operation, tiles are adaptively and progressively
loaded, in accordance with a tunable time constraint.
Progressive loading in turn means that data for the given
resolution may not be available for every frame drawn. To
mitigate this problem, the lowest resolution version of the
image, which accounts for a single tile is stored. If all of the
data cannot be drawn for a given frame, the lower resolution
tile is drawn behind the higher resolution tiles. Although this
proves to only happen in rare circumstances, it is much less
jarring to see a blurry variation of the data sharpen than to
see no data.

4.2. Replacement scheme
With data commonly exceeding texture memory size, a
replacement scheme is needed to optimally use the preallocated texture space. As texture tiles are loaded from disk,
they sequentially fill up the pre-allocated texture space. Once
the texture space is filled, tiles must be swapped out of texture

By stacking multiple data layers on top of one another, a
data stack can be created which contains multi-spectral and
possibly time-varying information. Each of the data layers is
re-sampled to fit the bounds of the data stack object. In this
way, data layers can be viewed at different resolutions. Also
data layers can be adjusted through affine transformations to
allow for simple adjustments.
Since at any point in time any slice of the data stack may
need to be accessed, every slice must be fully loaded. Each
layer slice is treated as a separate data object with its own
lookup tables and pre-allocated section of texture memory.
When the data stack is transformed (translated or rescaled),
the top-most layer slice is prioritized to load its tiles first, with
each deeper image layer loading subsequently in order.

5. Multi-Touch Interaction
While Multi-touch technology has only recently become used
in consumer applications, the history of multi-touch research
has spanned multiple decades. In 1984, Lee wrote his master’s thesis [Lee84] on the use of multi-touch and a year
later continued his work with Buxton and Smith [LBS85].
Unfortunately, while the interface technology was being developed, the computation power was still lacking. In 2001,
Westerman et al., wrote a paper in Human-Computer Interaction discussing how multi-touch could be used as an intuitive
computer interface [WEH01]. Later that year, the DiamondTouch, a multi-user touch system was produced [DL01]. In
2004, much buzz was created from Han [Han05] and Wilson
[Wil04] who created more accessible approaches to multitouch technology. Smith et al. [SGHB07] proposed creating
low cost pressure-sensitive surfaces in 2007.

5.1. Multi-touch technologies
Multi-touch can be acquired in a variety of ways, but the
general approaches can be classified into the categories of
capacitive touch, camera pairs, diffuse illumination (DI) and
frustrated total internal reflectance (FTIR). Each of these
methods has it’s own advantages and disadvantages.
Capacitive touch is implemented in several projects and
products like the DiamondTouch [DL01] and the iPhone.
Capacitive touch in a manner of speaking allows for the
user’s touch to complete a circuit. While this system would

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

K. Ponto et al. / Wipe-Off

2295

provide an effective method of acquiring user touches, it was
not selected as it would require a much higher cost for entry
for hardware, software and would also be challenging to scale
and use for rear projection.
Camera pair technology [Wil04] presents a good method
for creating quick depth maps. These systems detect how
close an object is to the camera. Because the precision of
these systems is not exact, it is not possible to tell whether
a user is touching the system or is simply is within a close
proximity to the screen. Further more, these systems can
not determine the amount of pressure the user is applying.
Additionally, these systems also requires that the screen be
semi transparent so that the cameras can see through them.
Diffuse Illumination [WWJ∗ 09] also is able to determine
proximity of user touches. In this method the background
is illuminated by an infrared emitter behind the screen. The
closer the user’s hand is to the surface the more in-focus the
reflected light is reflected back at the camera. This method
also lacks the ability to truly determine whether a user is actually touching the surface and the pressure of the touch itself.
Microsoft surface [Bro08] uses a combination of diffuse illumination and camera pair technology in order to determine
touch events.
Frustrated total internal reflection uses lighting from the
side as opposed to the back as seen in diffuse illumination.
When a user touches the screen, the light bouncing through
side lit material (generally acrylic) is disturbed and is scattered. A camera looking at the screen will see touch points
as being brighter than the background [Han05]. By adding a
piece of silicon on the surface, the pressure of the touch can
also be directly acquired [SGHB07].
The frustrated total internal reflection method was selected
as the multi-touch approach because of its advantages for
gathering real touches and pressures of user touches as well
as the ability to scale the screens to different sizes for future
research.

Figure 4: A System Diagram. Silhouette is proportional to
the table’s actual size.

Rear projection is used to illuminate the 88-centimetre diagonal table surface with a 4:3 aspect ratio of and SXGA
(1280 × 1024) resolution. An infrared sensitive camera
with IR band pass filter is used for the acquisition. Camera resolution is 640 × 480 at 8 bit grey-scale and 60 Hz
resulting in a touch resolution of 70 points per square
centimetre. Touch processing and visual analytics tasks are
performed on two networked nodes. The touch server algorithms are efficient enough to be run on a single-core 3
GHz Pentium 4 with 1 GB of RAM and a nVidia 6800 Ultra graphics card. The visual analytics node uses a more
powerful Intel Core 2 Extreme QX6700 (2.66 GHz), with
4 GB of RAM, and a nVidia GeForce 8800 GTX graphics
card.

5.2. Multi-touch hardware
In this paper we present the development, completion and
implementation of a multi-touch interface (Figure 4). Our
work endeavered to create a low cost pressure-sensitive surface as proposed by Smith et al. [SGHB07] based on the
frustrated total internal reflection technique to illuminate a
composite touch surface consisting of a sandwiched acrylic,
silicon and low-friction surface layers. This surface composition supports the creation of pressure-sensitive touch gestures rather than just binary touch events. The framing for
the table was built using extruded aluminium with notches
cut for the acrylic glass and LED light strips surrounding the
acrylic. The low-friction surface layer also serves as the final
diffuser.

5.3. Blob detection
In order to determine where on the screen a user is touching,
camera input is parsed in order to determine regions of activity called blobs. Blobs are processed in two independent
passes, one on the GPU and a second on the CPU. When
the image is first acquired by the camera, it is loaded into
texture memory and this texture image is then redrawn, rendering it into a frame-buffer object while a fragment shader
is activated. This render-to-texture approach allows data to
processed and retained on the GPU. Subsequent shaders can
then use the resulting texture held in the frame buffer object,
which provides a low cost approach to processing with kernel
filters.

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2296

K. Ponto et al. / Wipe-Off

Figure 5: The result of the filter output for each step. The final image uses false colours to indicate pressure.
Several filters are applied to the raw camera imaging as
shown in Figure 5, (1) correcting for intrinsic camera parameters such as lens distortion, (2) image warping to achieve a
match with the physical display surface, (3) Gaussian blurring to smooth noise, done first in the horizontal direction
and then again in the vertical direction to reduce texture
lookups, (4) the resulting image is subtracted from an averaged background image, before (5) all pixels which considered to be within the threshold of noise are removed leaving
only pixels that correspond to actual touch-points. These filters help to prepare the image so that it can be used for blob
detection.
Past this processing stage, the standard fragment programs
via the GPU are no longer effective and data are transferred
back to the CPU for the following processing steps. The data
can be traversed in a single pass using active edge tables
[KCC08]. By simply storing when a blob starts and stops for
each given scan-line, the data may be analysed with minimal
cache penalties. For each blob, a bounding box consisting of
the max and min value for both the x and y dimensions as
well as the maximum intensity value are found and stored.
The information for all of the touches is then packed, and
sent via UDP to the display node. While more data could be
sent, by only sending 5 integer values per touch data copying
is minimized and the system has the ability to scale for a
magnitude of users and touches without over saturating the
network.
By pipelining these two processes, performance is improved dramatically when processing large numbers of blobs
(from 30 to 60 fps). With image acquisition and blob processing distributed over two threads, frame-rate is unaffected
as long as the blob detection is faster than frame acquisition,
which is achieved.

5.4. Gestures
Once all of these touch-points are received on the display node, the application had to determine how to process
them into meaningful input. Each touch-point used a bounding box intersection test to determine if it is a new touch
or a continuation of an existing touch. The touch-points
are then hierarchically broken down into those interfacing
with the application’s I/O (buttons, menus, sliders, etc) and
those dealing with the data stack. The touch-points inter-

Figure 6: Gestures for transformations for different numbers of fingers.

secting the data stack are processed differently depending
on whether the user is in transformation or interrogation
mode.

5.5. Transformation mode
For the manipulation of changing the viewpoint of the object,
a very simple gesture system is created as shown in Figure 6.
For every touch that is currently touching the image stack, a
bounding box surrounding that point is created as well as for
the previous touch-points. The centre of the current bounding box is compared with the previous bounding box’s centre.
Any movement in this centre is added as a translation. Any
size change in the bounding box is turned into a scaling operation around the centre of the current bounding box. Through
this paradigm, users can zoom and translate simultaneously
with any number of fingers. In fact, users can even re-scale
and translate the image using different pressures of finger
presses.
It is also possible to add the act of rotation into this
paradigm. For this project, however, rotation functionality
is purposely removed as it created confusion for novice users
while offering only marginal benefits.

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

K. Ponto et al. / Wipe-Off

2297

Figure 7: Scratching off several layers, showing the infrared
layer. The infrared layer contains the representation of the
under-drawing for the painting.

Figure 8: Sandblasting through to the X-ray layer. Using
the sandblast technique it is easy to extract the metal bracing
behind the painting.

5.6. Interrogation mode

5.6.2. SandBlasting

Three different metaphorical gestures were found to be very
useful for this visceral data analysis.

The sandblast modality mimics the effect of slowly blasting
away layers. Users first select a target layer, which sets the
base layer which users can blast down to. In this way, user’s
touches will ‘blast away’ all layers above the target layer.

5.6.1. Wiping and scratching

To emulate a sandblast modality, a soft edged texture
drawn into the frame buffer several times randomly with in
a radius touch falling off equal to the distance squared from
the centre of the touch. The size of this soft edged texture is
randomly chosen with a size no greater than one fourth of
the area of the touch-point. This soft edge textures opacity
is set proportionally to pressure of the touch, but even with
hard presses remains fairly translucent (i.e. 10% opacity). By
continuously pressing a single spot, the layers will continue
to be whittled away. This technique is good for locating and
extracting features between layers as shown in Figure 8. In
this example, the metal bracing can be easily distinguished
in a single light pass. From this pass, this feature can easily
be extracted through another harder pressure pass.

The wiping and scratching modality mimics the effect of
wiping and scratching away layers as shown in Figure 7 for
scratching. Users first select a target layer, which sets the
base layer which users can scratch down to. In this way,
user’s touches will ‘wipe’ or ‘scratch’ away all layers above
the target layer.
The wiping effect is emulated by drawing a very soft edged
texture in the frame buffer. The size of the soft edged texture matched that of the users touch. The wiping modality removes layers by mimicking a Gaussian-like curve,
removing more data at the centre of the touch and less
data at the edge of the touch proportionally to the force
of the users touch. This modality allows for interrogation
between layers which blends removed data with persistent
data.
The scratching modality is different from the wiping
modality in as there is a hard distinction between removed
data and persistent data. To emulate a scratch type modality, a rough edged texture drawn into the frame buffer several times randomly with in a radius touch falling off equal
to the distance squared from the centre of the touch. The
size of this rough edged texture is randomly chosen with
a size no greater than one fourth of the area of the touchpoint. This modality allows for quick interrogations between
layers.

5.6.3. Drilling and squeezing
In the drilling and squeezing modes, the user does not select
a target layer, but instead controls the depth by the pressure
of the touch. The harder the user presses, the farther down
the image stack the deformation occurs as shown in Figure 9.
While in the drilling mode this cut remains constant, whereas
the squeezing mode simulates pushing through into a sponge,
which returns to form when the touch is released.
To emulate these drilling and squeezing effects, multiple
layers need to be modified at once. The force at which the
user’s touch the surface control the depth of the cut in the

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2298

K. Ponto et al. / Wipe-Off

Figure 10: Graph of frame-rate for processing different
amounts of blobs.
Figure 9: The result of user squeezing through the entire
data stack. The residual impression of the finger being in the
centre of the screen remains.

drilling mode. All of the layers above this depth are filled in
with a soft brush proportional to the size of the user’s touch.
In both methods, each touch is treated as a 3D input, based
on the size of the touch and the pressure. Because the depth
of the cut is controlled by the pressure of the touch, multiuser, or even single user, multiple finger interaction across
multiple data layers is possible.
The main difference between the squeezing and the drilling
modalities is that in the drilling method, as in the aforementioned methods, the data removed are maintained until the
user clears their user actions. In the squeezing mode, the
removal is of layers is only temporary and is ‘healed’ over
time.
Because the removal in the squeezing modality is restored
over time, a larger area around each touch is modified. Layers which the press is determined to intersect are filled with
a soft circle originating from the center of the touch. Each
subsequent higher layer is filled in with a larger soft circle
emulating this squeezing effect. These textures are slowly restored to their original form, mimicking a sponge-like recovery. This method is very useful for finding sharp differences
between data layers and multi-user input.

As can be seen from Figure 10, the processing stage runs
at 700 frames per second (fps) and higher when processing normal amounts of 20 blobs or less. As the number of
blobs increases, the performances degrades, but even while
processing 500 blobs at once, the frame-rate is still above
the required 60 fps needed to keep up with image acquisition. Even while processing an excessive 1200 blobs, the
processing remains above 30 fps.
For comparison, we can look to the performance of TouchLib, an open source alternative. TouchLib does its filtering
on the CPU as opposed to the GPU. By doing this filtering
on the GPU, the processing can be done in parallel, creating
a substantional speed up as shown in Figure 11.
Touchlib also uses contour detection for its blob detection. This method requires extra memory copies and dynamic
memory allocation as these contours are detected. In the active edge table method used in this paper, all memory can
be generated at startup and reused without need for dynamic
allocation. From Figure 11 one can see that the method is
over 50% faster when tracking a handful of blobs.
The drawing node was tested in its two operation modes.
In the transformation mode the number of frames drawn per
cycle was recorded. These frames were divided into frames
which were drawn only using the full resolution, and frames

6. Results
6.1. Performance
It is important for the image processing algorithms to be able
to keep up with the image acquisition. As more blobs are
detected, the greater the number of checks which must be
performed, as well as the greater the number of blobs which
must be processed and transferred.

Figure 11: Table of performance comparing time taken for
filtering and processing. TouchLib data taken from [SBD∗ ].

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

K. Ponto et al. / Wipe-Off

2299

Figure 14: Video capture frames showing the touch-point
appearing in frame 1 in the upper left and the subsequent
projected hot spot two frames later in the centre.

Figure 12: Graph of frame-rate while using the system in
transformation mode.

image. When the user pressed the surface, the projected image was changed to a white image, which in turn generated a
hot spot in the cameras view. Measurements were taken between detection of the first touch and the time the projected
image appeared on screen. As seen in Figure 14, two frames
passed between the touch and the detection of the projected
image. This accounts for somewhere between 30 and 50 ms
of delay internal to the system.
This latency comes from a variety of factors. The camera
runs at 60 Hz, but has an inherent latency of a single frame,
meaning approximately 16 ms of delay. It took approximately
2.5 ms to transfer the data from the camera to the GPU. To
apply all of the filters and transformations in the GPU took
approximately 1 ms. Transferring the data back to the CPU
took approximately 1 ms. Processing the data and sending
it via the network took approximately 2.5 ms. The latency
of sending the data through the network was on the order
of a few milliseconds. Unfortunately, the projector only has
a 60 Hz refresh rate, meaning that it may take up to an
additional 16 ms before the image is displayed.

Figure 13: Graph of frame-rate while using the system in
interrogation mode.

which were not fully loaded as shown in Figure 12. This
included times when these unloaded tiles may not be noticeable. It is important to note that the preview resolution was
needed less than 10% of the time, but by using this in place
of pausing to load more data, the frame-rate never dropped
under 60 Hz. In the interrogation mode, frame-rates were
consistently around 300 fps no matter what layer was selected, or how much information in the modifiable layer was
altered as shown in Figure 13.

6.2. Latency
Latency is an often overlooked aspect of measurement performance. This is often because latency is difficult to ascertain,
as measurement devices have their own inherent latency. To
evaluate the latency of our system, we removed the IR filtering so that the blob detection system would pick up the
projected image. Starting with an untouched surface, a black
image was projected which did not show up in the camera

While this upper bound of 50 ms may seem to be poor, it
is still fast when compared to other factors. Human reaction
times are generally on the order of hundreds of milliseconds
[TFM96]. Also in comparison, TouchLib has latencies of
93–225 ms [SBD∗ ]. This latency also needs to be contrasted
with interactivity, which is consistent at 60 Hz.

7. Applications
We have worked directly with art historians to provide a
powerful tool to explore and verify art works. Multi-spectral
data analysis can be acquired through a variety of techniques.
Generally, useful imaging modalities prove to be in the visible, infrared, ultraviolet and X-ray spectrums. Each of these
data layers are then correlated and can be very large. The
art historians found the ability to swiftly pan and zoom in
through these large data sets, as well as the ability to naturally
wipe away data layers in localized regions to be enabling for
a deeper understanding of the artwork.
Several cultural artefacts have been analysed through our
system. Multi-spectral layers from Leonardo da Vinci’s The
Adoration of the Magi and Cosme Tura’s St. George paintings
provided beautiful data for analysis. In the example of The
Adoration of the Magi, six different multi-spectral data layers

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

2300

K. Ponto et al. / Wipe-Off

were used. The visible layer encompassed 361 megapixels
while the pseudocolor IR, infrared reflectography, ultraviolet
colour fluorescence, X-ray and reverse-side visible layers
were 21,481, 21, 481 and 21 megapixels, respectively. All
told, these data set gave art historians hands on access to
almost 1.4 giga-pixels worth of information.
While this system’s ease of use was helpful for art historians, it proved to be even more useful for novice users.
The naturalness of the system lowered the barrier for entry
of everyday users to gain a greater understanding and appreciation for the works of art. Work has begun with local art
museums to create exhibits for the public based on the developed system. We envision putting these derivative systems
circa works of arts, loaded with their spectral data layers. In
this way, the general public can explore these artefacts, peeling back layers of paint, to reveal these work’s inner story.

8. Conclusions
In this paper we present a method to interrogate large-scale
multi-variate data in a very intuitive way. By providing the
visceral experience of clearing data with one’s hands, users
can easily explore and investigate these data sets. Unlike
many previous large-scale image stack interrogation methods, data can be analysed in localized regions allowing for
a more refined analysis. This technique of examination for
multi-variate data has strong applications in the field of cultural heritage. The natural method of ‘wiping’, ‘scratching’,
‘squeezing’, ‘sandblasting’ and ‘drilling’ through data provides an instinctual method of visualization study, accessible
for art historians and general public alike.
Acknowledgments
The research presented in this paper was supported by Stacy
and Paul Jacobs, the Friends of CISA3 and the UC San
Diego Chancellor’s Interdisciplinary Collaboratories Fund.
We would also like to thank Roger Jennings and Seth Sandler for their assistance on the pressure-sensitive touch surface design as well a Kai Doerr, So Yamaoka and the members of the Calit2 Graphics, Visualization and Virtual Reality
Laboratory (GRAVITY). The above technical assistance and
financial support are greatly appreciated. All opinions, findings, and conclusions are those of the authors and do not
necessarily reflect those of the sponsoring partners.
References
[Bro08] BROWN S.: Hands on Computing. Scientific American 299, 1 (2008).
[CI05] CASSINELLI A., ISHIKAWA M.: Khronos projector. In
SIGGRAPH ’05: ACM SIGGRAPH 2005 Emerging technologies (New York, NY, USA, 2005), ACM, p. 10.

[DL01] DIETZ P., LEIGH D.: Diamondtouch: a multi-user
touch technology. In UIST ’01: Proceedings of the 14th
annual ACM symposium on User interface software
and technology (New York, NY, USA, 2001), ACM,
pp. 219–226.
[EKCB03] EASTON R. L. J., KNOX K., CHRISTENS-BARRY W.:
Multispectral imaging of the archimedes palimpsest. In
Applied Imagery Pattern Recognition Workshop (2003),
pp. 111–116.
[Fli07] FLINT G.: The gigapxl project, 2007.
[Han05] HAN J. Y.: Low-cost multi-touch sensing through
frustrated total internal reflection. In UIST ’05: Proceedings of the 18th annual ACM symposium on User interface
software and technology (New York, NY, USA, 2005),
ACM, pp. 115–118.
[KCC08] KIM D., CHA K., CHAE S.: A high-performance
openvg accelerator with dual-scanline filling rendering.
IEEE Transactions on Consumer Electronics 54, 3 (2008),
1303–1311.
[KUDC07] KOPF J., UYTTENDAELE M., DEUSSEN O., COHEN
M. F.: Capturing and viewing gigapixel images. In
SIGGRAPH ’07: ACM SIGGRAPH 2007 papers (New
York, NY, USA, 2007), ACM, p. 93.
N.,
VISHWANATH
V.,
[KVV∗ 04] KRISHNAPRASAD
VENKATARAMAN S., RAO A., RENAMBOT L., LEIGH J.,
JOHNSON A., DAVIS B.: JuxtaView-a tool for interactive visualization of large imagery on scalable
tiled displays. In 2004 IEEE International Conference on Cluster Computing, (2004), pp. 411–
420.
[LBS85] LEE S., BUXTON W., SMITH K. C.: A multi-touch
three dimensional touch-sensitive tablet. In CHI ’85: Proceedings of the SIGCHI conference on Human factors in
computing systems (New York, NY, USA, 1985), ACM,
pp. 21–25.
[Lee84] LEE S.: A fast multiple-touch-sensitive input device.
Master’s thesis, University of Toronto (1984).
[Leh97] LEHRLE R. S.: Forensics, fakes, and failures: Pyrolysis is one part in the overall armoury. In Journal of
Analytical and Applied Pyrolysis 40-41 (1997), pp. 3–
19.
[Mar91] MARTINEZ K.: High resolution digital imaging of
paintings: The vasari project. Microcomputers for Information Management 8, 4 (1991), 277–83.
[MC05] MARTINEZ K., CUPITT J.: VIPS—A highly tuned
image processing software architecture. In IEEE

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

K. Ponto et al. / Wipe-Off

International Conference on Image Processing (2005),
pp. 574–577.
[MCSP02] MARTINEZ K., CUPITT J., SAUNDERS D., PILLAY R.:
Ten years of art imaging research. Proceedings of the IEEE
90, 1 (2002), 28–41.
[NR02] NAVRUD S., READY R.: Valuing cultural heritage. Edward Elgar Publishing Ltd., 2002.
[PDMDRP08] PELAGOTTI A., DEL MASTIO A., DE ROSA A.,
PIVA A.: Multispectral imaging of paintings. Signal Processing Magazine, IEEE 25, 4 (July 2008), 27–36.
[SBD∗ ] SCHO¨ NING J., BRANDL P., DAIBER F., ECHTLER F.,
HILLIGES O., HOOK J., L¨OCHTEFELD M., MOTAMEDI N.,
MULLER L., OLIVIER P., ET AL.: Multi-touch Surfaces: A
Technical Guide. Technical Report TUMI0833: Technical
Reports of the Technical University of Munich, 2008.
[SC93] SAUNDERS D., CUPITT J.: Image processing at the national gallery: The vasari project.
[SGHB07] SMITH J., GRAHAM T., HOLMAN D., BORCHERS J.:
Low-cost malleable surfaces with multi-touch pressure
sensitivity, pp. 205–208.
[Sre08] SREENIVASAN M.: Microsoft silverlight.

2301

[TFM96] THORPE S., FIZE D., MARLOT C.: Speed of processing in the human visual system. Nature 381, 6582 (1996),
520–522.
[WEH01] WESTERMAN W., ELIAS J., HEDGE A.: Multi-touch:
A new tactile 2-d gesture interface for human-computer
interaction. In Proceedings of the Human Factors and Ergonomics Society 45th Annual Meeting (Minneapolis/St.
Paul, MN, 2001), vol. 1, pp. 632–636.
[Wil83] WILLIAMS L.: Pyramidal parametrics. In SIGGRAPH
’83: Proceedings of the 10th annual conference on Computer graphics and interactive techniques (New York, NY,
USA, 1983), ACM, pp. 1–11.
[Wil04] WILSON A.: Touchlight: an imaging touch screen and
display for gesture-based interaction. In Proceedings of
the 6th international conference on Multimodal interfaces
(New York, NY, USA, 2004), ACM, pp. 69–76.
[WWJ∗ 09] WEISS M., WAGNER J., JENNINGS R., JANSEN Y.,
KHOSHABEH R., HOLLAN J. D., BORCHERS J.: Slapbook:
tangible widgets on multi-touch tables in groupware environments. In TEI ’09: Proceedings of the 3rd International Conference on Tangible and Embedded Interaction (New York, NY, USA, 2009), ACM, pp. 297–
300.

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

