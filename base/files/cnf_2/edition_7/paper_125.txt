Eurographics Symposium on Rendering 2009
Hendrik P. A. Lensch and Peter-Pike Sloan
(Guest Editors)

Volume 28 (2009), Number 4

BTF Compression via Sparse Tensor Decomposition
Roland Ruiters and Reinhard Klein
Institute for Computer Science II, University of Bonn †

Abstract
In this paper, we present a novel compression technique for Bidirectional Texture Functions based on a sparse
tensor decomposition. We apply the K-SVD algorithm along two different modes of a tensor to decompose it into
a small dictionary and two sparse tensors. This representation is very compact, allowing for considerably better
compression ratios at the same RMS error than possible with current compression techniques like PCA, N-mode
SVD and Per Cluster Factorization. In contrast to other tensor decomposition based techniques, the use of a sparse
representation achieves a rendering performance that is at high compression ratios similar to PCA based methods.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism—Color, shading, shadowing, and texture

1. Introduction
The reproduction of the reflectance properties of materials
is one of the central problems of photo realistic rendering.
In the last years, a lot of work in this area has concentrated
on the Bidirectional Texture Function (BTF) introduced by
Dana et al. [DvGNK97]. The BTF is a representation which
stores the appearance of a material in dependence on the position on the surface, the view and the light direction. It is
therefore able to faithfully reproduce a wide range of materials with spatially varying reflectance properties and complex
interreflection and self-shadowing effects.
Modern acquisition devices allow to measure discretely
sampled BTFs from material samples with both high spatial
and angular resolution. However, these measurements are
very large, consisting of several GB of raw data, and thus
various lossy compression algorithms have been proposed
in the last years. To be of practical use, these have to fulfill
two important requirements. On the one hand, they must provide high compression ratios at an acceptable degradation in
rendering quality. On the other hand, since during rendering usually every rendered pixel corresponds to a different
texture position, light and view direction, efficient random
access to the data is also necessary.

† e-mail: {ruiters, rk}@cs.uni-bonn.de
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

Many BTF compression techniques are based on matrix
factorizations (e.g. [KM03, LHZ∗ 04, SSK03]). These techniques, though, can only exploit correlations between the
columns of the matrix and thus do not take advantage of the
higher dimensional structure of BTF datasets. To overcome
this limitation, several approaches based on tensor decompositions have been proposed (e.g. [FKIS02, VT04, WWS∗ 05,
WXC∗ 08]). However, for these techniques random access
into the tensor is expensive as the reconstruction of individual entries requires evaluating long sums with many terms.
In [MMK03], the use of local PCA was proposed. This approach improves the decompression performance by clustering the ABRDFs and then performing a PCA on each cluster
independently. Since the samples in each cluster are represented in a different basis, only a smaller number of coefficients is needed. However, for each cluster a set of basis
vectors has to be stored which is not further compressed, reducing the total compression ratio.
In this paper, we propose a new sparse BTF tensor decomposition which combines the ability of tensor based techniques to exploit correlations in several dimensions with a
sparse representation that, similar to local PCA, reduces the
number of terms that have to be evaluated during decompression. This way, we achieve at the same time very high
compression ratios and a decompression performance which
is at high compression ratios superior to techniques based on
a PCA of the whole BTF dataset.

1182

R. Ruiters & R. Klein / BTF Compression via Sparse Tensor Decomposition

For this, we use the K-SVD algorithm from Aharon et
al. [AEB06] to split the BTF tensor into a dictionary and two
sparse tensors. By splitting the tensor along two different
modes, we can utilize correlations both in the spatial dimension and between different view directions. Since the dictionary itself is very compact and the two sparse tensors can
also be stored efficiently, this approach achieves very high
compression ratios. In our experiments, we achieved compression ratios that were better by a factor of three to four
than those provided by current state-of-the art methods.

2. Previous Work
2.1. BTF Compression
The Bidirectional Texture Function was introduced by
Dana et al. [DvGNK97]. It represents the appearance
of complex materials as a seven-dimensional function
ρ (c, x, ωi , ωo ) of color channel c, surface position x, light
direction ωi and view direction ωo . The BTF thus stores for
every surface position an apparent BRDF (ABRDF) called
function describing the appearance of this point in dependence on light and view direction. A considerable amount
of work has been published on the acquisition, rendering
and compression of BTFs. See [MMS∗ 04] for an overview.
In the following, we will concentrate on compression techniques based on either matrix factorizations or tensor decompositions. As these are not limited to a fixed set of basis functions, they work for a broad range of materials and
achieve good compression results.
Since the BTF is a seven-dimensional function, a tensor
is its natural representation. Therefore, compression techniques based on matrix factorization have first to represent
this tensor within two dimensional matrices. These are then
factorized and only the most significant components are
kept. Two main approaches for this exist. It is either possible to unfold the tensor or to split the whole dataset into
subsets which are then factorized independently.
In [SSK03], the images belonging to each view direction are grouped into matrices and are factorized independently. This allows for fast compression and decompression
but does not achieve very high compression ratios. The Per
Cluster Factorization [MMK03] instead divides the individual ABRDFs into several clusters, for which then a PCA is
calculated. Better compression ratios are possible via the full
matrix factorization [KM03,LHZ∗ 04]. Here, the whole BTF
is arranged in one large matrix, which is then factorized. This
way, correlations throughout the whole dataset can be utilized. A different approach is used by the chained matrix factorization [SBLD03], for which the data matrix is factorized
repeatedly. Each time, the remainder matrix is represented
using a different parametrization.
In [LBAD∗ 06], a sparse non-negative matrix factorization
is used to decompose a SVBRDF into a shade tree, which is

then used to edit the material. This is done by first factorizing a matrix, representing each of the the basis vectors in a
different parameterization and then factorizing these again.
The repeated use of a sparse matrix factorization is similar
to our approach, but as this paper is focused on editing the
factorizations are performed for each basis BRDF independently. Correlations between the basis BRDFs are thus not
utilized, limiting the achievable compression ratio.
Instead of representing the BTF as a matrix, it is also possible to directly decompose the tensor representation. Several techniques have been proposed which are either based
on a tensor product expansion [FKIS02] or on the N-mode
SVD [VT04,WWS∗ 05,WXC∗ 08]. These tensor based techniques, however, suffer from slow decompression because a
high number of terms has to be evaluated to compute one
entry of the tensor.
2.2. Sparse Signal Coding
The use of sparse representations of signals has received
considerable interest in recent years and has been used in
a wide range of applications like image denoising, restoration, classification and compression. These techniques represent a set of signals, given as the columns of the matrix Y,
as a sparse combination of the columns of a dictionary D:
Y ≈ DX. For a fixed dictionary D, a sparse representation
X with at most k entries in each column can be found by
solving the following problem:

min Y − DX
X

2

subject to ∀i : Xi

0

≤ k.

(1)

Here, Xi 0 is the number of non-zero entries in column i
of X. The exact solution to this problem is NP-hard, but several pursuit algorithms exist which efficiently approximate
the solution, among them are Orthogonal Matching Pursuit
(OMP) [PRK93], Basis Pursuit (BP) [CDS98] and the Focal
Undetermined System Solver (FOCUSS) [GR97]. For many
applications predefined dictionaries are used. For example,
in [NRH03] a technique for precomputed radiance transfer
is described which represents the transfer matrix in a HaarWavelet basis. Similar to our approach, this results in a compact sparse representation allowing for fast rendering. However, correlations in higher dimensions are not exploited by
this technique.
A still higher degree of sparsity can be achieved by learning a dictionary, which is especially tailored to the samples.
In [AEB06] it is shown, that a custom dictionary is superior
to both Haar and DCT dictionaries for image compression.
In this case, the user only specifies a dictionary size D and k.
Then, D and X in Equation 1 are minimized together. Several algorithms to find approximate solutions to this problem exist (e.g. [GR97], [LS00], [EAH00], [KDMR∗ 03]). In
[AEB06], Aharon et al. introduced the K-SVD and reported
superior results compared to the afore mentioned algorithms.
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

R. Ruiters & R. Klein / BTF Compression via Sparse Tensor Decomposition

1183

3.2. Sparse tensor decomposition

»
Y

·
D

A decomposition Y ≈ DX of a matrix Y into a dictionary
D and a sparse matrix X can be regarded as approximating
each of the columns of Y as a linear combination of at most
k atoms from a dictionary D. Here, Y is considered as a set
of vectors and D is a dictionary in which each column represents one atom. X then consists of sparse vectors, each of
which contains at most k non-zero entries describing how
one column of Y can be approximated as a combination of
the dictionary atoms.

X

(a)

»
T

·

D

X

(b)

Figure 1: (a) Matrix approximated as a sparse combination
of vectors. (b) Mode-3 tensor approximated as a sparse combination of mode-2 tensors.
It is a generalization of K-Means clustering which iterates
between a sparse coding step, in which X is optimized using
one of the pursuit algorithms, and a codebook update step,
in which D is optimized. This is done for each codebook entry independently by performing a SVD on a residual matrix
computed without the entry itself and for only those samples
which are represented by this codebook entry.
3. Theory

This can easily be generalized to tensors. Y is actually a
mode-2 tensor which we regard as consisting of mode-1 subtensors, arranged in a mode-1 tensor. Similarly, a mode-N
tensor T can be regarded as a collection of mode-M subtensors, which are then arranged in a mode-(N − M) tensor.
Each of these mode-M subtensors can then be approximated
as a linear combination of at most k dictionary atoms. Since
each of these dictionary atoms is itself a mode-M tensor, the
whole dictionary D is a mode-(M + 1) tensor.
For each of the subtensors from T , a sparse vector is
needed to describe which of the dictionary atoms are used
to approximate it. This can be represented as a mode-(N −
M + 1) tensor X , for which in one of the modes each fiber
has at most k non-zero entries. Figure 1 shows an illustration
of this kind of tensor decomposition for the case of a mode-3
tensor.
We thus approximate a tensor T ∈ RI1 ×I2 ×···×IN as

3.1. Definitions and notation
For the purpose of BTF compression, we regard a tensor
T ∈ RI1 ×I2 ×···×IN of order N as N-way array of tabulated
data. We assume that tensors are represented in a fixed basis
and do not distinguish between contravariant and covariant
indices by using upper and lower indices. Instead, we use
only subscripts, like Ti1 ,i2 ,...,iN , for the elements of a tensor.
We denote tensors by calligraphic letters (A, B, C, . . . ), matrices by bold upper-case letters (A, B, C, . . . ) and scalars by
non-bold letters (A, B,C, a, b, c, . . . ).
To write tensor decompositions, we use the Einstein summation convention, which states that repeated indices are implicitly summed over (e.g. ai j b jk = ∑ j ai j · b jk ). Using this
convention, C = Ai j B jk is used to denote that C is a tensor
whose elements are given by the following expression:

Ci,k = (Ai j B jk )i,k = ∑ Ai, j · B j,k .
j

Furthermore, we use the notation T(I1 I2 ...IM ×IM+1 ...IN ) to
describe the unfolding of a tensor into a matrix. During
this operation, the first M modes are mapped onto the rows
of the matrix and the remaining modes are mapped onto
the columns of the matrix. This results in a I1 I2 · · · IM ×
IM+1 · · · IN matrix.
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

T ≈ Di1 ···iM j X jiM+1 ···iN .
Here, D ∈ RI1 ×I2 ×···×IM ×D is the dictionary tensor containing D atoms and X ∈ RD×IM+1 ×···×IN is the tensor describing how the atoms from D are combined. X is a sparse
tensor in which each mode-1 fiber contains at most k nonzero entries.
To obtain such a decomposition for a given tensor T , we
first unfold the tensor in such a way, that the M modes corresponding to the dictionary entries are represented in one
column of the resulting matrix. Then, we use the K-SVD
[AEB06] algorithm to find the decomposition

T(I1 I2 ...IM ×IM+1 ...IN ) ≈ DX
of this unfolded tensor. By assigning D(I1 I2 ...IM ×D) = D
and X(D×IM+1 ...IN ) = X, the unfolding is then reversed to obtain the actual tensor decomposition.
This decomposition so far only utilizes correlations between the individual mode-M subtensors but not correlations
along other modes within each of the subtensors. In contrast
to matrix decompositions via SVD, the dictionary D is not

1184

R. Ruiters & R. Klein / BTF Compression via Sparse Tensor Decomposition

orthogonal. The atoms can therefore also still exhibit correlations to each other. This can be used to further improve
the compression by decomposing the dictionary D again,
this time using a different partitioning of the tensor modes.
When repeated for all modes, this finally results in one dense
mode-2 dictionary tensor D, a set of sparse mode-3 tensors
X (1) , . . . , X (N−1) and one sparse mode-2 tensor X (N) , decomposing T in the following way:
(1)

(2)

(N)

T ≈ Di1 j1 X j1 i2 j2 X j2 i2 j3 · · · X jN iN .
3.3. BTF Compression
Before the BTF compression, we subtract the mean ABRDF
from the dataset and store it together with the decomposition. This is often done for BTF compression because many
materials have a characteristic mean ABRDF which contributes strongly to nearly all samples. This way, we avoid
storing coefficients at each position for this ABRDF.
We then represent the seven dimensional BTF dataset as a
tensor B ∈ RC×L×V ×P , in which the C = 3 color channels,
the L light directions, the V view directions and the P different spatial positions are each represented in one mode. On
this tensor, we then perform a decomposition

B ≈ Lcl j V jvk Pkp
to obtain a dictionary L ∈ RC×L×D1 whose atoms represent the color in dependence on the light direction, a sparse
tensor V ∈ RD1 ×V ×D2 which gives for each view direction a
combination of dictionary atoms and finally a sparse tensor
P ∈ RD2 ×P which describes the spatial distribution of the
ABRDFs.
We decided to perform no further subdivisions in more
modes. Splitting the dictionary L into its two modes is not
necessary because L does not require much storage and thus
further compression would be of little use. It is possible
to represent the position on the surface in two modes, as
was for example done in [WWS∗ 05]. However, this would
increase the reconstruction costs further and strong correlations are only to be expected for regular patterns. Most
BTF datasets are acquired by measuring all combinations
of a fixed set of light and view directions. Furthermore, the
light and view directions are often sampled in a irregular pattern, which cannot easily be mapped onto two modes. Therefore, like other tensor-based approaches for BTF compression [VT04,WWS∗ 05], we represent each ABRDF by using
one mode for the view and one for the light directions as
otherwise the data would have to be resampled. This would
introduce additional errors, which can be avoided this way.
On the other hand, resampling would allow to use other parameterizations for the ABRDFs. For example, a parameterization via halfway and difference vector [Rus98] aligns

features of typical BRDFs along the axes of the coordinate
system and thus increases the correlations that can be exploited during compression.
4. Implementation
4.1. Compression
Performing the K-SVD calculations directly on the full BTF
dataset would be prohibitively expensive. Large datasets
would furthermore require an out-of-core implementation of
the K-SVD, which would be even slower. Therefore, we calculate the K-SVD not on the original dataset but on a projection into a lower dimensional subspace of the full BTF. For
this, we first calculate a truncated SVD B(C,L,V ×P) ≈ USVT .
We keep a rather high number (e.g. 300) of singular values,
which on the one hand reduces the size of the the dataset sufficiently to allow for further processing but still retains most
of the details in the input dataset. The SVD can be calculated
out-of-core on the GPU using the technique from [RRK09],
which reduces the time for this preprocessing to a few minutes for small datasets (e.g. 128 × 128 spatial and 81 × 81
angular resolution) and allows to preprocess even very large
datasets (e.g. 512 × 512 × 95 × 95 ≈ 26GB) within less than
an hour. Once this preprocessing has been completed, even
large datasets require only a few hundred MB and thus all
further processing can easily be performed in-core.
To get the first decomposition B ≈ Dlvk Pkp , we apply the
K-SVD to the matrix SVT , resulting in the sparse tensor P
and the dictionary matrix D, which is then projected back
into the higher dimensional space. The corresponding dictionary tensor is thus obtained by assigning D(C,L,V ×D2 ) =
UD. The second K-SVD can then directly be performed on
the unfolded tensor D(C,L×V,D2 ) because D is already sufficiently small to allow for in-core processing. During this
second calculation, we perform an additional reweighting of
D in which each atom of the dictionary is multiplied with the
norm of the corresponding row of P. This step is performed
to take the relative importance of the individual atoms during
the second K-SVD calculation into account, giving atoms
which contribute much to the final result a higher importance than rarely used ones. However, in our experiments the
K-SVD usually found very balanced dictionaries, in which
most atoms had a similar contribution to the result, and thus
we did not observe a big difference by this additional step.
Finding the best possible decomposition of datasets of this
size is probably not feasible as even the sparse coding step
is already NP-hard. Furthermore, optimizations like the use
of a projection into a lower dimensional subspace and performing the two K-SVD calculations independent from each
other, instead of simultaneously optimizing the dictionary
and both sparse tensors, are necessary to process these large
datasets. However, our results show (see Section 5), that the
approximations we can find with this approach are sufficient
to achieve very good compression ratios.
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

1185

R. Ruiters & R. Klein / BTF Compression via Sparse Tensor Decomposition

The compression times range from about four hours for a
BTF dataset with 128 × 128 spatial and 151 × 151 angular
resolution (k1 = 7, k2 = 10) to about 18 hours for a 14.77
GB dataset (335 × 346 × 151 × 152) compressed at a quite
high quality (k1 = 13, k2 = 13). These timings were performed on a computer with a Q6600 CPU and 4GB RAM
and include the parameter selection described in the next
section. For the K-SVD, we use the MATLAB implementation of the algorithm from [AEB06], which the authors
kindly made available on their website http://www.cs.
technion.ac.il/~elad/software/, using OMP as
pursuit algorithm.

4.2. Parameter Selection
For our compression technique, in total four parameters have
to be chosen. The dictionary sizes D1 and D2 for L and V
and k1 and k2 , the number of non-zero entries in the mode1 fibers of V and P. We currently use a fixed value of 256
for both D1 and D2 because this is the largest dictionary size
we can use when the indices for the sparse tensors are represented as bytes.
k1 and k2 are then chosen in dependence on the desired
compression ratio. Here, k2 primarily influences the quality of the spatial distribution, whereas the quality of the
angular approximation primarily depends on k1 . However,
because of parallax and shadowing effects, there is no total decoupling between those two parameters. For certain
applications, a strategic dimensionality reduction, as suggested in [VT04], can be desirable. However, in this paper we focus on finding a parameter combination, which
achieves the best Root Means Square (RMS) error, calculated as

1
CLV P

∑c,l,v,p (Tc,l,v,p − Tc,l,v,p )2 , where T is the

original and T the reconstructed tensor.
For a user specified maximum file size, there are several
possible combinations of k1 and k2 which could be used. To
select a combination which results in a small RMS error, we
would have to first compute the error for each. However, performing the full K-SVD calculations for each combination is
very expensive. Therefore, we apply a heuristic instead. We
observed that the final squared error can be approximated
very well by adding the squared errors that were obtained
during the first K-SVD and the weighted second one. We
now assume that dictionaries for different values of k1 and
k2 are still similar to each other and then avoid performing
the K-SVDs for all combinations by only calculating one decomposition with both high k1 and k2 . From this decomposition, we now estimate the errors that would result during
the two K-SVDs independent from each other keeping the
dictionaries fixed. With a fixed dictionary, it is still necessary to perform the pursuit calculations for each value of k
again. Instead, we simply greedily choose for each column
of the sparse matrices the k largest values from the result of
the K-SVD and calculate the error for this selection. When
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

matching pursuit is used, this is the same result as the pursuit algorithm gives. For OMP it is only an approximation,
which could however be improved by solving a linear system
of equations for each column.
Because of the simplifications we have made, the errors estimated this way are only very rough approximations.
More precise estimates could be found by performing for
each value of k a few K-SVD iterations with the already
found dictionary as initialization. However, we found that
the rough estimates are usually also sufficient to find a combination of k1 and k2 with a small final error. The decomposition is then finally repeated for these parameters, using the
afore calculated dictionaries as initialization to improve the
convergence speed.
4.3. Rendering
During rendering, we have to reconstruct random samples
from the tensor corresponding to a given position on the surface and a view and light direction. For this, we have to evaluate the sum
D2

Tc,l,v,p =

D1

∑ P j,p ∑ Vi,v, j Lc,l,i .

j=1

(2)

i=1

This would require O(D1 D2 ) operations. However, since
P is a sparse tensor, the term P j,p is non-zero for only k2
entries and only for these entries it is necessary to evaluate
the second sum at all. Similarly, since V is sparse, for this
sum only k1 terms have to be evaluated, resulting in a total
of O(k1 k2 ) operations. Thus, by using a sparse decomposition, the reconstruction cost can be considerably reduced
compared to classical tensor factorization techniques, which
have to evaluate the full sums (though with possibly smaller
values for D1 and D2 ).
For most practical applications, it is furthermore necessary to interpolate both in the angular and the spatial direction during reconstruction. We use bilinear filtering for the
spatial interpolation, and the angular filtering is performed
by calculating a Delaunay triangulation of the samples in the
hemisphere and then interpolating within the resulting triangles. Thus, 4 × 3 × 3 = 36 entries from the tensor have to be
decoded to obtain one filtered sample.
For PCA based compression techniques, the interpolation
can be performed independently in the angular domain (between 9 samples) and the spatial domain (between 4 samples), considerably improving the performance. However,
this is not possible for our technique. For each spatial position at which Equation 2 is evaluated, P has the non-zero
entries at different positions. Since the inner loop depends
on the index of the non-zero element, it iterates for each position over a different set of dictionary entries, for each of
which the angular interpolation has to be performed. Fortunately, adjacent points on the surface often have a similar

1186

R. Ruiters & R. Klein / BTF Compression via Sparse Tensor Decomposition

appearance and therefore also many of the non-zero entries
in common. When performing the bilinear interpolation, we
first iterate over the four contributing positions p, and compile a list of all indices j for which P j,p is non-zero. Since
we store the indices for the sparse tensors as sorted lists,
these can easily be merged to get a list of all contributing
indices. Then, the inner loop has to be evaluated for each
of these indices only once, eliminating repeated evaluations
and considerably improving the reconstruction speed. We
observed a speed-up of two in our experiments with this approach.
5. Results
160

PCA
PCF (32 cluster)
Sparse Tensor Decomposition

Render Time [s]

140
120
100
80
60
40
20
0
0.032

0.034

0.036

0.038

0.04

0.042

RMS Error

Figure 2: Rendering time in dependence on the BTF quality.
(On a Q6600 CPU, one core used)
To evaluate our technique, we used a BTF of a knitted fabric because it has a distinctive meso-structure creating both
shadowing and occlusion effects. It has a spatial resolution
of 256 × 256 and contains all combinations of 81 view and
81 light directions. We store the data as 16 bit floating point
values. Uncompressed the dataset requires about 2.4GB. On
this dataset, we investigate both the compression ratio and
the rendering performance. We measure both in dependence
on the RMS error, which is a good measure for the perceptual quality of a compressed BTF.
We compared our approach to compression based on a
PCA of the full BTF dataset (similar to [KM03, LHZ∗ 04]),
the per cluster factorization (PCF) [MMK03] and a tensor
factorization similar to the one proposed in [WWS∗ 05]. The
SVD and PCA are closely related, as the principal components of a centered data matrix X are given by the columns
of the matrix U with X = UDVT being the SVD of X. In this
paper, we use the term PCA to clarify, that we did not perform the SVD on the original dataset, but first subtracted the
mean. For all compared approaches, we unfolded the three
color channels into the light directions to exploit correlations
between the three color channels. Alternatively, it would be
possible to first perform a color decoupling and then store
the channels independently, using different numbers of coefficients for the different channels.
For the PCA results, we increased the number of components in steps of 5 from 5 to 100. For the PCF compression,

the graph shows results for two series, obtained by keeping
the number of clusters fixed at 16 and 32 and then varying the number of components between 1 and 29 (for 16
cluster) and between 1 and 19 (for 32 cluster). To perform
the tensor factorization, we used the tucker_als algorithm from the MATLAB Tensor Toolbox 2.2 [BK07]. As
was done in [WWS∗ 05], we compressed the spatial resolution to half of the resolution of the input images. Then we
calculated two series, one with the same number of entries
in the modes representing light and view direction, and one
for which we used twice as many entries to represent the
view than for the light. For our own approach, we used the
parameter selection technique described in Section 4.2.
In Figure 3, the file sizes necessary to achieve a given
RMS error are plotted. Compared to the best of the other
techniques, the PCA based compression, our sparse tensor decomposition achieves compression ratios which are
by a factor of three to four higher at the same RMS error. In Figure 4, we show a direct comparison between the
quality of BTF datasets compressed with the different techniques at about the same file size. Our approach preserves
considerably more high frequency details. Especially in the
protruding parts of the knitted fabric at the center and the
borders, which thus exhibit the strongest parallax effects,
more details are visible. Similarly, in the blue plastic sample
our technique preserves the specular highlights considerably
better than the other approaches. An example for a result at a
very high compression ratio for a large BTF dataset of 14.8
GB is show in Figure 5. Compared to the other techniques,
our approach preserves the structure of the fabric, shadows
and occlusion effects better.
To compare the rendering performance of different compression techniques under realistic conditions, we raytraced
a small sample scene containing a sphere with the knitted
fabric. In Figure 2, we compare the total required rendering times. We also performed measurements for BTF compression via N-mode SVD [WWS∗ 05]. Here, the rendering
time can be considerably accelerated by multiplying the tensor with the matrices in the spatial dimensions in a preprocessing step, which increases the required memory but eliminates the by far most expensive part during reconstruction.
However, even in this case the technique is still much slower
than the other approaches and thus the results are not included in the graph. A rendering with a RMS of 0.035 required more than 3000 seconds.
For high compression ratios our technique is faster than
PCA, but for higher qualities, it is necessary to increase both
k1 and k2 . While this increases the file size only linearly,
the rendering time growth with O(k1 k2 ) and thus at a certain
point PCA based techniques become faster. However, at this
point the rendering quality is for many applications already
sufficient. For example, the images in Figure 4 are rendered
at a RMS of 0.033, where our technique is about 25% slower
than the PCA based approach.
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

1187

R. Ruiters & R. Klein / BTF Compression via Sparse Tensor Decomposition
25
PCA
PCF (16 cluster)
PCF (32 cluster)
N-mode SVD (View=Light)
N-mode SVD (View=2*Light)
Sparse Tensor Decomposition

Size [MB]

20
15
10
5
0
0.03

0.035

0.04

0.045

0.05

RMS Error

Figure 3: Comparison of our approach (Sparse Tensor Decomposition) to current BTF compression techniques. At the same
RMS error, we achieve a compression ratio which is by a factor of 3 to 4 better than the PCA based compression.
Our technique is thus especially well suited for applications where high compression is necessary because here it
offers a combination of better quality together with good reconstruction performance.
6. Conclusion and Future Work
In this paper, we presented a BTF compression technique
based on a sparse tensor decomposition. We use the K-SVD
algorithm to decompose a tensor into a small dictionary and
two sparse tensors. This representation is very compact, allowing for a compression ratio which is at the same RMS by
a factor of three to four better than a PCA based approach.
At the same time, the approach is much faster than other tensor decomposition based approaches, achieving a rendering
performance similar to PCA based techniques.
In our current MATLAB implementation, the compression times are still quite high. However, in [RZE08] a faster
K-SVD techniques has recently been proposed, which might
considerably improve upon this. In the future, it would also
be interesting to investigate the use of our technique for realtime rendering. This is still a challenging problem as the texture filtering hardware of current GPUs cannot be utilized as
it is possible for PCA based techniques. On the other hand,
the small memory footprint of our approach is especially for
this type of application a very important advantage.
7. Acknowledgments
This work was supported by the German Science Foundation
(DFG) under research grant KL 1142/4-1.
References
[AEB06] A HARON M., E LAD M., B RUCKSTEIN A.: K-SVD:
An algorithm for designing overcomplete dictionaries for sparse
representation. Signal Processing, IEEE Transactions on 54, 11
(Nov. 2006), 4311–4322. 2, 3, 5
[BK07] BADER B. W., KOLDA T. G.: Efficient matlab computations with sparse and factored tensors. SIAM J. Sci. Comput. 30,
1 (2007), 205–231. 6
[CDS98] C HEN S. S., D ONOHO D. L., S AUNDERS M. A.:
Atomic decomposition by basis pursuit. SIAM J. Sci. Comput.
20, 1 (1998), 33–61. 2
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

[DvGNK97] DANA K. J., VAN G INNEKEN B., NAYAR S. K.,
KOENDERINK J. J.: Reflectance and texture of real-world surfaces. In IEEE Conference on Computer Vision and Pattern
Recognition (1997), pp. 151–157. 1, 2
[EAH00] E NGAN K., A ASE S. O., H USØY J. H.: Multi-frame
compression: theory and design. Signal Process. 80, 10 (2000),
2121–2140. 2
[FKIS02] F URUKAWA R., K AWASAKI H., I KEUCHI K.,
S AKAUCHI M.: Appearance based object modeling using
texture database: acquisition, compression and rendering. In
Eurographics workshop on Rendering (2002), pp. 257–266. 1, 2
[GR97] G ORODNITSKY I., R AO B.: Sparse signal reconstruction
from limited data using focuss: a re-weighted minimum norm
algorithm. Signal Process. 45, 3 (1997), 600–616. 2
[KDMR∗ 03] K REUTZ -D ELGADO K., M URRAY J. F., R AO
B. D., E NGAN K., L EE T.-W., S EJNOWSKI T. J.: Dictionary
learning algorithms for sparse representation. Neural Comput.
15, 2 (2003), 349–396. 2
[KM03] KOUDELKA M. L., M AGDA S.: Acquisition, compression, and synthesis of bidirectional texture functions. In In Proc.
3rd Int. Workshop on Texture Analysis and Synthesis (2003),
pp. 59–64. 1, 2, 6
[LBAD∗ 06] L AWRENCE J., B EN -A RTZI A., D E C ORO C., M A TUSIK W., P FISTER H., R AMAMOORTHI R., RUSINKIEWICZ
S.: Inverse shade trees for non-parametric material representation and editing. In SIGGRAPH ’06 (2006), pp. 735–745. 2
[LHZ∗ 04] L IU X., H U Y., Z HANG J., T ONG X., G UO B., S HUM
H.-Y.: Synthesis and rendering of bidirectional texture functions
on arbitrary surfaces. IEEE Trans. on Vis. and Computer Graphics 10, 3 (2004), 278–289. 1, 2, 6
[LS00] L EWICKI M. S., S EJNOWSKI T. J.: Learning overcomplete representations. Neural Comp. 12, 2 (2000), 337–365. 2
[MMK03] M ÜLLER G., M ESETH J., K LEIN R.: Compression
and real-time rendering of measured btfs using local pca. In Vision, Modeling and Visualisation (2003), pp. 271–280. 1, 2, 6
[MMS∗ 04] M ÜLLER G., M ESETH J., S ATTLER M., S ARLETTE
R., K LEIN R.: Acquisition, synthesis and rendering of bidirectional texture functions. In Eurographics 2004, State of the Art
Reports (2004), pp. 69–94. 2
[NRH03] N G R., R AMAMOORTHI R., H ANRAHAN P.: Allfrequency shadows using non-linear wavelet lighting approximation. ACM Trans. Graph. 22, 3 (2003), 376–381. 2
[PRK93] PATI Y., R EZAIIFAR R., K RISHNAPRASAD P.: Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition. In 27th Asilomar Conference on Signals, Systems, and Computers (1993), pp. 40–44. 2

1188

R. Ruiters & R. Klein / BTF Compression via Sparse Tensor Decomposition
Original

Sparse Tensor Decomposition

PCA

N-Mode SVD

PCF

256 × 256 × 81 × 81

k1 = 9, k2 = 11

18 components

12 view, 8 light coefficients

16 cluster, 4 components

2.4GB

3.0MB, RMS: 0.033

3.0MB, RMS: 0.041

3.1MB, RMS: 0.049

3.6MB, RMS: 0.040

128 × 128 × 151 × 151

k1 = 7, k2 = 10

9 components

32 × 32 spatial, 40 view, 20 light

2 cluster, 5 components

2.1GB

1.6MB, RMS: 0.024

1.6MB, RMS: 0.034

1.6MB, RMS: 0.040

1.7MB, RMS: 0.036

Figure 4: Comparison of rendering quality at the same file size. Our sparse tensor factorization conserves especially high
frequency components of the dataset considerably better than the other compression techniques.
Original

Sparse Tensor Decomposition

PCA

PCF

346 × 335 × 151 × 151

k1 = 8, k2 = 8

11 components

4 cluster, 4 components

14.77GB

3.9MB, RMS: 0.0058

4.0MB, RMS: 0.0074

3.6MB, RMS: 0.0082

Figure 5: Results at a very high compression ratio (≈ 1 : 3900) for a large BTF dataset.
[RRK09] RUITERS R., RUMP M., K LEIN R.: Parallelized matrix
factorization for fast btf compression. In EGPGV (2009), pp. 25–
32. 4
[Rus98] RUSINKIEWICZ S.: A new change of variables for efficient BRDF representation. In Eurographics Workshop on Rendering (1998). 4
[RZE08] RUBINSTEIN R., Z IBULEVSKY M., E LAD M.: Efficient
Implementation of the K-SVD Algorithm using Batch Orthogonal
Matching Pursuit. Tech. Rep. CS-2008-08, Technion - Comp.
Science Department, 2008. 7
[SBLD03] S UYKENS F., B ERGE K. V., L AGAE A., D UTRÉ P.:
Interactive rendering with bidirectional texture functions. Comp.
Graph. Forum 22 (2003), 463–472. 2

[SSK03] S ATTLER M., S ARLETTE R., K LEIN R.: Efficient and
realistic visualization of cloth. EGSR (2003). 1, 2
[VT04] VASILESCU M. A. O., T ERZOPOULOS D.: Tensortextures: multilinear image-based rendering. ACM Trans. Graph.
23, 3 (2004), 336–342. 1, 2, 4, 5
[WWS∗ 05] WANG H., W U Q., S HI L., Y U Y., A HUJA N.: Outof-core tensor approximation of multi-dimensional matrices of
visual data. ACM Trans. Graph. 24, 3 (2005), 527–535. 1, 2, 4, 6
[WXC∗ 08] W U Q., X IA T., C HEN C., L IN H.-Y. S., WANG H.,
Y U Y.: Hierarchical tensor approximation of multi-dimensional
visual data. IEEE Trans. on Visualization and Computer Graphics 14, 1 (2008), 186–199. 1, 2

c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

