Volume 28 (2009), Number 2

EUROGRAPHICS 2009 / P. Dutré and M. Stamminger
(Guest Editors)

Learning good views through intelligent galleries
Thales Vieira1 , Alex Bordignon1 , Adelailson Peixoto2 , Geovan Tavares1 , Hélio Lopes1 , Luiz Velho3 and Thomas Lewiner1
1 Matmidia

– PUC, Rio de Janeiro — Brazil,

2 CPMAT

– UFAL, Maceió — Brazil,

3 Visgraf

– IMPA, Rio de Janeiro — Brazil.

Abstract
The definition of a good view of a 3D scene is highly subjective and strongly depends on both the scene content
and the 3D application. Usually, camera placement is performed directly by the user, and that task may be laborious. Existing automatic virtual cameras guide the user by optimizing a single rule, e.g. maximizing the visible
silhouette or the projected area. However, the use of a static pre-defined rule may fail in respecting the user’s
subjective understanding of the scene. This work introduces intelligent design galleries, a learning approach for
subjective problems such as the camera placement. The interaction of the user with a design gallery teaches a
statistical learning machine. The trained machine can then imitate the user, either by pre-selecting good views
or by automatically placing the camera. The learning process relies on a Support Vector Machines for classifying views from a collection of descriptors, ranging from 2D image quality to 3D features visibility. Experiments
of the automatic camera placement demonstrate that the proposed technique is efficient and handles scenes with
occlusion and high depth complexities. This work also includes user validations of the intelligent gallery interface.
Categories and Subject Descriptors (according to ACM CCS): Artificial Intelligence [I.2.6]: Learning— Analogies;
Computer Graphics [I.3.6]: Methodology and Techniques— Interaction techniques.

1. Introduction
Although we live in a three-dimensional world, our perception of physical realities remains mediated primarily by our
visual senses, which are essentially bi-dimensional. Such dimensionality reduction usually implies in a loss of information. Humans deal with this sensorial limitation by resorting
to knowledge and contextual data. A particular experience
that we acquire as we explore the world is to position ourselves at vantage points which reveal the intrinsic 3D structure of objects of interest. Photographers develop this knowledge, learning where to place a camera to take a perfect shot.
With the advent of computers and advances in graphics technologies, we work more and more with threedimensional data using only two-dimensional visual interfaces, e.g. positioning virtual 3D camera through 2D interaction. In this context, the photographer task of producing a
good view can be laborious for both 3D specialists and inexperienced users: On one side, engineers and scientists using high-end graphics for CAD/CAM, medical imaging or
simulations typically need to render large collections or sequences of 3D scenes, guaranteeing that each shot captures
the relevant details for their specific applications. This repetitive task unfortunately requires advanced knowledge. On
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

the other side, 3D content has already come to the masses,
pledging for more intuitive interfaces. A great help for those
laborious tasks is a tool for the automatic generation of good
views, i.e., 2D images that show 3D content with minimal
loss, revealing the information desired by the user.
However, automatic viewpoint selection is a very hard
task for two main reasons: first, there are many factors which
must be taken into account to generate a good view, ranging
from perceptual issues to geometric analysis of the objects in
the scene; second, the “best” view heavily depends on what
we want to see, and thus, it is both a user and applicationdependent task.
In this paper we face the challenge of producing good
2D views by computer using the same approach as humans:
learning! To do so, we introduce intelligent galleries (see
Figure 1), an interface that learns from the user and evolves
with continuous usage. It incorporates learning into design
galleries, and can generate the desired views automatically
or semi-automatically.
Related Work Automatic camera placement has received
a constant interest over the past decades [CON08], with
mainly two strategies according to [CMN∗ 05]: visibility
maximization and view descriptor optimization.

718

T. Vieira & A. Bordignon & A Peixoto & G Tavares & H Lopes & L Velho & T Lewiner / Learning good views. . .

Figure 1: Overview of the intelligent gallery interface (from left to right): 1- The scene is displayed to the user as a gallery of
views. 2 - From the user subjective selection of good and bad views, the machine learns user’s preferences. 3- Upon validation,
a new gallery is generated by genetic reproduction of the selected views. 4- Then, the machine automatically selects and orders
the views in the gallery from its previous training. 5- The user can correct the automatic selection, repeating the reproduction
until converging to a satisfactory final view.

The pioneering work of Kamada and Kawai [KK88] computes good views for 3D objects by minimizing the number
of degenerated faces in orthographic projection. This simple
criterion has been enhanced to entail a larger amount of visible three-dimensional details and extended for perspective
projection in the works of Barral et al. [BPD00] and Gómez
et al. [GHST01]. Then, two-dimensional visibility criteria
have been introduced in Plemenos and Benayada [PB96],
maximizing the non-occluded projected area. Extending that
work, Vázquez et al. [VFSH01] propose a measure called
viewpoint entropy, defining the goodness of a view as the
amount of information it provides to the observer. This entropy considers both the projected area and the number of
visible faces from the 3D model normal dispersion. However
for terrains, where all the normals point in similar directions,
this entropy loses its pertinence. To workaround this, Stoev
and Straßer [SS02b] propose to maximize an average of the
entropy and the maximal scene depth. More complex definitions of distinctive regions have been proposed based on
saliency learning [LN07] and direct database search [SF08]
Similar descriptors have been used to generate a collection
of representative views for a single scene, based on surface
visibility [FCOL99] or on the notion of stable saliency-based
views [YSY∗ 06]. More view collections techniques are related to the next best view problem [SB04,HDKG08]. However, this work focuses on the single best view problem.
More recently, Lee et al. [LVJ05] introduce mesh saliencies as a multiscale curvature-based measure of regional importance. As an application, they define good views by maximizing the total saliency of visible regions, with a heuristic gradient-descent optimization. Polonsky et al. [PPB∗ 05]
also measure the goodness of view by introducing 2D and
3D view descriptors such as projected surface area, curvature, silhouette and topological complexity. Different good
views are obtained by maximizing these descriptors individually. Then, Sokolov et al. [SP08, ST06] define good views
minimizing a distance between the visible area distribution
of the polygons in the image and the corresponding distribution of areas in the 3D scene. Podolak et al. [PSG∗ 06]

propose to use symmetry to define good projections and orientations of 3D scenes. Of particular attention to us, the authors of [PPB∗ 05] conclude that combinations of such descriptors could lead to better results. Our approach deduces
such combinations from a learning process.
Those techniques have been further derived using art and
design criteria, which is particularly relevant in animation
contexts. Blanz et al. [BTBV99] define factors that influence
human preferred views for 3D objects, pointing out that the
relevance of each factor depends on the task, the object geometry and the object familiarity. Gooch et al. [GRMS01]
developed a computer system based on these factors and
heuristics used by artists. Beyond static scene criteria, cinematography constraints have been proposed to generate
good camera trajectories [BMBT00, Bar06, DZ94, HO00,
HCS96]. These criteria would be very useful to guide the
user in choosing good views and producing more elegant
animations, although we focus here on direct learning from
user’s experience rather than pre-established design rules.
Design galleries are described in the work of Marks et
al. [MAB∗ 97] as an efficient tool for assisting the user for
highly non-linear problems with subjective goals. We extend
here this proposal into intelligent galleries, which incorporates learning into design galleries. This allows assisting the
user by incorporating its own experience.
Contributions This work proposes to learn good views of
a 3D scene from user’s experience. As opposed to visibility or single view descriptor optimization approaches, it allows combining several criteria ranging from 3D scene content to 2D image arrangement. Our learning approach turns
this combination context-dependent. In particular, it does not
rely on pre-defined criteria, which are hard to define for such
subjective problem as camera placement.
We propose here an intelligent gallery interface, which
incorporates learning into design galleries. This leads to a
simple interface that continuously learns from the user. The
learning machine can then imitate the user to spare him from
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

719

T. Vieira & A. Bordignon & A Peixoto & G Tavares & H Lopes & L Velho & T Lewiner / Learning good views. . .

repetitive camera placement. The statistical learning is based
on Support Vector Machines, and we further analyze its dependency on each view descriptor. From this analysis, we
automatically adjust its parameters for each training set. Preliminary results show the effectiveness of the proposed interface in static and dynamic contexts.
2. Technical Overview
The definition of good camera from the user’s experience relies on three main ingredients: an adapted statistical learning
technique described in Section 3; an efficient computation
of image- and object-space descriptors detailed in Section 4;
and an intelligent gallery interface introduced in Section 5.
Descriptors. For each camera around an input 3D scene, a
set of descriptors of the rendered view is computed. These
descriptors either reflect the 2D image quality — e.g., the
proportion of the image covered by the scene, the complexity
of its silhouette curve or the alignment of the scene with the
image directions — or the visibility of 3D features — e.g.,
mesh saliency, connected components or velocity for fluid
simulations. We propose an efficient way to compute these
descriptors, allowing interactive performances.
Learning. The user’s selections in the intelligent gallery
serve as training set for the learning machine. More precisely, each view is represented as a vector v containing the
descriptors values. Each selected view is associated to a class
s = ±1, depending whether the user marked it as a good or
bad view. This way, we model the user selection as a discrete
mapping v → f (v) = s. We use an SVM formulation to incrementally build a function v → fˆ (v) = sˆ that statistically
approximates the user selection. We analyze this function fˆ
and adjust its parameters for each specific training.
Intelligent Galleries. The gallery initially collects views
from a uniform sampling of the camera position space. A
previously trained learning machine marks some of those
views good or bad, which the user may correct. Upon validation a new gallery is generated by genetic reproduction of
the good views, where the genes are the coordinates of the
camera: position and orientation. This correction/validation
process is repeated until the user obtains his best view of
the 3D model, or until the genetic optimization converges
according to the learning machine. The views in the gallery
are ordered according to the learning machine, guiding the
user to quickly select subjective good views.
With this formulation, the machine interacts with the
gallery interface the same way the user would do and learns
from his corrections. This can provide a fully automatic camera placement, which suits for repetitive camera placement
such as animation frames. The same framework can work in
semi-automatic mode, where the user controls and occasionally corrects the machine selections. Finally, it can be used
only to guide the user, by proposing at privileged gallery locations the machine’s best views.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

3. Statistical Learning
Our intelligent gallery learns continuously from the user’s
choices. To implement the learning, we use a derivation of
the Support Vector Machine (SVM) binary classifier, since
it provides a non-linear classification and copes well with
small training sets. SVM received a lot of attention in the
machine learning community since it is optimal in the sense
of the VC statistical learning theory [Vap00]. We refer the
reader to the book of Smola and Schölkopf [SS02a] for
a complete introduction to SVM techniques. This section
briefly describes some basics of SVM and how we derived it
for ordering views from a binary training. We further introduce a simple analysis of the SVM results, and an automatic
parameter adjustment based on this analysis.
SVM-based learning. During training, the SVM learning
machine receives as input set of pairs {(v1 , s1 ) , (v2 , s2 ) . . .},
each pair corresponding to a view submitted to user selection. The vector vi ∈ Rk contains the view descriptors
and the binary number si = ±1 codes if the user marked
the view as good or bad. The SVM produces a classifier
v → sign fˆ (v) = sˆ that imitates the user’s behavior. This
classifying function fˆ corresponds to a linear function, but
in a high or infinite-dimensional space F:
fˆ (v) = ∑ j α j s j ϕ v j , ϕ (v) + b

.

(1)

The function ϕ : Rk → F maps the descriptors space nonlinearly to the space F, leading to a non-linear classifier fˆ as
shown in Figure 2. In this work, we used a Gaussian kernel,
v −v

2

i.e. ϕ (v1 ) , ϕ (v2 ) = exp − 22σ21
with penalty weight
C = 1 for outliers. Note that this classifying function uses all
the components of the optimized vectors vi at once, i.e. it
combines all the different view descriptors in one function.
The SVM classifier fˆ is optimized for its sign to approximate the user marks s. However, we want to automatically
order the views from the best one to the worst, and we thus
need a continuous result. To do so, we directly use the function fˆ, interpreting that a high positive value of fˆ(v) means
that the view is good with high confidence.
Classifying function analysis. Equation (1) defines the
classifier fˆ from the training set, and not directly from the
descriptors. The dependency to a particular descriptor (i.e.
a component of v) is thus hard to interpret directly. We can
observe this dependency visually, plotting the classification
fˆ versus the values of a particular descriptor on the training
set (see Figure 3). Ideally, that plot for an influent descriptor would present a strong dependency (see Figure 3 (top)),
while a less influent descriptor would have a random horizontal distribution of values (see Figure 3 (bottom)).
We would like to relate the non-linear function ( fˆ) separately with each descriptor. However, non-linear analysis
such as Kernel PCA [SSM99] is not appropriate, since it

720

T. Vieira & A. Bordignon & A Peixoto & G Tavares & H Lopes & L Velho & T Lewiner / Learning good views. . .
1.5

1.8

Foreground Visibility

1

high influence

1.6
1.4

0.5

1.2

0
-0.5

1

-1

0.8
0.6

-1.5

0.4

-2

High Curvature

0.2

-2.5
0.0

0.2

0.4

0.6

0.8

1.0

Medium Curvature

0
0

0.5 Curvature
1
Low

1.5

2

2.5

3

3.5

4

4.5

5

High Saliency

Medium Saliency

1.5

1.8

Features 2D Positioning

1.4

Foreground Alignment

1.2

0.5

Foreground Visibility

1

0

high influence

Low Saliency

1.6

1

0.8
0.6

-0.5

0.4

-1

0.2

-1.5

0

0.0

Figure 2: Good camera positions (in
red) from training the side view of the
cow model (Figure 7). The estimation
of the good cameras emphasize the
non-linearity of the classifier.

Medium Saliency

0.2

0.4

0.6

0.8

Figure 3: SVM prediction fˆ vs descriptor value, for the training of Figure 2: the saliency (bottom) has a low
correlation with the fˆ, contrarily to
the foreground visibility (top).

would find relations with ϕ fˆ(x) . To avoid computationally expensive feature analysis or dimension reductions, we
adopt a heuristic approach: linearizing fˆ close to the average of the descriptors. This leads to a linear approximation
of the non-linear dependency between a given descriptor and
the classification. If the variance around the linear approximation is small, the dependency is well characterized by the
slope of the regression line. If the variance is high, the linearization may not be valid. This leads to a characterization
of the descriptor influence by the regression slope τ and the
variance η around it as τ(2 − η) (see Figure 4).
Automatic parameters adjustments. The adjustment of
the parameter σ used in the SVM classifier is done
in order to maximize the influence of the parameters
(∑d∈Descriptors τd (2 − ηd )), turning the learning more robust
by forcing to use more descriptors. To do so, we use a bisection method on the values of σ to maximize the number of
descriptors on the top right of the plots (see Figure 4). This
process runs in less than a few seconds since the training set
is fixed and small.
4. View Descriptors
In this section, we detail the collection of image-space (2D)
and object-space (3D) descriptors used to qualify a view.
Their values on a given view define the vector v, which
serves as input for learning. These view descriptors correspond to commonsense good views, and their combination
obtained by the classifier may thus help characterizing subjective good views. Several of the following descriptors have
already been used for camera positioning [Pas02, PKS∗ 03,
PPB∗ 05, LVJ05]. We propose variations of those and further
add application-specific descriptors such as velocity for simulation and groups in character animation. We further pro-

1.0

0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

Figure 4: Optimization of kernel parameter σ for the training of Figure 9
(2−variance vs. correlation): σ = 8
(bottom) maximizes the descriptors’
influences, compared to σ = 1 (top).

pose an efficient way to compute those descriptors for reaching interactive rates. The view descriptors refer either to the
object-space, counting how much of geometric features is
visible from the given camera, or to the image-space, evaluating the information contained in the rendered image (see
Figure 5).
Efficient Descriptor Computation Each object-space descriptor is a number associated to a feature, which is computed during pre-processing. For example, high curvature
is a 3D feature of the mesh, and the associated descriptor
counts how much of high curvature parts are visible for a
given view. To do so, each vertex of the 3D scene is associated to a 32-bits RGBA color that encodes the 3D features
present at that vertex. For example, if the curvature of a vertex is high and if the first bit of the red color encodes the high
curvature feature, then the color associated to that vertex will
have an odd red component. For a given camera, the scene is
rendered with those colors. Note that several passes can be
performed if the feature coding requires more than 32 bits.
The object-space descriptor associated to a feature is then
efficiently computed as the number of pixels in the rendered
image whose RBGA color encodes that feature. This counting and the computation of the 2D descriptors are done by
traversing the color buffer.

4.1. Object-space descriptors
In the current implementation of this work, we compute the
following 3D features: mean curvature, saliency, connected
components and visible patches (see Table 1 and Figure 5).
For specific applications such as fluid simulation or animation, other properties such as velocity or human character
groups are added to the descriptors.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Vieira & A. Bordignon & A Peixoto & G Tavares & H Lopes & L Velho & T Lewiner / Learning good views. . .

721

Figure 5: The view descriptors help the learning machine in characterizing good views. From left to right: mean curvature
(black to white scale), patches visibility (one color per patch), alignment in image plane, silhouette complexity and character
groups in an animation context. The top row shows views with high values of each descriptor, and corresponds to commonsense
good views, as opposed to the bottom row.

Geometric features. As geometric properties, we compute the mean curvature using the work of Meyer et
al. [MDSB02] and the saliency using the work of Lee et
al. [LVJ05]. For fluid simulation we add a 3D property corresponding to the magnitude of the velocity vector. Since
those 3D properties have very different scales depending on
the scene observed, we normalize them by equalizing their
histogram on the first gallery and scaling their values to the
unit interval. Each of these properties is then associated to
three features: low values (0 to 13 ), medium values ( 13 to 23 )
and high values ( 23 to 1).
Parts visibility. We use different partitions of the 3D scene.
Connected components characterize the distinct scene objects, while patches of approximately equal area describe
how much of the whole scene is visible (see Figure 5).
The patches are computed with a greedy strategy: the set of
patches is initialized to the set of triangles and the smallest
ones are merged by union-find operations. Descriptors are
obtained by counting the visible 2D patch size relative to the
original 3D area, and also the 2D component size multiplied

property
curvature

saliency
patches
connected
components

feature

descriptor

low mesh curvature
medium mesh curvature
high mesh curvature
low mesh saliency
medium mesh saliency
high mesh saliency
patch identifier
component identifier

Σ visible low curv.
Σ visible medium curv.
Σ visible high curv.
Σ visible low sal.
Σ visible medium sal.
Σ visible high sal.
Σ visible patches
Σ visible 2D area /
/ comp. 3D area
Σ visible 2D area ×
× comp. 3D area

component size

Table 1: Generic object-space descriptors.

c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

by the 3D area, the latter emphasizing big components. For
animation, the character groups induce another partition of
the scene. For normalization purposes, the set of parts visibility descriptors counts the visible pixels of each connected
component, patch or character group, divided by its 3D area.
4.2. Image-space descriptors
For image-space descriptors, we use the area of the projected
scene, the complexity of the scene silhouette, the distribution
of 3D curvatures in the image plane and the alignment of the
projected connected components with the image’s rectangular border (see Figure 5). These image-space descriptors are
computed directly on the rendered image, where we identify
the 3D connected components in the foreground from the
color coding of the 3D features. We extract the foreground’s
contours by a simple continuation method.
The complexity of the silhouette is defined as the total
integral of its curvature. The distribution of the 3D curvatures is computed by the average distance of the projected
vertices to the center of the image, weighted by their mean
curvature. A similar descriptor is obtained by replacing the
curvature by the component size. Finally, the alignment of
the connected components is obtained by linear regression
from the projected vertices’ position. This descriptor is the
average of the regression lines inclinations, weighted by the
number of vertices of each component.
5. Intelligent Gallery Interface
In this work, we introduce intelligent galleries, which extend design gallery interfaces [MAB∗ 97]. This gallery interface is populated by a genetic reproduction strategy, and
incorporates the learning machine described in Section 3. In
the gallery, the user marks good and bad views by a simple
click, and the learning machine continuously evolves with
those selections. The main actions can be reduced to four

722

T. Vieira & A. Bordignon & A Peixoto & G Tavares & H Lopes & L Velho & T Lewiner / Learning good views. . .

Figure 6: The intelligent gallery interface allows the user to teach and correct a learning machine while quickly reaching
his best view. It has mainly four actions: 1- open a 3D scene, which initializes the gallery; 2- automatically select and order
good and bad views using the current training; 3- correct or validate the selection and generate a finer gallery by genetic
reproduction; 4- use a particular view for final rendering. The training set accumulates all the user’s selections.

options (see Figure 6): 1 - load a new model; 2 - automatically select and order best views according to the current
learning machine; 3 - let the user correct and validate the
current selection and produce a new gallery from it; 4 - fine
render a final view. The user can also use advanced features,
such as saving and loading learning machines, load sequence
of models for automatic camera placement, etc. This section
details how the galleries are initialized and reproduced and
how the user interacts with the learning machine.
Initial population. When the user loads a model, the first
gallery must offer him a good coverage of the 3D model. To
do so with a reduced number of views in the gallery, we restrict the camera positions and orientation to a subset of the
whole possible camera positions and orientation R3 × S2 .
We distinguish terrain-like scenes, such as fluid simulation
or character animation, from single objects, such as animal
or vehicles models. For terrains, the camera positions are restricted to a parallelepiped above the terrain and the orientation are chosen inside a cone of 45◦ aperture. For single
objects, the cameras are oriented towards the center of the
object, and their positions are constrained to the surface of
a sphere around the object. The radius of the sphere is set
to twice the diagonal of the object’s bounding box. In both
cases, the cameras of the initial gallery are positioned on a
regular lattice of these subsets of R3 × S2 . The perspective
angle of the camera was fixed to 45◦ .
Interactive genetic optimization. The user can correct the
automatically selected good and bad views in the gallery
by a simple click. Upon validation, a new gallery is generated by genetic reproduction of the selected good views.
More precisely, each camera position and orientation is represented by a chromosome vector. This chromosome has 5
components for terrain-like models: 3 for the position and 2
for the orientation; and 2 components for single objects for
the spherical parameterization of the camera positions. The
cameras of the selected views are reproduced by generating random convex combinations of those chromosome vectors. Each selected camera randomly reproduces with four

other cameras. Our genetic reproduction thus generates a
new gallery of twice as much camera as previously selected
good views. It is repeated until either the user gets satisfied
or the learning machine’s choices converges.
Learning incorporation. The learning process tries to detect the views that the user would select. It must be trained
first, so at least the first selection of the gallery must be manual or obtained by a pre-defined training. Then, at each selection validation, the views of the gallery are inserted into
the training set and classified with s = ±1, depending on
whether the user marked the view as good or bad. The training is continuous, and can also be accumulated from different scenes. After each validation, the learning machine produces a new classifying function from the training set. Since
the total number of views in the gallery is small, the SVMbased optimization is instantaneous, so that the user can use
the automatic selection at any time. Finally, we guide the
user by putting at the start of the gallery (top left) the best
views according to the classifying function.

Figure 7: Automatic placement (no user intervention) from
training for side views of the cow: the classifier illustrated
in Figure 2 succeeds in automatically reproducing this best
view from only one training model.

6. Results
The proposed method for learning good views from user’s
experience has been tested in three different contexts: 1- automatically reproduce user’s experience on similar models;
2- check the interface’s efficiency from recording inexperienced user interaction; 3- generate automatic camera trajectories for 3D animations from user’s training on few frames.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Vieira & A. Bordignon & A Peixoto & G Tavares & H Lopes & L Velho & T Lewiner / Learning good views. . .

723

Figure 9: A continuous training, started on David’s head to
reach the left view, and completed on the Max Plank’s head.

Figure 8: Selections of best view of a fluid simulation from
a designer (top left) and from a fluid researcher (bottom
left) point of view. Each learning machine leads to automatic
views of other frames of the simulation, according to the respective user’s choice (right).
We used three groups of models: static usual CG models
(see Figures 7, 9 and 10), challenging scenes such as knots
(see Figure 11) or high depth complex scene (see Figure 12),
and frames of 3D animations (see Figures 8 and 14). The
fluid simulation and character animation were generated using Blender [Ble] and each contains one per-vertex extra information: the fluid velocity and the character’ identifier.
6.1. Automatic placement
Reproducibility. We first check that the learning strategy
combined with 3D and 2D descriptors is able to reproduce
simple definitions of best views. To do so, we used the cow
as a prototype for the animal models and navigated through
the gallery to obtain a side view of it (see Figure 7). Then,
we applied this training to the other animals in automatic
mode (no user intervention). The results are satisfactory even
for the horse, which has a slightly different pose. From the
analysis of the learning described in Section 3, the machine
learned mainly from the foreground visibility and saliency.
We further checked the stability of the learning by counting the false positive and false negative when removing randomly 20% of the training set and testing on the removed
samples (out-of-sample test). The average false positive for
10 random removals is 2.7% and 3.0% of false positive for
the training of Figure 9. For the training of Figure 7, there
are 5.7% false positive and 0.2% false negative.
Subjectivity. We then check that our interface takes into account the user’s subjectivity (see Figure 8). To do so, we
submitted one frame of the fluid simulation to two different users: a graphic designer, letting him free to choose his
best view and a fluid expert, asking him to focus on the fluid
flow features. The resulting views and selection sequences
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Figure 10: Comparison of our automatic view selection
(right column) from the training of Figure 9 with previous techniques: area maximization [PPB∗ 05] (top left), silhouette length maximization [PPB∗ 05] (middle left) and
saliency-based camera positioning [LVJ05] (bottom left).
obtained through the gallery interface were different. We
then applied the training obtained from each user on another
frame of the simulation and observed that the learning succeeded in selecting new views coherent with each training.
Comparison. We illustrate some result of our technique and
of previous works, from Polonsky et al. [PPB∗ 05] and Lee
et al. [LVJ05] in Figure 10. We used a unique training for
all sets, obtained for the front view of the David’s head and
Max Planck’s head (see Figure 9). Using this training in the
intelligent galleries without user intervention leads to good
automatic view selection. As compared to single criterion
decisions [PPB∗ 05, LVJ05], descriptor combinations fits to
a wider range of models.

724

T. Vieira & A. Bordignon & A Peixoto & G Tavares & H Lopes & L Velho & T Lewiner / Learning good views. . .

Figure 11: Training on a single trefoil knot (left) leads to a coherent classification of good views for a more complex knot
(decreasing view quality from left to right).

6.2. Intelligent interface efficiency
We proposed the intelligent gallery interface to users with no
particular experience in camera placement. The users chose
themselves which models to work on. We asked them to select a good view of the model through the intelligent gallery,
and then to obtain a similar view through a classical interface
(i.e. Blender). The learning curves and timings (see Table 2)
show that, after one or two models, the intelligent gallery
accelerates the user’s camera placement.

processing + interaction time (sec)

Challenging scenes. We further test our technique on more
challenging scenes where simple criterion would be hard
to determine explicitly. For example, 3D embeddings of
knots are usually challenging to render, while our intelligent
gallery is able to select clear views of complex knots from a
single training on the trefoil knot (see Figure 11).
Our technique handles nicely high depth scene and occlusion, for example when looking across a forest for the
front of an occluded animal (see Figure 12): a single training even on a low density forest leads to good view selection
of the animal front in more dense forests. Here, the occlusion is forced by maintaining the camera in a 2D plane. The
combination of occlusion with the front constraint is a complex criterion, which is correctly interpreted by our intelligent gallery.

User C

40

intelligent gallery
blender

30

20

10

0
triceratops

Model
cow (0)
doberman (1)
cat (2)
dinosaur (3)
cheetah (4)
knot 3.1c (0)
knot 7.7c (1)
knot 6.13c (2)
knot 138k (3)
triceratops (0)
dragon (1)
octopus (2)
mosquito (3)
woman (4)
squirrel (5)

Figure 12: Training on a high depth scene to catch the occluded dinosaur front (top). The automatic selection still
catches the dinosaur’s front, although the new random forest
contains more trees (middle and bottom).

# of
Views
60 (4)
44 (2)
20 (0)
20 (0)
20 (0)
36 (2)
20 (0)
20 (0)
20 (0)
28 (1)
24 (1)
24 (1)
20 (0)
20 (0)
20 (0)

dragon

octopus
mosquito
model in order

Preproc
2.140
4.218
8.875
17.172
4.703
6.890
6.672
3.375
3.735
2.313
21.328
9.531
5.141
4.359
5.281

Proc
8.280
6.595
3.281
3.469
3.234
4.671
3.235
3.250
3.250
4.906
3.250
4.375
3.234
3.297
3.328

woman

Inter-act
73.8
50.5
8.5
4.7
6.3
56.1
10.0
11.1
7.1
35.9
32.7
28.9
11.6
10.6
7.6

squirrel

Com-pare
41.3
40.9
42.6
33.8
41.3
29.2
22.5
26.6
21.9
30.7
27.4
23.6
29.4
23.9
26.7

Table 2: Efficiency of our intelligent gallery interface. The
total number of views generated by the galleries is completed by the number of galleries generated. The timings are
expressed in seconds, and the comparison was done using a
standard 3D interface [Ble] (crosses). The users adapted to
the interface on two or three models (dots) and use it faster
than traditional ones.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Vieira & A. Bordignon & A Peixoto & G Tavares & H Lopes & L Velho & T Lewiner / Learning good views. . .

725

6.3. Application to 3D video generation
We further test our method on 3D animation sequences,
where the camera placement must be repeated for each
frame. We trained a few frames of the sequence and the intelligent gallery then automatically selected the best views
for the key frames of the animation. For example in the 30
seconds character animation of Figure 14, we trained the first
and last frame. We then applied it on key frames every 2 seconds. For testing purposes, we did not include any advanced
constraints for the camera trajectory, such as respecting cinematographic rules. We only avoid the camera to oscillate by
picking across key frames, among the automatically selected
good views, the one that minimize the image displacement.
Between key frames, the cameras are interpolated by cubic
splines, using standard tools of Blender [Ble]. A similar test
has been performed on the fluid simulation of Figure 8, training on four frames adding the velocity magnitude and setting
the key frame interval to 1.6 seconds (see the video).
6.4. Limitations
The proposed interface has mainly two limitations. First, the
intelligent gallery is designed to learn from the user interaction on a small set of models and expects the user to apply on
models with similar objectives, e.g. a training set for a side
view should not be applied to obtain a front view. For example on Figure 13, using the training of Figure 9 on the knot of
Figure 11 or vice-versa leads to undesired views. Performing
an out-of-sample test of the training of Figure 9 but applying
on the samples of Figure 7 leads to 4.7% false positive and
52.0% of false negative: the front views are discarded in the
cow model, while being praised in the other training. The reverse experiment leads to 8.5% false positive and 53.5% of
false negative.
The second is related to the number of parameters (k)
of the virtual camera. For the genetic reproduction to be
able to generate all the possible cameras, it needs at least
to have an initial population of size 2k (the corner of the
hypercube), and much more in practice. A complete camera with position, direction, perspective angle, viewport. . .
would require an initial gallery too large for a reasonable interaction. To overcome this problem, new cameras need to
be inserted independently of the genetic reproduction, e.g.
sampling the view space [PPB∗ 05] or using a next-best-view
approach [SB04, HDKG08].
7. Next Steps
We proposed a new technique for automatic and semiautomatic determination of good views that learns from the
user’s subjective criteria. The introduction of intelligent galleries allows a smooth interface where the user incrementally teaches the machine while achieving his own best view.
Unlike usual techniques, our learning approach can automatically obtain different good views of the same scene depending on the user and the application. We further used the proposed technique for camera placement in 3D sequences with
simple trajectory restrictions, obtaining nice animations.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Figure 13: When transferring a training across very different applications, here from the training of Figure 9 to the
on of Figure 11 or vice-versa, the intelligent gallery may not
weight correctly the model features.

The actual implementation of the technique used a restricted set of descriptors for static 3D scenes. A greater set
of descriptors, including existing ones from design research,
would improve the learning process. Our framework can incorporate new descriptors with almost no alteration of speed
since the classification is instantaneous. As further research,
the use of intelligent galleries may improve other applications of design galleries, such as light sources positioning,
transfer function design or isosurface selection. A complete
user study will also be performed.
Acknowledgements The authors wish to thank Fabiano
Petronetto for constructive discussions, Eduardo Telles, Debora
Lima, Bernardo Ribeiro, Clarissa Marques and Afonso Paiva for
their help with the interface, and Marcos Lage and Rener Castro for
help with the coding. This work was supported in part by a grant
from the CNPq (Universal MCT/CNPq, Productivity scholarship,
Instituto Nacional de Ciência e Tecnologia), FAPERJ (Jovem
Cientista) and CAPES.

References
[Bar06] BARES W. H.: A Photographic Composition Assistant
for Intelligent Virtual 3D Camera Systems. In Smart Graphics
(2006), pp. 172–183.
[Ble]

B LENDER: Blender foundation. www.blender.org.

[BMBT00] BARES W., M C D ERMOTT S., B OUDREAUX C.,
T HAINIMIT S.: Virtual 3D camera composition from frame constraints. In Multimedia (2000), ACM, pp. 177–186.
[BPD00] BARRAL P., P LEMENOS D., D ORME G.: Scene understanding techniques using a virtual camera. In Eurographics
(2000). Short paper.
[BTBV99] B LANZ V., TARR M. J., B ÜLTHOFF H. H., V ETTER
T.: What object attributes determine canonical views. Perception
28, 5 (1999), 575–600.
[CMN∗ 05] C HRISTIE M., M ACHAP R., N ORMAND J.-M.,
O LIVIER P., P ICKERING J.: Virtual Camera Planning: A Survey. In Smart Graphics (2005), pp. 40–52.
[CON08] C HRISTIE M., O LIVIER P., N ORMAND J.-M.: Camera
Control in Computer Graphics. Computer Graphics Forum 27, 7
(2008).
[DZ94] D RUCKER S. M., Z ELTZER D.: Intelligent Camera Control in a Virtual Environment. In Graphics Interface (1994),
pp. 190–199.

726

T. Vieira & A. Bordignon & A Peixoto & G Tavares & H Lopes & L Velho & T Lewiner / Learning good views. . .

Figure 14: Training of a battle scene from only two frames: the views of the first frame (top left), where the character groups
are separated, are selected as good cameras; while the views of the last frame (top right) where the groups are equally mixed,
are also selected as good cameras. With this only training, the cameras for the key frames of the animation are automatically
positioned using our method.
[FCOL99] F LEISHMAN S., C OHEN -O R D., L ISCHINSKI D.:
Automatic Camera Placement for Image-Based Modeling. In Pacific Graphics (1999), IEEE, pp. 12–20.
[GHST01] G ÓMEZ F., H URTADO F., S ELLARÈS J. A., T OUS SAINT G.: Nice Perspective Projections. Visual Communication
and Image Representation 12, 4 (2001), 387–400.
[GRMS01] G OOCH B., R EINHARD E., M OULDING C.,
S HIRLEY P.: Artistic Composition for Image Creation. In
Rendering Techniques (2001), Springer, pp. 83–88.
[HCS96] H E L.-W., C OHEN M. F., S ALESIN D. H.: The virtual cinematographer: a paradigm for automatic real-time camera
control and directing. In Siggraph (1996), ACM, pp. 217–224.
[HDKG08] H ACHET M., D ECLE F., K NODEL S., G UITTON P.:
Navidget for Easy 3D Camera Positioning from 2D Inputs. In 3D
User Interfaces (2008), IEEE, pp. 83–89.
[HO00] H ALPER N., O LIVER P.: CamPlan: A Camera Planning
Agent. In Smart Graphics (2000).
[KK88] K AMADA T., K AWAI S.: A simple method for computing
general position in displaying three-dimensional objects. Computer Vision, Graphics, Image Processing 41, 1 (1988), 43–56.
[LN07] L AGA H., NAKAJIMA M.: Supervised Learning of
Salient 2D Views of 3D Models. In Nicograph (2007).
[LVJ05] L EE C. H., VARSHNEY A., JACOBS D. W.:
saliency. In Siggraph (2005), ACM.

Mesh

[MAB∗ 97] M ARKS J., A NDALMAN B., B EARDSLEY P. A.,
F REEMAN W., G IBSON S., H ODGINS J., K ANG T., M IRTICH
B., P FISTER H., RUML W., RYALL K., S EIMS J., S HIEBER
S. M.: Design galleries: a general approach to setting parameters for computer graphics and animation. In Siggraph (1997),
ACM, pp. 389–400.
[MDSB02] M EYER M., D ESBRUN M., S CHRÖDER P., BARR
A.: Discrete differential–geometry operators for triangulated 2–
manifolds. In VisMathematics (2002), pp. 35–57.
[Pas02] PASTOR O. E. M.: Visibility Preprocessing Using Spherical Sampling of Polygonal Patches, 2002.
[PB96] P LEMENOS D., B ENAYADA M.: Intelligent Display in
Scene Modeling: New Techniques to Automatically Compute
Good Views. In GraphiCon (1996).

[PKS∗ 03] PAGE D. L., KOSCHAN A. F., S UKUMAR S. R.,
ROUI -A BIDI B., A BIDI M. A.: Shape analysis algorithm based
on information theory. In Image Processing (2003), vol. 1,
pp. 229–232.
[PPB∗ 05] P OLONSKY O., PATANÈ G., B IASOTTI S., G OTSMAN
C., S PAGNUOLO M.: What’s in an image? The Visual Computer
21, 8–10 (2005), 840–847.
[PSG∗ 06] P ODOLAK J., S HILANE P., G OLOVINSKIY A.,
RUSINKIEWICZ S., F UNKHOUSER T.: A planar-reflective symmetry transform for 3D shapes. In Siggraph (2006), ACM,
pp. 549–559.
[SB04] S INGH K., BALAKRISHNAN R.: Visualizing 3D scenes
using non-linear projections and data mining of previous camera
movements. In Afrigraph (2004), ACM, pp. 41–48.
[SF08] S HILANE P., F UNKHOUSER T.: Distinctive regions of 3D
surfaces. Transactions on Graphics 26, 2 (2008), 7.
[SP08] S OKOLOV D., P LEMENOS D.: Virtual world explorations
by using topological and semantic knowledge. The Visual Computer 24, 3 (2008), 173–185.
[SS02a] S CHÖLKOPF B., S MOLA A. J.: Learning with Kernels.
MIT, 2002.
[SS02b] S TOEV S., S TRASSER W.: A Case Study on Automatic
Camera Placement and Motion for Visualizing Historical Data.
In Visualization (2002), IEEE.
[SSM99] S CHÖLKOPF B., S MOLA A. J., M ÜLLER K.-R.: Kernel principal component analysis. MIT, 1999, pp. 327–352.
[ST06] S OKOLOV D., TAMINE D. P. K.: Viewpoint quality and
global scene exploration strategies. In Grapp (2006).
[Vap00] VAPNIK V.: The Nature of Statistical Learning Theory.
Springer, 2000.
[VFSH01] VÁZQUEZ P.-P., F EIXAS M., S BERT M., H EIDRICH
W.: Viewpoint Selection using Viewpoint Entropy. In Vision
Modeling and Visualization (2001), Aka, pp. 273–280.
[YSY∗ 06] YAMAUCHI H., S ALEEM W., YOSHIZAWA S.,
K ARNI Z., B ELYAEV A., S EIDEL H.-P.: Towards Stable and
Salient Multi-View Representation of 3D Shapes. In Shape Modeling and Applications (2006), IEEE, p. 40.

c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

