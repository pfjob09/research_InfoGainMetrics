DOI: 10.1111/j.1467-8659.2010.01791.x
Pacific Graphics 2010
P. Alliez, K. Bala, and K. Zhou
(Guest Editors)

Volume 29 (2010), Number 7

Instant Propagation of Sparse Edits on Images and Videos
Yong Li1 Tao Ju2 Shi-Min Hu1
1 Tsinghua National Laboratory for Information Science and Technology
and Department of Computer Science and Technology, Tsinghua University
2 Washington University in St. Louis

Abstract
The ability to quickly and intuitively edit digital contents has become increasingly important in our everyday life.
We propose a novel method for propagating a sparse set of user edits (e.g., changes in color, brightness, contrast, etc.) expressed as casual strokes to nearby regions in an image or video with similar appearances. Existing
methods for edit propagation are typically based on optimization, whose computational cost can be prohibitive
for large inputs. We re-formulate propagation as a function interpolation problem in a high-dimensional space,
which we solve very efficiently using radial basis functions. While simple to implement, our method significantly
improves the speed and space cost of existing methods, and provides instant feedback of propagation results even
on large images and videos.
Categories and Subject Descriptors (according to ACM CCS): I.4.0 [IMAGE PROCESSING AND COMPUTER
VISION]: General—Image & Video - Interactive Painting & Editing

1. Introduction
With the ever increasingly availability of digital media such
as images and videos, it has become more and more important to be able to edit media in an intuitive and efficient manner. Many of these edits involve changing the appearances
(e.g., color, brightness, contrasts, etc.) of regions of interest. While it is possible to first segment such regions out,
segmentation can be a laborious process even with existing
semi-automated tools [BWSS09, WC08], especially when
the regions do not have clearly defined boundaries.
Edit propagation simplifies the task of image/video editing and avoids the need for explicit region segmentation.
In this scenario, the user only needs to draw a few strokes
indicating the desired edits, and the edits will be automatically propagated to nearby regions in the rest of the image
or video that have similar appearances to the region under
the strokes. An example is shown in Figure 1 for editing a
video sequence of a flower field (shown on the top). The user
strokes are provided on two frames shown on the top of (a,b),
which indicate her/his desire to change the red field to yellow (by the white strokes) while retaining the color in other
regions (by the black strokes). The edited video after propagation is shown at the bottom. Observe that the red flowers
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

in all frames (and viewed from different angles) have been
changed to yellow, only using the few strokes on two frames.
In previous works [LFUS06a, PL07, AP08, XLJ∗ 09], the
propagation task is formulated as an optimization problem.
The propagated edit (e.g., change in color) at each pixel is
solved to achieve two objectives. First, the edits at the stroke
pixels should match what the user has provided there. Second, pixels located nearby and having similar appearances
are more likely to receive a similar amount of edits. While
theoretically sound and achieving pleasing results, the optimization formulation has a number of inherent limitations
for practical applications:
• Efficiency. Solving the optimization problem typically involves a large system of equations whose size is proportional to the number of pixels in the input data. Even
with approximations, the state-of-art techniques [AP08,
XLJ∗ 09] cannot provide instant feedback of propagation,
which would be desirable for interactive editing.
• Storage. Solving the optimization problem requires a storage space that grows with the input data size. Hence it is
difficult for current methods to scale to very large data
(e.g., high-definition video or a movie).
• Implementation complexity. To reduce computational and

2050

YL&TJ&SMH / Instant Edit Propagation

Figure 1: Edit propagation: the original video “tulip” (top row) with user strokes on two frames (a,b), and the result after
the edits are propagated to the entire video (bottom row). This video clip is over 200MB in size and over 30 seconds long (910
frames). Computing the propagation using our method took only 15 seconds for the whole video and consumed 1MB of space.
Individual frames can be computed independently (taking around 0.017 second per frame) for instant preview.
storage cost, existing methods either employ approximating strategies, such as matrix sampling [AP08] and spatial
clustering [XLJ∗ 09], or rely on out-of-core implementations, both of which are non-trivial to implement.
In this paper, we offer a different perspective to edit propagation, and propose an alternative formulation that addresses
the above issues. Our key observation is that the optimization
objectives described above in existing methods essentially
prescribe a smooth function with a sparse set of constraints.
The function lies in a high-dimensional space where pixels
are represented by both their image coordinates and appearances, and the constraints lie at pixels covered by the input
strokes. Guided by the observation, we re-formulate propagation as a function interpolation problem, which we solve
using Radial Basis Functions (RBF). To compute propagated
edits, our algorithm only needs to solve a small linear system
whose size is proportional to the number of stroke pixels, instead of image pixels.
In comparison to existing optimization-based methods for
edit propagation, our method has a unique set of advantages
that would benefit practical applications:
• Instant feedback: Our method significantly improves the
efficiency of existing methods for propagating sparse edits, due to the drastically reduced complexity of computations that depends on the size of user-edits rather than the
data. Using the method, we can provide users with instant
feedback even on large images and videos.
• Scalable: The method consumes a trivial amount of memory (less than 1MB in all tests) and is scalable to very large
data, again since the computations scale with user-input
rather than the data itself.
• Simple to implement: The algorithm involves little more
than standard RBF interpolation, which can be easily implemented using a linear system solver.
2. Background
Appearance editing A significant amount of research has
been conducted in recent years for editing image or video appearances, particularly their tone [LFUS06b, EGSP05] and

color [YYSS04, XM06, WAM02, LLW04a]. For more efficiently editing without laborious segmentation of object
boundaries, some edge-aware methods are recently proposed
[CPD07, LSS09, FCA09, Fat09]. The idea of propagating
sparse user edits has also attracted much attention particularly due to their intuitiveness for interaction. Typically,
propagation is solved as an optimization problem. Example methods include colorization [LLW04b], tone adjustment [LFUS06a], material editing [PL07, AP08, XLJ∗ 09],
and BTF editing [XWT∗ 09]. In particular, AppProp [AP08]
considers a general, all-pairs energy function that allows
sparse edits to be more easily propagated to spatially discontinuous regions. The method of [XLJ∗ 09] further reduces
the computational complexity by solving for clusters of pixels with similar appearances and locality instead of solving
for individual pixels.
Function interpolation Constructing a smooth function in
space that interpolates values at given locations is a classical and well-studied problem in mathematics and geometric design. A standard approach is Radial Basis Functions
(RBF), which has a closed-form solution and gives additional flexibility in controlling the shape of the resulting
function via the choice of basis functions. RBF has been
employed in various graphics applications, most notably
for surface reconstruction from point clouds [CBC∗ 01].
There are other approaches for performing smooth interpolation with closed-form solutions, notably barycentric interpolation [Flo03, JMD∗ 07] and moving least squares (MLS)
[SOS04]. We pick RBF in this work due to its efficiency
over MLS (which requires solving a least square system at
each evaluated point) and its ability of handling point constraints (whereas barycentric interpolation typically requires
constraints along a closed boundary [FHL∗ 09]).
3. Problem formulation
The input to edit propagation is an image or video where,
at each pixel i, there is some user-specified edits gi (e.g.,
amount of change in color, contrast, etc.) with strength
wi ∈ [0, 1] (0 indicating no user constraints). We consider
the common scenario where the edits are sparse, meaning
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2051

YL&TJ&SMH / Instant Edit Propagation

wi = 0 for the majority of the pixels, except for those under
the user-provided strokes. The desired output is an edit ei at
all pixels i in the entire image or video.
Edit propagation is typically guided by two principles [AP08, XLJ∗ 09]. First, pixels with user-specified edits
should retain that specified amount of edit after propagation.
Second, pixels with similar locations (e.g., X,Y coordinates
and frame index) and having similar colors are more likely
to receive similar amount of edits. To achieve this effect, we
will compare two ways of formulating the propagation problem, one used in existing methods and based on optimization, and the other used in this work and based on function
interpolation.
3.1. Previous formulation: optimization
In this formulation, propagation is treated as a global minimization problem, which seeks edits ei at each pixel that
minimize an overall energy function capturing the two propagation principles. The exact form of the energy function
varies among existing works [LFUS06a,PL07,AP08]. As an
example, An and Pellacini [AP08] use an all-pair formulation of the energy as

∑i, j w j zi j (ei − g j )2 + λ ∑i, j zi j (ei − e j )2

(1)

where λ is a balancing weight, i, j enumerate over all pixel
pairs, and zi j measures the affinity between two pixels i, j in
an image and is expressed as:
zi j = exp(− fi − f j

2

).

(2)

Here, fi = (ci /σc , pi /σ p ) is a feature vector at pixel i comprising of its appearance ci (e.g., color in Lab space) and
position pi (e.g., in X, Y coordinates) weighted by parameters σc , σ p . For video, the feature vector can be expanded
to include the frame index ti of pixel i and a new parameter σt so that fi = (ci /σc , pi /σ p , ti /σt ). These parameters
σc , σ p , σt control the importance of appearance and locality
in determining the affinity between two pixels, and are set
subjectively based on user’s intention. For example, a low σc
would restrict edit propagation to pixels that have very similar colors with the user-edited pixels, and a low σ p would
restrict propagation to a local area around the user-edits.
Finding the edits ei that minimize the global energy function like that in Equation 1 requires solving a large linear
system of equations whose size equals the total number of
pixels in the data. To reduce the computational and storage
cost during minimization, approximation strategies such as
stochastic column sampling [AP08] and hierarchical clustering [XLJ∗ 09] have been proposed. However, the complexity
of these reduced minimization problems is still proportional
to the input data.
3.2. Our formulation: interpolation
Edit propagation can also be considered as a function interpolation problem. Consider a high-dimensional affinity
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

∗

space [XLJ 09] where each pixel i is represented by its feature vector fi (same as defined above, including both the
pixel’s location and appearance). The desired edits ei therefore are samples of a smooth function in the affinity space,
since close-by pixels in this space would have similar locality in the image space and similar colors, and therefore
should receive a similar amount of edits. In addition, this
function should interpolate (as much as possible) the userspecified edits gi at those pixels where wi > 0.
Therefore, we formulate the propagation task as finding
a smooth function h(f) in the affinity space subject to interpolation constraints at the pixels with user edits. To allow
“soft” interpolation, we formulate the constraints as a leastsquare energy function weighted by the edit strengths wi :

∑ wi (gi − h(fi ))2

(3)

i∈G

where G is the set of all user-edited pixels, i.e., those where
wi > 0. There are many choices of smooth functions. For the
ease of satisfying the interpolating constraints, we represent
h using Radial Basis Functions (RBF) centered at the useredited pixels as
h(f) =

∑ ai φ(

f − fi )

(4)

i∈C

where f is any point in the affinity space to be evaluated,
C ⊆ G is a selected subset of the user-edited pixels where
the basis functions are centered, ai are the unknown coefficients, and φ is some pre-defined radial basis. Following the
affinity definition in [AP08], we use Gaussian as our basis,
i.e., φ(r) = exp(−r2 ).
Finding the RBF h that satisfies the interpolation constraints in Equation 3 involves solving a square linear system with |C| variables, which are the unknown coefficients
ai (see more discussions in the next section). For sparse edits, |C| is much smaller than, and usually independent from,
the number of pixels in the data. Therefore the function h
can be constructed much more efficiently than minimizing
for a global energy function like that in Equation 1.
4. The algorithm
Computing edit propagation using our function-based formulation proceeds in two simple steps:
1. Reconstruction: Build the function h in Equation 4 by
solving for the coefficients ai that minimize the interpolation constraints in Equation 3.
2. Evaluation: The edit ei at each pixel i is computed simply by evaluating h at its feature vector, i.e., ei = h(fi ).
Below we give more implementation details for the reconstruction step, and compare the complexity of the algorithm
with previous approaches.
Reconstruction In our implementation, we randomly select
a set of pixels C from those with user-specified edits G, and

2052

YL&TJ&SMH / Instant Edit Propagation

let the user control the ratio α = |C|/|G|. Substituting Equation 4 into 3 yields a least-square minimization problem:

∑ wi (gi − ∑ a j φ(

i∈G

fi − f j ))2

(5)

j∈C

While we can directly solve for coefficients a j that minimize
the above energy using normal equations (which boils down
to a linear system with |C| equations), we have observed that
such results could contain negative coefficients a j that cause
edits h in distant part of the image or video to be unusually
high (see Figure2(c)). As a result, we additionally request
a j > 0 for all j ∈ C, and solve the minimization problem
using non-negative linear least squares [LH74]. The solution
will ensure that the edits will eventually diminish in distant
portions of the image or in regions with drastically different
appearances as the stroked region (see Figure2(b)).

5. Results
Here we demonstrate our method on edit propagation of several images and video clips. The video examples can be better appreciated in the accompanying video. In the examples,
we report the values of σ p and σt normalized to image resolution and video length, respectively.

Figure 3: Results of our method with different numbers of
basis functions. The parameters are σc = 0.2, σ p = 1.0.

Figure 2: Comparison between propagated edits h with
and without non-negative constraints. The edits are drawn
in gray scale where darker regions have lower values of h
The parameters are σc = 0.2, σ p = 0.1.
Complexity analysis The time and space complexity for the
reconstruction step are both O(|C||G|), where G is the set
of user-edited pixels, and |C| is the number of RBF. Evaluating the RBF takes O(|C|) space (to store the RBF coefficients) and O(n|C|) time for an input with n pixels. We note
that evaluation in practice is extremely fast (see next section), and has an “embarrassingly parallel” structure since
each pixel can be evaluated independently. Such independence implies that a portion of a large image with a fixed
size or a frame in a video can be instantly evaluated independently of the remaining data, providing users with real-time
feedback.
In comparison, the time and space complexity of previous
optimization-based methods are input-bounded. For example, the time and space complexity of AppProp [AP08] are
respectively O(m2 n) and O(mn), where m is the number of
sampled columns in their stochastic matrix sampling. Note
that the edits at all pixels are solved in one global system,
hence it does not allow previewing any portion of the image or video before the whole computation is finished. Using a KD-tree clustering scheme, the method in [XLJ∗ 09]
reduces the time and space complexity of minimization to
respectively O(m2 c) and O(mc) where c is the number of
cluters. Typically, c is a very large number (e.g., hundreds
of thousands), and can vary dramatically based on the image content and parameters such as σ p in the feature vector
definition. Moreover, the construction of the KD-tree takes
O(dn) time where d is the depth of the tree, and the tree has
to be re-built when new edits are provided by the user.

We first demonstrate in Figure 3 the results of the algorithm using varying numbers of RBF basis functions |C|
(Equation 4). In this case, the user edits (a) are intended to
change the color of only the pedals. Note that too few basis
functions (e.g., 5) cannot fully propagate the edits to distant
regions with similar appearances (e.g., some pedals are still
pink in (b)). On the other hand, increasing basis functions
beyond a certain number does not produce any visually significant differences. In all later examples, the percentage of
user-edited pixels selected as RBF centers, |C|/|G|, is set as
20%, which we have observed to yield a good balance between computational cost and propagation quality.
Figure 4 compares the result of our method on a video
with that of a recent optimization-based method [XLJ∗ 09].
Here, the user provided strokes on one frame of the video
(top of (a)), intending to change the blue body part of the bird
to green. Both our method and [XLJ∗ 09] manage to change
the body color on the rest of the frames (b-e), and the results
are similar. Note that due to the difference in problem formulation, the two algorithms will not produce identical results.
However, both results seem plausible by visual examination.
We show additional video and image editing results in Figures 5,6, and 7, demonstrating color and tone changes.

Figure 5: Adjusting the soft green tree to be a little withered
in the video "jungle". The parameters are σc = 0.1, σ p =
1.0, σt = 1.0.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

YL&TJ&SMH / Instant Edit Propagation

2053

Figure 4: Comparing results of our method (third row) and the KD-Tree method [XLJ∗ 09](second row) on editing the “bird”
video (top row) based on the user strokes in (top of a). The parameters are σc = 0.2, σ p = 1.0, σt = 1.0.

Figure 6: Adjusting the white mist in the video "Pandora"
to purple, giving a poisonous feel. The parameters here are
σc = 0.1, σ p = 1.0, σt = 1.0.

in processing large data. It takes around 0.02 seconds for reconstruction regardless of the size of the data. This agrees
with our complexity analysis that the computational cost of
this step only depends on the number of user-edited pixels and not on the size of the image or video. In addition,
evaluation is also efficient, taking less than 0.02 second for
each frame of the video. Such efficiency allows one to instantly see the result of propagation after putting down the
strokes. We demonstrate an interactive editing session in
the accompanying video, where the user incrementally adds
more strokes after inspecting the current results. In contrast,
both [AP08, XLJ∗ 09] require significantly more time especially on longer videos (e.g., the tulip video).
Note that our method requires almost negligible storage
space (which again is independent of the input data size, as
discussed in complexity analysis), whereas other methods
need significantly more. This advantage makes our method
applicable to movies, high-definitions videos, and streaming
media, and potentially suited to be ported to mobile devices
(e.g., iPad) where memory is a constraint.

Figure 7: Adjusting the tone mapping in a panorama
"Easter Island" with resolution 4800 × 1200. The parameters here are σc = 0.2, σ p = 1.0.
Performance We report the performance of our method on a
PC with 2.66GHz ,4 cores CPU, and 4 GB of RAM in Table
1 for the examples in the paper. In addition, we compare with
two optimization-based methods, AppProp [AP08] (using an
out-of-core implementation for dealing with large data) and
KD-tree [XLJ∗ 09], using the suggested parameter settings
in the original papers.
Observe from the table that our method is very efficient
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

6. Conclusion
We present a novel method to propagate user edits on images and videos based on similarity in locality and appearance. We formulate the problem as function interpolation,
and solve using the classical radial basis functions. Compared to previous approaches based on global optimization,
our method significantly improves computational efficiency
and storage consumption for sparse user edits, allowing instant feedback on images and videos with unlimited size,
while achieving similar visual results. The method is also
simple to implement.
In the future, we would like to explore further acceleration
of the evaluation step of our method using parallel implementations, harvesting the independent nature of evaluation.

2054

YL&TJ&SMH / Instant Edit Propagation
Data

Type.
Res.
Frame.Num
Time.Recons
Time.Eval/Frame
RBFs
Time.Eval
Memory
Time.all
KD-Tree
Memory
Time
AppProp
Memory

Easter Island

tulip

jungle

bird

Pandora

image
5.7M
0.02s
0.9s
1MB
1.2s
25MB
35s
2GB

video
205M
910
0.03s
0.017s
15s
1MB
60s
45MB
200min
100GB

video
26M
200
0.02s
0.015s
3s
1MB
5s
21MB
25min
12GB

video
30M
400
0.02s
0.013
4s
1MB
8s
23MB
48min
23GB

video
80M
165
0.02s
0.05s
8s
1MB
10s
35MB
70min
65GB

Table 1: Performance comparison among an out-of-core implementation of AppProp ( [AP08]), in-core version of the
KD-Tree method ( [XLJ∗ 09]), and our method.
We would also like to consider other choices of basis functions that might give better propagation results, as well as
better heuristics to adaptively determine the number of basis
functions and their centers. In addition, it would be interesting to incorporate edge-awareness [Fat09] in our affinity
definition to improve the behavior of the propagated edits at
object boundaries.
We chose RBF in this paper to represent the edit function, due to its efficiency in computation. However, the representation has its own limitation. In particular, the behavior of RBF away from the centers of the basis functions is
difficult to control, and may exhibit unintended undulations.
Although we managed to suppress some of the undulations
by enforcing positive RBF coefficients, this also makes the
function tend to zero far away from the centers, which can
be undesirable if the user wishes to change the appearance
of every pixel in the image/video (rather than within regions
of interest). This limitation points us to explore, in our future
work, alternative means for interpolation that is as efficient
as RBF but with better controls.
Acknowledgement: This work was supported by the National Basic Research Project of China (Project Number
2006CB303106), the Natural Science Foundation of China
(Project Number U0735001), the National High Technology
Research and Development Program of China (Project Number 2009AA01Z327), and in part by NSF grant IIS-0846072.

image processing with the bilateral grid. In SIGGRAPH ’07:
ACM SIGGRAPH 2007 papers (New York, NY, USA, 2007),
ACM, p. 103. 2
[EGSP05] E RIK R., G REG W., S UMANTA P., PAUL D.: High
dynamic range imaging: Acquisition, Display, and Image-Based
Lighting. Morgan Kaufmann, 2005. 2
[Fat09] FATTAL R.: Edge-avoiding wavelets and their applications. ACM Trans. Graph. 28, 3 (2009), 1–10. 2, 6
[FCA09] FATTAL R., C ARROLL R., AGRAWALA M.: Edgebased image coarsening. ACM Trans. Graph. 29, 1 (2009), 1–11.
2
[FHL∗ 09] FARBMAN Z., H OFFER G., L IPMAN Y., C OHEN -O R
D., L ISCHINSKI D.: Coordinates for instant image cloning. ACM
Trans. Graph. 28, 3 (2009), 1–9. 2
[Flo03] F LOATER M. S.: Mean value coordinates. Computer
Aided Geometric Design 20, 1 (2003), 19 – 27. 2
[JMD∗ 07] J OSHI P., M EYER M., D E ROSE T., G REEN B.,
S ANOCKI T.: Harmonic coordinates for character articulation.
ACM Trans. Graph. 26, 3 (2007), 71. 2
[LFUS06a] L ISCHINSKI D., FARBMAN Z., U YTTENDAELE M.,
S ZELISKI R.: Interactive local adjustment of tonal values. ACM
Trans. Graph. 25, 3 (2006), 646–653. 1, 2, 3
[LFUS06b] L ISCHINSKI D., FARBMAN Z., U YTTENDAELE M.,
S ZELISKI R.: Interactive local adjustment of tonal values. ACM
Trans. Graph. 25, 3 (2006), 646–653. 2
[LH74] L AWSON C. L., H ANSON R. J.: Solving Least Squares
Problems. Prentice-Hall, 1974. 4
[LLW04a] L EVIN A., L ISCHINSKI D., W EISS Y.: Colorization
using optimization. ACM Trans. Graph. 23, 3 (2004), 689–694.
2
[LLW04b] L EVIN A., L ISCHINSKI D., W EISS Y.: Colorization
using optimization. ACM Trans. Graph. 23, 3 (2004), 689–694.
2
[LSS09] L IU J., S UN J., S HUM H.-Y.: Paint selection. ACM
Trans. Graph. 28, 3 (2009), 1–7. 2
[PL07] P ELLACINI F., L AWRENCE J.: Appwand: editing measured materials using appearance-driven optimization. ACM
Trans. Graph. 26, 3 (2007), 54. 1, 2, 3
[SOS04] S HEN C., O’B RIEN J. F., S HEWCHUK J. R.: Interpolating and approximating implicit surfaces from polygon soup. In
SIGGRAPH ’04: ACM SIGGRAPH 2004 Papers (New York, NY,
USA, 2004), ACM, pp. 896–904. 2
[WAM02] W ELSH T., A SHIKHMIN M., M UELLER K.: Transferring color to greyscale images. ACM Trans. Graph. 21, 3 (2002),
277–280. 2

References

[WC08] WANG J., C OHEN M. F.: Image and Video Matting.
Now Publishers Inc., Hanover, MA, USA, 2008. 1

[AP08] A N X., P ELLACINI F.: Appprop: all-pairs appearancespace edit propagation. In SIGGRAPH ’08: ACM SIGGRAPH
2008 papers (New York, NY, USA, 2008), ACM, pp. 1–9. 1, 2,
3, 4, 5, 6

[XLJ∗ 09] X U K., L I Y., J U T., H U S.-M., L IU T.-Q.: Efficient
affinity-based edit propagation using k-d tree. In SIGGRAPH
Asia ’09: ACM SIGGRAPH Asia 2009 papers (New York, NY,
USA, 2009), ACM, pp. 1–6. 1, 2, 3, 4, 5, 6

[BWSS09] BAI X., WANG J., S IMONS D., S APIRO G.: Video
snapcut: robust video object cutout using localized classifiers.
ACM Trans. Graph. 28, 3 (2009), 1–11. 1

[XM06] X IAO X., M A L.: Color transfer in correlated color
space. In VRCIA ’06: Proceedings of the 2006 ACM international conference on Virtual reality continuum and its applications (New York, NY, USA, 2006), ACM, pp. 305–309. 2

[CBC∗ 01] C ARR J. C., B EATSON R. K., C HERRIE J. B.,
M ITCHELL T. J., F RIGHT W. R., M C C ALLUM B. C., E VANS
T. R.: Reconstruction and representation of 3d objects with radial basis functions. In SIGGRAPH ’01: Proceedings of the 28th
annual conference on Computer graphics and interactive techniques (New York, NY, USA, 2001), ACM, pp. 67–76. 2
[CPD07]

C HEN J., PARIS S., D URAND F.: Real-time edge-aware

[XWT∗ 09] X U K., WANG J., T ONG X., H U S.-M., G UO B.:
Edit propagation on bidirectional texture functions. Computer
Graphics Forum 28, 7 (2009), 1871–1877. 2
[YYSS04] YATZIV L., YATZIV L., S APIRO G., S APIRO G.: Fast
image and video colorization using chrominance blending. IEEE
Transactions On Image Processing 15 (2004), 2006. 2

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

