DOI: 10.1111/j.1467-8659.2009.01695.x
Eurographics/ IEEE-VGTC Symposium on Visualization 2010
G. Melançon, T. Munzner, and D. Weiskopf
(Guest Editors)

Volume 29 (2010), Number 3

A Multidirectional Occlusion Shading Model for Direct
Volume Rendering
Veronika Šoltészová1 Daniel Patel2 Stefan Bruckner3 Ivan Viola1
1 University

of Bergen, Norway
Michelsen Research, Norway
3 Simon Fraser University, Canada

2 Christian

Abstract
In this paper, we present a novel technique which simulates directional light scattering for more realistic interactive visualization of volume data. Our method extends the recent directional occlusion shading model by enabling
light source positioning with practically no performance penalty. Light transport is approximated using a tilted
cone-shaped function which leaves elliptic footprints in the opacity buffer during slice-based volume rendering.
We perform an incremental blurring operation on the opacity buffer for each slice in front-to-back order. This
buffer is then used to define the degree of occlusion for the subsequent slice. Our method is capable of generating
high-quality soft shadowing effects, allows interactive modification of all illumination and rendering parameters,
and requires no pre-computation.
Categories and Subject Descriptors (according to ACM CCS):
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism —Color, shading, shadowing, and texture

clusion shading model introduced by Schott et al. [SPH∗ 09]
is a forward scattering approximation based on a conical
phase function. While the method is capable of generating
realistic illumination at interactive frame rates, it requires
that the view and the light directions have to coincide. In
this paper, we introduce a multidirectional occlusion model,
which removes this constraint.

1. Introduction
Local illumination models, such as the Phong model, are
suited for conveying shape cues for well-defined structures
in volume data. However, they are generally not suitable
for visualization when the main goal is to emphasize threedimensional structural arrangements. In such a scenario, it
is important to convey information about relative positions
and distances between individual features. The human visual
system is adapted to inferring three-dimensional information
from illumination. Soft shadows, in particular, are effective
monocular depth cues. Not only do they provide occlusion
information, but the size and shape of the penumbra can be
used to judge distances. Global illumination models provide
these cues at high computational costs, especially for volume rendering. Visualization research has therefore focused
on the development of new global illumination approximations for volume data that limit the complexity and allow for
real-time image synthesis. For this purpose, precomputation
or parameter constraint strategies are frequently employed.
Both suffer from limited flexibility which can be problematic
when interactive exploration is required. The directional occ 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

The importance of illumination in 3D object perception
has been well-studied [BBC83,KMK94,BLK00]. To find out
how to best improve volume rendering, we have been conducting studies with medical illustrators. During our demonstrations of state-of-the-art visualization techniques to experienced medical illustrators, their first critique point was the
positioning of the light in the scene and the choice of nonstandard colors. While visualization researchers often carelessly define the light vector parallel to the view vector, this
is considered a novice mistake in the domain of illustration.
The resulting image is flat, akin to photos taken with builtin front flash. To give depth to an image, as a rule, medical
illustrators use illumination from the top left. To further op-

883

Šoltészová et al. / A Multidirectional Occlusion Shading Model for DVR

884

(a)

(b)

(c)

(d)

Figure 1: Visualizations of a human hand using raycasting (a), and sliced-based volume rendering (b), both using the Phong
illumination. Directional occlusion shading model with a headlamp illumination setup (c) and illumination from the top left (d).
Illumination with the top-left light position causes that the fingers cast soft shadows on the body and evoke strong depthperception cues.

timize the appearance of the depicted structures, manual fine
tuning is required.

vide additional cues. Max [Max95] gives a comprehensive
overview of different optical models for volume rendering.

The directional occlusion shading model for interactive
direct volume rendering takes the advantage of a headlight
setup for performance reasons: by placing the light source at
the eye position, the samples required for illumination can be
reused for compositing, allowing the method to perform both
operations in a single pass for a view-aligned slice through
the volume. Our approach uses elliptic occlusion footprints
computed from the light position, instead of the symmetric
spherical footprints which were used in the original paper.
We achieve the same performance with the additional possibility to position the light source anywhere within the hemisphere defined by the view vector. An example of the significant improvement of depth perception compared to the
previous method is shown in Figure 1. A visualization of a
human hand rendered with different techniques is compared
to the headlight and top-left shading. Both a professional illustrator and a user study confirmed our subjective assessment which favored the rendering result 1(d).

Yagel et al. [YKZ91] employed recursive ray tracing which allows for effects such as specular reflection
and shadows. Behrens and Ratering [BR98] added shadows to texture-based volume rendering by using an additional shadow volume. The model presented by Kniss et
al. [KKH02, KPH∗ 03] captures volumetric light attenuation effects including volumetric shadows, phase functions,
forward scattering, and chromatic attenuation using halfangle slicing. Hadwiger et al. [HKSB06] presented a GPUaccelerated algorithm for computing deep shadow maps for
volume rendering. Rezk-Salama [RS07] proposed a semiinteractive approach for GPU-based Monte Carlo volume
raytracing.

The remainder of this paper is structured as follows: In
Section 2, we review related work. Our multidirectional occlusion model is derived in Section 3. Section 4 provides
additional implementation details. Results are presented and
discussed in Section 5. Conclusions are drawn in Section 6.

2. Related Work
Levoy [Lev87] proposed the use of gradients in volume
rendering for evaluating a surface-based local illumination
model. While this common approach is effective in accentuating material boundaries, it suffers from noise. In particular, gradient-based shading fails to provide useful results
in nearly homogenous regions. Illumination models which
exploit the volumetric nature of the data can therefore pro-

Ambient occlusion as described by Zhukov et al. [ZIK98]
inverts the principle of light-exposure of a point in space
to obscurance by its close environment. Dachsbacher et
al. [DSDD07] refer to obscurance as antiradiance. They treat
visibility implicitly while propagating antiradiance as an additional quantity. The advantage of these approaches is that
they are view-independent: for fixed geometry, occlusion information only has to be computed once and can then be applied efficiently during rendering, for example using texture
mapping. Several fast techniques which utilize this concept
have been presented [Bun05, SA07]. Knecht [Kne07] and
Méndez-Feliu [MS09] provide comprehensive overviews of
rendering techniques based on ambient occlusion and obscurances.
In the context of volume visualization, the radiance at a
point is determined by shooting rays in all directions from
the point and averaging its degree of occlusion by other parts
of the volume. The result is an approximation of global diffuse illumination. It produces soft shadowing effects which
give a good indication of spatial relationships. However, the
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Šoltészová et al. / A Multidirectional Occlusion Shading Model for DVR

opacity at any point is determined by the transfer function.
Ambient occlusion therefore requires an expensive computation step every time the transfer function is modified. Stewart [Ste03] introduced vicinity shading, a variation of ambient occlusion to enhance perception of volume data by darkening depressions and crevices. To reduce evaluation costs,
occlusion calculations are reused. The approach of Ropinski et al. [RMSD∗ 08] relied on local histogram clustering
to precompute occlusion information for all possible transfer function settings. However, high frequency data, in particular the presence of noise, reduces the effectiveness of
their clustering approach and can lead to artifacts. Additionally, their precomputation process is very time and memory
consuming. Hernell et al. [HYL07] used a local approximation of ambient occlusion in volumes to limit computation
times. In subsequent work [HLY08, HLY09], they utilized
local piecewise integration to approximate global light propagation. This approach still requires ambience data for each
voxel to be recomputed when changing the transfer function, but their method is able to run interactively by limiting
the number of rays shot for evaluating the ambience and by
subsampling the rays using adaptive compression. In recent
work, Ropinski et al. [RDR10] described a volumetric lighting model which simulates scattering and shadowing. They
use slice-based volume rendering from the view of the light
source to calculate a light volume and raycasting to render
the final image.
View-dependent approaches do not require extensive precomputation and therefore allow fully interactive transfer
function modification. This is frequently achieved by limiting light evaluation from spherical local neighborhoods to
conical neighborhoods. Desgranges et al. [DEP05] use incremental blurring to achieve shading effects without the use of
a gradient. The approach by Bruckner and Gröller [BG07]
employed non-photorealistic shadowing and emission effects for the purpose of illustration. Finally, as stated in the
previous section, our method is an extension of the model by
Schott [SPH∗ 09].

3. Multidirectional Occlusion Shading
The Directional Occlusion Shading model by Mathias
Schott et al. (MS-DOS) [SPH∗ 09] describes an approximation of light scattering in particles of a volume. This simple method generates soft a shadow effect and hence provides important shape and depth-perception cues. Although
the approximation of the light transfer delivers slightly different results compared to reference images from a raytracer,
it provides visually compelling shading effects at interactive
frame-rates and with no precomputation. However, the light
transfer approximation in the MS-DOS model constrains the
light direction to the viewing direction. In this section, we
derive an approximation which does not limit the light to
this fixed direction.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

885

3.1. Light Transport Equation
The directional occlusion shading model approximates
transport of light energy L in a medium. Every point in the
environment receives a portion of energy, i.e., radiance composed by background radiance Lb and medium radiance Lm .
The medium radiance consists of the emitted radiance Le and
in-scattered radiance Li . The emitted radiance at a point x
depends only on the local environment of x. Unlike Le , the
in-scattered radiance Li integrates over global features:
Li (x, ω) =

4π

L(x, ωi )Φ(ω, ωi )dωi

(1)

where Φ(ω, ωi ) denotes the phase function for two light-ray
directions ω and ωi . Li quantifies the total radiance incident to point x from all directions ωi . From Equation 1, it
can be seen that Li requires an expensive recursive evaluation. The MS-DOS shading model and our model (multidirectional OS) simplify the evaluation which considerably
reduces the computational costs.
We assume that the medium emits light only in directions
within a specific cone. The phase function from Equation 1
can be therefore replaced by a simple cone-shaped phase
function Φθ,α (ω, ωi ) where θ is the aperture angle and α
the tilt angle of the cone. A schematic illustration of this
scenario is depicted in Figure 2. A particle at a point x scatters light which is received by particles inside the cone. The
in-scattering term Li is conceptually related to the fractional
visibility which is equivalent to the opacity and cumulates
information about ambient occlusion.
Like the original model, we use a slice-based volume renderer with an additional opacity buffer. Slices are composited in the front-to-back order and the opacity buffer is incrementally filtered and used to determine the accumulated
opacity for the next slice as shown in Figure 3. MS-DOS

ǫ
α

2θ

x

Figure 2: Conical phase function setup: a selected point in
space x scatters light which we approximate by a tilted cone
(α = tilt, θ = aperture). The axis of the cone is parallel to
the light direction. The projection of the light energy leaves
an elliptical footprint ε on a selected viewing plane.

Šoltészová et al. / A Multidirectional Occlusion Shading Model for DVR

886

x

x

θ

V

θ
d

2θ
α

L

V

V2

R
A1

α

C
O

A2
O

L

V1

front-to-back

operates on view-aligned slices and assumes that the direction of the light source is aligned to the viewing direction.
As a consequence, the opacity buffer can be convolved with
a symmetrical disc-shaped Gaussian kernel. To enable interaction with the light source, we change the symmetrical
disc-shaped kernel to an elliptical kernel. The ellipse ε is
defined by the intersection of a tilted cone which represents
the phase function Φθ,α (ω, ωi ) and the slice plane. The coneshaped phase function is tilted by an angle α which is limited
to [0, π2 − θ). This restricts the cone-section from degenerating into hyperbolas or parabolas. Figure 4 describes this
geometrical situation.

With known a1 and a2 , we use Equation 5 to calculate A
which is the major axis of the ellipse.
1
.
The center of the ellipse ε is in C with OC = a2 −a
2
We define a circular cone section κ which intersects the point
C. Consequently, the axis of the cone intersects κ in its center
S. This scenario is illustrated in Figure 4(b). The intersection
line ε ∩ κ is perpendicular to A1 A2 and intersects the center
C of ε. Consequently, ε ∩ κ is collinear with the minor axis of
ε. Figure 5 illustrates the side view of ε ∩ κ. In Figure 6, we
focus on the triangles △ XV2′V2 and △ SCO, and on the circle
κ. Basic analysis implies Equations 6, 7, 8, and 9. Solving
them, we determine B - the minor axis of ε.

sin α =
tan θ =

d′
OC

(6)

dR
d′

(7)

R′ = R + dR

(8)

(2)
B=

The axis of the cone intersects the plane at the point O. When
the tilt angle α = 0, the cone section is a circle, and a1 =
a2 = A. With a known R, we turn to the law of sine in the
triangles △ A1V1 O and △ OA2V2 . With α, θ, and R given,
Equations 3 and 4 yield a1 and a2 :
R
a1
=
sin( π2 − θ) sin( π2 + θ − α)

(3)

a2
R
=
sin( π2 + θ) sin( π2 − θ − α)

(4)

a1 + a2
2

A2

Figure 4: A geometrical description of the cone-shaped
phase function: the elliptical cone-section defines a circle
κ centered in S and intersecting the center C of the ellipse ε.
A side view (a) and a 3D view (b) of the planar cone-section.

This section describes the analytical computation of the elliptical kernel, namely, the derivation of its major and minor
axes A = |A1 A2 | and B = |CC′ | from a known tilt α, a cone
aperture θ and a slice distance d. According to Figure 4(a),
we derive R from d, θ and α as:

A=

ǫ

(b)

3.2. Analysis of the Geometrical Model

tan θ
R=d
cos α

C′

a2

(a)

Figure 3: Incremental blurring of the opacity buffer. We use
a view-aligned slice stack composited in the front-to-back
order.

V2 ′

S

V1 ′

a1

κ

A1

R′ 2 − OC

2

+ d′2

(9)

3.3. Weighting Function
Light rays collinear to the cone axis hit the slice with the
highest intensity. We revisit Figure 4(b): O is the point with
the highest incident energy. We define a weighting function
as follows:
WL (x, y) = 1 − k

(10)

with k defined implicitly by:
(5)

(x − (1 − k) OC )2 y2
+ 2 = k2
A2
B

(11)

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Šoltészová et al. / A Multidirectional Occlusion Shading Model for DVR

x
V2

V2 ′

R

α
A1

C′

C
R

′

S

A2

C

α

S

R′

C

S

V1

X dR V2 ′

V1 ′

(b)

(c)

(a)

V1 ′
Figure 5: A detailed side view of the intersection of ellipse ε
and circle κ.

Equation 11 results in a quadratic equation with two real
roots from which we take the maximum. A kernel with a
linear fall-off from O towards the border of the ellipse is illustrated in Figure 7(a). Additionally, we apply the Gaussian
function to smooth the fall-off of the weights as illustrated
in Figure 7(b).
3.4. On-the-fly Convolution
We apply an incremental convolution of the opacity buffer
Oi and the elliptical kernel Gε for each slice i. As the light
direction L changes, Gε has to be aligned respectively. We
project the light vector to the viewing plane which yields a
2D-vector L’ and rotate the kernel so that its major axis is
aligned with L’:
OC
L’
=
OC
L’

κ

d′ θ

d′

O

V2 ′

V2

O

θ

887

(12)

The weight-distribution in Gε depends only on the tilt, aperture, light direction, and slice distance. Therefore, an update
is triggered only if one of these parameters changes. In practice, we render the kernel Gε to a texture when an update is
triggered. First, we uniformly scale the ellipse so that it fits
into a unit-square. Second, we set-up the texture coordinates
so that Gε is aligned correctly. During the volume rendering
pass, we apply inverse scaling operation to regenerate Gε of
the correct size. In Figure 8, we visualize the gecko dataset
with different tilts and apertures.

Figure 6: We introduce new literals for selected primitives: (a) the triangle △ SCO, (b) the triangle △ XV2′V2
and (b) the circle κ. These primitves are defined in Figures 4
and 5 by the same color encoding. Note, that ||CC′ || = B
which is the minor axis of the ellipse ε.

The color contribution ci is multiplied by λi . The color ci
and opacity αi propagate to Ci+1 using traditional alphablending with the over operator. Our method requires no precomputation and performs at interactive frame-rates. Due to
incremental blurring of the opacity buffer, shadows cast by
highly occlusive regions fade-out smoothly with distance.
Compared to the state-of-the-art model, we thereby add a
movable light source with negligible performance penalty.

4. Implementation Details
Our new model was implemented as a plugin to VolumeShop [BG05] using C++ and OpenGL/GLSL. Using the
ARB_draw_buffers OpenGL extension, two render targets
are written for each slice: the intermediate rendered image
and the occlusion buffer. The elliptical blurring kernel is
stored in an additional texture which is updated whenever
the light source parameters change. For all examples in the
paper, we use a texture size of 128 × 128. When the lighting
parameters change, we recompute the footprint. The major
axis of the ellipse is aligned with the projection of the light
vector to the viewing plane by multiplying GL_TEXTURE
matrix stack by a rotation matrix. In case the ellipse grows or
moves out of the texture, we apply translation and scaling to

Based on the incremental convolution (∗), we calculate a
modulation factor λi for each sample on the slice i which
determines the visibility of the current slice:
λi =

1
1 + Gε ∗ Oi−1

(13)

In addition to the opacity buffer Oi , we use a color buffer Ci
for each slice. The opacity buffer for the next slice combines
the opacity buffer of the previous slice with the opacity of
the current slice αi :
Oi = Gε ∗ Oi−1 + αi

(14)

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

(a)

(b)

Figure 7: Elliptical kernels used for incremental blurring
of the opacity buffer: with linear fall-off (a) and Gaussian
fall-off of the weighting function (b).

Šoltészová et al. / A Multidirectional Occlusion Shading Model for DVR

888

(a)

(b)

(c)

Figure 8: Visualizations of the gecko CT dataset with different setup of aperture θ and tilt angle α: θ = 10◦ and α = 37◦ (a),
θ = 40◦ and α = 37◦ (b), and θ = 40◦ and α = 5◦ (c). Light is coming from the right for all images.

Figure 9: Visualizations of the carp CT dataset using the directional occlusion shading model with a headlamp illumination
setup (a) and using illumination setup conventional to medical illustrations (b).

5.1. Case Studies

years of professional experience who affirmed that the visualizations generated using this kind of illumination yield
stronger perception cues. We presented her visualizations using different lighting settings. Her task was to choose which
lighting conditions suit medical illustrations the best. She
consistently preferred image such as those depicted in Figures 10(b) and 10(d). The illustrator further confirmed that
interactive fine-tuning of the exact light placement is necessary in many cases, in order to avoid excessive darkening
of focus objects. In volume data, regions with high gradient
magnitude correspond to surface-like structures. Using the
gradient magnitude to add an additional local specular component to these objects can further improve perception. Figure 11 presents a computer tomography of a human foot generated with different illumination models and varying light
source positions: Figures 11(a) and 11(b) use the multidirectional OS model enhanced by specular highlights, and Figures 11(c) and 11(d) use the pure multidirectional OS model.

Medical illustrators generally place the light source in the
top left corner to improve depth perception. Figure 9 depicts
the carp CT dataset visualized under different illumination
conditions. While in Figure 9(a), the image lacks depth, Figure 9(b) emphasizes the detailed structure of the skeleton
through shadows. Similarly, Figure 10 shows cases where
illumination leads to better perception of structures. In Figure 10(a), the hand seems to directly contact the body. In
reality, there is a small gap which is visible in Figure 10(b).
Similarly for Figures 10(c) and 10(d): in Figure 10(d), the
eye sockets of the skull appear deeper than in Figure 10(c).
We consulted a certified medical illustrator with over 25

To gain a coarse impression on the impact of our technique on non-professionals, we also conducted a small user
study on a group of 42 participants with different backgrounds. We presented them two series of result images: the
human hand and the human thorax which are shown in Figures 1 and 12. Their task was to choose an image which in
their opinion yields the strongest depth cues. From the series
of different renderings of the hand, 39 participants (92.86%)
favored the top-left illumination in Figure 1(d), 2 participants (4.76%) preferred the raycasting in Figure 1(a) and 1
participant (2.38%) preferred the headlamp illumination in
Figure 1(c). A majority of 41 (97.62%) participants also pre-

fit it into the bounding box of the texture. During rendering,
the inverse transformation is applied to access the kernel at
correct positions. However, massive downscaling of the coordinate system may lead to a loss of precision. Users can
interactively adjust the tilt, the aperture, and the XY-rotation
of the light source. This gives the user full control to set the
light source arbitrarily in the hemisphere defined by the view
vector. The parameters aperture, tilt, and rotation are set by
sliders in the user-interface.

5. Results and Discussion
In this section, we provide case studies and comparisons to
other volume rendering approaches and analyze the performance of our new method.

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Šoltészová et al. / A Multidirectional Occlusion Shading Model for DVR

(a)

(b)

(c)

889

(d)

Figure 10: Visualizations of computer tomography data using the directional occlusion shading model with a headlamp illumination setup (a) and (c) using the illumination setup conventional to medical illustrations (b) and (d).

(a)

(b)

(c)

(d)

Figure 11: Visualizations of a human foot acquired by computer tomography using the directional occlusion shading model:
using the Phong illumination model with the headlamp illumination setup (a) and with the top-left lighting (b). Visualizations (c)
and (d) use the diffuse illumination model with the headlamp and the top-left light source setup respectively.

ferred the top-left illumination of the thorax in Figure 12(d)
and only one participant (2.38%) selected the raycasted image in Figure 12(a).
Local surface-based illumination of volume data employs
the gradient to substitute for the surface normal. However,
gradients estimation frequently performs poor in the presence of noise which can lead to distracting artifacts. Thus,
for modalities such as ultrasound, unshaded volume rendering is commonly employed. This makes the structures
in the data difficult to interpret even for experienced medical professionals. The directional occlusion shading model
as a gradient-free shading method can be used to improve
perception. Interactive light source modification enables the
user to inspect and understand the structures better. Figure 13 shows different visualizations of 3D cardiac ultrasound: 2D slices and 3D volume renderings. The clipping
plane reveals the inside of the heart chambers. During examination, physicians see the ultrasound visualizations on their
workstations as in Figure 13(a). We used a transfer function which shows the heart in a similar fashion. Figure 13(b)
shows that gradient-based shading is not well-suited for ultrasound data. Multidirectional occlusion shading, on the
other hand, reveals the structure, and interaction with light
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

source enables the user to better perceive the depth of the
cavities.
We described a shading model which does not require
precomputation and storage of additional data, unlike deep
shadow maps [HKSB06] or light volumes [RDR10], and
which allows arbitrary light position within the hemisphere
defined by the view vector. Half-angle slicing, introduced
in the work of Kniss et al. [KKH02], generates shadows by
using a slicing direction halfway between view and light direction. However, choosing a slicing direction which is nonparallel to the viewing directions leads to visible artifacts, especially when the light source tilt angle surpasses 60◦ . Figure 14 clearly demonstrates a situation when such artifacts
are visible when half-angle slicing is used. In the original
half-angle slicing approach, the order of the slices is reverted
if the light source is located in the hemisphere opposite to the
viewer. Reverting the traversal of the slice-stack is a possible extension of our approach which would not limit the light
vector to the hemisphere defined by the view vector.
5.2. Performance Analysis
We tested the described method on a workstation equipped
with an NVIDIA GeForce 295 GTX GPU with 1.7GB graph-

Šoltészová et al. / A Multidirectional Occlusion Shading Model for DVR

890

(a)

(b)

(c)

(d)

Figure 12: Visualizations of a human thorax we used for user study: using raycasting (a), sliced-based volume rendering (b),
both using the Phong illumination followed by the directional occlusion shading model with the headlamp illumination setup (c)
and illuminated from the top left (d).

(a)

(b)

(c)

(d)

Figure 13: Visualizations of 3D ultrasound of cardiac data: user interface of a 3D cardiac ultrasound workstation (a), a clipped
3D cardiac ultrasound visualization using direct volume rendering and Phong illumination model, rendered with a raycaster (b),
clipped 3D cardiac ultrasound visualization using the multidirectional occlusion shading model with light coming from the top
left (c) and the bottom left (d).

(a)

(b)

Figure 14: Visualizations of the bonsai dataset: slice-based
volume rendering using a view-aligned slice stack (a) and
using a half-angle-aligned slice stack (b).

ics memory, an Intel R Core i7 CPU with 3.07GHz and
12GB of RAM. We measured the performance of our implementation using the gecko dataset of resolution 512 ×
512 × 88 voxels, 0.5 voxels sampling distance and viewportresolution 768 × 407 pixels. We achieved interactive framerates of 19Hz with using the MS-DOS and 18Hz with mul-

tidirectional OS using a 37◦ angle of aperture while interacting with the viewing parameters. During interaction with
the light source, which required update of the kernel, we
achieved 14Hz frame-rates. For comparison, using the same
framework, a simple slice-based renderer with no shadowing and Phong illumination achieved 25Hz and a highquality raycaster with Phong illumination and no shadowing
achieved 21Hz. We performed the same test with the foot
dataset of resolution 256×256×256 voxels, 0.5 voxels sampling distance and viewport-resolution 531 × 311 pixels. We
achieved 15Hz while using the original MS-DOS approach,
14Hz using our new method, and 12Hz during light source
interaction. In this case, a simple slice-based renderer performed at 25Hz and a raycaster at 22Hz. These tests prove
that the interactive light source placement is a valuable extension of the original approach traded for a negligible performance penalty.

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Šoltészová et al. / A Multidirectional Occlusion Shading Model for DVR

6. Conclusions
In this paper, we presented a shading model for direct volume rendering, which enables the interactive generation of
high-quality soft shadow effects without the need for precomputation. Our method extends a previous technique to
enable interactive placement of the light source. Using elliptic instead of circular footprints, we achieve almost the same
performance while greatly improving the flexibility of the
method. Additionally, we discussed several applications of
such a shading model and consulted a professional illustrator to confirm the importance of freely modifying the light
direction.
7. Acknowledgments
This work was carried out within the IllustraSound research
project (# 193180), which is funded by the VERDIKT program of the Norwegian Research Council with support of
the MedViz network in Bergen, Norway. The authors wish to
thank the certified medical illustrator Kari Toverud for consulting, numerous respondents for their feedback and anonymous reviewers for their comments.
References
[BBC83] B ERBAUM K., B EVER T., C HUNG C. S.: Light source
position in the perception of object shape. Perception 12, 5
(1983), 411–416. 1
[BG05] B RUCKNER S., G RÖLLER M. E.: VolumeShop: An interactive system for direct volume illustration. In Proceedings of
IEEE Visualization (2005), pp. 671–678. 5
[BG07] B RUCKNER S., G RÖLLER M. E.: Enhancing depthperception with flexible volumetric halos. IEEE Transactions on
Visualization and Computer Graphics 13, 6 (2007), 1344–1351.
3
[BLK00] B RAJE W., L EGGE G., K ERSTEN D.: Invariant recognition of natural objects in the presence of shadows. Perception
˝ 398. 1
29, 4 (2000), 383 U
[BR98] B EHRENS U., R ATERING R.: Adding shadows to a
texture-based volume renderer. In Proceedings of IEEE Symposium on Volume Visualization 1998 (1998), pp. 39–46. 2
[Bun05] B UNNEL M.: GPU-Gems, vol. 2. Addison Wesley, 2005,
ch. Dynamic Ambient Occlusion and Indirect Lighting. 2
[DEP05] D ESGRANGES P., E NGEL K., PALADINI G.: Gradientfree shading: A new method for realistic interactive volume rendering. In Proceedings of Vision, Modeling, and Visualization
2005 (2005), pp. 209–216. 3
[DSDD07] DACHSBACHER C., S TAMMINGER M., D RETTAKIS
G., D URAND F.: Implicit visibility and antiradiance for interactive global illumination. ACM Transactions on Graphics 26, 3
(2007), 61.1–61.10. 2
[HKSB06] H ADWIGER M., K RATZ A., S IGG C., B ÜHLER K.:
GPU-accelerated deep shadow maps for direct volume rendering.
In Proceedings of SIGGRAPH/EUROGRAPHICS Symposium on
Graphics Hardware (2006), pp. 49–52. 2, 7
[HLY08] H ERNELL F., L JUNG P., Y NNERMAN A.: Interactive
global light propagation in direct volume rendering using local piecewise integration. In Proceedings of IEEE/EG International Symposium on Volume and Point-Based Graphics (2008),
pp. 105–112. 3
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

891

[HLY09] H ERNELL F., L JUNG P., Y NNERMAN A.: Local ambient occlusion in direct volume rendering. IEEE Transactions on
Visualization and Computer Graphics 99, 2 (2009). 3
[HYL07] H ERNELL F., Y NNERMAN A., L JUNG P.: Efficient
ambient and emissive tissue illumination using local occlusion
in multiresolution volume rendering. In Proceedings of Volume
Graphics 2007 (2007), pp. 1–8. 3
[KKH02] K NISS J., K INDLMANN G., H ANSEN C.: Multidimensional transfer functions for interactive volume rendering.
IEEE Transactions on Visualization and Computer Graphics 8,
3 (2002), 270–285. 2, 7
[KMK94] K ERSTEN D., M AMASSIAN P., K NILL D.: Moving
cast shadows and the perception of relative depth. Tech. Rep. 6,
Max-Planck-Institut für biologische Kybernetik, Tübingen, Germany, 1994. 1
[Kne07] K NECHT M.: State of the art report on ambient occlusion. Tech. rep., Technische Universität Wien, Vienna, Austria,
2007. 2
[KPH∗ 03] K NISS J., P REMOZE S., H ANSEN C., S HIRLEY P.,
M C P HERSON A.: A model for volume lighting and modeling.
IEEE Transactions on Visualization and Computer Graphics 9, 2
(2003), 150–162. 2
[Lev87] L EVOY M.: Display of surfaces from volume data. IEEE
Computer Graphics and Applications 8 (1987), 29–37. 2
[Max95] M AX N.: Optical models for direct volume rendering.
IEEE Transactions on Visualization and Computer Graphics 1, 2
(1995), 99–108. 2
[MS09] M ÉNDEZ -F ELIU À., S BERT M.: From obscurances to
ambient occlusion: A survey. "The Visual Computer" 25, 2
(2009), 181–196. 2
[RDR10] ROPINSKI T., D ÖRING C., R EZK -S ALAMA C.: Interactive volumetric lighting simulating scattering and shadowing.
In Proceedings of IEEE Pacific Visualization (2010), pp. 169–
176. 3, 7
[RMSD∗ 08] ROPINSKI T., M EYER -S PRADOW J., D IEPEN BROCK S., M ENSMANN J., H INRICHS K. H.: Interactive volume rendering with dynamic ambient occlusion and color bleeding. Computer Graphics Forum 27, 2 (2008), 567–576. 3
[RS07] R EZK -S ALAMA C.: GPU-based Monte-Carlo volume
raycasting. In Proceedings of Pacific Graphics 2007 (2007),
pp. 411–414. 2
[SA07] S HANMUGAM P., A RIKAN O.: Hardware accelerated
ambient occlusion techniques on GPUs. In Proceedings of Symposium on Interactive 3D Graphics and Games (2007), pp. 73–
80. 2
[SPH∗ 09] S CHOTT M., P EGORARO V., H ANSEN C.,
B OULANGER K., S TRATTON J., B OUATOUCH K.: A directional occlusion shading model for interactive direct volume
rendering. Computer Graphics Forum 28, 3 (June 2009),
855–862. 1, 3
[Ste03] S TEWART A. J.: Vicinity shading for enhanced perception of volumetric data. In Proceedings of IEEE Visualization
2003 (2003), pp. 355–362. 3
[YKZ91] YAGEL R., K AUFMAN A., Z HANG Q.: Realistic volume imaging. In Proceedings of IEEE Visualization 1991 (1991),
pp. 226–231. 2
[ZIK98] Z HUKOV S., I ONES A., K RONIN G.: An ambient light
illumination model. In Rendering Techniques (1998), pp. 45–56.
2

