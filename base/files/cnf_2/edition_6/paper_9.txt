DOI: 10.1111/j.1467-8659.2009.01547.x

COMPUTER GRAPHICS

forum

Volume 29 (2010), number 1 pp. 101–116

SPC: Fast and Efficient Scalable Predictive Coding
of Animated Meshes
Nikolˇce Stefanoski and J¨orn Ostermann
Institut f¨ur Informationsverarbeitung, Leibniz Universit¨at Hannover, Germany

Abstract
Animated meshes are often represented by a sequence of static meshes with constant connectivity. Due to their
frame-based representation they usually occupy a vast amount of bandwidth or disk space. We present a fast
and efficient scalable predictive coding (SPC) scheme for frame-based representations of animated meshes. SPC
decomposes animated meshes in spatial and temporal layers which are efficiently encoded in one pass through
the animation. Coding is performed in a streamable and scalable fashion. Dependencies between neighbouring
spatial and temporal layers are predictively exploited using the already encoded spatio-temporal neighbourhood.
Prediction is performed in the space of rotation-invariant coordinates compensating local rigid motion. SPC
supports spatial and temporal scalability, and it enables efficient compression as well as fast encoding and
decoding. Parts of SPC were adopted in the MPEG-4 FAMC standard. However, SPC significantly outperforms the
streaming mode of FAMC with coding gains of over 33%, while in comparison to the scalable FAMC, SPC achieves
coding gains of up to 15%. SPC has the additional advantage over FAMC of achieving real-time encoding and
decoding rates while having only low memory requirements. Compared to some other non-scalable state-of-the-art
approaches, SPC shows superior compression performance with gains of over 16% in bit-rate.
Keywords: mesh compression, animation, scalability, real-time, low complexity
ACM CCS: I.4.2 [Image Processing and Computer Vision]: Compression (Coding)—Approximate methods;
I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Animation; I.3.5 [Computer Graphics]:
Computational Geometry and Object Modeling—Hierarchy and geometric transformations

photometric attributes like vertex normals, colours or texture
coordinates.

1. Introduction
Animated meshes are used in numerous domains including
character animation and animation movies, computer generated imagery, computer games, and scientific simulation
and visualization. For distribution and exchange of animated
meshes mostly a frame-based representation is used, since
this kind of representation is generic and independent of the
content creation process. Such a representation consists of a
sequence of static meshes. Each static mesh or frame comprises mainly two types of data, geometry and connectivity.
Mesh geometry specifies positions of all mesh vertices in
3D space and connectivity defines the mesh topology which
usually remains constant through all frames. Besides connectivity and geometry each frame can also have additional
c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and
Blackwell Publishing Ltd. Published by Blackwell Publishing,
9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main
Street, Malden, MA 02148, USA.

Streaming of digital content over fixed and mobile access networks gains ever increasing popularity. The advantage of successively decoding streamed data over first
downloading the complete data and then decoding is obvious. As soon as enough data has been received, the receiving device can already start with decoding and displaying
the content. However, appropriate coding algorithms have
to fulfill several requirements. Besides allowing for efficient compression, these algorithms also have to provide a
streamable representation which, in addition, allows to scale
bit streams to varying network transfer rates and end-user
devices.

101

102

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC

In this paper, we present a scalable predictive coding
(SPC) scheme for animated meshes with constant connectivity. SPC efficiently encodes an animated mesh in only
one pass through all frames. Coding is performed in a
streamable and scalable fashion. This means that the produced bit stream allows successive decoding of frames or
groups of frames as well as scalable decoding, i.e. successively only parts of the bit stream can be decoded. Partial
decoding then results in a successively reconstructed animated mesh with reduced frame rate and/or reduced spatial
resolution.
We review related work in the following section. In
Section 3, we establish our terminology and notation and
give a brief overview and motivation of our coding approach.
We proceed with a detailed description of the scalable structure in Section 4. Our coding approach is presented in detail in Section 5. Finally, we discuss experimental results in
Section 6 and conclude in Section 7.

2. Related Work
In the last decade, various approaches were presented for
compression of frame-based representations of animated
meshes with constant connectivity. Usually, all these approaches first encode connectivity, since it is constant
through all frames. Alliez and Gotsman give in [AG03] an
extensive survey on connectivity compression algorithms.
Here we want to emphasize that vertex-valence-based coding approaches [TG98, AD012] are asymptotically optimal
[AD01b, Got03], and they are regarded as state of the art.
However, with an increasing number of frames, animated
mesh geometry requires ever more storage space, and the
storage space occupied by connectivity becomes negligible. Hence, many proposed coding algorithms have the goal
to achieve high compression efficiency for geometry while
using one of the state-of-the-art approaches for encoding
connectivity.
Coding of patches. Animated meshes can often be decomposed in patches which show strong spatial and temporal
coherence. This is usually the case for animated articulated
characters, where, e.g. a patch describing a forearm performs
a coherent motion over time, i.e. it performs mainly a rigid
motion with small shape deformations. Thus, many recently
proposed compression algorithms encode animated meshes
by detecting and encoding patches which perform coherent
motion [Len99, SSK05, AS07, MZP06]. In his influential
work [Len99], Lengyel proposed a heuristic solution for detecting patches, i.e. sets of vertices which describe similar
motion over time. The temporal evolution of each detected
patch is defined by a set or cluster of vertex trajectories. Dependencies within such clusters of vertex trajectories are then
exploited for compression assuming that changes of patch
geometry from frame to frame can be accurately described
by affine transforms. Later in [SSK05] a more sophisticated

solution for clustering of vertex trajectories was proposed
based on principal component analysis (PCA) techniques.
For each defined cluster a low dimensional local basis is
determined and a more compact representation of all vertex trajectories is derived by representing them with respect
to their assigned basis. Instead of determining a local basis
for each cluster of vertex trajectories, the authors propose in
[AS07] to determine for each patch in each frame a separate
local basis. Recently, Mamou et al. introduced in [MZP06]
an improved clustering and motion compensation strategy
based on a skinning approach. Similarly to [Len99], they
first determine patches and corresponding affine transforms,
which approximate the frame-wise motion of each patch. In
a second step, frame-wise motion of each vertex is described
as a weighted average of previously determined affine transforms. This approach allows to compactly describe a smooth
transition of motion over patch boundaries. After compensating motion, remaining residuals are transformed in temporal
direction using a discrete cosine transform, and transform
coefficients together with affine transforms and weights are
encoded.
All these approaches allow for efficient compression.
However, none of them is progressive or scalable. Some
of these approaches [SSK05, MZP06] do not allow framewise decoding making them applicable only in downloadand-play scenarios. In addition, corresponding encoders are
usually computationally demanding and memory intensive
because of the clustering procedure used.
Coding of key-shapes. A different compression approach
is based on the idea that the geometry of every frame of
an animation can be represented as a linear combination of
so-called key-shapes [Len99]. In order to achieve good compression with this approach, it is required that each frame can
be approximated by using only a low number of key-shapes.
In [AM00], the authors derive a basis of key-shapes by using
PCA. They apply a singular value decomposition (SVD) to
the matrix consisting of the geometry of the complete animation. Hence, an orthogonal basis of the frame-space and
the corresponding singular values are obtained. These values indicate the average contribution of a basis vector to the
quality of the geometry of a frame. Projecting the geometry
of each frame to those basis-vectors with the highest singular
values results in so-called PCA coefficients, which provide
a compact representation of geometry. In [KG04], the authors proposed to employ linear predictive coding (LPC) to
exploit dependencies between PCA coefficients for further
compression.
PCA-based key-shape approaches provide very good compression results for a small class of animated meshes, namely
animated meshes which mainly exhibit repetitive motion like
changing facial expressions in a facial animation. This is due
to the usage of a fixed small set of basis vectors which can
accurately describe only animations with limited variability
from frame to frame.

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

103

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC

Predictive coding. This class of compression approaches is
based on the assumption that animated meshes show strong
dependencies in a small spatio-temporal neighbourhood.
Ibarria and Rossignac [IR03] first presented a compression
approach of this type. They propose to employ a deterministic
region-growing algorithm to determine a vertex encoding order. Vertex positions are predictively encoded. The predicted
position for a vertex is calculated as a particular linear combination of already encoded neighbouring vertex positions of
the current and the previous frame. Later, Stefanoski and Ostermann [SO06] presented a related compression approach
using an angle-preserving predictor. This predictor is based
on the assumption that the dihedral angle between neighbouring triangles remains invariant from frame to frame. Compared to the previous approach [IR03], prediction accuracy is
significantly improved. A progressive approach for encoding
animated meshes was proposed by Guskov and Khodakovksy
[GK04]. Their approach decorrelates each frame spatially
from top to bottom using a spatial hierarchy and a wavelet
transform. Wavelet coefficients are compressed using differential coding. Although this approach is progressive, it
does not support scalable decoding. Partial decoding by neglecting bit planes results in error accumulations which can
lead to strong artifacts in the reconstructed animated mesh.
Recently, Stefanoski et al. [SLKO07, SKLO07] presented
a predictive coding approach supporting spatial scalability.
They propose a patch-based mesh-simplification algorithm to
derive a decomposition of connectivity in spatial layers. Local spatial and temporal dependencies between neighbouring
spatial layers and consecutive frames are exploited by estimating local motion. Remaining errors are independently
encoded generating an embedded bit stream which allows
spatially scalable decoding.
Standardization. The Moving Picture Experts Group
(MPEG) recently adopted a new standard for compression of
animated meshes referred to as Frame-based Animated Mesh
Compression (MPEG-4 FAMC) [MPE08, MSZ∗ 08, SO08].
It is based on the spatially scalable predictive approach of
Stefanoski et al. [SLKO07] and the skinning-based approach
of Mamou et al. [MZP06]. MPEG-4 FAMC supports several
modes of operation, e.g. targeting only efficient compression, which is relevant for download-and-play applications,
or in addition, employing several types of scalability, which
is extremely useful for streaming of animated meshes.
Our contribution. Locally rigid motion is present in many
animated meshes. We efficiently exploit locally rigid motion for efficient scalable compression. We employ predictive
coding in the space of rotation-invariant coordinates which
are calculated with respect to a spatially and temporally
scalable decomposition of the animated mesh. The scalable
decomposition which we employ is based on our previous
work [SLKO07, SKLO07] and it was recently adopted in the
MPEG-4 FAMC standard [MPE08, SO08]. The compression
efficiency of our scalable coding scheme is evaluated based
on various animated meshes. We report superior coding ef-

ficiency in comparison to the streaming mode and the scalable mode of FAMC [MPE08, MSZ∗ 08, SO08] as well as in
comparison to other non-scalable state-of-the-art approaches
[IR03, KG04, SSK05, GK04]. Our scalable approach has
also the advantage of enabling fast encoding and decoding
and requiring only low memory.
3. Overview
Before giving an overview of our coding approach we briefly
fix some definitions and notation used throughout the paper.
3.1. Definitions and notation
An animated mesh (K, G) consists of mesh connectivity K
and geometry G. Mesh connectivity K specifies the topology consisting of vertices, edges and triangle faces. More
formally, K is the union of the sets of vertices V and edges E
V := {1, . . . , V},
E := {{i, j } : i, j ∈ V and i, j are connected}.
We denote the 1-ring neighbourhood of a vertex v with respect to connectivity K as NK (v) := {w : {v, w} ∈ E} and
call the corresponding number of neighbouring vertices
|NK (v)| the vertex valence. In addition, we define the set
of all frame indices
F := {1, . . . , F}.
Geometry is represented by a matrix
G := (gij ) ∈ R3V×F
with

⎛

g3v−2,f

⎞

⎟
⎜
pfv := ⎝ g3v−1,f ⎠
g3v,f
indicating the position in 3D space of vertex v in frame f . Let
us denote by Gf ∈ R3V the f th column vector of G specifying
the geometry of frame f . Thus, the sequence G1 , . . . , GF
describes the temporal evolution of vertex positions of the
animated mesh (K, G).
3.2. Our approach
SPC encodes animated meshes and generates a spatially and
temporally scalable bit stream. A spatially scalable bit stream
is illustrated in Figure 1. Encoded bits of each frame are organized in spatial layers. A decomposition in spatial layers
allows to decode a reduced number of layers, which then
results in an animated mesh with reduced spatial resolution.
SPC supports also temporal scalability. Similar to spatial
scalability, a partial decoding of temporal layers results in
an animated mesh with a reduced frame rate, i.e. temporal

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

104

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC

Figure 1: Illustration of a spatially scalable bit stream.

resolution. The decomposition of an animated mesh in spatial and temporal layers is described in detail in Section 4. In
Section 5, the exploitation of intra and inter layer dependencies is described for obtaining an efficiently compressed bit
stream.
Spatial and temporal scalability are useful when an animated mesh has to be efficiently stored on storage media
in order to be distributed and later decoded and displayed
on various types of display devices. In this scenario, every
device can decode that part of the bit stream which leads
to an animated mesh with that frame rate and that spatial
resolution which best meets the display specifications or the
power consumption of the rendering device. A scalable bit
stream is also useful for streaming, since it allows to vary
the bit rate by transmitting a reduced number of spatial and
temporal layers. The number of transmitted spatial and temporal layers can be adapted over time in order to adapt the
transmission bit rate to varying network transfer rates. In addition, SPC performs only one pass through all frames of an
animated mesh, i.e. frames are successively encoded without
having a global view to all frames of the animation. With this
property and its real-time encoding/decoding performance,
SPC can be also used for efficient online encoding/decoding,
i.e. successively frames or small groups of frames can be
encoded in real-time and transmitted to the decoder.

4. Spatial and Temporal Scalability
The goal of scalability is to generate a bit stream that can be
partially decoded allowing a reasonable reconstruction of an
animated mesh. We achieve spatial and temporal scalability
by hierarchically decomposing the set of vertices V and the
set of frame indices F.
4.1. Hierarchical spatial decomposition
An important part of our coding algorithm is the construction
of a spatial hierarchy of an animated mesh. We successively
simplify mesh connectivity K creating a spatial hierarchy
(Kl )Ll=1 which is employed for scalable coding.

In our coding approach connectivities (Kl )Ll=1 are used to
guide coding of geometry of each frame Gf . Moreover, they
are used for structuring coded data in spatial layers. Because
these connectivities have to be determined before starting
with encoding geometry, we employ an approach which allows to construct connectivities (Kl )Ll=1 directly from the
initial connectivity KL := K, without exploiting geometry.
Thus, no bits have to be sent to the decoder for describing
connectivities (Kl )Ll=1 if connectivity K is encoded first. Our
primary goal is to construct a series of simplified connectivities whose vertices have a balanced number of neighbouring
vertices, i.e. dispersion of vertex valences around the average valence 6 should be low. There are two major reasons for
selecting such an approach. First, preventing the creation of
vertices with large valences ( 6) in the simplified connectivity prevents the creation of long narrow triangles which
are often visually unpleasant, and second, from the coding
point of view, large vertex valences also lead to many valence
3 vertices, which prevent an efficient predictive coding of geometry (Section 5). Alliez and Desbrun showed in [AD01a]
that removing vertices of valence ≤ 6 maintains a low statistical valence dispersion in the simplified connectivity.
Based on ideas for progressive compression of static
meshes presented in [AD01a], we create connectivities
KL , . . . , K1 by repeatedly employing a patch-based simplification algorithm which performs the simplification step
Kl → Kl−1 . This algorithm consists of two parts: patch decomposition and simplification.
Patch decomposition. A valence-d patch consists of a set
of triangles incident to a vertex of valence d. Patch decomposition determines a series of non-overlapping patches of
valence 6 or lower (Figure 2a) using a patch-based regiongrowing traversal algorithm [SLKO07]. The obtained set of
middle patch vertices represents an independent vertex set
which is denoted by Wl := {wil }. Because of the average
vertex valence 6, maximally 6/7 of all vertices of connectivity Kl have a valence > 6, i.e. at least 1/7 of all vertices have
a valence ≤ 6. In practice this fraction is significantly higher.
In our experiments Wl contained about 30% of the vertices
of Kl .
Simplification. During simplification all middle patch vertices wil ∈ Wl are removed (Figure 2b). This is done in
l
. Thereby, each time
the determined order, i.e. w1l , . . . , w|W
l|
after removing a middle patch vertex wil of valence d a
simple d-gon remains which is subsequently triangulated
(Figure 2c). Please note that only triangles, quads, pentagons
and hexagons can remain (d = 3, 4, 5, 6). In general, the
number of different triangulations of a d-gon depends on
the number of its vertices d. We define the set of all possible triangulations of a d-gon for d = 4, 5, 6 as Td := {τid }
(Figure 3). In order to select a triangulation τ ∈ Td for a dgon we apply a cost function (wil , τ ) which measures the
triangulation cost. We process all triangulations in Td (maximally 14) and select that triangulation τ ∈ Td which leads

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

105

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC

Figure 2: Overview of the patch decomposition and simplification process. First patches are determined (a), then middle patch
vertices are removed (b), and finally remaining polygons are triangulated (c).

Figure 3: Triangulations for patches of valences 4, 5 and 6
to the minimal cost. The triangulation cost is the cumulative
deviation of all valences of vertices in the neighbourhood
NKl (wil ) from the average valence 6 after having removed
vertex wil and applied triangulation τ
μ v, wil , τ − 6 .

wil , τ :=
v∈NKl (wil )

Here μ(v, wil , τ ) denotes the valence of vertex v after removing the middle vertex wil and triangulating the remaining
polygon using triangulation τ . Note that different triangulations can lead to different deviations, because valences of
vertices v change depending on the triangulation τ . We select that triangulation τ for triangulating a polygon, which
leads to the smallest deviation (wil , τ ). Overall, this kind
of selection favors the creation of vertices v with a balanced
number of neighbours.

Successive application of patch decomposition and simplification results in a series of connectivities {Kl }Ll=1 which
define a spatial hierarchy (Figure 4). First, patch decomposition and simplification is applied to the initial connectivity
KL := K. Thus, vertices v ∈ WL are removed and a simplified connectivity KL−1 consisting of vertices VL−1 = VL \WL
remains. Iterating this procedure, we obtain simplified connectivities Kl with Vl = Vl+1 \Wl+1 for l = L − 1, . . . , 1.
For the purpose of completeness, let us define W1 := V1 .
Please note that patch-based simplification guarantees that
the neighbours of each middle patch vertex w ∈ Wl+1 are
located in Vl = ∪lk=1 Wl , i.e.
NKl+1 (w) ⊂ Vl .
Obviously, sets (Wl )Ll=1 define a partition of the set of all
vertices of our initial connectivity K, i.e.
V = VL = W1 ∪ . . . ∪ WL
with Wi ∩ Wj = ∅ for i = j . This partition is used for structuring geometry in spatial layers.
A spatial layer is defined by a vector Gl ∈ R3|Wl | which
stacks all coordinates of vertex positions {pfv }v∈Wl . The geometry Gf of a given frame f will be partitioned and encoded
layer-wise in order
f

f

f

f

G1 , . . . , Gl , . . . , GL

(1)

producing a spatially scalable bit stream.

Figure 4: The first frame of the cow animation in original resolution (a) and simplified versions obtained after one (b) three
(c) and seven (d) iterations of decomposition and simplification.
c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

106

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC

Figure 5: Illustration of geometry G partitioned in spatial
layers, frames and groups of frames of size 8.
4.2. Hierarchical temporal decomposition
Besides constructing a spatial hierarchy, we construct also a
temporal hierarchy of an animated mesh. We subdivide the
set of all frame indices F into groups of frames (GOFs), and
define within each GOFs a hierarchical frame order. Encoding frames in this order will allow to structure coded data in
temporal layers. Hierarchical frame orders are employed in
video coding [SMW06] for supporting several layers of temporal scalability. We adapt this approach to animated meshes.
We define a GOFs
F k := {1 + k2T + n | n = 1, . . . , 2T }
as a set of 2T consecutive frames starting at frame k2T + 2
(Figure 5). Hence, sets F k represent a partition of the set of
all frames excluding the first frame1 . Now we define subsets
of each GOFs F k
Ftk := {1 + k2T + n2T −t | n = 1, . . . , 2t }

This hierarchical sub-sampling structure allows us to construct a partition of F k defining temporal layers
k
Htk := Ftk \Ft−1
for t = 1, . . . , T

(Figure 6b). All these subsets have the property
F k = H0k ∪ . . . ∪ HTk
Hik

with
is equal

∩ Hjk = ∅
to ∪ti=0 Hik .

for i = j . Furthermore, each subset

Ftk

Hence, the geometry of all frames of F k is encoded in a
hierarchical order
H0k , . . . , Htk , . . . , HTk .

1

Altogether, hierarchical temporal decomposition first subdivides the set of frames F in GOFs F k and then partitions
each F k in subsets (Htk )Tt=0 defining temporal layers. On the
other hand hierarchical spatial decomposition, in addition,
partitions the geometry Gf of each frame f ∈ Htk in vectors
f
(Gl )Ll=1 defining spatial layers.
5. Scalable Predictive Coding

for 0 ≤ t ≤ T (Figure 6a). Obviously, FTk is equal to our
initial set F k and each set Ftk for t < T can be obtained by
k
. Consequently, by displaying
dyadic sub-sampling of Ft+1
k
all frames of Ft we in fact display a GOFs with a reduced
frame rate.

H0k := F0k ,

Figure 6: Decomposition of the geometry of a group of
frames F k into (a) subsets of different frame rate and (b)
subsets representing different temporal layers. Blue frames
are part of the subsets.

(2)

Because of the finite number of frames, frames at the end of
the animated mesh are grouped in GOFs of smaller size (2c with
c < T ).

Given an animated mesh (K, G), first connectivity K is losslessly encoded. We use the valence-based coder of Touma and
Gotsman [TG98]. Subsequently, a sequence of connectivities
K1 , . . . , KL with KL = K is determined by iteratively applying patch-based decomposition and simplification presented
in Section 4.1. This sequence describes a spatial hierarchy
which is employed for predictive encoding of geometry G.
Geometry is lossily encoded. Hence, in the following we
will distinguish between the original geometry G and points
¯ and
pfv and lossily encoded and then decoded geometry G
distorted points p¯ fv .
Frame coding order. First, the geometry of the first frame is
predictively encoded. Then following GOFs are successively
encoded. The geometry of each GOFs is encoded in the hierarchical frame order (2). Thereby, the predictive coding
paradigm is employed using the prediction structure shown
in Figure 7. Already encoded frames (reference frames) are
used for prediction of the current frame. First, the base temporal layer consisting of the last frame of the GOFs is predictively encoded. It is predicted based on an already encoded
frame, which is either the last frame of the previous GOFs
(Figure 8a) or the first frame of the entire animated mesh
(Figure 8b). For prediction of frames of higher temporal layers in addition, already encoded frames of lower temporal

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC

107

even simple motion from frame to frame like global translation or rotation results in changes of almost all vertex positions. However, for efficient coding we are interested in
representations of geometry which remain almost invariant
for a large class of mesh deformations.

Figure 7: Hierarchical coding of a group of frames. Arrows
originate at encoded frames (reference frames) which are
used for predictive coding of dashed frames.

layers of the same GOFs are used. Hence, according to
Figure 7 the frame of the base temporal layer is onedirectionally predicted while frames of higher temporal layers are always bi-directionally predicted in hierarchical order.
Without loss of generality let us consider frame Gf being
f
currently encoded. Gf is partitioned in spatial layers (Gl )Ll=1
and all spatial layers are predictively encoded using the same
f
reference frames. First, the base spatial layer G1 is encoded.
Due to its efficiency, we employ the single-resolution approach [SO06]. Subsequently, each higher spatial layer is
transformed in the space of rotation-invariant coordinates
exploiting dependencies between spatial layers. Rotationinvariant coordinates are explained in the following section.
f
They represent spatial layer Gl with respect to the encoded
f
l−1
¯ i }i=1 and allow for an efficient additional
spatial layers {G
exploitation of dependencies between neighbouring temporal layers by predictive coding. Predictive coding in the space
of rotation-invariant coordinates is explained in Section 5.2.
5.1. Rotation-invariant coordinates
In their canonical representation all vertex positions pfv are
represented in a common global coordinate system. Hence,

Recently, such representations have been subject of increasing attention in the area of mesh editing and modeling
[SZGP05, SCO04, LSCO∗ 04]. Almost invariant representations of geometry are used to preserve local shape properties of a mesh when performing editing operations like
moving particular handle vertices. Differential coordinates
in general and Laplacian coordinates in particular allow the
preservation of details. In [SCO04] an input mesh is approximated with a least-squares mesh which has differential coordinates similar to the input mesh and meets some
editing constrains. However, differential coordinates are represented in a global coordinate system and consequently
lead to an acceptable shape preservation if editing operations involve only small local rotations of the input mesh
[ATFL05].
In our approach we represent spatial layers using rotationinvariant coordinates. We obtain rotation-invariant coordinates by representing Laplacian coordinates in corresponding local coordinate systems. Rotation-invariant
coordinates based on local coordinate systems were also used
in [KCVS98] in a framework for interactive multi-resolution
f
modeling. Let us assume that spatial layer Gl is our current
spatial layer which is encoded next, while coarser spatial
layers of the same frame are already encoded and known at
¯ fi }l−1
the decoder. We use encoded spatial layers {G
i=1 to repf
resent spatial layer Gl with rotation-invariant coordinates.
f
Spatial layer Gl is a vector which stacks the coordinates of
all vertex positions pfv with v ∈ Wl . All these vertex positions are needed to increase the spatial resolution in frame
f from a connectivity Kl−1 to a connectivity Kl . Let us recall that all vertices in the 1-ring neighbourhood of a vertex
v ∈ Wl with respect to connectivity Kl are also part of the
coarser connectivity Kl−1 . Because coarser spatial layers are
already encoded and known at the decoder we can use reconstructed vertex positions p¯ fw from the 1-ring neighbourhood
of vertex v to differentially represent vertex position pfv . To
simplify matters we use the abbreviation N (v) := NKl (v)
for the set of vertices in the 1-ring neighbourhood of vertex v ∈ Wl and we abbreviate with N := |NKl (v)| the number of vertices of this set. The three Laplacian coordinates

Figure 8: Illustration of a short term (a) and long term (b) prediction structure.
c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

108

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC
f
f
f
(dv,1 , dv,2 , dv,3 )

dfv =
according to

of vertex position pfv are calculated

dfv := pfv −

1
N

p¯ fw .

(3)

w∈N (v)

Thus, they represents the deviation of position pfv from the
centroid of its reconstructed 1-ring neighbourhood. Similar,
we define the corresponding Laplacian coordinates d¯ fv which
are calculated using only reconstructed vertex positions
1
d¯ fv := p¯ fv −
N

p¯ fw .

Figure 9: Approximation of 1-ring vertex positions using
samples of a skew ellipse. Ellipse radii x˜ and y˜ are used for
defining a local coordinate system.

(4)

w∈N (v)

radii x˜ and y˜ can be explicitly calculated according to
Laplacian coordinates are defined in the global coordinate
system. Thus, they are sensitive to mesh deformations other
then translational motion. Additional invariance against local rotational motion is achieved by representing them in
local coordinate systems. In order not to encode additional
data, local coordinate systems are derived from reconstructed
vertex positions of coarser spatial layers.
We calculate a local coordinate system mfv ∈ R3×3 for
each vertex v ∈ Wl in frame f based on reconstructed vertex positions p¯ fw with w ∈ N (v). Let w1 , . . . , wN represent
vertices in the 1-ring neighbourhood of vertex v in counterclockwise order with w1 := min (N (v)). Our goal is to determine local coordinate systems that lead to rotation-invariant
coordinates, i.e. they should have the property

x˜ =

N

2
N

y˜ =

sin(ωk) p¯ fwk ,
k=1

N

cos(ωk) p¯ fwk ,
k=1

while the center c˜ is equal to the centroid of points p¯ fwk . In
fact, radii x˜ and y˜ represent the Fourier coefficients of the
fundamental frequency 2π/N of the sequence p¯ fw1 , . . . , p¯ fwN .
Hence, they are insensitive against medium and high frequency distortions. Radii x˜ and y˜ can also be characterized
as spanning a tangent plane on a limit subdivision surface
which is obtained by Loop subdivision of the control mesh
¯ fl ) [HDD∗ 94].
(Kl , G
Now we derive orthonormal basis vectors for each vertex
v ∈ Wl in frame f according to
nfv =

T

mfv dfv = mrv T drv

2
N

x˜ fv × y˜ fv
f
x˜ v

×

f
y˜ v

,

x˜ fv

ufv :=

f
x˜ v

,

vfv :=

nfv × ufv
f

f

nv × u v

.

in the case that vertex positions prv , prw1 , . . . , prwN perform
pure rigid motion from frame r to frame f . However, we can
use only distorted vertex positions p¯ fw1 , . . . , p¯ fwN for specifying mfv , because only they are known at the encoder and
decoder.

Hence, the orthogonal matrix mfv := ufv vfv nfv represents
the local coordinate system assigned to vertex v ∈ Wl in
frame f (Figure 9). Consequently, the map

We specify mfv employing an approach which is robust
against medium and high frequency distortions. Undistorted
1-ring vertex positions pfwk usually lie close to a plane, i.e.
they tend to describe a nearly planar polygon. We use distorted vertex positions p¯ fwk to robustly estimate a basis x˜ , y˜
which spans this plane and subsequently use this basis to
derive a local coordinate system mfv .

transforms Laplacian coordinates dfv to their assigned local coordinate system giving rotation-invariant coordinates.
Please note that these coordinates are invariant against local
rigid motion if there are no low frequency distortions in the
reconstructed vertex positions.

We determine center and radii of a skew ellipse whose
samples best approximate points p¯ fwk in the least-squares
sense

Dl ∈ R3|Wl |

N

c + sin (ωk) x + cos (ωk) y − p¯ fwk

c˜ , x˜ , y˜ = arg min
c,x,y

2

k=1

with ω := 2π/N (Figure 9). Thus, radii x˜ and y˜ span a plane
in 3D space. Because of the orthogonality of sine and cosine,

T

mfv dfv

For convenience we define vector
f

which stacks all Laplacian coordinates of dfv with v ∈ Wl .
¯ fl consisting of Laplacian coordiSimilar we define vector D
f
¯
nates dv . Furthermore, we define matrix
Ml ∈ R3|Wl |×3|Wl |
f

which has a block-diagonal structure and whose blockdiagonal consists of all 3 × 3 matrices mfv for v ∈ Wl . Matrix

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

109

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC
f
Ml

is orthogonal by construction. Hence, mapping
fT

f

(5)

Ml Dl

gives a vector which stacks all rotation-invariant coordinates.
Hence, Laplacian coordinates decorrelate each frame spatially exploiting dependencies between spatial layers. A
subsequent transform in the space of rotation-invariant
coordinates allows an efficient additional exploitation of dependencies between neighbouring temporal layers using predictive coding.

Decoding. At the decoder, quantization indices are decoded
¯ fl . Disand dequantized giving distorted prediction errors E
tortions are due to the loss of accuracy caused by quantization at the encoder. Encoder and decoder perform the same
f
prediction. Dequantized prediction errors E¯ l are used to correct the prediction and to reconstruct geometry. According to
Equation (7) a vector of reconstructed Laplacian coordinates
¯ fl := Mfl Ufl + E¯ fl
D

is obtained. Finally, from Equation (4) the reconstruction
formula
1
p¯ fv := d¯ fv +
p¯ f .
N w∈N (v) w

5.2. Predictive coding
The geometry of each frame is encoded layer-wise. After
f
encoding base layer G1 with approach [SO06], higher spatial layers of frame f are subsequently encoded according
to the predictive coding paradigm. Each higher spatial layer
is first transformed in the space of rotation-invariant coordinates. These coordinates are then predicted based on already
encoded coordinates of corresponding spatial layers of reference frames. Finally, the prediction errors are quantized and
entropy encoded.
Encoding. Let Rf ⊂ F define the set of reference frames
assigned to some frame f according to Figures 7 and 8.
First, we determine rotation-invariant coordinates according
to Expression (5). Now we predict them using local coordinate systems and Laplacian coordinates of reference frames
that are known at the encoder and decoder according to
f
Ul

:=
r∈Rf

1
¯ rl .
Mr T D
|Rf | l

(6)

Note that for the first frame f = 1 the set of reference frames
f
Rf is empty. In this case the prediction Ul is the zero vector.
Here we want to emphasize that this prediction procedure is
based on the assumption that locally vertex positions change
almost rigidly from frame to frame, resulting in almost invariant or slowly changing rotation-invariant coordinates. Consequently, rotation-invariant coordinates can be accurately
predicted by interpolation using corresponding coordinates
of neighbouring already encoded frames (Equation (6)).
Subsequently, prediction errors
f

fT

f

f

El := Ml Dl − Ul

q(e) = e/

(9)

is derived which is used to reconstruct all vertex positions of
¯ fl .
spatial layer G
6. Experimental Results
The presented SPC encodes an animated mesh in only one
pass through all frames and thereby produces an embedded
bit stream structured in temporal and spatial layers. In the
following, we evaluate the coding efficiency of SPC. We
evaluate its performance on a test data set consisting of the
animated meshes horse gallop, dance, cow, face and jump
(Table 1). A demo version of our decoder and encoded animated meshes are available at our project page [SPC00].
The encoding performance of SPC is affected by 4 parameters: (1) the number of temporal layers (TL), (2) the number
of spatial layers (SL), (3) the quantization step size , (4) and
whether long term (LT) or short term (ST) reference frames
are used for the base temporal layer (Figures 8a and b).
We parameterize relative to the length d of the diagonal
of the first frame according to q := d/2q . In the following
all sequences encoded with SPC are using parameter 16 for
quantizing the first frame, while following frames are quantized with some fixed q0 with q0 ∈ {6, . . . , 16}.We observe
that for 12 quantization indices obey a peaked symmetric distribution close to a Laplacian distribution, while with
coarser quantization the distribution gets more similar to a
uniform distribution.

(7)

are calculated and quantized. Each coordinate of prediction
f
error vector El is uniformly quantized according to
q : R → Z,

(8)

+ 0.5

with being the quantization step size controlling the loss of
f
accuracy. Finally, the integer vector Ql consisting of quantization indices is entropy encoded. All quantization indices
are encoded bit-plane-wise using the fast binary arithmetic
coding approach proposed by Marpe et al. [MSW03].

Table 1: Overview of animated meshes used for evaluation.

Name
Horse gallop
Dance
Cow
Face
Jump

Number of vertices

Number of frames

8431
7061
2904
539
15 830

48
201
204
10 001
222

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

110

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC

Please note that an error caused by quantizing original
vertex positions with a quantization step size larger or equal
to 12 is usually regarded as visually lossless. We express the
bit rate of geometry in bits per vertex and frame (bpvf), which
is equal to the size of the generated bit stream expressed in
bits divided by the number of vertices and the number of
frames.
6.1. Analysis of SPC
We analyze the impact of different encoder settings on the
bit rate based on the animated mesh dance. For the other
animated meshes of the test data set, we obtain comparable
results. All coding results in this subsection are obtained by
using quantization parameter 16 for the first frame, while
all following frames are encoded using 12 .
The impact of the usage of rotation-invariant coordinates
on the coding efficiency is illustrated in Figure 10. Each curve
shows the number of bits required for encoding frames in
display order using different settings for the encoder. We encoded the animation using rotation-invariant coordiantes (ric)
as well as without using them (no ric). Not using rotationinvariant coordinates means that an identity matrix is used for
f
matrix Ml in Equations (6) and (7). In addition, we show coding results using different settings for the number of temporal
layers (TL) as well as for the type of reference frames for the
base temporal layer, i.e. long term (LT) or short term (ST)
reference frames. Note that for the setting TL = 1, frame
encoding order and frame display order are the same. In
Figure 10, we see that when encoding all frames independently of each other (no reference frames), for each frame
about 120 kbit are required. In comparison, the encoding of
all frames without using rotation-invariant coordinates and
with using the first frame as reference frame for all frames
(no ric, TL = 1, LT) leads to significant gains only for frames
temporally close to the reference frame. However, curve ric,
TL = 1, LT shows that using rotation-invariant coordinates

coding gains of about 50% can be obtained for all frames,
even for frames at a large temporal distance from the reference frame. In particular, we see that up to frame 10
the number of required bits per frame is increasing up to
51 kbit/frame, while for frames starting from frame 11, 5164 kbit/frame are required independently of the temporal
distance to the reference frame. Hence, the usage of ric significantly contributes to a better exploitation of long term
dependencies. In addition, this result gives an indication that
there is almost no difference if LT or ST reference frames
are used for encoding the base temporal layer, if the temporal distance between these frames is larger than 10 frames.
However, encoding with a higher number of temporal layers
allows an additional exploitation of short term dependencies
within each GOFs. Curve ric, TL = 4, LT shows that using
4 TLs, an additional coding gain is obtained compared to
using 1 TL. The same result but showing the average bit rate
per GOFs is shown by curve ric, TL = 4, LT (avg). Hence,
coding gains of about 50% are obtained compared to using 1
TL, which leads to an average bit rate of 26 kbits/frame.
The result of encoding all frames in display order using for
each frame the previous frame as reference frame is shown
by curve ric, TL = 1, ST. Obviously, an encoding using 4 TLs
(and using LT reference frames) leads to a stronger exploitation of short term dependencies within GOFs. Thus, coding
gains of 18% in average are obtained compared to using 1
TL and using only ST reference frames. Here we want to
emphasize that using LT reference frames has the additional
advantage that it also allows to start the decoding process at
an arbitrary GOFs if first the reference frame is transmitted
(this allows random access in streaming applications), while
when using only ST reference frames a random access is not
possible.
In our experimetns, we observe that a higher number of
spatial and temporal layers leads to an increased coding efficiency. However, we also notice that in the domain above
eight spatial and four temporal layers coding gains become
negligible.

Dance

160
140

6.2. Comparative analysis

120

We evaluate the compression performance of SPC in comparison to different modes of operation of the MPEG-4 FAMC
encoder [MSZ∗ 08]. We distinguish between

kbit / frame

100
80
60
40
20
0

2

12

22

32

42

52

62

72

82

92

102

112

122

frame

no reference frames
no ric, TL=1, LT
ric, TL=1, LT

ric, TL=4, LT
ric, TL=4, LT (avg)
ric, TL=1, ST

Figure 10: Impact of the usage of rotation-invariant coordinates and temporal scalability on the coding efficiency.

• download mode (labeled as FAMC (download)), which
shows best compression efficiency at the expanse of features like streaming and scalability. This mode is mainly
applicable in download and play scenarios.
• streaming mode (labeled as FAMC (streamable)), which
successively encodes groups of frames. This mode is
based on the download mode. We use in this mode the
same GOFs size as for SPC.

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

111

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC

Figure 11: Operational rate-distortion curves for animated meshes horse gallop, dance, cow and face. The gray shaded area
specifies the domain of visually lossless errors.

• scalable mode (labeled as FAMC (scalable)), which structures coded data in a spatially and temporally scalable
representation. We use in this mode the same number of
spatial and temporal layers as for SPC.
In addition, we evaluate the performance of SPC against
the state-of-the-art approaches AWC [GK04], Dynapack
[IR03], CPCA [SSK05], and KGC [KG04]. For comparison with other approaches than FAMC, we use results published in respective papers, since we have no executables or
implementations of these approaches. Please note that only
SPC and the scalable FAMC allow scalable decoding. The
wavelets-based approach AWC and the PCA-based approach
KGC are progressive but not scalable.
In Figure 11 comparative results are shown using operational rate-distortion curves illustrating the trade-off between

bit rate and distortion. SPC encodes all sequences of the test
data set using eight spatial and four temporal layers (this
corresponds to using a GOFs size of 8), while different operating points on the SPC curves are obtained by varying
the quantization parameter q0 with q0 ∈ {6, . . . , 16}. The
bit rate is expressed in bpvf and distortions between original and reconstructed animated meshes are measured using
the KG-error metric [KG04]. This metric corresponds to a
relative discrete L2 distance. It is calculated according to
¯ = 100
dKG (G, G)

¯
G−G
,
˜
G−G

with . representing the Frobenius norm while each frame
˜ f is a vector stacking V copies of the frame centroid
G
1/V Vv=1 pfv . Please note that a KG-error of approximately
0.05 corresponds to error caused by quantizing original

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

112

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC

Figure 12: Examples of original and corresponding reconstructed frames obtained from animations encoded with SPC at high
and low bit rate, respectively. (a–c) Frame 8 of animation horse gallop. (e–f) Frame 115 of animation dance. (g–i) Frame 108
of animation cow. (j–l) Frame 5000 of animation face.
vertex positions with a quantization step size of nearly
which is usually regarded as visually lossless.

12

The horse gallop animation (48 frames and 8431 vertices)
exhibits mainly articulated motion while the dance animation (201 frames and 7061 vertices) shows a combination of
articulated and global rotational motion. In Figures 12(a)–(f)
reconstructed frames of these animations are shown. According to Figures 11(a) and (b), our approach outperforms the
streaming mode of FAMC with coding gains of over 33% at
an error of 0.05. Compared with the scalable FAMC, SPC
achieves a coding gain of 2% for the horse gallop animation
while it loses 8% in bit rate in the case of the dance animation.
Here we want to emphasize that in the download mode as
well as in the scalable mode, FAMC derives a global motion
model by analyzing the entire animated mesh before starting
the encoding process. This requires several passes through
all frames. Hence, although the scalable FAMC employs a
global motion model this does not always lead to a gain in
comparison to SPC. SPC is not using such a global view. It
sequentially encodes the data in one pass exploiting locally
rigid motion. It adapts frame by frame to changes of geometry by exploiting dependencies to temporally close reference
frames in a predictive way. This approach enables fast and
efficient encoding using only a fraction of the memory re-

quired by FAMC. On the other hand, the download mode of
FAMC leads to significant gains at the expense of streaming
and scalability. We attribute this result to the higher freedom
to exploit dependencies in the signal, since in the download
mode, FAMC does not have to maintain a streamable and
scalable structure like SPC.
The third animated mesh of the test data set, the cow animation (204 frames, 2904 vertices), is modeled by physically
based animation and shows fast motion and strong local deformations. In Figure 11(c), we see that for this sequence
SPC and FAMC in download mode show comparable coding
efficiency while the scalable FAMC is outperformed by our
approach in the low bit-rate domain with coding gains of up
to 20%. The streaming mode of FAMC, the wavelet-based
approach AWC as well as the two PCA-based approaches
CPCA and KGC are outperformed by SPC with gains of
over 16%. Here, only SPC and AWC encode the animation
in one pass. This result demonstrates the strength of our scalable approach to fast adapt to strong frame-wise changes of
geometry. In Figures 12(g)–(i) corresponding reconstructed
frames are shown.
The compression results for the face animation (10001
frames and 539 vertices) are presented in Figure 11(d)

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC

while corresponding reconstructed frames are shown in
Figures 12(j)–(l). This animation shows temporally varying
facial expressions. The potentially huge degree-of-freedom
given by the large number of frames and the limited number of different facial expressions which actually appear in
the animation, make PCA-based approaches very attractive.
In addition, the low spatial resolution prevents an efficient
exploitation of spatial dependencies by our approach. Consequently, CPCA and KGC show significant coding gains
compared to SPC as well as to Dynapack, AWC, and all
modes of FAMC. However, except for SPC and the scalable
FAMC all other approaches are not scalable. SPC achieves
a 6% higher coding efficiency than the scalable FAMC at
an error of 0.05. We believe that the coding efficiency of
SPC can be significantly improved for animations having
this type of repetitive nature by selecting reference frames
based on the content of the animation. However, that would
mean that the fixed reference frame-structure which is used
by our approach for providing temporal scalability has to be
extended.
A comparison between FAMC and SPC based on other
animated meshes led to comparable results. We tested our
coding approach also using an prediction procedure similar to equation (6), which uses adaptive weights for reference frames instead of 1/|Rf |. However, that did not lead to
significant gains. In addition to the short term reference frame

113

structure (Figure 8a) which we used so far, we also evaluated
our approach using the long term reference frame structure
(Figure 8b). Long-term reference frames allow for random
access to GOFs by causing an overhead in bit rate of maximally 2% in our test data set.
Besides reconstructing an animated mesh in its full temporal and spatial resolution, our scalable approach allows also to
decode only parts of the embedded bit stream. This can result
in a reconstructed animated mesh with a reduced frame rate
(Figure 13a) or a reduced temporal resolution (Figure 13b)
or both. We evaluate the scalable decoding efficiency of our
approach in comparison to the scalable FAMC and the AWC
approach. AWC is progressive but not scalable, i.e. it incrementally decodes all vertex positions of each frame. To
achieve a fair comparison with AWC, SPC and the scalable
FAMC partly decode and reconstruct vertex positions and
in addition, interpolate all remaining vertex positions using
already reconstructed vertex positions. This is achieved by
decoding frames of the base temporal layer in full spatial resolution while frames of higher temporal layers are decoded in
at least base spatial resolution (Figure 13c). Vertex position
of missing higher spatial layers are interpolated according to
equations (8) and (9) by assuming that corresponding prediction errors are zero. Figures 14(a)–(d) show reconstructed
frames at different stages of this decoding and interpolation
procedure.

Figure 13: Partial decoding (a) with reduced frame rate and (b) with reduced spatial resolution. (c) Partial decoding and
interpolation of missing layers.

Figure 14: Frame 77 of the jump animation rendered using flat-shading. In (a) the original frame is shown having 15 830
vertices. (b) The reconstruction obtained after decoding five spatial layers consisting of 5905 vertices. (c) The result obtained
after additionally interpolating all missing spatial layers using reference frames 72 and 80. (d) The reconstruction obtained
after decoding all eight spatial layers. (e) A reconstruction obtained after decoding all eight spatial layers from a different bit
stream; the bit stream in this case was generated using coarser quantization. Note that reconstruction (c) has a higher L2 error
than (e) with respect to the original frame (a), but (e) is subjectively more distorted. (All L2 errors are calculated relative to the
length of the bounding box diagonal of frame 1.)
c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

114

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC
Table 2: Time for spatial and temporal decomposition and frame
rates for encoding and decoding geometry.

Name
Face
Cow
Dance
Horse gallop
Jump

Decomp. (sec)

Enc. (fps)

Dec. (fps)

0.5
0.8
2.0
2.1
4.4

1101
186
97
65
38

1293
233
105
85
46

6.3. Memory requirements

Figure 15: Operational rate-distortion curves for the jump
animation.
In Figure 15, we show the comparative result with AWC
and the scalable FAMC based on the high resolution animation jump. The operational rate-distortion curve for SPC
and the scalable FAMC is obtained by successively decoding
more spatial and temporal layers from the same bit-stream,
while the corresponding curve for AWC is obtained by several encodings with different settings. The bit rate is calculated by dividing the number of decoded bits by the number of frames and the number of vertices of the original
animated mesh. To be able to compare with the AWC approach, we use the same error metric employed in the corresponding paper [GK04]. This error metric measures the
surface to surface L2 error. Results show that SPC achieves
coding gains of about 14% compared to AWC in the domain
of low errors while in comparison to the scalable FAMC, SPC
achieves coding gains of over 15%. In addition, SPC has the
advantage over AWC of being scalable, while compared to
the scalable FAMC, SPC allows fast encoding and decoding
and requires only low memory.
Although the KG and the L2 error metric are widely used
for evaluation in the animated mesh coding community, they
show some drawbacks. High KG and L2 errors do not necessarily correlate with the expected visual sensation (cp.
Figures 14c and e) [GK04]. In addition, both metrics do
not penalize temporal artifacts like very disturbing flickering
which can appear when encoding high resolution animated
meshes at a low bit rate. Flickering is usually caused by applying coarse quantization which produces fast temporally
changing quantization noise in the reconstructed animated
mesh. We observed that low bit rate coding achieved by
partial encoding using fine quantization leads to a significant reduction of flickering at the decoder when interpolating
missing spatial layers. We explain this result by the property
of our prediction/interpolation procedure to transfer semantically meaningful details from temporally close reference
frames.

The total memory required by SPC is mainly determined by
the number of vertices V of the animated mesh and the size
of a GOFs 2T , i.e. SPC has a memory complexity of O(2T V).
Please note that the required memory does not depend on the
total number of frames but only on the number of vertices
and the number of supported temporal layers. This is due
to the property of SPC to successively encode/decode small
groups of frames using temporally close reference frames.
In our experimets, SPC required less than 4 MB of memory
for encoding/decoding of a high resolution animated mesh
having 15830 vertices (like the jump animation in Table 1)
using four temporal layers, while for an animated mesh of
about 3000 vertices it required only about 800 kB.
6.4. Timings
SPC encodes and decodes all layers in one pass. Hence,
it has linear time complexity O(VF). We tested our nonoptimized coder implementation on a 2.66 GHz Intel Core
i7-920 desktop PC. In Table 2, timing results are reported
obtained based on our test data set. The low decomposition
time as well the high encoding and decoding frame rates,
document the applicability of SPC for real-time encoding
and decoding.
7. Conclusion
In this paper, we have presented a scalable predictive coding
scheme for efficient compression of animated meshes with
constant connectivity. Frames are encoded group-wise by
successively increasing frame rate and spatial resolution. We
have shown that dependencies between neighbouring spatial and temporal layers can be efficiently exploited using
rotation-invariant coordinates which are derived by a robust
skew ellipse approximation. SPC allows for efficient compression, it achieves real-time encoding and decoding rates,
and it supports spatial and temporal scalability. Partial decoding can be performed, which allows to successively reconstruct animated meshes with reduced frame rate and/or
reduced spatial resolution. In addition, SPC enables efficient
interpolation of missing parts of partly reconstructed animated meshes. SPC outperforms the streaming mode and the

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC
∗

scalable mode of MPEG-4 FAMC [MPE08, MSZ 08, SO08]
with coding gains of over 33% and up to 15%, respectively.
Furthermore, it is shows superior compression efficiency in
comparison to other non-scalable state-of-the-art approaches
[IR03, KG04, SSK05, GK04] with gains of over 16% in bit
rate.
Acknowledgements

115

wise smooth surface reconstruction. In SIGGRAPH’94:
Proceedings of the 21st Annual Conference on Computer
Graphics and Interactive Techniques (New York, NY,
USA, 1994), ACM, pp. 295–302.
[IR03] IBARRIA L., ROSSIGNAC J.: Dynapack: space-time compression of the 3D animations of triangle meshes with
fixed connectivity. In Eurographics Symposium on Computer Animation (2003), pp. 126–135.

This work is partly supported by the EC within FP6 under
Grant 511568 with the acronym 3DTV. The authors would
like to thank Bob Sumner, Jovan Popovi´c, the MIT CSAIL
Graphics Lab, Zachi Karni, Igor Guskov and Matthias M¨uller
for providing access to the animations used in this paper.
Many thanks to Costa Touma and Craig Gotsman for providing an executable of their static mesh compression algorithm.

[KCVS98] KOBBELT L., CAMPAGNA S., VORSATZ J., SEIDEL
H.-P.: Interactive multi-resolution modeling on arbitrary
meshes. In SIGGRAPH ’98: Proceedings of the 25th
Annual Conference on Computer Graphics and Interactive Techniques (New York, NY, USA, 1998), ACM,
pp. 105–114.

References

[KG04] KARNI Z., GOTSMAN C.: Compression of soft-body
animation sequences. Computers & Graphics 28 (2004),
25–34.

[AD01a] ALLIEZ P., DESBRUN M.: Progressive compression
for lossless transmission of triangle meshes. In International Conference on Computer Graphics and Interactive
Techniques archive (New York, USA, 2001), pp. 195–
202.

[Len99] LENGYEL J. E.: Compression of time-dependent geometry. In Symposium on Interactive 3D Graphics (1999),
pp. 89–95.

[AD01b] ALLIEZ P., DESBRUN M.: Valence-driven connectivity encoding of 3d meshes. Computer Graphics Forum 20
(2001), 480–489.

[LSCO∗ 04] LIPMAN Y., SORKINE O., COHEN-OR D., LEVIN D.,
R¨OSSL C., SEIDEL H.-P.: Differential coordinates for interactive mesh editing. In Proceedings of Shape Modeling International (2004), IEEE Computer Society Press,
pp. 181–190.

[AG03] ALLIEZ P., GOTSMAN C.: Recent advances in compression of 3D meshes. In Advances in Multiresolution
for Geometric Modelling, Mathematics and Visualization.
N. A. Dodgson, M. S. Floater, and M. A. Sabin (Eds.).
Springer, Berlin, Heidelberg (2005), pp. 3–26.

[MPE08] MPEG-4: ISO/IEC JTC1/SC29/WG11: ISO/IEC
14496-16:2006/AMD 2: Frame-based animated mesh
compression (FAMC). ISO/IEC, Genf, 2008.

[AM00] ALEXA M., M¨ULLER W.: Representing animations
by principal components. Computer Graphics Forum 19,
3 (2000).

[MSW03] MARPE D., SCHWARZ H., WIEGAND T.: Contextbased adaptive binary arithmetic coding in h.264/avc
video compression standard. IEEE Transactions on Circuits Systems for Video Technolgy 13, 7 (2003), 620–
636.

[AS07] AMJOUN R., STRAER W.: Efficient compression of 3d
dynamic mesh sequences. Journal of the WSCG 15 (2007),
99–106.
[ATFL05] AU O. K.-C., TAI C.-L., FU H., LIU L.: Mesh editing with curvature flow laplacian. In Symposium of Geometry Processing (2005).

[MSZ∗ 08] MAMOU K., STEFANOSKI N., ZAHARIA T.,
OSTERMANN J., PREˆ TEUX F.: Frame-based compression of
animated meshes in mpeg-4. In Proceedings of the IEEE
International Conference on Multimedia & Expo (July
2008).

[GK04] GUSKOV I., KHODAKOVSKY A.: Wavelet compression
of parametrically coherent mesh sequences. In Eurographics Symposium on Computer Animation (August 2004),
pp. 183–192.

[MZP06] MAMOU K., ZAHARIA T., PREˆ TEUX F.: A skinning
approach for dynamic 3D mesh compression: Research
articles. Comput. Animat. Virtual Worlds 17, 3–4 (2006),
337–346.

[Got03] GOTSMAN C.: On the optimality of valence-based
connectivity coding. Comput. Graph. Forum 22, 1 (2003),
99–102.

[SCO04] SORKINE O., COHEN-OR D.: Least-squares meshes.
In Proceedings of Shape Modeling International (2004),
IEEE Computer Society Press, pp. 191–199.

[HDD∗ 94] HOPPE H., DEROSE T., DUCHAMP T., HALSTEAD M.,
JIN H., MCDONALD J., SCHWEITZER J., STUETZLE W.: Piece-

[SKLO07] STEFANOSKI N., KLIE P., LIU X., OSTERMANN J.:
Layered coding of time-consistent dynamic 3D meshes

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

116

N. Stefanoski & J. Ostermann / Scalable Predictive Coding SPC

using a non-linear predictor. In Proceedings of the
IEEE International Conference on Image Processing
(September 2007).
[SLKO07] STEFANOSKI N., LIU X., KLIE P., OSTERMANN J.:
Scalable linear predictive coding of time-consistent 3D
mesh sequences. In Proceedings of 3DTV-CON, The True
Vision—Capture, Transmission and Display of 3D Video
(May 2007).
[SMW06] SCHWARZ H., MARPE D., WIEGAND T.: Analysis of
hierarchical B pictures and MCTF. In Proceedings of the
IEEE International Conference on Multimedia & Expo
(July 2006), pp. 1929–1932.
[SO06] STEFANOSKI N., OSTERMANN J.: Connectivity-guided
predictive compression of dynamic 3D meshes. In Proceedings of the IEEE International Conference on Image
Processing (October 2006).

[SO08] STEFANOSKI N., OSTERMANN J.: Spatially and temporally scalable compression of animated 3d meshes with
mpeg-4/famc. In Proceedings of the IEEE International
Conference on Image Processing (October 2008).
[SPC] http://www.tnt.uni-hannover.de/∼stefanos/projects/
spc/.
[SSK05] SATTLER M., SARLETTE R., KLEIN R.: Simple and
efficient compression of animation sequences. In Eurographics Symposium on Computer Animation (2005),
pp. 209–217.
[SZGP05] SUMNER R. W., ZWICKER M., GOTSMAN C., POPOVIC´
J.: Mesh-based inverse kinematics. ACM Trans. Graph.
24, 3 (2005), 488–495.
[TG98] TOUMA C., GOTSMAN C.: Triangle mesh compression.
In Proceedings of Graphics Interface (1998), pp. 26–34.

c 2009 The Authors
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

