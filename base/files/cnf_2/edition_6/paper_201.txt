DOI: 10.1111/j.1467-8659.2010.01792.x
Pacific Graphics 2010
P. Alliez, K. Bala, and K. Zhou
(Guest Editors)

Volume 29 (2010), Number 7

Video Painting via Motion Layer Manipulation
Hua Huang†

Lei Zhang

Tian-Nan Fu

School of Electronics and Information Engineering, Xi’an Jiaotong University, China

Abstract
Temporal coherence is an important problem in Non-Photorealistic Rendering for videos. In this paper, we present
a novel approach to enhance temporal coherence in video painting. Instead of painting on video frame, our approach first partitions the video into multiple motion layers, and then places the brush strokes on the layers to
generate the painted imagery. The extracted motion layers consist of one background layer and several object
layers in each frame. Then, background layers from all the frames are aligned into a panoramic image, on which
brush strokes are placed to paint the background in one-shot. The strokes used to paint object layers are propagated frame by frame using smooth transformations defined by thin plate splines. Once the background and object
layers are painted, they are projected back to each frame and blent to form the final painting results. Thanks to
painting a single image, our approach can completely eliminate the flickering in background, and temporal coherence on object layers is also significantly enhanced due to the smooth transformation over frames. Additionally, by
controlling the painting strokes on different layers, our approach is easy to generate painted video with multi-style.
Experimental results show that our approach is both robust and efficient to generate plausible video painting.
Categories and Subject Descriptors (according to ACM CCS):
Generation—Display Algorithms

1. Introduction
Video painting is a challenging task in the field of NonPhotorealistic Rendering (NPR), which strives to interpret
the video contents with particular painterly styles. Due to its
expressive ability, painterly rendering has been extensively
studied and applied in various fields, such as film production, electronic entertainment, virtual reality, scientific visualization and so on [HIL02]. Unlike photorealistic rendering
which pursuits physical realism, painterly rendering aims at
the abstraction of objects in essence, and this is usually implemented by a family of stroke-based rendering (SBR) approaches. SBR mimics the brush movement during the creation of artistic painting, which generates the painted imagery by placing a series of strokes on the canvas. Fig. 1
shows an example of Van Gogh’s painting and one painted
video frame produced by our approach in this paper. It can
be seen that brush strokes visibly spread over the image to
convey certain expression. However, directly applying these
imagery painting approaches to videos will result in visu-

† Corresponding author: huanghua@xjtu.edu.cn
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

I.3.3 [Computer Graphics]: Picture/Image

ally unpleasing results, since it ignores the temporal coherence across the frames and severe flickering and scintillation
can be observed [HP00]. Hence, how to achieve a good temporal coherence remains in video painting as an important
problem. In this paper, we focus on painterly rendering on
video sequences using SBR, and introduce a novel solution
based on motion layer extraction, which is able to efficiently
reduce the flickering and enhance temporal coherence.

Figure 1: Left: Vincent Van Gogh’s “Starry Night” (1889).
Right: Painting effect of a video frame by our approach.

An important component of our approach is motion layer
extraction of video sequence, which is a very active research

2056

H. Huang, L. Zhang, T. N. Fu / Video Painting via Motion Layer Manipulation

subject in computer vision [SX05]. Motion layer extraction
means to partition the video scene into multiple planar regions, each of which undergoes a consistent motion in adjacent frames. Motion layer extraction provides an explicit
recognition for video contents, which has broad applications
in video representation, analysis, compression and synthesis [FPR08]. However, to the best of our knowledge, there
is yet no attempt to adopt motion layer extraction to video
painting. In this paper, we demonstrate that by virtue to motion layer extraction, temporal coherence in video painting
can be greatly improved with much less visual flickering.
The main contribution of this paper is to introduce motion
layer extraction into video painting, and propose a sketchdriven layering approach for video painting. Then, by manipulating the extracted motion layers, we propose a novel
approach to brush stroke placement. In contrast to painting on video frames, our approach places the strokes on the
motion layers, which can completely eliminate the visual
flickering in the background, and simultaneously enhance
the temporal coherence on the object layers. Based on the
motion layer representation, our approach can also produce
painting effect with multi-style for a single video.
2. Related Work
Image Painting We now first review previous work on image painterly rendering, of which example-based and strokebased approaches are two main threads. Since our approach
paints videos with explicit brush strokes, we mainly review
the related work belonging to this scope. But to be complete,
we still mention two classical example-based painter rendering approaches: one is in [HJO∗ 01] which learnt painterly
styles by training a painted image with multi-scale filters,
and the other is in [WWYS04] which borrowed techniques
from texture synthesis to characterize painterly style as
stroke textures.
Stroke-based approach is broadly used in image painterly
rendering. Hertzmann [Her98] proposed a painting scheme
by placing a series of strokes on the canvas in a roughto-fine way, which is able to express different rendering
styles by simply controlling the strokes size and orientation. Painting image can also be formalized as an energy
optimization problem, which finds the optimal placement
of strokes as the minimization of some appropriately defined functional. Hertzmann [Her01] introduced a scoring
function to measure the similarity between a source image
and its painted look, and employs a trial-and-error relaxation to solve it. Although the relaxation did not guarantee to
converge, it usually results in an economical strokes placement but plausible painting effect. To enrich the physical appearance, the painting strokes could be incorporated with a
height map to add the lighting effect [Her02]. Stroke orientation is an important ingredient in stroke-based painting,
which guides the local movement of brush and is commonly
characterized by a vector-field. Olsen et al. [OMG05] in-

troduced an interactive vector fields design approach by assigning properties to strokes in segmented regions, and then
painted the whole image based on these local strokes. Zhang
et al. [ZMT06, ZHT07] used a combinatorial basis vector
field to create desired stroke orientations. Their approach is
flexible to control both geometry and topology of the vector
field, which can be easily adopted in painting stroke generation. Recently, Zeng et al. [ZZXZ09] presented a novel
painting approach based on a hierarchial parsing tree representation of an image. By taking the semantic information
into account, their approach could generate more realistic
oil-painting effects.
Video Painting Compared to painting one static image,
video painting is more challenging due to the existence of
the temporal coherence issue when painting over frames.
Meier [Mei96] used sampled particles to track the stroke position throughout the video sequence, which could minimize
the “shower door” effect. However, that approach needs an
explicit 3D geometry for particles attachment. Optical flow
is a powerful tool that can be utilized to enhance temporal
coherence. Hertzmann and Perlin [HP00] estimated video
changes based on the optical flow between two adjacent
frames, and then applied painting only on regions undergoing significant changes. Alternative approaches using optical flow are to populate the strokes along the flow vector
that propagates painting from an initial frame to the subsequent frames [Lit97,HE04,ZLH09]. This strategy is demonstrated to be able to reduce flickering efficiently, but seriously depends on the computing accuracy of optical flow.
Klein et al. [KSFC02] created the so-called rendering solid
to place strokes in a video cube by stacking all the frames.
Since each solid contains local pixels from a few adjacent
frames, a coherent stroke placement could be maintained in
painting process. To emphasize the dynamic flow effect, Lee
et al. [LLY09] used optical flow to capture motion information in the video sequence, and then manipulate the strokes
complying with the motion field, which could enhance the
dynamic appearance of objects with obvious motions. Similarly, Zhang et al. [ZCZ∗ 09] adopted the optical flow to
produce water animation with Chinese painting style. Recently, Kagaya et al. [KBD∗ 10] proposed an object-based
painterly rendering system, which is similar to our approach.
Their approach advects painting strokes by solving a Laplace
equation defined on the whole video sequence. To get multistyle painting effects, they used Video Tooning [WXSC04]
to decompose the video into disjoint components, and specify style parameters adhering to each component. However,
flickering still remains a problem in their painting results.
In the NPR of 3D models, there are also some works dedicated to reduce visual flickering. Kalnins et al. [KDMF03]
enhanced the temporal coherence of stylized silhouettes by
propagating the parameterization of strokes on the 3D models frame by frame. To enable multi-style painting, Chi and
Lee [CL06] proposed a multiresolution scheme to interactively place strokes on the spheres. By building the sphere
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2057

H. Huang, L. Zhang, T. N. Fu / Video Painting via Motion Layer Manipulation

hierarchy of 3D points, the local regions structures could
be maintained after rendering, which could produce smooth
painting results. Yan et al. [YCLL08] used painterly art maps
to transfer stroke textures from sampling images to 3D models. The coherence in animation was achieved by tracking
the splat nodes.
Motion Layer Extraction Finally, we briefly summarize
some work on motion layer extraction in the field of computer vision. Interested readers are referred to [SX05] for
a detailed review. Adelson and Wang [AW93] used clustering on the optical flow to represent video with sets of overlapping layers. Their approach is not stable to produce accurate layers since the computation of optical flow is unreliable. Ke and Kanade [KK02] extracted layers by classifying pixels with linear subspace constraints of affine homography among frames. Xiao and Shah [SX05] proposed a
layered representation by expanding seed regions with similar motion parameters, and then used graph cut to optimize
the region boundaries between different layers. Fradet et
al. [FPR08] proposed a restricted graph in the vicinity of
layer boundaries to refine the layer representation. By combining rigid motion constraints, their approach is able to extract more accurate layer representations. These existing approaches, however, usually apply parametric affine motion
as constraints to group pixels, thus have the limitation in extracting layers from video with diverse motions.
3. Algorithm Overview
Our video painting approach relies on motion layer extraction of the input video, and strokes placement for painterly
rendering. At the motion layer extraction stage, the video
scene is first partitioned into multiple planar regions using
a sketch-driven approach. Then, based on the motion layers, our approach reconstructs a panoramic image that contains the background layers from all the frames. Meanwhile,
smooth transformations for each object layer between adjacent frames are constructed. The brush strokes are subsequently placed on the background panorama in one-shot, and
on the object layers by conforming to the transformations.
At the rendering stage, we employ some classical SBR approach (e.g. [Her98]) to paint the panorama and object layers respectively with the generated strokes, and then project
the painted layers back to each frame. Finally, we can get
the video painting by blending all the painted layers from
back to front in every frame. So the key of our approach is
to populate the strokes (including its properties, e.g., color,
size, opacity, orientation) over different motion layers. In the
sequel sections, we will elaborate the details in each phase.
4. Sketch-Driven Motion Layer Extraction
For a video sequence {Ik }, motion layer extraction is performed to extract compact layer regions from each frame,
i.e., Ik = {Lkr , 1 ≤ r ≤ N}, where r is the layer label, and N
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

is the cardinal number of desired layers. Motion layer extraction amounts to assigning each pixel a label li ∈ {1, ..., N} to
indicate which layer it belongs to. For simplicity, we assume
that the video contains a fixed number of layers across the
frames. The extracted layers commonly consist of one background layer LN and several object layers {L1 , ..., LN−1 }.
Usually, the background layer is static or panning and zooming with camera motion, and the object layers include moving objects throughout the frames. It should be pointed that
the layer number N depends on the contents and motions
involved in the video scene. Each layer supports a semantic
region possibly with similar motion style. Although there are
some approaches (e.g., [SX05]) that can automatically compute the layer number, it is not robust for videos containing
complex motions. Hence, our approach employs some interactive sketches to determine the layer number explicitly as
describled below.
To handle diverse videos with different motions, we introduce a sketch-driven approach to extract layers in each
frame. User first draws a set of sketches S = {Sir } in one key
frame, where r is the layer label of the corresponding sketch.
The sketches commit the seed regions for extracting the desirable layers by solving a multi-label problem in current
frame. Then, these sketches are subsequently propagated to
extract layers in the successive frame (see Fig. 2). Our approach shares some similarities with the time-sequential motion layer extraction approach presented in [FPR08]. But instead of using restricted graph, we resort to sketch propagation to extract layers frame by frame, which releases the
limitation of only using affine motion model. Fig. 2 shows
that our approach can generate better layer extraction than
the approach in [FPR08].

4.1. Layer labeling on one frame
Assuming that the seed regions in the frame Ik are correctly
established by sketches propagation, we are now ready to
extract motion layers in the frame Ik . The layer extraction
can be formulated as a multi-label problem using the typical Maximum A Posterior (MAP) estimation of a Markov
Random Fields (MRF) [KZ04] model. Formally, the optimal
labeling on Ik can be found by minimizing the following objective function:
E(l) = ∑ Di (li ) + λ
i

∑

V<p,q> (l p , lq )

(1)

<p,q>∈N

where i denotes every pixel in Ik and N is the 8-connectivity
neighborhood relationship between pixels.
In Equation (1), V<p,q> is the smooth item preserving the
continuity for assigning labels to two adjacent pixels, and
Di (li ) is the data item measuring the conformity when assigning label li to pixel i. We adopt the similar definition
of V<p,q> and Di (li ) as described in [BJ01] and [FPR08],
which is able to extract layers with similar motion models.

2058

H. Huang, L. Zhang, T. N. Fu / Video Painting via Motion Layer Manipulation

Figure 2: Top: User specifies several sketches on one key frame, whereafter these sketches are propagated over frames to
extract motion layers. Middle: Each frame is partitioned into multiple layers using the sketches as seed regions. Layer labels
are indicated by gray scale. Bottom: In each frame, the motion layers are extracted using restricted graph [FPR08].

In details, we have
V<p,q> (l p , lq ) =

1
|l p − lq |
1 + C p −Cq

(2)

where C p denotes the color at pixel p, · is the L2 norm in
Lab color space, and
Di (li ) = ω1 Ri (li ) + ω2 Mi (li ) + ω3 Ti (li )

curate layer representation. Here, we employ the closed form
solution in [LLW06] to solve the alpha matting. The trimap
needed in the matting is automatically generated by simply
dilating and eroding the layer regions. Then, by computing
the alpha matting, the frame Ik is finally converted to a layered representation as Ik = {(Lkr , αrk )}.

(3)

where
Ri (li ) = − ln (Pr(Ci |li ))

(4)

π
Mi (li ) = arctan It (i) − It+1 (i ) 2 − τ +
2
1, : li = li
Ti (li ) =
0, : otherwise

(5)
(6)

In the definition of data items, Ri (li ) is the classical color
conformity term, set as the negative log-probability of color
distribution with respect to layer li . The color distribution
is modeled as the Gaussian Mixture Model (GMM) on the
sketches with label li . Mi (li ) is the motion penalty term defined as the color difference between a pixel i in current
frame It and the pixel i in the next frame It+1 according to
its motion vector. The term Ti (li ) enforces the temporal consistence, which maintains the consistent labeling between i
and i under the motion model on layer li .
Energy function (1) can be minimized by using αexpansion [BVZ01] graph cuts, which efficiently assigns optimal labels to each pixel. Then, the pixels with the same label are grouped into one layer, which generates a partition in
Ik . To refine the region boundaries, especially in the presence
of skin, hair or spike, we use image matting to get a more ac-

Figure 3: The sketches are enclosed by a set of local windows [BWSS09] in (a), and then each sketch is propagated
along the averaging motion vector in the local windows (b).

4.2. Sketch propagation over frames
Inspired by the work in [BWSS09], our approach uses the
local window propagation to move the sketches from one
frame to the successive frame for propagation. The local
windows are defined along the contour of each sketch (Fig. 3
(a)), and neighboring windows are required to overlap 30%
of the window size. Then, the contour is moving along the
motion vector by averaging the optical flow in the local windows. For example in Fig. 3 (b), let x be a point on the
contour of the sketch Si , then the new position of x is computed as x = x + v¯x , where v¯x is the average flow vector in
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

H. Huang, L. Zhang, T. N. Fu / Video Painting via Motion Layer Manipulation

2059

Figure 4: During the propagation, the sketches need revision to ensure the correct seed regions for motion layer extraction.
Otherwise, the sketches that cover inappropriate regions (a) might generate layers with mislabeled region (b). Hence, each
sketch is evaluated by the color probability as its reliability (c). Sketches with smaller reliability are revised using binary graph
cut (d). These revised sketches commit correct seed region to extract desirable layers (e). (f) shows the matting results of the
object layers extracted with trimap generated by eroding and dilating the object layers.

the sketch while covered by the local window. Through the
sketches propagation, we can get the seed regions on the next
frame.
It is worthy noting that sketch propagation is not always
guaranteed to generate desired seed regions for correct layer
extraction. For example in Fig. 4, the propagated sketches S33
and S43 result in mislabeling if they are directly used as the
seed regions. So ahead of solving Equation (1), we should
examine the reliability of each sketch for setting the multilabel in current frame. Suppose we have the correct layer
extraction from the previous frame, then the reliability of
sketch Sr can be defined as the color probability E r (i) =
Prr (i) of each pixel in the sketch. Here, the probability is
computed from the GMM on the corresponding layer Lr in
previous frame. If E r (i) < σ, then Sr needs to be revised by
deleting pixels with low probability. The sketch revision can
be set as a binary-label problem, which solves the following
equation:
E(l) = ∑ Ri (li ) + λ
i

∑

V<p,q> (l p , lq )

(7)

<p,q>∈N

where Ri (li ) and V<p,q> are defined as above. Then we get
the revised sketch S˜r , which is used as the seed regions for
layer labeling and next propagated to the sequel frames.
5. Stroke Placement on Motion Layers
We now turn to place brush strokes on the extracted motion
layers. Final painting result is generated by populating these
strokes on the video frames, and the rendering process is
drawn later in Section 6. So in this section, we focus on the
stroke placement over motion layers.
As mentioned at the beginning, temporal coherence is
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

the main issue in handling video painting. Simply painting
frame by frame will incur severe flickering, which is caused
by painting the static areas of the scene differently in different frames [HP00]. Hence, the key to eliminate flickering
is to paint the same region in each frame in the same way.
Considering the motion models of each layer, we place the
strokes on layers instead of directly painting each frame. For
the background layer, we first reconstruct the panoramic image by recovering the camera motion, and place the strokes
on the panoramic image to obtain the painted background
of the video scene. For the object layers, we propagate the
painting strokes from the initial frame to the successive
frames in accordance with the object motion. Next, we will
discuss stroke placement on the background layer and object
layers respectively.

5.1. Placing strokes on background layer
Background painting is crucial to achieve a temporal coherent output, because static background usually makes flicker
more apparent during frame transition. The background in
each frame is actually cast from a static scene in accordance with camera motion, hence it is possible to reconstruct the background with a panoramic image, and populate strokes on the panorama to get a coherent painting. Actually, enhancement of temporal coherence by constructing
panoramic background is also used in other video processing, like video resizing [WFS∗ 09, ZHM09] and visualization [CM10], which can generate desirable temporal coherence.
Constructing the panoramic image essentially amounts
to locating the background layer LkN in each frame in a
global coordinate system. So the background layers need

2060

H. Huang, L. Zhang, T. N. Fu / Video Painting via Motion Layer Manipulation

Figure 5: Top: Background layers from all the video frames are aligned to generate a panoramic image. Middle: The background layer in each frame is corresponding to the aligned region in the panorama. Then, strokes are placed to painting the
panorama. Bottom: Painting background layer in each frame is obtained by retrieving the painted panorama by the inverse
alignment.

to be aligned together with appropriate transformation Hk :
LkN → L˜ kN . Based on these transformations, we can obtain the
panoramic image P = {L˜ kN } (see Fig. 5). The key to solve
these homographies is to find the correspondence between
frames. Existing approach, like SIFT [Low99], can be used
to detect distinctive features in each frame, and find the best
correspondence by minimizing the cost of SIFT description.
To remove the unreliable matching, we use the classical iterative Random Sample Consensus (RANSAC) approach [FB81] to detect outliers. Then Hk is computed as
the optimization of the following matching functional:
Hk = argmin ∑ pkl − T · pk+1
l
T ∈G

2

(8)

l

where pkl ∈ Ik and pkl+1 ∈ Ik+1 are the remaining corresponding feature points in two adjacent frames, and G is
a specified transformation set, e.g., affine transformations,
projective transformations, polynomial transformations, etc.
In our experiments, affine transformation is sufficient to reconstruct a pleasing panorama.
Thus, for each pixel Ik (i) on the background layer, there
has a corresponding point in P by the transformation Hk .
Then, brush strokes are placed on the panorama in oneshot, and form the final painting results as described in Section 6. Finally, painted video frame is obtained by the inverse
transformation Hk−1 from the panoramic image to the corresponding frame. Fig. 5 shows the painted panorama and
background layer in each frame. Since the background in

each layer is formed from a single static panoramic image,
there is completely no flickering during frame transition.

5.2. Placing strokes on object layers
Each object layer usually contains an object with consistent
motion over the frames. Thus the painting strokes should
vary with the moving object to get the consistent painting in
transition. Similar to foreground layer alignment, we need
the transformation of object layer between two consecutive
frames. Since the motion in object layer may be quite arbitrary, the transformation need to be defined with sufficient
degrees of freedom. Hence, we employ the thin plate spline
(TPS) [BMP02] to define the object layer transformation
since it is able to express a wide range of motion style. In this
case, the allowed transformation group G in the optimization
function (8) becomes G = {T (x, y) = ( f1 (x, y), f2 (x, y))},
and f1,2 are defined as
n

f (x, y) = c0 + c1 x + c2 y + ∑ ωi ψ ( (x, y) − (xi , yi ) ) (9)
i=1

where c0 , c1 , c2 , ωi are the TPS coefficients, and the kernel
function ψ(r) = r2 log r2 .
To set the TPS transformation, we need the corresponding points on the layers between two frames (see Fig. 6).
These points can be detected using SIFT based matching as
in Section 5.1. Then, TPS coefficients are solved by solving
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

H. Huang, L. Zhang, T. N. Fu / Video Painting via Motion Layer Manipulation

2061

Figure 6: Top: Corresponding points (in color) on the first object layer between two adjacent frames, and the strokes are
propagated between them by smooth transformation. Middle: Strokes are propagated on the second object layer. Bottom:
Painting results for the two object layers are generated from the propagated strokes.

a linear system:
K
PT

P
0

ω
c

=

pk+1
0

where Ki j = ψ (xi , yi ) − (x j , y j ) , the i-th column of PT
is (1, xi , yi )T , ω and c and the TPS coefficients to be computed. The right hand is the corresponding feature points in
next frame Ik+1 .
Based on the TPS transformation (9), the strokes can be
propagated over the sequence, i.e., transferring positions,
orientation and other parameters of strokes between adjacent frames. To further improve the temporal coherence of
object layers, we propose to use both the forward and backward stroke propagations when computing the strokes in the
current frame Ik , i.e., by averaging the stroke parameters
transformed from Ik−1 and Ik+1 . Then, based the propagated
strokes, painting on the object layer can be obtained with
desirable styles (see Fig. 6).

6. Painterly Rendering
When the brush strokes are established on the background
panorama and object layers throughout the frames, we can
produce the painting results by stacking all the strokes in
order. Our approach adopts the stroke growing procedures
in [Her98]. User first specifies the painterly style by setting
a series of stroke parameters, like color, size, opacity and
so on. Then, each stroke starts from the placement position
and prolongs its area according to the local orientation to
finally cover the whole image. Also, user can vary the stroke
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

parameters on different layers to generate painting effect of
multi-style (see Fig. 7).
When video frame is covered by all the strokes, the final
painted video is produced by blending painted background
layer and object layers from back to front. For each pixel Ii ,
we use the matting weight αrk to compose its painted look,
i.e., Ii = ∑r αri I¯ir and I¯ir is the painted pixel on layer Lr .
7. Experimental Results
We now present some results of our video painting approach based on motion layers. Only minor interaction needs
to draw sketches to indicate the layer extraction in one
key frame, and possibly refine the layer boundaries after
sketch propagations. Then, the remaining process is performed automatically. The runtime required to produce the
final painted video depends on the number of motion layers
and frame size. The PC configuration for running our approach is 3GHz Intel Core(TM)2 Duo CPU with 4G RAM.
Averagely, it takes about 3 minutes for processing 80 images, where frame size is 1280 × 720 and contains 3 layers.
Accompanying video displays all the painting results for the
different inputs.
The major merit of our motion layer based video painting is that visual flickering can be completely removed on
the background, and significantly reduced on the object layers as well. We compare our painting results with the ones
in [HE04] and [KBD∗ 10]. The side-by-side comparison
can be viewed in the accompanying video. Although the
method in [HE04] can generate smooth painting results,
there is still flickering on the background. As for the results

2062

H. Huang, L. Zhang, T. N. Fu / Video Painting via Motion Layer Manipulation

Figure 7: Video frame is painted with different styles. The gray scale mask shows the extracted layers.

of [KBD∗ 10], the flickering looks much apparent. In contrast, our approach is able to produce painted video with less
flickering.
Fig. 7 shows an example of painting video with multistyle. The video is partitioned into two layers. Fig. 7 (a)
shows the result by painting object layer with the curved
brush strokes [Her98] and our SBR approach for the background layer. In Fig. 7 (b), we use our SBR approach to paint
the object layer and Litwinowicz’s approach [Lit97] for the
background layer. It can be seen that our approach is easy to
generate multi-style paintings.
Fig. 8 shows more painted video frames by our approach.
Although the motion models differs in different input videos,
our approach is able to extract the appropriate layers for
strokes placement. Fig. 8 (a) and (d) are partitioned into two
layers, and (b) and (c) are of three layers, which are indicated
as the gray scale mask. It shows that by manipulating motion
layers for strokes placement can generate plausible painting
effect. Note that the video contents are now exhibited with
non-photorealistic appearance by series of brush strokes.

ence of complex motion. This is because usually even object
layers contain multiple components with separate motions.
As the future work, we are planning to borrow the technique
of image parsing [ZZXZ09] to get the parsing tree representation of input video, and by propagating strokes over tree
nodes to further improve the temporal coherence on object
layers. Besides, when the background contains irregular motions, such as wave and smoke, painting on the panoramic
can not reflect the dynamic change in the scene. In this scenario, some coherent perturbation might be added on the
painted panorama to enrich its movement sense.
Acknowledgements
We thank the anonymous reviews for their helpful comments. This work was partly supported by the National Natural Science Foundation of China (Grant No. 60970068),
the Key Project of Chinese Ministry of Education (Grant
No. 109142), and the MOE-Intel Joint Research Fund
(Grant No. MOE-INTEL-09-07). Lei Zhang was partly supported by China Postdoctoral Science Foundation (Grant
No. 20100471618).

8. Conclusions and Future Work
We present a novel video rendering approach by painting
on the motion layers. The motion layers are obtained by
a sketch-driven label propagation, and each video frame is
partitioned into compact layer regions. To enhance the temporal coherence, particularly eliminating visual flickering,
the brush strokes are place on different layers in accordance
with their motion models. Background layers are aligned to
a panorama on which strokes are populated in one-shot, and
strokes on object layers are propagated with smooth transformation between adjacent frames. Our motion layer based
painting approach can completely eliminate flickering on the
background layer and significantly reduce flickering on the
object layers.
Limitations Our approach can not totally eliminate the
visual flickering on the object layers, especially in the pres-

References
[AW93] A DELSON E. H., WANG J. Y. A.: Representing moving
images with layers. IEEE Transactions on Image Processing 3, 5
(1993), 625–638.
[BJ01] B OYKOV Y. Y., J OLLY M. P.: Interactive graph cuts for
optimal boundary & region segmentation of objects in n-d images. In Proc. ICCV ’01 (2001), pp. 105–112.
[BMP02] B ELONGIE S., M ALIK J., P UZICHA J.: Shape matching and object recognition using shape contexts. IEEE Transactions on Pattern Analysis and Machine Intelligence 24, 24 (2002),
C509–C522.
[BVZ01] B OYKOV Y., V EKSLER O., Z ABIH R.: Fast approximate energy minimization via graph cuts. IEEE Transactions on
Pattern Analysis and Machine Intelligence 23, 11 (2001), 1222–
1239.
[BWSS09]

BAI X., WANG J., S IMONS D., S APIRO G.: Video

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

H. Huang, L. Zhang, T. N. Fu / Video Painting via Motion Layer Manipulation
snapcut: Robust video object cutout using localized classifiers.
ACM TOG 28, 3 (2009).
[CL06] C HI M. T., L EE T. Y.: Stylized and abstract painterly
rendering system using a multiscale segmented sphere hierarchy.
IEEE Transactions on Visualization and Computer Graphics 12,
1 (2006), C61–C72.

2063

[Low99] L OWE D. G.: Object recognition from local scaleinvariant features. In International Conference on Computer Vision (1999), pp. 1150–1157.
[Mei96] M EIER B. J.: Painterly rendering for animation. In Proc.
SIGGRAPH ’96 (1996), pp. 477–484.

[CM10] C ORREA C. D., M A K. L.: Dynamic video narratives.
In Proc. SIGGRAPH ’10 (2010).

[OMG05] O LSEN S. C., M AXWELL B. A., G OOCH B.: Interactively vector fields for painterly rendering. In Proc. of Graphics
Interface ’05 (2005), pp. 241–247.

[FB81] F ISCHLER M. A., B OLLES R. C.: Random sample consensus: A paradigm for model fitting with application to image
analysis and automated catrography. Comm. of the ACM 24
(1981), 381–395.

[SX05] S HAH M., X IAO J. J.: Motion layer extraction in the
presence of occlusion using graph cuts. IEEE Transactions on
Pattern Analysis and Machine Intelligence 27 (2005), C1644–
C1659.

[FPR08] F RADET M., P ÉREZ P., ROBERT P.: Time-sequential
extraction of motion layers. In Proc. ICIP ’08 (2008), pp. 3224–
3227.

[WFS∗ 09] WANG Y. S., F U H. B., S ORKINE O., L EE T. Y., S EI DEL H. P.: Motion-aware temporal coherence for video resizing.
ACM TOG 28, 5 (2009).

[HE04] H AYS J., E SSA I.: Image and video based painterly animation. In Proc. Non-Photorealistic Animation and Rendering
’04 (2004), pp. 113–120.

[WWYS04] WANG B., WANG W. P., YANG H. P., S UN J. G.:
Efficient example-based painting and synthesis of 2d directional
texture. IEEE Transactions on Visualization and Computer
Graphics 10, 3 (2004), C266–C277.

[Her98] H ERTZMANN A.: Painterly rendering with curved brush
strokes of multiple sizes. In Proc. SIGGRAPH ’98 (1998),
pp. 453–460.
[Her01] H ERTZMANN A.: Paint by relaxation. In Proc. Computer
Graphics Interational ’01 (2001), pp. 47–54.
[Her02] H ERTZMANN A.: Fast paint texture. In Proc. NonPhotorealistic Animation and Rendering ’02 (2002), pp. 91–98.

[WXSC04] WANG J., X U Y. Q., S HUM H. Y., C OHEN M. F.:
Video tooning. ACM TOG 23, 3 (2004), C574–C583.
[YCLL08] Y EN C. R., C HI M. T., L EE T. Y., L IN W. C.:
Styliezed rendering using samples of a painted image. IEEE
Transactions on Visualization and Computer Graphics 14, 2
(2008), C468–C480.

[HIL02] H ERTZMAN A., I NTERRANTE V., L UM E. B.: Recent
Advances in Non-Photorealisitc Rendering for Art and Visualization. SIGGRAPH Course. Proc. SIGGRAPH ’02, 2002.

[ZCZ∗ 09] Z HANG S. H., C HEN T., Z HANG Y. F., H U S. M.,
M ARTIN R. R.: Video-based running water animation in chinese
painting style. Science in China Series F: Information Science
52, 2 (2009), C162–C171.

[HJO∗ 01] H ERTZMANN A., JACOBS C. E., O LIVER N., C UR LESS B., S ALESIN D. H.: Image analogies. In Proc. SIGGRAPH
’01 (2001), pp. 327–340.

[ZHM09] Z HANG Y. F., H U S. M., M ARTIN R. R.: Shrinkability maps for content-aware video resizing. Computer Graphics
Forum 27, 7 (2009), C1797–C1804.

[HP00] H ERTZMANN A., P ERLIN K.: Painterly rendering for
video and interaction. In Proc. Non-Photorealistic Animation and
Rendering ’00 (2000), pp. 7–12.

[ZHT07] Z HANG E., H AYS J., T URK G.: Interactive tensor field
design and visualization on surfaces. IEEE Transactions on Visualization and Computer Graphics 13, 1 (2007), C94–C107.

[KBD∗ 10] K AGAYA M., B RENDEL W., D ENG Q. Q., K ESTER SON T., T ODOROVIC S., N EILL P. J., Z HANG E.: Video painting with space-time-varying style parameters. IEEE Transactions
on Visualization and Computer Graphics, to appear (2010).

[ZLH09] Z HANG S. H., L I X. Y., H U S. M.: Online Video
Stream Stylization. Technical Report TR-090801, Tsinghua University, Beijing, 2009.

[KDMF03] K ALNINS R. D., DAVIDSON P. L., M ARKOSIAN L.,
F INKELSTEIN A.: Coherent stylized silhouettes. ACM TOG 22,
3 (2003), 856–861.
[KK02] K E Q. F., K ANADE T.: A robust subspace approach to
layer extraction. In Proc. of the Workshop on Motion and Video
Computing (2002).
[KSFC02] K LEIN A. W., S LOAN P. J., F INKELSTEIN A., C O HEN M. F.: Stylized video cubes. In Proc. Symposium on Computer Animation ’02 (2002), pp. 15–22.
[KZ04] KOLMOGOROV V., Z ABIH R.: What energy function can
be minimized via graph cuts? IEEE Transactions on Pattern
Analysis and Machine Intelligence 26, 2 (2004), 147–159.
[Lit97] L ITWINOWICZ P.: Processing images and video for an
impressionist effect. In Proc. SIGGRAPH ’97 (1997), pp. 407–
414.
[LLW06] L EVIN A., L ISCHINSKI D., W EISS Y.: A closed form
solution to natural image matting. In Proc. CVPR ’06 (2006),
pp. 61–68.
[LLY09] L EE H. C., L EE C. H., YOON K. H.: Motion based
painterly rendering. Computer Graphics Forum 28, 4 (2009),
C1206–C1216.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

[ZMT06] Z HANG E., M ISCHAIKOW K., T URK G.: Vector field
design on surfaces. ACM TOG 25, 4 (2006), C1294–C1326.
[ZZXZ09] Z ENG K., Z HAO M. T., X IONG C. M., Z HU S. C.:
From image parsing to painterly rendering. ACM TOG 29, 1
(2009).

2064

H. Huang, L. Zhang, T. N. Fu / Video Painting via Motion Layer Manipulation

Figure 8: Painterly rendering of video frames by our approach. Motion layer extraction is indicated by the gray scale mask.

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

