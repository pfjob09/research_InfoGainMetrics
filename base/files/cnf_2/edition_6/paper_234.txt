DOI: 10.1111/j.1467-8659.2010.01751.x

COMPUTER GRAPHICS

forum

Volume 29 (2010), number 8 pp. 2372–2386

Bayesian Scheme for Interactive Colourization,
Recolourization and Image/Video Editing
Oscar Dalmau1 , Mariano Rivera1 and Teresa Alarc´on2

2 Centro

1 Centro de Investigaci´
on en Matem´aticas, A.C. Jalisco S/N, Colonia Valenciana C.P. 36240, Guanajuato, Gto, M´exico
Universitario de los Valles, Universidad de Guadalajara, Carretera Guadalajara Ameca Km. 45.5 C.P. 46600, Ameca, Jalisco, M´exico

Abstract
We propose a general image and video editing method based on a Bayesian segmentation framework. In the first
stage, classes are established from scribbles made by a user on the image. These scribbles can be considered as
a multi-map (multi-label map) that defines the boundary conditions of a probability measure field to be computed
for each pixel. In the second stage, the global minima of a positive definite quadratic cost function with linear
constraints, is calculated to find the probability measure field. The components of such a probability measure
field express the degree of each pixel belonging to spatially smooth classes. Finally, the computed probabilities
(memberships) are used for defining the weights of a linear combination of user provided colours or effects
associated to each class. The proposed method allows the application of different operators, selected interactively
by the user, over part or the whole image without needing to recompute the memberships. We present applications
to colourization, recolourization, editing and photomontage tasks.
Keywords: colourization, recolourization, image editing, segmentation, Markov random field
ACM CCS: Computer Graphics [I.3.8 ]: Applications—Image Colourizarion, Recolourizaction

2. we extend the previous operator to other image and
video editing tasks,

1. Introduction
In this paper, we propose an interactive method for image/video colourization and editing. Our method is based on
a probabilistic Bayesian framework for image segmentation.
Our strategy consists of applying a particular image transformation to each segmented region. We demonstrate that our
framework is very general, it can be applied to monochrome
or colour images, as well as to video. Our technique accepts
different kind of transformations: colourization, recolourization, tonal transformations, artistic effects and geometric
transformations.
Our proposal extends and generalizes our previous conference paper [DRM07], in which we proposed an interactive
method for the particular task of colourization. The principal
contributions of this paper are
1. we improve the colourization operator proposed in our
previous work [DRM07],
c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics
Association and Blackwell Publishing Ltd. Published by
Blackwell Publishing, 9600 Garsington Road, Oxford OX4
2DQ, UK and 350 Main Street, Malden, MA 02148, USA.

3. we introduce the distance as a feature for probabilistic
segmentation methods,
4. we introduce an off-line entropy control for probabilistic segmentation methods.
Unlike reported approaches for image editing, we present
an interactive framework for grey and colour image/video
editing tasks. The criterion we use for extracting similar regions is based on a similarity measure given by probability
distributions. In the case of colourization (or recolourization), our method assumes that regions with similar intensity
(colour) distributions should have similar colours. In general,
the same transformation is applied to regions with similar
distribution.
Recent works for image editing are reported in Refs.
[LLW04, WC05, LFUS06, YS06, DRM07, AP08]. Most

2372

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

Figure 1: Interactive colourization using multi-maps for defined regions. (a) Original greyscale image (luminance channel), (b) multi-map and (c) coloured image in which the
chrominance channels are provided by the user. The colourization was achieved with the method presented in this paper.
of them tackle a particular image editing task. Some solutions to the matting problem have been proposed in Refs.
[WC05, RKB04, WC05, LRAL08]. Lischinski et al. present
an interactive technique for the local adjustment of tonal
values [LFUS06]. An and Pellacini present an interactive
editing method for tonal adjustment, and for changing the
appearance (low and high dynamic range) of colour images
[AP08]. Both methods use a propagation strategy based on a
quadratic functional. Yatziv and Sapiro present an approach
close related with our scheme, see Ref. [YS06]. They compute a layered map of geodesic distances from each pixel
to the scribbles. Another particular editing task is image
colourization. This technique consists of introducing colour
to greyscale, sepia or monochromatic images, for instance
see Refs. [LLW04, SBv04, YS06, DRM07].
In spite of the fact that many colourization algorithms have
been developed in recent years [RAGS01, WAM02, Hor02,
BR03, CWSM04, WH04, LLW04, SBv04, QG05, TJT05,
KV06, YS06, QWCO∗ 07, DRM07], only a few of them
are based on a segmentation procedure [CWSM04, TJT05,
KV06, DRM07]. The reason seems to be, for colouring purposes, that regions to be segmented come from different
groups: faces, coats, hair, forest, landscapes and, up to now,
there has not been a proficient method that automatically discerns among this huge range of features. In the absence of
a general purpose automatic segmentation algorithm, other
alternatives have appeared in recent years and techniques
that use human interaction (semi-automatic algorithms) have
been introduced as part of computer vision algorithms.
In interactive colourization procedures, one uses a
greyscale image as the luminance channel and estimates the
chrominance for each pixel. This is achieved by propagating the colours from user labelled pixels. The process is
illustrated in Figure 1. One can find two main groups in
colourization methods. In the first group, one can set those
methods in which the colour is transferred from a source
colour image to a greyscale one using some corresponding
criteria [RAGS01, WAM02, Hor02]. In the second group are
the semiautomatic algorithms that propagate the colour information provided by a user in some regions of the image
[LLW04].

2373

Among automatic methods we can mention the following
works. Reinhard et al. stated the basis for transferring colour
between digital images [RAGS01]. Afterwards, Welsh et al.
extended the previous method for colourizing greyscale images using a scan-line matching procedure [WAM02]. In that
technique, the chromaticity channels are transferred from
the source image to the target image by finding regions that
best match their local mean and variance of the luminance
channel. To improve the last method, Blasi and Recupero
proposed a sophisticated data structure for accelerating the
matching process: the antipole tree [BR03]. Chen et al., in
Ref. [CWSM04], proposed a combination of composition
[PD84] and colourization [WAM02] methods. First, they extract objects from the image to be colourized by applying a
matting algorithm, then each object is colourized using the
method proposed by Welsh et al., and in the last step, they
make a composition of all objects to obtain the final colourized image. Tai et al. treat the problem of transferring colour
among regions of two natural images [TJT05].
Automatic colour transfer methods will equally transfer colour between regions with similar luminance features
(grey-level mean, standard deviation or higher-level pixel
context features, as in [ICOL05]). However, this approach
may fail in many cases. For instance, consider the simple
case in which the grey mean is the used feature, then a grass
field and a sky region may have similar greyscale values,
but different colours. Therefore, in general, colourization requires high-level knowledge that can only be provided by the
final user: a human. For this reason, semiautomatic methods
have successfully been used in image colourization. These
methods take advantage of user interaction for providing
high-level knowledge [LLW04, KV06, YS06, DRM07]. For
instance, Levin et al. presented a novel interactive method
for colourization based on an the minimization of a freeparameter cost functional [LLW04].
Video colourization is, in general, implemented as an hybrid technique: the user provides scribbles for some frames
and afterward such information is propagated to the reminder
frames. Previous solutions to this problem are reported in
Refs. [LLW04, SBv04, YS06]. The method in Ref. [YS06]
uses the idea of geodesic distance in volumetric data (i.e.
with the time as the third dimension). In spite of the method
presents good results it is limited to work with sequences
with small displacements: videos with fixed background
(without camera movements) and short movements of objects. Another approach to video colourization is presented
in Ref. [LLW04]. This approach uses optical flow (Lukas
and Kanade [LK81]) information for redefining 3D pixels
neighbouring.
In this paper, we propose an interactive image/video editing framework. Our method is general enough for implementing very different editing process for image and video. The
paper is organized as follows. Section 2 presents a description of the general scheme. To simplify our presentation, we

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2374

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

study the particular case of colourization in Section 3. Then
the generalization and extensions to other editing effects are
presented in Section 4. Section 5 presents an application for
video colourization. That application illustrates how to combine intensity and spatial distance features in our framework.
Section 6 shows experimental results and, finally, we present
our conclusions in Section 7.

2. Description of the Proposed Scheme
Our proposal uses a segmentation procedure based on the
minimization of a quadratic cost function with well-known
convergence and numerical stability properties [ROM07,
RDT08]. The scheme can be summarized in the following
steps:
1. Feature learning,

Figure 2: Result of colourization using 6 classes. (a)
greyscale image, (b) scribbled image, (c) palettes link and
(d) colourized image.

3. Image Colourization
In this section, we present our proposal scheme for colourization task, thoroughly explaining all its details and benefits.
In the next sections, we extend this scheme to recolourization
and other interactive tasks.

2. Image layering,
3. Layering process and composition operator.
In this scheme, the interactive process is concentrated in
the feature learning stage, see details in Section 3.1. Basically, some scribbles are made by a user on an image, these
scribbles should describe regions on which some transformation will be applied. We have then, a set of colours used
to make the scribbles in some regions of the image. This
colour set is named the label palette. Moreover, we have an
operator bank composed by filters (e.g. directional blurring),
enhancement operators, tone transfer functions [GW02], geometric transformations (rotation, scaling, etc.), artistic effects [Hol88, McA04], a second set of colours named the
colourization palette, in the case of colourization. How many
colours (for the label palette) and similarly, how many operators (in the operator bank) to use is decided by the user.
The label palette is actually formed by colours that are considered as labels, and will be used for the learning process.
The feature learning process consists of achieving empirical
distributions of certain features of the image, for example
in our experiments we use colour (intensity) empirical histograms on regions marked by the user, although other local
characteristics could also be included.
The second step, image layering, conceptually consists of
a soft segmentation of the image, or a sequence of images,
in regions with an assumed similar colour histogram, or in
general it is assumed that regions to be transformed have similar distribution of certain feature: intensity, colour, distance,
etc. This approach requires achieving a robust segmentation
method that is, in itself, a challenging problem. Afterwards,
comes the Layering process. In this step, the bank of operators will be used. Then, the layering process depends on the
selected operators or on the specific editing task. Finally, a
composition operator is applied to combine the transformed
layers.

In the image colourization task, the layering process, and
composition operator of the general scheme presented in Section 2 are combined in one step that we call here transferring
colour operator. First, some scribbles are made by a user on
a greyscale image, these scribbles should describe regions
that will have the same colour. The operator bank is actually
the colourization palette that will be used to colourize the
image. Therefore, we have two colour palettes with the same
number of colours, the first one containing colours used to
make the scribbles (label palette) and the second one containing colours to colourize the image (colourization palette)
[Figure 2(c)]. In the second step, we apply a probabilistic
image segmentation method and, finally, in the last step, we
need to build an appropriate transferring colour function and
assign colour to each detected region.

3.1. Feature learning
The introduced notation in this section will be preserved for
rest of the paper. We consider the segmentation case in which
some pixels in the region of interest, , are labelled by hand
in an interactive process. Assuming K as the class label set,
we define the pixels set (region) that belongs to the class k as
Rk = {r : R(r) = k}, and
R(r) ∈ {0} ∪ K, ∀r ∈

,

(1)

is the label field (class map or multi-map) where R(r) =
k > 0 indicates that the pixel r is assigned to the class k
and R(r) = 0 if the pixel class is unknown and needs to be
estimated. Let g be an intensity image such that g(r) ∈ t,
where t = {t1 , t2 , . . . , tT } are discrete intensity values. Let
hk (t) : R → R be the intensity empirical histogram on the
marked pixels which belong to class k, where
hk (t) =

r∈Rk

δ(g(r) − t)
|Rk |

(2)

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

is the ratio between the number of pixels in Rk whose intensity
is t and the total number of pixels in the region Rk . We
smooth hk (t), see [DHS01, HTF09], and denote as hˆ k (t) the
smoothed normalized histograms (i.e. t hˆ k (t) = 1) of the
intensity values, then the likelihood of the pixel r to a given
class k is computed with
vk (r) =

(hˆ k ◦ g)(r) + ε
, ∀k > 0,
K
[(hˆ j ◦ g)(r) + ε]

(3)

j =1

where ◦ is the composition operator, that is (hˆ k ◦ g)(r) =
hˆ k (g(r)), ε is a small positive real value, we used ε =
1 × 10−4 . Note that, although parametric models (such as
Gaussian Mixture Models) can be used for defining the likelihood functions, we used smoothed normalized histograms
because they are computationally more efficient, that is they
are implemented as look up tables. In our experiments, we
are just using the pixel intensity as local feature, however
other local statistics could be taken into account, as the local
mean of the intensity values on a pixel neighbourhood.

3.2. Image layering
Now the task is to compute the probability measure field α
at each pixel such that αk (r) is the probability of the pixel r
to be assigned to the class k. Such a probability vector field
α must satisfy
K

αk (r) = 1,

(4)

k=1

αk (r) ≥ 0, ∀k ∈ K, ∀r ∈
α(r) ≈ α(s), ∀r ∈

,

, ∀s ∈ Nr ,

(5)
(6)

where Nr denotes the set of the first neighbours of r:
Nr = {s ∈ : |r − s| = 1}. Note that, the conditions in
Equations (4) and (5) constrain α to be a probability measure
field and the condition in Equation (6) to be spatially smooth.
The probability measure field α allows us to divide the image
in layers in correspondence to the established classes.
Although our framework admits any probabilistic segmentation method [MVRN01, MAB03, ROM07] we select a
Quadratic Markov Measure Field (QMMF) model [ROM07]
because the solution conducts to a linear system. According
to [ROM05, ROM07], the smooth image multi-class segmentation is formulated as the maximization of the posterior distribution, that takes the form P (α | R, g) ∝ exp[−U (α, θ )]
and the maximum a posteriori (MAP) estimator is computed
by minimizing the cost function:
α(r)T D(r)α(r) +

U (α) =
r

λ
wrs ||α(r) − α(s)||22 ,
2 s∈N
r

(7)

2375

subject to the constraints in Equations (4) and (5); where
D(r) = −diag{log v1 (r), log v2 (r), . . . , log vK (r)}
is a definite-positive diagonal matrix, that is vk (r) ∈
(0, 1) ∀r ∈ , k ∈ K.
The soft constraint in Equation (6) is enforced by introducing a Gibbsian prior distribution based on Markov Random
Field (MRF) models [GG84, Li01, WG04] and the spatial
smoothness is controlled by the positive parameter λ in the
regularization potential [second term in Equation (7)], where
the weight wrs ≈ 0 if a class edge is probably allocated between the pixels r and s, otherwise wrs ≈ 1. As the weight
function, see [DRM07], we use
wrs =

β
β + |g(r) − g(s)|2

(8)

where β is a small positive value, that is β = 10−3 .
The convex quadratic programming problem in
Equation (7) subject to the constraints in Equations (4) and
(5) can efficiently be solved by using the Lagrange multiplier procedure for the equality constraint [Equation (4)].
Unlike the proposal in [ROM05], we are not penalizing the α(r)’s entropy, and thus the energy functional in
Equation (7) is convex. Therefore, we guarantee convergence to the global minima see [ROM05, ROM07]. For our
purposes, we have found that the mode (hard segmentation
computed as the winner-take-all) of the solution should be
correct. The entropy, that controls the smooth transition between classes, can be adjusted off-line at the composition
stage (Section 3.3).
3.3. Transferring colour operator
Once the measure field, α, is computed, colours Ck =
[Rk , Gk , Bk ]T in RGB colour space are selected by the user
to form the colourization palette, thereafter every selected
colour is assigned to a colour (or class) in the corresponding
label palette by the user. Then, the colour Ck is converted into
a colour space in which the intensity and the colour information are independent. For example, the colour spaces: Lαβ
[RCC98, RAGS01], YIQ [GL00, WOZ02, YK03, GWE04],
YUV [WOZ02, GWE04], I1 I2 I3 [GL00], HSV [GWE04],
CIE-Lab and CIE-Luv [WS82]. In general, we denote the
transformed colour space by LC1 C2 :
⎛
⎛
⎞
⎞
Rk
Lk
⎜
⎜
⎟
⎟
⎜ C1k ⎟ = T ⎜ Gk ⎟ ,
(9)
⎝
⎝
⎠
⎠
C2k
Bk
where Lk is the luminance component, C1k , C2k are the
chrominance components for the class k; T is the applied transformation. Such a transformation is linear for the
YIQ, YUV, I1 I2 I3 spaces. For the CIE, lαβ and HSV spaces,
the transformation T is non-linear. The reader can find details of the colour transformations used in this paper in Refs.

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2376

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

[RCC98, WS82, OKS80]. For computing the colour component (to colourize) at each pixel, we obtain the components
l(r), c1 (r) and c2 (r)(∀r ∈ ) in the LC1 C2 colour space, see
Equations (10)–(12). We based our colourization operator on
the one proposed by Dalmau et al. in [DRM07]. Unlike that
work, we propose to change the luminance component of
the original image. The colour components are obtained as a
linear combination of the chrominance components C1k and
C2k . Our colourization operator is defined as
ˆ
l(r) = (f ◦ l)(r),

(10)

K

c1 (r) =

αˆ k (r)C1k ,

(11)

αˆ k (r)C2k ,

(12)

k=1

K

c2 (r) =
k=1

where lˆ is the first component of T (g), that is lˆ is the luminance component of the grey intensity image g, and f is
a transformation function of the luminance component, that
is an enhancement operator, a piecewise-linear transformation, a tone transfer function or an intensity transformation
function [GW02]. The introduction of the function f is very
important because now we can modify the luminance channel of the original image, and for instance, in colourization
task, it allows us to colourize dark region with lighter colours.
Section 4.1 presents a procedure for estimating f for adapting the image intensity to be close to the one of the selected
colour.
ˆ
can be understood as the
The components αˆ k (r) of α(r)
contribution (matting factor) of the class colour Ck to the
pixel r. The matting factors are computed with
αˆ k (r) =

αkn (r)
,
K
n
j =1 αj (r)

(13)

where the n-factor allows us to control the colour transition smoothness (off-line entropy control), for n → ∞ the
colourization is achieved with flat colours such as in a cartoon. According to our experiments, the entropy control can
easily and effectively be adjusted with the power n. Note that,
because of Equation (4), for n = 1, one has αˆ k (r) = αk (r).
Finally, the coloured image g˜ is transformed into the RGB
colour space by applying the corresponding inverse transformation
⎞
⎛
l(r)
⎟
⎜
c (r) ⎟
˜ = T −1 ⎜
(14)
g(r)
⎝ 1 ⎠.
c2 (r)
Observe that the step of assigning colour to each region
Rk is completely independent of the segmentation stage. It
means that, once we have computed the vector measure field,

Figure 3: Colourization using six classes. Each image represents a component of the vector measure of the probability
of each class (αk layer).

α, for the whole image, we can reassign colours to one or
more regions by just recomputing the colour components
with Equations (11) and (12), and transforming the image
with the Equation (14). Also it may be possible to assign the
same colour to different regions. This makes the proposed
method very versatile.
In summary, the process of colourization begins by making
a ‘hard’ pixel labelling from the user marked regions (multimap): for every r ∈ Rk with k > 0, we set αk (r) = 1 and
αl (r) = 0 for l = k. Next, the remainder pixels are “soft”
segmented by minimizing the functional in Equation (7).
Figure 2 illustrates the colourization process. Panel (a) shows
the original greyscale image, the scribbles for six classes are
shown in Panel (b) and the coloured image in Panel (d).
The computed class memberships (probabilities or αk layers) are shown in Figure 3. Finally, the colour assignment
to each region is done by applying Equations (10)–(12) in
LC1 C2 colour space followed by the inverse transformation
into RGB colour space. Note that smooth inter-region probability transitions produce smooth colour transitions (as in the
face regions) and, conversely, sharp intensity edges produce
sharp colour transitions.

4. Extension and Generalization of the Colourization
Scheme
In this section, we extend our colourization scheme to image
and video editing. The Section 4.1 presents a procedure for
adjusting the luminance component according to the selected
colourization palette. Sections 4.2 and 4.3 present our extension for image recolourization. Section 4.4 generalizes the
transferring colour operator to other operators. Section 4.5

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

presents a mechanism for including in our framework other
image features, as for instance shortest path distance.

4.1. Luminance adjustment
One problem of the colourization methods in [DRM07,
LLW04, YS06] is that they only use the chrominance components of the colours provided by the user. So, in many cases,
the resulting colourized image has regions whose colours are
very different from those given in the colourization palette.
This is due to, the values of the luminance channel in these regions mismatched with the corresponding luminance channel
of the user given colour. In this section, we study a particular
transformation function of the luminance component lˆ of the
image. This function is of the form
⎧
⎪
0,
ak lˆ ≤ 0,
⎪
⎨
ˆ = ak l,
ˆ
(15)
fk (l)
0 < ak lˆ < M,
⎪
⎪
⎩ M,
ak lˆ ≥ M,
where ‘M’ is the maximum value of the luminance component in the LC1 C2 space and ak is a scalar value (luminance
gain control) for each class. The aim is to obtain at each
region to be colourized a colour as similar as possible to that
given by the user, in the colourization palette. Although ak
could be manually adjusted, we found that this gain control ak
can automatically be computed by solving the optimization
problem
ak = arg min
a

ˆ − Lk )2 , s.t.: 0 ≤ a ≤ bk , (16)
(a l(r)
r∈ k

where k is the region of the image that corresponds to
ˆ > 0) and Lk is the
(with l(r)
the class k, bk = max M l(r)
r∈ k ˆ
luminance component of the kth colour in the colourization
palette, see Equation (9). The solution to (16) is given by the
closed formula
ak = min bk ,

Lk

ˆ

r∈ k l(r)
lˆ2 (r)

.

r∈ k

The semi-automatic or interactive recolourization is a
straightforward extension of colourization task. The difference is that now, the image g is given in the RGB colour
space, so g(r) ∈ t, where t = {t1 , t2 , . . . , tT } are vectorial
values that correspond to channels in the RGB colour space.
Therefore, the density distributions for each class are then
empirically estimated in RGB colour space by using a histogram technique, similar to the colourization case, but now
δ(g(r)−t)

is
in R3 , i.e. hk (t) : R3 → R, where hk (t) = r∈Rk|Rk |
the ratio between the number of pixels in Rk whose colour is
equal to t and the number of pixel in the region Rk , see Ref.
[DHS01, HTF09]. Then, we can compute the likelihood of
each pixel r belonging to class k. That is, let hˆ k : R3 → R be
the smoothed normalized histograms ( t hˆ k (t) = 1) of vectorial values, then likelihood of the pixel r to a given class k
is computed by using the Equation (3).

4.3. Quasi-automatic recolourization
We present a variant for colourization that reduces the user
interaction. Our method relies on the assumption that for a
particular class of images the likelihood density functions
have previously been learned. The instance we present is the
case in which the classes correspond to linguistic colours:
brown, green, orange, red, etc. [BK69]. In Ref. [AM09], Alarcon and Marroquin proposed a segmentation method based
on a combination between a colour categorization method
and Bayesian technique. The elaborated colour categorization model describes each voxel in the colour space L∗ u∗ v ∗
as a vector of probabilities, whose components express the
degree to which the voxel belongs to one of the eleven colour
categories established by Berlin and Kay [BK69] in 1969,
as basic and universal. Alarcon and Marroquin obtained
the colour categorization model by an interactive technique.
They performed a colour naming experiment considering 336
colour samples and 32 subjects. Each subject was instructed
to select the most likely colour basic category k for the observed colour sample c. With this procedure the likelihood,
P (k | c), for each shown colour sample is calculated, using
the expression [AM09]
P (k | c) =

4.2. Semi-automatic recolourization
The recolourization task consists of changing colours in the
original image. Unlike colourization in which g is a grey
level, now g represents a colour image. A common approach
to such a task is based on the definition of a context independent mapping function from the original colours to the
new ones. This simple approach, although computationally
efficient, fails with noisy images or when similar colours are
mapped to different colours. Here, we propose two possible
extensions of the above-presented colourization scheme for
recolourization task, that is robust to the above-mentioned
problems.

2377

Akc
, k = 1, . . . , K,
M

(17)

where Akc denotes the number of assignments to the class k
for the colour sample c, and M is the total number of subjects;
K is the number of the used colour basic categories and it was
considered K = 11. The number of colour classes included
in [AM09] is non-arbitrary, but is based on relevant findings
about the human colour naming process, reported by Berlin
and Kay [BK69]. Nevertheless, the colour model proposed by
Alarcon and Marroquin, as specified in [AM09], can include
different categories, respect to those established by Berlin
and Kay. More details related to the colour model elaboration are in Ref. [AM09]. As a result of the colour naming
experiment done in Ref. [AM09] the likelihood, P (k | c), for

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2378

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

a limited number of colour samples in the colour space was
calculated. To know the likelihood for the whole space, Alarcon and Marroquin [AM09] used an interpolation procedure
in which each category is modelled as a linear combination
of quadratic splines. After the interpolation process, the likelihood function P˜ (k, c) is obtained, i.e. the likelihood P˜ (k, c)
is known for each category k and for all colours c in the colour
space. The computed solution is interpreted as a probabilistic measure field (a probabilistic dictionary), that is used in
Ref. [AM09] as likelihood in a segmentation Bayesian technique. From the above, the obtained colour model gives a
quasi-automatic segmentation method, that could be applied
in many image processing tasks. colour video restoring and
video recolourization are examples of these tasks, in which
the use of this technique leads to the considerably reduction of the human effort. The quasi-automatic recolourization approach, proposed in this paper, considers the colour
categorization model P˜ , see [AM09], and the algorithm is
the following:
Algorithm 1: Quasi-automatic recolourization
Require: I (colour image) and P˜ (colour categorization model)
1. Set the likelihood v using P˜ , i.e. vk (r) ← P˜ (k | I (r)), for all k,
r.
2. Probabilistic segmentation, i.e. Minimize the functional in
Equation (7) subject to the constraints in the Equations (4)–(5).
3. Apply the colourization operator, see Equations (10)–(12).

Note that the segmentation (steps 1 and 2) is completely
automatic. The semiautomatic part is hidden in the colour
naming experiment of the colour categorization model P˜ , that
in our recolourization experiments we assume to be known,
Ref. [AM09]. Due to the colour categorization model, now
the label palette is not fixed by the user, but automatically
by P˜ , and it includes only the basic colour categories set by
Berlin and Kay [BK69]. The solution found in step 2 gives
the segmentation of colour image in terms of basic colours.
However, as specified in [AM09], we can learn different
colour categories and different number of them, that is K =
11. This will depends on the colour composition of the image,
or set of images, for the specific application. In the step 3,
the label palette and the colourization palette are considered
for applying colourization operator (Section 3.3). Like in the
colourization approach (Section 4.3), the colour palette is
again established interactively by the user.

Figure 4: To each label (left column) is assigned a composition operator (right column).
1. Feature learning,
2. Image layering,
3. Layering process and Composition operator.
The first two stages are similar to the Section 3, and only
the third stage is changed. The transferring colour function
is replaced by: layering process and a more general operator,
named here composition operator. The layering process includes tonal transferring functions, artistic/geometric transformations, image filters, etc. Instances of such processing
are blur or sharpen, or any other effect or combinations. The
composition operator allows us to combine the information
(processed layers) obtained in the previous step. This operator set could be applied at the same time or sequentially.
In this modality, the user widens his interaction options.
As explained in Section 2, the interactive process of stage
one consists of scribbling, labelled pixels, made by a user in
different regions of interest. Actually, these labelled pixels
are the training data of the (supervised) learning stage. The
second stage is the same as in the Section 4.3. In the third
stage, an operator set F = {F1 , F2 , . . . , FK } is selected
by the user. The triad operator Fk = [Fl,k , Fc1 ,k , Fc2 ,k ]T acts
over the layer k and over each channel, that is L, C1 and C2 .
Hence, similarly to colourization, each operator in the operator set must be linked to a colour in label palette (Figure 4).
Finally, the operator set F is applied on each detected layer,
and is combined using the following composition operator:
K

l(r) =

ˆ
αˆ k (r)Fl,k [l](r),

(18)

αˆ k (r)Fc1 ,k [cˆ1 ](r),

(19)

αˆ k (r)Fc2 ,k [cˆ2 ](r).

(20)

k=1

K

c1 (r) =
k=1

K

c2 (r) =

4.4. Image editing effects

k=1

The scheme explained in Section 4.3 can be very useful
for other interactive image processing tasks, if some slight
change is made. Now we explain the more general scheme, in
which colourization is a particular case. Based on the scheme
presented in Section 2, we have the following interactive
general scheme:

We note again that F is a family of operators. That is, these
operators can be functions that change the intensity or colour
of the image (tone or colour transferring functions) similar to
the ones used in Refs. [LFUS06, AP08], and also functions
that change the geometry of the image (artistic or geometric
transformations), see Ref. [Hol88].

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

Then the solution to the unmarked pixels is computed by
solving the set of linear systems

Instances of such operators are:
1. The identity operator: E[f ] = f ,

LX = −B T M,

2. The layer selection: Sk [f ] = αk f ,
3. The multiple layer selection: Suppose that the user select a set of layers A ⊂ K then SA [f ] = k∈A Sk [f ],
4. The colourization, see Section 4.3,
5. The recolourization, see Section 4.4, and
6. The matting, see Refs. [LRAL08, RKB04, WC05],
among many others. We show some combinations of operators in the experiment section, Section 6, where characteristics of experiments are discussed, see Figures 19, 20, 21.
We remark that the layer operator F can be composed by
a sequential application of basic operators, for instance a
selection-colourization-bluring (see our experiments)

4.5. Shortest-path distances
Intensity (or colour) distributions allow to segment images
into classes where the pixels have similar intensity or colour.
Under this assumption, a class can be partitioned in different (disconnected) regions. However, there can be particular
tasks where connectedness is an important feature to be taken
into account. For instance, the methods in [Gra06, YS06] segment images in regions with small geodesic distances. We
can incorporate connectedness information into the QMMFs
framework by defining the pixel likelihood that depends on
some kind of distance between each pixel to the user scribbles.
In this work, we incorporate the distance between pixels
in form of a graph connectivity, thai is the larger distance,
the smaller connectivity factor (and conversely) [YVW∗ 05].
According to the QMMF model, the connectivity factor is
better expressed as a likelihood probability u. This can be
computed, for instance, by means of a diffusion [Gra06], in
particular, with the random walker formulation through the
combinatorial Dirichlet problem
wrs xr − xs 2 ,

X = arg min
x

2379

(21)

r, s :
R(r) = 0,
s ∈ Nr

with X = [xrk ]r:R(r)=0,k∈K , x r ∈ RK and boundary conditions
at the marked pixels. The probability of the class corresponding to marked pixels is fixed to 1 and for the other classes
fixed to zero
⎧
⎨ 1 R(r) = k, k > 0,
mrk =
otherwise.
⎩0

where L = [wrs ]r,s:R(r)=R(s)=0 , B = [wrs ]r,s:R(r)=0,R(s)=0 and
M = [mrk ], see [Gra06] for details. Then the connectedness
likelihood is
⎧
⎨ mrk R(r) > 0,
(22)
uk (r) =
otherwise.
⎩ xrk
Now we have two sources of information (likelihoods) u and
v: u that encodes some kind of spatial distance and v the
colour (intensity) likelihood. Then, assuming independence
between the information sources, the joint likelihood v¯ is
given by
v¯k (r) = uak (r)vk(1−a) (r),

(23)

where the mix parameter (power) 0 ≤ a ≤ 1 denotes our
confidence, or reliability, on each source. Thus, the join likelihood can be computed in a preprocessing stage and being
directly used in the QMMF framework by substituting D(r)
by
¯
D(r)
= −diag{log v¯1 (r), log v¯2 (r), . . . , log v¯K (r)}
into the cost function (7).
5. Application: Video Colourization
As a demonstration of the Section 4.5, we present an application for video colourization that incorporates two likelihood
sources: shortest path distances and intensity similarity, that
is we use the joint likelihood (23).
In the literature, one can find previous interactive works
dealing with this problem [LLW04, SBv04, YS06]. The
method presented in [YS06] is suitable for colourization of
videos with fixed background (without camera movements)
and small object displacements. The approach presented in
[LLW04] uses implicitly optical flow what allows to colourize more complex videos: larger displacements and camera
movements. This method does not propagate the scribbles,
but calculates an overall video colourization by redefining
neighbouring pixels, establishing in this way a temporal coherency. That is, the pixels r0 = (x0 , y0 ) and r1 = (x1 , y1 ), in
the frames t and t + 1 respectively, are neighbours if
(r0 + d(r0 , t)) − r1 < T ,

(24)

where T is a parameter, d(r, t) = d(x, y, t) = (dx (x, y, t),
dy (x, y, t)) represents the optical flow computed by an standard method, in particular [LLW04] uses the Lucas and
Kanade algorithm [LK81].
Here we present a video colourization method that also
incorporates an optical flow-based strategy. The basic idea

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2380

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

Figure 5: Scribbles made on one frame of the video.
This frame is available in a video at http://www.cs.huji.
ac.il/∼yweiss/colourization/.

Figure 7: Comparison between our method and the method
reported in [LLW04] when colourizing pixels are located far
away from the user’s scribbles or when colourizing pixels
are in disconnected regions. (a) Image, (b) scribbles, (c)
colourization using the Levin et al. method, (d) colourization
using the Dalmau et al. method and (e) colourization using
our proposal.

Figure 6: Four colourized frames representative of the full
colourized video.

consists of: given a frame g(r, t) and its corresponding user
scribbles mask(r, t) (a multi-map) then, we propagate the
scribbles to the frame t + 1 by warping the mask(t) using the
optical flow d(r, t) to obtain the multi-map at frame t + 1.
That is
mask(r, t + 1) = mask( r + d(r, t) + q , t),

(25)

where q = (0.5, 0.5), and · is the floor function, i.e. x
is the largest integer not greater than x. Then, we use the
estimated (warped) multi-map to colourize the t + 1-frame.
After a number of propagated multi-map, the user needs to
make small rectifications to the multi-map due to the error
introduced by the optical flow propagation process or large
scene changes. The algorithm continues up to a desired number of frames.
In Figure 5, we show the multi-map on the frame of the
video used for the experiment, available at http://www.cs.
huji.ac.il/∼yweiss/colourization/. Next panels show three
mulit-maps estimated by our method based on the standard
multi-grid optical flow algorithm of the Horn and Schunck
[HS80]. Figure 6 depicts four colourized frames of the video.
We note that our method relies strongly on the computed optical flow. It is implemented as a frame by frame colourization
and does not include a temporal regularization. However, that
temporal regularization can be introduced as in [LLW04].

6. Experiments and Results
To evaluate the performance of our proposal, several experiments were carried out. First, we describe the colourization
process by using an experiment. Then, we compare the performance of the colourization method proposed by Levin
et al. in [LLW04] and our method. In addition, we show
some other results of colourization and recolourization that
demonstrate the method capabilities. And finally, we present
some experiments of image editing.

Figure 8: Comparison between our method and the method
reported in [LLW04]. (a) Original image, (b) multi-map, (c)
Levin et al. result and (d) our result.

Figure 9: Comparison between our method and the method
reported in [LLW04] when colourizing dark regions with
light colours, or when colourizing light regions with dark
colours. (a) Image, (b) scribbles, (c) colourization using
the Levin et al. method, (d) colourization using the Dalmau
et al. method and (e) colourization using our proposal.
Most of the images presented here were taken from Berkeley Image Database [MFTM01], available at http://www.
eecs.berkeley.edu/Research/Projects/CS/vision/bsds/. First,
they were converted into greyscale images and then the process of colourization was applied.
6.1. Colourization experiments
Figures 7–10 compare our method with the one proposed
by Levin et al. [LLW04] and Dalmau et al. [DRM07].
In the experiments of Figures 7–9, the label palette and
colourization palette are the same, and the colours in both

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

2381

Figure 10: Comparison between our method and the method
reported in [DRM07]. (a) label palette (left column) and
colourization palette (right column), (b) multi-map, (c) Dalmau et al. result and (d) our result.
palettes are the ones used in the corresponding multi-map.
In general, the methods reported in [LLW04] and [DRM07]
have good results in many situations. However, Figures 7
and 8 show that the Levin et al. method, code available
at http://www.cs.huji.ac.il/∼yweiss/Colourization/, obtains
poor results when the image has disconnected regions that
need to be colourized with the same colour, or when the
pixels to be colourized are far away from the user’s scribbles. Both methods, the reported in [LLW04] and [DRM07],
also work poorly when colourizing dark regions with light
colours or when colourizing light regions with dark colours
(Figure 9). To show the influence of the luminance component transformation in the recolourization context, in Figure
10 we compare our proposal with the Dalmau et al. method.
Based on the visual comparison between Figure 10 (c) and
Figure 10(d), we conclude that the colour RGB composition
of the image obtained from the transformation luminance
function [Figure 10(d)], is more similar to the colours fixed
in the colourization palette than the image obtained without
using the transformation luminance function [Figure 10(c)].
To give a numerical argument of the above explanation, a test
of the RGB composition of the red class, see Figure 10 panels (a) and (b), was carried out. The RGB components used
to recolourize this class were [208, 54, 88], and the average
RGB components, corresponding to the red class, obtained
by Dalmau’s and our method were [153.4, 10.1, 32.7] and
[207.5, 62.2, 88.7], respectively.
In Figure 11, we show additional experiments that demonstrate the method capability. Moreover, once we have computed the probability measure field, α, we can reassign
colours to some labels and colourize the image by just reapplying Equations (10)–(12). In Figure 12, we present experimental results that illustrate the method flexibility by
changing colours and keeping the memberships fixed.
Experiments with different colour models demonstrate that
the final results do not depend on the chosen model but on
the user ability for selecting the appropriate colour palette,
see Figure 13.
6.2. Semi-automatic recolourization
Figure 14 illustrates a semi-automatic recolourization. The
first row, panels (a) and (b) show the original image and

Figure 11: Columns from left to right: greyscale image,
multi-map and coloured image.

Figure 12: Flexibility for assigning and reassigning colours
to images. (a) Original image and (b)–(d) colourizations.
the multi-map, respectively. Panels (c) and (d) show two recolourizations: panel (c) shows a recolourization that was
reached using the likelihood (Equation (3) and Section 4.2)
and panel (d) illustrates a recolourization that was attained
by using the QMMF based on the probabilistic segmentation,
α. For comparison purposes, in Figure 15 we show details
of the original image and the corresponding colourizations.
Note that in likelihood based colourization some regions in

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2382

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

Figure 13: Colourization results using different colour models: (a) Original image, (b) YUV, (c) LUV, (d) I1 I2 I3 , (e) Lαβ
and (f) HSV.
Figure 16: Recolourization with imprecise scribbles.

Figure 14: Semi-automatic recolourization. The first row,
from left to right, shows: (a) original image, (b) multi-map,
(c) recolourization by using likelihood and (d) recolourization by using segmentation. The second and third rows show
the class memberships (α layers) corresponding the likelihood and the soft segmentation, respectively.

Figure 15: Likelihood versus segmentation. Zoom of subimages taken from recolourization using, as a measure field,
the likelihood and the segmentation respectively. (a) Subimage, (b) recolourization using likelihood and (c) recolourization using segmentation.

the butterfly body are colourized in green because the colour
similarity of the butterfly body and the leaves. As we can see,
from this experiment, the likelihood is not enough for obtaining good recolourizations and in general the regularization
stage is needed.

Figure 17: Left Image, scribble on the first video frame of
the video in first row of Figure 18. Right image, automatic
generated multi-map from the user scribbles, see text.

The proposed scheme also admits a multi-map with imprecise scribbles, see the left image in first row of Figure 16.
This image shows a multi-map with scribbles that overlap
different regions (the horses and the grass). The left image
in the second row shows the recolourized image using this
multi-map. In this case, the vector field α is computed on
the whole image, that is α(r) is also computed in the pixels
marked by the user (R(r) = 0), and the multi-map is only
used to compute the likelihoods. The right panel in the first
row shows the automatically refined multi-map computed
by keeping the resulted classification at those pixels whose
entropy is small: 1 − k αk2 (r) < 0.4. The right panel in the
second row of Figure 16 depicts the recolourized image using
the refined multi-map.
In the next experiment, we assume that colour distributions
do not change throughout the video sequence. Therefore, the
method only needs one multi-map, that is the user scribbles
only on the first frame of the sequence. Left panel in Figure 17
exhibits the scribbles for three classes provided by the user
(multi-map). The right panel shows the multi-map refined
similarly to the previous experiment. First row in Figure 18
shows demostrative frames from the original video and the
semi-automatic recolourization is shown in second row. We
remark that about 300 frames were colourized using the

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

Figure 18: First row show 4 frames of a video sequence of
approximately 10 seconds (about 300 frames). Second row
shows the semiautomatic recolourization (the required user
scribbles are presented in Figure 17). Third row shows the
quasi-automatic recolourization. The difference in colourization corresponds to different selected {colourization palettes.

2383

Figure 19: (a) Original image, (b) mask obtained from vector measure field, (c) extracted colourized object (d) photomontage using the mask in panel (c).

distribution estimated from the first frame, the video duration
is approximately of 10 s.
6.3. Quasi-automatic recolourization
The follow experiments illustrate the quasi-automatic recolourization presented in Section 4.3. In this case, we
use fixed likelihood density functions that correspond to
the colour categorization model P˜ from Ref. [AM09], with
K = 11. As the label palette is fixed, the only user interaction
is the linkage between the label palette and the colourization
palette. Except for this small interaction, the procedure is
fully automatic.
Results of a video colourization experiment are shown in
the third row of Figure 18. The recolourized video frames
agree with the original frames in first row. We note that the
difference between recolourizations (second and third row) is
due to different colourization palettes used. Important to remark it is that both strategies preserve transparency between
the bear and the water.
6.4. Other image editing experiments
In this section, we illustrate the capability of our method
by some editing examples. As our method allows to divide
the image in layers, we can select some particular layers for
segmenting and colourizing a particular object (Figure 19).
More formally, let A ⊂ K be a layer set selected by a user.
Then, the mask in Figure 19(b) can be obtained by
mask(r) =

αˆ k (r).

(26)

k∈A

With the mask, Figure 19(b), we can now extract the colourized image Figure 19(c), or simply we can make a blending

Figure 20: Combining different operators: blur (red class),
identity (green class) and recolourization (blue class). (a)
Original colour image, (b) multi-map and (c,d) multioperator composition.
(image composition) with the colourized image and another
image [Figure 19(d)].
It is also possible to combine several operators
(Section 4.4). In Figure 20, three operators were used: directional blur (for the background), recolourization (for some
red parts in the car) and identity (for the rest of the car). The
goal is to introduce motion effect and recolourize part of the
car. We remark that the applied colourization method is not
a simple mapping between colours. It can be seen that while
some of the car’s red regions have been changed, others remain unchanged, such as the flag on the window and the logo
on the door [Figures 20(c) and (d)].

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2384

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

7. Conclusions
We have presented a three-stage interactive image and video
editing procedure. The first step consists of scribbles made by
a user over the image. The second step consists of computing
a probabilistic segmentation and in the third step the colour
or effect properties are specified.

Figure 21: Artistic effects. Source images (first row) and
artistic effects using a combination of the processed image
sources (second row), see text.

Moreover, the interactive general scheme accepts more
than one source image. Figure 21 show two source images
(first row). Thus, given the multi-map provided by interaction
(left panel of first row) we can obtain the composed images in
the second row. From left to right, using a pond ripple effect
to the source image 1 weighed by the layer that corresponds
to the red class and the source image 2 weighed by the
layer that corresponds to the green class (third image from
the left). Moreover, the composed image at the right can be
computed by using a circular ripple effect to the source image
2 weighed by the layer that corresponds to the green class
and the source image 1 weighed by the layer that corresponds
to the red class.
Regarding the computational cost, the image layering is
the most time consuming step in our proposal. Our method
computes the layering of an image by solving a convex
quadratic programming problem. This minimization corresponds to solving a positive definite and symmetric linear
system. The length of the linear system depends on the size
of the image and the number of classes. We implement our
approach using a Gauss–Seidel scheme in Matlab with .m
and .mex files. Although it is not the most efficient implementation, a 480×640 colour image can be segmented in
about a second per class on an Intel 2.5Mhz iMac. Our algorithm can be implemented using Multigrid Gauss-Seidel.
Moreover, it can be parallelized in blocks (for a multi-core
CPU) or in cellular automata (for a GPU based implementation, CUDA or OpenCL). This is beyond the scope of the
current work but we have a CUDA based implementation
for binary segmentation at a rate of 60 frames per second on
an NVIDIA 8800 GT. Our implementation is based on the
QMMF algorithm reported in [RD10].

The proposed interactive editing method is based on the
multi-class probabilistic image segmentation algorithm. The
segmentation process consists of the minimization of a linearly constrained positive definite quadratic cost function and
thus the convergence to the global minima is guaranteed. We
associate an image transformation (colourization, recolourization, directional blur, edge enhancement, etc) to each class
and then the pixel colour components are a linear combination of a set of operators applied over regions defined by the
classes. The colour component values depend on the computed probability of each pixel belonging to the respective
class.
We have demonstrated the flexibility of the presented
scheme by implementing different image and video editing
applications, as for instance: colourization, recolourization,
blending, blur and combinations. Our method can successfully be used for applying particular transformations to isolated objects. It also accepts different source images. As
result, complex artistic effects are obtained.
A quasi-automatic recolourization version is presented that
could be very useful in video colour restoration and video
recolourization.
We have extended the QMMF models to accept mixture
of likelihoods. New likelihoods can effectively codify the
segmentation of other methods. In particular, we investigate
the pixel connectivity through the shortest-path distance. The
shortest-path based likelihood was computed by the random
walker segmentation method. We included a video colourization application that relies on this approach.

Acknowledgments
This research was supported in part by CONACYT (grant
61367) and PROMEP (grant 103.5/08/2919). O. Dalmau was
also supported in part by a PhD scholarship from CONACYT,
Mexico. The authors thank the anonymous reviewers for their
advices and comments that helped to improve the quality of
the paper.

References
[AM09] ALARCO´ N T. E., Marrqu´ın J. L.: Linguistic colour
image segmentation using a hierarchical Bayesian approach. Color Research and Application 34 (August
2009), 299–309.

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

[AP08] AN X., PELLACINI F.: Appprop: all-pairs appearancespace edit propagation. In SIGGRAPH’08: ACM SIGGRAPH 2008 Papers (New York, NY, USA, 2008), ACM,
pp. 1–9.
[BK69] BERLIN B., KAY P.: Basic Color Terms: Their Universality and Evolution. University of California, Berkeley,
1969.
[BR03] BLASI D., RECUPERO R.: Fast colorization of gray
images. In Eurographics Italian Chapter 2003 (2003).
[CWSM04] CHEN T., WANG Y., SCHILLINGS V., MEINEL C.:
Grayscale image matting and colorization. In Proceedings
of Asian Conference on Computer Vision (ACCV 2004)
(27–30 January 2004), pp. 1164–1169.
[DHS01] DUDA R. O., HART P. E., STORK D. G.: Pattern
Classification (2nd edition). John Wiley & Sons, Inc., New
York, 2001, pp. 164–165.
[DRM07] DALMAU O., RIVERA M., MAYORGA P. P.: Computing the alpha-channel with probabilistic segmentation for
image colorization. In IEEE Proceeding Workshop in Interactive Computer Vision (ICV’07) (2007), pp. 1–7.
[GG84] GEMAN S., GEMAN D.: Stochastic relaxation, Gibbs
distributions and Bayesian restoration of images. IEEE
PAMI 6 (1984), 721–741.
[GL00] GUO P., LYU M. R.: A study on color space selection for determining image segmentation region number. In Proceedings of the 2000 International Conference
on Artificial Intelligence (IC-AI’2000), Monte Carlo Resort, Las Vegas, Nevada, USA (2000), vol. 3, pp. 1127–
1132.
[Gra06] GRADY L.: Random walks for image segmentation.
IEEE Transactions on Pattern Analysis and Machine Intelligence 28, 11 (November 2006), 1768–1783.
[GW02] GONZALEZ R. C., WOODS R. E.: Digital Image Processing (2nd edition). Prentice-Hall, Upper Saddle River,
New Jersey, USA, 2002, pp. 76–107.
[GWE04] GONZALEZ R. C., WOODS R. E., EDDINS S. L.:
Digital Image Processing Using Matlab. Prentice-Hall,
Upper Saddle River, New Jersey, USA, 2004, pp. 204–
207.
[Hol88] HOLZMANN G. J.: Beyond Photography—The Digital
Darkroom. Prentice-Hall, Englewood Cliffs, NJ, 07632,
1988, pp. 109–114.
[Hor02] HORIUCHI T.: Estimation of color for gray-level image by probabilistic relaxation. In Proceedings of the IEEE
International Conference Pattern Recognition (2002),
pp. 867–870.

2385

[HS80] HORN B. K., SCHUNCK B. G.: Determining Optical
Flow. Technical Report, Massachusetts Institute of Technology, Cambridge, MA, USA, 1980.
[HTF09] HASTIE T., TIBSHIRANI R., FRIEDMAN J.: The Elements of Statistical Learning: Data Mining, Inference,
and Prediction (2nd edition). Springer Science+Business
Media, 233 Spring Street, New York, NY, USA 10013,
2009, pp. 208–209.
[ICOL05] IRONY R., Cohen-Or D., LISCHINSKI D.: Colorization by example. In Eurographics Symposium on Rendering 2005 (EGSR’05) (2005), pp. 201–210.
[KV06] KONUSHI V., VEZHNEVETS V.: Interactive image
colorization and recoloring based on coupled map
lattices. In Graphicon’2006 Conference Proceedings,
Novosibirsk Akademgorodok, Russia (2006), pp. 231–
234.
[LFUS06] LISCHINSKI D., FARBMAN Z., UYTTENDAELE M.,
SZELISKI R.: Interactive local adjustment of tonal values.
In SIGGRAPH’06: ACM SIGGRAPH 2006 Papers (New
York, NY, USA, 2006), ACM, pp. 646–653.
[Li01] LI S. Z.: Markov Random Field Modeling in Image
Analysis. Springer-Verlag, Tokyo, 2001, pp. 11–15.
[LK81] LUCAS B. D., KANADE T.: An iterative image registration technique with an application to stereo vision (IJCAI). In Proceedings of the 7th International Joint Conference on Artificial Intelligence (IJCAI’81) (April 1981),
pp. 674–679.
[LLW04] LEVIN A., LISCHINSKI D., WEISS Y.: Colorization
using optimization. ACM Transactions on Graphics 23, 3
(2004), 289–694.
[LRAL08] LEVIN A., Rav-Acha A., LISCHINSKI D.: Spectral matting. IEEE Transactions on Pattern Analysis and Machine Intelligence 30, 10 (2008), 1699–
1712.
[MAB03] MARROQUIN J. L., ARCE E., BOTELLO S.: Hidden Markov measure field models for image segmentation. IEEE Pattern Analysis and Machine Intelligence 25
(2003), 1380–1387.
[McA04] MCANDREW A.: Introduction to Digital Image Processing with Matlab. Thomson Course Technology, 2004,
pp. 87–139, 449–466.
[MFTM01] MARTIN D., FOWLKES C., TAL D., MALIK J.: A
database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of the 8th International Conference Computer Vision (2001), vol. 2,
pp. 416–423.

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2386

O. Dalmau et al. / Bayesian Scheme for a General Image and Video Editing Method

[MVRN01] MARROQUIN J. L., VELAZCO F., RIVERA M.,
NAKAMURA M.: Gauss-Markov measure field models for
low-level vision. IEEE Pattern Analysis and Machine Intelligence 23 (2001), 337–348.
[OKS80] OTHA Y., KANADE T., SAKAI T.: Color information
for region segmentation. Computer Graphics Image Processing 13 (1980), 22–241.
[PD84] PORTER T., DUFF T.: Compositing digital images.
Computer Graphics 18, 3 (1984), 253–259.
[QG05] QIU G., GUAN J.: Color by linear neighborhood
embedding. In IEEE International Conference on Image
Processing (ICIP’05) (11–14 September 2005), pp. III988–91.
[QWCO*07] QING L., WEN F., Cohen-Or D., LIANG L., XU
Y. Q., SHUM H.: Natural image colorization. In Rendering
Techniques 2007 (Proceedings Eurographics Symposium
on Rendering) (2007), Eurographics.
[RAGS01] REINHARD E., ASHIKHMIN M., GOOCH B., SHIRLEY
P.: Color transfer between images. IEEE Computer Graphics and Applications 21, 5 (2001), 34–41.
[RCC98] RUDERMAN D. L., CRONIN T. W., CHIAO C. C.: Statistics of cone responses to natural images: Implications for
visual coding. Journal of Optical Society of America A 15,
8 (1998), 2036–2045.
[RD10] RIVERA M., DALMAU O.: Quadratic programming for
probabilistic image segmentation. Technical Report I-1006/25-06-2010, Centro de Investigacion en Matematicas,
A.C. CIMAT, Jalisco S/N, Valenciana, Guanajuato, Gto,
Mexico, June 2010. Also submitted to IEEE Transactions
on Image Processing (2009).
[RDT08] RIVERA M., DALMAU O., TAGO J.: Image segmentation by convex quadratic programming. In Proceedings of
International Conference on Pattern Recognition (2008),
pp. 1–5.
[RKB04] ROTHER C., KOLMOGOROV V., BLAKE A.: “Grabcut”:
Interactive foreground extraction using iterated graph
cuts. ACM Transactions on Graphics 23, 3 (2004), 309–
314.
[ROM05] RIVERA M., OCEGUEDA O., MARROQUIN J. L.: Entropy controlled Gauss-Markov random measure fields for
early vision. In Proceedings of the VLSM (2005), vol.
LNCS 3752, pp. 137–148.
[ROM07] RIVERA M., OCEGUEDA O., Marroqu´ın J. L.:
Entropy-controlled quadratic Markov measure field mod-

els for efficient image segmentation. IEEE Transactions
on Image Processing 16, 12 (2007), 3047–3057.
ˇ ARA J.: Unsuper[SBv04] S´YKORA D., BURIA´ NEK J., Z´
vised Colorization of Black and White Cartoons. In
NPAR ’04: Proceedings of the 3rd International Symposium on Non-Photorealistic Animation and Rendering (New York, NY, USA, 2004), ACM, pp. 121–
127.
[TJT05] TAI Y. W., JIA J., TANG C. K.: Local color transfer via
probabilistic segmentation by expectation-maximization.
In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) (2005), vol. 1,
pp. 747–754.
[WAM02] WELSH T., ASHIKHMIN M., MUELLER K.: Transferring color to greyscale images. ACM Transactions on
Graphics (TOG) 21, 3 (2002), 277–280.
[WC05] WANG J., COHEN M.: An interactive optimization
approach for unified image segmentation and matting. In
ICCV (2005), vol. 2, pp. 936–943.
[WG04] WON C. S., GRAY R. M.: Stochastic Image Processing. Kluwer Academic/Plenum, New York, 2004,
pp. 11–21.
[WH04] WANG C. M., HUANG Y. H.: A novel color transfer algorithm for image sequences. Journal of Information Science and Engineering 20, 6 (2004), 1039–
1056.
[WOZ02] WANG Y., OSTERMANN J., ZHANG Y.-Q.: Video Processing and Communications. Prentice-Hall, Upper Saddle River, New Jersey, 2002, pp. 18–19.
[WS82] WYSZECKI G., STILES W.: Color Science: Concepts
and Methods, Quantitative Data and Formulae (2nd
edition), John Wiley & Sons, Inc., New York, 1982,
pp. 165–168.
[YK03] YANG C. C., KWOK S. H.: Efficient gamut clipping
for color image processing using LHS and YIQ. Optical
Engineering 42 (March 2003), 701–711.
[YS06] YATZIV L., SAPIRO G.: Fast image and video colorization using chrominance blending. Image Processing, IEEE
Transactions on 15, 5 (2006), 1120–1129.
[YVW*05] YEN L., VANVYVE D., WOUTERS F., FOUSS F.,
VERLEYSEN M., SAERENS M.: Clustering using a random
walk based distance measure. In ESANN 2005: European Symposium on Artificial Neural Networks (2005),
pp. 317–324.

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

