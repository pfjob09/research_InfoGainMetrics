DOI: 10.1111/j.1467-8659.2010.01793.x
Pacific Graphics 2010
P. Alliez, K. Bala, and K. Zhou
(Guest Editors)

Volume 29 (2010), Number 7

Efficient Mean-shift Clustering Using Gaussian KD-Tree
Chunxia Xiao

Meng Liu

The School of Computer, Wuhan University, Wuhan, 430072, China

Abstract
Mean shift is a popular approach for data clustering, however, the high computational complexity of the mean
shift procedure limits its practical applications in high dimensional and large data set clustering. In this paper,
we propose an efficient method that allows mean shift clustering performed on large data set containing tens
of millions of points at interactive rate. The key in our method is a new scheme for approximating mean shift
procedure using a greatly reduced feature space. This reduced feature space is adaptive clustering of the original
data set, and is generated by applying adaptive KD-tree in a high-dimensional affinity space. The proposed method
significantly reduces the computational cost while obtaining almost the same clustering results as the standard
mean shift procedure. We present several kinds of data clustering applications to illustrate the efficiency of the
proposed method, including image and video segmentation, static geometry model and time-varying sequences
segmentation.
Categories and Subject Descriptors (according to ACM CCS): I.4 [Computing methodologies]: Image Processing
and Computer Vision—Applications

1. Introduction
Mean shift is a well established method for data set clustering, it has been widely used in image and video segmentation [CM02] [WTXC], object tracking [CRM03], image denoising [BC04], image and video stylization [DS02]
[WXSC04], and video editing [WBC∗ 05], it also has been
extended to geometry segmentation [YLL∗ 05] and 3D reconstruction [WQ04]. Mean shift works by defining a Gaussian kernel density estimate for underlying data, and clusters
together the points that converge to the same mode under
a fixed-point iterative scheme. Although mean-shift works
well for data clustering and obtain pleasing clustering results, however, the high computational complexity is the one
of main difficulties to apply mean shift to cluster large data
set, especially for those situations where the interactive and
even real time clustering processing are preferred.
The complexity for the standard mean shift procedure
is O(τdn2 ), where n is the number of the data points, the
τ is the number of the iterations for mean shift clustering procedure, and d is the dimension of the point. The
most expensive computing is to find the closest neighborhood for each point in the data space, which is a multidimensional range searching method. Even using one of the
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

most popular nearest neighbor search method, the ANN
method [AMN∗ 98], given a query point q and ε > 0, a
(1 + ε) approximate nearest neighbor of q must be computed in O(cd,ε log n) time, where cd,ε is a factor depending on dimension d and ε. Therefore, when processing large
data sets, the high time complexity leads to serious difficulty.
Although many acceleration techniques have been proposed
[EDD03, GSM03, YDGD03, WBC∗ 05, CP06, PD07, FK09],
further improvements are still desirable for both performance and clustering quality.
In this paper, inspired by the fast high-dimensional filtering method using Gaussian KD-trees [AGDL09], we propose an efficient paradigm for mean-shift procedure computing. Our method is based on following key observation,
since the mean shift procedure clusters those points that are
feature similar, while there are many clusters of points which
are high similar in feature, it is wasteful to perform mean
shift procedure for each point to converge to the mode. Thus
we first cluster the original point set into clusters based on
feature similarity using KD-tree [AGDL09], and obtain the
Gaussian weighted samples of the original data set, which is
the reduced feature space for approximating original point
set. Then instead of computing mean shift directly on origi-

2066

C. Xiao & M. Liu / Efficient Mean-shift Clustering Using Gaussian KD-Tree

nal individual points, we compute on the samples (which is
of a much smaller number) to obtain the modes of the sample
space. Finally we find the closest mode for each point based
on Gaussian weighted feature similarity, and construct the
final clustering results.

Georgescu et al. [GSM03] accelerated mean shift by performing fast nearest neighbor search with spatially coherent
hash tables. Carreira-Perpinán [CP06] studied four acceleration strategies and found that spatial discretization method
(using uniform down sampling schemes) performed best.

As mean shift is performed on a greatly reduced space
(typically thousands of times smaller than original data
set), and all stages of our algorithm are data-parallel across
queries and can be implemented the algorithm in CUDA
[Buc07], we can cluster the large data set in real time or at interactive rate (for a video with 1.44 × 107 pixels in Figure 5).
Furthermore, as the sample space is an approximate feature
space of the original data set generated using the proposed
Gaussian weighted similarity sampling, our method receives
accurate results comparable with the standard mean shift that
performed on the original data set. In addition, our method
uses only an extremely small fraction of resources, for both
time and memory consuming.

Paris and Durand [PD07] applied the sparse representation of the density function to accelerate mean shift, similar
to the bilateral filtering [PD06], they first binned the feature points in a coarse regular grid, and then blurred the
bin values using a separable Gaussian. The computational
complexity and memory scale exponentially with the dimension d. Wang et al. [WLGR07] used a dual tree to speed
up Mean Shift by computing two separate trees, one for the
query points, and one for the reference points. Compared to
the methods of [YDDD03] [PD07], this method maintains
a relative error bound of the Mean Shift iteration at each
stage, leading to a more accurate algorithm, however, the
performance of this method is much lower than [YDDD03]
[PD07]. More recently, Freedman and Kisilev [FK09] proposed a sampling technique for Kernel Density Estimate
(KDE), they constructed a compactly represented KDE with
much smaller description complexity, this method greatly
accelerates the mean shift procedure, however, the accuracy
of the mean shift clustering depends on the number of the
random samples.

This paper is organized as follows. In section 2, we give
the related work, section 3 is the main part of our paper, we
describe the proposed fast mean shift clustering method. In
section 4, we give the applications of our method and comparisons with the related mean shift acceleration methods,
and we conclude in section 5.
2. Related work
Mean shift was first presented by Fukunaga and Hostetler
[FH75], and it was further investigated by Cheng et al.
[Che95] and Comaniciu et al. [CM02]. Mean shift is now a
popular approach for data set clustering, and has been widely
used in image and video segmentation [CM02] [WTXC], object tracking [CRM03], image denoising [BC04] and image
and video stylization [DS02] [WXSC04], it also has been
extended to geometry segmentation [YLL∗ 05] and 3D reconstruction [WQ04], and many image and video editing
methods are based on the mean shift clustering preprocessing [WBC∗ 05].
One of the main difficulties in applying Mean Shift based
clustering to large data sets is its computational complexity.
For each Gaussian weighted average iteration, the complexity of brute force computation is quadratic in the number
of data points. There are several existing techniques which
have been developed to increase the speed of Mean Shift.
Comaniciu and Meer [CM02] used axis-aligned box windows, however, this produces many limit points and adjacent
points are merged as a post-process. Dementhon [DeM02]
used multiscale structure to accelerate video segmentation.
Yang at al. [YDDD03] applied the Fast Gauss Transform
to speed up the sums of Gaussians in higher dimensions
that were used in the Mean Shift iteration. This method is
effective for Gaussian weighted average with large filtering radius, however, performing weighted average in a relative small radius does not benefit much from this method.

Many methods have applied Gaussian KD-Trees for
accelerating image and video processing. Adams et al.
[AGDL09] applied Gaussian KD-Trees for accelerating
high-dimensional filtering includes the bilateral image filter [TM98], bilateral geometry filtering [JDD03, FDCO03]
and image denoising with nonlocal means [BCM05]. We
borrow some ideas from [AGDL09] for adaptive clustering
in this paper. Xu et al. [XLJ∗ 09] used K-D Tree to build
adaptive clustering for accelerating affinity-based image and
video edit propagation. As an alterative, Xiao et al. [XNT10]
used quadtree based hierarchical data structure to accelerate
edit propagation. KD-Trees have been widely used in accelerating graphics rendering [HSA91], the real-rime KDtree construction on graphics hardware also have been proposed [HSHH07] [ZHWG08]. Our method applies Gaussian KD-Trees [AGDL09] to build a hierarchy and clustering
for the large data set to accelerate the mean shift computing. Compared with Paris and Durand [PD07], whose complexity is exponential in the dimension d of the point, our
tree-based mean shift provides with excellent performance,
as its runtime and memory consuming both scale linearly
with dimension of the points. With the samples generated
using similarity-based KD-tree clustering, our method obtains more accurate results than [FK09] when using similar
number of samples.
3. Fast mean shift clustering
We first give brief review of mean shift, then we describe
the proposed fast mean shift clustering method, including
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

C. Xiao & M. Liu / Efficient Mean-shift Clustering Using Gaussian KD-Tree

data set clustering preprocess using KD-tree, sample feature
space computing, mean shift modes computing in reduced
feature space, modes interpolation. We also present the complexity analysis and GPU implementation of the proposed
algorithm.
3.1. Review of mean shift
Given point data set {xi }ni=1 , where xi ∈ Rd is d dimensional
feature vector, each is associated with a bandwidth value
hi > 0. An adaptive nonparametric estimator of this data set
is defined as
1 n 1
∑ hd k
n i=1
i

fK (x) =
where K (x) = ck,d k

x

2

2

x − xi
hi

(1)

> 0 is kernel function satisfying

K (x) ≥ 0 and Rd K (x)dx = 1, By taking the gradient of (1)
the following expression can be obtained.
2c n 1
∑ hd g
n i=1
i

∇ fK (x) =

2

x − xi
h

m (x)

(2)

where g (x) = −k (x), and m (x) is the so-called mean shift
vector
∑ni=1
m (x) =

1
xi g
hd+2
i

∑ni=1

1

hd+2
i

g

x−xi
hi
x−xi
hi

2
2

−x

(3)

The expression (3) shows that at location x the weighted
average of the data points selected with kernel K is proportional to the normalized density gradient estimate obtained
with kernel K. The mean shift vector thus points toward the
direction of maximum increase in the density. The following gradient-ascent process with an adaptive step size until
convergence constitutes the core of the mean shift clustering
procedure
∗
xi+1
=

∑ni=1

1
xi g
hd+2
i

∑ni=1

1

d+2 g

hi

xi∗ −xi 2
hi
xi∗ −xi 2
hi

, j = 1, 2, ...

(4)

The starting point of the procedure xi∗ can be chosen as
points xi , and the convergence points of the iterative procedure are the modes of the density. The all points that converge to the same mode are collected and considered as a
cluster. More details are described in [CM02].
3.2. Fast mean shift computing
The weighted average of expression (4) is the most time consuming computing of mean-shift when the number n of the
data set is large (for example, 109 pixel in video streaming).
Given an arbitrary set of point {xi }ni=1 with feature vector of
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2067

d dimension, a naive computation of mean shift vector expression (4) would take O(dn2 ) time, as every point interacts
with every other point. A simple way to accelerate the mean
shift procedure is using the weighted average of the closest
points of xi , and the bandwidth value hi can be set depending on the neighborhood size. However, using this scheme,
we have to perform the nearest neighborhood search, which
is also a time consuming operation for large data set, especially for data set with high dimension vector.
To accelerate the weighted average operation of expression (4), instead of computing expression (4) for individual points in the data set, we approximate original data set
by piece-wise linear segments in the feature space based on
similarity measure, each represented by a cluster of nearby
pixels, and the size of each cluster is adapted to the size of
the similar feature space. The generated clusters can be considered as the samples of the data set, which is of a much
smaller number than the number of point. Then instead of
solving the mean shift procedure (4) directly on individual points as done in previous methods, we solve it on the
samples based on Gaussian weighted average on a neighborhood, finally we interpolate the clustering results to the
original data set.
We cluster the point data based on similarity measure between the points, which is defined in the feature space of
input data set. We define the similarity between the points
using both spatial locality p and value v of point, which constitutes the feature space of the input data set. For example,
in image case, point x is a pixel with its position p = (x, y)
and its color value v = (r, g, b) (Lab color space). Thus, each
point x is a five-dimensional feature vector whose axes are
x = (p, v) = (x, y, r, g, b). As stated in [AP08] [XLJ∗ 09], the
similarity measure (or affinity) between two points can be
2
defined as zi j = exp − xi − x j
, the position p and value
v also can be weighted by parameters. For video, the feature
vector can be expanded to include the frame index t and motion estimation ς of point x, and feature vector is expressed
as seven-dimensional vector x = (p, v,t, ς). For image, we
compute the mean shift clustering procedure (4) in feature
space where each point is associated with both spatial locality p and value v.
3.2.1. KD-Tree adaptive clustering
We apply KD-tree to adaptively cluster the data set in the
feature space, and subdivide finely in the regions where the
point feature vectors are different, subdivide coarsely in the
regions where the feature vectors are similar. In image KDtree clustering, for example, we subdivide the homogeneous
regions coarsely, while subdivide the edges regions finely.
Then by representing each cluster with a sample, we can receive an accurate approximate feature space of the original
data set with much less samples.
KD-tree clusters the data set based on feature space in a
top down way. Starting from a root cell, the top rectangular

2068

C. Xiao & M. Liu / Efficient Mean-shift Clustering Using Gaussian KD-Tree

cell represents all points in the data set, we recursively split a
cell to two child cells adaptively along a dimension that is alternated at successive tree levels. Similar to [AGDL09], each
inner cell of the tree T represents a d-dimensional rectangular cell which stores six variables: the dimension d along
which the tree cuts, the cut value Tcut on that dimension,
the bounds of the cell in that dimension Tmin and Tmax , and
pointers to its children Tle f t and Tright . Leaf nodes contain
only a d-dimensional point which is considered as a sample.
This sample represents the points that contained in the leaf
m
cell, the kd-tree stores m samples y j j=1 of original data
set in d-dimensions, one sample point per leaf. The samples
m
y j j=1 construct the reduced feature space of the original
data set.
As illustrated in Figure 1, we adaptively cluster the image into clusters. The image is adaptively clustered, where at
the edge regions, more samples are placed, while at the homogeneous regions, coarse samples are placed. The sample
factor can be changed by the stopping criteria. We present
two thresholds for stopping criteria, one is the size of the
cell, other one is the variance σ of the similarity measure zi j
between the points in the cell. By using these two thresholds we can generate different sampling factor for image as
well as respecting for the feature distribution of the data set.
Figure 1 shows some cluster results with different sampling
factor.

an affinity based sample y∗j for each sample yi . The affinity
based sample y∗j can be considered as the weighted similarity average for those feature vector {xi } that is most similar
to the sample yi . We compute and sum the affinity similarity zi j between xi and each y j ∈ N (xi ) to obtain the affinity
based sample y∗j of the sample yi : y∗j = y∗j + zi j xi , then y∗j is
normalized by the sum of the similarity zi j . The generated y∗j
is a feature vector of d-dimensions. The affinity based sample {y∗j }mj=1 is a more accurate samples of the original point
set, and will be used in the mean shift clustering procedure
and modes interpolation. Then for each sample, we store two
vectors, one is feature vector yi , the other is the affinity based
sample y∗j .
3.2.3. Mean-shift modes computing
After obtaining the affinity based samples {y∗j }mj=1 for original data set feature space, instead of computing the mean
shift clustering procedure in the original space, we compute the mean shift procedure on the reduced feature space
{y∗j }mj=1 . Note for each iteration, we find for each sample y∗j
the nearest neighborhood N y∗j in the sample space, and
perform following gradient-ascent process with an adaptive
step size until convergence:

u j+1 =

∑y∗i ∈N (y∗j ) y∗i g
∑y∗i ∈N (y∗j ) g

2
u j −y∗
i
h
2
u j −y∗
i
h

, j = 1, 2, ...

(5)

Iterating an infinite number of times the expression (5) is
guaranteed to bring u j to the mode in reduced feature space.
Practically, in the implementation, we find that the meanshift clustering procedure (5) tends to converge in a very
small number of steps, typically around 6.
As the number of the samples y∗j
Figure 1: Image sampling using adaptive KD-tree. (a) Original image (636×844), (b) image clustering with 1,050 samples, (c) image clustering with 11,305 samples.

3.2.2. Sample feature space computing
m

We obtain the samples y j j=1 in d-dimensions, which are
adaptive samples of the feature space of original data set.
m
The samples y j j=1 can be considered a approximate feature space of original space. To make a more accurate approximate feature space, we scatter the point xi to the samm
ples y j j=1 based on the affinity similarity zi j between the
point xi and sample y j , and obtain an affinity similarity based
sample space.
Similar to the splatting stages in [AGDL09], we scatter
each point xi to its nearest neighborhood N (xi ) in samples
m
y j j=1 (We apply the KD-tree to search the high dimension nearest neighborhood N (xi ) for point xi ), and compute

m
j=1

is much smaller

n, the
than the number of the points {xi }ni=1 , that is m
computational complexity of mean shift vector computing
has been reduced significantly. Furthermore, the mean shift
is performed in the affinity similarity based reduced feature
space, which leads to more accurate modes. We apply the
KD-tree to perform the high dimension nearest neighbor research for each iteration computing. After performing mean
shift iterations for each sample y∗j , we receive the modes
m
{zk }sk=1 of the reduced feature space y∗j j=1 , which are
the approximate modes of original data set. All samples converging to the same mode are clustered together.
3.2.4. Modes interpolation
To interpolate the modes computed in the reduced feature
space to the original data set, one naïve approach is to find
a nearest mode zl ∈ {zk }sk=1 for each point xi . This can be
considered as a hard clustering. As an alternative method, we
give a soft clustering method which generates more smooth
clustering results. This method works by applying weighted
based interpolation.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

C. Xiao & M. Liu / Efficient Mean-shift Clustering Using Gaussian KD-Tree

The mode interpolation works as follows, for each point
m
xi , we find the nearest samples N (xi ) in y∗j j=1 . Each sam-

ple y∗j ∈ N (xi ) converges to a mode u j , that is, y j → u j .
Based on the affinity similarity zi j between xi and y∗j , by normalizing the weights zi j : ∑ j zi j = 1, the final mode are computed as: xi → ∑ j=1 zi j u j . When we compute the weighted
m
mode interpolation over all the samples y∗j j=1 , similar
to [FK09], we will receive the cartoon-like clustering results.
Note that in the interpolation stage, the size of the N (xi )
is not always the same as that performed in the sample space
computing stage. In our experiment, we set neighborhood
size between 10 and 20, and receive satisfied results. As
the samples are significantly smaller than the number of the
original point set, using the GPU accelerated KD-tree nearest neighborhood search, the search performing is fast. In
addition, since we determine the final mode for xi based on
the weighted similarity, the results are more accurate. Figure
3 shows the results using the two different mode selection
methods, one is the nearest-sample based mode selection,
the other is weighted modes interpolation. As illustrated in
Figure 3, using the weighted modes interpolation, we receive
smoother and more accurate results.

2069

up-sample clustering results to the original data sets which
takes O (n (log m + d)) time. Recall that m
n, this results
in a total complexity O (dn log n). Compared with standard
mean shift procedure with complexity O(τdn2 ), the proposed method significantly accelerated.
Applying the method presented by [ZHWG08], a KD-tree
is efficiently built on the GPU for the input points with highdimensional feature vector. Three stages of our proposed algorithm, including the points scattering, mean shift clustering procedure in the reduced space, and modes interpolation, all incorporate the high dimensional nearest neighbor
search. As the high dimensional nearest neighbor search can
be implemented in parallel using GPU [AGDL09], thus, our
method is even fast to process large data set with high dimensional feature vector. We implement the proposed algorithms in CUDA, and run it on an NVIDIA GeForce GTX
285 (1GB) graphics card. We observe a typical speedup of
20x over our single-threaded CPU implementation running
on an Pentium(R) Dual-Core CPU E5200@2.50GHz with
2GB RAM, which allows our method to be applied in an
interactive mean shift clustering framework for moderatesized data set.
4. Applications and comparisons

(a)

(b)

(c)

Figure 2: Image segmentation results comparison. (a) The
original image, (b) the result using nearest sample based
mode selection, (c) the result using weighted modes interpolation.

We apply the proposed fast mean shift clustering to following applications, image segmentation, video segmentation, geometry model segmentation, and animated object
segmentation. We also present the comparison results on
both performance and segmentation quality with the most
related methods. Our approach is implemented using C++
on a machine equipped with Pentium(R) Dual-Core CPU
E5200@2.50GHz with 2GB RAM. The GPU acceleration is
based on CUDA [ http: http://www.nvidia.com/CUDA
]
and run on a NVIDIA GeForce GTX 285 (1GB) graphics
card.
4.1. Image segmentation

3.3. Complexity analysis and GPU implementation
Our algorithm accelerates the mean shift procedures by computing a lower resolution feature space and then interpolating the clustering result to the original data set. By using a KD-tree structure, we construct a reduced feature
space for input n d-dimensional data points with m feam
ture vector: {xi }ni=1 → y∗j j=1 , and m
n . Assuming
the depth of Gaussian tree is O (log m), the complexity of
tree construction is O (ndlog m). Performing nearest neighborhood for each of the n input points to scatter values into
the tree takes O (n (log m + d)) time. If performing τ iterations for mean shift clustering procedure, computing the
mean shift iteration in the reduced space with m feature
vector takes O (τm (log m + d)) time. In the last stage, we
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

We apply the proposed fast mean shift method for image
segmentation. All pixels that converge to same mode are
collected together and are considered to be the same segment. In image case, we define the feature vector of pixel
xi = (σ p pi , σc ci ) comprising its position p = (x, y) and its
color value c = (r, g, b) (Lab space) which can be weighted
by parameters σ p and σc . Thus, each pixel xi is a fivedimensional vector.
Figure 3 presents the segmentation results generated applying our fast mean shift method based on different sampling factor. As illustrated in Figure 3, there are 6 × 106
pixels in the original image. Even with very high sampling
factor such as n/m = 4, 096, the segmentation results is still
pleasing. With much less samples, the image can be clustered in high speed even without using GPU acceleration. It

2070

C. Xiao & M. Liu / Efficient Mean-shift Clustering Using Gaussian KD-Tree

takes only total 0.958 seconds on CPU to perform mean shift
clustering with sampling factor n/m = 1, 024.
In Figure 4, we present image segmentation results for
the images with different sizes, and give the comparison results with standard mean shift method [CM02], the accelerated method of Paris and Durand [PD07], and compactly
represented KDE of Freedman and Kisilev [FK09]. We comprise with these methods on both performance and segmentation quality. Compared with [CM02], some weak features
may be lost using our method since they may be incorporated into the salient features during data Gaussian KD-tree
clustering, however, the salient features may be better kept,
as illustrated in Figure 4. As shown in Figure 4, given the
same sampling factor, our method generates higher quality
compared with [PD07] and [FK09], especially at the regions
with weak edges. The complexity of [PD07] depends on the
dimension d of the point, when processing high-dimensional
data, this method does not show much advantage. However,
our method is fast even with low sampling factor and high dimensional data sets. as shown in Table 1. It takes our method
5.91 second to cluster 6.6 × 106 pixels on CPU, while it take
105.5 seconds using [PD07]. Using our method incorporating GPU implementation, our method shows greater advantage when processing large data sets with high dimensional feature vector, it takes less than 0.2 second to cluster
6.6 × 106 pixels on GPU.

4.2. Video segmentation
Mean shift clustering can also be used in video segmentation [DeM02]. As video streaming usually contains millions of pixels, practical video segmentation using mean
shift clustering depends heavily on the performance of the
mean shift procedure. In addition, compared with image
data, the dimensions of feature space are higher, which further increase the computational complexity of mean shift
procedure. Thus, it is impracticable to segment long range
video streaming by performing standard mean shift clustering without using acceleration techniques. However, using
our fast mean shift clustering method, we can perform video
segmentation at interactive rate.
We define a feature space {xi }ni=1 of seven dimensions for the video. The feature vector at each pixel xi =
(σ p pi , σc ci , σt ti , σς ςi ) comprises its position pi (x and y coordinate), color ci (Lab color vector), time ti and motion ςi ,
these four kinds of features can be weighted by parameters
σ p , σv , σt and σς , and the values of these parameters are defined as constants for all pixels. As illustrated in Figure 5,
there are 1.44 × 107 pixels in the video, and we first cluster
the video with sampling factor 16348 using KD tree. It takes
our method 16.23 seconds to perform the mean shift clustering on CPU, and 1.2 seconds on GPU. It takes 320.3 seconds
on CPU using [PD07].

Figure 5: Video segmentation. Left column, input video
(600 × 480 × 50), from top to down, 1th frame, 28th frame,
50th frame. Right column, segmentation results.

4.3. Mesh model segmentation
Similar to image and video segmentation in image processing and analysis, surface segmentation is one of the most
important operations in geometry processing and modeling.
Yamauchi et al. [YLL∗ 05] adopted the mean shift clustering to mesh partition, and produced feature sensitive mesh
segmentation. In Figure 6, we give the mesh segmentation
using the proposed fast mean shift cluttering. We define a
feature space {xi }ni=1 of six dimensions for the mesh model.
The feature vector at vertex xi = (σ p pi , σv vi ) comprises its
position pi (x, y and z coordinate) and normal vi (three dimension vector) which can be weighted by parameters σ p
and σv . The values of σ p and σv are defined as global constants for all vertices.
In Figure 6, using the variant of mean shift procedure to
the mesh model, we receive a patch-type segmentation results, and the segmentation results are sensitive to the salient
surface features. Furthermore, we adopt the hierarchical image segmentation method [PD07] to mesh model and generate the hierarchical segmentation results. Note that our fast
mean shift method is guaranteed to produce segmentation results which catch the meaningful components, no additional
computation is needed to compute the hierarchical results.
For a mesh model with 70, 994 vertices, it takes 0.31 second for our method to compute the results. We also give the
comparison results with [YLL∗ 05]. As shown in Figure 6,
our approach generates more convincing results.
4.4. Animated geometry object segmentation
The proposed fast mean shift also can be used to accelerate
the animated object (geometry surface sequences) segmenc 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2071

C. Xiao & M. Liu / Efficient Mean-shift Clustering Using Gaussian KD-Tree

(a)

(b)

(c)

(d)

(e)

Figure 3: Image segmentation using different sampling factor. (a) Original image, (b) n/m =256, (c) n/m =1,024, (d) n/m
=4,096, (e) n/m =16,384.
Data set
bird
Obama
castle
Video
Mesh
Horse

Data set size
564×752
1128×1504
2256×3008
600 × 480 × 50
70994
30784 × 60

d
5
5
5
7
6
30

n/m
1024
4096
4096
16348
512
4094

Kd-tree construction
0.904
1.013
5.105
13.898
0.225
1.311

Modes computing
0.024
0.016
0.088
0.084
0.020
0.053

Modes interpolation
0.030
0.108
0.717
2.248
0.065
0.136

time
0.958
1.137
5.910
16.23
0.310
1.500

Table 1: Performance of our method for different kinds of data sets. Note that we perform clustering for animated object (Horse)
on GPU. All other data sets are performed on CPU.

Then the vertices of animated object can be clustered efficiently using the proposed GPU- accelerated mean shift
clustering algorithm.

(a)

(b)

(c)

(d)

(e)

(f)

Figure 6: Hierarchical decomposition of static mesh model.
(a)-(e) is the results of our proposed hierarchical decomposition method, (f) is the result of [YLL∗ 05].

tation. Inspired by [LXL∗ ], we first compute approximately
invariant signature vectors ξi for each vertex of the animated
object [LG05], which is a local and high dimensional approximately invariant under shape rigid/scaling transformations. Then both the geometric attributes (vertex position pi
and its normal vi ) and its local signature vector ξi of each
vertex xi on the animated object can be weighted by parameters σ p , σv and σξ , which construct the high dimensional
feature space of the animated object xi = (σ p pi , σv vi , σξ ξi ).
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

In Figure 7, we give the animated object segmentation results. There are total 1.6 × 106 vertices in the animated object with 50 frames. We use d = 24 dimensions for the signature vector ξi (ξi ∈ R24 ) in our implementation. It takes 1.5
seconds on GPU to complete the mean shift iterations (10 iterations in this example) with sampling factor n/m = 4096.
We also give the comparison results with [WB10]. Wuhrer
and Brunton [WB10] performed the animated object segmentation in dual space of the mesh model. They found
near-rigid segments whose segment boundaries locate at regions of large deformation, and assumed that the vertex-tovertex correspondences of the input meshes were known.
As an alternative, our method relies on the local high dimensional signature vector information for clustering, incorporating with the proposed fast mean shift clustering techniques, which ensures the decomposed parts more meaningful and temporally-coherent results in higher speed.
5. Conclusion
In this paper, we propose a new algorithm for accelerating
compute mean shift clustering. Using KD-tree to adaptively
cluster the original data set into clusters with similar feature similarity, the clusters construct the samples of the original data set. Then we compute the mean shift procedure on

2072

C. Xiao & M. Liu / Efficient Mean-shift Clustering Using Gaussian KD-Tree

(a)

(b)

(c)

(d)

(e)

Figure 4: Image segmentation comparisons. (a) original image, (b) image segmentation using standard mean shift (EDISON),
(c) image segmentation using [PD07], (d) image segmentation using [FK09], (e) image segmentation using our proposed
method.

the greatly reduced sampled feature space and generated the
modes, and finally by using the Gaussian importance weight,
we upsample the computed modes to the original data set
to get final clustering results. Our algorithm significantly
speeds up the performance while not sacrificing the accuracy. Our method is especially useful for high resolution images, long time video sequences and geometry models segmentation with large point set.

was partly supported by NSFC (No. 60803081), National
High Technology Research and Development Program of
China (863 Program) (No. 2008AA121603), Natural Science Foundation of Hubei Province (2008CDB350), State
Key Lab of CAD&CG (No. A0808), the Fundamental Research Funds for the Central Universities (6081005).
References

Acknowledgment

[AGDL09] A DAMS A., G ELFAND N., D OLSON J., L EVOY M.:
Gaussian kd-trees for fast high-dimensional filtering. ACM
Transactions on Graphics (TOG) 28, 3 (2009), 21.

We would like to thank the anonymous reviewers for their
valuable comments and insightful suggestions. This work

[AMN∗ 98] A RYA S., M OUNT D., N ETANYAHU N., S ILVER MAN R., W U A.: An optimal algorithm for approximate nearc 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

C. Xiao & M. Liu / Efficient Mean-shift Clustering Using Gaussian KD-Tree

2073

[GSM03] G EORGESCU B., S HIMSHONI I., M EER P.: Mean shift
based clustering in high dimensions: A texture classification example. In ICCV (2003), pp. 456–463.
[HSA91] H ANRAHAN P., S ALZMAN D., AUPPERLE L.: A rapid
hierarchical radiosity algorithm. ACM SIGGRAPH Computer
Graphics 25, 4 (1991), 206.
[HSHH07] H ORN D., S UGERMAN J., H OUSTON M., H ANRA HAN P.: Interactive kd tree GPU raytracing. In Proceedings
of the 2007 symposium on Interactive 3D graphics and games
(2007), ACM, p. 174.

Figure 7: Animated object decomposition result comparison. Top row: our results, bottom row: Wuhrer and Brunton [WB10].

est neighbor searching fixed dimensions. Journal of the ACM
(JACM) 45, 6 (1998), 891–923.
[AP08] A N X., P ELLACINI F.: Appprop: all-pairs appearancespace edit propagation. ACM Trans. Graph 27, 3 (2008), 40.
[BC04] BARASH D., C OMANICIU D.: A common framework
for nonlinear diffusion, adaptive smoothing, bilateral filtering and
mean shift. Image and Vision Computing 22, 1 (2004), 73–81.
[BCM05] B UADES A., C OLL B., M OREL J.: A non-local algorithm for image denoising. In CVPR 2005 (2005), pp. 60–65.
[Buc07] B UCK I.: Gpu computing: Programming a massively
parallel processor. In Proceedings of the International Symposium on Code Generation and Optimization (2007), IEEE Computer Society, p. 17.
[Che95] C HENG Y.: Mean shift, mode seeking, and clustering.
IEEE Transactions on Pattern Analysis and Machine Intelligence
17, 8 (1995), 790–799.
[CM02] C OMANICIU D., M EER P.: Mean shift: A robust approach toward feature space analysis. IEEE Transactions on pattern analysis and machine intelligence 24, 5 (2002), 603–619.

[JDD03] J ONES T., D URAND F., D ESBRUN M.: Non-iterative,
feature-preserving mesh smoothing. ACM Transactions on
Graphics 22, 3 (2003), 943–949.
[LG05] L I X., G USKOV I.: Multi-scale features for approximate
alignment of point-based surfaces. In SGP (2005), Eurographics
Association, p. 217.
[LXL∗ ] L IAO B., X IAO C., L IU M., D ONG Z., P ENG Q.: Fast
Hierarchical Animated Object Decomposition Using Approximately Invariant Signature. Submmited to The Visual Computer.
[PD06] PARIS S., D URAND F.: A fast approximation of the bilateral filter using a signal processing approach. ECCV (2006).
[PD07] PARIS S., D URAND F.: A topological approach to hierarchical segmentation using mean shift. In CVPR (2007), pp. 1–8.
[TM98] T OMASI C., M ANDUCHI R.: Bilateral filtering for gray
and color images.
[WB10] W UHRER S., B RUNTON A.: Segmenting animated objects into near-rigid components. The Visual Computer 26, 2
(2010), 147–155.
[WBC∗ 05] WANG J., B HAT P., C OLBURN R., AGRAWALA M.,
C OHEN M.: Video cutout. ACM Transactions on Graphics 24, 3
(2005), 585–594.
[WLGR07] WANG P., L EE D., G RAY A., R EHG J.: Fast mean
shift with accurate and stable convergence. In Workshop on Artificial Intelligence and Statistics (AISTATS) (2007), Citeseer.
[WQ04] W EI Y., Q UAN L.: Region-based progressive stereo
matching. In CVPR (2004), vol. 1, Citeseer, pp. 106–113.

[CP06] C ARREIRA -P ERPINÁN M.: Acceleration strategies for
Gaussian mean-shift image segmentation. In CVPR (2006),
vol. 1.

[WTXC] WANG J., T HIESSON B., X U Y., C OHEN M.: Image
and video segmentation by anisotropic kernel mean shift. Computer Vision-ECCV 2004, 238–249.

[CRM03] C OMANICIU D., R AMESH V., M EER P.: Kernel-based
object tracking. IEEE Transactions on Pattern Analysis and Machine Intelligence 25, 5 (2003), 564–577.

[WXSC04] WANG J., X U Y., S HUM H., C OHEN M.: Video tooning. In ACM SIGGRAPH 2004 Papers (2004), ACM, pp. 574–
583.

[DeM02] D E M ENTHON D.: Spatio-temporal segmentation of
video by hierarchical mean shift analysis. Language 2 (2002).

[XLJ∗ 09] X U K., L I Y., J U T., H U S., L IU T.: Efficient affinitybased edit propagation using KD tree. In ACM SIGGRAPH Asia
2009 papers (2009), ACM, pp. 1–6.

[DS02] D E C ARLO D., S ANTELLA A.: Stylization and abstraction of photographs. In SIGGRAPH (2002), ACM, pp. 769–776.

[XNT10] X IAO C., N IE Y., TANG F.: Efficient Edit Propagation
Using Hierarchical Data Structure. IEEE Transactions on Visualization and Computer Graphics (2010).

[EDD03] E LGAMMAL A., D URAISWAMI R., DAVIS L.: Efficient
kernel density estimation using the fast gauss transform with applications to color modeling and tracking. IEEE Transactions on
Pattern Analysis and Machine Intelligence 25, 11 (2003), 1499–
1504.

[YDDD03] YANG C., D URAISWAMI R., D E M ENTHON D.,
DAVIS L.: Mean-shift analysis using quasi-Newton methods. In
ICIP (2003), vol. 3, Citeseer, pp. 447–450.

[FDCO03] F LEISHMAN S., D RORI I., C OHEN -O R D.: Bilateral
mesh denoising. ACM Transactions on Graphics (TOG) 22, 3
(2003), 950–953.

[YDGD03] YANG C., D URAISWAMI R., G UMEROV N., DAVIS
L.: Improved fast gauss transform and efficient kernel density
estimation. In ICCV (2003), pp. 664–671.

[FH75] F UKUNAGA K., H OSTETLER L.: The estimation of the
gradient of a density function, with applications in pattern recognition. IEEE Transactions on Information Theory 21, 1 (1975),
32–40.

[YLL∗ 05] YAMAUCHI H., L EE S., L EE Y., O HTAKE Y.,
B ELYAEV A., S EIDEL H.: Feature sensitive mesh segmentation
with mean shift. In SMA (2005), vol. 243, IEEE.

[FK09] F REEDMAN D., K ISILEV P.: Fast Mean Shift by compact
density representation.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

[ZHWG08] Z HOU K., H OU Q., WANG R., G UO B.: Real-time
kd-tree construction on graphics hardware. In ACM SIGGRAPH
Asia 2008 papers (2008), ACM, pp. 1–11.

