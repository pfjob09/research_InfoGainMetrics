DOI: 10.1111/j.1467-8659.2009.01667.x
Eurographics/ IEEE-VGTC Symposium on Visualization 2010
G. Melançon, T. Munzner, and D. Weiskopf
(Guest Editors)

Volume 29 (2010), Number 3

A Salience-based Quality Metric for Visualization
H. Jänicke1 and M. Chen2
1 Ruprecht-Karls-Universität
2 Swansea

Heidelberg, Germany
University, Great Britain

Abstract
Salience detection is a principle mechanism to facilitate visual attention. A good visualization guides the observer’s attention to the relevant aspects of the representation. Hence, the distribution of salience over a visualization image is an essential measure of the quality of the visualization. We describe a method for computing such
a metric for a visualization image in the context of a given dataset. We show how this technique can be used to
analyze a visualization’s salience, improve an existing visualization, and choose the best representation from a set
of alternatives. The usefulness of this proposed metric is illustrated using examples from information visualization,
volume visualization and flow visualization.
Categories and Subject Descriptors (according to ACM CCS):
Generation—Line and curve generation

1. Introduction

I.3.3 [Computer Graphics]: Picture/Image

tion. They also presented a user study where eye-tracking
was used to show the effectiveness of salience-guided visualization. In [LVJ05], Lee et al. used salience to extract interesting regions of a surface mesh. Unlike these techniques,
where visual salience was tailored to improve a specific visualization technique, we aim at a general metric to assess
the quality of a visualization.

To which parts of a visualization do viewers pay attention,
and how well does the expected attention match with the
importance of the data depicted? These are questions that
every visualization creator would be curious about as the answers would affect how a visualization is perceived and on
what the observers learn from it. However, these questions
are also rather intricate, and hence difficult to answer.
It is known that salience detection is a principle mechanism to facilitate visual attention [Pal99, TCW∗ 95, DD95],
especially in object-based attentional selection [Dun84,
RLS98]. Visual salience measures how much an item stands
out with respect to neighboring items. The higher this value,
the more visual attention it attracts. This suggests that the
effectiveness of a visualization can be improved by having a
desired level and spatial location of salience in the visualization. It also suggests that a measure of “appropriateness” of
the salience should be a quality metric for visualization.

Dictionaries define quality in general as a grade of excellence or superiority. As for visualizations, a technique
is commonly considered to be superior to another one, if
questions concerning the underlying data can be answered
more easily, faster or more accurately. Often this aspect
of a visualization’s quality is assessed with user studies
[KHI∗ 03], where subjects have to perform tasks that relate to the target group’s interaction with the visualization.
User studies comparing different visualization techniques
were carried out, for example, for uncertainty [SZB∗ 09],
flow [LKJ∗ 05, FCL09], hurricane [MSM∗ 08], and volume
[HGL∗ 99]) visualization.

Salience has been featured in previous works in visualization. For example, Ebert and Rheingans [ER00] proposed to
use non-photorealistic rendering to enhance salient regions
of volume data. Hladuvka et al. [HKG01] employed second
derivatives to address the same problem. Kim and Varshney [KV06] developed a tool that allows users to specify
salience-based emphasis to guide the viewer’s visual atten-

While user studies are very useful to evaluate some fundamental characteristics of a technique, it is not possible to
conduct a user study for each individual visualization every time when it is created. The quality of visualization images depends on main factors, including the domain-specific
requirements, the user’s needs and expectations, the source
data set and the techniques used. Hence, it is desirable to pro-

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

1183

1184

(a) Relevance mask

H. Jänicke & M. Chen / A Salience-based Quality Metric for Visualization

(b) Visualization

(c) Visual salience

Figure 1: Visualization-based importance: (a) Relevant
structures in the (b) visualization as defined by the visualizer, where black areas are most relevant. (c) Visual salience
of the image in (b).
vide users with alternative means to measure visualization
quality. Ideally such a measure is generic, readily available
and easily applicable to many types of visualization.
Researchers in visualization have proposed a number of
general quality metrics, including abstraction [CYWR06,
RWX∗ 07, Che05], profit [vW05], and esthetics [FB09]. A
quality metric based on abstraction quantifies how well a
visualization represents the underlying data. This is highly
relevant in applications where the data cannot be directly depicted as it is either too large, features more than two dimensions, or is manipulated to focus on certain aspects. A quality measure based on profit compares the benefits and costs
of a visualization and supports an economic model of visualization. A quality metric based on artistic appeal is given
by esthetics criteria, where the goal is to find the visually
most pleasing visualization. In this paper, we propose to add
a salience-based metric to this collection of metrics that favors visualizations that guide the observer’s attention to relevant parts of the image. We consider that the requirements
for such a quality metric include the followings:
• it must be a verifiable measure stated in quantitative terms.
• it allows and encourages comparison between visualization results generated using different control parameters.
• it provides a computer-generated measure complementary
to user studies as it is not practical to conduct a user study
for each visualization image.
In this work, we present a method for computing such a
metric for a visualization image in the context of a dataset
(Section 2). The underlying computational model is featurebased and supports across-scale analysis. We discuss the relevance of this metric to attention in visualization (Section
3). We demonstrate its usefulness with examples of information visualization, volume visualization and flow visualization (Section 4). We also discuss what has been measured
by the proposed quality metric and how to incorporate it into
visualization software and processes (Section 5).
2. A Salience-based Quality Metric
Visual salience is an important property in all visualizations. It defines which parts of the image stand out and will

likely attract a lot of attention. Ideally these areas should
coincide with the important parts of the data set. To measure the correspondence between relevant structures and the
observer’s attention, these quantities have to be defined for
each pixel in the visualization image, and can be computed
by the visualizer to evaluate and optimize the visualization
under construction. Here we purposely divide the users into
two categories, namely visualizers who create visualization,
and observers who are the target viewers of the visualization.
Sometimes, a visualizer can also be the sole target observer.
2.1. Data Relevance
Data relevance is a measurement of importance from the visualizer’s perspective. In the image space, data relevance can
be defined as a relevance mask, which tells for each pixel in
the image, how important the corresponding data is in the
context of the visualization. For example, for a sample visualization in Figure 1(b), a data relevance mask is shown in
Figure 1(a). Ideally the data relevance would correlate to the
salience of the visualization as depicted in Figure 1(c).
For some quality metrics (e.g., abstraction [CWM∗ 09]),
data relevance is an integral part of computation. For other
quality metrics (e.g., esthetics [FB09]), data relevance is often not included in the computation at all. A salience-based
metric has the flexibility to accommodate both approaches.
In many situations, the visualizer knows which parts of
the visualization are important and shall draw the attention
of the observer. Commonly these parts depict important features or structures in the data set. In these situation, a data
relevance mask can be created simply in the image space
using a brush or space-fill tool, which are available in most
image editing software. Such a tool can also be incorporated
in a visualization software to support a quality metric.
In situations where the important features and structures
are unknown to the visualizer, automated techniques such as
feature detection algorithms (e.g., [PW94, Hen98]) or data
statistics (e.g., [JWSK07]) can be used in the data space. The
computed object-space data relevance is then projected onto
the image space, resulting in a data relevance mask.
2.2. Visual Salience
The human visual system is very good at analyzing imagery
data. Complex scenes can be interpreted in real time, despite the limited speed of the neuronal hardware. This high
performance can only be achieved by selective analysis of
the scene [TCW∗ 95]. Visual salience provides the human
vision system with stimuli that attract our attention, facilitating the selective analysis. Neural mechanisms and computational models of visual salience have been extensively
studied in several disciplines, including neuroscience, physiology, psychology, and computer vision. There is a large
volume of literature, ranging from models that mimic the human perception (e.g. [DD95, WRKP05, IKN98]), techniques
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1185

H. Jänicke & M. Chen / A Salience-based Quality Metric for Visualization

that measure information theoretic properties, such as local
complexity [KB01] and information gain between different
scales [Jäg95].
Treisman and Gelade proposed that computing salience is
to find locations where local visual attributes differ significantly from the surrounding image attributes [TG80]. Niebur
and Koch provided the first implementation of salience
map [NK96] with color, intensity, orientation and motion
cues. Itti et al. improved the computation model by comparing the visual salience of local properties at different
scales [IKN98]. There are a variety of application-specific
salience measures that focus on different image properties
such as color, structure and gradients, e.g., [CH02, HKG01,
CWM∗ 09].
To compare the visual attention stimulated by different
parts of the given visualization, a salience map has to be
computed. As our work is concerned with how observers
perceive a given visualization, we will base our work on the
general model of [IKN98]. In this model, three relevant visual properties are selected: intensity contrast, color opponency, and orientation. These properties are used to define
whether an image pixel stands out or does not. We did not
include the motion in salience computation, as we consider
here only static visualizations. The algorithm to compute a
salience map then consists of three steps:
• Compute feature maps (color, intensity and orientation)
• Compare feature maps across scales
• Combine features into a salience map
2.2.1. Feature Maps
The first step towards a salience map is the computation of
feature maps from the input image. Feature maps, F, encode changes across different scales for the three relevant
visual properties, intensity contrast, color opponency, and
orientation. Itti et al. [IKN98] apply this hierarchical procedure to compare local characteristics to its surrounding.
Each scale has a different spatial resolution computed using
dyadic Gaussian pyramids [Lin94]. The changes are measured locally to mimic the visual receptive field. Therefore,
a local neighborhood Nb(p) around a pixel p is selected and
the sum of pair-wise differences between all pixel values in
the fine resolution, v(x, s1 ), and its projected counterparts in
the coarser resolution, v(x, s2 ), are computed:
F(p) =

∑

|v(x, s1 ) − v(x, s2 )|

(1)

x∈Nb(p)

Itti et al. [IKN98] propose a set of six distinct feature maps
for each property using the following combination of scales:
s1 ∈ {2, 3, 4} and s2 = s1 + ds, where ds ∈ {3, 4}.
The intensity I of each pixel p of the image is obtained by
computing the mean, I(p) = (r + g + b)/3, of the three color
channels r, g, and b. The intensity feature map, FI , is given
by:
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

FI (p, s1 , s2 ) =

∑

|I(x, s1 ) − I(x, s2 )|

(2)

x∈Nb(p)

Color opponency is important in color vision, as certain
pairs of colors intensify the perception of each other. Such
double-opponent color pairs are red/green and blue/yellow.
The feature maps are based on the normalized color components. Normalization is performed with respect to intensity
to decouple hue from intensity. The color opponency feature
map for red/green FRG is given by:
FRG (p, s1 , s2 ) =

(3)

| (r(p, s1 ) − g(p, s1 )) − (g(p, s2 ) − r(p, s2 )) |

∑
x∈Nb(p)

The third property, orientation, is measured using oriented
Gabor pyramids for angles θ ∈ {0◦ , 45◦ , 90◦ , 135◦ }. The resulting orientation maps O tell how good each of the four
filters represents the local image structure. The feature map,
with angle α of the Gabor filter, is given by:
FO (p, α, s1 , s2 ) =

∑

|O(x, α, s1 ) − O(x, α, s2 )|,

(4)

x∈Nb(p)

2.2.2. Across-scale Analysis
The feature map computation results in 42 feature maps.
The final goal is to compute a salience map that summarizes
salience using a single scalar value for each pixel. Hence,
the different values have to be combined in a single number. This is achieved in two steps. First, the feature maps for
each property are combined, and in a second step, the salient
regions are extracted from all three combined maps.
The three maps combining the different scales are called
“conspicuity maps”. They are obtained through across-scale
addition of the individual feature maps. Therefore, each feature map is reduced to scale 4 and normalized (N(x), see
[IKN98] for more detail) and afterwards, pixel values are
added point-wise:
F(p) = ∑ ∑ N(FI (p, s1 , s2 )),

(5)

s1 s2

where s1 and s2 are defined as earlier. If applicable, different modalities are added as well and we receive conspicuity
maps for intensity, color opponency, and orientation.
2.2.3. Salience Map
In our analysis we found that the contribution of the three
conspicuity maps to total salience is not equal. Especially
color is a very dominant visual clue and attracts a lot of
visual attention. In Section 3.2, a contribution map is presented that depicts the contribution of the channels to visual
salience. Based on this map, the formula of total salience is
modified to account for different contributions:
S = λ1 N(FI ) + λ2 N(FC ) + λ3 N(FO ),

(6)

where λ1 , λ2 and λ3 , with λ1 + λ2 + λ3 = 1, are weights to
account for variable contributions.

1186

H. Jänicke & M. Chen / A Salience-based Quality Metric for Visualization

(a) Difference field

(b) Color mapping

(c) Quality chart

Figure 2: Matching of user defined relevance and salience map: (a) Difference field between the relevance mask and the salience
map. (b) corresponding color map. (c) The quality chart provides an intuitive quality rating.
2.3. Quality Metric
After the definition of data relevance (relevance mask I) and
srangevisual salience (salience map S), the quality of a given
visualization can be determined. The goal is to create a visualization that directs visual attention to the areas that are relevant to the visualizer. Hence, a straightforward idea to measure the quality would be to sum squared differences over all
pixels p. This measure, however, has only little informative
value, as small variations in large insignificant regions often
dominate the results. Hence, we propose a more detailed visual result that accounts for the different relevance classes
and allows for an easy to analyze quality metric.
The idea is to depict for each importance class, how well
visual salience matches data relevance. Accordingly, the first
step is the computation of the difference field between relevance mask and salience map. Negative values indicate that a
structure with little relevance attracts a lot of visual attention
and positive values that relevant structures obtain too little
visual attention. In a second step, the differences are subdivided into three categories according to the desired relevance
and sorted for each category individually. The resulting data
can be represented using a bar chart that can be compared to
the distribution in the relevance mask and salience map.
We found that a continuous representation, as induced by
the continuous salience values, is rather difficult to interpret. Hence, we discretized the salience map according to
the user defined relevance into three classes and obtain the
color scheme as given in Figure 2(b). A sample quality chart
with additional annotations is given in Figure 2(c). The xaxis encodes the number of pixels per image and the two
lower bars indicate how many pixels were assigned to the
three importance classes by the salience map and the user respectively. The upper bar depicts the matching between the
two classes and can be thought of as a rearranged difference
field as given in Figure 2(a).
The most important quantity is the matching of the high
relevance structures (left section in the chart), as all relevant
features shall gain visual attention. In the ideal case of a perfect visualization, the number of relevant pixels as defined
by the user and the salience map would match exactly (same
length of lower dark blue bars) and all pixels were classified

correctly (“match” bar section entirely white). A secondary
criterion is the matching of the irrelevant structures (rightmost section of the chart), which indicates how many irrelevant areas attract too much attention.
3. Analysis of Visual Attention
The quality metric as described in the previous section allows the user only to see whether a visualization is good or
bad given the specified purpose. However, we want to allow
for a more detailed analysis and an iterative improvement of
the initial visualization. Therefore, the user needs information on where to improve the visual relevance. We propose
two different visualizations for this purpose: The first representation is an overlay of the original visualization and the
quantized salience map to indicate salient structures, and the
second one depicts the contributions of the visual channels.
3.1. Highlighting of Salient Structures
To highlight salient structures in the original visualization,
we have to merge two images, the visualization and the
salience map. We found direct overlays combined with isoline representations difficult to read and therefore, chose to
decrease the visual impact of the original visualization and
use it solely as a reference frame. Hence, we transform the
visualization to a gray scale image and manipulate the color
distribution to make dark edges more pronounced and to attenuate large areas. The resulting gray scale visualization is
then overlayed with the salience map, and isolines are added
to distinguish between the three major salience classes as
defined earlier.
3.2. Contribution of Individual Channels
To allow for the analysis of the contribution of the visual
channels to the final salience map, we provide a contribution map. The contribution map in Figure 3(c) shows where
each of the three channels contributes strongly. The thresholds can be interactively manipulated by the user. We commonly set the isovalue for all channels to 85 (range 0–255)
to reflect the earlier partitioning into three relevance classes.
Red areas indicate strong responses by the color opponency
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

H. Jänicke & M. Chen / A Salience-based Quality Metric for Visualization

(a) InfoVis Wiki Tag Cloud

(b) Salience Map

(c) Contribution map

(d) Tag Cloud on communication

(e) Salience Map

(f) Contribution map

1187

Figure 3: Two TagCloud visualizations (InfoVis): The TagClouds represent keywords of (a) the InfoVis Wiki (courtesy of InfoVis:Wiki team) and (b) communication. (b,e) Overlay of the original image with the salience map. (c,f) Contribution maps
(color opponency (red), intensity change (gray) and orientation (blue)).
channel, blue areas encode the orientation channel and grey
color indicates strong changes in intensity. Overlapping areas are represented using distinct patterns. If two channels
have strong influence, the area features a checker-board pattern consisting of the two contributing colors. The pattern
for three overlapping channels was chosen such that all colors are assigned about the same amount of squares and none
of them dominates the overall structure. Outlines are added
to ease distinction of overlapping areas.
4. Test Cases
We applied our quality measure to visualizations from three
major fields in visualization: TagClouds from information
visualization, Volume Ray Casting (VRC) from volume visualization, and methods from flow visualization. With each
application, we concentrated on a different aspect of visual
quality analysis: What attracts attention in a visualization?
(TagCloud), Which visualization for which purpose? (VRC),
and Which visualization is best? (Flow Visualization).
4.1. Information Visualization – Tag Clouds
A tag cloud is a visual representation of a set of words providing information about the relevance of the tags. The larger
the size of a word, the more important it is within the given
context, e.g., a web page or blog. Color is often used in an
artistic sense to make the image visually more appealing.
The first example in Figure 3(a) shows the tag cloud
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

displayed on the InfoVis wiki homepage (www.infoviswiki.net). The words in the tag cloud are colored in red,
black, sand color and shades of gray. Judging from the size
of the words, the most relevant terms are information, visualization, data, and research. However, when looking at
the salience map in Figure 3(b), we see that the visual attention does not well agree with the size of the words and
smaller words in intense colors appear very prominent. The
contribution map in Figure 3(c) reveals why small words become visually so important. Each color indicates areas that
are conspicuous according to one of the channels color opponency (red), intensity (gray), and orientation (blue). The
more channels are active, the higher the visual salience. In
general the contribution map reveals a very inhomogeneous
distribution of conspicuity. We see that the black words are
conspicuous with respect to intensity change and orientation,
i.e., on a fine scale we have strong contrast and more vertical
orientation of parts of the letters and on a coarse scale lower
contrast and horizontal orientation of the entire word. The
high salience of the red words results from the strong color
salience combined with changes in orientation again. In general we can say that the different channels compete strongly
for visual attention and the choice of color does not support
the intended representation of relevance of words.
A better choice of colors was made in the example in Figure 3(d). The contribution map (Figure 3(f)) shows a much
more homogeneous pattern and the different channels support each other. Hence, largest words attract most attention. To improve a visualization with respect to the choice

1188

H. Jänicke & M. Chen / A Salience-based Quality Metric for Visualization

of colors, different color schemes have to be evaluated for
their visual salience and depending on the application the
color scheme that best fits the relevance has to be applied to
achieve best salience patterns.

4.2. Volume Visualization – Volume Ray Casting
Volume rendering is a technique to compute a 2D projection of a three-dimensional data set that allows for a layered representation of three dimensional intricate structures.
A common problem is to find a suitable transfer function
that provides a good characterization of the different, partly
overlapping structures in the 3D volume.
The image in Figure 4(a) shows a direct volume rendering (DVR) of a CT engine data set. We can observe that
the outer surface is very easy to perceive, while the internal
structures and tunnels through the object are barely visible.
In [CWM∗ 09], Chan et al. proposed perception-based transparency optimization to increase the perception of semitransparent layers. The visualization of the same data set using their new algorithm is given in Figure 4(b). As proposed
by the authors, the different layers of structures inside the
data set are easy to perceive. However, it is rather difficult to
relate the structures to each other and to observe depth clues.
Figures 4(c) and 4(d) show the respective salience maps
of the two techniques. With the standard technique (Figure
4(c)), most attention is focused on the outline and surface
of the engine and partially on the red structure in the center.
In many areas multiple conspicuity channels are activated
(Figure 4(e)). The attention to the outline of the structure is
induced by all three attention channels. The central part, on
the contrary, owes its visual attraction solely to the choice
of red color. The perception-based algorithm (Figure 4(d))
features a more distributed salience map with smaller peaks
that guide the observer’s attention to many different structures of the data set. This distribution is more easily visible
in the contribution map in Figure 4(f). Like in the DVR contribution map, several structures exist that are conspicuous
in multiple channels. The structures, however, are less homogeneous and more widely distributed over the image.
The salience analysis supports the first impression. With
the standard DVR visualization, a lot of visual attention is
directed to the outline of the engine representation and the
overall impression is more homogeneous due to coherent
conspicuous structures. The perception-based representation
on the contrary guides the user to many different structures
in the data set and makes the observer focus more on details.
Depending on the task, either of the two visualizations
can be favorable. While the standard techniques provides a
better impression of the overall structure of the engine, the
perception-based algorithm disperses attention over the entire data set and makes internal structures easier to see.

(a) Standard DVR

(b) Perception-based

(c) Salience overlay

(d) Salience overlay

(e) Contribution map

(f) Contribution map

Figure 4: Volume rendering of engine dataset: (a) Direct
volume rendering and (b) perception-based transparency
optimization of the same data set (images courtesy of M.Y. Chan [CWM∗ 09]). (c,d) Overlay of the original image
with the salience map. Dark blue regions indicate highest
salience. (e,f) Contribution maps for color opponency (red),
intensity change (gray) and orientation (blue).
4.3. Scientific Visualization – Flow Visualization
Most available visualization software tools provide a large
variety of algorithms and in many application areas the same
data set can be represented in several ways, even if this only
means a change in the color scheme. The question that arises
is which technique is suited best to represent the data. To
find an answer the user commonly has to try different techniques and compare them or base the decision on experience.
User studies are a useful tool to find typical characteristics
of different techniques and compare them, but do not capture
all the subtle details and differences that occur in daily life.
Hence, we aim in this test case at a meaningful and simple
way to compare the quality of different visualizations based
on visual salience.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1189

H. Jänicke & M. Chen / A Salience-based Quality Metric for Visualization

(a) Grid-based vectors

(b) Jittered vectors

(c) Scaled Triangles

(d) Image-guided FlowFish

(e) LIC

Figure 5: Flow visualizations: (top) The same field is depicted using different techniques (req courtesy of D. Laidlaw [LKJ∗ 05]).
(2nd row) Salient structures are colored in blue. (3rd row) Contribution maps, red for intensity-band, grey for intensity and blue
for orientation. (bottom) Difference between the salience map and the relevance mask. Color coding as given in Figure 2(b).
The visualizations in Figure 5(top) depict different flow
visualizations [LKJ∗ 05] of the same data set. From left to
right, we see a grid-based representation of the vector data,
a depiction where the vectors have been jittered, a visualization using scaled triangles, a representation with imageguided flow fish and a line integral convolution (LIC) of the
vector field.
The images in the second row depict the original image
along with an overlay with the salience map. As all visualizations are gray-scale images, the color opponency channel
in the salience computation was substituted by the intensityband channel [IKN98]. We see that the first three visualizations attract visual attention in approximately the same areas.
The flow fish and LIC representation feature a more irregular pattern, with only few areas of high salience and large
unconnected areas of moderate salience.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

The contribution maps of Figure 5(a,b) look quite similar with all three channels agreeing in most areas. In the
scaled triangles visualization, the orientation contrast channel is activated in most areas where large triangles are clearly
visible. The discontinuities in the representation, e.g., close
to the critical points and separatrices, are picked up in the
intensity-band and intensity channel. The contribution maps
of the flow fish and LIC visualization show that the dense depiction of local directions induces only little visual attraction
as the overall structure of the entire image is very homogeneous. Attention is only attracted by spurious changes in the
intensity that are often artefacts of the algorithm.
Figure 6(a) shows the important features that the visualizer wants to show, and Figure 6(b) the corresponding relevance mask. The difference fields between the relevance
mask and the salience maps are displayed in the bottom row

1190

H. Jänicke & M. Chen / A Salience-based Quality Metric for Visualization

(a) Scaled triangles

(a) Features

(b) Relevance mask

Figure 6: Relevant structures in the flow visualization data
set. (a) Depiction of the interesting features. (b) Relevance
mask as designed by the visualizer.
of Figure 5. As these images are rather difficult to interpret,
we reorganize the data and display it using the quality chart.
The results are given in Figure 7. Good performance with
respect to drawing attention to the features is achieved by
the “scaled triangles” and “grid-based vectors” visualization,
where a large subset of the highly important section (left part
of the bars) is colored white, i.e., correct amount of visual
attention. The “scaled triangles” visualization features much
smaller and more focused high salience areas, whereas these
areas are much larger in the “grid-based vectors” visualization. Worst quality according to visual salience have “LIC”
and “jittered vectors” as all regions are about equally visually attractive. Hence, the user has to scan the entire image
– which in this case is not too difficult, as the picture is very
small – in order to detect relevant structures.
5. Discussions
In the previous section, we explored the application of our
salience-based quality metric using test cases from three major fields in visualization. In general, the results gave a very
good indication on which part of a visualization needs improving or which one to choose for a given purpose. In this
section, we compare our results with earlier findings, discuss
shortcomings, and outline the use of the metric in practice.
The first question that arises is, does the salience map
reflect the distribution of human attention? The quality of
the salience map model was evaluated in several studies.
In [IK99], Itti shows that the model can be used to identify suspicious objects in complex scenes. In [BI05], Baldi
and Itti found that the human gaze is attracted to surprising
areas which is captured by the model as well and the study
in [BI08] reveals that the saliency map can be used to predict
eye location. Our results are consistent with these findings.
In the tag cloud example in Section 4.1, we measured the
quality of different color selections. This analysis, however,
is purely based on salience, and we do not take personal preferences about color or esthetics into account. Many people
might find the first image (InfoVis wiki) visually more interesting or pleasing. From an information communication
point of view, the second example features the better choice

(b) Jittered vectors

(c) Grid-based vectors

(d) Image-guided flow fish

(e) LIC

Figure 7: Match statistics for the flow visualizations: Each
chart gives an overview over the relevance classes as defined
by the user (lower bar) and the salience map (central bar),
as well as the match between the two classifications (upper
bar). See Figure 2(c) for a more detailed explanation.
of color. A combined measure of visual salience and esthetics might further improve the analysis to create images that
attract correct attention and are visually pleasing.
The flow visualizations investigated in Section 4.3 are part
of a user study conducted by Laidlaw et al. [LKJ∗ 05]. In this
study, the subjects of the study had to accomplish different
feature location, definition and tracking tasks. They found
that the methods have different strengths and weaknesses
and none is optimal for all tasks. Best results were achieved
with techniques that depict directed integral curves and critical points, e.g., the image-guided flow fish technique. Our
analysis, however, rated the “scaled triangles” and “gridbased vectors” best, while LIC and “jittered vectors” feature a poor salience profile. These seemingly contradicting
results have an easy explanation. While our quality metric
picks up areas that stand out within the image, Laidlaw et
al. measured performance according to time and error. The
images are relatively easy and can be easily scanned as a
whole by a human. Thus, the subjects could quickly spot the
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

H. Jänicke & M. Chen / A Salience-based Quality Metric for Visualization

1191

relevant positions and took most of the time to perform the
more challenging task, e.g., determining the exact location
of a critical point. As features are found easily even if they
do not stand out, visual salience has hardly any effect except
a little influence on the timing. Local precision, the second
performance measure in the user study, cannot be captured
by our salience-based quality metric and is an interesting direction for future research.

visualization created will be in conveying information to the
observers, and whether they would mislead observers. The
visualizer can apply the salience and contribution maps to
evaluate each created visualization in an objective and visual
manner. In this case, the creation of the relevance map is
not essential, as the visualizer usually has a mental image of
such a map, and can mentally overlay this mental image with
a salience or contribution map.

The second question that arises is, how can one use this
quality metric in practice? In the previous sections, we mentioned several different maps, namely salience map, contribution map, and relevance map. The first two maps are generated directly from a visualization by using the method discussed in Section 3. The relevance map can be created automatically from the data or manually by the visualizer as
discussed in Section 2.1. Based on these, we can envisage
different levels of support from a visualization system for a
salience-based quality metric. At the basic level (level 1), a
system provides a utility for computing salience and contribution maps, allowing users to evaluate their created visualizations visually in a manner similar to Figures 4(b,c,e,f),
5(e,f) and 6(rows 2 and 3). At level 2, a system provides
users with a utility to create a relevance map by paintingover a visualization. Alternatively, the system allows the
users to import an image of a relevance map painted by a
third-party image editing tool. With a relevance map, a visualizer can evaluate a visualization using a difference field as
exemplified by Figure 3 and Figure 6(row 4). At level 3, a
system provides a utility to generate a relevance map using
a data-space algorithm (e.g., feature detection, uncertainty
estimation, etc.). Such a utility is normally data-type dependent, and often application dependent.

In both Scenarios, the use of this quality metric also provides users with opportunities to accumulate understanding
and experience about different methods and associated parameter space from an information theory and perception
perspective. From the perspective of quality assurance, Scenario A focuses on the tools used in a visualization process,
while Scenario B focuses on the “product” of the process.
Like most quality assurance processes, it is unlikely that a
single quality metric can meet all the requirements. Hence
using a combination of quality metrics is highly desirable.

With these three levels of support, we can also envisage
different scenarios, where the quality metric is used. Two
typical scenarios are given below:
Scenario A: Evaluating Techniques. For exploratory visualization, a user is a visualizer as well as an observer, and
may face a collection of optional technique and a large parameter space for each technique (e.g., opacity and color
transfer functions, visual mapping, etc.). It would take a
huge effort to explore all such options on a daily basis, while
the priority should be given to exploring a large data space
through visualization. Hence, the user can determine a default setting which would be most effective for everyday use
as follows. (1) Take a typical example data set, create a visualization using a method, obtain a relevance map with the
abovementioned level 2 or 3 support. (2) Experiment with
different methods and parameters, and apply the salience
map, contribution map, and difference field to each of the
created visualizations. Determine the most effective method
and parameter set. (3) Using the selected method and parameter set as the default setting to explore other data sets.
Scenario B: Evaluating Visualizations. In this case, a user
is a visualizer and may need to determine how effective the
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

6. Conclusions
In this paper, we presented a novel quality metric for visualization based on visual salience. This metric favors visualizations that guide the observers visual attention to relevant parts of the image and is generically applicable to all
areas of visualization. Using different analysis tools, i.e.,
salience overlays, contribution maps and quality charts, we
could show how our quality metric can be used to choose the
best visualization from a set of alternatives, and to establish
an effective default setting for a visualization process.
In most cases visualizers have a mental image of what
they want to show to the observer with their visualization.
So far the common technique to assess the match between
expectation and actual results were user studies that are very
time consuming. With our technique the visualizer is provided a fast and easy to apply tool that tells how well the
visualization meets the expectations.
The most important aspect of our quality-metric is that the
technique can be easily integrated into existing visualization
software. Within these frameworks it is a valuable addition
not only for visualization experts but also for novices that
obtain easy to understand feedback on the quality of the visualization they created.
As we have seen in the discussion, there are several directions for an improved quality metric for visualization. Not
only correctness of the visualization has to be ensured (abstraction measures), but also task performance (user studies) and esthetics (esthetics measure). Although we recognize that it is unrealistic to expect a single quality metric to
adequately serve all needs, combining different quality measures might result in a more comprehensive feedback on the
quality of a visualization that helps create correct, meaningful, easy to understand and esthetic visualizations.

1192

H. Jänicke & M. Chen / A Salience-based Quality Metric for Visualization

References
[BI05] BALDI P. F., I TTI L.: Attention: Bits versus wows. In
Proc. IEEE Int. Conf. on Neural Netw. and Brain, Beijing, China
(Oct 2005), Zhao M., Shi Z., (Eds.), vol. 1, pp. PL56–PL61.
[BI08] B ERG D. J., I TTI L.: Memory, eye position and computed saliency. In Proc. Vision Science Society Annual Meeting
(VSS08) (May 2008).
[CH02] C OLLOMOSSE J. P., H ALL P. M.: Painterly rendering
using image salience. Eurographics UK Conference, Annual 0
(2002), 122.
[Che05] C HEN C.: Measuring the quality of network visualization. In JCDL ’05: Proceedings of the 5th ACM/IEEE-CS joint
conference on Digital libraries (New York, NY, USA, 2005),
ACM, pp. 405–405.

[JWSK07] JÄNICKE H., W IEBEL A., S CHEUERMANN G.,
KOLLMANN W.: Multifield visualization using local statistical
complexity. IEEE Transactions on Visualization and Computer
Graphics 13, 6 (2007), 1384–1391.
[KB01] K ADIR T., B RADY M.: Saliency, scale and image description. Int. J. Comput. Vision 45, 2 (2001), 83–105.
[KHI∗ 03] KOSARA R., H EALEY C. G., I NTERRANTE V., L AID LAW D. H., WARE C.: User studies: Why, how, and when? IEEE
Comput. Graph. Appl. 23, 4 (2003), 20–25.
[KV06] K IM Y., VARSHNEY A.: Saliency-guided enhancement
for volume visualization. IEEE Transactions on Visualization
and Computer Graphics 12, 5 (2006), 925–932.
[Lin94] L INDEBERG T.: Scale-Space Theory in Computer Vision.
Kluwer Academic Publishers, 1994.

[CWM∗ 09] C HAN M.-Y., W U Y., M AK W.-H., C HEN W., Q U
H.: Perception-based transparency optimization for direct volume rendering. IEEE Transactions on Visualization and Computer Graphics 15, 6 (2009), 1283–1290.

[LKJ∗ 05] L AIDLAW D. H., K IRBY M., JACKSON C., DAVID SON J. S., M ILLER T., DA S ILVA M., WARREN W., TARR M.:
Comparing 2D vector field visualization methods: A user study.
IEEE TVCG 11, 1 (January-February 2005), 59–70.

[CYWR06] C UI Q., YANG J., WARD M., RUNDENSTEINER E.:
Measuring data abstraction quality in multiresolution visualizations. IEEE Transactions on Visualization and Computer Graphics 12, 5 (2006), 709–716.

[LVJ05] L EE C. H., VARSHNEY A., JACOBS D. W.: Mesh
saliency. In SIGGRAPH ’05: ACM SIGGRAPH 2005 Papers
(New York, NY, USA, 2005), ACM, pp. 659–666.

[DD95] D ESIMONE R., D UNCAN J.: Neural mechanisms of selective visual-attention. Ann. Rev. Neurosci. 18 (1995), 193–222.
[Dun84] D UNCAN J.: Selective attention and the organization of
visual information. J. Exp. Psy. Gen. 113, 4 (1984), 501–517.
[ER00] E BERT D., R HEINGANS P.: Volume illustration: nonphotorealistic rendering of volume models. In VIS ’00: Proceedings of the conference on Visualization ’00 (Los Alamitos, CA,
USA, 2000), IEEE Computer Society Press, pp. 195–202.

[MSM∗ 08] M ARTIN J. P., S WAN J. E., M OORHEAD R. J., L IU
Z., C AI S.: Results of a user study on 2d hurricane visualization.
Computer Graphics Forum 27 (May 2008), 991–998(8).
[NK96] N IEBUR E., KOCH C.: Control of selective visual attention: Modeling the ‘where’ pathway. Neural Information Processing Systems 8 (1996), 802–808.
[Pal99] PALMER S.: Vision Science: Photons to Phenomenology.
MIT Press, Cambridge, 1999.

[FB09] F ILONIK D., BAUR D.: Measuring aesthetics for information visualization. In IV ’09: Proceedings of the 2009 13th International Conference Information Visualisation (Washington,
DC, USA, 2009), IEEE Computer Society, pp. 579–584.

[PW94] PAGENDARM H.-G., WALTER B.: Feature detection
from vector quantities in a numerically simulated hypersonic
flow field in combination with experimental flow visualization.
In VIS ’94: Proc. of the conf. on Visualization ’94 (Los Alamitos,
CA, USA, 1994), IEEE Computer Society Press, pp. 117–123.

[FCL09] F ORSBERG A., C HEN J., L AIDLAW D.: Comparing 3d
vector field visualization methods: A user study. IEEE TVCG 15,
6 (2009), 1219–1226.

[RLS98] ROELFSEMA P. R., L AMME V. A. F., S PEKREIJSE
H.: Object-based attention in the primary visual cortex of the
macaque monkey. Nature 395, 6700 (Sept. 1998), 376–381.

[Hen98] H ENZE C.: Feature detection in linked derived spaces.
In VIS ’98: Proc. of the conf. on Visualization ’98 (Los Alamitos,
CA, USA, 1998), IEEE Computer Society Press, pp. 87–94.

[RWX∗ 07] RUNDENSTEINER E. A., WARD M. O., X IE Z., C UI
Q., WAD C. V., YANG D., H UANG S.: Xmdvtoolq:: qualityaware interactive data exploration. In SIGMOD ’07: Proc. of the
2007 ACM SIGMOD international conference on Management
of data (New York, NY, USA, 2007), ACM, pp. 1109–1112.

[HGL∗ 99] H ANS P., G RANT A. J., L AITT R. D., R AMSDEN
R. T., K ASSNER A., JACKSON A.: Comparison of threedimensional visualization techniques for depicting the scala
vestibuli and scala tympani of the cochlea by using highresolution mr imaging. American journal of neuroradiology, 7
(1999), 1197–1206.

[SZB∗ 09] S ANYAL J., Z HANG S., B HATTACHARYA G., A M BURN P., M OORHEAD R.: A user study to compare four uncertainty visualization methods for 1d and 2d datasets. IEEE TVCG
15, 6 (2009), 1209–1218.

[HKG01] H LADUVKA J., KÖNIG A., G RÖLLER E.: Salient
ˇ
representation of volume data. In Joint EurographicsâA˘ TIEEE
TVCG Symp. Visualization (2001), pp. 203–211.

[TCW∗ 95] T SOTSOS J. K., C ULHANE S. M., WAI W. Y. K.,
L AI Y., DAVIS N., N UFLO F.: Modeling visual attention via
selective tuning. Artif. Intell. 78, 1-2 (1995), 507–545.

[IK99] I TTI L., KOCH C.: Target detection using saliency-based
attention. In Proc. RTO/SCI-12 Workshop on Search and Target Acquisition (NATO Unclassified), Utrecht, The Netherlands,
RTO-MP-45 AC/323(SCI)TP/19 (Jun 1999), pp. 3.1–3.10.

[TG80] T REISMAN A., G ELADE G.: A feature integration theory
of attention. Cognitive Psychology 12 (1980), 97–136.

[IKN98] I TTI L., KOCH C., N IEBUR E.: A model of saliencybased visual attention for rapid scene analysis. IEEE Trans. Pattern Anal. Mach. Intell. 20, 11 (1998), 1254–1259.
[Jäg95] J ÄGERSAND M.: Saliency maps and attention selection
in scale and spatial coordinates: an information theoretic approach. In ICCV ’95: Proc. of the Fifth Int. Conf. on Computer
Vision (Washington, DC, USA, 1995), IEEE Computer Society,
pp. 195–202.

[vW05] VAN W IJK J.: The value of visualization. In C. Silva,
E. Groeller, H. Rushmeier (eds.), Proc . IEEE Visualization 2005
(2005), pp. 79–86.
[WRKP05] WALTHER D., RUTISHAUSER U., KOCH C., P ER ONA P.: Selective visual attention enables learning and recognition of multiple objects in cluttered scenes. Comput. Vis. Image
Underst. 100, 1-2 (2005), 41–63.

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

