DOI: 10.1111/j.1467-8659.2009.01629.x
EUROGRAPHICS 2010 / T. Akenine-Möller and M. Zwicker
(Guest Editors)

Volume 29 (2010), Number 2

The Virtual Director:
a Correlation-Based Online Viewing of Human Motion
J. Assa and L. Wolf and D. Cohen-Or
The Blavatnik School of Computer Science, Tel Aviv University

Abstract
Automatic camera control for scenes depicting human motion is an imperative topic in motion capture base animation, computer games, and other animation based fields. This challenging control problem is complex and combines both geometric constraints, visibility requirements, and aesthetic elements. Therefore, existing optimizationbased approaches for human action overview are often too demanding for online computation.
In this paper, we introduce an effective automatic camera control which is extremely efficient and allows online
performance. Rather than optimizing a complex quality measurement, at each time it selects one active camera
from a multitude of cameras that render the dynamic scene. The selection is based on the correlation between
each view stream and the human motion in the scene. Two factors allow for rapid selection among tens of candidate views in real-time, even for complex multi-character scenes: the efficient rendering of the multitude of view
streams, and optimized calculations of the correlations using modified CCA. In addition to the method’s simplicity
and speed, it exhibits good agreement with both cinematic idioms and previous human motion camera control
work. Our evaluations show that the method is able to cope with the challenges put forth by severe occlusions,
multiple characters and complex scenes.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism—Camera control

1. Introduction

occlusions and different significance of limbs throughout the
motion [DZ94, BTM00, HHS01, MK06, CO06, ACO∗ 08].

Human motion animations in games and 3D environments
applications are ubiquitous and continue its rapid growth. As
a result, the necessity for motion-sensitive camera handling
techniques is growing respectfully. Controlling the camera
to generate video clips which expresses well the motion
is challenging since the problem poses many requirements
with conflicting constraints. Several studies had proposed
systems specializing on human motion data, which demonstrated encouraging results; however such solutions are offline methods or can only be applied for simple scenes.

An alternative to the optimization methods are rule based
methods which rely on cinematopgrahy guidelines referred
to as “cinematic idioms” formulated in the early days of the
cinemtography theory. As basis for the theory, the founders
of cinemtography had suggested using multiple cameras
which are orchestrated together using a known set of cinematic idioms rules. These guidelines were designed to introduce simple scene configurations describing the scene
events, in a coherent manner. For example, in a conversation scene, the discussing parties are to be viewed by the
cameras to provide the viewer the feeling of taking part in
the conversation [Ari76].

Camera control methods that support human motion apply various optimization techniques, searching for an optimal camera control path subject to many constraints including general ones which hold for any scene such as collision avoidance, occlusion detection, coherency and camera
smooth and slow-moving trajectory, and constraints specific
for human action such as significance of gestures, internal

The cinematic idioms, which describe the preset camera
configuration and their switching strategy, were suggested
for a certain set of specific scenarios, and were implemented
by several of the early camera control systems [HCS96],
however, the extension of these idioms to other scenes and

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

595

596

J. Assa & L. Wolf & D. Cohen-Or / The Virtual Director

Figure 1: Our method generates a coherent presentation of human motion animation scenes by automatically evaluating the
view streams of a multitude of virtual cameras and constructing the sequence of the most suitable streams in real-time.

scenarios were not pursued. The cinematic rules abstraction
suggested by several camera control methods use geometric
constraints which are easy to compute [DZ94,HO00,CO06].
However they are limited and insufficient for expressing the
details of human motion based scenes as much as optimization methods [ACCO05, ACO∗ 08].
In this work, we introduce an effective automatic method
which is both fast enough to allow online computation and
generic in a sense that it can be applied to arbitrary human
action scenes. The key idea is to use a multitude of virtual
cameras from which at any given time the best view is selected. The premise is that a rather modest number cameras is dense enough so that the view from at least one of
them is expressive enough. Our method is based on a fast
measurement of the correlation between the given scene animation and the camera output. This measurement allows
to assess how descriptive each view is with respect to the
source animation, and select the best view which captures
the essence of the original animation. We further extend this
notion of correlation to define a strategy for determining instants in time where a switch between the available virtual
cameras can take place, thus introducing multi-shot scheme.
Our technique in a sense mimics the professional solution
of real-time camera crew, e.g., the teams which cover online
TV sport events. In such teams, the camera control director
primarily selects the best camera view stream at each given
time.
As we highlight in our novel approach, picking the best
view is by far computationally easier than optimizing the
camera pass over the whole space. Nevertheless, as we
demonstrate, the online video generated by the multitude
of camera is expressive enough and approximates well the
quality of an offline optimization method. Furthermore, the

simplicity of our method allows handling of complex scenes
under severe occlusion conditions as well as multi-character
scenes, thus extending the applicability to new cases which
were not handled by previous approaches.
The presented method introduces several additional contributions which we examine in this work: first, we show
that the view-animation correlation can be effectively used
to define a partial order between the various view streams.
This partial order, under some restrictions, can be used to
compose video summaries of scenes even without the animation data. Secondly, we show that the scheme fits existing
camera idioms, and therefore can be considered as an extension and generalization of the “spirit” of the defined idioms to different scenes and scenarios. Lastly, the simplicity
of the method can be used in various applications, such as
autonomous game viewing cameras, quick overview generation for 3D authoring tools, etc.
The following section describes some of the recent advances in camera control and human motion sensitive camera control, next we describe the details of our method in
Sections 3, 4. We discuss our results, show a comparison to
both previous work, and present a user study which assess
our results in Section 5. We conclude with a summary in
Section 6.

2. Related work
The problem of camera control focuses on searching for a
suitable camera configuration for capturing a scene narrative, while obeying a set of cinematographic rules [Mas65].
The problem is challenging since the camera configuration
is subject to numerous constraints, such as occlusion, objects visibility, and layout in the resulting image [GW92,
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

J. Assa & L. Wolf & D. Cohen-Or / The Virtual Director

597

rapid actions, it is shown that the method generates a nonsmooth and unpleasing camera results. Assa et al. [ACO∗ 08]
describe an optimization method with several constraints
specifically tuned to the description of human action, such
as self occlusion and action saliency. They use a global optimization to generate a suitable camera path, with possible
splitting the scene-capturing to multiple shots. Their work is
based on quantum annealing and cannot provide an online
solution.

Figure 2: An example of three input views (top three strips),
and animation stream (bottom strip). Each frame is presented as a single column in strips. The view columns include the view pixels vectors and the animation data includes the pose joints details. Our method automatically
seeks for correlation between the view and the animation
data, and uses the correlation to select the best view.

CMN∗ 05]. A survey of the different approaches to the problem is described in Christie and Olivier overview [CO06].
The art of cinematography placed a special attention on
action scenes with single and multiple actors. Guidelines
were established, defining a set of standard principles of
camera configurations and transitions [Ari76,Mas65,Kat91]
often referred to as cinematography idioms such as establishing the scene configuration, avoiding jump cuts, avoiding crossing the scene action line and other rules. An early
attempt to mimic some of the idioms in a complete framework is the “The Virtual Photographer” [HCS96]. In their
work they use state machine control together with predefined
camera configuration to implement an idioms based camera control system. In general, methods for camera control
had handled various soft and hard idioms related constraints,
based on geometric properties of the camera position, and on
the attributes of captured view (e.g., [DZ94,HCS96,BTM00,
HHS01, LST04, MK06]).
Most of the existing camera control systems are generic
and are not designed to specifically support or express human actions. Only recently, several studies are addressing
this problem with specialized methods for human motion.
Kwon and Lee [KL08] introduce a camera control technique for character based scenes. Their approach is based
on the measuring the motion area, that is, the integrated
area spanned by the character bones motion. Their work focus on a selection of static camera positions which are then
extended into a camera path by interpolation. In cases of
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

The intelligent galleries [VBP∗ 09] uses supervised learning to realize good camera positions for various scenes.
Their approach, although not specifically designed to human
action, suggests that a single general framework such as machine learning analyzing the camera views can be used to
define camera preferences in many different domains. Similarly, in this work we show that the general tool of motion
correlation can by applied in an unsupervised manner to human aware camera control solution.
Although the effectiveness of using multiple shots in
scenes is known since the dawn of cinematography, not
many camera control systems had utilized this tool. A brief
review of existing camera control studies shows that only
a small number of the work handles multiple shots. These
studies typically employ shots to solve cases of cameraobject collisions, or cases in which the camera view quality
deteriorates as a result of a rapid scene action coupled with
slow camera movement [DZ94, HO00, CO06, ACO∗ 08].

3. Views correlation
Our goal is to present a dynamic scene by using a sequence
of views that capture as much of the scene action as possible.
Given a set of view streams, we would therefore like to prioritize them according to how well they express a given animation stream. We use a simple view representation, which
employs for each frame the vector of all pixel values. As
we show in this work, even low resolution rendering of the
views convey sufficient information for the purpose of our
processing. The animation stream consists of motion capture data joint details, such as their relative angles. Note that
even the low resolution frames and straightforward animation representations used, produce large input data streams.
A single frame of a typical view stream is of 9,000D. This
large amount of data, illustrated in Figure 2, require efficient
computational means.
While the view and the animation representations differ
in their properties, both reflect information arising from the
underlying motion, and are therefore correlated. The premise
is that expressive views have higher correlation with the animation stream. Moreover, if the view contains a significant
amount of other types of variability that are caused by camera motion or by the motion of background objects, the correlation diminishes.

598

J. Assa & L. Wolf & D. Cohen-Or / The Virtual Director

3.1. Efficient computation of CCA
The correlation between the view stream vector and the animation stream vector is computed via the Canonical Correlation Analysis (CCA) technique. CCA is a statistically solid
tool that is frequently used in various data analysis [Bor98].
It was introduced by Hotelling as a method of examining
the dependencies between two sets of random variables. The
technique finds two linear transformations such that the correlation between the transformed pairs of variables is maximal.
Let {xi }ni=1 and {yi }ni=1 be sets of real-valued vectors
with zero mean (if they are not, the mean is subtracted first)
such that xi ∈ RdX and yi ∈ RdY . Let X = [x1 , x2 , . . . , xn ] and
Y = [y1 , y2 , . . . , yn ] be matrices with xi and yi in their i’th column, respectively. Given a dimension d, CCA finds matrices
WX of dimensions dX × d and WY of dimensions dY × d that
maximize the element-wise correlations of the transformed
vectors WX xi and WY yi subject to the constraints that the elements of the transformed vectors are pairwise uncorrelated
and of unit variance. The optimization objective of CCA in
matrix form is given by:
max

WX ,WY

tr(WX XY WY )

(1)

subject to WX XX WX = WY YY WY = I .

(2)

After applying the projections WX and WY , the vectors
from both input spaces become d dimensional vectors. Denote these transformed vectors as x˜ = WX x j and y˜ = WY y j .
Denote by v(r) the r-th coordinate of a vector v. Eq. 1 is
equivalent to maximizing the sum of the canonical correlations which are the d scalars defined as ∑ni=1 x(r)
˜ y(r),
˜
for 1 ≤ r ≤ d. This is done subject to constraints of the
form:∑ni=1 x(r)
˜ x(s)
˜ = ∑ni=1 y(r)
˜ y(s)
˜ = δrs , where δrs is defined to be 1 if r = s, and 0 otherwise.
A numerically favorable way to solve the CCA optimization problem is by means of Singular Value Decomposition
(SVD) and matrix square root [GVL96], namely the decomposition of the matrix
1

1

M = (XX )− 2 XY (YY )− 2 .

(3)

Let M = USV denote the Singular Value Decomposition
of M, the canonical correlations are readily available as the
diagonal elements of S. WX and WY , which are not required
by our technique, can be readily extracted from U and V .
In our application, CCA is computed over two types of
vectors: a typically long view-based vector x j of dX elements, and a much shorter action-based vector y j or length
dY . The analysis is performed to short time windows of
n = 40 frames or less, where each frame i constitutes one
matching pair (xi , yi ), and we set d = n. To directly compute
the CCA as above, one would need to construct and invert
matrices that have as many rows and columns as the length
of xi . Next, we describe how to transform the problem to a

smaller one, which involves much smaller matrices, which
are of the size of the time window. This technique can be
seen as a simplification of kernel CCA [Aka01].
Let KX , KY be the Gram matrices which are n × n matrices such that (KX )i j = xi x j , and (KY )i j = yi y j . Straightforward linear algebra arguments imply that we can write
Wx = XA and Wy = Y B for two matrices A and B of size
n × d. Then, the correlation matrix (Eq. 1) is expressed as
Wx XY Wy = A X XY Y B = A KX KY B ,
and similarly for the constraints (Eq. 2)
WX XX WX = A X XX XA = A KX KX A
WY YY WY = B Y YY Y B = B KY KY B.
This yields a standard CCA problem where X and Y are replaced by the Gram matrices KX and KY , this time solving
for A and B:
max
A,B

tr(A KX KY B)

subject to A KX KX A = B KY KY B = I .
The proposed SVD solution still holds, only X and Y are replaced by KX and KY respectively, and the decomposition
is performed to matrices of size n × n. Since n is typically
˜ we are able to perform hundreds of such decomsmall (40)
positions per second.
3.2. Measuring the correlation
At each point in time, each view stream k is represented
(k) (k)
(k)
by a set of vectors x1 , x2 , ..., xn which encode the last
n frames as seen at this particular view. We employ the simplest encoding and use the low resolution rendering of the
scene as the representation of each view. This has the advantage of allowing faster rendering of views by conventional
graphics cards.
The human motion parameters are represented by conventional means. At each point in time, the poses in the last n
frames are encoded as a series of vectors y1 , y2 , ..., yn . Within
each such vector y j , m joints are represented as 3m dimensions. Each single joint is encoded as three entries storing
either the joint global location and rotation, or the skeleton
relative location and relative rotation.
(k)

We employ CCA to the pairs {(xi , yi )}ni=1 arising from
each view k to prioritize the expressiveness of views. If a
view is informative regarding the underlying motion parameters, then we can expect that the correlation between the
pairs of vectors to be high at least along one direction. Moreover, if the view captures several uncorrelated aspects of the
underlying motion, we can expect to find significant correlation in several orthogonal directions. Therefore, for each
view k we employ two statistics obtained by CCA: the first
canonical correlation (hk ), and the number of canonical correlations above a certain threshold (nk ). The first measure
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

599

J. Assa & L. Wolf & D. Cohen-Or / The Virtual Director

Figure 3: The resulting canonical correlations for several
views of a single character exercises scene. Here we highlight the curve characteristics used by our method - the
largest canonical correlation hk , and the number of canonical correlations nk above a certain threshold.

expresses the dominance of the most prominent component
which is shared by both the view and captured motion. The
second measure expresses the number of uncorrelated directions in which the view and the scene motion are similar. The
threshold ensures that the counted directions are strongly expressed in both the view and the parameterized motion. An
example of the two measurements are shown in Figure 3. An
example of the behavior of hk over time for a simple scene
of a single character is shown in Figure 4.
4. Method description
The availability of the correlation-based measures suggests
a simple process for generating an online clip of a given animation scene from multiple views. The resulting clip is generated by continuously selecting views which are best representatives of the animation:- the method selects the view
with the best quality measure as the current view stream,
and use it until its accumulated erosion signal exceeded a
given threshold. After which it select a different view and
continue in the same manner. The details of each of these
steps are described next.
4.1. View Streams Settings
The determination of a camera control path is known to be a
time consuming task since the underlying optimization combines many constraints. Instead of selecting a single “best
quality” view we use simple methods to suggest a set of different camera paths. To explore the stability of our solution
we have implemented several different strategies for generating the candidate camera paths, including randomly generated camera paths and cinematography based path heurisc 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Figure 4: The largest canonical correlation (hk ) as a measurement for the quality of a view . Results shown are of several views for a single character exercises scene. The gray
ribbon indicates the views which will be considered during
a shot switch, i.e., those with hk that is sufficiently close to
the highest one.

tics. Random constructions include simple random selection
of static camera vantage points, and paths which comprise of
random straight line and simple arcs as the camera path. Following common cinematography guidelines, we used several
path generation methods including character tracking camera at three-quarters view, an over-the-shoulder camera, various simple vantage point cameras positioned on the front,
back,right, and left sides of the scene, dollying along the
scene movement line, and a distant “director shot” camera.
An example of the large variety of selected paths is illustrated in Figure 7(b) and in Figure 9. While the differences
in results of the various strategies will be discussed in Section 5, the following next steps were applied in the same
manner to every set of camera paths.
4.2. Erosion of the current view
Cinematography suggests that views are switched in cases
where the current view is no longer effective, when a better
expressive view can be selected or when sufficient time had
elapsed since the last switch [Ari76, HCS96]. To accommodate these guidelines, we use an accumulated view erosion
signal (AVES). This is a monotonically increasing signal;
Once it reaches a value of 1 a new view is selected.
AVES incorporates a time erosion, which is controlled by
a switching time parameter t¯ set to 90 frames (three seconds), the number of frames since the last view switch t, and
the quality of the various views hk (t), k = 1..N in comparison to the quality of the currently active view hc (t). Specifically, AV ES(0) = 0 and the accumulation formula is:
AV ES(t) = AV ES(t − 1) + 1 −

hc (t)
maxk hk (t)

t 1
· + .
t¯ t¯

600

J. Assa & L. Wolf & D. Cohen-Or / The Virtual Director

incorporates sufficient motion components, and the second
biases the quality toward ones with frontal character views.
4.4. Implementation details
The presented method does not include any computational
bottlenecks. The various views are rendered at thumbnails
of 64 × 48 pixels, and the CCA calculations are performed
on 40 × 40 matrices, which are created by multiplying tall
thin matrices by their transpose. All of the example clips
shown in the accompanying video were generated at realtime speeds on a Pentium M 2.1GHz, with 1.5GB of memory, with standard ATI Mobility FireGL graphics card, and
by using non-optimized Matlab and c++ implementations.
Figure 5: The accumulated view erosion signal (AVES) function. When the signal exceeds a certain threshold, a view
switch is performed.

This formula limits the number of frames between view
switch events to t¯. In case where the highest quality view is
not the active view, AVES promotes an earlier view switch.
The progress of AVES depends on the relative qualities of
the currently best view and the active view, as well as on the
amount of time since the last view switch. The latter ensures
that shots do not change too rapidly. Figure 5 illustrates the
accumulation term behavior over time.

4.3. Switching to a new view
In considering which view would become the next view, the
view quality score hk plays an important role in conjunction
with geometric considerations, which eliminate views that
will make the video clip incoherent. First, to avoid what is
known as “jump cut” the spatial lookat angle difference between the previous and the new view is constrained to be
above a given threshold. Second, in fast action movements
the new and old views are constrained to be on the same side
of the scene progress line. Both of these constraints are easily calculated, and geometrically unsuitable views are dismissed early to avoid unnecessary computations.
Further, we rule out views with relatively low correlation
to the animation, as depicted by the first canonical correlation hk (Sec. 3.2). Specifically, we only consider candidate
views k for which hk > τ max hi , for τ = 0.9.
i

Finally, we evaluate the candidate views by examining a
combination of the number (nk ) of canonical correlations
above a threshold of σ and the number of character face pixels. The latter is estimated rapidly as the area of the projection of a disk placed on top of the character’s face onto the
image. In our system σ = 0.1 and the score is computed as
qk = nk + fk . The first term, ensures that the selected view

Our method is robust with regard to view resolution and
quality. The thumbnail views we employ were found experimentally to have the same level of performance as higher
quality views, hence we are able to render very rapidly. The
openGL based rendering of the views thumbnails were performed at 720 frames per second, which allows comparing
up to around 20 distinct views, each rendered at 30 FPS. The
fast CCA computation was performed at a pace of 100 CCA
calculations per second, using Matlab. As a result, such a
mid- to low-range system could support a real-time rate of
about 20 views at 30 FPS. For scenarios such as computer
games where the system resources are shared with other
threads, we experimented with reducing the frame rate to
20 and 15 FPS, with minimal to no impact on the quality
of the results. Additional reduction in the number of frames
per second used for analysis resulted in deteriorating performance for fast moving action scenes.
We have tested the technique on various clips with variable number of potential views, by using complex and simple scenes, and multiple characters. Some of the results are
presented in Figure 9 and in the accompanying results and
user study videos.
5. Results and Discussions
5.1. Cinematic idioms
Cinematographic idioms include guidelines for camera work
for specific scenes, which describe a preset cameras placement, and a switching policy between them [Ari76, Mas65].
Using these idiom-based preset camera locations for our
method cameras, we can observe interesting similarities between our method selection policy and the idioms switching
guidelines, as shown in Figure 6.
Single character action. We have placed cameras at different directions around a exercising character (top, bottom, sides, frontal and back views and three-quarter views).
While examining the output of our method, it becomes apparent that for a single-character the highest quality (hk )
views are often the three-quarters view, and in several cases,
the top view. Incorporating the face area cue, the threequarters view becomes significantly superior to the top-view
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

J. Assa & L. Wolf & D. Cohen-Or / The Virtual Director

Figure 6: Our method shot selections mimics the behavior
defined in known cinematic idioms. The top row shows a
setup which captures a long character walk. Bottom row: the
camera setup for a conversation-like scene. Each of the images show the resulting camera shot sequence, color coded
according to the used cameras. For better impression we refer the viewer to the accompanying video to this paper.

and all other standard views. This result matches the known
cinematic rule in which character actions should be captured from the three-quarters view, since it best represents
the depth of character while performing the actions [Ari76].
Motion along a path. Capturing a character walk along a
path, we placed cameras at uniform intervals. The method
generates a sequence of long panning shots. This behavior is
the suggested idiom for capturing large movement of characters over a vast scenery [Ari76].
Back and forth interaction between two persons. Back
and forth interaction is common, e.g., in dialogue scenes.
Here, we’ve placed three cameras, as suggested by similar
idioms - two cameras were placed pointing to the figures,
and a third was placed to capture both. Due to the lack of audio or lip-movement data, we used a ball passing sequence,
which is analog in this respect to a verbal dialogue. The selected camera sequence significantly resembles the sequence
suggested for shooting a conversation between two persons,
according to their “turn” in the dialogue [Ari76].
5.2. Evaluation
We evaluate the results of our method, by using several
means. First we compare it to the results of the recent work
of Assa et al. [ACO∗ 08]. As their work only handles scenes
with limited complexity and single characters, we further
continue with an evaluation of our results by performing a
similar user study, which compares our results to that of a
professional animator and a naïve camera control method.
The numerical comparison is performed using the view
metrics suggested by [ACO∗ 08]: (1) the total energy E of
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

601

Figure 8: Results of our user study comparison. Participants were shown 4 scenes, each rendered by 4 camera control methods (shown in different colors). They were asked to
grade the information,professionalism and expressiveness of
the clips from 1-5 (5 being the best score).

the generated camera path; (2) the external energy term,
Eexternal which includes the saliency biased spatial viewpoint potential (without the camera smoothness term); and
(3) the “standard viewpoint potential metric” V . Note that as
stated by [ACO∗ 08] the three metrics measure only specific
aspects of the animation, and the professional human animator obtains worse results in those metrics than the method
of [ACO∗ 08]. In all three measures, our method energy is
between their reported results of a human animator and their
method values. The difference between the approaches is
best illustrated in Figure 7, where we show the camera path
of Assa et al., an example of our random views, and the resulting selected views. For a better impression and comparison we direct the reader to examine the supplementary video
material of this work.
Following the evaluation methods used in recent studies,
we compared the information, professionalism and expressiveness on several clips. The compared methods include the
most correlative single camera location (Naive), our method
(Our), and two professional animator results - with no design
restriction (Animator1 ), and with camera switching every 34 seconds restriction (Animator2 ). For a better impression
we direct the reader to examine the supplementary video
material of this work. The clips were presented in a random
order to 20 computer graphics students, who were asked to
grade from 1-5, 5 being the best score, according to how
informative is the result, how professional it looks, and its
expressiveness. The results from the user study, (Figure 8)
indicate that our solution scores better then the single camera location option, and in some cases it matches the professional animator options score. In all three criteria a t-test
indicates that our method significantly outperforms (p-value
< 5%) the best single location camera. The user study is presented in the supplementary material of this paper.

602

J. Assa & L. Wolf & D. Cohen-Or / The Virtual Director

Figure 7: A top level view of the generated character root (shown in dark gray), and the camera path used to capture the motion
(color coded according to cameras). (left) illustrates the resulting camera shots of Assa et al., (middle) illustrates the various
camera movement path used by our algorithm (right) shows the automatically selected shot sequence. The sequence of color
coded camera shots are shown below.

As part of the evaluation we have tested our system on
cluttered scenes. Our method was able to deal with low
to medium level of clutter. However, for highly cluttered
scenes, a view was sometimes picked in which the main action is occluded in the middle of the frame while the sides
of the frame are uncluttered. This behavior differs from the
typical choice of human directors. Note that all prior computational methods assume no clutter at all, and are not robust
even to low levels of clutter. We examined the method’s robustness to different character models, geometries and textures. When examining characters with significantly different proportions, as the ones shown in Figure 9 4th row, unorthodox viewpoints were consistently selected by our technique, such as the top and back views, as the large head
movements are clearly seen from these views. These selections suggest that the currently common viewpoint selection
convention is specifically tuned toward a standard human
character proportions, and suggests the alternative of using
more neutral proportions characters for the camera selections in specific scenarios. Different character textures, did
not show any significant difference, as long as the character
was sufficiently represented by the low resolution views.
We have evaluated the performance of our method, by
using a variable number of cameras, starting from 3 up to
50 cameras in various scenes. As described in Section 4.1,
we used different camera placement strategies which include
both random placement and movement, but also idiom-based
configurations. While idiom-based configurations promoted
better results with fewer cameras than the random strategies,
even the random placements of cameras provided satisfactory results with reasonable number of cameras. Although
such a comparison highly depends on the viewed motion, in
many cases a small number (up to ten cameras) of the randomly selected viewpoints were sufficient to generate satisfactory results. Moreover, when using simple heuristics as
in idioms configurations, usually a 4-5 set of cameras were
sufficient to capture the motion in a pleasing manner.

5.3. Selection without animation information
We adopt the method to cases where only the view streams
are provided. This problem is ill-posed since it is not clear
which apparent motion is the significant one without semantic reasoning or additional cues. However, it is possible to
infer relative dominance of some of the views. For example,
comparing the relative quality of views depicting the same
part of the scene may be possible, while there is no comparison between views of different scene parts. Computationally,
given a set of views of a scene, we examine the correlation
of each view pair, by using multivariate linear regression.
( j)
(k)
Given two views xi and xi , i = 1..n, we obtain a matrix
( j)

(k)

A jk which minimizes E jk = ∑i ||A jk xi − xi ||2 , and a ma(k)
( j)
∑i ||Ak j xi − xi ||2 .

The retrix Ak j that minimizes Ek j =
sulting error estimate E jk evaluates, using a linear approximation, how well view j captures the information of view k.
This is run for time windows of n = 100 frames. If E jk <<
Ek j , we determine that view j dominates view k, and thus
presents the underlying motion better. We employ this partial order to obtain at each point in time the list of all views
for which there is no dominating view. Figure 10 shows an
example, and another example is provided in the accompanying video on a difficult scene from the CMU quality of life
grand challenge (http://kitchen.cs.cmu.edu).

5.4. Limitations
The main limitations of the proposed method emanate from
its simplicity. Since the automatic method does not employ
sophisticated detection and recognition algorithms, and the
CCA correlates the view to the global animation, the selected
views might include close-ups and even extreme close-ups
shots. This problem can be minimized with adoption of
stronger supporting heuristics, which will be used to ensure
that the selected view is sufficiently inclusive. The semantic meaning of the view can also influence the quality of the
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

J. Assa & L. Wolf & D. Cohen-Or / The Virtual Director

603

Figure 9: An illustration of some of our results. Each of the 7 clips show 6 equally spaced view frames from the resulting clip,
and the computed shot sequences, color-coded according to used cameras. The presented clips include (top to bottom), a fight
scene, single character exercises, blindman’s buff game, dance, basketball match, ball passing and a long walk. The colorcoded camera initial configurations for clips 1,3,5,7 are illustrated on the right. We show here only a sample of the camera
path selection strategies which were used. For a better impression of our method’s results, we refer the reader to examine the
accompanying video clip to this paper.

result. Theoretically, since the method is not biased toward
certain body parts, views which do not focus on the significant limbs may be selected. Incorporating a the method of
Assa et al. [ACO∗ 08] would provide a bias toward the significant limbs according to the action.
An additional limitation emerges from the method’s incapability to predict the potential future correlation between
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

the views and the animation. In some cases, immediately after a certain view is selected, its quality quickly deteriorates,
and as a result a new view selection should be made. Systems which attempted to predict the scene progress, as the
work of Halper et al. [HHS01] did not produce a significant
improvement. A practical approach for solving this problem
can be resolved by creating a short delay (a couple of sec-

604

J. Assa & L. Wolf & D. Cohen-Or / The Virtual Director

References
[ACCO05] A SSA J., C ASPI Y., C OHEN -O R D.: Action synopsis:
Pose selection and illustration. In SIGGRAPH 2005 Conference
Proceedings (Aug. 2005), vol. 24, ACM, pp. 667–676.
[ACO∗ 08] A SSA J., , C OHEN -O R D., , Y EH I.-C., L EE T.-Y.:
Motion overview of human action. ACM Transactions on Graphics (Also Proceedings of ACM SIGGRAPH ASIA) 27, 5 (2008).
[Aka01] A KAHO S.: A kernel method for canonical correlation
analysis. In Int. Meet. of Psychometric Soc. (2001).
[Ari76] A RIJON D.: Grammar of the film language. SilmanJames Press, 1976.

Figure 10: Using multivariate linear regression to discover
a partial order of views in respect to the motion they represent. This figure describes the results of the single character
exercises clip, with 6 different views - top, front, back, right,
left and 3/4 view. Each row presents the dominance of one
view over the other (high dominance is shown in yellow, and
no apparent dominance in white).

[Bor98] B ORGA M.: Learning Multidimensional Signal Processing. PhD thesis, Linköping University, Sweden, SE-581 83
Linköping, Sweden, 1998.
[BTM00] BARES W. H., T HAINIMIT S., M C D ERMOTT S.: A
model for constraint-based camera planning. In Smart Graphics,
Papers from the 2000 AAAI Spring Symposium (2000), vol. 4,
AAAI Press, pp. 84–91.
[CMN∗ 05] C HRISTIE M., M ACHAP R., N ORMAND J.-M.,
O LIVIER P., P ICKERING J.: Virtual camera planning: A survey.
In Smart Graphics (2005), pp. 40–52.
[CO06] C HRISTIE M., O LIVIER P.: Camera control in computer
graphics. In Eurographics 2006 Star Report (2006), pp. 89–113.

onds) between the inputs and the resulting stream, allowing
a look ahead into the future and selecting accordingly.
6. Summary

[DZ94] D RUCKER S. M., Z ELTZER D.: Intelligent camera control in a virtual environment. In Proceedings of Graphics Interface ’94 (1994), pp. 190–199.
[GVL96] G OLUB G. H., VAN L OAN C. F.: Matrix computations.
Johns Hopkins University Press, Baltimore, MD, USA, 1996.

In this work, we present a novel technique which is based
on exploring the animation-view correlation, introducing a
fast and simple method for multi-shot video overview of a
given human-action animation. This technique is shown to
fit several known cinematic idioms, as well as to produce
satisfactory results compared to previous work in this field.
The applications of techniques for real-time human motion
aware camera control system are vast and include both autonomous camera system for gaming and virtual environments. Such methods also suggest tools to guide novice animators in building a preliminary video clips which highlight
their animation. This can be extended to automatic generation of video summaries of animation which can be created
on demand in an online manner.

[GW92] G LEICHER M., W ITKIN A.: Through-the-lens camera
control. In SIGGRAPH 1992 conference proceedings (New York,
1992), ACM, pp. 331–340.

Although we did not specifically present in this work how
to use additional signals to correlate to the view and the motion, it is easy to foresee usage of similar methods which
synchronize music, voice and other cues. This may result
in additional methods which can promote automatic music
video generation. The presented extension to the case where
no animation model is provided holds potential for a lowoverhead multi-camera live video blogging.

[KL08] K WON J.-Y., L EE I.-K.: Detemination of camera parameters for character motions using motion area. The Visual
Computer 24 (2008), 475–483.

7. Acknowledgments
This work was supported in part by grants from the Israel
Science Foundation founded by the Israel Academy of Sciences and Humanities.

[HCS96] H E L.-W., C OHEN M. F., S ALESIN D. H.: The virtual cinematographer: a paradigm for automatic real-time camera
control and directing. In SIGGRAPH 1996 Conference Proceedings (1996), ACM, pp. 217–224.
[HHS01] H ALPER N., H ELBING R., S TROTHOTTE T.: A camera engine for computer games: Managing the trade-off between
constraint satisfaction and frame coherence. In EG 2001 Proceedings (2001), vol. 20(3), Blackwell Publishing, pp. 174–183.
[HO00] H ALPER N., O LIVIER P.: Camplan: A camera planning agent. In AAAI 2000 Spring Symposium on Smart Graphics
(2000), AAAI Press, pp. 92–100.
[Kat91] K ATZ S. D.: Film Directing Shot by Shot: Visualizing
from Concept to Screen. Michael Wiese Productions, 1991.

[LST04] L IN T.-C., S HIH Z.-C., T SAI Y.-T.: Cinematic camera
control in 3d computer games. In WSCG (2004), pp. 289–296.
[Mas65] M ASCELLI J. V.: The Five C’s of Cinematography: Motion Picture Filming Techniques. Graphic Publications, 1965.
[MK06] M C C ABE H., K NEAFSEY J.: A virtual cinematography
system for first person shooter games. In Proceedings of International Digital Games Conference (2006), pp. 25–35.
[VBP∗ 09] V IEIRA T., B ORDIGNON A., P EIXOTO A., TAVARES
G., L OPES H., V ELHO L., L EWINER T.: Learning good views
through intelligent galleries. In Eurographics (Munich, 2009),
pp. 717–726.

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

