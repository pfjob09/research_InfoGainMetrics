DOI: 10.1111/j.1467-8659.2010.01662.x

COMPUTER GRAPHICS

forum

Volume 29 (2010), number 6 pp. 1955–1968

Reducing Plenoptic Camera Artifacts
T. Georgiev1 and A. Lumsdaine2
1 Adobe
2 Indiana

Systems, Inc., San Jose, CA, USA
University, Bloomington, IN, USA
lums@cs.indiana.edu

Abstract
The focused plenoptic camera differs from the traditional plenoptic camera in that its microlenses are focused on
the photographed object rather than at infinity. The spatio-angular tradeoffs available with this approach enable
rendering of final images that have significantly higher resolution than those from traditional plenoptic cameras.
Unfortunately, this approach can result in visible artifacts when basic rendering is used. In this paper, we present
two new methods that work together to minimize these artifacts. The first method is based on careful design of
the optical system. The second method is computational and based on a new lightfield rendering algorithm that
extracts the depth information of a scene directly from the lightfield and then uses that depth information in the
final rendering. Experimental results demonstrate the effectiveness of these approaches.
Keywords: focused plenoptic camera, plenoptic rendering, lightfield rendering, artifact reduction, depth
estimation
ACM CCS: Image Processing And Computer Vision [I.4.3]: Imaging Geometry—Super Resolution

1. Introduction
The plenoptic camera was introduced in [AB91], as a technique for capturing 3D data and solving computer-vision
problems. It was designed as a device for recording the distribution of light rays in space, that is the 4D plenoptic function or radiance. The light field and lumigraph, introduced
to the computer graphics community respectively in [LH96]
and [GGSC96], established a framework for analysing and
processing these data. In 2005, Ng improved the plenoptic
camera and introduced new methods of digital processing,
including refocusing [NLB∗ 05, Ng05].
Because it captured the full 4D lightfield, Ng’s handheld
plenoptic camera could produce effects well beyond the capabilities of traditional cameras. Image properties such as
focus and depth of field could be adjusted after an image
had been captured. Unfortunately, traditional plenoptic cameras suffer from a significant drawback; they render images
at disappointingly low resolution. For example, images rendered from Ng’s camera data have a final resolution of 300 ×
300 pixels.
c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and
Blackwell Publishing Ltd. Published by Blackwell Publishing,
9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main
Street, Malden, MA 02148, USA.

The plenoptic camera’s poor resolution stems from the
way it samples the 4D radiance of a scene. This sampling is
based on the assumption that spatial and angular information
are captured independently. The position of the microlenses
captures spatial information while the pixels under each microlens sample the angular distribution at the position of each
microlens.
A different approach, called ‘full resolution lightfield rendering’ [LG08], can produce final images at much higher
resolution based on a modified plenoptic camera (the ‘focused plenoptic camera’ [LG09]). This modified camera is
structurally different from the earlier plenoptic camera with
respect to microlens placement, microlens focus, and, most
importantly, with respect to the assumptions made about the
sampling of the 4D radiance. The traditional plenoptic camera focuses the main lens on the microlenses and focuses the
microlenses at infinity. The focused plenoptic camera instead
focuses the main camera lens well in front of the microlenses
and focuses the microlenses on the image formed inside the

1955

1956

T. Georgiev & A. Lumsdaine / Reducing Plenoptic Camera Artifacts

Figure 1: Image directly rendered from our plenoptic camera 2.0 data, and the same image rendered with artifact reduction.
camera—that is each microlens forms a relay system with
the main camera lens. This configuration introduces a flexible trade-off in the sampling of spatial and angular dimensions and allows positional information in the radiance to be
sampled more effectively. As a result, the focused plenoptic
camera can produce images of much higher resolution than
can traditional plenoptic cameras.

mation of a scene directly from a plenoptic image and then
using this depth information to render an artifact-free image.
The results of our approach can be seen in Figure 1, which
shows an image rendered without depth information and the
same image rendered with depth information.

An intuitive way of understanding how the focused plenoptic camera can achieve its higher resolution is to consider the
full resolution rendering process. Since the microlens images
are focused, they capture entire sections of the scene with respect to positional coordinates. Thus, the final rendered image can be composed by assembling together appropriately
sized sections (or ‘patches’) from the microimages created
by the microlenses. The number of pixels in the assembled
patches represents the increase in resolution of the focused
plenoptic camera over the traditional plenoptic camera (i.e.,
the focused plenoptic camera may render 15×15 pixels per
microlens where the traditional plenoptic camera would only
render one pixel per microlens).

1. We present a description of the depth-based artifacts
resulting from full resolution rendering;

Unfortunately, the basic full resolution rendering process
can produce severe artifacts, as can be seen in some of the
results in Lumsdaine [LG08, LG09] and in Figures 10(a) and
(b) of this paper. These artifacts result because the patch size
necessary to produce artifact-free full resolution rendering is
dependent on the depth in the scene. Consequently, different parts of a scene will require different patch sizes to be
properly rendered.
Such depth-dependence suggests one approach for
artifact-free rendering—namely, to use depth information
from the scene so that different (and correct) patch sizes can
be used for rendering different parts of the scene. In this paper, we present two approaches for minimizing depth-based
artifacts. The first is based on camera design to minimize the
artifacts. The second is based on extracting the depth infor-

The contributions of this paper are as follows:

2. We present an optical analysis and camera design that
minimizes depth-based artifacts in full resolution rendering;
3. We develop an algorithm for extracting depth information of a scene directly from a plenoptic image;
4. We develop a rendering algorithm that uses depth information to render images that are essentially artifact
free; and
5. We demonstrate a working model and final rendering
based on our new camera model and new rendering
approach.
2. Related Work
The work reported in this paper extends the full-resolution
rendering approach for the focused plenoptic camera. In our
new approach, depth information is extracted from the captured lightfield image and then used to render full-resolution
images without artifacts. Individual aspects of our approach
can be found in other work and we discuss the most salient
of those here.
The basic structure of the focused plenoptic camera can
first be seen in the early work of Lippmann [Lip08], who considered the array of lenslets as an array of cameras focused
on the photographed object. Lippmann proposed the idea

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

T. Georgiev & A. Lumsdaine / Reducing Plenoptic Camera Artifacts

1957

of using a lenslet array to capture the radiance of a scene
and produce what he called integral photographs. These
photographs captured not only a 2D picture, but also the
‘3D relief’ of the scene.
More recently, the basic principles of the focused plenoptic camera have been independently considered by a number of researchers including Ng [Ng06], Fife [FGW08], and
Lumsdaine [LG08, LG09]. In [Ng06], Ng considers different
configurations of the handheld plenoptic camera in which the
sensor is not located precisely at the focal plane of the microlenses. Analysis and experiments with this configuration
indicate higher spatial resolution for the traditional plenoptic camera can be achieved by moving the sensor closer to
the microlenses. Analysing and leveraging the properties of
the focused plenoptic camera specifically for the purposes
of full-resolution rendering appears to have first been proposed in [LG08] and [LG09]. The approach described in
these works was well-suited to a single depth plane; artifacts
resulted in the rendered image for portions of the scene at
other depths. The microlens arrays in [FGW08] are focused
on the image plane, and the use of the overlapping microimages for high-resolution 2D image reconstruction as well as
for depth estimation is discussed. While [FGW08] briefly
discusses image rendering, the significant contribution, of
the work is the multi-aperture image sensor itself. Given the
similarity between the multi-aperture image sensor and the
focused plenoptic camera, we should expect to see the same
artifacts in both systems. Similarly, we would expect that
the multi-aperture system would benefit from depth-based
full-resolution rendering in eliminating those artifacts.
Adelson and Wang considered techniques for depth estimation from an image captured by a plenoptic camera in
[AW92]. In that work, the plenoptic camera was of a traditional design, that is the main lens was focused on the microlenses and the microlenses were focused at infinity. Parts
of the scene that were focused by the main lens in front of,
or behind, the microlenses resulted in different characteristics of images in neighbouring microlens images. A one-pass
displacement algorithm over the microlens images was used
to estimate scene depth. This algorithm is similar to the one
used for depth estimation in the present paper, the primary
difference being the characteristics of the input data to the
algorithm (unfocused vs. focused, respectively).
Lightfield rendering can be considered in the larger context of image-based rendering, an approach that uses multiple
images for rendering novel views. The need for incorporating depth information in image-based rendering was recognized in [GGSC96] and [BBM∗ 01]. The work by Chai
et al. [CCST00] analysed the sampling rate necessary for
lightfield representation that would be necessary to produce
rendered images free of aliasing. The paper suggests decomposing scenes into multiple layers of constant depth
and rendering each layer as appropriate for its associated
depth.

Figure 2: The conventional plenoptic camera.
3. The Plenoptic Camera
3.1. Traditional plenoptic camera
The traditional plenoptic camera is based on an array of
microlenses at the image plane of the main camera lens,
with the sensor one focal length behind the microlenses (see
Figure 2). Considering that the focal length of the main
camera lens is much greater than the focal length of the
microlenses, we have each microlens focused on the main
camera lens aperture (equivalently, at infinity), not on the
object being photographed. Thus, each microlens image is
completely defocused relative to that object. As a rule, these
individual microimages look blurry and unrecognizable to a
human observer.
Following the development in [LG08, LG09], we denote
the radiance in a scene by r(q, p), where q and p represent
position and direction in ray space, respectively. The traditional plenoptic camera samples the radiance as shown in
Figure 3. Each microlens image is a vertical stack of samples
in the (q, p) plane, capturing strictly the angular distribution
of the radiance at the image plane.
Images are rendered from the radiance captured by the traditional plenoptic camera by integrating all angular samples
at a particular spatial point. However, each spatial point is
sampled by a single microlens, so rendering involves integrating all of the pixels in each microimage. As a result, and
as designed, rendering from the plenoptic camera produces
only one pixel per microlens, producing a rendered image
with very low resolution. Even with 100,000 microlenses,

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1958

T. Georgiev & A. Lumsdaine / Reducing Plenoptic Camera Artifacts

p
d
a

Microimage
Pixels

d
b

q

d

Figure 3: Sampling of the radiance r(q, p) by the microlens
array represented in the two-dimensional (q, p) plane. Each
pixel samples a single direction in the directional coordinate
and samples a span of d (the microlens and microimage size)
in the positional coordinate.

a
b

d

Figure 5: Sampling of the radiance r(q, p) by the microlens
array of the focused plenoptic camera, represented in the
two-dimensional (q, p) plane. The microlens aperture is given
by d; a and b are the spacing from the microlens plane to
the image plane and from the microlens plane to the sensor,
respectively.

In playing this role, each microlens forms a relay imaging
system with the main camera lens. The position of each microlens satisfies the lens equation, 1/a + 1/b = 1/f , where
a, b, and f are respectively the distance from the microlens
to the main lens image plane, the distance from the microlens
to the sensor, and the focal length of the microlens. In our setting, b is greater than f . A different setting is possible, where
the main lens image is considered a virtual image formed
behind the sensor, a is negative, and b is less than f . We do
not discuss this version of the camera in this paper, but the
treatment of such a case would be similar.
The focused plenoptic camera samples the radiance as
shown in Figure 5. Each microlens image is a slanted stack
of samples in the (q, p) plane, capturing both angular and
positional distribution of the radiance at the image plane.
Figure 4: The focused plenoptic camera.
the handheld plenoptic camera reported in [NLB∗ 05] produces a final image of only 300 × 300 pixels.
3.2. Focused plenoptic camera
As shown in Figure 4, the focused plenoptic camera is based
on an array of microlenses focused on the image plane of
the main lens. Thus, each microlens captures a portion of the
image formed by the main lens. We can think of the main
camera lens as positioned slightly forward, so the image
is formed some distance in front of the microlenses. The
microlenses serve as an array of real cameras, reimaging
parts of that image onto the sensor.

As with the traditional plenoptic camera, images are rendered from radiance captured with the focused plenoptic
camera by integrating the angular samples at a particular
spatial point (see Figure 6). Unlike the traditional plenoptic camera, however, the angular samples for a given spatial point are sampled by different microlenses. Rendering
thus involves integrating across microlens images rather than
within microlens images.

3.3. Rendering with the focused plenoptic camera
One instance of rendering that is useful for understanding
full-resolution rendering—and for understanding the artifacts that arise—is to render a single view. This is equivalent to rendering a scene captured with a small (or pinhole)
aperture.

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

T. Georgiev & A. Lumsdaine / Reducing Plenoptic Camera Artifacts

each one capturing part of the scene and each one capturing a
slightly different perspective of the scene. A square aperture
was used in our camera to minimize unused sensor space
between microimages.

p
Microimage
Pixels

q

View

1959

Pixels
Rendered
Image

Figure 6: Full resolution rendering of a single view with
the focused plenoptic camera. In this case four pixels are
rendered from each microlens image.
Consider such a rendering process with the particular radiance sampling pattern shown in Figure 6. In this example,
rendering with a single view covers four samples from each
microlens image. The rendered image has four times as many
pixels as there are microlenses—four times the resolution of
the traditional plenoptic camera. In general, the attainable
resolution of a full-resolution rendered image depends on
the depth of the scene.
As derived in [LG09], the spatial resolution of a fullresolution image is b/a times the spatial resolution of the
sensor. Resolution increases for image planes closer to the
microlens plane (where b/a approaches unity) or, equivalently, for planes in the scene that are closer to the main lens
(in the foreground). Thus, image planes in the foreground
can be rendered with a higher resolution (larger number of
pixels per microlens) than image planes in the background.
As can be seen in Figure 6, different pixels taken from a
single microlens are not truly at the same direction since the
microlens samples at a slant. As a result, there is a discontinuity in direction as we move from one microimage to the
next due to the zigzag shape of the sampling. However, under
approximately Lambertian conditions over some reasonable
range of directions, there will not be visible discontinuities
between patches in the rendered image due to the sampling
pattern.
The actual rendering algorithm is fairly straightforward.
The pseudocode for it, along with a graphical depiction of it,
are given in Figure 7.
To illustrate the rendering algorithm in more detail, consider Figures 8(a) and (b). A section of an image recorded
by the sensor in our focused plenoptic camera is shown in
Figure 8(a). As with the traditional plenoptic camera, the focused plenoptic camera records a flattened 2D representation
of the 4D light field. The flat consists of an array of images,

When we choose the number of pixels per microimage,
we crop out squares of that size (the ‘patch size’) from the
microlens images and combine them together into a final
rendered image. The choice of one patch size or another puts
different world planes ‘in focus.’ In other words, patches
match each other perfectly only for one image plane behind
the main lens. By the lens equation, this corresponds to a
given depth in the real world. A different patch size would
correspond to a different depth. In other words, refocusing
is accomplished in the focused plenoptic camera through
choice of the patch size in the full-resolution rendering
algorithm.
Unfortunately, as seen in the initial work on full resolution
rendering, severe artifacts occur when patches do not match
up correctly, that is when the chosen patch size is not correct for the depth of the scene (or part of a scene) [LG08,
LG09]. In the following section we will discuss the nature of
these artifacts in more detail and present our approaches for
effectively eliminating them.

4. Artifacts
With the full-resolution rendering approach described above,
two different kinds of artifacts can arise: when the patch size
is too large for the scene being rendered or when the patch
size is too small for the scene being rendered. The resulting
artifacts are blocky in either case (with the block size equal to
the patch size) because regions taken from adjacent microlens
images will not match at their boundaries. In the former case
(patch size too large), we have too much of the microlens
image, whereas in the latter case (patch size too small), we
have too little of the microlens image. A detailed example of
each case is shown in Figures 9(a) and (b).
Figures 10(a) and (b) show how artifacts appear in scenes
with varying depths. Figure 10(a) demonstrates the rendering
of a view of a scene generated with a patch size that is
appropriate to correctly represent the foreground. Notice the
artifacts in the background. Figure 10(b) shows the same
image with a patch that renders the pencil holder correctly.
Notice the blocky artifacts in the extreme foreground.
Although Figures 10(a) and (b) deliberately show uncorrected artifacts, the artifacts are significantly reduced relative
to what we have observed with our earlier uncorrected camera designs [LG08]. We take a two-pronged approach to further reduce (and essentially eliminate) these artifacts. First,
with proper camera design, we are able to capture radiance
images that show almost no artifacts with basic rendering
for all parts of the captured scene at distances greater than
1 m. (The images in Figures 10(a) and (b) are captured at a

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1960

T. Georgiev & A. Lumsdaine / Reducing Plenoptic Camera Artifacts
Patch
Microlens
Image

Full Resolution Rendering Algorithm.
Given: Discrete Nx × Ny × nx × ny radiance
r[i,j,k,l], patch size P .
Output: Rendered P ∗ Nx × P ∗ Ny image
I[s,t].
For (s,t) in (P*Nx, P*Ny)

Captured Radiance

nx

P

P

ny

Nx
Ny

I[s,t] = r[i,j,k,l],
where i=(s/Nx)*P, j=(t/Ny)*P,
k=(s%Nx)*P, and l=(t%Ny)*P.

P · Nx

Rendered
Image

P · Ny
Full Resolution
Rendering

Figure 7: The full resolution rendering algorithm creates a final rendered image from P × P patches of each nx × ny microimage. With Nx × Ny microimages in the captured radiance, the final rendered image is P · Nx × P · Ny . Pseudocode for
the algorithm is given on the left, a graphical depiction of it on the right.

Figure 8: Illustration of full resolution rendering with real lightfield data. (a) A section of the image captured by the sensor in our
focused plenoptic camera. Notice that the microimages are in focus. A square main-lens aperture was used to pack microimages
more efficiently. (b) Regions of four microlenses become a contiguous part of the rendered image. The four microimages used
are outlined with a green rectangle in Figure 8(a).
suboptimal distance for our camera, 0.5 m.) Second, by extracting depth information from the captured radiance image
and using that information in the rendering process, we are
able to essentially eliminate depth-related artifacts in the focused plenoptic camera. We present our hardware-based and
software-based approaches in the remainder of this section.

4.1. Reducing the artifacts in optics hardware
The artifacts noted above can be reduced significantly by
appropriate camera design. The two primary camera design
considerations are the size of the microimages and the variability of minification (which we define as a/b).

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

T. Georgiev & A. Lumsdaine / Reducing Plenoptic Camera Artifacts

1961

Figure 9: Full resolution rendering when the patch size is incorrect (too large or too small) for the scene being rendered.
(a) Patch size (12) too large. (b) Patch size (4) too small.

Figure 10: Two types of uncorrected artifacts with the focused plenoptic camera. (a) The optimal patch size for rendering the
foreground is too large for the background, resulting in artifacts. (b) The optimal patch size for rendering the background is too
small for the foreground, resulting in artifacts.

As discussed in Section 3.3, the rendering process samples
the captured radiance in a zigzag pattern so that there is a
discontinuity in the directional component of the sampling
between adjacent patches. If the scene meets the Lambertian
assumption, then this discontinuity will not be a problem.
However, this assumption will not always hold (or will not
hold over sufficiently large ranges of directions). The effects
of the zigzag sampling pattern can be minimized by using
small patches from each microlens image. To most efficiently
use the captured data we therefore also require that the microimages themselves be small.

There is a limiting factor in how small the microimages can
be, however—namely, the edge artifacts in each microimage.
The edge pixels in a microimage are not completely illuminated and, consequently, are dark and noisy. With an N × N
square aperture, the ratio of boundary pixels to total pixels
is (4N − 4)/N 2 or approximately 4/N . To minimize the effects of boundary pixels, we need the microimage to be fairly
large—at least 10 × 10 but preferably even larger.
Practical properties of current technology also impose
limits on how small we can make the microimages. When

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1962

T. Georgiev & A. Lumsdaine / Reducing Plenoptic Camera Artifacts
B min
micro
lens

b

A min

∞

F

a

main
lens

Δa

Figure 11: Depth of field,

a, in a plenoptic camera 2.0. Microlens minification is M = a/b.

constructing a plenoptic camera, we do not want to remove
the cover glass protecting the sensor. The distance from the
cover glass to the sensor thus establishes a minimum distance
b0 from the microlenses to the sensor. Since the F-number
of the microlenses needs to match the F-number (F# ) of the
main lens [Ng06, GIBL08, GL09], the size of the microimages must be greater than b0 /F# .
In addition to microimage size, the second and most important factor influencing artifacts is variance in minification.
A scene would be rendered without any artifacts at all if constant depth were present in the scene—leading to constant
minification to focus the microlenses at that depth (assuming, of course, that the patch size is correctly chosen for that
minification value). As we have discussed, artifacts arise in
the full-resolution rendering approach because real scenes
are not at constant depths and a mismatch exists between
the ideal patch size for the minification at a given microlens
and the patch size that we are using throughout the rendered
image.
To minimize artifacts with hardware, we seek to minimize
this mismatch by appropriate optics design in the camera. In
particular, we seek to minimize the variance in minification
across all microlens images comprising the scene. Equivalently, as we will see, we seek to maximize the depth of
field for the microimages. With reference to Figure 11, the
minification is given by
M = a/b,

...

(1)

fact that b is the same for all microlenses, we derive
(see Figure 11).

a

a

To minimize artifacts, we would like the range of a to
map to as large a range of depths in the scene as possible.
In particular, as shown in Figure 11, we want the range from
some nearest plane Amin to infinity to be imaged into the a
interval.
To derive conditions on a for minimizing artifacts, we use
Newton’s form of the lens equation for the main camera lens,
(F − Amin )(F − Bmin ) = F 2 ,

(2)

where Amin is the distance from the main lens to the outside object, Bmin is the distance from the main lens to the
image, and F is the focal length of the main camera lens.
Our condition that Amin to infinity be imaged to the a
interval immediately implies that (F − Bmin ) = a. From
equation (2), it follows that
a=
or, since Amin

F2
,
F − Amin

(3)

F approximately,
| a| =

F2
,
Amin

(4)

Therefore, our condition on a for minimizing artifacts is
the following:
F2
.
Amin

where a is the distance from the microlens to the image (the
depth) and b is the distance from the microlens to the sensor.
The lens equation specifies a value of a for a plane that will
be exactly in focus for a given value of b. However, due to
the small microlens apertures and finite sensor pixel size, for
a fixed value of b, there is a range of values for a over which
the microimage will be in focus. We denote this range a
(see Figure 11).

4.2. Case study

Because of different scene depths, different areas of the
captured radiance are imaged at different minifications. Our
condition that minification must not differ much for different
microimages can be expressed as M/M
1, where M
is the difference in minification between two microimages.
Using the definition of minification (1), and considering the

For the results presented in Section 5, we are using a mediumformat camera with main lens F = 80 mm. Based on the
conditions above (Eq. 5), we want to image objects as close
to the main lens as A = 1000 mm. With this limit, we obtain
a
6.4 mm. However, we are not free to increase a too
much, because with fixed b, a large value of a produces a large

a

(5)

In other words, artifacts are reduced greatly if we position
the main lens image as far from the microlenses as possible.
2
The scale at which we are working is defined by AFmin .

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

T. Georgiev & A. Lumsdaine / Reducing Plenoptic Camera Artifacts

M. As observed in [LG08, LG09], a large value of M is related
to a large number of almost identical images in different
microlenses. In our experience, rendering in such a mode is
rather wasteful. We do not achieve good resolution and also
use considerable space on the sensor. One way to improve
the image in such a case would be to use super resolution
techniques. However, in this paper we will concentrate on
methods that make the number of images small and direct
imaging more efficient by design in its use of sensor real
estate.
Consider now how we might reduce the value of M. It
has been shown [GL09] that the image-side depth of field
for a microlens image is pF# , where p is the size of a pixel
at the sensor and F# is the image-side F-number for the
microlens. The corresponding object-side depth of field, x,
for the microlens would be
x = M 2 pF# .

(6)

The image on the sensor is sharp only if the object is within
the depth of field. In other words, considering (4), we find
x = M 2 pF# >

F2
.
A

(7)

Solving for M 2 , we obtain
M2 >

F2
.
ApF#

(8)

If we plug in our numbers (F = 80 mm, A = 1 m, p =
6.8μ), we get M 2 > 941/F# . We can choose M 2 = 1000/F# .
Clearly, there is only one way to make M smaller: by choosing a large F# . The limitation here is diffraction in wave
optics. When we consider the size of our pixels, the largest
possible F-number is around 10. Based on that, we have
selected microlenses with F# = 7.5. This gives us M =
11.5.
To avoid problems associated with removing the cover
glass, we have chosen microlenses with a focal length of
f = 1.5 mm. Using an extension tube, we have moved the
main lens image forward in the camera body. The result is
approximately a = 13 mm. This corresponds to M = 8.6.
The result of this slightly smaller M is that some objects will
be outside the depth of field and will appear slightly blurry.
At the same time, however, this is the most efficient imaging
in terms of using sensor space.
As mentioned above, the reason for selecting such a long
focal length, f = 1.5 mm, is so that the microlens array can
be placed directly on the cover glass of the sensor. This is
a non-destructive practical approach to building a lightfield
camera. It is possible only with Plenoptic Camera 2.0 because
in order to match the F-number of the main lens (around
4) at this focal length, we need a large spacing between
microlenses of around 0.5 mm. Considering that even the

1963

large medium-format sensor is less than 50 mm across, we
can fit no more than 100 microlenses in each direction. In
a traditional plenoptic camera, this would produce a final
rendered image of 100 × 100 pixels, which would not be a
useful image size. However, using the Plenoptic 2.0 approach
and full resolution rendering, we are able to render lightfield
images at more than 1000 × 1000 pixels—an image size that
approaches (and in some cases may fully meet) expectations
for modern digital photography.

4.3. Reducing the artifacts in software
Blocky artifacts occur due to the different minification in
different microlens images of the flat light field. To render a
complete image without artifacts, we must render each portion of the image according to that part of the flat’s required
minification by using the patch corresponding to the locally
required minification.
In rendering, neighbouring patches overlap. The condition for proper rendering is straightforward—overlapping
regions must match up. That is the same portion of the
scene that is captured by different microlenses must be rendered to the same position in the output image. This matching condition suggests the following two-pass algorithm for
rendering.
1. For each microlens, determine the minification and corresponding patch size that results in the best match with
all of its neighbours.
2. Render final image with the saved patch value for each
microlens.
Determining the minification that provides the best match
between two microlens images is essentially an image registration problem. We can exploit several aspects of our system
to streamline this process. First, the microimages in the captured radiance are very precisely aligned, due to the precise
manufacturing of the microlens array. Thus, the difference
between neighbouring microimages along the horizontal and
vertical axes of the lens array will only be horizontal and
vertical translations, respectively. Moreover, based on the
previously discussed optics design of the camera, there are
bounds on how large the shift between microlens images
can be. These characteristics of the captured radiance greatly
simplify the depth estimation algorithm. The pseudocode for
the depth estimation algorithm, along with a graphical depiction of it, are given in Figure 12.
Our depth estimation algorithm produces an array of patch
size values that we can subsequently use in rendering the
final image. To render the final image, we modify the full
resolution rendering algorithm in the following way. In the
basic plenoptic 2.0 algorithm we render a final image by
visiting each microlens image and extracting a sub-image

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1964

T. Georgiev & A. Lumsdaine / Reducing Plenoptic Camera Artifacts

Depth Estimation Algorithm.
1. For each N × N microlens image in the flat
a. Select a m × m window from the center of that microlens
image.
b. For k = −N +
2 to k = N −
i. Compute the cross correlation between the m ×m window and a corresponding window centered at kx in the
neighboring microlens image along the x axis.
ii. Record the value of kx with the best correlation.
iii. Compute the cross correlation between the m ×m window and a corresponding window centered at ky in the
neighboring microlens image along the y axis.
iv. Record the value of ky with the best correlation.

Compute
Cross Correlation

N
N

m

m
kx

Compute
Cross Correlation

ky

c. Record value of k equal to average of kx on left and right
boundaries and ky on top and bottom boundaries
2. Return the array of recorded values of k.
Figure 12: The depth-estimating algorithm computes cross correlations of the center portion of a microlens image with
similarly-sized patches in the neighbouring microlens image to the east and to the south. Because of the geometry of the
microlens array, offsets will only be in one dimension along each axis. The best correlation value in each direction are saved in
kx and ky . Note that the algorithm avoids cross-correlation at the image boundary.

Figure 13: Image rendered using depth correction. (a) Rendered image. (b) Estimated depth. Lighter regions correspond to
foreground (and larger patch size values).

(a patch) of a fixed patch size. In the modified algorithm,
rather than using a fixed patch value, we look up the value
for the given microlens from the patch size array.

5. Experimental results

By extracting depth information as described above and
then rendering the image with different magnification at each
microlens, we produce the image in Figure 13. Note that
regions of the image at all depths are rendered essentially
artifact-free.

The camera used for the experiments reported here is medium
format, with an 80-mm lens and a 39-megapixel digital
back from Phase One. The pixel size is 6.8 μm. The lens
is mounted on the camera with a 13-mm extension tube
to provide the spacing needed to establish the appropriate

5.1. Camera

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

T. Georgiev & A. Lumsdaine / Reducing Plenoptic Camera Artifacts

1965

Figure 14: Basic rendering applied to our fountain light field captured with artifact-reduction optics. Artifacts are significantly
reduced compared to 10, most notably when patch size is chosen for background rendering. Note that the foreground in
Figure 14(b) appears blurry due to the small patch size used in the image which required subsequent magnification to present
as the same size as Figure 14(a). (a) Patch size chosen for rendering the foreground. (b) Patch size chosen for rendering the
background.
distance from the focal plane to the microlens array and the
sensor. (The rationale for the extension as a way of reducing
artifacts was described in Section 4.1.)
The microlens array is custom-made by the Axetris Division of Leister Process Technologies. Microlenses are etched
into a fused silica wafer of thickness 1 mm. We designed the
microlens array to work with the sensor without removing
the cover glass. For that purpose, microlenses have a focal
length of 1.5 mm and are spaced 0.5 mm apart. This way,
the array matches a main-lens F-number as low as F /3. To
achieve more efficient use of sensor space, we modified the
circular F /2 aperture of our main camera lens to a square
aperture of approximately F /3, considering the extension
tube. We have adjusted the aperture to achieve a reasonably
tight fit of the aperture squares on the sensor.
Our microlens apertures are custom designed. They are
made by depositing a chromium mask on the lens-side surface of the microlens wafer, leaving only the central area
of each microlens uncovered. Both microlens positions and
apertures have micron precision. Apertures are circular with
diameter 240 μm. As a result, the microlenses work at around
F /6.25 to F /7.5 depending on their exact positioning. Note
that the F-number matching condition requires the main-lens
F-number to match the ratio of b (distance from microlens
to sensor) to the distance between microlens centers. It is
not a condition of matching the microlens F-number, which
remains a free parameter to optimize. We use this parameter to achieve a good trade-off between speed and depth of
field.

The microlens array is cut to a rectangle the size of the
infrared (IR) filter and is positioned in place of the IR filter,
with some appropriate (sub-mm) spacing above the cover
glass to precisely adjust the value of b. In our experience,
adjusting this spacing proved to be extremely important in
fine tuning the microlens focusing and producing sharp micro
images. The IR filter was removed and placed in front of the
main camera lens. An alternative would be to put the IR filter
inside the extension tube, which would require cutting and
reshaping it so that it fits the circle of the extension tube.
5.2. Images
A section of our raw image directly from the sensor is shown
in Figure 8(a). Note the sharp in-focus image in each microimage. The quality of those images facilitates rendering of sharp final views of the scene. The full-resolution
rendering method produces focused images like those in
Figures 10(a), (b), 14(a) and 16.
Artifacts are clearly visible in Figures 10(a) and (b). While
one constant patch size represents a certain depth well, it
is not appropriate for other depths. If distances or camera parameters are not within the design range described in
Section 4.1, artifacts will result.
If all optical and scene parameters are in the appropriate
range, artifacts, even if present, can be reduced as much as we
want by positioning the image far from the microlenses. It is
possible to design a camera completely free of artifacts, but
the size of the final image or the parallax will be very poor.

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1966

T. Georgiev & A. Lumsdaine / Reducing Plenoptic Camera Artifacts

Nevertheless, rendering of our final image (Figure 16)
using this coarse depth map produces images significantly
better than constant-depth rendering (Figure 14(a)). The
same depth map is used to generate both of the stereo views
in Figure 16.

Figure 15: Depth map extracted from our fountain light field
with our depth-estimation algorithm. Depth is extracted on
a per-microlens basis. Lighter regions correspond to foreground (and larger patch size values).
Instead, we have chosen a trade-off, combining reasonably
good parallax while still admitting some artifacts, as seen in
Figure 14(a).
Our final modification is to make the depth (or patch
size) parameter variable throughout the image. This parameter is computed for each microlens according to the algorithm in Section 4.3. The computed depth map is shown in
Figure 15. Notice that it is a per-lens depth that does not
account for depth variations within a single microimage.

We produce high resolution views of the photographed
scene, frozen in time. To convey some feeling of the 3D
nature of our lightfield results in the printed version of the
paper, we have generated left and right views, and printed
the two images side by side, switched. Readers who can use
the crossed-eyes viewing technique can directly experience
stereo from the image. Readers are also welcome to view
the parallax movie included with the supplemental materials
of this paper. It is generated from the same lightfield data
and conveys the same sense of 3D as the stereo views. (Note
that the lightfield approach is much more than stereo 3D
because it represents any synthetic position of the camera,
while stereo is based only on two positions.)
In our experience with more than 100 photographs, our
straightforward (and coarse) methodology for creating depth
maps largely eliminates artifacts. For use in full resolution
rendering, precise depth information is not always required
because those locations where the depth map is not precise
are exactly those locations where such a depth map is not
needed for artifact reduction. Simply no artifacts and no texture detail exist in such areas. When an area needs artifact
reduction, it has significant detail, and then depth can be reliably extracted with our algorithm. Note also that the depth
map is not needed for stereo 3D representation, but only for
artifact correction. Even without any depth map the stereo
effect is very good—microlens images are naturally stereo
views.

Figure 16: Rendering of a stereo pair from our fountain light field. Left and right images have been switched for crossed-eyes
viewing. The depth map has been used for artifact reduction only, not for computing the stereo effect, which comes directly from
our lightfield rendering algorithm.
c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

T. Georgiev & A. Lumsdaine / Reducing Plenoptic Camera Artifacts

6. Conclusions
In this paper we have analysed a new version of the plenoptic
camera and we have analysed the cause of artifacts generated with the basic ‘full-resolution’ rendering algorithm. An
understanding of the reasons for those artifacts leads to methods for artifact reduction. We have shown two approaches,
each of which can reduce and, when combined, almost eliminate those artifacts. The first approach involves positioning
the main lens image far from the microlens array so that
different depths in the world lead to little change in minification in the microimages. The second approach consists
of assuming variable depth (variable patch size) throughout
the microlenses. This depth parameter is computed and then
used for artifact reduction during rendering.
In isolation, each approach provides good artifact reduction. Because the combined approach is so effective at artifact
elimination, it allows us the additional flexibility of being
able to experiment with tradeoffs between parallax and artifacts. In our observations, such a camera and associated
rendering process is a practical method for 3D and refocused
imaging. The final rendered images are of high quality with a
resolution between 1 and 2 megapixels. As sensor technologies advance, and as the focused plenoptic technologies are
refined, there should be no obstacles for this technology to
become the method of choice in 3D imaging.
We almost never observe depth-based artifacts in our final
images rendered with the combined hardware and software
improvements. Nevertheless, our current results can be further improved in several areas, which are subjects of on-going
work. Particular improvements include: (1) better depth map
calculation, (2) better microlenses fabrication, and (3) greater
use of the data. First, the algorithm certainly has room for improvement in accuracy and consistency among microlenses.
Graph cut or other methods could be applied. Second, while
the bulk of the microlenses have very tight tolerances in terms
of focal length, positioning, and aperture, the microlens array still contains defects. About 1% of the microlenses have
the wrong profile or focal length (some are even totally flat)
and fall far outside of the expected tolerances. Also, because of contamination, some of the apertures are partially
covered with dust particles, resulting in incorrect matching
and poorly computed depths. Finally, as we can see from
Figure 8(b), most of our captured data remains unused. While
we are 100 times better than traditional rendering (which produces 1 pixel per microlens), we are still using only a small
fraction of the captured image data. If we enlarge the rectangles that are used from each microlens image and perform
appropriate blending between overlapping microlens images,
much better results can be achieved in terms of smoothness
and noise reduction.
The images rendered with the approach described in this
paper are ‘all in-focus.’ We are essentially rendering with a
single view (small aperture) for the entire scene, using the
correct patch size for varying depths in the scene. Although

1967

the focused plenoptic camera samples in the angular direction
more sparsely than does the traditional plenoptic camera, the
focused plenoptic camera nevertheless does capture multiple
views of the entire scene. Using these multiple viewpoints
to produce refocusing and depth-of-field rendering effects is
the subject of current work and the topic of a forthcoming
paper.
Acknowledgments
The authors acknowledge the support of Adobe Systems, Inc.
for this work. The authors are grateful to Georgi Chunev for
his assistance in preparing images for this paper. The authors
would like to thank the anonymous reviewers of this paper
for their valuable comments.
References
[AB91] ADELSON E., BERGEN J.: The plenoptic function and
the elements of early vision. Computational Models of
Visual Processing (Jan. 1991), 3–20.
[AW92] ADELSON E. H., WANG J. Y. A.: Single lens stereo
with a plenoptic camera. IEEE Transactions on Pattern
Analysis and Machine Intelligence (Feb. 1992), 99–106.
[BBM*01] BUEHLER C., BOSSE M., MCMILLAN L., GORTLER
S., COHEN M.: Unstructured lumigraph rendering. In Proceedings of the 28th Annual Conference on Computer
Graphics and Interactive Techniques SIGGRAPH ’01
(New York, NY, USA, 2001), ACM Press, pp. 425–432.
[CCST00] CHAI J., TONG X., CHAN S., SHUM H.: Plenoptic sampling. In Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques
International Conference on Computer Graphics and Interactive Techniques (New York, NY, USA, 2000), ACM
Press/Addison-Wesley Publishing Co., pp. 307–318.
[FGW08] FIFE K., GAMAL A. E., WONG H.-S. P.: A 3Mpixel
multi-aperture image sensor with 0.7μm pixels in 0.11μm
CMOS. In IEEE ISSCC Digest of Technical Papers (Feb.
2008), pp. 48–49.
[GGSC96] GORTLER S. J., GRZESZCZUK R., SZELISKI R., COHEN
M. F.: The lumigraph. In Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive
Techniques SIGGRAPH ’96 (New York, NY, USA, 1996),
ACM Press, pp. 43–54.
[GIBL08] GEORGIEV T., INTWALA C., BABAKAN S.,
LUMSDAINE A.: Unified frequency domain analysis of lightfield cameras. In Proceedings of the 10th European Conference on Computer Vision: Part III (Marseille, France,
Oct. 12–18, 2008). D. Forsyth, P. Torr, and A. Zisserman
(Eds.). Lecture Notes in Computer Science, vol. 5304,
Springer-Verlag, Berlin, Heidelberg, Germany (2008), pp.
224–237.

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1968

T. Georgiev & A. Lumsdaine / Reducing Plenoptic Camera Artifacts

[GL09] GEORGIEV T., LUMSDAINE A.: Depth of field in
plenoptic cameras. In Eurographics 2009—Annex. (April
2009), pp. 5–8.

[Ng06] NG R.: Digital light field photography. PhD thesis,
Stanford University, Stanford, CA, USA, 2006. AdviserPatrick Hanrahan.

[LG08] LUMSDAINE A., GEORGIEV T.: Full Resolution Lightfield Rendering. Tech. rep., Adobe Systems, January 2008.

[NLB*05] NG R., LEVOY M., BRDIF M., DUVAL G., HOROWITZ
M., HANRAHAN P.: Light field photography with a handheld plenoptic camera. Stanford University Computer Science Tech. Rep., 2005.

[LG09] LUMSDAINE A., GEORGIEV T.: The focused plenoptic
camera. In Proceedings of the International Conference on
Computational Photography (San Francisco, CA, USA,
2009).
[LH96] LEVOY M., HANRAHAN P.: Light field rendering. In
ACM Transactions on Graphics (1996), pp. 31–42.
´
r´eversibles. Photogra[Lip08] LIPPMANN G.: Epreuves
phies integrales. Acad´emie des sciences (March 1908),
446–451.
[Ng05] NG R.: Fourier slice photography. In ACM SIGGRAPH 2005 Papers (Los Angeles, California, July
31–August 04, 2005), M. Gross (Ed.). SIGGRAPH ’05
(New York, NY, USA, 2005), ACM Press, pp. 735–744.

Supporting Information
Additional Supporting Information may be found in the online version of this article.
Fountain: Captured lightfield for fountain examples
Perspective: Animated gif showing changing perspective
Please note: Wiley-Blackwell is not responsible for the content or functionality of any supporting materials supplied by
the authors. Any queries (other than missing material) should
be directed to the corresponding author for the article.

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

