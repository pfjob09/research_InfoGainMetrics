DOI: 10.1111/j.1467-8659.2010.01801.x
Pacific Graphics 2010
P. Alliez, K. Bala, and K. Zhou
(Guest Editors)

Volume 29 (2010), Number 7

Image Synthesis for Branching Structures
Dominik Sibbing

Darko Pavi´c

Leif Kobbelt

RWTH Aachen University

Abstract
We present a set of techniques for the synthesis of artificial images that depict branching structures like rivers,
cracks, lightning, mountain ranges, or blood vessels. The central idea is to build a statistical model that captures
the characteristic bending and branching structure from example images. Then a new skeleton structure is synthesized and the final output image is composed from image fragments of the original input images. The synthesis part
of our algorithm runs mostly automatic but it optionally allows the user to control the process in order to achieve a
specific result. The combination of the statistical bending and branching model with sophisticated fragment-based
image synthesis corresponds to a multi-resolution decomposition of the underlying branching structure into the
low frequency behavior (captured by the statistical model) and the high frequency detail (captured by the image
detail in the fragments). This approach allows for the synthesis of realistic branching structures, while at the same
time preserving important textural details from the original image.
Categories and Subject Descriptors (according to ACM CCS):
Generation—Line and curve generation

1. Introduction
The generation of images is one of the major activities of
human creativity. However, with the existence of large image collections on the internet the necessity to generate new
images and illustrations from scratch becomes less and less
important since for most objects and scenes some image material is already available. Hence, techniques are coming into
focus that support image synthesis by decomposing and recombining existing material. Such techniques find more and
more applicability in movies or computer games where the
repetition of textures appears unnatural.
In this paper we are presenting an approach to generate
images that contain branching structures like rivers, cracks,
lightnings, mountain ranges, or blood vessels. Our algorithm
is able to geometrically synthesize similar structures as seen
in the input image. This can be done automatically by the
system (using parameters learned form the input image) but
it also provides some control mechanisms to the user to
guide the structure synthesis. Hence the automatic generation of multiple similar images, as well as the creation of individual designs guided by a person, becomes possible. Our
algorithms for image analysis and synthesis are integrated
into a three stage interactive workflow that provides various
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

I.3.3 [Computer Graphics]: Picture/Image

semi-automatic selection tools in order to effectively control
the process.
In the first step an input image is chosen. From this image, the user extracts the branching structures (Sect. 3). The
structure is analyzed in the second step in order to derive a
statistical model which captures the characteristic style of
the branching structure. This model is applied to synthesize new abstract branching structures, i.e. polygonal skeletons with associated thickness coefficients (Sect. 4). Technically our model describes a Markov process where new
extending segments are selected by matching sub-sequences
of segments in the input structure. The selection probabilities
are adapted such that locally similar segments and segments
which satisfy some global characteristics are more likely
to be chosen (Fig. 1). In the final step the output image is
generated by a fragment-based image completion technique
which first copies matching fragments from the input image to cover the region where the branching structure passes
through and then fills the regions in between the branches
(Sect. 5). In order to produce high quality images we employ
Graph Cuts to optimize seams between fragments, Poisson
image editing to reduce the gradient across the seams, and
image warping techniques to avoid discontinuities along the
polygonal skeletons.

2136

Dominik Sibbing & Darko Pavic & Leif Kobbelt / Image Synthesis for Branching Structures

(a)
(b)
(c)
(d)
Figure 1: Based on the input image and its structure information (a) our system learns a statistical structure model from the
given example. It is then able to synthesize images with visually similar looking branching structures (d). The overall process is
divided into the geometric structure reconstruction (b) and the fragment-based image synthesis (c).
2. Related Work
Curve synthesis One of the most prominent methods to
generate tree-like structures is to use L-Systems which were
introduced by Lindenmayer [Lin68]. In L-Systems a grammar is defined and used to recursively replace modules under
certain conditions. These systems are very suitable for tree
structures and were, e.g., used to synthesize plants [PH89].
A stochastic model can be added to those systems but the
underlying rules of an L-System are designed only for a certain application (e.g., generating plants or trees). Palubicki
et al. [PHL∗ 09] use an L-System to simulate the growth of
self-organizing trees. The creation of a tree is controlled by
a set of well defined rules and includes a guidance model
which directs the plant away from shadowed regions. In contrast our method does not rely on a predefined set of rules. It
rather deduces the necessary parameters to recover a specific
structure from an example.
Hertzmann et al. [HOCS02] proposed an elegant method
to map a transformation given for a source curve to a target curve. For this, an initial target curve is required as input
which is then used as a guidance for the synthesis process.
This concept of Curve Analogies was e.g. used by Wu et
al. [WZSB08] to edit trajectories in motion data. Our model,
together with the space colonization, is able to synthesize arbitrary curves (of varying thickness) automatically without
any given target curve, while optionally the user is still allowed to include some degree of guidance by adapting the
underlying density field as described in Sect. 4.5. Since we
uniformly resample the curves it is possible to define a genuinely scale-free similarity measure between curves by just
comparing angles instead of evaluating a positional distance
function including least-squares-optimized translation and
rotation parameters for the source structure to align with the
target structure [HOCS02]. In addition our model includes a
branching model which allows for a larger variety of synthesized structures and hence is quite different.
Kim and Lin [KL04] present a specialized framework
based on a simplified Helmholz equation to compute the
spread of electric charge along a 3D grid. Using a special renderer based on the Atmospheric Point Spread Function they produce impressive images of lightnings. Chen et

al. [CEW∗ 08] proposed a specialized framework for urban
modeling. The user defines a set of directions from which an
underlying tensor field is computed which guides the generation of a street graph. Aliaga et al. [AVB08] proposed an interactive method to generate images of urban layouts. From
exemplary images they extract a street graph and synthesize
a new city showing similar street characteristics. In this setting the streets work as separators between image patches
such that seams between patches are never an issue. Similar to [CEW∗ 08] these systems focus on a very specialized
purpose of rendering urban layouts or physical phenomena,
while our method focuses on tree-like structures in general.
Tan et al. [TZW∗ 07] present a semi-automatic method to
generate botanic trees from images. They reconstruct visible
3D branches from 3D points obtained from structure from
motion, extend them assuming branches to be self-similar
and add leaves to produce 3D models of botanic trees.
Image Synthesis In general texture synthesis aims at
creating arbitrary large textures from small input examples.
Efros and Leung [EL99] use a Markov random field to compute new colors for single pixels by comparing a small
already synthesized local neighborhood with parts of the
source image. Ashikhmin [Ash01] took a user defined image
to guide the synthesis process and exploit the coherence of
neighboring source pixels. In [HJO∗ 01] a labeling of image
regions is performed by the user. From another user-defined
image, showing the same labels, a similar image can be synthesized by filling regions with pixels of corresponding labels. The resulting images are convincing, but compared to
our system which is also able to produce variants of the underlying structure via a statistical model, this method is less
flexible, since the user has to provide the afore mentioned
labeled regions.
In order to fill large regions one can also employ patch
based methods which copy whole fragments instead of pixels of a source image. Brooks and Dodgson [BD02] proposed an approach to generate self-similar textures, which
works well for unstructured textures like a field of flowers, unstructured rifts, sand or stones. Fragment based methods often have problems blending to adjacent fragments
in order to avoid seams. Kwatra et al. [KSE∗ 03] introduced a nice idea to adapt the path of a seam such that
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Dominik Sibbing & Darko Pavic & Leif Kobbelt / Image Synthesis for Branching Structures

differences in color values of neighboring pixels across the
seam are minimized. In our image synthesis we also employ
their method of Graph Cuts to produce visually pleasing results. Also very sophisticated methods to generate large (infinite) textures from compact input samples are proposed
in [LH05, LH06, KEBK05], where repetition artifacts and
seams are nearly invisible. All pixel and fragment based algorithms require rather unstructured input images. Since we
are able to synthesize the geometric content of an image by
generating a branching curve skeleton, we are focusing on
the synthesis of a completely different category of images.
Zhou et al. [ZSTR07] proposed a method to generate height
maps for terrain rendering from exemplary height fields and
a user-defined sketch. They detect significant features in the
height field and transfer surrounding image content to similar features of the sketch. Combining these specialized textures, which are height fields, with a well designed renderer
they produce impressive images of terrains. Compared to
our work this method needs a user-defined sketch in order
to identify corresponding features for the synthesis of the
underlying geometry. Although we allow user guidance, we
are also able to synthesize new (more arbitrary) structures
in a completely different, learning based, approach. Rosenberger et al. [RCOL09] compute different layers of a texture
in order to create a control map for the texture synthesis.
They focus on computing 2D layers whose boundaries show
similarity to boundaries of the input layers. These layers are
used as a control map to actually create a final texture. In our
work we do not consider inhomogeneous textures showing
layered content (like peeling paint on bare metal with areas
of rust) for which their algorithm is well suited. Our focus
is on branching structures and as such it is a very different
problem instance.For a more thorough survey and detailed
explanations of example based texture synthesis we would
like to refer the interested reader to [WLKT09, KW07].
Fragment based image completion techniques [TCLT07,
HE07] fill missing regions by copying whole fragments
from a source to a target image position. The original approach [DCOY03] creates impressive results, but is rather
slow and also has problems to correctly synthesize structures shown in the image. In [SYJS05] the authors consider
structures to be user defined. They distinguish between the
structure and texture in the images and apply the completion
task in two steps. First, they use belief propagation to propagate image structures along the user defined paths. Then,
for the remaining missing parts a fragment-based completion approach is applied [TCPT03]. We use similar ideas, but
in our method we can synthesize structures via a statistical
model. In contrast to [SYJS05] we apply a greedy variant of
structure propagation, which is more suited for the synthesis of tree-like structures, which starts with an empty image.
For the remaining missing parts in the image we apply the
flexible interactive image completion approach of [PSK06],
but this is surely exchangeable by some standard technique
reproducing rather homogeneous image content [WLKT09].
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2137

Contributions
• Our similarity measurement between curves compares
angles and thickness and is a scale-free measure that
does not include computationally involved least-squaresoptimization of translation and rotation parameters.
• We achieve local as well as global control over the synthesis procedure by modeling local similarity as a Markov
process and computing global statistics based on histograms.
• Our branching model is automatically learned for different structures via support vector machines.
• We incorporate advanced image completion techniques
into our synthesis: Graph Cuts for seam optimization,
Poisson image editing to adapt color values across seams,
and warping techniques to avoid discontinuities along
structures.
• Our system supports optional user interaction during the
synthesis, which makes the system a powerful sketch tool
for the recombination of existing image material.
3. Generating Input Structures
When describing the characteristic style of a curve-like feature in an image we have to distinguish three different levels
of detail (scales). On the coarsest scale the univariate feature follows some global path across the image. On the next
finer scale we can identify a local bending behavior which is
characteristic for a given phenomenon. For example a lightning has a more jaggy appearance than a river. On the finest
scale we can identify small detail on texture structures, e.g.,
houses on the river banks.
The goal of this paper is to synthesize new images with
branching structures from examples by separating these
three levels of detail. For the newly generated images we
define a new shape on the coarsest scale only, i.e. we define
new paths which should be followed by the branching structure. The characteristic bending and furcation as well as the
fine detail from the input, however, should be preserved as
faithfully as possible.
In order to achieve this we use a statistical geometric
structure model for the mid-scale shape which controls the
characteristic bending behavior. The finest level detail is preserved by composing the final output from texture fragments
of the input image.
The decision which geometric feature magnitude (kilometer, meters, centimeters) belong to which of the three levels
of detail depends very much on the image content and on the
specific application. Hence it is not a reasonable approach to
try to perform this decomposition fully automatic. We rather
let the user perform this decomposition by manually producing the example data which then serves as the input to the
structure analysis and synthesis stages. This manual process
includes the generation of a polygonal skeleton [x0 , . . . , xn ]
for each branch of the structure plus adding a branch thickness ri to the vertices xi of the skeleton.

2138

Dominik Sibbing & Darko Pavic & Leif Kobbelt / Image Synthesis for Branching Structures

Our interactive system conveniently supports this manual
process. After loading an input image, the user simply selects points xi along the branches of the structure. In the
example of Fig. 1 we selected 65 points which are further
refined (Fig. 1a) by the system (Sect. 4.1). Here the precision requirements are not very high. A simple maximum
gradient detection operator in the direction perpendicular to
the branch orientation provides some initial estimate for the
local branch thickness which can be adjusted by the user.
After a few minutes of interaction, the branching structure
is captured and can be passed to the subsequent, automatic
processing stages.

growing a branch by adding the next vector of m segments (αn+1,m , rn+1,m ) = (αn+1 , rn+1 ), . . . , (αn+m , rn+m )
we consider only the finite history of the k + 1
previous angles and thicknesses (αn−k,k+1 , rn−k,k+1 ) =
(αn−k , rn−k ), . . . , (αn , rn ), see Fig. 2. By putting both, the
angle and the thickness, into one model we can reflect the
observation that fine branches often bend differently from
the thicker main branches.
(αn−k , rn−k )

(αn+m , rn+m )

(αn , rn )

(αn−1 , rn−1 )

(αn+1 , rn+1 )

4. Statistical Structure Synthesis
The look and style of a branching structure is determined by
a mathematical model for the characteristic bending behavior as well as for the number and distribution of furcations.
In order to achieve maximum flexibility, we handle both in
separate statistical models that are specifically designed to
support the extension of existing branches (growing) and
to decide where new branches should start. The models are
learned from examples, i.e. we expect a polygonal branching
structure as input and extract the models from it. In the synthesis phase we can then use these models to generate new
branching structures showing the same characteristic behavior as the input.
The synthesis starts by generating one non-furcated main
branch, where one, e.g., can initialize the main stem by copying the first elements from the input structure. Then a set
of furcation points is determined and at each such point, a
new branch is grown. This process is recursively repeated
until a stopping criterion is meet, e.g., maximum levels of
branch generations, maximum total branch length, or maximum spatial density of branches (see Sect. 4.4).
4.1. Input Structure
A single branch is given by a polygonal skeleton with vertices [x0 , . . . , xn ]. For each vertex xi , we add the local thickness ri as an attribute. In order to obtain an orientation independent representation, we use the angles αi between successive edges of the polygonal skeleton as an additional attribute. For the sake of simplicity we assume that the polygonal skeleton has been uniformly resampled such that the
edge lengths xi+1 − xi are constant. This implies that the
angles αi and thicknesses ri together with the constant edge
length h uniquely and completely define the geometry of a
branch up to rigid transformations.
The different (sub-)branches of the input structure are
stored as separate edge sequences with the additional information from which vertex in the parent they branch off.
4.2. Bending Model
Markov Model. We model the characteristic bending
of a branch by a Markov process [Nor98], i.e. when

Figure 2: Illustration of the model parameters. The previous
neighborhood depends on the parameter k. The nodes in this
neighborhood are colored in blue while the next m nodes,
which will be appended, are displayed in green.
More concretely, we take the angle and thickness history
(αn−k,k+1 , rn−k,k+1 ) and find the best matching c candidates
among all sub-sequences Ci = (αˆ i,k+1 , rˆ i,k+1 ) in the input
structure. Here, the quality Qi of a candidate Ci is measured
scale-invariantly by the distance
Qi = dist(αn−k,k+1 , αˆ i,k+1 ) · dist(rn−k,k+1 , rˆ i,k+1 )
where we define
dist(X, Y) =

1
+1
X−Y +1

such that the quality score always remains > 1 and the singularity for X = Y is avoided. Then we randomly select one
of the c candidates with highest quality with a probability
proportional to their quality scores and the m elements following it are appended to the output branch. The parameters
that control the behavior of this model are m, k and c. The
parameter m influences the size of the extension that is added
in each step and hence the size of what is considered a “typical” feature. If the target feature size is s and the uniform
resampling step width is h we set m = s/h. The parameter k
defines the order of the Markov process and is set to k = 3m
in all of our experiments, because it consistently led to most
plausible results. The number of candidates c finally controls
the variability of the output. For c = 1 we deterministically
extend the branch with the best matching sub-sequence. By
default, we set this parameter to c = 2 in our experiments.
Fig. 3 illustrates the effect of these parameters in a simple
example setting.
Histogram matching. In order to additionally reproduce
some global characteristics of the structures we employ histogram matching similar to [KFCO∗ 07]. We adapt the quality scores Qi such that histograms of angles and thickness of
the synthesized match the input structure. For the input structure we calculate normalized histograms Hˆ α and Hˆ r , which
reflect its angle and thickness distribution, at the beginning
of the synthesis process. Analogously the histograms of the
target structure before extending are defined as Hα and Hr ,
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Dominik Sibbing & Darko Pavic & Leif Kobbelt / Image Synthesis for Branching Structures

should look like. Both decisions should, again, depend on the
local shape of the branch such that the characteristic structure of the input is preserved.

(a)
(b)

(c)

(d)

(e)

(f)

Figure 3: In this figure we demonstrate the different effects
of choosing the size of the neighborhood k, the size of the
extension m and the variability c. (a) source structure. (b) k
and m are very high but we allowed more variability (c = 2).
The result is a slight deviation from the source structure. In
(c) and (d) we omitted the variability (c = 1) and chose a
large look-ahead (m = 15). In (c) we only considered a small
value for k = 2 which results in a repeating pattern of the
same structure. In (d) we chose k to be large which results
in reproducing the source structure exactly. In (e) and (f)
we set the size of the extension to be small ( m = 2 ) and
allowed some variability (c = 2). When little is known about
the past (k = 2) the produced structure looks not at all like
the source structure (e). The effect of increasing the size of
the neighborhood k with small m is finally shown in (f).
whereas Hαi and Hri denote the prospective histograms after
adding a candidate Ci . In all our experiments each histogram
splits the interval between the minimal and maximal value
observed in the input structure into B = 16 bins.
To achieve global similarity we adapt the quality scores
according to
Q∗i = Qi · e ωG

2139

(∆α (i)+∆r (i))· nmt

where ∆x (i) = B1 Hˆ x − Hx 2 − B1 Hˆ x − Hxi 2 measures the
improvement of the histogram similarity, if the the candidate
Ci would be added. The factor nmt , nt being the number of
nodes of the structure synthesized so far, compensates for the
decreasing influence of the m new nodes on the histogram
as the number nt of existing nodes grows. Here ωG , which
we set to 50 by default, is a weight which can be used to
control the influence of the histogram matching. In this way
the score of a candidate is increased or decreased, depending
on the amount of improvement or impairment of the global
similarity.
4.3. Furcation Model
For the furcation model, we need to distinguish between the
decision where to place a furcation and how this furcation
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

To determine the orientation and bending behavior of
the child branch being attached at some vertex xi of the
parent branch, we can use a similar Markov process as
in Sect. 4.2 with the added feature that we can now even
use a non-causal neighborhood (αi−k,2k+1 , ri−k,2k+1 ) =
(αi−k , ri−k ), . . . , (αi+k , ri+k ) instead of just a causal one because the parent branch is completed before child branches
are added. Once the initial segment of the child branch is
synthesized, we switch to the causal model described in
Sect. 4.2 for further branch growing.
Since the input usually contains only a relatively small
number of furcations, it may lead to statistically biased results if the decision of where to place a furcation point is
also based on the same non-causal Markov process. This is
why we “interpolate” the furcation examples by computing a
linear classifier that maps sub-sequences of length 2k + 1 on
a parent branch to furcation probabilities. We use a support
vector machine (SVM) to find the optimal classifier [Bis07].
For each vertex xi on each branch of the input structure we take the non-causal neighborhood
(αi−k , ri−k ), . . . , (αi+k , ri+k ) ∈ R2k+2 and the index distances to the previous and next furcation points (or the endpoints of the branch), n− and n+ respectively. Notice that
due to the uniform resampling of the input structure, index
distances are equivalent to discretized arc-length distances.
These 4k + 4 dimensional samples are labeled with +1 if xi
is a furcation point and −1 if xi is no furcation point. The
SVM computation will produce the linear classifier L (i.e.,
the hyperplane in R4k+5 ) that fits the input with a maximum
margin. In our implementation we used the SVM code provided by [CL01].
For a given branch we now pick d vertices with maximum
furcation probability and start synthesizing child branches
from there. Hence d controls the furcation complexity of the
resulting branching structure and can be choosen randomly
with a probability reflecting the number of sub-branches observed at branches of the input structure or manually. The
coefficients n− and n+ effectively prevent furcation points
from clustering. If a minimum distance between furcation
points should be guaranteed then the furcation points have
to be picked sequentially with each furcation blocking further sub-branches in its immediate neighborhood.
4.4. Space Colonization
So far we have treated the growing process of each branch
independently. This, however, may lead to collisions and
crossings between branches which is considered unnatural
in many cases (e.g. rivers). This is why we use a technique
similar to [PHL∗ 09] in order to promote branch growing directions that conquer empty image regions and avoid growing into regions where other branches already exist.

2140

Dominik Sibbing & Darko Pavic & Leif Kobbelt / Image Synthesis for Branching Structures

We model the attractiveness of an image region by computing a density map which assigns to each point p a value
D(p) = exp(−d(p, B)2 /ρ2 )
with d(p, B) measuring the minimum Euclidean distance
between p and the set of previously generated branches B
(= polygonal skeletons with thickness). The standard deviation ρ is a parameter which controls how close the different
branches are allowed to approach each other. Everytime a
branch has grown by adding m new segments to it, the density map is updated. For efficiency reasons we restrict the
update to a local vicinity with radius 3ρ since outside of this
radius the change in density can be neglected.
The density map is included into the candidate selection process by computing for each extension candidate (cf.
Sect. 4.2) the average growing direction (e.g. by principal
component analysis) and then finding the maximum of the
density map along that direction and within a distance not
larger than three times the length of the extension. This density maximum Di ∈ [0, 1] is then used to adapt the relative
candidate selection probabilities by appropriately scaling the
quality scores
Q∗∗
= Q∗i /(Di + ε)
i
with some maximum bound of 1/ε = 103 on the scaling factor. An intersection of that extension with the structure can
robustly be detected by looking at the density function along
the extension, which then shows a decreasing, increasing and
again decreasing behavior. We explicitly set the quality score
to zero whenever we detect such intersections.

(a)

(b)

(c)

(d)

(e)

Figure 4: Guidance along a prescribed path. (a) Shows the
source image. In (b) we show the user defined density map,
which is used to guide the generation as shown in (c). The
missing regions of the synthesized river (d) is filled using
interactive image completion and the result is shown in (e).
The input to the image synthesis stage consist of the original input image, the user-defined input branching structure S
(source structure), and the automatically synthesized output
branching structure T (target structure).
We first compute a set of image fragments Fi covering
the source structure S and then compose the branches of T
from sequences of such fragments. In the selection of fragments we optimize for geometric similarity as well as for
color consistency, i.e. the skeleton and thickness profile of
the source branch segment covered by Fi has to match the
corresponding skeleton and thickness profile of the target
branch segment, and the pixel colors in the overlapping region of neighboring fragments in the target image have to
match as well.

4.5. Guided Synthesis
5.1. Fragment Definition
We can use another optional density map G(p) in order to
guide the synthesized branches along a prescribed path. For
this we simply define a map with G(p) = 1 everywhere except for a smooth valley along the prescribed path. Then we
scale the selection probabilities of the extension candidates
as in the previous section. With the width of the valley we
can control how precisely the branch follows the path (Fig.
4). Notice that if the valley is chosen too narrow, i.e. the
width being smaller than the amplitude of the characteristic
bending features, the result will look less realistic since with
stronger guidance we are effectively suppressing the midscale features of the bending model.
5. Fragment-Based Image Synthesis
In the next step we are generating the final output image
by re-combining fragments from the input image. Similarly
to [SYJS05] we exploit the synthesized branching structure
to first cover the branches by a sequence of overlapping fragments. This is done in the same order as the structure synthesis, i.e. we first cover the main branch and then add the
sub-branches at the furcation points. Finally the remaining
gaps between the branches are filled by an image completion technique like [PSK06].

A fragment Fi is a disk-shaped image region of the input
image, specified by its center position zi with the radius
si . Since the fragments are primarily used to synthesize the
branching structure, source fragments have their center zi
on the polygonal skeleton of a source branch. The radius
of a fragment is by default set to three times the (linearly
interpolated) thickness of the branch at zi . In order to guarantee sufficient overlap between neighboring fragments, we
set their distance zi+1 − zi to λ (si + si+1 ) with λ = 0.75 in
all our experiments. Additional fragments are placed at the
furcation points of S in order to be able to synthesize proper
branchings in T . On the target branch structure we define a
similar set of (yet empty) fragments G j with the same radius
and distance convention.
Each fragment Fi or G j is associated with a segment
of a polygonal skeleton which passes through the fragment center. We call this segment and the corresponding
thickness profile the shape of the fragment. In order to
simplify the shape comparison between fragments we resample these segments and thickness profiles to a fixed
number of k uniformly distributed samples that we collect in a 3k dimensional vector (p1 , . . . , pk , µ1 , . . . , µk ) or
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Dominik Sibbing & Darko Pavic & Leif Kobbelt / Image Synthesis for Branching Structures

2141

(q1 , . . . , qk , ν1 , . . . , νk ) respectively (i.e., k 2D sample positions and k thickness values). Notice that this is in contrast
to Sect. 4.1 where we resampled the skeleton with a constant
step width h since here the same number of samples for fragments with different radii leads to different step widths. The
fixed number of samples allows us to compare even fragments of different sizes in a meaningful manner. Also notice
that we are using 2D sample positions and not angles to describe the geometry of the skeleton.

where G j−1 = Ri( j−1), j−1 (Fi( j−1) ). The parameter ωC balances shape versus color matching and depends on the contrast in the image and the variability of the branch structure.
Just like in Sect. 4.2 we can increase the variability of the
output by randomly picking one of the c best matching candidate fragments with a probability inversely proportional to
the corresponding value of the objective function.

5.2. Branch Synthesis

The greedy branch synthesis selects a new source fragment
to be copied in every step. In the target image these fragments need to be composed in a seamless manner. Possible reasons for visible seams in the output can be shape
matching errors between Ri( j), j (Fi( j) ) and G j (causing jaggy
artifacts) or color matching errors. We compensate shape
matching error by smoothly deforming the rigidly transformed source fragments so that the shape deviation is eliminated. The color matching error is eliminated by using photometric correction [PGB03] (Fig. 5).

The task of branch synthesis consists of finding the best
combination of source fragments Fi( j) that we copy to the
target fragments G j so that we obtain maximum shape
matching between Fi( j) and G j and maximum color consistency between successive target fragments G j−1 and G j .
While in principle this is a complex global optimization
problem similar to the one discussed in [SYJS05] it turned
out in our experiments that in our setting it is sufficient to
add one fragment at a time in a greedy fashion to obtain very
good results.
The shape similarity between a source fragment Fi and a
target fragment G j is measured by the difference between
their shape information fi and g j . However, since the representation of the shape information in terms of 2D positions
is not orientation independent, we first have to compute the
rigid transform Ri j that brings the skeleton segment of Fi in
best alignment to the skeleton segment of G j . We do this in
closed form by shifting Fi ’s center to the center of G j and
rotating Fi so that the square error
k

∑

Ri j (pl ) − ql

2

l=1

is minimized. This corresponds to one step of a 2D version
of the ICP algorithm [TL94]. Under this rigid transform the
shape (non-)similarity between Fi and G j is measured by
k

S(Fi , G j ) =

∑

Ri j (pl ) − ql

2

+ ωS |µl − νl |2

(1)

l=1

with some weight coefficient ωS which by default we set to
ωS = 2. Notice that even if the positional deviation grows
with the size of the fragment (but the thickness difference
does not), we still do not need to adjust ωS since the fragment
size is defined as a multiple of the thickness and hence both
terms in (1) scale equally.
The
color
consistency
C(G j−1 , G j ) =
C(Ri( j−1), j−1 (Fi( j−1) ), Ri( j), j (Fi( j) )) between two overlapping target fragments is simply measured by the sum of
squared color differences in the region of overlap.
With these ingredients we can now formulate the greedy
branch synthesis procedure which copies in each step that
source fragment Fi ( j) to the next target fragment G j which
minimizes the objective function
i( j) = arg mini S(Fi , G j ) + ωC C(G j−1 , Ri j (Fi )),
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

5.3. Fragment Composition

(a)
(b)
(c)
Figure 5: (a): Fragment composition without deformation
(b): Deforming fragments avoids visible artifacts caused by
shape matching errors. (c): Correcting the global color shift
by adding constraints to the photometric correction.
After the rigid transform Ri( j), j the remaining shape deviation between the skeleton polygons of Ri( j), j (Fi( j) ) and G j
is described by the displacement vectors dl = Ri( j), j (pl ) −
ql . In order to obtain perfect alignment, we compute a
smooth displacement field duv which assigns a displacement
vector to each pixel in G j such that the energy functional
E = ∑ 4duv − du−1,v − du+1,v − du,v−1 − du,v+1

2

is minimized subject to the constraint that the vectors dl are
interpolated. This requires the solution of a linear system
with the number of unknowns being the number of pixels in
the fragment. In order to find this minimizer in a convenient
way, we use the freely available constrained solver presented
in [BZK09]. Since the shape deviation between Ri( j), j (Fi( j) )
and G j can be expected to be rather small, the deformation
field is smooth and injective such that no artifacts are generated in this step (Fig. 5).
In order to remove seams caused by color mismatch between fragments we apply photometric correction [PGB03]
where color gradients instead of pixel colors are copied and
the final target fragment colors are computed by an integration step guaranteeing zero gradients along common boundary with previous fragments. Applying this technique directly, however, tends to produce significant global color

2142

Dominik Sibbing & Darko Pavic & Leif Kobbelt / Image Synthesis for Branching Structures

shifts as we keep on adding new fragments (see Fig. 5). To
prevent this color shift, we introduce additional constraints
to the photometric correction forcing the colors of the copied
fragment to remain unchanged along the boundary where the
new fragment is not adjacent to a previous one.
Similar to [KSE∗ 03] we compute new seams which partition the overlapping regions of two adjacent fragments such
that color values across that new seam are similar. This can
efficiently be done by using Graph Cuts. The graph is constructed by connecting pixels on the boundary of the overlap region to source and sink and connecting every pixel of
that region with its neighbors. The cost function between two
neighboring pixels is defined such that it is proportional to
the color difference between source and target fragment at
these positions. The min-cut or max-flow algorithm will then
identify the optimal cut through the overlapping region.
A final problem is that when adding sub-branches to the
main branch, we need to overwrite previously synthesized
fragments to properly introduce the furcation points. This,
however, is very easy to achieve since we know exactly
where the fragments need to be placed and what their radius
is. Hence, we can simple delete that region from the output
image and continue with the image synthesis (Fig. 6). One
could also use squared shape fragments where the computation of the overlapping region and inside tests can be performed more efficient. Nevertheless in our experiments we
observed that round fragments cause less artifacts and accelerate the computation of Graph Cuts and photometric correction by ∼ 20%, because of smaller overlapping regions.

Figure 6: To generate proper furcations, parts of the parent
branch need to be deleted before synthesizing sub-branches.
5.4. Image Completion
After the branching structure has been synthesized, we complete the output image by filling the remaining gaps using
the image completion technique described in [PSK06]. Here
we make sure that only those fragments are copied from the
input image that do not overlap with branches since otherwise pseudo-branches might be generated.
6. Results
We performed our experiments on an Intel Core i7 with
2.7GHz in a single threaded prototypical application. Drawing a simple source tree takes about 2 minutes of user interaction. For, e.g., Fig. 1 we specified only 65 points. The
reliability of a maximum gradient detection operator to estimate the radius strongly depends on the input images: In

Example
River
Mountain
Collard
Lightning

#SF / #TF
68/139
31/43
68/71
251/378

R
67
144
100
20

IS (sec)
30.6
72.8
50
8.6

Table 1: Timings for the image synthesis. #SF and #TF denote the number of source and target fragments.
Fig. 1 we re-defined almost all radii while in the river example (Fig. 7) we only corrected less than ten skeleton nodes.
Training the SVM takes about one minute. Note that both
steps need to be done only once for one source image.
In Fig. 7 we present some results. For larger and more
complex output structures please see the accompanying supplemental material. The first column shows the input image
from which the structures are extracted and learned. After
synthesizing the structure which is used in the fragmentbased image synthesis we obtain images as shown in the
second column. The final result produced with an interactive
image completion technique is shown in the third column.
In order to quantitatively evaluate the result of the structure
synthesis we compute histograms from the angle and thickness distribution of input and target structure. In Fig. 7 we
show these histograms right to the synthesized image. The
corresponding histograms for the input and output images
are placed on top of each other for more convenient comparison. The left histograms show the angle and the right
histograms the thickness distributions. In Fig. 8 we show
an example where we additionally considered a proper perspective rectification for a source image not captured from a
frontal view.
To create the target structure the user decides how much
he wants to interact. The system provides metaphors to automatically extend existing single branches (point and click),
to alternating extend all branches, to place furcations automatically or manually, to manually delete unwanted subbranches or to edit the density map in order to provide a
guidance at the coarsest scale (Sect. 3). Since extending existing branches can be done instantaneously this part is fully
interactive. The river and mountain range (Fig. 7) was generated without manual interaction. For the collard and lightning example we deleted several branches in a last step,
which took less than two minutes. Synthesizing image content along the structure is again an automated process and
involves solving linear systems for the warping, photometric correction and computing new seams. The computation
time for the image synthesis (IS) strongly depends on the radius (R) of the fragments (Table 1). Our interactive image
completion for filling the gaps between the structure takes
the most time (10 minutes) and we plan to replace this by
a fully automatic method which adjusts positions and size
of the target fragments starting at furcation points. Common
texture synthesis methods, e.g. [BD02, KSE∗ 03], should be
able to handle this task, since in this stage we only need to
synthesize rather homogeneous image content. With this the
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2143

Dominik Sibbing & Darko Pavic & Leif Kobbelt / Image Synthesis for Branching Structures
0.4

0.25

0.35
0.2
0.3

0.25
0.15

0.2

0.1
0.15

0.1
0.05
0.05

0
140

0
150

160

170

180

190

200

210

220

230

240

0.4

7

8

9

10

11

12

13

14

15

16

7

8

9

10

11

12

13

14

15

16

30

35

40

45

50

55

60

65

70

75

30

35

40

45

50

55

60

65

70

75

0.25

0.35
0.2
0.3

0.25
0.15

0.2

0.1
0.15

0.1
0.05
0.05

0
140

0
150

160

170

180

190

200

210

220

230

240

0.18

0.25

0.16
0.2

0.14

0.12
0.15
0.1

0.08
0.1
0.06

0.04

0.05

0.02

0
155

0
160

165

170

175

180

185

190

195

200

0.18

0.18

0.16

0.16

0.14

0.14

0.12

0.12

0.1

0.1

0.08

0.08

0.06

0.06

0.04

0.04

0.02

0.02

0
155

0
160

165

170

175

180

185

190

195

200

0.35

0.25

0.3
0.2
0.25

0.15
0.2

0.15
0.1

0.1
0.05
0.05

0
150

0
155

160

165

170

175

180

185

190

195

0.4

25

30

35

40

45

50

25

30

35

40

45

50

0.25

0.35
0.2
0.3

0.25
0.15

0.2

0.1
0.15

0.1
0.05
0.05

0
150

0
155

160

165

170

175

180

185

190

195

0.2

0.2

0.15

0.15

0.1

0.1

0.05

0.05

0
120

0
140

160

180

200

220

240

0.2

1.5

2

2.5

3

3.5

4

4.5

5

5.5

1.5

2

2.5

3

3.5

4

4.5

5

5.5

0.25

0.18

0.16

0.2

0.14

0.12

0.15

0.1

0.08

0.1

0.06

0.04

0.05

0.02

0
120

0
140

160

180

200

220

240

Figure 7: Results generated by our system. The first column shows the input image. The synthesized structure is seen in the
second column, while the third column shows the final result. For each result we computed a histogram for the angles (left) and
thickness values (right). The corresponding histograms for the input and output images are placed on top of each other for more
convenient comparison. In the last row we allowed that the interactive image completion adds fragments showing the nerve of
the lightning, hence some additional small branches appeared.

(a)
(b)
(c)
(d)
(e)
Figure 8: Crack structure example. The image of a crack (a) is rectified (b) to allow for perspectively correct copy and paste
operations during the image synthesis process. (c): Structure propagation only. (d): Initial Solution. (e): Back-distorted solution
with similar perspective as the one shown in the input image.
generation of multiple outputs from a source image becomes
more easy.
Limitations The main limitation of our system is, that
it can only handle bi-furcation or tree-like structures, i.e. it
is not possible to synthesize structures that split and merge
again as observed at cracks in dry ground or city layouts.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

For this, methods like [BD02] seem to be more suitable. Our
system is purely 2D and cannot produce 3D structures. It is
also hard to create images showing 3D structures, like trees
or plants in general. A user could identify the structure at
a certain depth layer, but the image synthesis will probably
fail, since it is very likely that fragments at a wrong depth

2144

Dominik Sibbing & Darko Pavic & Leif Kobbelt / Image Synthesis for Branching Structures

layer are selected, which will result in a scrambled looking
image. For future work we would like to detect source structures automatically or at least strongly machine guided and
automatize the time consuming interactive image completion.
7. Conclusion
In this paper we presented an approach to synthesize images containing branching structures. Our branching model
is a Markov model where proceeding angles and thickness
parameters are used to find the most similar structures on
a source structure and expand a target structure by the successive elements from the source structure. The quality score
used in this process is updated due to a global similarity measurement involving histograms over the angle and thickness
distribution. As seen in the results these histograms indicate
that the synthesized structures have similar global characteristics. Our space colonization approach prevents the structure from self-intersecting and allows for guidance by a user.
For each individual structure we learn a branching model by
using support vector machines. In the image synthesis part
we use multiple techniques to increase the image quality.
References
[Ash01] A SHIKHMIN M.: Synthesizing natural textures. In I3D
(2001), pp. 217–226. 2
[AVB08] A LIAGA D. G., VANEGAS C. A., B ENEŠ B.: Interactive example-based urban layout synthesis. In SIGGRAPH Asia
(2008), ACM, pp. 1–10. 2
[BD02] B ROOKS S., D ODGSON N.: Self-similarity based texture
editing. In SIGGRAPH (2002), ACM, pp. 653–656. 2, 8, 9
[Bis07] B ISHOP C. M.: Pattern Recognition and Machine Learning, 1 ed. Springer, October 2007. 5
[BZK09] B OMMES D., Z IMMER H., KOBBELT L.: Mixedinteger quadrangulation. ACM TOC 28, 3 (2009), 1–10. 7
[CEW∗ 08] C HEN G., E SCH G., W ONKA P., M ÜLLER P.,
Z HANG E.: Interactive procedural street modeling. ACM TOC
27, 3 (2008), 1–10. 2

[KFCO∗ 07] KOPF J., F U C.-W., C OHEN -O R D., D EUSSEN O.,
L ISCHINSKI D., W ONG T.-T.: Solid texture synthesis from 2d
exemplars. ACM TOG 26, 3 (2007), 2:1–2:9. 4
[KL04] K IM T., L IN M. C.: Physically based animation and rendering of lightning. In PG ’04: Proceedings of the Computer
Graphics and Applications, 12th Pacific Conference (Washington, DC, USA, 2004), IEEE Computer Society, pp. 267–275. 2
[KSE∗ 03] K WATRA V., S CHÖDL A., E SSA I., T URK G., B O BICK A.: Graphcut textures: image and video synthesis using
graph cuts. In SIGGRAPH (2003), ACM, pp. 277–286. 2, 8
[KW07] K WATRA V., W EI L.-Y.: Example-based texture synthesis. In SIGGRAPH Course #15 (2007). 3
[LH05] L EFEBVRE S., H OPPE H.: Parallel controllable texture
synthesis. In SIGGRAPH (2005), pp. 777–786. 3
[LH06] L EFEBVRE S., H OPPE H.: Appearance-space texture
synthesis. In SIGGRAPH (2006), pp. 541–548. 3
[Lin68] L INDENMAYER A.: Mathematical models for cellular interaction in development i. filaments with one-sided inputs. In
Journal of Theoretical Biology (1968). 2
[Nor98] N ORRIS J.: Markov Chains (Cambridge Series in Statistical and Probabilistic Mathematics). Springer, July 1998. 4
[PGB03] P ÉREZ P., G ANGNET M., B LAKE A.: Poisson image
editing. ACM TOC 22, 3 (2003), 313–318. 7
[PH89] P RUSINKIEWICZ P., H ANAN J.: Lindenmayer systems,
fractals and plants. Springer-Verlag New York, Inc., 1989. 2
[PHL∗ 09] PALUBICKI W., H OREL K., L ONGAY S., RUNIONS
ˇ
A., L ANE B., M ECH
R., P RUSINKIEWICZ P.: Self-organizing
tree models for image synthesis. In TOC (2009), vol. 28, ACM,
pp. 1–10. 2, 5
[PSK06] PAVI C´ D., S CHÖNEFELD V., KOBBELT L.: Interactive
image completion with perspective correction. Vis. Comput. 22,
9 (2006), 671–681. 3, 6, 8
[RCOL09] ROSENBERGER A., C OHEN -O R D., L ISCHINSKI D.:
Layered shape synthesis: automatic generation of control maps
for non-stationary textures. In SIGGRAPH Asia (2009). 3
[SYJS05] S UN J., Y UAN L., J IA J., S HUM H.-Y.: Image completion with structure propagation. In SIGGRAPH (2005), ACM,
pp. 861–868. 3, 6, 7
[TCLT07] T ING H., C HEN S., L IU J., TANG X.: Image inpainting by global structure and texture propagation. In Proc. of int.
Conf. on Multimedia (2007), pp. 517–520. 3

LIBSVM: a library
[CL01] C HANG C.-C., L IN C.-J.:
for support vector machines, 2001. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm. 5

[TCPT03] T OYAMA C. E., C RIMINISI A., P ÉREZ P., T OYAMA
K.: Object removal by exemplar-based inpainting. In CVPR
(2003), pp. 721–728. 3

[DCOY03] D RORI I., C OHEN -O R D., Y ESHURUN H.:
Fragment-based image completion. ACM TOC 22, 3 (2003),
303–312. 3

[TL94] T URK G., L EVOY M.: Zippered polygon meshes from
range images. In SIGGRAPH (1994), ACM, pp. 311–318. 7

[EL99] E FROS A. A., L EUNG T. K.: Texture synthesis by nonparametric sampling. In ICCV (1999), p. 1033. 2
[HE07] H AYS J., E FROS A. A.: Scene completion using millions
of photographs. SIGGRAPH 2007 26, 3 (2007). 3
[HJO∗ 01] H ERTZMANN A., JACOBS C. E., O LIVER N., C UR LESS B., S ALESIN D. H.: Image analogies. In SIGGRAPH
(2001), ACM, pp. 327–340. 2
[HOCS02] H ERTZMANN A., O LIVER N., C URLESS B., S EITZ
S. M.: Curve analogies. In EGRW (2002), pp. 233–246. 2
[KEBK05] K WATRA V., E SSA I., B OBICK A., K WATRA N.:
Texture optimization for example-based synthesis. In SIGGRAPH (2005), pp. 795–802. 3

[TZW∗ 07] TAN P., Z ENG G., WANG J., K ANG S. B., Q UAN L.:
Image-based tree modeling. In SIGGRAPH (2007), ACM, p. 87.
2
[WLKT09] W EI L.-Y., L EFEBVRE S., K WATRA V., T URK G.:
State of the art in example-based texture synthesis. In EG-STAR
(2009). 3
[WZSB08] W U Y., Z HANG H., S ONG C., BAO H.: Space-time
curve analogies for motion editing. In GMP08 (2008). 2
[ZSTR07] Z HOU H., S UN J., T URK G., R EHG J. M.: Terrain
synthesis from digital elevation models. IEEE Trans. on Visualization and Computer Graphics 13, 4 (2007), 834–848. 3

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

