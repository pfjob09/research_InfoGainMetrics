DOI: 10.1111/j.1467-8659.2011.02061.x
Pacific Graphics 2011
Bing-Yu Chen, Jan Kautz, Tong-Yee Lee, and Ming C. Lin
(Guest Editors)

Volume 30 (2011), Number 7

A Single Image Representation Model for Efficient
Stereoscopic Image Creation
Younghui Kim, Hwi-ryong Jung, Sungwoo Choi, Jungjin Lee and Junyong Noh
Graduate Shool of Culture Technology, KAIST, Republic of Korea

Abstract
Computer graphics is one of the most efficient ways to create a stereoscopic image. The process of stereoscopic CG
generation is, however, still very inefficient compared to that of monoscopic CG generation. Despite that stereo
images are very similar to each other, they are rendered and manipulated independently. Additional requirements
for disparity control specific to stereo images lead to even greater inefficiency. This paper proposes a method to
reduce the inefficiency accompanied in the creation of a stereoscopic image. The system automatically generates
an optimized single image representation of the entire visible area from both cameras. The single image can
be easily manipulated with conventional techniques, as it is spatially smooth and maintains the original shapes
of scene objects. In addition, a stereo image pair can be easily generated with an arbitrary disparity setting.
These convenient and efficient features are achieved by the automatic generation of a stereo camera pair, robust
occlusion detection with a pair of Z-buffers, an optimization method for spatial smoothness, and stereo image pair
generation with a non-linear disparity adjustment. Experiments show that our technique dramatically improves
the efficiency of stereoscopic image creation while preserving the quality of the results.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism—

1. Introduction
Stereoscopic images can be far more simply and efficiently
created using computer graphics than by using stereo camera
rigs for real-time filming. Stereo filming requires very precise calibration between both cameras whereas a computer
graphics stereo camera simply bypasses this step. Controlling the disparity of a stereoscopic image pair is also much
easier in computer graphics as scenes are rendered repeatedly under identical conditions. Furthermore, computer generated stereo images do not require any additional synchronization or correction as there is no geometrical misalignment or error introduced by hardware imperfections.
However, the process of stereoscopic CG generation is
still very inefficient compared to that of monoscopic CG
generation. First, despite that the two images are very similar due to the close proximity of the stereo cameras, they are
rendered independently. This wastes a vast amount of time as
high quality rendering of each image requires heavy computation. Second, image manipulation entails repetition of the
same tasks to handle the image from each side of the camera
and image manipulations of both sides are very hard to be
c 2011 The Author(s)
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd. Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ,
UK and 350 Main Street, Malden, MA 02148, USA.

synchronized. This introduces crucial inefficiency since rendered images are commonly manipulated in the compositing
process. For example, a user must draw numerous masks and
features twice to emphasize the local specular highlights or
to remove artifacts from rendered images. Moreover, matte
painted background composition and a spatial kernel based
filter such as depth blur have to be applied twice with the
same precise parameters and placement settings. Another
problem lies in camera dependent shading components such
as specular highlights and reflection. These introduce disparity even for the same depth, resulting in visual fatigue
[Men09] [Aut08] [McD10]. These components thus need to
be deleted or synchronized in the image pair in order to ensure a high quality stereoscopic image.
Control of disparity is another new requirement for the
creation of stereoscopic images. Typically, the disparity budget is carefully calculated in pre-production. However, a user
requests editing of disparity levels [LHW∗ 10] in an effort to
best reflect the original intention and to prevent visual fatigue. Consequently, the final disparity budget can be different from that determined in the pre-production stage. More-

2068

Younghui Kim et al. / A Single Image Representation Model for Efficient Stereoscopic Image Creation

over, a different display environment requires a different
amount of disparity [DSS07, LHW∗ 10]. Stereo images targeted for movie theatres will have lessened virtuality when
viewed on 3DTV. Similarly, stereo images created for 3DTV
will have heightened parallax, leading to visual fatigue when
viewed in a theatre. As children have less separation between
the eyes, they tend to perceive more parallax than adults do.
The disparity budget needs to be properly adjusted depending on the audience and the display device. On a general
stereo pipeline, these disparity adjustments lead to repetition
of the rendering computation.
This paper proposes an effective technique to minimize
the inefficiency accompanied in stereoscopic image creation.
Our method renders a single image that represents all visible areas from both stereo views using an efficient stereo
specific multiple viewpoint representation model. As a single image is rendered, stereo rendering takes much less time.
Furthermore, the camera-dependent shading components are
automatically synchronized and any additional image manipulation can be simply accomplished with half the effort.
A stereo image pair can be easily created from the single
rendered image while the disparity can be freely adjusted
within the maximum allowable disparity. This also means
that a new stereo image pair with different disparity can be
conveniently reproduced, depending on the user’s intention
or the display environment.
We take a similar approach to existing multiple viewpoint
representation models. Some studies [Hal98, PRA06, RP08]
have enumerated the advantages of using multiple viewpoint
representation for efficient creation of a stereoscopic image.
For our stereo application, the following conditions have to
be satisfied by a single image representation.
• Elimination of occlusion error occurring in a stereo camera pair
• No repetition of unnecessary rendering computation
• Preservation of the original shapes of scene objects
• Easy generation of a stereo image pair from the single
rendered image with free disparity setting
• Satisfaction of the above conditions regardless of scene
complexity
Despite reported success, the existing methods do not satisfy
all the conditions listed above. We describe this in greater
detail in Related Work.
Our method is designed to satisfy the above conditions for
the efficient creation of a stereo image. Our contributions can
be summarized as follows. 1) The visible regions from either of the stereo cameras are automatically determined and
a single image that encodes the information is rendered. 2)
The single rendered image is spatially smooth and maintains
the original shapes of scene objects. The image can be easily
manipulated by a user with the same degree of convenience
as experienced in monoscopic image creation. 3) A stereo
image pair can be easily generated from the rendered image
with any given disparity setting. This process also ensures

that non-linear as well as linear disparity mapping can be
easily controlled. Consequently, our technique greatly expedites the process of stereoscopic image creation. We verify
this by performing diverse experiments and presenting the
obtained results.

2. Related Work
Most existing multiple viewpoint representation models focus on generating a unique perspective. The purpose of these
methods is to express artistic impression and to represent
3D scene information efficiently. Multirama [WFH∗ 97] produces a multi-perspective panoramic image from a given
camera path. Agrawala et al. [AZM00] resolve object visibility by assigning each scene object to a local camera. Yu and McMillan [YM04a] smoothly combine the
fragments obtained from multiple general linear cameras
(GLC) [YM04b] into a single image. Coleman and Singh
[CS04] deform the object geometry for non-linear projection. The graph camera [PRAV09], the curved ray camera
(CRC) [CRPH10], and the multi-center-of-projection camera (MCOP) [RB98] respectively render a single image containing the region visible from the camera path defined by
the user. The images generated by these representation models are spatially smooth and visually plausible, and can be
manipulated by the user with ease. Although these methods can handle large occlusion error, they are not suitable
for stereo application. The occlusion error in a stereo image
is typically widely distributed over the entire image. These
methods offer no strategy to pinpoint the occlusion regions
and instead generate multi-perspective images depending on
user inputs.
Another application of a multiple viewpoint representation model, also known as an occlusion camera, is to minimize occlusion error and thereupon generate a novel view
using Image Based Rendering (IBR). The single pole occlusion camera (SPOC) [MPS05] generates nearby views from
an image that is radially distorted. Correction of occlusion
error allows an object to be displayed on a volumetric 3D
display [PRA06]. However, SPOC is only suitable for single
objects. The depth discontinuous occlusion camera (DDOC)
[PA06] assumes that occlusion error occurs near the region
where depth information becomes discontinuous. The occlusion is reduced by distorting the regions near the silhouette
of objects. This method can be used for multiple objects.
SPOC and DDOC handle the occlusion error tied to unspecified nearby cameras. However, local loss of resolution
and distortion around the boundary of each object can occur
frequently. The epipolar occlusion camera (EOC) [RP08]
is a stereo specific model that automatically identifies occlusion regions from a given stereo camera pair. However,
the images generated by EOC are rendered with discontinuous rays, which leads to undesirable spatial unsmoothness.
Consequently, the resulting images are not easily manipulated via common techniques and cannot represent the comc 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

Younghui Kim et al. / A Single Image Representation Model for Efficient Stereoscopic Image Creation

2069

Figure 1: Overview of our method. The colors in (b) and (d) indicate the visibility tags. The red color is only visible from the
left camera, the blue color is only visible from the right camera, and the green color is visible from both.

mon camera dependent shading components smoothly. Furthermore, the assumption employed by DDOC and EOC to
find occluisions is not robust, as the user must set a precise
threshold. Occlusion error may occur even in regions of continuously changing depth. Halle’s method [Hal98] renders
multiple viewpoints by exploiting perspective coherence in
spatially varying viewpoints. It generates volumetric data for
acceleration of direct rendering. However, the results cannot
be manipulated at all.
Stereoscopic image manipulation has been actively researched in recent years. Lang et al. proposed a method to
manipulate the disparity of a stereo image pair [LHW∗ 10].
Chang et al. [CLC11] recently proposed a content aware display adaptation method that manipulates scene disparity. Lo
et al. [LvBK∗ 10] proposed a method that deals with the composition between stereoscopic images. For efficient stereoscopic image creation Sawhney et al. [SGH∗ 01] use a stereo
image pair where one view is rendered at a lower resolution. They then synthesize the lower resolution view to have
the same resolution as another view. Although these studies
share the same basic goal of minimizing the extra work in the
creation of a stereoscopic image, their underlying methodology is comprehensively different from that of our approach.

3. Method Overview
Figure 1 presents an overview of the proposed method,
which is comprised of three main steps. The pre-rendering
process computes a buffer that contains spatially smooth visible points, allowing efficient and high-quality stereoscopic
rendering. The rendering process simply renders a scene
with the computed result from the pre-rendering stage. The
post-rendering process creates a stereo pair of images according to a given disparity setting.
In the pre-rendering process (Section 4), the maximum
disparity is set based on a widely adopted guideline, and
temporary left and right cameras are created (Section 4.1).
Using each left/right Z-buffer (the upper images of Figure 1(b)), the points visible from either of the cameras
are collected (Section 4.2). Finally, the collected points
are smoothly placed in a single image (Section 4.3). The
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

smoothly placed points are rendered with refracted rays in
the rendering process (Section 5). Any ray-tracing based rendering algorithm and diverse shaders such as global illumination, final gathering, ambient occlusion, and so on can be
used for this. The post-rendering process (Section 6) generates a stereo image pair from the single depth image while
disparity can be linearly and non-linearly adjusted.
4. Pre-rendering Process
4.1. Maximum Inter-ocular Distance Determination
The adjustment of the disparity of a stereoscopic image can
be achieved by controlling the distance between both cameras, namely, the Inter-Ocluar Distance (IOD). This means
that a viewpoint simulation from arbitrary camera placement
is needed to adjust the disparity of the stereoscopic image.
The aim of our method is to enable disparity adjustments by
view interpolation between a left-/right-most placed camera
pair. To obtain the left-/right-most placed camera pair, the
maximum value of IOD should be determined in advance.
Intead of a convergence setting, a parallel stereo camera setting is employed, as the parallel stereo camera does not introduce keystone distortion to the stereo image pair [Lip97].
The IOD in the parallel stereo camera setting is proportional
to the disparity [Lip82] :
Pi =

Mf
· IOD,
zi

(1)

where Pi is the screen disparity of the point having Z-depth
of zi . M denotes the ratio between the screen and the film
width; M = (Screen width)/(Film width). f is the focal
length of the camera lens and IOD is to be decided. Equation 1 indicates that the disparity Pi and Z-depth zi are inversely proportional. Consequently, the closest point to the
camera introduces the maximum amount of disparity under
the same IOD. A professional stereoscopic content creation
company [DSS07] uses −26cm as the maximum allowable
disparity for the parallel stereo camera setting in order to
ensure minimization of visual fatigue (The negative sign is
due to the subtraction order of the disparity computation.
See [DSS07] for more details). The maximum IOD associated with the maximum allowable disparity can be computed

2070

Younghui Kim et al. / A Single Image Representation Model for Efficient Stereoscopic Image Creation

as follows:
maximum IOD =

−26cm
· max zi .
Mf

(2)

Here, note that zi always has a negative value. Our experiments were performed with a 55" 3DTV. The screen width
was set to 121cm and the f ilm width was set based on the
scene created by a user working with Autodesk Maya 2010.
4.2. Visible Points Buffer (VPB) Generation
A visible points buffer (VPB) is created by the temporary
left/right cameras using the maximum IOD. The VPB contains all the points visible from both cameras. It also stores
visibility tag information that indicates from which camera
each point is visible. The tag can be one of le f t, right, or
both.
Initially, a pair of visibility buffers, VL and VR , is created utilizing the Z-buffers, ZL and ZR , corresponding to the
temporary left/right camera image space SL and SR . Here,
posL
for the position pi
of a point pi in SL , the correspondposR
ing position in SR , pi , can be calculated using a perspective projection. As shown in Figure 2(a), if the depth value
posL
posR
ZL (pi ) of a point pi and the depth value ZR (pi ) of the
posR
are the
point observed at the corresponding position pi
same, the point pi is visible from both cameras. On the other
posL
hand, if the depth value ZL (p j ) of point p j is smaller
(due to the negative sign) than the corresponding depth value
posR
ZR (p j ), point p j is occluded in the right camera (FigposR

is placed outside of the image space SR ,
ure 2(b)). If pk
point pk is also invisible from the right camera (Figure 2(c)).
The visibility tag for each point pi in SL can be determined
by
for pi in SL ,
if

posR
posL
ZR (pi ) > ZL (pi )

or

posR
pi

∈
/ SR ,

tag(pi ) = le f t,

else,

tag(pi ) = both.
(3)

The visibility tag for each point in SR can be determined in
a similar manner.
From the pair of visibility buffers, VPB representing all
the points visible from both cameras is created. VPB is constructed by combining the information from VL and VR . For
example, VL is copied over to VPB first, followed by insertion of the points that are only visible from the right camera,
p posR ∈ SR , tag(p) = right, to the appropriate position with
respect to the copied points from VL . As a result, VPB contains the points with tag(p) = le f t, tag(p) = both from SL
and the points with tag(p) = right from SR . Note that the
points with tag(p) = both are placed without duplication.
Relative horizontal positions of the points from VL and
VR should be carefully determined. The relative position of
the points in the rendered image space indicates the same

Figure 2: The strategy to decide the visibility of each point.
The colors of the circles in the Z-buffer represent their Zdepth values. Each position in each Z-buffer indicates p posL
and p posR .

Figure 3: VPB is constructed first by copying the left visibility buffer VL to V PB (a) and only the points visible from
the right camera are inserted (c) by comparing their relative positions in VR (b). The illustration shows that the point
of interest q is inserted between pi and p j . Each circle is a
point. The color of the circle indicates the visibility tag (the
red = le f t, the green = both, and the blue = right).

relative position in 3D space. If a point q is surrounded by
two points pi and p j in the rendered image space, point q
will be placed between pi and p j in 3D space. Let’s consider the follwing case. Assume that point q in VR is about to
be inserted to VPB (Figure 3(a)). The horizontal positions of
posR
posR
and p j
are compared with the position of the candipi
date point q. If points pi and p j surround the candidate point
q in SR (Figure 3(b)), the point q should be placed between
those two points in VPB (Figure 3(c)). This ordered placement of visible points provides significant convenience for
the posterior processes.
Although VPB contains all the points to be rendered in a
single image in the rendering process and is adequate for
efficient rendering, it cannot be manipulated in the same
manner as done in the monoscopic CG creation process. As
shown in Figure 4, the arrangement of the points is not very
smooth, neither vertically nor horizontally. This would make
it difficult to apply various spatial kernel based filters or userdrawn mask maps, or any features in a compositing process.
Without the assurance of spatial smoothness in VPB, the repc 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2071

Younghui Kim et al. / A Single Image Representation Model for Efficient Stereoscopic Image Creation

Figure 4: From the discontinuous initial VPB(left), the
points at which the tag changes (a) and the boundary of the
image (b) are extracted. Each cell represents a pixel and the
cross marked cells are samples to be extracted. The red, blue,
and green indicate the different visibility tags.

resentation of the user-drawn features would be jagged in a
resulting stereo image pair and the application of common
filters would introduce undesirable artifacts. This is a crucial
point, as most CG generated images are manipulated through
the application of numerous filters and user-drawn features
using image editing software such as Adobe Photoshop and
The Foundry Nuke. In addition, camera ray dependent shader
components such as specular highlights, reflection, and so on
can suffer from discontinuity of the camera rays.
4.3. Optimization
The optimization process produces a spatially smooth visible points buffer, V PB, from the initially jagged VPB. It is
achieved by generating a mesh from the sample vertices in
VPB and optimizing the mesh to remove spatial jaggedness.
V PB can be produced from the optimized mesh.
A simple sampling method extracts
the vertices from VPB to compose a mesh. Figure 4 illustrates our sampling strategy. While horizontally traversing
each row of VPB, we select the two points at which the tag
changes as sample vertices, v. Additionally, the leftmost and
rightmost points are also selected as samples. Note that the
sample vertices v are extracted among the points p in VPB.

Mesh generation

A 2D triangular mesh, Φ = (K,V ), is created from the
sample vertices. K denotes the connectivity of the vertices and V is the set of extracted sample vertices V =
{v1 , . . . , vn }. The Delaunay triangulation creates a wellshaped mesh [PS85]. The mesh Φ does not overlap in the
two dimensional space SV PB . Note that the boundary of the
initial mesh is not vertically well-aligned in the right side,
as shown in the region bounded by the dotted gray box in
Figure 4. This is because the points in VR were inserted to VL
while considering relative positions only in the construction
of VPB.
The initial triangular mesh Φ is refined to satisfy the following conditions to produce Φ:

Mesh optimization

c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

Figure 5: When the vertex movement by tensile energy
(Equation 5) causes overlapping (a), the fixing energy
(Equation 7) moves the vertex back (b).

(i) Maximal preservation of the original shape of each face
(ii) No overlap or flipping of the faces
(iii) Complete coverage of the rectangular image space
Condition (i) can be satisfied by defining and matching
the shape surrounding each vertex. Laplacian surface editing [SCOL∗ 04] addressed the cotangent weighted Laplacian
coordinates to describe the neighborhood ring of a vertex vi
as follows:
1
(4)
Ł(vi ) = vi −
∑ wi j v j ,
wi j∈Ω
i
where Ωi contains the neighbor vertices and wi is the sum
of wi j . wi j is the contangent weight defined as 12 (cot αi j +
cot βi j ) [DMSB99]. Our goal is to minimize the difference
between the Laplacian coordinate of a vertex from VPB and
that of a vertex from the original image space, SL or SR . Energy minimization is performed on the network of springs
that connect each vertex.
k
(5)
Et (vi ) = (δi − Ł(vi ))2 ,
2
where k is the elastic constant of the spring and is constant
for all the vertices. Ł(vi ) indicates the Laplacian coordinate
of vertex vi in VPB. The Laplacian coordinate δi from the
original space is determined by
if tag(vi ) = right,
else,

posR

δi = Ł(vi

),

posL
δi = Ł(vi ).

(6)

If tag(vi ) = both, δi can be calculated from the mesh represented in any of the left/right spaces. We simply use SL .
Meanwhile, the energy is independently defined for the horizontal and vertical coordinates. However, the vertical positions of vertex vi are identical in SL , SR , and SV PB . Therefore, it is sufficient to compute the energy in the horizontal
coordinate only.
Condition (ii) ensures a spatially smooth arrangement of
the resulting mesh. Applying Equation 5 alone often leads
to overlapping of faces. An additional energy function corrects this problem. Note that our optimization scheme is different from the common mesh parameterization approaches

2072

Younghui Kim et al. / A Single Image Representation Model for Efficient Stereoscopic Image Creation

[Tut63, Flo97, EDD∗ 95, Flo03]. Our optimization technique
minimizes the difference between Laplacian coordinates,
|δi − Ł(vi )|=0, not the Laplacian coordinate itself, Ł(vi )=0,
as described in Equation 5. In this case, the positive weights
cannot ensure that vi will be placed inside the neighborhood
ring.
Figure 5 shows the case of overlapping. Recall that vertex
vi can only be moved along the horizontal line (dotted line in
Figure 5). We denote the intersection between the horizontal
line and triangle edges (or vertices in some cases) as hi j . The
relative order between hi j and vi in the initial mesh Φ can
then be recorded. If the relative position of vertex vi is altered
by the tensile energy from Equation 5, for example, vi moves
to the left side of hi0 , (Figure 5(a)), and overlapping occurs.
Moving vi back to hi0 removes overlapping (Figure 5(b)).
The fixing energy that moves vi within the time step ∆t can
be computed as
m
E f (vi ) = 2 ∑(hi j − vi )2 ηi j ,
(7)
∆t j
where ηi j is a boolean function that sets ηi j = 1 when the
order of vi and hi j is changed relative to the initial mesh. m
represents the mass of each vertex vi , which is constant for
all the vertices.
The refined mesh Φ respects the energy functions and satisfies the first two conditions, (i) and (ii). To satisfy condition (iii), we first move the right-most vertices in the mesh
to align with the vertical end line of the target image. The
size of the target image can be arbitrary, although we use the
same size as SL or SR , which was defined by the user initially.
The system then simulates the evolution of the positions for
the remaining vertices with a time step of ∆t. To update
the evolution in a conservative system, the Euler-Lagrange
equation is employed, which is defined as [Met07],
d ∂L ∂L
−
= 0.
dt ∂v˙i ∂vi

(8)

Lagrangian L = 12 mv˙2 − ∑ E, where ∑ E = Et + E f from
Equation 5 and 7, respectively. Assuming that the acceleration of each vertex v¨i is constant, as ∆t is very small, solving
Equation 8 yields
∆vi = kt λ(δi − Ł(vi )),

(9)

∆vi = ∆vi + ∑(hi j − (vi + ∆vi ))ηi j ,

(10)

j

where λ = k∆t 2 /2m. All our demonstrations used λ = 0.05.
The simulation stops when the mesh Φ is balanced or
the number of iterations is greater than N = 5000. The
mesh Φ reaches a balanced state if the average movement,
average(|∆vi |), of the vertices is smaller than a threshold
value, which is 0.001 for all our demonstrations. Figure 6
shows the evolution of the mesh until it reaches a balanced
state through iteration. The algorithm computing a refined

Figure 6: The decrease in the average movement through
iterations. The different colors represent different cenes.

mesh Φ satisfying the above conditions from the initial mesh
Φ is presented in Algorithm 1.
Algorithm 1 Mesh optimization algorithm
1: procedure MESH O PTIMIZAION(Φ)
Φ ← Φ, iter ← 0;
2:
3:
for v j ∈ Φ and v j is boundary vertex do
4:
v j mapped to the boundary of image space;
5:
end for
6:
while ∆ ≥ 0.001 and iter ≤ 5000 do
7:
∆ = 0;
8:
for vi ∈ Φ and vi is not boundary vertex do
9:
∆vi from Equation 9;
10:
∆vi from Equation 10;
11:
vi ← vi + ∆vi ;
12:
∆ ← ∆ + abs(∆vi )/(# o f vertices);
13:
end for
14:
iter ← iter + 1;
15:
end while
16:
return Φ;
17: end procedure
The refined V PB where all visible points are smoothly distributed can be computed from
the sparse information of Φ. The points within each face in
Φ are warped from VPB to V PB (Figure 1(d)). As a result,
each point in VPB can be compressed or expanded in V PB.
A bi-linear filter is used to prevent discontinuity in V PB.
Note that the three dimensional position can be calculated
posL
posR
from vi
and vi
and the Z-buffers, ZL and ZR , in both
image spaces.
Refined V PB Acquisition

5. Rendering Process
The points in the refined V PB can be rendered with the center camera using a modified ray-tracing method. Each pixel
in the image space of V PB indicates a unique position in the
three dimensional space, without redundancy. This means
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

Younghui Kim et al. / A Single Image Representation Model for Efficient Stereoscopic Image Creation

Figure 7: The primary rays refracted toward a unique 3D
position are used in the ray-tracing. If the Z-depth of an intersection (the black dots in (b)) is closer than the expected
Z-depth, the ray passes the intersection to reach the point
that should be rendered.

Figure 8: The smooth placement of the points in the rendered image enables the image surface to be constructed in
3D space (c) from the rendered image (a) and V PB(b).

that rendering with V PB ensures single rendering of any
point without repetition.
As illustrated in Figure 7(a), all the pixels can be rendered
by refracting the rays such that they travel toward the corresponding three dimensional positions through the camera focal plane. Figure 7(b) shows a situation with occlusion. This
situation is resolved by comparing the Z-depth values of intersections with the expected Z-depth value. As explained
in Section 4.2, if the Z-depth values of the intersections are
closer than expected, the refracted ray passes through the intersections. At this moment, a small range was given to the
expected Z-depth values in order to prevent the occurrence
of errors by discretization and smoothing from the warping
process. In our demonstration, a range of ±0.05 was used
according to the scene unit setting.

2073

Figure 9: Direct manipulation of the depths of the vertices
on the image surface produces the effect of non-linear disparity adjustment. Applying a non-linear function (graph in
(b)) to the original image surface (a) leads to a non-linearly
modulated image surface (b). Here, modulation along the
depth axis is also applied to the horizontal and vertical positions of each vertex on the same scale.

left and right camera viewpoints. However, we devised a
much more efficient rendering technique that fully exploits
the characteristics of our rendered single image. Note that
all the points in the image have unique 3D positions, with
which the image can be bijectively placed in 3D space. This
forms a surface as shown in Figure 8. The surface can be
one-to-one mapped to the 2D parameter domain, which has
previously been defined by the image space of the rendered
image. Consequently, a novel viewpoint image can be easily
rendered by generating a mesh surface constructed from the
points as vertices while mapping the rendered image onto
the surface as texture. A simple Z-buffering method can be
applied, as the surface does not contain any holes. We call
this surface the image surface.
The post-rendering process includes linear adjustment
to modify the global disparity level and non-linear adjustment to control the local disparity, similar to Lang et al.
[LHW∗ 10]. Changing the distance between stereo cameras(IOD) set under the maximum IOD yields linear adjustment. Applying a non-linear function to the depths of vertices in the image surface achieves non-linear adjustment.
When we manipulate only the depth of a vertex, the neighboring region may be represented on a different scale in the
resulting stereo image pair due to the perspective projection.
This problem can be simply resolved by applying the same
scale of manipulations to the horizontal and vertical positions of each vertex. Figure 9 shows an example of an image surface (a) and the same image surface manipulated by
a non-linear function (b), which leads to contrast enhancement. This approach is very useful to manipulate the relative
depth between objects, as shown in Figure 12(f-j).

6. Post-rendering Process
A stereoscopic image pair can be easily generated from the
rendered image with our representation model. The rendered
image and V PB contain color information and the 3D position of each point, respectively. Use of any image based
rendering or volume splatting method will generate novel
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

7. Results and Discussion
We implemented our method using C++. The rendering
method was implemented as a lens shader for MentalRay for
Maya. All of the experiments were performed on a 2.8GHz
Intel quad-core processor with an nVidia 8800GT graphics

2074

Younghui Kim et al. / A Single Image Representation Model for Efficient Stereoscopic Image Creation

Figure 10: A comparison between the regions rendered from
a center camera (the upper row) and from our novel representation model (the lower row).

Figure 11: Our method produces a stereo image pair (b)
much faster compared to that obtained by a conventional
stereoscopic rendering technique (a).

chipset. All demonstrations were rendered with MentalRay
for Maya 2010 in HD resolution (1280x720).
Figure 10 compares the image generated from our representation model (the lower row) to that from a commonly
rendered image with a center camera (the upper row). The
left column represents a rendered image (a) and the right
column shows the amount of rendered region (b). As can be
seen in the figure, our representation model effectively suppresses widely distributed fine occlusions as well as large
scale occlusions. A much larger region is rendered from our
model (the orange circle in the lower row) compared to the
general model (the orange circle in the upper row). Furthermore, the rendered image is spatially smooth and the shape
of each object is also preserved.

Figure 13: The results from the image manipulation of userdrawn features (a,b) and spatial kernel based filtering (c,d).

Figure 11 shows three cases for comparison. The left column shows the reference stereo image pair generated by a
common stereoscopic rendering and the right column shows
the result generated by our method. Both were generated
with the same disparity setting. The required time and the
Root Mean Square Error(RMSE) are noted. The time costs
in the parentheses on the right indicate the computation time
for our method excluding the rendering computation. Each
scene contains very complex rendering computations including Final Gathering, Ambient Occlusion, and Image Based
Lighting. The results of these experiments verify that our
method can generate a stereo image pair much faster, without loss of visual quality, than common stereo rendering
methods do. The image in the third row contains extremely
reflective materials. Reflection is a representative camera dependent shading component. While the image has relatively
large RMSE, the result is still visually plausible. Although
our method may not be physically correct, the reflection is
guaranteed to be synchronized in both of the stereo images.
This is a very important feature as any slight discrepancy
caused by this kind of camera dependent shading component
will result in visual fatigue [Men09] [Aut08] [McD10].
Figure 12 shows the advantage in disparity adjustment.
A stereo image pair can be generated by adjusting the linear disparity (the upper row). From left to right, each column shows the rendered image from our method (a) and the
stereo image pair generated with increasing degree of disparity (from 25% to 100% of the allowable disparity (b-e)).
The lower row shows the results obtained with non-linear
disparity adjustments. From left to right, the images are the
reference stereo image pair (f), and the stereo image pairs
with higher and lower depth contrast (g,h). The remaining
two images show resulting stereo image pairs whose disparities were non-linearly manipulated by different non-linear
functions including depth separation (i) and the emphasis
of a certain object (j) (represented as different graphs). Our
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

Younghui Kim et al. / A Single Image Representation Model for Efficient Stereoscopic Image Creation

2075

Figure 12: Our method allows convenient linear (the upper row) and non-linear (the lower row) disparity adjustments.

Figure 14: Rendered image can suffer from local distortions
(c) when a scene has a drastically varying occlusion pattern
along the vertical direction (b).

disparity adjustment was performed in 9fps, which is fast
enough for comfortable user interaction. This strategy can
be more effectively used in the generation of a multi-view
stereo image for an autostereoscopic display. Instead of rendering 8 or more multi-views separately, our method can
generate arbitrary horizontal nearby views between views
from the left-/right-most cameras. Note that vertical nearby
views cannot be generated, since our method is based on the
horizontal epipolar baseline.
Figure 13 illustrates the adaptability of our spatially
smooth rendered image to common image manipulation
methods. For each row, the left images are the rendered images from our method (the upper images) and the images
manipulated (the lower images) by common methods such
as user-drawn features (a) and spatial kernel based image
filter applications (c). The mustache and letters were drawn
on to the rendered single image by a user (a). The Chalk
& Charcoal filter in Adobe Photoshop and a simple composition of backgroud sky were applied to an image (c). The
large image on the right side represents the results from our
stereo image pair generation. Each manipulation in the single rendered image was automatically applied to both stereo
images.
Figure 14 shows that our method has a limitation for
the scene where the occlusion pattern is drastically changed
along the vertical direction. Due to drastic discontinuities of
the boundary of the initial VPB (b), local distortions can arise
from the fixed boundary optimization scheme of our method
(the red circle in (c)). However, this is a rare case in comc 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

Figure 15: When disparity is excessive, some faces of the
image surface can cover the background object (a). These
faces can be situationally removed by checking the object
ID of the composing vertices (b). The yellow and magenta
colors of surfaces indicate different object IDs.

mon scenes and the use of free boundary optimization may
remedy the situation.
The process of the maximum IOD determination (Section 4.1) prevents employment of excessive disparity between two views. However, if a user chooses to set the IOD
to a very large distance for any reason as shown in Figure 15,
it may happen that the faces between the objects in the image surface (color varying faces in (a)) cover the background
object (yellow faces). This can be resolved by removing the
faces composed of vertices with different object IDs given
in the rendering process (b).
Ensuring temporal coherency across the smoothly rendered images from V PB was not considered in the design
of our method. Acknowledging the importance of ensuring
temporal coherency across image frames for animation sequences, we leave this issue as future work. The proposed
method renders a single image using a pre-defined optimized
projection from V PB, instead of using a perspective projection. Consequently, our approach is not suitable for application to a scanline based renderer. In an extreme case, if
the surface of an object fluctuates heavily, occlusion error
can be large with our method. This error originates from the
possibility that local deep holes on the surface can be invisible from both stereo cameras. Nevertheless, the present
experiments show that, for most cases, our method handles

2076

Younghui Kim et al. / A Single Image Representation Model for Efficient Stereoscopic Image Creation

complex geometry very well with a stereo camera pair that
has a maximum IOD value.

[Flo97] F LOATER M.: Parameterization and smooth approximation of surface triangulations. CAGD (1997), 231–250. 6
[Flo03] F LOATER M.: Mean value coordinates. CAGD (2003),
19–27. 6

8. Conclusion
We proposed a method to reduce typical inefficiency accompanied in the creation of stereoscopic images. All visible areas from a stereo camera pair are efficiently rendered into
an optimized single image. The camera dependent shading
components can be rendered smoothly and are guaranteed
to be synchronized. The rendered image can be manipulated
via conventional methods, as the image is spatially smooth
and maintains the original shapes of scene objects. From a
single image, a stereoscopic image pair can be easily generated with linear and non-linear disparity adjustments. Any
manipulation by the user is automatically applied to both
stereo images. Consequently, our method dramatically reduces the extra work involved in the creation of a stereoscopic image, achieving almost comparable convenience to
that experienced in the creation of a monoscopic image.

[Hal98] H ALLE M.: Multiple viewpoint rendering. In Proc. of
ACM SIGGRAPH ’98 (1998). 2, 3
[LHW∗ 10] L ANG M., H ORNUNG A., WANG O., P OULAKOS S.,
S MOLIC A., G ROSS M.: Nonlinear disparity mapping for stereoscopic 3d. In Proc. of ACM SIGGRAPH 2010 (2010). 1, 2, 3, 7
[Lip82] L IPTON L.: Foundations of the Stereoscopic Cinema. Ven
Nostrand Reinhold, 1982. 3
[Lip97] L IPTON L.: StereoGraphics Developers’ handbook.
StereoGraphics Corporation, 1997. 3
[LvBK∗ 10] L O W., VAN BAAR J., K NAUS C., Z WICKER M.,
G ROSS M.: Stereoscopic 3d copy & paste. In Proc. of ACM
SIGGRAPH ASIA 2010 (2010). 3
[McD10] M C D ONALD J.: Nvidia 3d vision automatic best practices guide. NVIDIA (2010), 25. 1, 8
[Men09] M ENDIBURU B.: 3D Movie Making: Stereoscopic digitial cinema from script to screen. Elsevier in Oxford, UK, 2009.
1, 8
[Met07] M ETAXAS D.: Physics-based Deformable Models.
Kluwer Acamedic, Dordrecht, 2007. 6

Acknowledgement
This work was supported by culture contents industry research and development program of KOCCA/MCST(2107602-003-10743-01-007, Software Development for 2D to
3D Stereoscopic Image Conversion).
References
[Aut08] AUTODESK: Stereoscopic filmmaking whitepaper: The
business and technology of stereoscopic filmmaking. Autodesk
(2008), 6. 1, 8
[AZM00] AGRAWALA M., Z ORIN D., M UNZNER T.: Artistic
multiprojection rendering. In Proc. of Eurographics Workshop
on Rendering 2000 (2000). 2
[CLC11] C HANG C., L IANG C., C HUANG Y.: Content-aware
display adaptation and interactive editing for stereoscopic images. IEEE Transaction on Multimedia (2011). 3
[CRPH10] C UI J., ROSEN P., P OPESCU V., H OFFMANN C.: A
curved ray camera for handling occlusions through continuous
multiperspective visualization. IEEE Transactions on Visualization and Computer Graphics (2010), 1235–1242. 2

[MPS05] M EI C., P OPESCU V., S ACKS E.: The occlusion camera. Computer Graphics Forum, Eurographics 2005 (2005). 2
[PA06] P OPESCU V., A LIAGA D.: Depth discontinuity occlusion
camera. In Proc. of ACM Symposium on Interactive 3D Graphics
2006 (2006). 2
Three[PRA06] P OPESCU V., ROSEN P., A LIAGA D.:
dimensional display rendering acceleration using occlusion camera reference images. IEEE/OSA Journal of Display Technology
(2006). 2
[PRAV09] P OPESCU V., ROSEN P., A DAMO -V ILLANI N.: The
graph camea. In Proc. of ACM Siggraph 2009 (2009). 2
[PS85] P REPARATA F., S HAMOS M.: Computational Geometry:
An Introduction. New Yourk: Springer-Verlag, 1985. 5
[RB98] R ADEMACHER P., B ISHOP G.: Multiple-center-ofprojection images. In Proc. of ACM SIGGRAPH 1998 (1998),
pp. 199–206. 2
[RP08] ROSEN P., P OPESCU V.: The epipolar occlusion camera.
In Proc. of ACM Symposium on Interactive 3D Graphics 2006
(2008). 2
[SCOL∗ 04] S ORKENE O., C OHEN -O R D., L IPMAN Y., A LEXA
M., ROSSL C., S EIDEL H.: Laplacian surface editing. Computer
Graphics Forum, Eurographics 2004 (2004). 5

[CS04] C OLEMAN P., S INGH K.: Ryan: rendering your animation nonlinearly projected. In Proc. of ACM International symposium on Non-photorealistic Animation and Rendering 2004
(2004). 2

[SGH∗ 01] S AWHNEY H. S., G UO Y., H ANNA K., K UMAR R.,
A DKINS S., Z HOU S.: Hybrid stereo camera: An ibr approach
for synthesis of very high resolution stereoscopic image sequences. In Proc. of ACM SIGGRAPH 2001 (2001). 3

[DMSB99] D ESBRUN M., M EYER M., S CHRODER P., BARR
A.: Implicit fairing of irregular meshes using diffusion and curvature flow. In Proc. of ACM SIGGRAPH 99 (1999), pp. 317–
324. 5

[Tut63] T UTTE W.: How to draw a graph. London Mathematical
Society 1963, 13, 1963. 6

[DSS07] D E J OHN M., S EIGLE D., S USINNO J.: Stereoscopic
geometry of 3d presentations. In Second in a series of technical
report from In-three (2007). 2, 3
[EDD∗ 95] E CK M., D EROSE T., D UCHAMP T., H OPPE H.,
L OUNSBERY M., S TUETZLE W.: Multiresolution analysis of arbitrary meshes. In Proc. of ACM SIGGRAPH 95 (1995), pp. 173–
182. 6

[WFH∗ 97] W OOD D., F INKELSTEIN A., H UGHES J., T HAYER
C., S ALESIN D.: Multiperspective panoramas for cel animation.
In Proc. of ACM SIGGRAPH ’97 (1997). 2
[YM04a] Y U J., M C M ILLAN L.: A framework for multiperspective rendering. In Proc. of Eurographics Symposium on Rendering (2004). 2
[YM04b] Y U J., M C M ILLAN L.: General linear cameras. In
Proc. of European Conference on Computer Vision 2004 (2004),
pp. 14–27. 2

c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

