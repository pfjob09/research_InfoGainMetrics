DOI: 10.1111/j.1467-8659.2011.02048.x
Volume 30 (2011), Number 7

Paciﬁc Graphics 2011
Jan Kautz, Tong-Yee Lee, and Ming C. Lin
(Guest Editors)

Motion Retrieval Using Low-Rank Subspace Decomposition
of Motion Volume
Chuan Sun1 , Imran Junejo2 , Hassan Foroosh1
1 Division
2 Department

of Computer Science, University of Central Florida, USA
of Computer Science, University of Sharjah, United Arab Emirates

Abstract
This paper proposes a novel framework that allows for a ﬂexible and an efﬁcient retrieval of motion capture data
in huge databases. The method ﬁrst converts an action sequence into a novel representation, i.e. the Self-Similarity
Matrix (SSM), which is based on the notion of self-similarity. This conversion of the motion sequences
into compact and low-rank subspace representations greatly reduces the spatiotemporal dimensionality of the
sequences. The SSMs are then used to construct order-3 tensors, and we propose a low-rank decomposition
scheme that allows for converting the motion sequence volumes into compact lower dimensional representations,
without losing the nonlinear dynamics of the motion manifold. Thus, unlike existing linear dimensionality
reduction methods that distort the motion manifold and lose very critical and discriminative components, the
proposed method performs well even when inter-class differences are small or intra-class differences are large.
In addition, the method allows for an efﬁcient retrieval and does not require the time-alignment of the motion
sequences. We evaluate the performance of our retrieval framework on the CMU mocap dataset under two
experimental settings, both demonstrating promising retrieval rates.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism—Animation; H.3.3 [Information Storage and Retrieval]: Information Search and
Retrieval—

1. Introduction
Generation of human motion capture dataset is a very
time consuming and an expensive process, yet very critical
for animation and movie industry [MRC05, SH07]. An
equally important problem is to identify or retrieve an
action sequence that might already be present in the
motion capture dataset. Additional complications occur
when a particular action sequence in the database has
many variants [MRC05, SKK04]. These variations can be
caused by individual differences in expression, posture,
motion clothing, perspective effects and camera motions.
Also, actions frequently involve and depend on manipulating
objects, which adds another layer of variability. For example,
we might have two instances of a kick motion sequence,
which may seem similar visually but may differ signiﬁcantly
if compared numerically on a frame-by-frame basis. This
work proposes a novel method that aims to address some
of the above mentioned issues.
With the growth of available motion databases, indexing
c 2011 The Author(s)
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd. Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ,
UK and 350 Main Street, Malden, MA 02148, USA.

and retrieval of motion data has received an increasing
attention in the literature. Various approaches have been
proposed in the past few years. The work in [LZWP03]
constructed a hierarchical tree of clusters of motions by
using the extracted keyframes for each motion, with deeper
levels of the tree corresponding to joints deeper in the
skeletal hierarchy. The DTW-based indexing technique in
[CVB∗ 03] is applied to mocap editing operations such
as time-warping, ﬁltering, or motion-warping. [KG04]
proposed a retrieval strategy by precomputing a match web
as an efﬁcient searchable representation of all possibly
similar motion segments. [MRC05] presented an approach
where they ﬁrst deﬁne various kinds of geometric features
that are claimed to be invariant under spatial variations.
Then, they perform an adaptive segmentation to achieve
invariance under temporal deformations. Two motion clips
are then considered as similar if they have similar
progression of geometric features. [Kov04] propose an
automated method for identifying logically similar motions
in a data set and use them to build a continuous and

1954

C. Sun, I. Junejo & H. Foroosh / Motion Retrieval Using Low-Rank Subspace Decomposition of Motion Volume

intuitively parameterized space of motions. However, they
require time correspondences for matching two sequences.
[DGL09] use a motion pattern discovery and matching
scheme that decomposes human motions into a part-based,
hierarchical motion representation. A fast string match
algorithm is then used for matching. [KTWZ10] propose a
method for global similarity searches based on kd-tree-based
local neighborhood searches. The work in [LZP07] applied
SVM on extracted geometric motion vectors for human
motion classiﬁcation. However, since the motion retrieval
problem always involves a tradeoff between accuracy and
efﬁciency, these methods either focus more on “accuracy”
with less “efﬁciency”, or vice versa [Lin06].
In contrast to the above mentioned works, the
proposed framework is based on exploring the notion
of “self-similarity”, which has received signiﬁcant attention
recently. The work in [BCD04] describes a gait recognition
technique based on the visual self-similarity of a walking
person to classify the movement patterns of different
people. [CD00] shows the effective use of the self-similarity
in recognizing different types of biological periodic
motions. In [JDLP10], the authors explore self-similarities
of action sequences over time to capture the structure of
temporal similarities and dissimilarities within an action
sequence. For a given action sequence, they compute the
distances between action representations for all pairs of
time-frames and store the results in a Self-Similarity Matrix
(SSM). They consider the temporal similarities between
frames of image sequences. The method in [JDLP10] also
exploits the notion of image self-similarity. For a given
action/motion sequence, they ﬁrst extract some low level
features. The distances between extracted features for all
pairs of time frames are computed and this results in a SSM.
Each action sequence is thus reduced to a 2D SSM matrix,
and the authors then proceed to extracting some useful
features from these SSMs and use them to train their action
recognition system.
This paper makes multiple contributions to this area.
The ﬁrst contribution is that, we propose a retrieval
framework that does not require time alignment in contrast
to the conventional methods. We employ the notion of
self-similarity derived from the recurrence plot theory, and
generate a unique, symmetric structure to summarize every
motion sequence. Second, we propose a novel scheme of
subspace dimensionality reduction for fast motion sequence
retrieval based on rank-1 tensor decomposition. Compared
with the recent approaches that involve dynamic time
warping (DTW) for building score-based correspondences
between related events, our tensor subspace decomposition
condenses an action into its meaningful representation
by reducing it to its lowest rank feature vectors. Third,
as a byproduct of low rank decomposition, reduced
time-complexity, and hence fast indexing and retrieval is
achieved to handle large databases.

Fig.1 gives an overview of our proposed framework.
First, we set up skeletal model for motion sequence
poses for initial representation; Second, we convert
motion sequence into a series of self-similarity matrix
representations in temporal dimension under Euclidean
distance metric, thereby creating a Motion Sequence Volume
(MSV) structure that encodes the internal dynamics of a
motion sequence. Third, the structure is decomposed into
three low-rank compact vectors using an optimal iterative
algorithm. Finally, we employ the cross correlation based
similarity measure for the ﬁnal retrieval phase.
The remainder of this paper is organized as follows.
Section 2 describes the terminology and the skeletal model
representation for motion datasets. Section 3 is dedicated
to the key ideas of our framework: the self-similarity
matrix representation, MSV generation, and MSV tensor
decomposition. We provide our similarity measure criterion
and evaluation details in Section 4, and the last section
presents discussions and conclusion.
2. Motion capture data representation
The work in this paper is focused on motion sequence
retrieval, which has been extensively explored for over
a decade. The task is to query a motion sequence and
ﬁnd the sequences whose distance to the query is either
below a threshold τ or among the k smallest. Our primary
motivation in this paper is to devise an effective data
representation and framework that identiﬁes similar motions
that are numerically dissimilar. Our secondary motivation is
to propose a method that allows the animators to quickly ﬁnd
similar motions within a large motion capture database.
2.1. Terminology
Human pose can be represented using a simpliﬁed model
of human skeleton, composed of bones that are connected
by joints. Motion capture can then be regarded as the
process of recording a temporal sequence of 3-dimensional
joint positions. The position of all joints at a given time
is known as a pose, described as a vector p ∈ R3×|J| ,
where |J| is the number of joints in the skeletal model and
each joint requires 3 elements to describe its 3D position.
A mocap sequence can be then formally described as a
time-dependent sequence of poses. This can be represented
by a 2D matrix S ∈ RT ×(3×|J|) , where T is the number of
poses (frames) in the mocap sequence.
2.2. Skeletal model
The CMU mocap dataset uses a skeletal model of 32 joints
(i.e., |J| = 32). It provides a detailed conﬁguration for the
joints such as the thumb joint. However, efﬁcient retrieval
or classiﬁcation does not require detailed joints information
when performing comparison between motion classes. For
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

C. Sun, I. Junejo & H. Foroosh / Motion Retrieval Using Low-Rank Subspace Decomposition of Motion Volume
/ŶŝƚŝĂů ƌĞƉƌĞƐĞŶƚĂƚŝŽŶ

^ĞůĨŝͲƐŝŵŝůĂƌŝƚǇ ďĂƐĞĚ
DŽƚŝŽŶ ^ĞƋƵĞŶĐĞ sŽůƵŵĞ
ĐŽŶƐƚƌƵĐƚŝŽŶ

ͻ ^ŬĞůĞƚĂů ŵŽĚĞů ƐĞƚƵƉ

ͻ ^ĞůĨͲƐŝŵŝůĂƌŝƚǇ
ƌĞƉƌĞƐĞŶƚĂƚŝŽŶ
ͻ dĞŵƉŽƌĂů
ĐŽŶĐĂƚĞŶĂƚŝŽŶ

^ƵďƐƉĂĐĞ ƌĞƉƌĞƐĞŶƚĂƚŝŽŶ

1955

DŽƚŝŽŶ ƌĞƚƌŝĞǀĂů

ͻ /ƚĞƌĂƚŝǀĞ ůŽǁͲƌĂŶŬ
ƚĞŶƐŽƌ ĚĞĐŽŵƉŽƐŝƚŝŽŶ
ͻ ŽŵƉĂĐƚ
ƌĞƉƌĞƐĞŶƚĂƚŝŽŶ
ŐĞŶĞƌĂƚŝŽŶ

ͻ ƌŽƐƐͲĐŽƌƌĞůĂƚŝŽŶ ďĂƐĞĚ
ĂŶĚ dtͲďĂƐĞĚ
ƐŝŵŝůĂƌŝƚǇ ŵĞĂƐƵƌĞ ĂŶĚ
ƌĞƚƌŝĞǀĂů

Figure 1: The overﬂow of our retrieval framework for motion capture data.

11
13
9

8
10

12
6

7
5

1
2
3
4

Figure 2: The tracked joint positions and their indices for
forming the initial vector

from the CMU Mocap dataset. It can be observed that
apparently walk and run exhibit similar motion patterns.
The walkturn motion is also similar to walk motion except
that the human body turns by some angle while walking.
While traditional motion retrieval approaches sometimes fail
to discriminate these pairs of near-identical patterns, our
method is capable of ﬁnding their characteristic differences
by making it possible to focus on a subspace of the motion
volume as described in Section 3.

3. Subspace representation

Figure 3: SSMs built for Run and Kick actions in the CMU
Mocap dataset by stacking the 13 key limb joints. All SSMs
are of dimension 13 × 13 and are generated from the vector
by concatenating the tracked limb points
instance, the variation in thumb joint position is obviously
less important than that of the femur joint position. Also,
since it is believed that 13 joints are sufﬁcient to represent
a motion action [Joh73], we evaluate our motion capture
sequences using a skeletal model consisting of 13 most
signiﬁcant joints, namely the left knee, right knee, left
foot, right foot, heep, left elbow, right elbow, left shoulder,
right shoulder, neck, head, left hand, and right hand. We
concatenate these joints into a vector in the order speciﬁed
by indices from 1 to 13, as shown in Fig.3.
In this way, each frame of an action sequence can be
initially represented by a vector of size 13, which is used
in the subsequent Motion Sequence Volume construction
stage. In Fig.4, we show 5 out of 12 motion sequences
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

Since the 3D human motion data is typically
high-dimensional and nonlinear, operations on such
data often involve dimensionality reduction as a critical
step before further processing. Forbes et al. [FF05] used
the weighted PCA in searching of motion data. Barbic
et al. [BSP∗ 04] used PCA to cut a long motion data
stream into single behavioral segments. The method
in [YKSN07] detects key motion poses in locations
where local variations are sharp with a Multi-Dimensional
Scaling (MDS) technique. Due to the fact that each
frame encoded by a high-dimensional vector has a
nonlinear relationship with other frames, [HWX∗ ] adopted
the Local Linear Embedding (LLE), an unsupervised
non-linear dimensionality reduction technique, to compute a
low-dimensional, neighborhood-preserving embedding for
motion poses. The approach attempts to discover nonlinear
structures in high-dimensional data by exploring the local
symmetries of linear reconstructions.
Unfortunately, popular low-dimensional embedding
approaches like PCA and MDS tend to create distortions in
nonlinear manifolds due to their linearity, and thus hinder
the detailed dynamics of the motion. As a result, they
are suitable for datasets where inter-class distances are
sufﬁciently large to distinguish different motion sequences.
On the other hand, the LLE method is highly susceptible
to local minima in optimization, and tends to emphasize
preserving purely local geometry and ignores distant inputs.
To tackle this problem, we perform the subspace
projection and dimensionality reduction from a different
perspective. The main motivation is to deal with the

1956

C. Sun, I. Junejo & H. Foroosh / Motion Retrieval Using Low-Rank Subspace Decomposition of Motion Volume

nonlinearity of the high-dimensional input motion manifold
by using the multilinear algebra and the recurrent plot
theory. Starting from the construction of self-similarity
matrix, our technique builds an order-3 tensor from the
input motion, and then performs an optimal rank-1 tensor
decomposition to reduce a motion sequence into its compact
and discriminative representation.
3.1. Self-similarity matrix
According to the recurrence plot theory [BC04],
Self-Similarity Matrix (SSM) provides important insights
into the dynamics of a vector with advantages and
promising applications already demonstrated in high
dimensional spaces in various ﬁelds [BCD04, CD00]. The
intuition behind the SSM is that, if we view the vector v
as a trajectory in a 2D Euclidean space, the SSM itself
captures the internal dynamics of this trajectory in a matrix
form [BC04]. Since motion sequences can be represented
in matrix form by concatenating consecutive poses, we
observe that SSM is particularly suitable for modeling the
internal dynamics of motion data.
Since SSM constitutes the building block of our
subsequent Motion Sequence Volume representation, we
ﬁrst brieﬂy describe some preliminary properties of SSM.
An SSM can be expressed by a N × N matrix
Ri, j (η, v) = Θ(η − vi − v j

q ),

where i, j = 1, ..., N and N is the length of a feature vector v,
and η is a distance threshold.
The threshold η ﬁlters the values of the elements in the
SSM. We set η = 0 in this paper because this will give us a
complete representation. Θ(·) can be the Heaviside function
(i.e. θ(x) = 0, if x < 0, and θ(x) = 1 otherwise) and · is
some norm. Concretely, for a row vector v = (v1 , v2 , ..., vn ),
in which the component vi can also be a column vector such
that vi = (vi1 , vi2 , ..., vim )T , the self-similarity matrix can be
explicitely expressed by
⎛
⎞
0
d12 d13 · · · d1n
⎜ d21
0
d23 · · · d2n ⎟
⎜
⎟
Mη (v) = ⎜ .
(1)
..
..
.. ⎟
.
⎝ .
.
.
. ⎠
dn1

dn2

dn3

···

0

where di j can be the distance between vi and v j under some
distance metric q such that di j = vi − v j q . SSMs behave
differently under various distance norms for di j = vi − v j q
and the threshold η. We use the q -norm in this paper as our
distance metric, which is deﬁned by
m

1

di j = { ∑ |vik − v jk |q } q ,

(2)

k=1

where v is a m × n matrix. The motivation for using this
metric is that it provides ﬂexibility for modeling the motion
dynamics in a coherent manner. In particular, this metric

gives the Manhattan distance and Euclidean distance when
q = 1 and q = 2, respectively. Thus, given a threshold η and
a distance metric q, vector v ∈ Rd uniquely corresponds to a
self-similarity matrix Mvη .
Based on our skeletal model representation, we compute
the SSM for each pose pi in the motion sequence using
Eq.(1) and Eq.(2). Note that in Eq.(2), the two distance
metrics correspond to two types of SSMs. For q = 1 and
q = 2, we denote the corresponding SSM schemes M(1) (pi )
and M(2) (pi ), respectively, where pi is the pose at frame i.
3.2. Generating Motion Sequence Volume
In order to characterize the dynamics of the input
motion sequence based on the SSMs, we construct a
3D structure called Motion Sequence Volume (MSV) by
linearly concatenating the intermediate SSMs in temporal
dimension. We deﬁne the MSV for pose p under distance
metric q as
V (q) (p) = {M(q) (p1 ), M(q) (p2 ), ..., M(q) (pt )},

(3)

where t is the temporal dimension of the pose, and V (q) (p)
denotes the concatenation of the SSM of each individual
pose pi .
It can be observed that for different motion sequences, the
resulting MSVs might be of different temporal dimensions,
but they all have identical spatial dimension since their
components M(q) (pi ) are computed under the same skeletal
model conﬁguration. More speciﬁcally, the dimension of all
MSV is 13 × 13 × t in our method, where t denotes their
corresponding temporal dimension.
Fig.5 illustrates 5 different MSVs computed using
Euclidean metric for 5 motion sequences in Fig.4. We use
xyz axis to denote the spatiotemporal dimension of the
MSV structure. xy-plane encodes the spatial dimensions of
SSMs, z direction depicts the temporal dimension. As we
can see, the similarity and hence confusion between walk
and run can be largely reﬂected in their MSV structure.
Especially, we can observe that the yz-plane of both motions
have similar spatial patterns given their different motion
durations. Similar observations can be made for walk and
walkturn sequences.
3.3. MSV tensor decomposition
As the MSV is a characteristic, unique, and a symmetric
3D structure, we developed an iterative algorithm using the
tensor theory to extract the most compact and optimized
discriminative features from it. The motivation for this
subspace decomposition is twofold: ﬁrst, it reduces the
dimensionality of the problem by exploiting the redundancy
in the symmetric structure of the motion manifold; second,
it does not distort the manifold by imposing linearity as
it is common practice in existing dimensionality reduction
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1957

C. Sun, I. Junejo & H. Foroosh / Motion Retrieval Using Low-Rank Subspace Decomposition of Motion Volume

(a) Walk

(b) Run

(c) Jack jump

(d) Walkturn

(e) Cartwheel

Figure 4: Several selected Mocap motion sequences out of 12 motion classes from CMU Mocap dataset (walkturn, golf, fjump,
ﬂystroke, jjack, jump, cartwheels, drink, kick, walk, bend, run). Each motion sequence is performed by various actors. Each
pose is represented by a skeletal model consisting of 13 joints.

(a) Walk MSV

(b) Run MSV

(c) Jack Jump MSV

(d) Walkturn MSV

(e) Cartwheel MSV

Figure 5: The constructed Motion Sequence Volume for the selected 5 motion sequences in Fig.4. Each slice in xy-plane is the
Euclidean metric based SSM of dimension 13 × 13. The z direction represents the temporal dimension of the motion sequence.
Each volume structure is symmetric, encoding the internal dynamics of motion.
0.4

0.4

0.3

0.25

The primary vector U (1)

The primary vector U (1)

The primary vector U (1)

0.4

0.35

0.35

0.3

0.25

0.35

0.3

0.25

0.2
0.2

2

4

6
8
Tracked joint number

10

12

2

4

6
8
Tracked point number

10

2

12

4

6
8
Tracked point number

10

12

0.09

The secondary vector U (3)

The secondary vector U (3)

0.09
0.085
0.08
0.075
0.07
0.065
0.06
50

100
150
Frame number

The secondary vector U (3)

0.085

0.07
0.065
0.06
0.055
0.05
0.045

200

0.08
0.075
0.07
0.065
0.06
0.055
0.05

100

200
300
Frame number

400

500

50

100

150
200
Frame number

250

300

Figure 6: Comparisons for the decomposed primary vector U (1) and the secondary vector U (3) between 3 classes of motions.
The columns from left to right correspond to walk (6 samples), walkturn (8 samples), and cartwheels (7 samples), respectively.
The ﬁrst and the second row correspond to the comparisons for the primary vector and secondary vector, respectively. In each
column, the curves in the same color denote the same motion samples within identical class (Better to view in color).

c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1958

C. Sun, I. Junejo & H. Foroosh / Motion Retrieval Using Low-Rank Subspace Decomposition of Motion Volume

techniques. This latter motivation, is important, as also
veriﬁed experimentally later, for preserving the dynamics of
the motion in this process of dimensionality reduction.
If a tensor can be represented as an outer product of a
number of vectors, then this tensor is said to be of rank-1.
The rank of tensor A is deﬁned as the minimum number
of rank-1 tensors that sum up to A. To obtain an optimal
rank-1 decomposition of MSV, we propose an alternating
least-squares (ALS) method by optimizing the components
of the factorization of a given MSV in an iterative fashion
similar to [Kro83, KDL80].
Given a real Nth-order tensor A ∈ RI1 ×I2 ×···×IN ,
there exists a scalar λ and N unit-norm vectors
U (1) ,U (2) , · · · ,U (N) such that a rank-1 tensor
Aˆ = λU (1) ◦ U (1) ◦ · · · ◦ U (N) minimizes the least-squares
cost function
ˆ = A − Aˆ
f (A)

2

over the manifold of rank-1 tensors. This can be analyzed
using the Lagrange multipliers that yields the following
equations [DLDMV00]:
T

T

T

A ×1 U (1) · · · ×n−1 U (n−1) ×n+1 U (n+1) · · ·

this paper for the optimal decomposition. Although the rate
of convergence of this method has not yet been analyzed in
the literature, it is proved in [ZG01] that linear convergence
of ALS can be achieved in a neighborhood of the optimal
solution by GRQ. Hence, we propose the Algorithm 1 given
below for the optimal MSV decomposition.
Algorithm 1: MSV rank-1 decomposition
input : A 3-order tensor MSV A ∈ RI×I×K , where I is
the spatial dimension of SSM, and K is the
temporal dimension of A
output: Two vectors ρ and ε that minimize
A − λρ ◦ ρ ◦ ε 2 , where ρ ∈ RI , ε ∈ RK , and
ρ 2= ε 2=1
Initialize U 0 = [ρ(0) , ε(0) ]T ;
for t ← 0 to Nmaxiteration do
ρ(t+1) = A×2 ρ(t) ×3 ε(t) ;
ε(t+1) = A×1 ρ(t) ×2 ρ(t) ;
ρ(t+1) = ρ(t+1) / ρ(t+1) ;
ε(t+1) = ε(t+1) / ε(t+1) ;
λ(t+1) = A×1 ρ(t+1) ×2 ρ(t+1) ×3 ε(t+1) ;
end

T

×N U (N) = λU (n) ,
T

T

T

A ×1 U (1) ×2 U (2) · · · ×N U (N) = λ,

In Algorithm 1, the ×i for i = 1, 2, 3 denotes the
multiplication between a tensor and a vector in mode-i of
that tensor, whose result is also a tensor, namely,

U (n) = 1.
Our objective is to ﬁnd a rank-1 decomposition of MSV
such that there exists a scalar λ and three vectors U (1) ,U (2)
and U (3) that minimize the following cost function
(1)

min ∑ (ai jk − λUi
i, j,k

(2)

(3)

◦U j ◦Uk )2 ,

(4)

where ai jk denotes the MSV, a 3-order tensor. The ◦ is the
outer product operator for vectors, i and j are spatial mode
indices and i, j ∈ [1, I], where I is the spatial size of the MSV;
while K ∈ [1, T ], where K is the frame number for the MSV.
Since each vector U (1) ,U (2) and U (3) is determined only up
to a scale factor, we have
U (1)

2

= U (2)

2

= U (3)

2

B = A×i ρ ⇐⇒ (B) jk =

I

∑ Ai jk ρi .

i=1

Starting with random initial values for ρ and ε, the algorithm
alternately changes ρ (or ε) while ﬁxing the other one and
iteratively achieves the optimal decomposition. The iteration
stops when the difference between A and Aˆ arrives at
a sufﬁciently small value (i.e., 10−10 ), as illustrated in
Fig.7. We observe that when the iteration number is 5, it
is experimentally sufﬁcient to obtain a rank-1 tensor Aˆ =
λρ ◦ ρ ◦ ε that achieves the optimal decomposition of the
original MSV A.

= 1.
3.4. The decomposed vectors

On the other hand, MSV is symmetric in the spatial
dimensions, since its elements remain constant under any
permutation of the indices i and j, i.e. ai jk = a jik . Therefore
U (1) = U (2) .

(5)

For clarity of presentation, we denote U (1) ,U (2) as
ρ, and U (3) as ε, and we will call them the primary
vector ρ, and secondary vector ε, respectively. Under the
constraint of Eq.(5), the Eq.(4) can be solved by the
technique of Generalized Rayleigh Quotient (GRQ) [ZG01].
We adopt the alternating least squares algorithm (ALS) in

We now discuss the intuition for each decomposed vector
and how each vector can be used to interpret spatial-temporal
variations of human motion. In algorithm 1, ρ(t+1) is
updated using both ρ(t) and ε(t) by ×2 and ×3 operators,
leaving A unaffected in mode-1 dimension; while ε(t+1)
is updated using the two ρ(t) by ×1 and ×2 operators,
leaving A unaffected in mode-3 dimension. It is known
from tensor theory and symmetry property of the MSV
that the mode-1 and mode-2 of the MSV is identically
responsible for spatial dimension, and the mode-3 for the
temporal dimension. In other words, the primary vector ρ
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

C. Sun, I. Junejo & H. Foroosh / Motion Retrieval Using Low-Rank Subspace Decomposition of Motion Volume
Rank−1 approximation error

0

10

−5

10

−10

10

−15

10

−20

10

1

2

3

4

5
6
Iteration number

7

8

9

10

Figure 7: Rank-1 tensor decomposition error for MSV

1959

within the same class from their generated MSVs (Fig.5).
Their decomposed vectors U (1) and U (3) , however, clearly
demonstrate strong similarities given different execution
speeds and body joints moving patterns. Speciﬁcally, within
the same motion class, the primary vector U (1) of motion
samples largely overlap. It is worth noticing also that even
for motion samples from different classes, their primary
vectors may still overlap to some extent, but their secondary
vectors will instead largely capture the motion dynamics.
4. Similarity measure

mainly encodes the dynamics of the spatial variance of MSV
while the secondary vector ε mainly encodes the temporal
variance.
By using the algorithm, each motion sequence in
the database is transformed to three low-dimensional
vectors, namely, two identical vectors U (1) ,U (2) mainly
corresponding to the spatial dimensions of the motion
sequence, and another vector U (3) mainly corresponding to
the temporal dimension. The retrieval can then be performed
at the event level rather than at the frame level, which leads
to huge reduction in memory consumption and run time for
large-scale databases, while preserving the dynamics of the
motion manifolds, as shown in our experiments.
In order to illustrate the difference between the
decomposed vectors for motions from the same action
classes, we randomly chose 6 samples for walk, 8 samples
for walkturn, and 7 sample for cartwheel from the CMU
Mocap dataset. Samples within the same motion classes
have various frames and execution speeds, as shown in the
second row of Fig.6. Take the walk for example. Although
its 6 samples are performed by various actors under different
speeds, their decomposed vectors (U (1) and U (3) ), show
strong similarities. This is also true for the walkturn and
cartwheel classes. Especially, the cartwheel class involves
more complex body movements and orientation variations
than walk, and hence the decomposed secondary vectors
U (3) vary a lot compared with walk and walkturn. However,
we can still identify the striking similarities when comparing
their curves.
Another noticeable example is the comparison between
walk and walkturn. Logically, both motion classes are very
similar except that the walkturn motion class involves a
turning angle. In other words, the spatial conﬁgurations of
the body skeleton joints at a speciﬁc frame for both motion
classes are very similar but these spatial conﬁgurations vary
to some extent along the time dimension. Therefore, the
primary vectors for both motion classes are slightly similar.
However, the secondary vectors encode the subtle variations
between those two motion classes, as illustrated also in
Fig.6.
Note that it may not be straightforward to visualize
or compare directly the similarities of motion sequences
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

For motion retrieval, ﬁnding motion sequences that are
similar to a given query motion p is highly dependent
on the similarity measure. Variations in the same action
class are allowable as long as we can correctly identify
the matches. However, it is known that similar motions
may be numerically dissimilar because corresponding
poses may have various joint orientations and angular
velocities. As a result, traditional methods based on
aggregating frame-by-frame scores, often fail to distinguish
between numerical similarity and visual similarity. In other
words, they fail to differentiate motions that are different
versions of the same class of action [KG04]. On the
other hand, linear subspace decomposition methods that
achieve dimensionality reduction, distort the non-linear
motion manifold, and hence cause the opposite problem of
concealing the dissimilarity between two close, but different
classes of actions.
Let p(i) and p( j) be the two initial motion sequences. By
dimensionality reduction via rank-1 tensor decomposition,
we obtain their corresponding approximated vector pairs
(1)
(2)
(3)
(1)
(2)
(3)
v(i) = {Ui ,Ui ,Ui } and v( j) = {U j ,U j ,U j },

respectively. Speciﬁcally, for both v(i) and v( j) , their ﬁrst
two components are vectors of size 13, and the size of
the third component is equal to the motion sequence frame
number. We deﬁne two similarity metrics in this paper for
motion retrieval, namely the cross-correlation based metric
Dxcorr and the dynamic time warping based metric Ddtw ,
respectively. The Dxcorr is deﬁned as
Dxcorr (p(i) , p( j) ) =

3

(k)

∑ d(Ui

(k)

,U j ),

(6)

k=1

with
(k)

(k)

d = maxC(Ui ,U j ),
where k = 1, 2, 3 and r is the component dimension. The
function C returns the vector containing the cross-correlation
(k)
(k)
values between the components Ui and U j . For various
motion sequences, their execution frames are probably of
unequal length, leaving those two components of unequal
size, then we zero-pad the shorter vector to the length of the
longer vector. By this similarity measure, the more similar
(k)
(k)
Ui and U j are, the larger the value of D will be.

1960

C. Sun, I. Junejo & H. Foroosh / Motion Retrieval Using Low-Rank Subspace Decomposition of Motion Volume

Table 1: Comparison of retrieval rate (%) of various
methods from 4 selected motion classes
Ours using Ddtw
Ours using Dxcorr
Deng [DGL09]
Kovar [KG04]
Forbes [FF05]
Liu [LZWM05]

walk
93.0
95.7
92.1
82.0
74.4
80.0

run
95.2
97.2
98.0
85.3
80.0
93.1

jump
92.0
83.4
87.4
86.7
63.1
70.9

kick
100.0
86.8
78.0
75.2
65.5
63.9

Overall
95.1
90.8
88.9
82.3
70.8
76.9

The other metric Ddtw is deﬁned as
Ddtw (p(i) , p( j) ) =

3

(k)

∑ DTW (Ui

(k)

,U j ),

(7)

k=1

where the function DTW is used for measuring the
similarity between the two input sequences. Note that
although DTW-based comparisons are widely used in
motion retrieval, our usage here has particular advantage that
the computation is performed on our decomposed vectors
rather than on initial motion data as in conventional methods
[KG04].
5. Results and evaluations
We consider the motion sequences corresponding to
12 actions (walkturn, golf, fjump, ﬂystroke, jjack, jump,
cartwheels, drink, kick, walk, bend, run) from the CMU
mocap database. This amounts to a total of 196 motion
clips with the duration length ranging from 90 to 1000
frames per clip. The motion actions are chosen so that
they span various classes to illustrate the generality of
our technique. For example, the walk and run actions are
similar in nature representing different degrees of a single
locomotion class, whereas jump, kick, etc are distinct motion
types. All experiments were implemented with Matlab with
4GB memory and a 2.66 GHz Core2Duo processor.

CQk = {Q1 , Q2 , ..., Qk }, where we set k = 20 in this paper.
Our aim is to establish how many motions in CQk have the
identical motion class as f (q). For example, the walk and
run motions sometimes cannot be correctly differentiated
because they are visually different but numerically similar.
Intuitively, given a walk motion for querying, we want our
retrieved results have more candidates for walk rather than
run, and have less or even no candidates for golf motion.
To evaluate the retrieval accuracy of our approach, we
conducted experiments similar to the settings in [KG04]
and [DGL09]. Our objective was to query the same motion
in two different types of datasets. The ﬁrst one was the
labeled dataset Dlabeled with the same motion class, and the
second one was a mixture of unlabeled motions Dunlabeled
with various classes. The Dlabeled can be considered as
the ground-truth compared with the Dunlabeled . In order
to make comparison with the experiments in [KG04] and
[DGL09], we collected a motion dataset consisting of 4
motion categories, namely walk, run, jump, and kick, with
80 sequences and 4570 frames from the CMU mocap
dataset. Other motion categories were not included in this
experiment either because their numerical experimental
results were unavailable, or because they were tested in a
different dataset. The work in [MR06] used the True-Positive
Ratio as an accuracy criterion that was deﬁned as the
percentage of the top k retrieved candidates from the mixed
dataset Dunlabeled that were in the correct labeled dataset
Dlabeled . Here we also used this criterion and compared
our approach with several related works, as shown in Table
1. Those four motion categories were correctly retrieved
under the two similarity measures (Dxcorr and Ddtw ) at high
mean rates (90.8% and 95.1%). Especially, the dynamic
time warping based similarity measure outperformed the
corss-correlation based similarity measure in this testing
scenario.
5.2. Experiment 2

5.1. Experiment 1
To efﬁciently measure the similarity of the query motion
with motions in our dataset, we ﬁrst performed the motion
retrieval using the given query motion q under similarity
measure in both Eq.(6) and Eq(7). Since the degree of
similarity actually represents the relevance between the
queried motion and all retrieved candidate motions, all the
retrieved motion clips were sorted according to their degree
of similarity to the queried motion. In other words, for each
query, we maintained a sorted list consisting of all motions in
descending order denoted by Q = {Qi }, where i = 1, 2, ..., N
with N being the number of all motion sequences. Let C =
{c j }, where j = 1, 2, ..., J, be the set of motion classes.
Here J = 12 since we considered 12 motion classes. Let the
function f denote the correspondence between a motion Q j
and its class ci , namely ci = f (Q j ). For a query motion q, let
CQk be the set of classes for the top k candidate motions, i.e.,

It is known that confusion matrix is a good representation for
rate comparison, which contains information about actual
and predicted classiﬁed (or retrieved) data performed by a
classiﬁcation (or retrieval) system. For a retrieval system,
each column of this matrix represents the instances in
a retrieved motion class, while each row represents the
instances in an actual motion class. An important advantage
of a confusion matrix is that it is easy to see if the system
is confusing two classes (i.e. commonly mislabeling one as
another). For this reason, as an alternative of true-positive
ratio, the confusion matrix is adopted herein to evaluate the
efﬁciency of our method on more than 4 motion classes
compared with the previous experiment.
Below are the details of how the experiment was
conducted. Here, the input is only labeled motions instead
of the two separate datasets Dunlabeled and Dlabeled as in
the ﬁrst experiment. We considered not only the number
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

0.0

4.2

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

2.4

0.0

0.0

jjack

0.0

0.0

4.2

0.0

0.0

jump

0.0

0.0

0.0

0.0

0.0

cartwheels

0.0

12.5

drink

0.0

100.0 0.0 0.0 0.0
0.0 25.0 0.0 0.0 75.0 0.0 0.0
0.0 0.0 14.3 0.0 0.0 85.7 0.0
25.0 0.0 0.0 0.0 0.0 0.0 75.0

96.3 1.9 0.0 0.0
0.0 0.4 96.1 0.0 2.3
0.0 2.8 0.0 83.3 0.0
0.0 0.0 1.6 0.0 98.4

kick

0.0

0.0

walk

2.3

bend

0.0

run

0.0

0.0

jump

0.0

0.0

16.7

cartwheels

0.0

2.4

0.0

drink
kick

0.0
0.0

0.0
0.0

0.0
1.9

0.0

0.0

0.0

0.0

walk

0.8

0.0

0.4

0.0

0.0

0.0

0.0

bend

0.0

0.0

11.1

0.0

2.8

0.0

0.0

run

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

5.4

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

100.0 0.0 0.0 0.0
0.0 100.0 0.0 0.0
0.0 0.0 94.6 0.0
flystroke 0.0 0.0 0.0 100.0
0.0

0.0

ju
m

f ly

ru
n

ki

ju
m

f ly

97.6 0.0 0.0 0.0
0.0 0.0 79.2 0.0 0.0
0.0 2.4 0.0 95.2 0.0
0.0 0.0 4.2 0.0 83.3
0.0

0.0

golf

fjump

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

16.7

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

0.0

1961

ru
n

0.0

0.0

be
nd

0.5

ck

0.0

ki

0.0

p

0.0

jja
ck

5.9

st

0.5

0.0

walkturn

f ju
m

0.0

jjack

w
al
k

ca
rtw
he
el
dr
s
in
k

p

ro
ke

tu
rn
0.0

0.0

go
lf

0.0

0.0

w
al
k

be
nd

4.2

0.0

ck
0.0

0.0

p

0.0

0.0

95.8 0.0 0.0 0.0
0.0 100.0 0.0 0.0
fjump 0.0 0.0 93.2 0.0
flystroke 0.0 0.0 0.0 95.8

jja
ck

0.0

0.0

st

0.0

0.0

f ju
m

0.0

golf

go
lf

walkturn

w
al
k

w
al
k

ca
rtw
he
el
dr
s
in
k

p

ro
ke

tu
rn

C. Sun, I. Junejo & H. Foroosh / Motion Retrieval Using Low-Rank Subspace Decomposition of Motion Volume

100.0 0.0 0.0 0.0
0.0 0.0 93.0 0.0 4.7
0.0 0.0 0.0 83.3 0.0
0.0 0.0 4.8 0.0 95.2
0.0

Figure 8: The confusion matrices of motion retrieval for CMU mocap dataset using two similarity measures. (Left)
Cross-correlation based measure; (Right) Dynamic time warping based measure
of correctly retrieved motions, but also the classes of
mistakenly retrieved motions. Fig.8 shows the two confusion
matrices of mean retrieval rates using the similarity
measures in Eq.(6) and Eq.(7) for all the 196 motion clips
with 12 classes. The diagonal of this matrix denotes the
correct retrieval rate for a certain motion sequence, and
the off-diagonal elements denote the rate for mistakenly
retrieved classes. For the ﬂystroke motion in the ﬁrst
confusion matrix of Fig.8, for instance, 95.8% of all the
retrieved motion clips are correctly retrieved as ﬂystroke,
whereas only 4.2% are incorrectly retrieved as j jack motion,
and for the golf motion, 100% of all retrieved motions are
correctly identiﬁed. This means that, for the golf action our
method retrieves the motion with no confusion.
Since logically the walkturn and the walk are very
similar but numerically the former one involves a turning
angle while walking, as shown in Fig.6 and analyzed in
Section 3.4, it may not be easy to differentiate them using
conventional retrieval methods. But both confusion matrices
demonstrate that our method can nicely differentiate them
with very low confusion (< 5%). Another special pair
is the fjump and the jump. It is worth mentioning that
both logically and numerically those two categories are
extremely similar: the fjump involves jumping forward at
a certain distance; while the jump involves landing on the
same location. The two confusion matrices show that our
approach can still have somewhat acceptable retrieval rates.
Moreover, the overall mean retrieval rates of our method
in this experiment is 93.1% and 91.8%, respectively, for
the two similarity measures. It can be noticed that the
overall performance of cross-correlation based measure is
slightly better than the dynamics time warping measure, but
its performance falls behind when handling some motion
categories like walkturn, ﬂystroke, jjack, and kick.
5.3. Time and space complexity
Since we reduce each motion sequence to low-rank vectors
for similarity measure, the retrieval can be performed at
the event level rather than at the frame level. This can
largely reduce the run time for large-scale databases, while
preserving the dynamics of the motion manifolds, as shown
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

in our experiments. In particular, the time complexity
required to build and store our index structure is linear, O(n)
compared with DTW-based strategies, which are quadratic,
O(n2 ), with regards to the frame number n in the database.
Based on the mocap sequence terminology in Section
2.1, the space complexity for an original motion dataset of
m clips can be expressed as S = ∑m
i=1 3 × h × Ti , where h
is the number of joints in skeletal model (h = 13 in this
paper), and Ti is the temporal duration for the ith motion clip.
Using our low-rank subspace decomposition in Algorithm
1, the space complexity becomes S = ∑m
i=1 (h + Ti ) since the
length of U (1) , U (2) , and U (3) are h, h, Ti , respectively. The
compression ratio is thus
R = S /S =

m
1
+ .
3 ∑m
T
3h
i
i=1

As we can see, a smaller R corresponds to a larger saving
in space complexity. In order to reduce R, one can either
increase the quantity ∑m
i=1 Ti or h. Intuitively, the quantity
∑m
i=1 Ti is relatively large for large capture datasets. As
modern motion capture datasets grow increasingly large, our
technique becomes particularly suitable in the applications
where both retrieval efﬁciency and accuracy are required.
6. Discussion and conclusion
In summery, our technique transforms the initial motion
sequences into three vectors that are discriminating the
motion manifold based on its dynamics as a whole rather
than on frame-by-frame basis. The advantage of this
approach lies in the following aspects. First, no alignment
for individual poses is required when matching the query
pose with the one in the reference database. Second, for
comparing poses, which may be represented by a large
matrix when the motion time is large, our generated vectors
are merely three low dimensional vectors. This leads to
huge saving in motion sequence storage and processing.
Third, as discussed earlier, our method resolves motion
sequences while preserving non-linear characteristics, and
avoids confusions due to inter-class similarity, or intra-class
dissimilarity.

1962

C. Sun, I. Junejo & H. Foroosh / Motion Retrieval Using Low-Rank Subspace Decomposition of Motion Volume

This paper has thus presented an efﬁcient method for
retrieval of motion capture data that in particular solves the
problem in cases when other methods typically get confused.
One main contribution of this work is a new framework for
motion retrieval without explicit time alignment compared to
the traditional methods. Also, compared with the traditional
DTW-based approaches, our technique only requires storing
a collection of compact reference vectors generated by
our iterative algorithm rather than complete sequences of
motion. Of course, our method can be adopted as an efﬁcient
complement to the DTW-based approaches in scenarios
where exact frame-by-frame alignment is also required.
References
[BC04] B LASCO R., C ARMEN M.: Synchronization analysis
by means of recurrences in phase space.
PhD thesis,
Universitatsbibliothek, 2004. 4
[BCD04] B ENA BDELKADER C., C UTLER R., DAVIS L.: Gait
recognition using image self-similarity. EURASIP Journal on
Applied Signal Processing 2004 (2004), 572–585. 2, 4
[BSP∗ 04] BARBI Cˇ J., S AFONOVA A., PAN J., FALOUTSOS C.,
H ODGINS J., P OLLARD N.: Segmenting motion capture data
into distinct behaviors. In Proceedings of Graphics Interface
2004 (2004), Canadian Human-Computer Communications
Society, pp. 185–194. 3
[CD00] C UTLER R., DAVIS L.: Robust real-time periodic
motion detection, analysis, and applications. Pattern Analysis
and Machine Intelligence, IEEE Transactions on 22, 8 (2000),
781–796. 2, 4
[CVB∗ 03] C ARDLE M., V LACHOS M., B ROOKS S., K EOGH E.,
G UNOPULOS D.: Fast motion capture matching with replicated
motion editing. In ACM SIGGRAPH (2003), Citeseer. 1
[DGL09] D ENG Z., G U Q., L I Q.: Perceptually consistent
example-based human motion retrieval. In Proceedings of the
2009 symposium on Interactive 3D graphics and games (2009),
ACM, pp. 191–198. 2, 8
[DLDMV00] D E L ATHAUWER L., D E M OOR B.,
VANDEWALLE J.: On the Best Rank-1 and Rank-(R˜ 1,
R˜ 2,..., R˜ N) Approximation of Higher-Order Tensors. SIAM
Journal on Matrix Analysis and Applications 21, 4 (2000),
1324–1342. 6
[FF05] F ORBES K., F IUME E.: An efﬁcient search algorithm
for motion data using weighted PCA. In Proceedings of the
2005 ACM SIGGRAPH/Eurographics symposium on Computer
animation (2005), ACM, pp. 67–76. 3, 8
[HWX∗ ]

H U Y., W U S., X IA S., F U J., C HEN W.: Motion
track: Visualizing variations of human motion data.
In
Paciﬁc Visualization Symposium (PaciﬁcVis), 2010 IEEE, IEEE,
pp. 153–160. 3

[KG04] KOVAR L., G LEICHER M.:
Automated extraction
and parameterization of motions in large data sets.
In
ACM Transactions on Graphics (TOG) (2004), vol. 23, ACM,
pp. 559–568. 1, 7, 8
[Kov04] KOVAR L.: Automated extraction and parameterization
of motions in large data sets. ACM Transactions on Graphics 23
(2004), 559–568. 1
[Kro83] K ROONENBERG P.: Three-mode principal component
analysis: Theory and applications. DSWO press, 1983. 6
[KTWZ10] K RÜGER B., TAUTGES J., W EBER A., Z INKE
A.:
Fast local and global similarity searches in large
motion capture databases. In Proceedings of the 2010 ACM
SIGGRAPH/Eurographics Symposium on Computer Animation
(2010), SCA ’10, pp. 1–10. 2
[Lin06] L IN Y.: Efﬁcient human motion retrieval in large
databases. In Proceedings of the 4th international conference
on Computer graphics and interactive techniques in Australasia
and Southeast Asia (2006), ACM, pp. 31–37. 2
[LZP07] L I C., Z HENG S., P RABHAKARAN B.: Segmentation
and recognition of motion streams by similarity search. ACM
Transactions on Multimedia Computing, Communications, and
Applications (TOMCCAP) 3, 3 (2007), 16–es. 2
[LZWM05] L IU G., Z HANG J., WANG W., M C M ILLAN L.: A
system for analyzing and indexing human-motion databases. In
Proceedings of the 2005 ACM SIGMOD international conference
on Management of data (2005), ACM, pp. 924–926. 8
[LZWP03] L IU F., Z HUANG Y., W U F., PAN Y.: 3D motion
retrieval with motion index tree. Computer Vision and Image
Understanding 92, 2-3 (2003), 265–284. 1
[MR06] M ULLER M., RODER T.:
Motion templates for
automatic classiﬁcation and retrieval of motion capture data.
In Proceedings of the 2006 ACM SIGGRAPH/Eurographics
symposium on Computer animation (2006), Eurographics
Association, pp. 137–146. 8
[MRC05] M ULLER M., RODER T., C LAUSEN M.: Efﬁcient
content-based retrieval of motion capture data.
In ACM
Transactions on Graphics (TOG) (2005), vol. 24, ACM,
pp. 677–685. 1
[SH07] S AFONOVA A., H ODGINS J.: Construction and optimal
search of interpolated motion graphs. In ACM SIGGRAPH 2007
papers (2007), ACM, pp. 106–es. 1
[SKK04] S AKAMOTO Y., K URIYAMA S., K ANEKO T.: Motion
map: image-based retrieval and segmentation of motion data.
In Proceedings of the 2004 ACM SIGGRAPH/Eurographics
symposium on Computer animation (2004), Eurographics
Association, pp. 259–266. 1
[YKSN07] YASUDA H., K AIHARA R., S AITO S., NAKAJIMA
M.: Motion belts. In ACM SIGGRAPH (2007), pp. 05–09. 3
[ZG01] Z HANG T., G OLUB G.: Rank-1 approximation of
higher-order tensors. SIAM J. Matrix Anal. Appl 23, 534-550
(2001), 4. 6

[JDLP10] J UNEJO I., D EXTER E., L APTEV I., P ÉREZ
P.:
View-independent action recognition from temporal
self-similarities. IEEE transactions on pattern analysis and
machine intelligence (2010), 172–185. 2
[Joh73] J OHANSSON G.: Visual perception of biological motion
and a model for its analysis.
Attention, Perception, &
Psychophysics 14, 2 (1973), 201–211. 3
[KDL80] K ROONENBERG P., D E L EEUW J.:
Principal
component analysis of three-mode data by means of alternating
least squares algorithms. Psychometrika 45, 1 (1980), 69–97. 6
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

