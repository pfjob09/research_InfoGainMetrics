DOI: 10.1111/j.1467-8659.2011.01912.x
Eurographics / IEEE Symposium on Visualization 2011 (EuroVis 2011)
H. Hauser, H. Pfister, and J. J. van Wijk
(Guest Editors)

Volume 30 (2011), Number 3

Curve Density Estimates
O. Daae Lampe1,2 and H. Hauser2

2 Institute

1 Chr. Michelsen Research, Norway
of Informatics, University of Bergen, Norway

sin(x)

0 1 2

x

Figure 1: The Curve Density Estimate of a high frequency sine curve with a normalized histogram of evaluated values densely
sampled along the x axis. Our continuous representation of this curve closely matches that of the histogram.
Abstract
In this work, we present a technique based on kernel density estimation for rendering smooth curves. With this
approach, we produce uncluttered and expressive pictures, revealing frequency information about one, or, multiple
curves, independent of the level of detail in the data, the zoom level, and the screen resolution. With this technique
the visual representation scales seamlessly from an exact line drawing, (for low-frequency/low-complexity curves)
to a probability density estimate for more intricate situations. This scale-independence facilitates displays based
on non-linear time, enabling high-resolution accuracy of recent values, accompanied by long historical series for
context. We demonstrate the functionality of this approach in the context of prediction scenarios and in the context
of streaming data.
Categories and Subject Descriptors (according to ACM CCS):
Generation—Line and curve generation

1. Introduction
In the context of time-dependent data the drawing of function graphs is one of the most natural and at the same time
one of the most effective data visualization techniques. As
long as the spatial complexity of the graph is limited, this
immediate translation of data into a graph is straight forward and provides intuitive results. If the curve to draw,
however, becomes very long or the spatial complexity increases, for example when considering a fractal curve, then
the simple plotting of such a curve or graph will likely result
c 2011 The Author(s)
Journal compilation c 2011 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

I.3.3 [Computer Graphics]: Picture/Image

in problems with overdraw and cluttering. This overdraw in
an example graph led us to the question: Why did our regular graph of a sine curve look so different when its samples
where drawn in a scatterplot instead? (see Figure 2 a and
c). The scatterplot, when drawn with transparency, resembles the histogram of these values, as shown in Figure 3.
This histogram shows the distribution of the evaluated values, but the curve representation completely obscures this
distribution, even when applying transparency, as shown in
Figure 2b. In this paper, we investigate an alternative way of

634

O. Daae Lampe & H. Hauser / Curve Density Estimates
1.8
1.6
1.4
1.2
1.0
0.8
0.6

a)

0.4
0.2
0.0
−1.0

−0.5

0.0

0.5

1.0

Figure 3: 30 bins histogram of y = sin(x) for regularly sampled values of x.
b)

c)

d)

f)
Figure 2: Figures displaying the sine curve from zero to
1000π. In the top figure, a, an opaque line is used, and because of overdraw, displays only the extent of the function.
In the second figure, b, a transparent line is used. The third
figure, c, is a scatter-plot of the samples drawn transparent,
and shows the same distribution as the histogram. The fourth
figure, d, is aggregated with moving mean, standard deviation and extent. As opposed to Figure 4, this data is unsuitable for this type of aggregation. In the bottom figure, our
technique, the Curve Density Estimate, is applied, and the
distribution corresponds with that found in the histogram in
Figure 3.

rendering such curves, that does not display the same problems, but keep the clear benefits of the regular curve.
As, perhaps, a very trivial summary, a function graph displays a single measured value, on one axis, with its continuous changes over another axis. Unless the changes are
piecewise continuous, a curve is not an appropriate choice
of visualization.
One of the biggest challenges when drawing function
graphs is that they are mainly useful for displaying frequencies that, on the extreme, is at least greater than the pixel
with of the display. The common way to deal with this
is to either constrain/zoom in on the axis, or to aggregate
values, to "smooth" out rapid changes. In Figure 4, aggre-

Figure 4: Intel opening stock price with a moving average,
standard deviation and extent.

gated stock prices for Intel are shown, with the black curve
being the center-shifted moving mean, enveloped in the
curves’ standard deviation. The outer polygon is the moving max-min. This enveloped curve, as described by Miksch
et al. [MSHP99], captures the overall movement of the underlying data very well, and its standard deviation polygon
reveals important information about the frequency or stability of the smoothed data. This method works best when
the data is close to the normal distribution (or at least unimodal).In the case of a bi- or multimodal data distribution,
however, this aggregation looses certain expressiveness. The
visualized mean can easily associate with highly improbably data values, for example, in the middle between two
modes. Moreover, the moving mean relies on a certain window, which either provides lagging results or it is undefined
for the latest values.
As a simple example we consider a sine curve from zero
to a number larger than the amount of available pixels in the
horizontal direction. A naïve approach to display this curve
is shown in Figure 2a, which suffers from overdraw, and
would mainly only display the extent of the curve. A first
approach on how to solve this problem could be to apply
transparency, shown in Figure 2b. The transparency could
correctly display the amount of overdraw, but this does not
correctly display the distribution of the curve. The distribution of a curve, alternatively described as its continuous histogram by Bachthaler and Weiskopf [BW08], is found by
taking regular samples along its parameter axis, and inserting evaluated values in a histogram. The histogram for the
sine curve is shown in Figure 3. It is worth noting that the
curve with transparency will have a single visible mode at
zero, which is almost the opposite of what the histogram indicates, with two modes at one and minus one.
c 2011 The Author(s)
Journal compilation c 2011 The Eurographics Association and Blackwell Publishing Ltd.

O. Daae Lampe & H. Hauser / Curve Density Estimates

A second technique on how to deal with this high frequency sine curve could be to first aggregate it. In Figure 2d,
we show, similar to Figure 4, the moving average, standard
deviation, and extent. Using a sufficiently large window, the
average of the sine curve is stable at zero, and its standard
deviation is constant. Part of the problem is that this way a
model, with a normal distribution, is enforced onto the data.
If there is a mismatch between the assumed model and the
data, such a visualization will not be expressive.
With our technique applied to the sine curve, as shown in
Figure 1 and Figure 2f, we recreate the distribution without
any prior knowledge of model, and will do so independent
of frequency, zoom level and screen resolution. That said,
we do not propose to replace the aggregation techniques, as
in Figure 4 or other techniques, where the model is known,
but provide a default view, that can either be used before the
model is established (if it exist), or to investigate how well a
selected model fits the data.
With this paper we introduce a novel way of displaying
function graphs, that also supports:
• Graphs with a frequency higher than the pixel-width of its
display
• Smooth transition between high frequency areas and single line curve
• The creation of a probability density estimate that does
not assume a normal distribution of values
• The probability density of both single and multiple curves
(and a mix between those)
In the following, we first discuss related work, then move
on to the theoretical details of curve density estimates, before we add technical implementation details. Lastly, we apply our technique to real world data before providing the
summary and conclusions.
2. Related Work
Existing techniques, improving or extending the curve, fall
briefly into the categories: compact views, overdraw views
and, distribution views.
Compact Views: By utilizing techniques to compress the
value axis, the aspect ratio is improved such that longer timeseries can be shown on less space without contracting the
time axis, and thus avoid the frequency problem. Saito et
al. [SMY∗ 05] designed a compact graph view, that utilizes
a colored banding to overlay multiple ranges of the curve
on top of each other. Using this banding, which was further refined into the horizon graph by Panopticon [Rei08],
a precise value can be read out, while reducing the physical
height down to an eighth. When the value range of a process is known, and also can be defined in levels, such as low,
normal and high, all values in these ranges can be replaced
with colors to produce a compact visualization, as described
by Bade et al. [BSM04]. They provide an interesting examc 2011 The Author(s)
Journal compilation c 2011 The Eurographics Association and Blackwell Publishing Ltd.

635

ple of body temperature graphs, where there are clearly defined normal levels. Another compact graph visualization are
the Sparklines as introduced by Tufte [Tuf06], which strips
the curve down to a text-line sized graph, that can even be
included mid-text. As a separate thread of compact visualization techniques, is the pixel based category, where each
sample is displayed using a colored pixel. In 2008, Hao et
al. [HKD∗ 08] provided an evaluation on how best to place
such pixels, while keeping temporal coherence. While not
quite a compact view, Kincaid proposed combining high frequency time series with a focus+context interaction [Kin10].
This interction provides both an overview, and a detailed
view down to the individual samples.
Overdraw Views: Techniques that deal with visual clutter of high frequency by introducing schemes to blend multiple overdraws. Most visualization packages allows the
user to specify different opacities, effectively implementing
an overdraw view. In 2002, Jerding and Stasko introduced
the Information Mural [JS02] to deal with high frequency
graphs and other cluttered 2D visualizations. This technique
downscales large, original, and uncluttered views to miniatures, while counting the overdraws to each pixel. This overdraw count is then used to apply a greyscale color.
Distribution Views: Techniques that by aggregation
deduce the distribution of a single, or multiple curves.
Hochheiser and Shneiderman [HS04] utilized envelopes that
displayed the full extent of curves, in their TimeSearcher
application. In 2004, Kosara et al. [KBH04] described the
TimeHistogram, where the time axis was divided into intervals, and a separate histogram was calculated for each of
these intervals. These histograms, with colored 1D representations, was then in turn displayed along the time axis. In
the work by Muigg et al. [MKO∗ 08] multiple curves where
binned while aggregating both the count and the directions,
to create a visualization close to that of a flow field, utilizing
line integral convolution (LIC) to overlay direction on top
of the frequency. Bade et al. [BSM04] introduced an extension to the information mural [JS02], that adds the median,
the 25 and 75 percentiles and extent. Which is similar to
BinX [BM04] which visualizes long time series by binning
along the time axis at different levels of aggregation and
then displays mean, minimum, maximum value, and standard deviation per bin. Johansson et al. [JLC07] discussed
a blending scheme to introduce temporal changes in parallel coordinate plots (PCP). Their implementation aggregates
continuous changes on top of each other forming, what can
be described as a continuous 1D histogram from discrete
samples. This 1D histogram is then the basis for creating
a polygon that is drawn to the next axis in the PCP. Feng
et al. [FKLI10] also introduced an extension of PCP, that
is the result of mapping the 2D KDE between each axis,
into its corresponding parallel coordinate version. Furthermore, Feng et al. [FKLI10] also introduced several enhancements to interaction and brushing techniques to better suite
frequency data.

636

O. Daae Lampe & H. Hauser / Curve Density Estimates

The technique proposed in this paper is an extension of
our previous work [LH11], where we first introduced the
concept of a line kernel to kernel density estimation. The line
kernel is used to reconstruct continuous changes, by connecting consecutive samples, forming an elongated kernel,
and that integrates up to one independent of the distance between samples. In this work we extend this line kernel, and
show how to reduce it into an exact and continuous, parametric formulation. In our previous work, we utilized a table
of pre-integrated convolved results, whereas this extension
allow us to directly evaluate the exact result. Additionally, in
this work, we introduce a curve visualization, called curve
density estimates (CDE), that provide distributional characteristics along the time axis, comparable to the concept of a
continuous 1D KDE. This visualization is enabled by a moving column based normalization scheme further detailed in
Section. 3. Several other extensions are also provided here,
over our previous work, e.g., non-linear time, single curve to
multiple curves transition.
3. Curve Density Estimates (CDE)
The main rationale behind the use of kernel density estimation (KDE) as a basis is that it does not impose any model
on the data. Given a discrete set of samples, with an appropriate bandwidth and kernel, KDE can truthfully approximate any probability density estimate (PDE). For an extensive overview of KDE we refer to Silverman [Sil86]. The
KDE is defined as the sum of a number of kernels, one kernel
per sample. With (x1 , x2 , ..., xn ) being samples corresponding to an unknown density f , the according KDE is defined
as
1 n
fˆh (x) = ∑ Kh (x − xi )
n i=1

=

1 n
x−x
∑K h i ,
nh i=1

(1)

with K a suitable kernel. As the kernel, often the normal distribution, N(x) =
2

√1
2πσ2

− (x−µ)
2σ2

e

2

, is used, with µ being the

mean, σ the variance, and h the bandwidth, or kernel size.
The blue vertical graph to the bottom right of Figure 5
shows an 1D KDE. This KDE is created from the set of
points where the black curves intersect the green line in the
upper-left graph in the same figure. This 1D KDE clearly reveals the bimodal nature of this dataset. Our curve density
estimates (CDE) in the lower graph in Figure 5, can be interpreted as a continuous series of these 1D KDEs. However,
instead of modeling our solution by expanding 1D KDEs,
we find a solution via a 2D KDE. A standard 2D KDE can
be created using the unconnected sample-points from the
dataset, inserted into Eq. 1. This approach will not create
a continuous distribution when the samples get far apart, but
rather be the distribution of the previously mentioned scatterplot (2c). The consecutive samples in the time series represent a continuous change from one value to the next, and
thus the probability, given two samples, should not be 0.5
at each sample, but rather be distributed evenly from one to

the next. We achieve this by building upon a line kernel Lk
defined by two consecutive data samples, and their positions
pi and pi+1 [LH11]:
Lk (x) =

1
0

ci KH x − ((1 − φ)pi + φpi+1 ) dφ,

(2)

with KH being the 2D normal distribution kernel. To enable a
proper reconstruction from uneven sampling in time, we insert the elapsed time between the two samples in the scaling
factor ci .
We now can reduce Eq. 2 to 1D by only considering values on the line defined by pi and pi+1 . We name this 1D
equation Lk1D (x). Furthermore we define the 2D points pi
and pi+1 to their 1D equivalents (they are per definition on
this line), qi and qi+1 , respectively. This 1D line kernel is
then defined as the integral of Gaussians placed along a line
segment. So for any point x, we observe that Lk1D (x) is defined by the sum of these kernels, and that all those kernels
incrementally have a mean/µ that is greater and greater than
x. By turning this problem around, we deduce that the integral on one position of kernels with its mean moving away,
is equal to the finite integral over a single kernel. The integral of the normal distribution is a cumulative distribution
function (cdf). This distribution function is defined by
cdf(x, µ, σ) =

x−µ
1
1 + erf √
2
2σ2

.

(3)

Considering a point x where x < q1 , and for explanation purposes a q2 → ∞. At this point x, f (x) = cdf(x, q1 , σ), since
it is equal to the unbound integral of all the kernels starting from q1 towards ∞. However, since q2 is actually a finite value and we do not have any contribution from kernels
beyond this point, we have to remove this from our equation. The contribution from all kernels starting at q2 going
towards ∞ is similarly, f (x) = cdf(x, q2 , σ). We then conclude, that for the two points q1 and q2 , where q1 < q2 , the
line kernel, in 1D, is given by:
Lk1D (x) =

1
cdf(x, q1 , σ) − cdf(x, q2 , σ) ,
|q2 − q1 |

(4)

with |q2 − q1 |, the length between these points, applied for
normalization, since
cdf(x, q1 , σ) − cdf(x, q2 , σ)dx = q2 − q1 .

(5)

One important quality of this line kernel is when q1 approaches q2
lim Lk1D (x) = N(x)

q1 →q2

(6)

the line kernel approaches the normal distribution, N(x), an
observation which was also previously made by Kniss et
al. [KPI∗ 03].
The next step, is to expand this 1D line kernel, over to
our 2D case again. In our previous work [LH11], we relied
c 2011 The Author(s)
Journal compilation c 2011 The Eurographics Association and Blackwell Publishing Ltd.

O. Daae Lampe & H. Hauser / Curve Density Estimates

637

Figure 5: 100 cumulative random curves with a slight bimodal trend. The top graph show the curves with slight transparency.
All the samples at the green line, at x = 90, are drawn using a histogram and a 1D KDE in to the right. The graph to the bottom
shows the CDE. Note how the 1D KDE corresponds to the green line drawn over the CDE as well.

on the product kernel to define the line kernel. Here, to expand our 1D parametric line kernel to 2D, we also rely on
a product kernel, but we let the first kernel be our 1D line
kernel, and the other, the normal distribution. Let w be the
point x projected onto the line L defined by p1 and p2 , i.e.,
w = x + |r · (x − p1 )|r, with r a unit vector perpendicular to
the line. Then let u be the distance |p1 − w| and v the distance |x − w|. This then gives the new and parametric 2D
definition of the line kernel:
Lk (x) = ci Lk1D (u) · N(v)

(7)

The KDE using our line kernel that will continuously reconstruct the sample points (p1 , p2 , ..., pn ) is defined by
fLk (x) =

n−1

∑ Lk (x, pi , pi+1 )

(8)

i=0

By evaluating this KDE we get a density field with the integral
fLk (x)dx =

n

∑ ci ,

(9)
4. Technical Details

i=1

since individually, all kernels integrate up to one, but are
scaled by ci . Usually, to create a probability density estimate, we would normalize by this integral, but instead we
propose to normalize it after rasterization, and then only column by column, individually. Given the 2D grid, G, evaluated by fLk (x), we create a column-normalized grid Gn by,
h

Gn [i, j] = G[i, j]/ ∑ G[i, jˆ],

where h denotes the height of the grid. Figure 6a, displays two curves, that coincide before separating, and Figure 6b the rasterized result, after this column-wise normalization. The rationale for this normalization is to, first, have
an intuitive number indicating how much time the curves
spent where, and secondly be able to interpret every column as a 1D probability density estimate. A one indicating that all the curves was here 100% of the time, and 0.5,
50%, regardless of how many curves are used, and importantly, if non-linear time is used, regardless of how compressed time is. Utilizing this normalization will render single curves, with small changes, with the most intense values from the chosen color map, and also effectively applying anti-aliasing. However, when the curve reside in the
same column, with large changes, e.g., when zooming out,
or showing long time-series, the normalization will give the
probability density/continuous histogram of where the curve
"spent its time". Figure 7 shows two different curves, and
their rasterized results after rendering them into a single column.

(10)

jˆ=0

c 2011 The Author(s)
Journal compilation c 2011 The Eurographics Association and Blackwell Publishing Ltd.

While we see the usefulness of this technique in off-line renderings for static displays, we will, in this section, first address the challenges we face when either rendering real-time
streaming data, or need interactivity, e.g. zooming and panning. In order to overcome the challenges and achieve good
frame-rates, we offload all the calculation to the GPU. When
rendering large time-series, we store vertex arrays, containing the samples, in GPU residing memory, in chunks sorted
temporally. Because of this temporal ordering, we perform

638

O. Daae Lampe & H. Hauser / Curve Density Estimates

a)

b)
xx

p₁ p₂

a)

b)

c)
x

p₁

x

0

0

0.1 0.2 0.4 0.5 0.5 0.5

1

1

0.8 0.6 0.2

0

0

0.1 0.2 0.4 0.5 0.5 0.5

0

0

0

p₁ p₂

x

x

p₁x

p₃

x

p₃

p₄

x

p₄

Figure 8: The three different proxy geometry schemes for
reconstructing line kernels. The circles indicate the vertices
needed. The colored curves below, depicts the evaluated kernel in the corresponding cross-sections above (not normalized).

Figure 6: Two coinciding curves splitting up, in a, and
the rasterized result after normalization, in b. Note that all
columns sum up to one.

y

0.4

y

1/6

0.1

1/6

0.01

1/6

0.01

1/6
1/6

0.1

x

0.4

since the line kernel Lk is in effect the sum of multiple normal distributions, and its edge will follow the cdf function.
This line kernel consists of two quads, meeting at the centerpoint between the points (pi + pi+1 )/2. This case is depicted
in Figure 8b.

x

1/6

Figure 7: Two different curves, on the left, and their corresponding views, on right, after being rescaled down to one
column.

an intersection test, with the chunks’ span vs. that of the
view, to determine if it should be rendered or not. The chunk
containing the most recent values, however, are stored in
main memory, to be able to constantly append new values
to it. To minimize the memory usage, the memory structure only contains the samples once, and the proxy geometry, needed by the line kernels, is constructed in a geometry
shader step. The geometry shaders input are the consecutive
samples, using the GL_LINE_STRIP_ADJACENCY, and it
will output the proxy geometry in three distinct ways.
In the first case, a single, unconnected kernel is constructed as a quad. This case is used when the two consecutive samples’ distance is less than a threshold |pi −pi+1 | ≤ ε,
and that threshold, expressed in terms of pixels, should be
one. In our experience, however, there is no significant visual change when increasing ε to three pixels (given that the
bandwidth is larger than that as well). This case is depicted
in Figure 8a.
In the second case, when ε < |pi − pi+1 | ≤ cσ2 , we have
a line kernel, but one that have a single mode, or maxima.
This occurs for c ≈ 5 (see our previous work [LH11] for a
discussion on this clamping, and for the errors introduced),
when distance is approx. five times that of the bandwidth,

The third case, is when the distance between the points is
sufficiently large, i.e., |pi − pi+1 | > cσ2 , to have a "flat" region between them. In this case the proxy geometry consists
of three quads, two for the end caps, and one for the continuous region in between. This case is depicted in Figure 8c. In
this case, we can simplify the calculation of the line kernel
Lk , to only include the normal kernel perpendicular to the
centerline, for the middle segment.
Another usage for the geometry shader, in addition to the
three previous cases, is applied when non-linear axes are
used. The geometry shader evaluates, according to the distance between the two points, the error introduced by a single linear kernel, and performs a subdivision if needed.
Now, after the geometry is constructed, the kernels evaluate Eq. 7, and their sum is stored in a 32bit floating point
texture. The fragment shader, parameterized with u and v, respectively, along and across the geometry, evaluating Eq. 7,
calculates N(u) using a table lookup, and the Lk1D (v) using existing erf hardware accelerated GLSL implementation.
Then, two steps remain, namely, the column normalization,
and the application of a color-map. Before the normalization of the columns, however, we first need to calculate the
sum per column. We store these sums in a 1D floating point
texture, with the same width as the source image, the 2D
rasterized grid. To calculate the sum, we first bind the 1D
texture as a frame buffered object (FBO), as the render target, and then in a fragment shader, iterate over all texels in
the corresponding column of the 2D texture, calculating the
sum. As the last step, we apply the normalization division
per fragment, while simultaneously applying the color-map.
c 2011 The Author(s)
Journal compilation c 2011 The Eurographics Association and Blackwell Publishing Ltd.

O. Daae Lampe & H. Hauser / Curve Density Estimates

5. Applications
In this section we cover a varied set of applications for the
technique of curve density estimates (CDE). With these applications we show both the generality of the proposed technique while also highlighting specific areas where this technique can outperform the current state of art. Some of these
examples are also covered as videos in the supplemental material to demonstrate the interactivity.
5.1. High Frequency Curves
A high frequency curve has significant amplitude fluctuations within short spans, in terms of the visualized area.
Sound is an excellent example for high frequency curves.
In Figure 9 we display the CDE of the waveform from
Beethovens Symphony No. 5. The top image in this figure
displays five minutes and 34 seconds, with almost 15 million samples. We use this example specifically, to show our
techniques independence from zoom level. Where the top of
Figure 9 showed over five minutes, the next zoom level, in
the second graph, spans three seconds. In the third graph, we
can directly see the curve, as this graph spans 50 milliseconds. We show this as three discrete zoom levels in this figure, but while interacting with the application, this zooming
action is both seamless and smooth. The second and the bottom graph, in Figure 9, both show the same timespan of an
interesting piece where a bassoon makes an intricate pattern.
This pattern is however, completely lost in the bottom graph,
which is the default waveform viewer in Audacity [Aud].
This intricate pattern is made, in part, by the curve seen in
the third zoom level. In this zoom level we see that the curve
has two distinct repeating peaks, one with a single mode and
the other with two modes, and it is these modes that make
up the intricate pattern.
5.2. Prediction Curves
Alan Cox once said:
I figure lots of predictions is best. People will forget the ones I get wrong and marvel over the rest.
which, somehow, nicely fit the scheme on how modern
weather forecasts are done. Instead of a single prediction, an
ensemble of possible futures are outlined. However, when a
forecast is prepared for public display, it is most often reduced to a single, most likely outcome. When the ensemble
of curves spread out, forming a normal distributed pattern,
the mode, can correctly be presented as the likely outcome,
and the variance, can be presented as the prediction certainty.
However, when the ensemble spreads out with two modes,
as shown in Figure 5, this model breaks down. We suggest
two different use cases for CDE in prediction. In the first
case, real-time data is combined with prediction ensemble
curves. The historical data will appear as a solid line, and
at the most recent sample, an ensemble of curves will possibly spread out, defining the density estimator of the future
c 2011 The Author(s)
Journal compilation c 2011 The Eurographics Association and Blackwell Publishing Ltd.

639

outcome. This solid line, representing the measured observations is shown in Figure 5, where at x = 5 the CDE spreads
out into the distribution of future outcomes. By using the
full ensemble, rather than the best represented outcome, the
operator can also prepare for worst case scenarios, if their
probability reaches a given threshold.
The next use case for prediction is repeating, or cyclic
patterns. We can find one such cyclic pattern in the yearly
temperature. In Figure 10 we show temperature readings,
per hour for ten full years by the weather station at Flesland in Bergen, Norway. Data courtesy of eKlima [Nor]. The
temperatures are drawn as ten overlapping curves, using Microsoft Excel in the lower graph. In the middle graph, the
moving average over the temperature for all years, and its
standard deviation is aggregated and shown. The top graph
show the CDE for these ten years. By choosing a specific
date, the vertical column there represent the probability of
which temperatures are likely at that date (according to history). For example, in early June we can see that the probability for the temperature for the EuroVis event is spread out
from seven to approximate 18 degrees (given both night and
day temperatures). Another way to interpret this CDE, is by
looking at the intensity of the highest mode. The higher the
mode, the more stable temperature, and vice versa. Historically the temperature is more unstable (less likely to predict a correct outcome) in the winter months, Nov. through
Feb. One interesting finding here is the disparity between
the mean, as shown in the second graph in Figure 10 and the
CDE, for Nov. and Dec., probably due to the less normality
of the distribution here.
5.3. Process Visualization
In process visualization, the priorities are often first placed
on understanding the now, the current situation, followed by
both prediction and understanding the historical data. A visualization that receives streaming data, should both emphasize the most recent data, while providing an overview of
historical data. Streaming data are, often shown in a temporal window with the most recent values in one end, and the
historical data towards the other end. A suitable temporal
window is selected, which must be sufficiently small to see
the current values, and then data older than this are removed.
Figure 11 shows an example use of our CDE with non-linear
time, which serves both to emphasize the recent values, to
the left, and provide a historical overview that fades into an
aggregated probability density estimate.
In drilling, as in many other processes that produce data,
we find several data sources that produce bi or multi-modal
data. In Figure 12 we show one such example, where the
hook-load over time is showed. This image resembles that
in our previous work [LH11], but here it displays time over
hook-load, while the other displayed depth over hook-load.
Hook load is measured in tonnes, and behaves in a bimodal
fashion because the hook is either lifting the entire drill

640

O. Daae Lampe & H. Hauser / Curve Density Estimates

Figure 9: Beethoven, Symphony No. 5 shown using CDE on the full waveform. The top image show five minutes and thirty
seconds. The gray box in this top figure shows the timespan zoomed into for the second figure. The second figure shows an
interesting feature, spanning three seconds, where a bassoon is playing. The third figure spans 50 milliseconds, zoomed into
from the second figure. The bottom figure shows the same span as the second, but using Audacity’ viewer [Aud]

6. Summary and Conclusions

Figure 11: By compressing time with a semi-logarithmic
scale, a high level of detail can be read out on recent values,
while an overview is available. The logarithmic exponent is
given on the x axis.

string, or when the drill string is attached in slips to the
platform, is zero (actually the weight of the hook itself approx. 40 tonnes). This figure shows the progress over six
hours, and we can quickly read out that most of the time has
been spent with the drill string attached to the slips, since
the mode is highest at 40 tonnes. The second finding, is the
spans where the hook load was only at 40, meaning that the
operation stalled, and precious time was lost.

We have described the need for a visualization that can represent curves independent of frequencies, zoom level and models, which does not yet exist in the current state of the art.
We provide a novel technique on how to render curves independent of sampling-rate, zoom level and curve frequencies.
Since kernel density estimates does not impose any model on
the distribution, our solution will correctly display data with
a single mode, bimodal, tri-modal, or indeed with any underlying model. We have provided implementation details,
to promote the usage of this technique in both interactive
and real-time settings. Furthermore we have provided several compelling examples of real world usage, showing both
where this technique can improve current usages of visualization, but also the versatility of this as a general technique.
For future work, we intend to see how we can use the described technique to provide aid in modeling of data, providing immediate feedback on model suggestions. Furthermore
we plan to apply this technique into the daily usage for process visualization, and establish its performance with a user
study.
c 2011 The Author(s)
Journal compilation c 2011 The Eurographics Association and Blackwell Publishing Ltd.

641

O. Daae Lampe & H. Hauser / Curve Density Estimates

Jan.

Feb.

Mar.

Apr.

May

June

July

Aug.

Sep.

Jan.

Feb.

Mar.

Apr.

May

June

July

Aug.

Sep.

Oct.

Oct.

Nov.

Nov.

Dec.

Dec.

Figure 10: Temperature readings by station nr. 50500 at Flesland, Norway, years 2000 through 2009 with month on the x axis.
The top image using CDE, the middle image smoothing using a moving mean and standard deviation, and output by Microsoft
ExcelTM on the bottom. Notice that the uncertainty/spread of temperature is greater in the winter months Nov. to Feb., than the
rest of the year, shown clearly in the CDE. June contains the most stable temperature, represented as the high density there.

Figure 12: Process data from a drilling operation showing hook-load in tonnes over time in seconds. The right view shows the
moving mean, standard deviation and extent, which for this bimodal distribution works particularly bad. The left view displays
the curve density estimate of the same data.

7. Acknowledgements
The work presented here is a part of the project “e-Centre
Laboratory for Automated Drilling Processes” (eLAD), participated by International Research Institute of Stavanger,
Christian Michelsen Research and Institute for Energy Techc 2011 The Author(s)
Journal compilation c 2011 The Eurographics Association and Blackwell Publishing Ltd.

nology. The eLAD project is funded by grants from the Research Council of Norway (Petromaks Project 176018/S30,
2007-2011), StatoilHydro ASA and ConocoPhillips Norway.

642

O. Daae Lampe & H. Hauser / Curve Density Estimates

References
[Aud] AUDACITY:
The Free Audio Editor and Recorder.
audacity.sourceforge.net. [Online; accessed Nov2010]. 7, 8
[BM04] B ERRY L., M UNZNER T.: BinX: Dynamic exploration
of time series datasets across aggregation levels. In Proc. IEEE
InfoVis 2004 (2004), pp. 215–216. 3
[BSM04] BADE R., S CHLECHTWEG S., M IKSCH S.: Connecting time-oriented data and information to a coherent interactive
visualization. Proceedings of the SIGCHI conference on Human
factors in computing systems (2004), 105–112. 3

[Sil86] S ILVERMAN B.: Density Estimation for Statistics and
Data Analysis. Chapman & Hall/CRC, 1986. 4
[SMY∗ 05] S AITO T., M IYAMURA H. N., YAMAMOTO M.,
S AITO H., H OSHIYA Y., K ASEDA T.: Two-tone pseudo coloring: Compact visualization for one-dimensional data. Proc. IEEE
Symp. on Inf. Visualization 2004 (InfoVis 2005) (2005), 173–180.
3
[Tuf06] T UFTE E.: Beautiful evidence, vol. 23. Graphics Press
Cheshire, CT, 2006. 3

[BW08] BACHTHALER S., W EISKOPF D.: Continuous scatterplots. Proc. IEEE Visualization 2008 14, 6 (2008), 1428–1435.
2
[FKLI10] F ENG D., K WOCK L., L EE Y., I I R.: Matching Visual
Saliency to Confidence in Plots of Uncertain Data. IEEE Transactions on Visualization and Computer Graphics 16 (2010), 980–
989. 3
[HKD∗ 08] H AO M., K EIM D., DAYAL U., O ELKE D., T REM BLAY C.: Density Displays for Data Stream Monitoring. Computer Graphics Forum 27, 3 (2008), 895–902. 3
[HS04] H OCHHEISER H., S HNEIDERMAN B.: Dynamic query
tools for time series data sets: timebox widgets for interactive
exploration. Proc. IEEE InfoVis 2004 3, 1 (2004), 1–18. 3
[JLC07] J OHANSSON J., L JUNG P., C OOPER M.: Depth cues
and density in temporal parallel coordinates. Proceedings
of Eurographics/IEEE-VGTC Symposium on Visualization 7
(2007), 35–42. 3
[JS02] J ERDING D., S TASKO J.: The information mural: A technique for displaying and navigating large information spaces.
Proc. IEEE Visualization 2002 4, 3 (2002), 257–271. 3
[KBH04] KOSARA R., B ENDIX F., H AUSER H.: Timehistograms for large, time-dependent data. Joint Eurographics–
IEEE TCVG Symposium on Visualization (2004). 3
[Kin10] K INCAID R.: Signallens: Focus+context applied to electronic time series. Proc. IEEE Visualization 2010 16 (2010),
900–907. 3
[KPI∗ 03] K NISS J., P REMOZE S., I KITS M., L EFOHN A.,
H ANSEN C., P RAUN E.: Gaussian transfer functions for multifield volume visualization. In Proc. IEEE Visualization 2003
(2003), pp. 497–504. 4
[LH11] L AMPE O. D., H AUSER H.: Interactive Visualization of
Streaming Data with Kernel Density Estimation. In Proceedings
of the IEEE Pacific Visualization Symposium 2011 (accepted for
publication) (March 2011). 4, 6, 7
[MKO∗ 08] M UIGG P., K EHRER J., O ELTZE S., P IRINGER H.,
D OLEISCH H., P REIM B., H AUSER H.: A four-level focus+context approach to interactive visual analysis of temporal
features in large scientific data. Computer Graphics Forum 27, 3
(may 2008), 775–782. 3
[MSHP99] M IKSCH S., S EYFANG A., H ORN W., P OPOW C.:
Abstracting steady qualitative descriptions over time from noisy,
high-frequency data. AIMDM ’99 Proceedings of the Joint European Conference on Artificial Intelligence in Medicine and Medical Decision Making (1999), 281–290. 2
[Nor] N ORWEGIAN M ETEOROLOGICAL I NSTITUTE:
eklima.met.no. [Online; accessed Nov-2010]. 7

eKlima.

[Rei08] R EIJNER H.: The development of the horizon graph. In
IEEE Visualization Workshop: From Theory to Practice: Design,
Vision and Visualization (2008). 3
c 2011 The Author(s)
Journal compilation c 2011 The Eurographics Association and Blackwell Publishing Ltd.

