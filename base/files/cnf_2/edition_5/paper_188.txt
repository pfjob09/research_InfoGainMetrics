DOI: 10.1111/j.1467-8659.2011.02042.x
Pacific Graphics 2011
Bing-Yu Chen, Jan Kautz, Tong-Yee Lee, and Ming C. Lin
(Guest Editors)

Volume 30 (2011), Number 7

Rephotography Using Image Collections
Kun-Ting Lee

Sheng-Jie Luo

Bing-Yu Chen

National Taiwan University, Taiwan

(a)

(b)

(c)

Figure 1: This paper presents a rephotography technique that reproduces a photograph at the same viewpoint of a reference
historical photograph. (a) The reference historical photograph. (b) A number of existing photographs which do not exactly
match the viewpoint of the reference photograph. (c) Our rephotography result.
Abstract
This paper proposes a novel system that “rephotographs” a historical photograph with a collection of images.
Rather than finding the accurate viewpoint of the historical photo, users only need to take a number of photographs
around the target scene. We adopt the structure from motion technique to estimate the spatial relationship among
these photographs, and construct a set of 3D point cloud. Based on the user-specified correspondences between
the projected 3D point cloud and historical photograph, the camera parameters of the historical photograph are
estimated. We then combine forward and backward warping images to render the result. Finally, inpainting and
content-preserving warping are used to refine it, and the photograph at the same viewpoint of the historical one is
produced by this photo collection.
Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Picture/Image
Generation—; I.4.9 [Image Processing and Compute Vision]: Applications—.

1. Introduction
Rephotography, an act of taking a photograph from the same
viewpoint of the same scene of a reference historical photograph, is very useful while presenting the evolution of a
specific location, a building or a city. It provides good materials for studying history. If an historical photograph and
a modern one of the same scene can be aligned well, one
clearer way to visualize the scene’s past-and-now is putting
them together and adjusting their transparency. However,
it is very challenging for a photographer to find the accuc 2011 The Author(s)
⃝
c 2011 The Eurographics Association and Blackwell PublishComputer Graphics Forum ⃝
ing Ltd. Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ,
UK and 350 Main Street, Malden, MA 02148, USA.

rate viewpoint manually because it includes six degrees of
freedoms (DOFs). Therefore, precise rephotography techniques have been scientifically studied for a long time. Recently, Bae et al. [BAD10] proposed a real-time estimation
technique for rephotography that guides users to the desired viewpoint for taking the photograph. However, sometimes the users may have no chance to go back to the place
to take the photograph, and most people are not very easy
to be guided. Therefore, rephotography using existing pho-

1896

Lee et al. / Rephotography Using Image Collections

tographs or videos of the reference scene could be a very
useful post-process approach.

the same scene into a 3D space as well as constructing the
structure of the scene.

In this paper, we present a rephotography technique that
“rephotographs” a historical photograph with a number of
modern photographs. It takes the historical photograph as the
reference, and renders an image that has the same view by
the modern photographs. Therefore, users only need to take
a number of photographs at the same scene without a careful study of the reference photograph in advance. The set of
photographs are combined together to construct a set of 3D
point cloud of the scene by using the structure from motion
(SfM) technique. Then we analyze the parameters of the reference photograph and render an initial rephotography result
by using the image-based rendering (IBR) techniques. Finally, inpainting and content-aware warping techniques are
adopted to refine it.

Image-Based Rendering. Image-based rendering (IBR) is a
technique that renders novel views directly from a set of input images [SK00]. According to the geometry model used,
it can be classified into three categories: rendering with no
geometry, rendering with implicit geometry and rendering
with explicit geometry. The first category, rendering with
no geometry, uses no geometry information at all. Without
geometry, the sampling must be very dense, or the possible motion is restricted [SH99, Lip80, GGSC96, LH96]. The
second category, rendering with implicit geometry, relies on
positional correspondences across a small number of images to render new views. The term implicit implies that
the geometry is not directly available [CW93, SD96, LF94].
The third category is rendering with explicit geometry,
which has direct 3D information (i.e. the representation of
3D coordinates or depth along a sight line). The previous
works [DYB98, DTM96] render novel views using viewdependent texture maps. [SGHS98] stores several depth layers for every pixel to acquire the depth information. Lipski et al.proposed an image-based free-view point system
[LLB∗ 10] based on multi-image morphing. Unsynchronized
multi-view recordings can be used as input, and then their
system can interpolate both spatial and temporal varying
viewpoints. Zheng et al. [ZCA∗ 09] computes the depth map
by merging the multi-view stereo and segmentation algorithms. With an iterative correction process, they can obtain
a rough depth map and render the novel view. The rendering
result with the explicit geometry is more reliable and precise; therefore, our method is also based on it. In our work,
we compute the depth map with 3D space projection, which
will be introduced later.

Specifically, our contributions are as follows. (1) We propose a technique for rephotographing the historical photos
while user-specified viewpoint is not necessary. (2) A hybrid technique that combines the image-based rendering and
warping for generating the accurate view.
2. Related Work
Rephotography. Rephotography has become a popular research topic for a long time in photography field, and has
acquired more attention in computer vision recently. Given
a reference photograph of a scene, the goal is to take a photograph exactly from the same viewpoint as the reference
one. Bae et al. [BAD10] proposed a novel system that allows users to go back to the scene, and guides them to
reach the desired viewpoint based on the reference photograph. The system is useful because even amateur users can
rephotograph a scene according to the guidance. However,
sometimes users may have no opportunity to go back to the
place to take the photo. Instead of providing the guidance,
our system adopts IBR techniques to render a novel view of
the scene based on a set of existing photographs. Therefore,
users do not need to reach the accurate viewpoint of the reference photograph when taking the photographs. To utilize
the historical photographs, Schindler and Dellaert [SDK07]
build time-varying 3D models of cities from several historical and modern photographs. Then users can transit in the
3D space when browsing these photos.
Structure from Motion. Estimating the camera position and
reconstructing the scene via multiple images are core problems in computer vision. Structure from motion (SfM) is a
popular technique [Har97, Har92], which can automatically
recover the camera motion and scene structure from two or
more images. SfM can be used in a wide range of applications including the reconstruction of virtual reality models
from video sequences, photogrammetric survey and special
effect of movie. Photo tourism [SSS06] adopts this technique, and computes the camera parameters from a collection of photos from the web. It integrates all photographs of

Multi-View Reconstruction. The multi-view stereo (MVS)
problem, which aims to construct a 3D model from a collection of images, has become more and more popular recently.
Seitz et al. [SCD∗ 06] categorizes MVS algorithms into
four types of approaches: 3D volumetric techniques [HK06,
TD06], surface evolution techniques [ES04, PKF05], depth
map based techniques [GCS06, SFVG06] and feature/seed
points based techniques [GSC∗ 07,LPK07]. Our approach is
similar to the depth map based techniques, which compute
the dense depth estimates first.
3. System Overview
Figure 2 summarizes the whole process of our system. Given
a collection of photographs taken at the same scene of a
reference photograph, the system first analyzes the camera
parameters as well as constructs a 3D point cloud of the
scene by performing the SfM technique [SSS06] over them
(Section 4). Based on the user-specified corresponding features between the reference photograph and 3D point cloud,
the camera parameters of the reference photograph are estimated, and the depth map of each photograph is also comc 2011 The Author(s)
⃝
c 2011 The Eurographics Association and Blackwell Publishing Ltd.
⃝

1897

Lee et al. / Rephotography Using Image Collections

User-specified
Matched Features

Camera
Calibration

Blending
Inpainting

Camera Calibration for
Historical Photograph

3D Point Cloud
Construction

Contentpreserving
warping

Forward warping
Depth Map
Estimation

Scene
Reconstruction

3D Point Cloud
&
Depth map

Backward warping

Initial
Rephotograph

Reprojection

Hole Filling &
Perspective
Refinement

Rephotography Process

Input

Output

Figure 2: An overview of the proposed method.
length, radial distortion coefficient, rotation matrix, transformation matrix of each camera, and a set of 3D points with
color information are extracted in this step.
4.2. Segmentation-based Depth Map Estimation
(a)

(b)

(c)

(d)

Figure 3: An example of depth map estimation. (a) One of
the photographs. (b) The segmentation result of (a). (c) & (d)
The depth maps without and with using neighbors’ features.
puted. The system then renders a novel view which matches
the viewpoint of the reference photograph by projecting the
3D point cloud onto it. The depth map of the novel view
is also computed simultaneously, and the pixels of the novel
view are reprojected back to each input photograph to generate an initial result (Section 5). Finally, inpainting [CPT03]
and content-preserving warping [LGJA09] are performed to
handle the scene structure mismatch due to the error (Section 6).
4. Scene Reconstruction
Given a set of photographs taken at the same scene, the camera parameters of these photographs are estimated through
camera calibration, and the 3D point cloud of the scene is
constructed from them. Then the depth maps of these photographs are estimated.

In this step, the goal is to estimate the depth map of each
photograph. First, each input photograph is segmented by
[CM02, MG01] as shown in Figure 3 (b). The depth values
in the same segment are assumed to be smooth. Next, the
3D point cloud is projected onto each input photograph, so
there are several projected points in each segment. Then, we
assign the depth values of the projected points with the zaxis distance under the camera coordinate. Specifically, we
estimate the depth value of each pixel pi as:
D(pi ) =
where

Wi =

1
Wi

∑

f j ∈Sk

∑

f j ∈Sk

d( f j )
,
|| f j − pi ||

(1)

1
,
|| f j − pi ||

f j is the projected point of the j-th point in the point cloud,
Sk is the k-th segment, and d(·) is the depth value.
The generated depth map is shown in Figure 3 (c). There
are some pixels that have no depth value (red area) because
there is no 3D point projected on these segments. To solve
this problem, we use the projected points of their neighbor
segments as its projected points, and compute the depth values as the same way. However, the depth values of a segment may be wrong because of projection errors. Therefore, we compute the color difference between projected
points c p and average color of the segment cs . If it satisfies
∥c p − cs ∥ < σ (in our experiment, σ = 15), the depth value
of the projected point d(p) is credible. Figure 3 (d) shows
the result.
5. Reprojection

4.1. Camera Calibration and Point Cloud Construction
To reconstruct the 3D structure of the scene covered by the
set of photographs, we estimate the intrinsic and extrinsic
matrices of each camera, and generate the 3D point cloud
of the scene using the SfM technique [SSS06]. The focal
c 2011 The Author(s)
⃝
c 2011 The Eurographics Association and Blackwell Publishing Ltd.
⃝

In this section, we describe the details of the historical-view
photograph generation. It is done by estimating the camera
parameters of the historical photograph. Then the 3D point
cloud of the scene constructed before are projected to the
historical-view camera to render an initial result.

1898

Lee et al. / Rephotography Using Image Collections

d1
3D Point Cloud

Figure 4: The SIFT algorithm fails when matching the historical photograph and a modern photograph of “Patheon”.

Figure 6: Some points are projected onto the same pixel.

3D Point Cloud

Photograph 1

Historical-view
Virtual Camera

d2

Historical-view
Virtual Camera

Photograph 2

Figure 5: Every pixel of the photos is “unprojected” into the
3D space, and projected to the historical-view camera.
5.1. Camera Calibration for the Historical Photograph
The method we used to calibrate the reference photograph is
similar to [BAD10]. In [BAD10], they estimate the extrinsic and intrinsic matrices by minimizing the sum of squared
projection error of the matched points between the reference and existing photos. The matched features in their work
are chosen from the SIFT correspondences [Low04]. However, there are some potential problems if the reference photo
is noisy. Figure 4 shows a failure case of SIFT correspondence. Too many feature correspondences are incorrect due
to the differences of photograph qualities, noise and perspective. Therefore, we let the user directly specify the projected point correspondences between the modern and historical photographs. To compute the camera parameters of
the historical photograph, the initial guess of the camera parameters are required, e.g.,focal length. We apply the focal
length of the camera used to specify the correspondences as
the initial guess, and the rest initial guesses are set similarly
to [BAD10]. Then we adopt the Levenberg-Marquardt algorithm to estimate the camera parameters of the historical
photograph.
5.2. Forward Warping
Now, we have all camera parameters of each photograph and
its depth map, so we can unproject every pixel on the photos
to 3D space as shown in Figure 5. Next, we project all the 3D
points onto the historical viewpoint in terms of the reference
camera pose we estimated in the previous step. However,
there may be several points projected on a single pixel as
shown in Figure 6. To reduce the influences of occlusion, we
favor the color and depth values of the closer points. There-

(a)

(b)

Figure 7: The results of (a) forward warping and (b) backward warping.
fore, we assign different weights to them based on the distance between the point and reference virtual camera. The
weight function is designed as:
C(pi ) =
where

1
Wi

Wi =

∑

C(Pj )

Pj ∈pro j(pi )

1
,
D(Pj )

(2)

1
,
D(P
j)
P ∈pro j(p )

∑

j

i

pi is the i-th pixel on the reference view plane, pro j(pi ) is
the set of points projected onto pi , Pj is the j-th 3D point,
D(·) is the projected distance, and C(·) is the color information. Figure 7 (a) shows the result of forward warping.
Besides, we can obtain a corresponding depth map of the
historical photograph by the following equation:
d(pi ) = min {D(Pj )|Pj ∈ pro j(pi )},

(3)

where d(·) is the depth value and D(·) is the projected distance. This equation means that the depth values are decided
by the distance of the closest projected point, and Figure 8
(a) shows the result. Obviously, there are a large number of
noises on the depth map. Hence, those noises are removed by
median filter as shown in Figure 8 (b). In addition, because
the points projected on sky or ground are rare or even none,
the depth value of sky or ground may be wrong. Therefore,
the sky detection [HEH05] is performed before computing
the depth map of the reference photo, so the error can be
reduced as shown in Figure 9.
5.3. Backward Warping
With the depth map and camera pose of the historical photograph, we then unproject every pixel on the historical viewc 2011 The Author(s)
⃝
c 2011 The Eurographics Association and Blackwell Publishing Ltd.
⃝

Lee et al. / Rephotography Using Image Collections

1899

NTU
CKHS Patheon St. Paul
# of photos
16
14
11
44
Resolution
855×570 535×356 1024×768 700×567
Depth map(sec.) 5.558
14.72
33.18
28.62
Ref. resolution 600×450 400×265 465×300 400×283
Ref. view(sec.)
10.93
5.38
7.78
5.34
(a)

(b)

Figure 8: Depth maps of the historical-view photograph (a)
before and (b) after smoothing.

(a)

(b)

Figure 9: Forward warping (a) without and (b) with sky detection.

Table 1: Information of the experimental data and computation time.
6.2. Image Inpainting
Although we have the forward and backward warping images, there are probable holes in the result image (red areas
in Figure 11 (a)) because of occlusion, projection error or
lack of scene structure information. Inpainting [CPT03] is
adopted to fill such holes, and Figure 11 (b) shows the result. The pixels in the holes are selected one by one in a
best-first algorithm. Then we search the whole image to find
a squared patch that is the most similar to the patch centered
at the selected pixel.
6.3. Content-preserving Warping

point to the 3D space and reproject them onto existing photos
(Figure 10) to acquire the color information on them. Then,
we average the pixel colors to fill in the image. Figure 7 (b)
shows the result of backward warping.
6. Hole Filling and Perspective Refinement
Though we have both of the forward and backward warping
photos of the reference viewpoint, they both have some holes
and a little bit distortion when comparing to the reference
photo as shown in Figure 7.
6.1. Blending
Since we have both of the forward and backward warping
photos, we first blend these two photos together to fill some
holes. In our implementation, the holes of backward warping
image are filled with the forward warping image. Figure 11
(a) shows the result.

3D Point Cloud

Photograph 1

Historical-view
Virtual Camera

Photograph 2

Figure 10: “Unprojecting” every pixel of the historical-view
to the 3D space and reprojecting onto each camera viewpoint of the photographs to acquire the color information.
c 2011 The Author(s)
⃝
c 2011 The Eurographics Association and Blackwell Publishing Ltd.
⃝

If we take photographs at the same viewpoint with different
cameras, the photographs look different in perspective from
others. Even the camera pose of the historical photograph
is estimated accurately, the rendered result may be different
from the historical one (Figure 11 (b)). Therefore, a contentpreserving warping technique [LGJA09] is adopted to adjust
mismatch of the scene structure. We first construct a grid
mesh on the result image. Then the corresponding points between the result image and historical photograph specified
by the user are taken as constraints. More specifically, we
warp the mesh such that the user-specified points of the result image locate exactly at the position of the corresponding
points of the historical photograph. Figure 11 (c) shows the
original grid mesh, and (d) shows the warped grid mesh. The
warped result is shown in Figure 11 (e).
7. Result and Discussion
Table 1 shows the information of our experimental data and
computation time, which includes the number of photos,
their resolutions, depth map generation time, resolution of
the historical photograph, and result generation time. All results are generated on a laptop PC with an Intel Core2 Duo
T6500 2.1GHz CPU and 4GB RAM, and the photographs
are taken by Canon EOS 500D.
Figure 12 shows some rephotography results. The first
row shows “the main gate of National Taiwan University
(NTU)”. The estimated viewpoint is very close to the actual viewpoint of the reference photo. Details of the result
looks a little blurring. It’s due to the blending of pixel colors
that are projected from the 3D space. When several points
are projected on a single pixel, the closest point to the projection plane is not taken directly, but all pixels contribute

1900

Lee et al. / Rephotography Using Image Collections

(a)

(b)

(c)

(d)

(e)

Figure 11: (a) The result of blending the forward and backward warping photos. (b)The result after performing inpainging.
(c) The original grid mesh. (d) The warped grid mesh. (e) The result after performing content-preserving warping. Note that
perspective of the wall alters obviously in the yellow rectangles of (b) and (e).
a portion according to their distances to the plane. The second row shows the “Ruins of St. Paul” case, which is collected from the Internet. In this case, the correctness of the
viewpoint we estimated is also good. Because of diverged
viewpoint angles of the photographs on the Internet, we
may not retrieve all information in the scene of the reference photo. If the photo sequence lacks too much information of the scene in the reference photo, the holes would be
very large (red area), and unable to be restored by inpainting. The third row shows “the red building of Chien-Kuo
High School (CKHS)”. In this case, some structures are distorted because of the content-preserving warping. Adding
straight line constraints would be helpful for structure preserving. Some fragmentary artifacts are visible due to imperfect depth map. If the depth map is estimated more accurately, the rephotography result could be improved significantly.
Figure 13 shows a comparison of our result with its
ground-truth. We took a number of photographs in front of
“the administration building of NTU”, and selected one of
them as the ground-truth reference photograph (Figure 13
(a)). The remaining photographs in the collection (Figure 13
(b)) are used to generate the result as shown in Figure 13 (c).
Figure 13 (d) shows the difference between our rephotography result and the ground-truth. When the red is darker, it
indicates that the error is bigger. The comparison shows that
our rephotography result is similar to the ground-truth.
8. Conclusion and Future Work
In this paper, we proposed a technique which automatically
rephotographs by user-specified feature matching between a
set of existing photographs and a reference photograph. According to the 3D point cloud and camera parameters estimated by SfM, the depth maps of the existing photographs
are first estimated. We then estimate the camera pose of the
historical photo based on the SfM result and user-specified
correspondences. In addition, the artifacts produced by the
wrong depth value of the sky and ground are reduced using
sky detection. The technique helps users to produce rephotography results even when they cannot go back to the original place to take photos. Future research directions include

Figure 12: The rephotography results. The cases from top to
bottom are NTU, Ruins of St. Paul, and CKHS. Left column:
The reference photos. Right column: Their rephotography
results.
the improvement of the precision of the depth map, which
is one of the critical parts of our technique. We believe our
work will benefit the computer vision, IBR and computational photography.
References
[BAD10] BAE S., AGARWALA A., D URAND F.: Computational
rephotography. ACM TOG 29, 3 (2010), 24:1–24:15. 1, 2, 4
[CM02] C OMANICIU D., M EER P.: Mean shift: A robust approach toward feature space analysis. IEEE TPAMI 24, 5 (2002),
603–619. 3
[CPT03] C RIMINISI A., P EREZ P., T OYAMA K.: Object removal
by exemplar-based inpainting. In Proc. IEEE CVPR (2003),
vol. 2, pp. 721–728. 3, 5
[CW93]

C HEN S. E., W ILLIAMS L.: View interpolation for imc 2011 The Author(s)
⃝
c 2011 The Eurographics Association and Blackwell Publishing Ltd.
⃝

1901

Lee et al. / Rephotography Using Image Collections

(a)

(b)

(c)

(d)

Figure 13: A comparison of our rephotography result and the ground-truth which is selected from the same image collection. (a)
The ground-truth image. (b) The image collection. (c) Our rephotography result. (d) The error image which shows the difference
between (a) and (c).
age synthesis. In Proc. ACM SIGGRAPH (1993), pp. 279–288.
2

[Low04] L OWE D. G.: Distinctive image features from scaleinvariant keypoints. IJCV 60, 2 (2004), 91–110. 4

[DTM96] D EBEVEC P. E., TAYLOR C. J., M ALIK J.: Modeling
and rendering architecture from photographs: a hybrid geometryand image-based approach. In Proc. ACM SIGGRAPH (1996),
pp. 11–20. 2

[LPK07] L ABATUT P., P ONS J.-P., K ERIVEN R.: Efficient multiview reconstruction of large-scale scenes using interest points,
delaunay triangulation and graph cuts. In Proc. IEEE ICCV
(2007), pp. 1–8. 2

[DYB98] D EBEVEC P., Y U Y., B ORSHUKOV G.: Efficient
view-dependent image-based rendering with projective texturemapping. In Proc. EGWR (1998), pp. 105–116. 2

[MG01] M EER P., G EORGESCU B.: Edge detection with embedded confidence. IEEE TPAMI 23, 12 (2001), 1351–1365. 3

[ES04] E STEBAN C. H., S CHMITT F.: Silhouette and stereo fusion for 3d object modeling. CVIU 96, 3 (2004), 367–392. 2
[GCS06] G OESELE M., C URLESS B., S EITZ S. M.: Multi-view
stereo revisited. In Proc. IEEE CVPR (2006), vol. 2, pp. 2402–
2409. 2
[GGSC96] G ORTLER S. J., G RZESZCZUK R., S ZELISKI R., C O HEN M. F.: The lumigraph. In Proc. ACM SIGGRAPH (1996),
pp. 43–54. 2

[PKF05] P ONS J.-P., K ERIVEN R., FAUGERAS O.: Modelling
dynamic scenes by registering multi-view image sequences. In
Proc. IEEE CVPR (2005), vol. 2, pp. 822–827. 2
[SCD∗ 06] S EITZ S. M., C URLESS B., D IEBEL J., S CHARSTEIN
D., S ZELISKI R.: A comparison and evaluation of multi-view
stereo reconstruction algorithms. In Proc. IEEE CVPR (2006),
vol. 1, pp. 519–528. 2
[SD96] S EITZ S. M., DYER C. R.: View morphing. In Proc.
ACM SIGGRAPH (1996), pp. 21–30. 2

[GSC∗ 07] G OESELE M., S NAVELY N., C URLESS B., H OPPE
H., S EITZ S. M.: Multi-view stereo for community photo collections. In Proc. IEEE ICCV (2007), pp. 1–8. 2

[SDK07] S CHINDLER G., D ELLAERT F., K ANG S. B.: Inferring
temporal order of images from 3D structure. In Proc. IEEE CVPR
(2007), pp. 1–7. 2

[Har92] H ARTLEY R. I.: Estimation of relative camera positions
for uncalibrated cameras. In Proc. ECCV (1992), pp. 579–587. 2

[SFVG06] S TRECHA C., F RANSENS R., VAN G OOL L.: Combined depth and outlier estimation in multi-view stereo. In Proc.
IEEE CVPR (2006), vol. 2, pp. 2394–2401. 2

[Har97] H ARTLEY R. I.: In defense of the eight-point algorithm.
IEEE TPAMI 19, 6 (1997), 580–593. 2
[HEH05] H OIEM D., E FROS A. A., H EBERT M.: Automatic
photo pop-up. ACM TOG 24, 3 (2005), 577–584. 4

[SGHS98] S HADE J., G ORTLER S., H E L.- W., S ZELISKI R.:
Layered depth images. In Proc. ACM SIGGRAPH (1998),
pp. 231–242. 2
[SH99] S HUM H.-Y., H E L.-W.: Rendering with concentric mosaics. In Proc. ACM SIGGRAPH (1999), pp. 299–306. 2

[HK06] H ORNUNG A., KOBBELT L.: Hierarchical volumetric
multi-view stereo reconstruction of manifold surfaces based on
dual graph embedding. In Proc. IEEE CVPR (2006), vol. 1,
pp. 503–510. 2

[SK00] S HUM H.-Y., K ANG S. B.: A review of image-based
rendering techniques. In Proc. SPIE VCIP (2000), pp. 2–13. 2

[LF94] L AVEAU S., FAUGERAS O.: 3-D Scene Representation as
a Collection of Images and Fundamental Matrices. Tech. Rep.
2205, INRIA, 1994. 2

[SSS06] S NAVELY N., S EITZ S. M., S ZELISKI R.: Photo
tourism: exploring photo collections in 3D. ACM TOG 25, 3
(2006), 835–846. 2, 3

[LGJA09] L IU F., G LEICHER M., J IN H., AGARWALA A.:
Content-preserving warps for 3D video stabilization. ACM TOG
28, 3 (2009), 44:1–44:9. 3, 5

[TD06] T RAN S., DAVIS L.: 3D surface reconstruction using
graph cuts with surface constraints. In Proc. ECCV (2006),
pp. 219–231. 2

[LH96] L EVOY M., H ANRAHAN P.: Light field rendering. In
Proc. ACM SIGGRAPH (1996), pp. 31–42. 2

[ZCA∗ 09] Z HENG K. C., C OLBURN A., AGARWALA A.,
AGRAWALA M., S ALESIN D.: A Consistent Segmentation Approach to Image-based Rendering. Tech. Rep. CSE-09-03-02,
University of Washington, 2009. 2

[Lip80] L IPPMAN A.: Movie-maps: An application of the optical videodisc to computer graphics. ACM SIGGRAPH CG 14, 3
(1980), 32–42. 2
[LLB∗ 10] L IPSKI C., L INZ C., B ERGER K., S ELLENT A.,
M AGNOR M.: Virtual video camera: Image-based viewpoint
navigation through space and time. CGF 29, 8 (2010), 2555–
2568. 2
c 2011 The Author(s)
⃝
c 2011 The Eurographics Association and Blackwell Publishing Ltd.
⃝

