DOI: 10.1111/j.1467-8659.2011.02065.x

COMPUTER GRAPHICS

forum

Volume 30 (2011), number 8 pp. 2328–2340

A Flexible Approach for Output-Sensitive Rendering of
Animated Characters
A. Beacco1 , B. Spanlang2 , C. Andujar1 and N. Pelechano1
1 MOVING

Research Group, Universitat Polit`ecnica de Catalunya, Spain
{abeacco, andujar, npelechano}@lsi.upc.edu
2 Dept. de Personalitat, Avaluaci´
o i Tractament Psicol`ogic, Universitat de Barcelona, Spain
bspanlang@ub.edu

Abstract
Rendering detailed animated characters is a major limiting factor in crowd simulation. In this paper we present
a new representation for 3D animated characters which supports output-sensitive rendering. Our approach is
flexible in the sense that it does not require us to pre-define the animation sequences beforehand, nor to precompute a dense set of pre-rendered views for each animation frame. Each character is encoded through a small
collection of textured boxes storing colour and depth values. At runtime, each box is animated according to the
rigid transformation of its associated bone and a fragment shader is used to recover the original geometry using a
dual-depth version of relief mapping. Unlike competing output-sensitive approaches, our compact representation
is able to recover high-frequency surface details and reproduces view-motion parallax effectively. Our approach
drastically reduces both the number of primitives being drawn and the number of bones influencing each primitive,
at the expense of a very slight per-fragment overhead. We show that, beyond a certain distance threshold, our
compact representation is much faster to render than traditional level-of-detail triangle meshes. Our user study
demonstrates that replacing polygonal geometry by our impostors produces negligible visual artefacts.
Keywords: crowd rendering, relief mapping, level-of-detail, animated characters
ACM CCS: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Colour, shading, shadowing,
and texture.

1. Introduction
Real-time crowd rendering is a key ingredient in many applications, from urban planning and emergency simulation, to
video games and entertainment. Crowd simulations typically
require hundreds or thousands of agents, each one with its
own individual behaviour. Real-time rendering of detailed
animated characters in such simulations is still a challenging
problem in computer graphics.
Detailed characters are often represented as textured
polygonal meshes which provide a high-quality representation at the expense of a high rendering cost. The animation
of polygonal meshes is usually achieved through skeletal animation techniques: a set of geometric transformations are applied to the character’s skeleton, and a weighted association
between the mesh vertices and the skeleton bones (skinning)
c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics
Association and Blackwell Publishing Ltd. Published by
Blackwell Publishing, 9600 Garsington Road, Oxford OX4
2DQ, UK and 350 Main Street, Malden, MA 02148, USA.

defines how these transformations modify the mesh geometry. Polygonal meshes are suitable for simulations involving
a relatively small number of agents, but not for large-scale
crowd simulations, as the rendering cost of each animated
character is roughly proportional to the complexity of its
polygonal representation.
A number of techniques have been proposed to accelerate rendering of animated characters. Besides view-frustum
and occlusion culling techniques, related work has focused
mainly on providing level-of-detail (LOD) representations so
that agents located far away from the viewpoint are rendered
in a more efficient way with little or no impact on the visual
quality of the resulting images. A typical approach is to store,
for each animated character, a small subset of independent
polygonal meshes, each one representing the character at a

2328

A. Beacco et al. / Output-Sensitive Rendering of Animated Characters

different level of detail. Unfortunately, most surface simplification methods are devoted to simplifying static geometry
and do not work well with dynamic articulated meshes. As
a consequence, the simplified versions of each character are
often created manually. Moreover, these simplified representations either retain a large number of vertices, or suffer from
a substantial loss of detail, which is particularly noticeable
along character silhouettes.
Image-based pre-computed impostors [DHOO05, TC00,
TLC02] provide a substantial speed improvements by rendering distant characters as a textured polygon, but suffer
from two major limitations: all animations cycles have to be
known in advance (and thus animation blending is not supported), and resulting textures are huge (as each character
must be rendered for each discrete animation frame and view
angle); otherwise characters appear pixelized.
Using separate impostors for different body parts provides a much more memory-efficient approach. Polypostors
[KDC∗ 08] subdivide each animated character into a collection of pieces, each one represented using two-dimensional
(2D) polygonal impostors. Unfortunately, the representation
is view-dependent, the animation sequence still has to be
known at construction time, and character decomposition is
done manually.
Relief mapping has been proven to be a powerful tool
to encode detailed geometry and appearance information.
Most importantly, since relief maps support efficient randomaccess, impostors based on relief mapping are output sensitive, i.e. their rendering cost is roughly proportional to the
area of their screen projection. This feature makes relief impostors especially suitable for accelerating the rendering of
scenes involving a huge number of objects.
In this paper we present a new representation for animated characters (Figure 1) which uses relief impostors to
represent the different body parts of the character delimited by the bones of the skeleton. Each character is encoded
through a collection of OBBs, where each box represents
the geometry influenced by a particular bone. Textures are
projected orthogonally onto the six faces of each box. For
each face we store two textures encoding colour, normal
and depth values. During animation the bounding boxes
are transformed rigidly by a vertex shader according to the
transformation of the associated bone in the animated skeleton. A fragment shader efficiently recovers the details of the
avatar’s skin and clothing using an adapted version of relief
mapping.
Unlike competing output-sensitive approaches, our compact representation has no pre-processing requirements (construction can be performed at load time) and does not require
us to pre-define the animation sequences or to select a subset of discrete views. Our performance experiments show a
significant improvement with respect to geometry rendering.
We have also conducted user perception tests validating our

2329

technique for rendering agents at middle and far distances
from the observer.

2. Related Work
2.1. Crowd rendering acceleration
Rendering a large number of highly realistic animated characters can become a major bottleneck if we render the full
geometry of all characters with animation and skinning. To
achieve highly realistic populated scenes in real time several
techniques have been developed. A well-known solution to
this problem involves applying LOD for the characters depending on their distance to the camera [PPB∗ 97]. Ciechomski et al. [CSMT05] avoid computing the deformation of a
character’s mesh by storing pre-computed deformed meshes
for each key-frame of animation, and then carefully sorting
these meshes to take cache coherency into account.
Impostors have been suggested to avoid rendering the 3D
geometry during simulation time. Aubel et al. [ABT98] described dynamic impostors, where a multi-resolution virtual
human was constructed to be rendered off-screen, from a
view-angle and with its animated position, into a buffered
texture. The texture was then mapped into a 3D polygon
oriented towards the camera. This texture is only refreshed
when necessary. Pre-generated impostors were first used by
Tecchia et al. [TC00] by rendering each character from several viewpoints and for every animation frame of a simple
animation cycle. The images were stored in a single texture
atlas, and each crowd agent was rendered as a single polygon with suitable texture coordinates according to the view
angle and frame. Pre-generated impostors with improved
shading have also been used [TLC02]. Impostors can render
crowds consisting of tens of thousands of agents, but require
a large amount of memory, and at close distances they appear
pixelated.
Dobbyn et al. [DHOO05] introduced the first hybrid system that presented impostors on top of a full, geometrybased human animation system, and switch between the two
representations with minimal popping artefacts. Coic et al.
[CLM07] described a similar hybrid system but with three
LODs, by introducing a volumetric layered based impostor
between flat impostor and geometry to help achieve continuity during transitions.
In order to reduce the memory requirements of impostors,
while keeping a high level rendering efficiency, 2D polygonal
impostors have been used [KDC∗ 08], where an impostor is
used per body part and viewing direction. When the character
is animated, dynamic programming shifts the vertices of the
2D polygon to approximate the actual rendered image as
closely as possible.
Pettre et al. [PCM∗ 06] described a three-LOD approach,
combining the animation quality of dynamic meshes with the

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2330

A. Beacco et al. / Output-Sensitive Rendering of Animated Characters

Figure 1: Overview of our approach: A bounding box is created for each articulated part of an animated character (a), colour
(b), normal (c) and depth information is projected onto the box faces, which are rendered through relief mapping (d). Image (e)
shows a crowd with about 5000 agents, all of them rendered with our relief impostors.
high performance offered by static meshes and impostors. A
GPU acceleration crowd rendering is presented in [MR06],
alternating the use of a single impostor per agent with pseudoinstancing of polygonal meshes.

2.2. Relief mapping
Among image-based techniques, relief mapping [POC05]
has proven to be useful for recovering high-frequency geometric and appearance details. Relief maps store surface
details in the form of a heightfield. Typically the RGB channels encode a normal or a colour map, while the alpha channel
stores quantized depth values. The programmability of modern GPUs allows us to recover the original geometry by a
simple ray-heightfield intersection algorithm executed in the
fragment shader [POC05]. Acceleration techniques for computing the ray-heightfield intersection include, among others,
linear search plus binary search refinement [POC05], varying
sampling rates [Tat06], pre-computed distance maps [BD06],
cone maps [PO07] and quadtree relief-mapping [SG06].
Other recent techniques adopt a relief mapping approach to
encode details in arbitrary 3D models with minimal supporting geometry [BD06, ABB∗ 07]. Unfortunately, these outputsensitive approaches are limited to static geometry.
Only a few works attempt to animate geometry encoded
as relief impostors. In [PON08] the animator is requested
to create an animation by manually defining and moving a
few control points in texture space. Radial basis functions
are used to warp the original image by texture coordinate
modification. The above method suffers from two major limitations: control points defining the animation are just moved
in 2D, providing only image-warp animation, and it does not
support standard skeletal animation.

3. Our Approach
3.1. Overview
We aim at increasing the number of simulated agents in
real-time crowd simulations by reducing the rendering cost

of individual agents. This involves using a simple representation for animated characters supporting output-sensitive
rendering, so that rendering times are roughly proportional
to the number of rendered fragments, instead of depending
on the complexity of the underlying surface. Therefore only
characters that are very close to the observer are rendered
as polygonal meshes, while the rest of the agents are rendered using our new relief impostor method. We assume the
input character conforms to the de facto standard in character animation and thus consists of a textured polygonal
mesh (skin), a hierarchical set of bones (skeleton) and vertex
weights. We assume that both the skin and the skeleton have
been designed in a reference pose. The nodes of the skeleton represent joints and the edges represent the bones. Since
each bone can be easily identified by its origin, we can use
the term joint interchangeably. The transformations affecting
joints in the hierarchy are assumed to be rigid. The vertex
weights describe the amount of influence of each joint on
each vertex.
Since we want to keep pre-processing and memory costs
at a minimum while still supporting real-time mixing of animation sequences, we use a separate relief impostor for each
animated part of the articulated character. Our representation
for distant characters consists of a collection of OBBs, one
for each bone in the skeleton, along with a collection of textures projected into the OBB faces, encoding colour, normal
and depth values (Figure 1). The OBB will be transformed
in the same way as the bones of the skeleton, giving the
impression that our impostor character is animated.
Our approach differs from previous work in several aspects. First, we do not attempt to animate a single relief
impostor representing a whole character [PON08], but to
provide relief impostors representing an already animated
character. Second, we require much less memory than competing image-based approaches which require pre-rendering
the character for every possible animation frame for a large
set of view angles. Third, compared to previous imagebased techniques, the cost of adding new characters is drastically lower as new animations can be added at no cost at
all. Furthermore, our technique allows blending animations
and also running animations at arbitrary speeds (including

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

A. Beacco et al. / Output-Sensitive Rendering of Animated Characters

slow-motion) since we are not limited to a discrete set
of animation frames Finally, our method provides a detailed rendering for any character, viewpoint and animation
sequence.
Our implementation relies on the Halca animation library
[Spa09, GS10] to draw the animated characters from which
we create our impostors. Halca is a hardware-accelerated
library for character animation which is based on the Cal3D
XML file format [cal] to describe skeleton weighted meshes,
animations and materials. Our current implementation works
with any animated avatar and any animation that can be
exported to the Cal3D format.

2331

threshold . In this approach a triangle is attached to a joint
if any of its vertices has a weight above . This approach
assigns triangles to one or more joints, except for triangles
where none of the weights are above the threshold. In this
particular case we fall back to the first approach, i.e. the triangle is assigned to the joint with the highest influence. On the
one hand high values for result in less protruding artefacts,
but on the other hand low values for result in less cracks.
Experimentally, we found that a threshold = 0.5 worked
well on all our test characters, minimizing both cracks and
protruding artefacts (Figure 5).
Notice that all the strategies above only use vertex weights
and thus are pose-independent.

3.2. Construction

Step 2. The second step is to choose a suitable pose for

The construction of our relief impostors from a given 3D
character proceeds through the following steps, described in
detail below:

capturing the impostors. Triangles are captured according to
the chosen pose, i.e. after mesh vertices have been blended
according to the pose (using linear blend skinning). This
choice of pose affects the captured geometry. Ideally, we
should select a pose representing a somewhat average pose
of the animation sequence.

1. Associate mesh triangles with impostors.
2. Select a suitable pose for capturing the impostors.
3. Compute the bounding boxes with the chosen pose.
4. Capture the textures of each bounding box.

Step 1. We start by assigning mesh triangles with impostors, where each impostor corresponds to a joint of the
articulated character. We assume that each input vertex vi is
attached to joints J 1 , . . . Jn with weights w = (w1 , . . . wn ).
Now the problem is, given a triangle with vertices v 1 , v 2 ,
v 3 , to decide which impostors the triangle will be attached
to. This determines which triangles will be captured by the
impostor. Since we want to keep pre-processing tasks at a
minimum, we only tested simple, automatic solutions.
One extreme option is to allocate mesh triangles to joints,
attaching each triangle to a single joint. More specifically,
each triangle is attached to the joint having the largest influence over the triangle (the influence of a joint over a triangle
is computed as the sum of the joint weights over the triangle’s
vertices). It turns out that this partition tends to produce visible gaps in the joint boundaries during animation; the higher
the deviation with respect to the reference pose, the larger
the resulting gaps.
The opposite approach would consist of attaching each
triangle to a bone if at least one of its vertices is influenced by
the bone, regardless of the corresponding weight. Therefore
some triangles (those around joints) would be attached to a
variable number of impostors, resulting in overlapping parts
among joints. These overlapping parts produce protruding
geometry when the character is rendered in a pose other than
the capture pose.
Therefore we propose an optimized strategy where triangles are assigned to joints based on a given user-defined

For example, if the animation sequence shows a character
walking with the arms in a rest position, it is better to capture
the triangles around the shoulder with the arms in such a position rather than when stretching arms out sideways. Since
impostors will undergo only a rigid transformation, choosing
a pose corresponding to a walking animation keyframe tends
to minimize artefacts around joints. Our current implementation uses a pose from a walking animation sequence, rather
than the reference pose. Notice that the above choice only
affects triangles influenced by multiple joints; triangles influenced by a single joint will be reconstructed in their exact
position regardless of the selected pose.

Step 3. Once a suitable pose has been chosen, we deform
the mesh accordingly by applying linear blend skinning to the
mesh vertices, i.e. the transformed vertex v is computed as
v = wi MJi v, where MJi is the rigid transformation matrix
from the reference-pose of joint Ji to its actual position in
the chosen posture. The bounding box of each impostor is
then computed as the oriented bounding box (OBB) of the
(transformed) triangles attached to the impostor.

Step 4. The last step is to render the deformed mesh
to capture the relief maps corresponding to each one of the
six faces of its bounding box. For each bounding box face,
we set up an orthographic camera with its viewing direction
aligned with the face’s normal vector, and then render the
triangles attached to the corresponding impostor. We capture
the following RGBA textures (Figure 2):
• Colour map: the RGB channels encode the colour, and the
alpha channel encodes the minimum (front) depth value
zf .

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2332

A. Beacco et al. / Output-Sensitive Rendering of Animated Characters

Figure 2: Colour (a), normal (b), front depth (c) and back
depth (d) values are encoded as two RGBA textures.
• Normal map: the RGB channels encode the normal vector,
and the alpha channel encodes the maximum (back) depth
value zb .
Front depth values are captured by rendering the attached
triangles with the default GL_LESS depth comparison function. Likewise, back depth values are captured by clearing
the depth buffer with a zero value (instead of the default unit
value) and switching depth comparison to GL_GREATER.
Although storing both depth values is redundant (front depth
values of a face equal one minus back depth values of the
opposing face), we have chosen this option to improve the
locality of texture fetches during rendering.
In order to speed up rendering we reorganized the textures
to have both depth values in the same texture. We still keep
only two textures, but now the four channels of the first
texture encode (R, G, zf , zb ) and the second texture encodes
(B, nx , ny , nz ). This reduces to one half the number of texture
fetches during the ray-heightfield intersection step of relief
mapping.
The vertices of all bounding boxes of a character are stored
in a single Vertex Buffer Object (VBO), which is shared by
all the instances of the same character.
Colour and normal maps of each character are stored in
texture arrays to avoid texture switching while rendering the
instances of the same articulated character.
Since a typical animated character for crowd simulation
consists of about 21 bones, this accounts for storing 21 ×
6 × 2 = 252 RGBA textures per character. This is quite
reasonable, considering that competing output-sensitive approaches need to capture the character for each view angle
(typically 136 discrete view directions are sampled) and for
each animation frame (typically sampled at 10 Hz). So for
1 s of animation, 64 × 64 textures (which provides a resolution of about 1 cm per texel for geometry, colours and
normals) and 4 bytes per pixel, it would require about 136 ×
64 × 64 × 10 × 4 = 22 MB approximately. With our technique each character requires only about 4 MB of storage.
3.3. LOD for relief impostors
As the characters move away from the camera, we can further
speed up rendering by having a hierarchical representation of

our relief impostors. We construct a new representation with
fewer boxes by merging boxes, i.e. a father node absorbs
its child nodes. The OBB associated with the father node
is recomputed to include the geometry of the child nodes.
New textures are captured for the new OBBs and for this
representation all the geometry included in an OBB will
undergo the rigid transformation applied to the father. For
instance, if we enclose the hand, fore-arm and upper-arm
inside a single OBB, then the hand will not move other
than following the upper-arm transformations. Notice that
once the user selects the target number of boxes for each
LOD and the bones associated with each of them, the task
of creating OBBs and capturing textures is fully automatic.
For the experiments we used relief impostors with 21, 7 and
1 boxes (see Figure 4). The 1-bone LOD has obviously no
deformations, which is appropriate only for characters very
far away from the camera.
3.4. Real-time rendering
Our current prototype uses multiple LOD representations for
each character type; a textured polygonal mesh which is used
for agents close to the viewpoint, and the multi-resolution
impostor set described earlier for the rest of agents. We first
render nearby polygonal agents (grouped by character type
to minimize rendering state changes) and then the rest of the
agents as impostors (again grouped by character type and
LOD).
Each character is rendered through an adapted version of
relief mapping over the fragments produced by the rasterization of the transformed bounding boxes. The CPU-based
part of the rendering algorithm proceeds through the following steps:
1. Bind the corresponding texture arrays (colour and normal maps) into different texture units, and bind also
the VBO with the geometry of the bounding boxes in
the pose used to capture the impostors. These steps are
performed only once per character type.
2. Draw the bounding box associated with each bone, to
ensure that a fragment will be created for any viewing
ray intersecting the underlying geometry.
The vertex shader multiplies the incoming vertices of the
bounding boxes by the corresponding rigid transformation
matrix so that they follow the original skeleton animation.
The vertex shader also transforms the variables encoding the
location and orientation of each relief map, as these will be
used in the fragment shader.
The most relevant part of the rendering relies on the fragment shader, which uses the depth values stored in the colour
and normal maps to find the intersection P of the fragment’s
viewing ray with the underlying geometry. For this particular task any ray-heightfield intersection algorithm can be

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

A. Beacco et al. / Output-Sensitive Rendering of Animated Characters

2333

Figure 3: Test data set. Each mesh contains between 4 K
and 6 K polygons.

adopted. Pyramidal displacement mapping [OKL06] is particularly suitable as it guarantees finding the correct intersection on any heightfield and viewing condition. Our current
prototype though is based on the simpler relief mapping algorithm described in [POC05].
The fragment shader receives as input the following information:
• World space viewpoint coordinates E.
• World space fragment coordinates C.
• The origin o of the face, i.e. the vertex whose texture
coordinates are (0, 0).
• An orthonormal basis of the bounding box face, consisting
of a normal vector n and two vectors (u, v) aligned along
the horizontal and vertical sides of the transformed face.
The fragment shader computes the intersection of the fragment’s viewing ray r = (C − E) with the heightfield encoded
by the displacement values stored in the relief map. If no intersection is found, the fragment is discarded. As in [POC05],
we use first a linear search by sampling the ray r at regular
intervals to find a ray sample inside the object, and then a
binary search to find the intersection point. This allows us to
retrieve the diffuse colour of the fragment being processed,
along with a normal vector to compute per-fragment lighting.
Unlike classic relief mapping, we use two depth values zf and
zb per texel. During the search process, a sample along the
ray with depth z is classified as interior to the object iff zf ≤
z ≤ zb .

4. Results
We have implemented the construction and rendering algorithms described in C++ and OpenGL 3.2.
The algorithms have been tested on a collection of detailed
human characters from the aXYZ Design’s Metropoly 2 data
set (Figure 3). When converted to Cal3D format, the triangle
meshes had 4–6 K triangles, and used 2048 × 2048 texture
atlases for diffuse colour and normal data. All models were
initially rigged to 67-bone skeletons. Reported results have
been measured on an Intel Core2 Quad Q6600 PC equipped
with a GeForce 8800 GT.

Figure 4: Relief impostors consisting of 21, 7 and 1 boxes.
4.1. Impostor creation
The conversion of the input character meshes into a multiresolution collection of relief impostors took on average
859 ms on the test hardware, including all steps detailed in
Section 3.2. We created three LOD representations with 21,
7 and 1 boxes, respectively (Figure 4).
All relief textures (diffuse, normal and depth maps) were
captured at 64 × 64 pixels and stored in a single texture
array shared by all the instances of the same character. This
resulted in 21 × 6 × 642 × 8 = 3.95 MB for the finest
LOD, 1.31 MB for the intermediate LOD and 192 KB for
the coarsest LOD, i.e. about 5.25 MB per character type.
Although our image-based representation is a bit redundant, both within a LOD level (a single surface point is often
captured by 1–3 box faces) and across levels (each LOD has
its own collection of relief maps), it is still several orders of
magnitude more efficient, in terms of memory space, than
competing image-based approaches requiring a separate image for each view direction and animation frame.
4.2. Image quality
Our relief impostor representation aims at accelerating the
rendering of animated characters at the expense of some
image quality loss. Image artefacts in the resulting images
may fall into the following categories (the surface associated to a particular bone will be referred to as a surface
patch):
Surface undersampling due to non-height field patches.
We represent each surface patch with six orthogonal
relief maps, where each relief map stores a single
depth value per texel. Therefore we assume that surface patches look like a heightfield when seen from any
of these six directions. More formally, we assume that
for any axis-aligned ray there is at most one frontface
and at most one backface intersection with the surface

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2334

A. Beacco et al. / Output-Sensitive Rendering of Animated Characters

patch. If this assumption fails for some axis-aligned
direction, the extra intersections between the nearest
one at zn and furthest one at zf will be ignored, causing
the relief mapping algorithm to reconstruct the surface
as if the whole segment from zn and zf were interior to
the object. This could be dealt with by storing multiple
depth values per texel, as in [PO06]. Fortunately, our
surface patches for the 21-bone skeleton correspond to
simple body parts such as upper and lower leg, upper
and lower arm, chest and head, for which the heightfield
condition earlier typically holds. Therefore the assumption above produces no artifacts without requiring the
storage of additional depth values.
Texel-to-pixel ratio. Since our impostors are image-based,
the accuracy of the geometric and appearance details is obviously limited by the texel-to-pixel ratio
[DHOO05]. Therefore we must ensure that textures are
large enough to keep the texel-to-pixel ratio above 1:1
for all the viewing distances associated to the textures.
Since our 64 × 64 textures guarantee the above ratio,
no image undersampling artefacts appear in the final
images.
Depth quantization. The relief mapping algorithm relies
on depth values to find the intersection of per-fragment
viewing rays with the underlying heightfield. We quantize such depth values (which are relative to the box
dimensions) using 8-bit integers. Since our boxes have
moderate sizes (with the longest edge typically below
50 cm), 8-bit quantization results in (at least) 0.2 mm accuracy, which is sufficient to prevent any visible quantization artefact.
Missing ray–surface intersections. Some relief mapping
implementations do guarantee that correct ray–surface
intersections are always found [OKL06, SG06] whereas
others do not [POC05]. This issue has been extensively
discussed in the literature and can be dealt with in multiple ways (see e.g. [SG06]).
Lack of geometric skinning. This issue is by far the major limiting factor when considering the valid distance
range for our relief impostors. Recall that we animate
each relief impostor using the rigid animation of the associated bone. This contrasts with geometric skinning
techniques (such as linear blending and dual-quaternion
blending) typically applied when animating geometrybased characters, where some vertices are influenced by
more than one bone. In our case, each surface patch is
fully influenced by its corresponding bone. This obviously results in some artefacts around joints (triangles
influenced by a single bone are reconstructed correctly
though). These artefacts might include cracks (e.g. if
mesh triangles are assigned to a single bone) or overlapping parts (e.g. if each mesh triangle is assigned to
all the bones influencing the triangle, regardless of the
weights). Fortunately, our optimized construction results in much less artefacts around joints (Figure 5).

Figure 5: Artefacts due to lack of geometric skinning: (a)
original mesh, (b) impostors created by assigning each triangle to a single joint, (c) impostors created by assigning
to each joint all the triangles influenced by the joint and (d)
impostors created by our threshold-based strategy.
Figure 6 shows multiple views of one character rendered
with our 21-bone impostors. Note that these artefacts are
hardly noticeable for moderate viewing distances (see also
the accompanying video). Figure 7 shows several animation
frames of the characters in the test data set rendered with our
21-bone impostors. Although the images show that artefacts
may appear around the joints, these are very hard to perceive
in the context of a crowd simulation. Figure 8 compares
renders using 21-bone and 7-bone representations, respectively. The 1-bone LOD obviously supports no deformations
and thus it is reserved for characters very far away from the
camera.
One of the features of our approach is the joint handling
of geometric and appearance details, encoded through displacement, diffuse and normal maps. The effects of reducing
the size of the texture maps is illustrated in Figure 9. A side
benefit of this approach is that we can use a mipmap pyramid for better minification filtering with no colour bleeding
artefacts. This is a feature often lacking in polygonal characters, which typically use texture atlases with multiple disconnected patches, thus hindering mipmapping rendering.

4.3. Mesh versus impostor rendering
Comparing the performance of our impostors with that of the
full-resolution mesh is clearly unfair, as in a real-world application, each character instance would be rendered using an
appropriate LOD chosen according to, among other factors,
its distance to the viewpoint. We thus compare our approach
with a discrete collection of LOD meshes. We use the following notation. LOD representations using relief impostors
will be denoted as Rj , j being the number of bones/boxes. As
stated earlier, we constructed three representations R21 , R7
and R1 with 21, 7 and 1 bones, respectively. LOD representations using textured polygonal meshes will be denoted as
Mi , i being the percentage of original polygons, M 100 denoting the full-resolution mesh. We simplified the input mesh to

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

A. Beacco et al. / Output-Sensitive Rendering of Animated Characters

2335

Figure 6: An animated character rendered using our 21-box relief impostor representation.
generate LODs M 90 , M 85 , . . . M 5 and M 2.5 (Figure 10). Mesh
simplification was accomplished using the Optimize filter of
Autodesk 3DS MAX 2010.
We are now interested in a criterion to measure the quality
of each representation, which will be used both to compare
mesh- and impostor-based representations, and to select the
appropriate LOD according to the distance to the viewer.
Note that a measure of the geometric approximation error
only makes sense for static polygonal meshes, and that it
would ignore visual errors due to distortions of the diffuse
and normal maps. We thus adopt an image-space error metric,
computed using multiple animation frames and view directions.
Let L be a particular LOD representation of an animated
character, using either mesh geometry or relief impostors
(i.e. L ∈ {M 100 . . . M 2.5 , R21 , R7 , R1 }). Let φ(L, d) be the
average image difference resulting from rendering a character

at distance d using the representation L instead of the fullresolution mesh M 100 .
For the sake of clarity, we can consider distance d rather
than screen-projected area or subtended solid angle, provided that we fix some viewing conditions. For all the discussion later, we used a 21 LCD monitor with a 1024 ×
1024 viewport. The field of view of the camera was set to
60◦ , and the viewing distance from the LCD monitor was
set to 60 cm. Note that these typical viewing conditions for a
desktop user can be used to determine both the viewing angle
subtended by an animated character at some particular distance from the camera, as well as its screen-projected area.
Thus from now on we will refer only to character-to-camera
distances.
Since the image difference obviously depends on both the
animation frame and the viewing angle, we can compute φ(L,
d) by selecting a representative set of animation frames and a

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2336

A. Beacco et al. / Output-Sensitive Rendering of Animated Characters

Figure 8: Rendering relief impostors with 21 bones (top)
and 7 bones (bottom). Note that in the 7-bone representation
the head bone has been collapsed with the trunk and thus
both undergo the same transformation.

Figure 7: Relief impostors corresponding to the test data
set.

sufficiently dense discretization of the view directions in the
Gauss sphere, and averaging the resulting image differences.
We chose the root mean square (RMS) error as an objective
measure to compare the image differences. We could have
adopted perceptual-based image metrics [YN04], which integrate factors of the human visual system that reduce the
sensitivity to errors, but these metrics are more appropriate
for comparing two final, complete images rather than renders
of individual agents. This is because high-level HVS models
go beyond simple models of brightness and contrast and consider for example masking effects, i.e. decreased visibility of
a signal due to background contrast. These effects can be
measured only on complete images, where each pixel has a
well-defined context. In our case we aim at comparing the
rendering of individual characters (thus only a part of the final
image) without prior knowledge on the context/background.
This is why we discarded HVS-based metrics for comparing
the renderings of single, isolated characters, and we chose
the broadly adopted RMS error for this purpose. We thus
computed φ(L, d) by averaging (in the L2 norm sense) the
RMS image differences along four equally spaced frames
and 10 view directions uniformly distributed on the upper
half-sphere surrounding the character. This amounts to 40

Figure 9: Relief impostors with decreasing texture sizes.

samples for computing φ(L, d), which gives quite reliable
results.
Given a certain distance d, we are interested in the simplest
mesh level Mi and the simplest relief level Rj the rendering
of which produces an image error below some threshold ε.
In other words, we want to compute min i {φ(Mi , d) < ε}
and min j {φ(Rj , d) < ε}. We call the resulting pair Mi , Rj the
optimal representation for distance d.
We conducted an informal user study to decide a proper
error threshold, considering the viewing conditions detailed
earlier. Nine users (aged 23–35) participated in the experiment. Users were presented a video showing an animated character side-by-side, one side rendered using detailed polygonal meshes, and the other side using impostors.
Every 10 s we doubled the distance from the character to
the camera, thus decreasing its screen-projected area. Users
were requested to stop the movie as soon as they perceived
no difference between both sides of the image. We recorded
the resulting RMS error. We observed that the average RMS
error was 0.004, and that none of the users were able to find
any difference for an RMS error below 0.003. Therefore we
set ε = 0.003 to prevent users from perceiving any visual

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

A. Beacco et al. / Output-Sensitive Rendering of Animated Characters

2337

Figure 12: Render times for the polygonal-based and reliefbased LODs that keep the RMS error below 0.003.
Figure 10: From left to right we show the original mesh
M 100 and some examples of the simplified meshes M 75 , M 55 ,
M 35 , M 20 and M 2.5 .
4.4. Choosing the fastest representation
Beyond a given distance (15 m for the chosen threshold), we
can choose to render the characters using either the optimal
mesh-based or the optimal relief-based representation. Since
both provide images with similar quality, it makes sense to
choose the appropriate representation according to its performance.
For each distance value d, we measured render times for
the optimal mesh-based and the optimal relief-based representations computed earlier. Render times were measured using OpenGL’s timer queries, which provide accurate timings.
Each query block enclosed the OpenGL drawing commands
that need to be executed for each character instance.
Figure 11: Minimum number of primitives (triangles/quads)
to be drawn (per character instance) to keep the RMS error
below 0.003 for one of the test characters. Triangle mesh
rendering requires one order of magnitude more primitives
(notice the log-scale).

difference between the original and the simplified representation.
The resulting number of primitives for this error threshold
are shown in Figure 11. Note that triangle mesh rendering requires drawing about one order of magnitude more primitives
than impostor rendering, under matching quality conditions.
For example, for a character at 15 m, a 4 k triangle mesh
(8 k vertices) is needed to keep the RMS error below ε =
0.003, whereas the matching impostor has 126 quads (168
vertices). In terms of per-vertex processing, the polygonal
mesh requires about 33 k matrix operations for skinning,
whereas our impostors requires just 168 operations. For a
character at 40 m, the mesh requires 916 matrix operations
whereas the impostors only 48. For close-up characters (d <
15 m), the relief-based representation leads to an error above
the threshold and thus we fall back to polygonal rendering.

For mesh rendering, we used the hardware-accelerated
Halca animation library [Spa09, GS10]. Render times are
shown in Figure 12. As with image differences, times were
averaged for multiple animation frames and view directions,
taking 40 samples per distance value.
When rendering polygonal meshes, the bottleneck is likely
to be in the vertex processing stage due to the large amount of
matrix operations needed to implement skinning. This makes
rendering times quite insensitive to the number of fragments
produced. However, since for increasing distances we use a
more simplified mesh (see Figure 11), the rendering times
decrease accordingly.
Relief impostors achieve a drastic reduction in the number
of primitives to be drawn, and each vertex is influenced by
a single bone. This results in a very small number of pervertex computations when compared to the equivalent level
using mesh geometry. On the downside, fragment processing is more involved due to relief mapping computations. As
a consequence, rendering times for impostors also decrease
with increasing distances, but this time the shape of the resulting curve can be attributed more to the smaller number of
fragments rather than to the reduced number of primitives.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2338

A. Beacco et al. / Output-Sensitive Rendering of Animated Characters

Table 1: Frame rates for different crowds and camera settings.

Setting

Agents

Polygonal
mesh

Our
approach

Speed
up

Camera 1
Camera 2
Camera 3
Camera 1
Camera 2
Camera 3
Camera 1
Camera 2
Camera 3

2000
2000
2000
4000
4000
4000
10 000
10 000
10 000

21 fps
23 fps
25 fps
10 fps
10 fps
12 fps
4 fps
4 fps
4 fps

50 fps
47 fps
49 fps
38 fps
36 fps
34 fps
16 fps
15 fps
14 fps

2.4×
2.0×
2.0×
3.8×
3.6×
2.8×
4.0×
3.7×
3.5×

Note that for characters beyond 15 m, using the relief-based
representation results in a performance gain.
4.5. Crowd rendering performance
The results provide optimal switch distances for individual agents, but do not show actual frame rates when rendering a complete crowd. Therefore we also measured the
frame rate using two different strategies for selecting the
appropriate LOD: (1) using only mesh-based levels M 100 ,
M 95 , . . . M 2.5 , chosen according to Figure 11, and (2) using the full-resolution mesh M 100 or the optimal relief-based
level, whichever is fastest. According to the results discussed,
we used the following criteria in option (2) to choose the appropriate representation for each agent: full-resolution mesh
for d < 15, R21 for 15 ≤ d < 34, R7 for 34 ≤ d < 56 and R1
for d > 56.
Besides the hardware and the number of simulated agents,
the actual frame rate depends on many factors, including
population density (the higher the density, the higher the
number of instances requiring fine LOD levels and thus the
lower the frame rate), camera field of view (the higher the fov,
the higher the perspective distortion, thus allowing coarser
LOD levels), screen resolution and number of agents actually
visible.
For the following comparison we used a crowd with a
varying number of agents rendered into a 1024 × 600 viewport (see accompanying movies). Table 1 shows the resulting
frame rates for the different crowd scenarios shown in Figure 13 . Note that our approach provides a speed up between
2× and 4×.
4.6. User study
We conducted a user study to validate our impostor-based
approach in terms of image quality. The main goal of the experiment was to evaluate whether users perceive any image
quality loss when using our impostors instead of polygonal

Figure 13: Camera settings for the performance test.

meshes. For this purpose, we rendered an animated crowd
with two different strategies: (1) using the full-resolution
mesh for all characters, and (2) using for each character the
optimal representation (mesh or relief impostors), chosen according to the criteria discussed in Section 4.5. We produced
a 25 s movie for different crowd settings (Figure 13). We used
exactly the viewing conditions detailed in Section 4.3. In order to assess image quality with respect to the reference image, we grouped these movies in pairs, stacking horizontally
the movie using impostors with the one using full-resolution
meshes (see accompanying videos). The movie using impostors was stacked on the left/right randomly.
Nine subjects (aged 23–35) participated in the experiment.
Users were requested to watch five pairs of videos (which
were presented in a random order) and to decide which of the
two sides (left/right) had better image quality than the other,
if any. This yields a total of N = 45 trials. Let fi be the (relative) frequency of users choosing the side with impostors
as the best movie. Likewise, let fg be the frequency of users
choosing the side with geometry, and fu the frequency of
users unable to decide which side is better. The absolute frequencies we observed from the 45 samples of our user study
where ni = 15, ng = 14 and nu = 16. We consider the null hypothesis to be ‘giving the answers by chance’ which implies
that all conditions should be chosen with equal probability,
i.e. H 0 : fi = fg = fu = 1/3. The corresponding significance
levels for a two-sided test against the null hypothesis that
each proportion is 1/3 are 0.5, 0.7 and 0.8, therefore the null

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

A. Beacco et al. / Output-Sensitive Rendering of Animated Characters

2339

acter instance. In matrix palette skinning, bone matrices for
each frame and for each animation are stored in graphics
memory. This allows each agent to have its own distinct pose
and animation [Dud07a]. Note however that palette skinning
only saves memory bandwidth; it does not affect the number
of matrix operations in the vertex shader. Thus the benefits of
our approach over mesh rendering in terms of skinning (less
vertices to be transformed, and a single bone influencing
each vertex) still hold. Other avenues for future work include
conducting psychophysical studies supporting the LOD selection instead of the current metric based on RMS. This
will evaluate not only the quality of static images but also
the impact of visualizing animated characters through relief
impostors.
Figure 14: Image rendered with full-resolution polygonal
meshes (left) and our approach (right).
hypothesis cannot be rejected. This means that the choices
of the subjects were equivalent to random choices, and thus
our impostor-based technique can be used for rendering acceleration with negligible visual artifacts (see Figure 14).

Acknowledgments
This research has been partially funded by the Spanish Government Grant TIN2010-20590-C01-01 and the TRAVERSE
ERC Advanced Grant 227985.
References

5. Conclusions and Future Work
We have presented a new method to accelerate the rendering
of crowds by using static relief impostors on rigidly animated
bounding volumes. Our method allows for real time rendering of thousands of agents. Compared to previous work where
impostors were used, our method provides the advantage of
being independent from both the viewing direction and the
animation clips available. These two advantages offer not
only important savings in terms of the memory required to
store the impostors, but also that the library of animations can
be increased on-the-fly without the need for capturing new
impostors. Unlike previous image-based approaches, our approach does support blending between animation clips and
even real-time motion capture data.

[ABB∗ 07] ANDUJAR C., BOO J., BRUNET P., FAIREN M., NAVAZO
I., VAZQUEZ P., VINACUA A.: Omni-directional relief impostors. Computer Graphics Forum 26, 3 (September 2007),
553–560.
[ABT98] AUBEL A., BOULIC R., THALMANN D.: Animated impostors for real-time display of numerous virtual humans.
In VW ’98: Proceedings of the First International Conference on Virtual Worlds (London, UK, 1998), SpringerVerlag, pp. 14–28.
[BD06] BABOUD L., D´ECORET X.: Rendering geometry with
relief textures. In GI ’06: Proceedings of Graphics Interface 2006 (Toronto, Ont., Canada, Canada, 2006), Canadian Information Processing Society, pp. 195–201.

Our technique could be used in combination with other
GPU-based acceleration techniques such as geometry instancing and matrix palette skinning. Geometry instancing
[Dud07a] provides performance gains on the CPU side by
minimizing drawing calls, and thus both polygonal-based and
impostor-based rendering can benefit from this technique.
On the downside, geometry instancing poorly supports LOD
rendering and frustum culling. Unfortunately, since all visible characters belonging to a given LOD are drawn with
a single API call, these tasks have to be completed for all
characters before actual rendering starts, effectively sequentializing rather than parallelizing CPU-GPU work. As future
work we will explore the potential of these GPU acceleration
techniques to further speed up our rendering.

[cal] Cal3d. 3d character animation library. http://home.
gna.org/cal3d/. Accessed 21 September 2011.

Both mesh- and impostor-based rendering can benefit also
from matrix palette skinning [Dud07b] to avoid sending to
the GPU the transformation matrices for each bone and char-

[DHOO05] DOBBYN S., HAMILL J., O’CONOR K., O’SULLIVAN
C.: Geopostors: A real-time geometry/impostor crowd
rendering system. In I3D ’05: Proceedings of the 2005

[CLM07] COIC J., LOSCOS C., MEYER A.: Three LOD for
the Realistic and Real-Time Rendering of Crowds with
Dynamic Lighting. Research Report RN/06/20, Universit´e
Claude Bernard, LIRIS, France, April 2007.
[CSMT05] CIECHOMSKI P. D. H., SCHERTENLEIB S., MA¨IM J.,
THALMANN D.: Reviving the roman odeon of aphrodisias:
Dynamic animation and variety control of crowds in virtual heritage. In Proc. 11th International Conference on
Virtual Systems and Multimedia (VSMM 05) (Ghent, Belgium, 2005), pp. 601–610.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2340

A. Beacco et al. / Output-Sensitive Rendering of Animated Characters

symposium on Interactive 3D graphics and games (New
York, NY, USA, 2005), ACM, pp. 95–102.
[Dud07a] DUDASH B.: Animated Crowd Rendering. In GPU
Gems 3. H. Nguyen (Ed.). Addison-Wesley Professional
(2007), pp. 39–52.
[Dud07b] DUDASH B.: Skinned Instancing. In NVIDIA
SDK 10 (2007), http://developer.download.nvidia.com/
SDK/10/direct3d/screenshots/samples/SkinnedInstancing
.html. Accessed 21 September 2011.
[GS10] GILLIES M., SPANLANG B.: Real-time character engines comparing and evaluating real-time character engines for virtual environments. Special Issue on Presence
19 (2010).
ˇ ARA J.,
[KDC∗ 08] KAVAN L., DOBBYN S., COLLINS S., Z´
O’SULLIVAN C.: Polypostors: 2d polygonal impostors for
3d crowds. In I3D ’08: Proceedings of the 2008 Symposium on Interactive 3D Graphics and Games (New York,
NY, USA, 2008), ACM, pp. 149–155.
[MR06] MILLAN E., RUDOMIN I.: Impostors and pseudo instancing for gpu crowd rendering. In GRAPHITE ’06:
Proceedings of the 4th International Conference on Computer Graphics and Interactive Techniques in Australasia
and Southeast Asia (New York, NY, USA, 2006), ACM,
pp. 49–55.

Techniques for High-Performance Graphics and GeneralPurpose Computation (2007), Addison-Wesley Professional, pp. 409–428.
[POC05] POLICARPO F., OLIVEIRA M., COMBA J.: Real-time
relief mapping on arbitrary polygonal surfaces. In I3D
’05: Proceedings of the 2005 Symposium on Interactive
3D Graphics and Games (New York, NY, USA, 2005),
ACM, pp. 155–162.
[PON08] PAMPLONA V., OLIVEIRA M., NEDEL L.: Animating Relief Impostors Using Radial Basis Functions Textures. In Game Programming Gems VII. Scott Jacobs
(Ed.). Charles River Media, Inc., Hingham, Massachusetts
(2008), pp. 401–412.
[PPB∗ 97] PRATT D., PRATT S., BARHAM P., BARKER R.,
WALDROP M., EHLERT J., CHRISLIP C.: Humans in largescale, networked virtual environments. Presence 6, 5
(1997), 547–564.
[SG06] SCHRODERS M., GULIK R.: Quadtree relief mapping. In Proceedings of the 21st ACM SIGGRAPH/EUROGRAPHICS Symposium on Graphics
Hardware (2006), ACM, pp. 61–66.
[Spa09] SPANLANG B.: HALCA Hardware Accelerated Library for Character Animation. Tech. Rep., Universitat
de Barcelona, 2009.

[OKL06] OH K., KI H., LEE C.-H.: Pyramidal displacement
mapping: A GPU based artifacts-free ray tracing through
an image pyramid. In Proceedings of the ACM Symposium
on Virtual Reality Software and Technology (2006), VRST
’06, pp. 75–82.

[Tat06] TATARCHUK N.: Dynamic parallax occlusion mapping with approximate soft shadows. In I3D ’06: Proceedings of the 2006 Symposium on Interactive 3D Graphics
and Games (New York, NY, USA, 2006), ACM, pp. 63–
69.

[PCM∗ 06] PETTRE´ J., CIECHOMSKI P., MA¨IM J., YERSIN B.,
LAUMOND J., THALMANN D.: Real-time navigating crowds:
scalable simulation and rendering: Research articles.
Computer Animation and Virtual Worlds 17, 3–4 (2006),
445–455.

[TC00] TECCHIA F., CHRYSANTHOU Y.: Real-time rendering
of densely populated urban environments. In Proceedings
of the Eurographics Workshop on Rendering Techniques
2000 (London, UK, 2000), Springer-Verlag, pp. 83–88.

[PO06] POLICARPO F., OLIVEIRA M. M.: Relief mapping of
non-height-field surface details. In Proceedings of the
2006 Symposium on Interactive 3D Graphics and Games
(2006), I3D ’06, pp. 55–62.
[PO07] POLICARPO F., OLIVEIRA M.: Relaxed cone stepping for relief mapping. In GPU Gems 3: Programming

[TLC02] TECCHIA F., LOSCOS C., CHRYSANTHOU Y.: Imagebased crowd rendering. IEEE Computer Graphics and
Applications 22, 2 (2002), 36–43.
[YN04] YEE Y. H., NEWMAN A.: A perceptual metric for
production testing. In SIGGRAPH’04: ACM SIGGRAPH
2004 Sketches (New York, NY, USA, 2004), ACM,
p. 121.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

