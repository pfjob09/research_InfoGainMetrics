DOI: 10.1111/j.1467-8659.2011.02057.x
Pacific Graphics 2011
Bing-Yu Chen, Jan Kautz, Tong-Yee Lee, and Ming C. Lin
(Guest Editors)

Volume 30 (2011), Number 7

Motion Deblurring from a Single Image
using Circular Sensor Motion
Yosuke Bando†,§ , Bing-Yu Chen‡ , and Tomoyuki Nishita§
† TOSHIBA

Corporation
University of Tokyo
‡ National Taiwan University
§ The

Abstract
Image blur caused by object motion attenuates high frequency content of images, making post-capture deblurring an ill-posed problem. The recoverable frequency band quickly becomes narrower for faster object motion
as high frequencies are severely attenuated and virtually lost. This paper proposes to translate a camera sensor
circularly about the optical axis during exposure, so that high frequencies can be preserved for a wide range of
in-plane linear object motion in any direction within some predetermined speed. That is, although no object may
be photographed sharply at capture time, differently moving objects captured in a single image can be deconvolved with similar quality. In addition, circular sensor motion is shown to facilitate blur estimation thanks to
distinct frequency zero patterns of the resulting motion blur point-spread functions. An analysis of the frequency
characteristics of circular sensor motion in relation to linear object motion is presented, along with deconvolution
results for photographs captured with a prototype camera.
Categories and Subject Descriptors (according to ACM CCS): I.4.1 [Image Processing and Computer Vision]: Digitization and Image Capture— I.4.3 [Image Processing and Computer Vision]: Enhancement—Sharpening and
deblurring

1. Introduction
Motion blur often spoils photographs by losing image sharpness. As motion blur attenuates high frequency content of
images, motion deblurring is an ill-posed problem and often comes with noise amplification and ringing artifacts
[BLM90]. Although image deconvolution techniques advance continuously to tackle this problem, motion deblurring
is still challenging since the recoverable frequency band easily becomes narrow for fast object motion as high frequencies are severely attenuated and virtually lost.
A simple countermeasure, called follow shot, can capture
sharp images of a moving object as if it were static by panning a camera to track the object during exposure. However,
there are cases where follow shot is not effective: 1) when
object motion is unpredictable; 2) when there are multiple
† yosuke1.bando@toshiba.co.jp
‡ robin@ntu.edu.tw
§ {ybando, nis}@nis-lab.is.s.u-tokyo.ac.jp
c 2011 The Author(s)
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd. Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ,
UK and 350 Main Street, Malden, MA 02148, USA.

objects with different motion. This is because follow shot
favors particular motion one has chosen to track, as much
as a static camera favors “motion” at the speed of zero (i.e.,
static objects): objects moving differently from favored motion degrade.
This paper explores a camera hardware-assisted approach
to single-shot motion deblurring that simultaneously preserves high frequency image content for different object motion. Under the assumption that object motion is in-plane
linear (having arbitrary 2D directions) within some predetermined speed, we propose to translate a camera sensor circularly about the optical axis during exposure, so that the
camera partially “follow-shots” various object motion. As a
result, differently moving objects can be deconvolved with
similar quality.
Our work is inspired by Levin et al. [LSC∗ 08], who
proved that constantly accelerating 1D sensor motion can
render motion blur invariant to 1D linear object motion
(e.g., horizontal motion), and showed that this sensor motion
evenly distributes the fixed frequency “budget” to different

1870

Y. Bando, B.-Y. Chen, T. Nishita / Motion Deblurring from a Single Image using Circular Sensor Motion

object speeds. Similarly to Cho et al. [CLDF10], we intend
to extend Levin et al.’s budgeting argument to 2D (i.e., inplane) linear object motion by sacrificing motion-invariance.
However, unlike Cho et al.’s and other researchers’ multishot approaches [CLDF10, RAP05, YSQS07, AXR09], in
this paper we would like to seek a single-shot solution because increasing the number of exposures may incur other
issues such as capture time overhead/delay between multiple
exposures, signal-to-noise ratio degradation if shorter exposure time per shot is used, or violation of the linear object
motion model if total exposure time becomes longer.
By losing motion-invariance, we inevitably reintroduce an
issue inherent to the classical motion deblurring problem,
which [LSC∗ 08] resolved for 1D motion; we need to locally estimate a point-spread function (PSF) of motion blur
as it depends on object motion. Fortunately, although we rely
on user intervention to segment images into different moving objects, we can show that PSF discriminability for each
moving object is higher for the circular sensor motion camera than the previous single-shot image capture strategies,
thanks to distinct frequency zero patterns of the PSFs.
Although we cannot guarantee worst-case deblurring performance as the PSFs have frequency zeros, circular sensor
motion can be shown to provide 2/π (≈ 64%) of the optimal
frequency bound on an average. We show deconvolution results for simulated images as well as real photographs captured by our prototype camera, and also demonstrate other
advantages of circular sensor motion: 1) motion blurred objects in an image are recognizable (e.g., text is readable) even
without deconvolution; 2) the circular motion strategy has no
180◦ motion ambiguity in PSF estimation; it can distinguish
rightward object motion from leftward one.
2. Related Work
Capture time approach: Motion blur can be reduced using
short exposure time, but signal-to-noise ratio worsens due to
loss of light. Recent cameras are equipped with image stabilization hardware that shifts the lens or sensor to compensate for camera motion acquired from a gyroscope. This is
effective for preventing camera shake blur but not for object
motion blur. Ben-Ezra and Nayar [BEN04] acquired camera motion from a low resolution video camera attached to
a main camera, which was used for camera shake removal
for the main camera. Tai et al. [TDBL08] extended their
approach to handle videos with non-uniform blur. Joshi et
al. [JKZS10] used a 3-axis accelerometer and gyroscopes to
guide camera shake removal.
Raskar et al. [RAT06] developed a coded exposure technique to prevent attenuation of high frequencies due to motion blur at capture time by opening and closing the shutter during exposure according to a pseudo-random binary
code. The method was extended to be capable of PSF estimation [AX09] and to handle non-uniform/nonlinear blur
[TKLS10, DMY10].

The trade-offs among the coded exposure photography,
motion-invariant photography [LSC∗ 08], and ours are summarized as follows (refer also to [AR09] for detailed comparison between the coded exposure and motion-invariant
strategies). A static camera can capture static objects perfectly, but high frequencies will be rapidly lost as object motion gets faster. The coded exposure strategy significantly reduces this loss of frequencies. The motion-invariant strategy
best preserves high frequencies for 1D (horizontal) object
motion up to the predetermined speed, denoted by S, but it
does not generalize to other motion directions. The circular
motion strategy can treat any direction, and it achieves better high frequency preservation for target object speed S than
the coded exposure strategy. Similar to the motion-invariant
strategy, the circular motion strategy degrades static scene
parts due to sensor motion, but it can partially track moving
objects so that they are recognizable even before deconvolution. Unlike the other strategies, the circular motion strategy has no 180◦ motion ambiguity in PSF estimation. These
trade-offs will be explained in more detail and demonstrated
in the following sections.
Cho et al. [CLDF10] proposed a two-shot approach with
the motion-invariant strategy aimed to two orthogonal directions (i.e., horizontal and vertical). In contrast, this paper pursues a single shot approach. Direct comparison with
multi-shot approaches requires elaborate modeling of capture time overhead and noise and is out of the scope of this
paper, but we will present some observation in Sec. 4.
Post-capture approach (PSF estimation and image deconvolution): This field has a large body of literature, and we
refer the readers to [KH96] for the early work. Recently,
significant advancement was brought forth by the incorporation of sophisticated regularization schemes and by extending the range of target blur to non-uniform and/or large
ones [FSH∗ 06,Lev06,Jia07,SXJ07,SJA08,YSQS08,JSK08,
CL09, XJ10, KTF11]. Some researchers used multiple images [RAP05,YSQS07,AXR09,ZGS10], some of which use
different exposure times or flash/no-flash image pairs.
Other applications of sensor motion: Some researchers
proposed to move sensors for different purposes. Ben-Ezra
et al. [BEZN05] moved the sensor by a fraction of a pixel
size between exposures for video super-resolution. Mohan
et al. [MLHR09] moved the lens and sensor to deliberately
introduce motion blur that acts like defocus blur. Nagahara
et al. [NKZN08] moved the sensor along the optical axis to
make defocus blur depth-invariant.

3. Circular Image Integration
Fig. 1(a) shows the proposed motion of a camera image sensor. We translate the sensor along a circle perpendicular to
the optical axis while keeping its orientation. We use the
phrase “circular motion” to emphasize that we do not rotate
the sensor itself.
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

Y. Bando, B.-Y. Chen, T. Nishita / Motion Deblurring from a Single Image using Circular Sensor Motion

During exposure time t ∈ [−T, +T ], the sensor undergoes
one revolution with constant angular velocity ω = π/T . Letting the radius of circular motion be R, the sensor moves
along the circle with constant speed Rω, which corresponds
to the target object speed S in the image space. The corresponding object speed in the world space (i.e., actual speed
in a scene) is determined by the camera optics and the distance to the object from the camera. Given exposure time 2T
and the target object speed S, the appropriate radius is therefore R = ST /π. Taking an xy plane on the sensor, the sensor
motion goes through a spiral in the xyt space-time volume as
shown in red in Fig. 1(b).
y

Sensor

t
x

y

Lens

Radius R

(a)

Fig. 2 shows simulated motion blur PSFs and their power
spectra of various object motions observed from a static
camera, the coded exposure camera [RAT06], the motioninvariant camera [LSC∗ 08], and our circular motion camera. While the power spectrum of a static object for a static
camera is perfectly broadband, those of moving objects become quickly narrowband as the object speed increases. The
coded exposure camera makes power spectra broadband at
the cost of losing light blocked by the shutter, but the tendency of bandwidth narrowing for faster motion remains.
The motion-invariant camera produces similarly broadband
power spectra for horizontal motions (they are not completely identical due to the tail clipping effect [LSC∗ 08]),
but vertical frequencies are sacrificed as motion direction
deviates from horizontal. The circular motion camera produces power spectra that extend to high frequency regions in
all cases. Although they have striped frequency zeros, these
zeros facilitate PSF estimation as described in Sec. 5.

Exposure
time 2T

x
Optical axis

1871

4. Analysis

(b)

Figure 1: Circular sensor motion. (a) The sensor is translated circularly about the optical axis. (b) Sensor motion
trajectory (red curve) in the space-time volume.
(1)
(2)

Levin et al. [LSC∗ 08] proved that constantly accelerating
1D sensor motion is the only sensor motion that makes PSF
invariant to 1D linear object motion. From this finding we
can see that there is no sensor motion that makes PSF invariant to 2D linear object motion. Hence, we must abandon
motion-invariance, and we seek to extend Levin et al.’s another finding that their sensor motion evenly and nearly optimally distributes the fixed frequency “budget” to different
object speeds.
The intuitive explanation for optimality of constant camera acceleration for 1D case is as follows. Fig. 3(a) shows the
range of speed [−S, +S] that must be taken care of. We can
cover the entire range by accelerating a camera beginning
at speed −S until it reaches +S. The camera tracks every
speed at one moment during exposure. By extending to 2D,
the range of velocity (speed + direction) we must cover becomes a disc as shown in green in Fig. 3(b). We are no longer
able to fill the entire disc by a finite sensor motion path, and
we opt to trace only the circumference of the disc (shown in
blue), which can be achieved by moving a sensor circularly.
The reasons for doing so are threefold.

(3)
(4)
(5)
(6)
(7)
(8)
(a)

(b)

(c)

(d)

(e)

(f)

(g)

Figure 2: Motion blur PSFs and their corresponding log
power spectra. Rows: (1) PSFs and (2) power spectra
for a static camera. (3)(4) Coded exposure camera. (5)(6)
Motion-invariant camera. (7)(8) Circular motion camera.
Columns: (a) Static object. (b)(c) Horizontal object motion
at different speeds. (d)(e) Oblique object motion. (f)(g) Vertical motion.
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1. It makes theoretical analysis easier. Although full frequency analysis of 3D xyt space-time is difficult, we were
able to draw some insights into frequency characteristics
of circular sensor motion.
2. Tracing the circumference alone can be shown to deal
with velocity in the interior of the disc fairly well.
3. It makes implementation of camera hardware easier.
Multi-revolution and multi-shot approaches: As for Reason 2 above, to further treat different object speeds evenly,
one can consider sampling the interior of the velocity disc
by a set of concentric circles. However, this does not bring
in significant improvement of PSF power spectra, since the

1872

Y. Bando, B.-Y. Chen, T. Nishita / Motion Deblurring from a Single Image using Circular Sensor Motion

phases of the Fourier transform of multiple circular motions
cancel each other when superimposed, resulting in a set of
power spectra as shown in Fig. 4(1), which is qualitatively
similar to the one shown in the bottom row of Fig. 2. That
is, for multiple PSFs whose Fourier transforms are given
as Fi ( fx , fy ) (i = 0, 1, · · ·), the combined power spectrum
| ∑i Fi ( fx , fy )|2 can be zero for some frequency (a, b) even
if |Fi (a, b)|2 > 0 for some or all i. If multiple shots were
allowed, the phase cancellation would not occur, and the
combined power spectrum would be ∑i |Fi ( fx , fy )|2 , meaning that frequency zeros of one PSF could be filled with
non-zero frequencies of the other PSFs [AXR09]. Therefore, whether or not to take multi-shot approaches depends
on a cost model for multiple shots (if there is no cost, taking many images is preferable, for example), which is left
as future work. For a single shot approach, moving a sensor
in two orthogonal directions as in [CLDF10] during exposure produces power spectra shown in Fig. 4(2), which have
tendency of bandwidth narrowing for faster object motion.
sy

sx
0

–S

+S

(a)

S

O

sx

(b)

Figure 3: The range of velocity (sx , sy ) that needs to be covered for (a) 1D case and (b) 2D case (shown in green). We
trace only the circumference of the disc (shown in blue).
(1)

where f = ( fx , fy ) is a 2D spatial frequency, ft is a temporal
frequency, and Ω spans the entire xy plane.
It can be shown that the 2D Fourier transform of a motion
blur PSF for object velocity v is a 2D slice of p(f,
ˆ ft ) along
the plane of ft = −v · f = −sx fx − sy fy (Fourier projectionslice theorem [Bra65]). Therefore, given a maximum speed
S, the volume in the 3D fx fy ft frequency domain that these
slices can pass through is confined to the outside of the cone
as | ft | ≤ S|f|, called the wedge of revolution in [CLDF10], as
shown in blue in Fig. 5(a). We would like | p(f,
ˆ ft )| to have
as large value as possible within this volume, so that motion
blur PSFs up to S have large power spectra. However, the
budget is exactly 2T along each vertical line f = c (the line
shown in red andR green in Fig. 5(a)) for any given spatial
ˆ ft )|2 d ft = 2T [LSC∗ 08].
frequency c: i.e., | p(c,

To assign the 2T budget so that any 2D linear object motion below S has a similar amount of PSF spectral power, we
consider the following two criteria.

Effectiveness: The budget should be assigned as much as
possible within the line segment of ft ∈ [−S|c|, +S|c|] which
is shown in red in Fig. 5(a). In other words, we would like
to avoid assigning the budget to the other portions of the
line (shown in green in Fig. 5(a)) as they correspond to object speeds beyond S and the budget will be wasted. Because
the budget is exactly 2T unless we close the shutter during
exposure, less assignment to some portion means more assignment to the other.
Uniformity: The budget should be distributed evenly across
the line segment, so that every object motion PSF has an
equal amount of spectral power.
Therefore, optimal assignment in which both effectiveness and uniformity are perfect gives T /S|c| to each point
on the line segment.

(2)
Figure 4: Power spectra of the motion blur PSFs from
(1) two-revolution circular sensor motion and (2) horizontal sensor motion followed by vertical one. The order of
columns is the same as in Fig. 2.

Now we review the frequency budgeting argument of
[LSC∗ 08] for the case of 2D object motion.
We consider a camera path in the xyt space-time volume.
δ(x − m(t))
0

for t ∈ [−T, +T ]
,
otherwise

Z Z +T
Ω −T

δ(x − m(t))e−2πi(f·x+ ft t) dtdx,

Ω

since the integrand is non-zero only at |x| = R and at t =
m−1 (x). Jacobian |dm(t)/dt| = Rω is introduced in the denominator. By using polar coordinates as x = r cos θ and
y = r sin θ,
p(f,
ˆ ft ) =

(2)

δ(|x| − R) −2πi ft m−1 (x) −2πif·x
e
e
dx, (3)
Rω

Z

(1)

where x = (x, y), m(t) specifies the camera position at time
t, and δ(·) is a delta function. We would like to consider its
3D Fourier transform, denoted by p:
ˆ
p(f,
ˆ ft ) =

Now we take the 3D Fourier transform of the circular sensor
motion m(t) = (R cos ωt, R sin ωt), a spiral in the xyt spacetime as shown in Fig. 1(b). By integrating Eq. (2) with respect to t, we obtain:
p(f,
ˆ ft ) =

4.1. Frequency Budgeting

p(x,t) =

4.2. Spectrum of Circular Sensor Motion

Z

Ω

δ(r − R) −2πi ft θ/ω −2πif·x
e
e
dx.
Rω

(4)

This is a hard-to-integrate expression, but we can proceed if
we focus on a set of discrete ft slices where k = 2π ft /ω is
an integer as shown in Fig. 5(b), as (see Appendix A):
| p(f,
ˆ ft )|2 = 4T 2 Jk2 (2πR|f|),

(5)

c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1873

Y. Bando, B.-Y. Chen, T. Nishita / Motion Deblurring from a Single Image using Circular Sensor Motion

S√ cx2 + cy2
cy
cx

ft

ft

ft

fy

k=4
k=3
k=2
k=1
k=0

J0(z)
J1(z)
J2(z)
J3(z)

0.5
fy

fy
fx

fx

fx

1

J20(z)

0

z

-0.5
0

(a)

J10(z)

(b)

5

10

(c)

15

20

25

30

(d)

Figure 5: (a) The cone defining the volume (shown in blue) whose slices passing through the origin correspond to the power
spectra of motion blur PSFs below the speed S. (b) Discrete ft slices. (c) fy slices. The hyperbolic intersections with the cone
are shown in purple. (d) Plots of Bessel functions Jk (z) of the first kind for some k, which correspond to the slices in (b).

where Jk (z) is the k-th order Bessel function of the first kind
[Wat22], which is plotted for some k in Fig. 5(d).
We show the effectiveness and uniformity of this distribution as described in Sec. 4.1. For effectiveness, we show
| p(f,
ˆ ft )|2 is small inside the cone | ft | ≥ S|f|, shown in white
in Fig. 5(a). By simple algebraic manipulation, we have
2πR|f| < k inside the cone. As can be observed in Fig. 5(d)
particularly clearly for k = 10 and 20, Bessel functions Jk (z)
start from zero at the origin (except for k = 0), and remain
small until coming close to the first maximum value, which
is known to be around z = k + 0.808618k1/3 > k [Wat22].
ˆ ft )|2 is
Therefore, Jk (z) is small for z < k, which means | p(f,
small inside the cone.
Next, we show the uniformity of the distribution. Eq. (5)
can be approximated using the asymptotic form of Bessel
functions [Wat22] for z ≫ k2 as:
| p(f,
ˆ ft )|2 ≈

kπ π
4 T
cos2 2πR|f| −
−
.
π S|f|
2
4

(6)

This equation indicates that, at any given spatial frequency
f which is sufficiently large, | p(f,
ˆ ft )|2 is a sinusoidal wave
with an amplitude of (4/π)(T /S|f|), which is independent
of ft and hence uniform along the ft direction. The amplitude itself is greater than the optimal assignment T /S|f|
as described in Sec. 4.1, and averaging the cosine undulation in Eq. (6) reveals that the assigned frequency power is
(2/π)(T /S|f|) on an average, meaning that the circular sensor motion achieves 2/π (about 64%) of the optimal assignment.
To verify the above argument, we show a numerically
computed power spectrum of a spiral in Fig. 6 by three fy
slices as shown in Fig. 5(c), along with the power spectra of
the other camera paths. The motion-invariant camera nearly
optimally assigns the budget for the fy = 0 slice corresponding to horizontal object motion, but it fails to deliver the budget uniformly for other cases. Our circular motion camera
distributes the budget mostly evenly within the volume of interest, with condensed power around the cone surface corresponding to the maximum value of Bessel functions, which
results in a moderate tendency to favor the target speed.
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

(1)

(2)

(3)

(4)

(a)

(b)

(c)

(d)

Figure 6: Camera paths in the space-time and 2D slices of
their 3D log power spectra. Purple curves show the intersections with the cone of target speed S. Rows: (1) Static
camera. (2) Coded exposure camera. (3) Motion-invariant
camera. (4) Circular motion camera. Columns: (a) Camera
path in the xt space-time. See Fig. 1(b) for the circular sensor motion path. (b) Slice at fy = 0. (c)(d) Slices off the fx ft
plane ( fy = 0).

5. PSF Estimation
As shown in the bottom row of Fig. 2, the power spectra
of PSFs resulting from circular sensor motion have different frequency zeros depending on object motion, serving as
cues for PSF estimation. According to the model presented
in [LFDF07], PSF discriminability between candidate PSFs
i and j can be measured using the following equation.
D(i, j) =

1
N

∑

fx , fy

σ ( fx , fy )
σi ( f x , f y )
− log i
σ j ( fx , fy )
σ j ( fx , fy )

− 1,

(7)

1874

Y. Bando, B.-Y. Chen, T. Nishita / Motion Deblurring from a Single Image using Circular Sensor Motion

where N is the number of discretized frequency components,
and σi ( fx , fy ) is the variance of the frequency component at
( fx , fy ) in images blurred with PSF i, which is given as:
σi ( f x , f y ) =

β|Fi ( fx , fy )|2
+ η,
|Gx ( fx , fy )|2 + |Gy ( fx , fy )|2

(8)

where Fi and (Gx , Gy ) are the Fourier transforms of PSF i
and of the gradient operators, β is the variance of natural
image gradients, and η is the noise variance (we set β =
5.6×10−3 and η = 1.0×10−6 ). D(i, j) becomes large when
the ratio of σi and σ j is large, especially when either of them
is zero (i.e., their frequency zero patterns are different).
To compare the PSF discriminability for various capture
strategies, we generated a set of PSFs corresponding to all
possible (discretized) object motions, and plot in Fig. 7(a)
the minimum value of Eq. (7) among all the pairs of the
PSFs. We set the target object speed as S = 50 pixel/sec, and
considered object speed up to 1.5S. Motion direction and
speed were discretized by 15◦ and 5 pixels/sec, respectively.
As shown by the red line, all of the capture strategies except
ours have (almost) zero discriminability. This is because objects moving in opposite directions at the same speed produce (almost) the same motion blur except for the circular
motion camera (see Fig. 7(b)(c)). We also plot the PSF discriminability (green line) apart from this 180◦ ambiguity by
limiting object motion direction to [0◦ , 165◦ ]. In this case,
too, the circular motion camera gained the highest value.

PSF discriminability

2

6. Experiments
Simulation: We evaluated the frequency preservation
gained from the various image capture strategies by simulating motion blur for a set of 12 natural images, and by measuring the mean squared errors (MSE) between the Wienerdeconvolved images and the original unblurred images. Example images are shown in Fig. 8. Fig. 9 plots the deconvolution noise increase in decibels as 10 log10 (MSE/σ2 ), where

(1)

(2)
4.8 dB

34.3 dB

34.8 dB

35.3 dB

11.1 dB

31.0 dB

32.3 dB

31.9 dB

21.6 dB

24.5 dB

31.2 dB

32.5 dB

28.1 dB

26.7 dB

26.1 dB

25.6 dB

(b)

(c)

(d)

(3)

(4)

All motions
Non-opposite

1.5

(5)

1
0.5

(6)
0
Static
Coded
Motion- Circular
camera exposure invariant
(ours)
Capture strategy (camera motion)

(a)

(b)

(c)

(7)

Figure 7: (a) Plot of PSF discriminability. (b) PSFs of 45◦
object motion direction for static, coded exposure, motioninvariant, and circular motion cameras (from top to bottom).
(c) PSFs of 225◦ direction at the same speed.
Thanks to this high PSF discriminability, simple hypothesis testing works well in estimating PSFs for the circular
motion camera, for which we used so-called MAPk estimation [LWDF09]. We examine all possible object motions and
pick the motion (equivalently the PSF) that gives the largest
value for the following log posterior probability distribution.
log p(Fi |B) =

∑

fx , fy

log(σi ( fx , fy )) +

|B( fx , fy )|2
,
σi ( f x , f y )

(9)

where B is the Fourier transform of a motion blurred image.

(8)

(a)

Figure 8: Simulated motion blurred images and their Wiener
deconvolution results. The values indicate deconvolution
noise increase. Rows: (1) Blurred and (2) deblurred images
for a static camera. (3)(4) Coded exposure camera. (5)(6)
Motion-invariant camera. (7)(8) Circular motion camera.
Columns: (a) Static object. (b)(c)(d) Horizontal, oblique,
and vertical object motion at the target speed S.

c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1875

o

Horizontal (0 )

0

40
35
30
25
20
15
10
5
0

10 20 30 40 50 60 70
Object speed [pixels/sec]
Static

o

Oblique (45 )

0

Noise increase [dB]

40
35
30
25
20
15
10
5
0

Noise increase [dB]

Noise increase [dB]

Y. Bando, B.-Y. Chen, T. Nishita / Motion Deblurring from a Single Image using Circular Sensor Motion

10 20 30 40 50 60 70
Object speed [pixels/sec]

Coded exposure

40
35
30
25
20
15
10
5
0

o

Vertical (90 )

0

Motion-invariant

10 20 30 40 50 60 70
Object speed [pixels/sec]

Circular (ours)

Figure 9: Plots of deconvolution noise increase for different object speeds and directions. The exposure time is 1 sec for all the
cameras. The vertical gray lines indicate the target object speed S = 50 pixels/sec for the motion-invariant camera and ours.
The length 50 code containing 25 ‘1’s [AXRT10] was used for the coded exposure camera (half the light level).

we assumed noise corruption for motion blur to be Gaussian of standard deviation σ = 10−3 for [0, 1] pixel values.
The motion-invariant camera shows excellent constant performance for horizontal motion up to the target speed S, but
for other directions, deconvolution noise increases for faster
object motion. The coded exposure camera and ours have
no such directional dependence. The coded exposure camera performs almost as perfectly as a static camera for static
objects with marginal increase in deconvolution noise due to
light loss, and the noise gradually increases for faster object
motion. The circular motion camera also maintains stable
performance up to and slightly beyond S. It moderately favors the target speed S, for which it has lower deconvolution
noise than the other cameras except for the motion-invariant
camera for horizontal object motion. The downside of our
strategy is the increased noise for static objects.

For deblurring, we performed the PSF estimation described in Sec. 5 for each user-segmented object, and applied the deconvolution method of Shan et al. [SJA08]. The
deblurred objects and the background are blended back together. The PSF estimation took 20 min for a 512 × 512
image on an Intel Pentium 4 3.2GHz CPU.
Fig. 11 shows an example of multiple objects moving in
different directions and at speeds. The digits and marks on
the cars are visible in the deblurred image. For comparison,
Fig. 12 shows closeups of the results from the static and
circular motion camera images, in which we used simpler,
Wiener deconvolution to better demonstrate high frequency
preservation. More details were recovered for the circular
motion camera image with less deconvolution noise.

Real examples using a prototype camera: For prototyping
we placed a tilted acrylic plate inside the camera lens mount
as shown in Fig. 10, and rotated it so that the refracted light
rays were translated circularly. The plate is 3mm thick with
a refraction index of 1.49, and the tilt angle is 7.7◦ , resulting
in a circular motion radius R of 0.13mm. This corresponds
to 5 pixels in our setup, and the target object speed is S =
31.4 pixels/sec with the exposure time 2T = 1.0 sec.
Side view
Camera
body

Worm gear
Motor

(a)

(b)

(c)

Figure 11: Toy cars. (a) From a static camera. (b) From the
circular motion camera. (c) Deblurring result of (b).

Sensor
Ring gear + acrylic plate

Figure 10: Prototype camera based on a Canon EOS 40D.
The lens is detached to reveal the modified lens mount. After
passing through the lens, incoming light (shown in red) is
displaced via the tilted acrylic plate, and the displacement
sweeps a circle on the sensor while the plate rotates (yellow).

c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

(a)

(b)

(c)

(d)

Figure 12: Comparison of Wiener deconvolution results for
the toy car example. (a)(c) Results for the static camera image. (b)(d) Results for the circular motion camera image.

1876

Y. Bando, B.-Y. Chen, T. Nishita / Motion Deblurring from a Single Image using Circular Sensor Motion

(a)

(b)

(c)

(d)

Figure 13: Squat motion. (a) From a static camera. (b) From the circular motion camera. (c) Deblurring result of (b). (d)
User-specified motion segmentation. Four regions are enclosed by differently-colored lines.

(a)

(b)

(c)

Figure 14: Moving people. (a) From a static camera. (b) From the circular motion camera. (c) Deblurring result of (b).

Fig. 13 shows an example of an object whose parts are
moving differently. Fig. 13(d) shows the user-specified motion segmentation, for which manual specification took less
than a minute. The regions overlap so that they can be
stitched smoothly at the borders after deconvolution. Details
such as fingers and wrinkles on the clothes were recovered.
Fig. 14 shows an example with a textured background.
Due to occlusion boundaries, artifacts can be seen around the
silhouettes of the people, but the deblurred faces are clearly
recognizable. It is worth mentioning that the circular motion
camera tells us that the man was moving downward while
the woman was moving leftward (not upward or rightward),
which is neither available information from the static camera
image in Fig. 14(a) nor from the other capture strategies.
We also note that details such as facial features are already
visible in Fig. 14(b) even before deconvolution, which were
successfully identified by an automatic facial feature point
detector as shown in Fig. 15. These motion identification and
recognizable image capture capabilities may be useful for
surveillance purposes.
Comparison using a high-speed camera: For comparison
with the other capture strategies, we used high-speed camera
images of a horizontally moving resolution chart provided
online [AXRT10]. Blurred images are simulated by averaging 150 frames from the 1,000 fps video, resulting in a 39pixel blur. The length 50 code was used for the coded expo-

(a)

(b)

(c)

(d)

(e)

(f)

Figure 15: Results of facial feature point detection [YY08]
for Fig. 14. (a)(d) Detection failed for the static camera image in Fig. 14(a), as the faces are severely blurred. (b)(e)
Detection succeeded for the circular motion camera image
in Fig. 14(b) even before they were deblurred, since the facial features are already visible. (c)(f) Detection also succeeded for the deblurred image in Fig. 14(c).

sure camera, spending 3 msec for each chop of the code. For
fair comparison, the motion-invariant and circular motion
cameras were targeted to an object speed of 50 pixels (not
39 pixels) per exposure time. We tilted the camera by 90◦
to simulate the “vertical” object motion relative to the camera. As shown in Fig. 16, the coded exposure deblurring produced a less noisy image than the static camera, but oblique
streaks of noise can still be seen. The motion-invariant camera produced a clean image for horizontal object motion, but
the result for vertical object motion exhibits severe noise.
The circular motion camera produced clean images for both
motion directions.
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

Y. Bando, B.-Y. Chen, T. Nishita / Motion Deblurring from a Single Image using Circular Sensor Motion

Motion-inv. (horizontal)

Circular (horizontal)

Coded exposure

Motion-inv. (vertical)

Figure 18: Blurred images used for the subjective evaluation. The shown images correspond to vertical object motion
captured with static, coded exposure, motion-invariant, and
circular motion cameras (from left to right). All of the six
pairs of these four images were presented to the subjects.
Relative recognizability scale

Static camera

1
0.8
0.6
0.4
0.2
0

Circular (vertical)

Figure 16: Comparison using high-speed camera images.
For each pair of images, the left one is a simulated blurred
image, and the right one is its deconvolution result. Please
magnify the images on the PDF to clearly see the differences.

Subjective evaluation of recognizability: We have argued
that motion-blurred objects are more recognizable with circular camera motion than the other image capture strategies.
To quantitatively back up this claim, we conducted subjective evaluation where 55 persons were asked which of presented images were more recognizable to them (i.e., image
textures and patterns such as facial features and text were
more clearly seen or readable). We used the three images
shown in Fig. 17, and we synthetically motion-blurred each
image with the four image capture strategies as shown in
Fig. 18. We presented every pair of the four blurred images to the subjects, and asked them to select either image (paired-comparison test). Therefore, 18 pairs were presented to the subjects (6 pairs for each image). This test was
done four times with different object motion: static, horizontal, oblique, and vertical. From the number of “votes” from
the subjects, relative recognizability can be quantified using
Thurstone’s method [Thu27] as shown in Fig. 19. For static
objects, recognizability was of course the best with a static
camera, but for moving objects, the circular motion camera
gained the highest values for all of the motion directions.

1877

Object
motion

Static
Coded
Motion- Circular
camera exposure invariant
(ours)
Capture strategy (camera motion)
Static
Horizontal

Oblique
Vertical

Figure 19: Relative recognizability scale between various
image capture strategies.

7. Conclusions
We have proposed a method to facilitate motion blur removal
from a single image by translating a camera sensor circularly
about the optical axis during exposure, so that high frequencies can be preserved for a wide range of in-plane linear object motion within some target speed. We analyzed the frequency characteristics of circular sensor motion, and investigated its trade-offs between other image capture strategies.
The advantages include reduced deconvolution noise at the
target speed, improved PSF discriminability, and image recognizability without deconvolution.
The prototype implementation of the camera hardware
may appear complicated, but it will be much simpler as
technologies advance. In this paper we confined ourselves
to in-plain linear object motion, and we also assumed userspecified motion segmentation. We would like to address
these limitations in the future. Another issue of our method
is that static objects are also blurred. One way to alleviate
this is to pause the sensor for a fraction of exposure time.
We intend to investigate ways to control the degree to which
static and moving objects are favored relative to each other.
Acknowledgments

Figure 17: Original images used for the subjective evaluation, presented only once in the beginning of the test.

c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

We thank Takeshi Naemura, Yusuke Iguchi, Yasutaka Furukawa, and the anonymous reviewers for their feedback;
Paulo Silva, Pablo Garcia Trigo, Napaporn Metaaphanon,
Yusuke Tsuda, and Saori Bando for their help; and those who
kindly participated in the subjective evaluation.

1878

Y. Bando, B.-Y. Chen, T. Nishita / Motion Deblurring from a Single Image using Circular Sensor Motion

References
[AR09] AGRAWAL A., R ASKAR R.: Optimal single image capture for motion deblurring. In CVPR (2009), pp. 2560–2567.

[NKZN08] NAGAHARA H., K UTHIRUMMAL S., Z HOU C., NA YAR S. K.: Flexible depth of field photography. In ECCV (2008),
pp. 60–73.

[AX09] AGRAWAL A., X U Y.: Coded exposure deblurring: optimized codes for PSF estimation and invertibility. In CVPR
(2009), pp. 2066–2073.

[RAP05] R AV-ACHA A., P ELEG S.: Two motion-blurred images
are better than one. Pattern Recog. Letters 26, 3 (2005), 311–
317.

[AXR09] AGRAWAL A., X U Y., R ASKAR R.: Invertible motion
blur in video. ACM TOG 28, 3 (2009), 95:1–95:8.

[RAT06] R ASKAR R., AGRAWAL A., T UMBLIN J.: Coded exposure photography: motion deblurring using fluttered shutter.
ACM TOG 25, 3 (2006), 795–804.

[AXRT10] AGRAWAL A., X U Y., R ASKAR R., T UMBLIN J.:
Motion blur datasets and matlab codes. http://www.umiacs.
umd.edu/~aagrawal/MotionBlur/, 2010.
[BEN04] B EN -E ZRA M., NAYAR S. K.: Motion-based motion
deblurring. IEEE Trans. PAMI 26, 6 (2004), 689–698.
[BEZN05] B EN -E ZRA M., Z OMET A., NAYAR S. K.: Video
super-resolution using controlled subpixel detector shifts. IEEE
Trans. PAMI 27, 6 (2005), 977–987.
[BLM90] B IEMOND J., L AGENDIJK R. L., M ERSEREAU R. M.:
Iterative methods for image deblurring. Proceedings of the IEEE
78, 5 (1990), 856–883.
[Bra65] B RACEWELL R. N.: The Fourier transform and its applications. McGraw-Hill, 1965.
[CL09] C HO S., L EE S.: Fast motion deblurring. ACM TOG 28,
5 (2009), 145:1–145:8.
[CLDF10] C HO T. S., L EVIN A., D URAND F., F REEMAN W. T.:
Motion blur removal with orthogonal parabolic exposures. In
IEEE Int. Conf. Computational Photo. (2010), pp. 1–8.
[DMY10] D ING Y., M C C LOSKEY S., Y U J.: Analysis of motion
blur with a flutter shutter camera for non-linear motion. In ECCV
(2010), pp. 15–30.
[FSH∗ 06] F ERGUS R., S INGH B., H ERTZMANN A., ROWEIS
S. T., F REEMAN W. T.: Removing camera shake from a single photograph. ACM TOG 25, 3 (2006), 787–794.
[Jia07] J IA J.: Single image motion deblurring using transparency. In CVPR (2007), pp. 1–8.
[JKZS10] J OSHI N., K ANG S. B., Z ITNICK C. L., S ZELISKI R.:
Image deblurring using inertial measurement sensors. ACM TOG
29, 4 (2010), 30:1–30:8.
[JSK08] J OSHI N., S ZELISKI R., K RIEGMAN D.: PSF estimation
using sharp edge prediction. In CVPR (2008), pp. 1–8.
[KH96] K UNDUR D., H ATZINAKOS D.: Blind image deconvolution. IEEE Signal Processing Magazine 13, 3 (1996), 43–64.
[KTF11] K RISHNAN D., TAY T., F ERGUS R.: Blind deconvolution using a normalized sparsity measure. In CVPR (2011),
pp. 1–8.
[Lev06] L EVIN A.: Blind motion deblurring using image statistics. In Advances in Neural Information Processing Systems
(NIPS) (2006).
[LFDF07] L EVIN A., F ERGUS R., D URAND F., F REEMAN
W. T.: Image and depth from a conventional camera with a coded
aperture. ACM TOG 26, 3 (2007), 70:1–70:9.
[LSC∗ 08] L EVIN A., S AND P., C HO T. S., D URAND F., F REE MAN W. T.: Motion-invariant photography. ACM TOG 27, 3
(2008), 71:1–71:9.
[LWDF09] L EVIN A., W EISS Y., D URAND F., F REEMAN W. T.:
Understanding and evaluating blind deconvolution algorithms. In
CVPR (2009), pp. 1964–1971.
[MLHR09] M OHAN A., L ANMAN D., H IURA S., R ASKAR R.:
Image destabilization: programmable defocus using lens and sensor motion. In IEEE Int. Conf. Computational Photo. (2009),
pp. 1–8.

[SJA08] S HAN Q., J IA J., AGARWALA A.: High-quality motion
deblurring from a single image. ACM TOG 27, 3 (2008), 73:1–
73:10.
[SW71] S TEIN E. M., W EISS G.: Introduction to Foureir analysis on Euclidean spaces. Princeton University Press, 1971.
[SXJ07] S HAN Q., X IONG W., J IA J.: Rotational motion deblurring of a rigid object from a single image. In ICCV (2007),
pp. 1–8.
[TDBL08] TAI Y.-W., D U H., B ROWN M. S., L IN S.: Image/video deblurring using a hybrid camera. In CVPR (2008),
pp. 1–8.
[Thu27] T HURSTONE L. L.: A law of comparative judgement.
Psychological Review 34, 4 (1927), 273–286.
[TKLS10] TAI Y.-W., KONG N., L IN S., S HIN S. Y.: Coded
exposure imaging for projective motion deblurring. In CVPR
(2010), pp. 2408–2415.
[Wat22] WATSON G. N.: A treatise on the theory of Bessel functions. Cambridge University Press, 1922.
[XJ10] X U L., J IA J.: Two-phase kernel estimation for robust
motion deblurring. In ECCV (2010), pp. 157–170.
[YSQS07] Y UAN L., S UN J., Q UAN L., S HUM H.-Y.: Image deblurring with blurred/noisy image pairs. ACM TOG 26, 3 (2007),
1:1–1:10.
[YSQS08] Y UAN L., S UN J., Q UAN L., S HUM H.-Y.: Progressive inter-scale and intra-scale non-blind image deconvolution.
ACM TOG 27, 3 (2008), 74:1–74:10.
[YY08] Y UASA M., YAMAGUCHI O.: Real-time face blending
by automatic facial feature point detection. In IEEE Int. Conf.
Automatic Face & Gesture Recognition (2008), pp. 1–6.
[ZGS10] Z HUO S., G UO D., S IM T.: Robust flash deblurring. In
CVPR (2010), pp. 2440–2447.

Appendix A: Fourier Transform of a Spiral
According to [SW71], 2D Fourier transform of a function
g(r)e−ikθ is given as G( fr )e−ikφ , where (r, θ) and ( fr , φ) are
the polar coordinates in the primal and frequency domains,
respectively (i.e., fr = |f| ≡ |( fx , fy )|), and we have:
G( fr ) = 2πi−k

Z ∞
0

g(r)Jk (2π fr r)rdr.

(A.1)

Applying this theorem to Eq. (4) with k = 2π ft /ω leads to:
p(f,
ˆ ft ) =

2πi−k

Z ∞
δ(r − R)
0

Rω

Jk (2π fr r)rdr e−ikφ

1
J (2π fr R) e−ikφ
ω k
= 2T Jk (2πR|f|) i−k e−ikφ .
= 2πi−k

(A.2)

c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

