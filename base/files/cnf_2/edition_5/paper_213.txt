DOI: 10.1111/j.1467-8659.2011.01905.x

COMPUTER GRAPHICS

forum

Volume 30 (2011), number 8 pp. 2156–2169

Non-Linear Beam Tracing on a GPU
Baoquan Liu1,2,∗ , Li-Yi Wei3 , Xu Yang4 , Chongyang Ma5 , Ying-Qing Xu4 , Baining Guo4,5 and Enhua Wu2,6

1 University of Bedfordshire, UK
Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences, China
3 Microsoft Research
4 Microsoft Research Asia
5 Tsinghua University, China
6 University of Macau, China
Baoquan.Liu@beds.ac.uk, liyiwei@stanfordalumni.org, {xuyang, yqxu, bainguo}@microsoft.com,
machy85@gmail.com, ehwu@umac.mo
2 State

Abstract
Beam tracing combines the flexibility of ray tracing and the speed of polygon rasterization. However, beam tracing
so far only handles linear transformations; thus, it is only applicable to linear effects such as planar mirror
reflections but not to non-linear effects such as curved mirror reflection, refraction, caustics and shadows. In
this paper, we introduce non-linear beam tracing to render these non-linear effects. Non-linear beam tracing is
highly challenging because commodity graphics hardware supports only linear vertex transformation and triangle
rasterization. We overcome this difficulty by designing a non-linear graphics pipeline and implementing it on top of
a commodity GPU. This allows beams to be non-linear where rays within the same beam do not have to be parallel
or intersect at a single point. Using these non-linear beams, real-time GPU applications can render secondary
rays via polygon streaming similar to how they render primary rays. A major strength of this methodology is that
it naturally supports fully dynamic scenes without the need to pre-store a scene database. Utilizing our approach,
non-linear ray tracing effects can be rendered in real-time on a commodity GPU under a unified framework.
Keywords: nonlinear beam tracing, real-time rendering, GPU techniques, reflection, refraction
ACM CCS: I.3.3 [Picture/Image Generation]: Display algorithms; I.3.7 [Three-Dimensional Graphics and
Realism]: Shading, Ray tracing

1. Introduction
Beam tracing was proposed by Heckbert and Hanrahan in
1984 [HH84] as a derivative of ray tracing that replaces individual rays with beams. One primary advantage of beam
tracing is efficiency, as it can render individual beams via
polygon rasterization, which can be efficiently performed by
today’s commodity graphics hardware. Beam tracing also

∗

The fundamental aspects of this work were conducted while
Baoquan Liu was visiting Microsoft Research Asia.
c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics
Association and Blackwell Publishing Ltd. Published by
Blackwell Publishing, 9600 Garsington Road, Oxford OX4
2DQ, UK and 350 Main Street, Malden, MA 02148, USA.

naturally resolves sampling, aliasing, and LOD issues that
can plague conventional ray tracing. However, a major disadvantage of beam tracing is that it so far handles only linear
transformations; thus, it is not applicable to non-linear effects
such as curved mirror reflection.
We introduce non-linear beam tracing to render non-linear
effects such as curved mirror reflection, refraction, caustics
and shadows. We let beams be non-linear where rays within
the same beam do not have to be parallel or intersect at a
single point. Beyond smooth ray bundles, our technique can
also be applied to incoherent ray bundles on bump-mapped
surfaces. We achieve all these by customizing the vertex

2156

B. Liu et al. / Non-linear Beam Tracing on a GPU

program to estimate a bounding triangle for the projection
region and the fragment program to render the projection
region out of the bounding triangle.
Our main technical innovations are: a general non-linear
beam tracing framework over the predominately linear ones
in prior methods (e.g. [HH84]); and a bounding triangle algorithm that is both tight and easy to compute, and remains so
for both smooth and perturbed beams. Our additional technical contributions include a parameterization that allows
higher order projection continuity across adjacent beams,
and a non-linear beam tree construction with proper memory
management supporting multiple beam bounces.
Our non-linear beam tracing approach has several advantages. First, because we utilize polygon rasterization for
rendering beams, the performance rides in proportion with
advances in commodity graphics hardware. This also makes
it easier to integrate our method with existing polygonrasterization based methods such as games. Secondly, our
algorithm naturally supports fully-dynamic or largepolygon-count scenes because it does not require pre-storage
of a scene database. Thirdly, we provide a unified framework
for rendering a variety of non-linear ray tracing effects that
have often been achieved via individual heuristics in previous
graphics hardware rendering techniques.
Our method has several limitations. Quality-wise, nonlinear beam tracing is only an approximation to ray tracing, and can only handle at most two levels of ray bounces.
Our result may shift the locations of the rendered reflections/refractions. The shift is usually not perceptually objectionable or even noticeable (e.g. Figure 1) and our rendered motions remain smooth with proper parallax and
occlusion effects. Performance-wise, our approach has a
worst case complexity of O(MN) with respect to the num-

2157

ber of beams N and scene triangles M. (The dependency
on N could be significantly reduced by frustum culling.)
Because of this, our approach is most suitable for large or
smooth interfaces (e.g. reflection off the mirrored teapot as in
Figure 1). However, we have found approximation via coarse
beams quite adequate because human perceptions are not
very sensitive to the accuracy of complex reflections or refractions.
2. Previous Work
2.1. Ray tracing
Ray tracing is the primary method for rendering global illumination effects. Most existing fast ray tracing approaches
store the scene geometry into a database, and pre-compute
certain acceleration data structures for efficient rendering.
As such, they are most suitable for rendering static scenes
[HSHH07]. Extending this approach for dynamic scenes has
proven to be a major challenge; despite impressive results by
recent techniques as surveyed in [WMG*07, KC07, WIP08,
Wal10, PBD*10], it remains difficult to match the performance of previous ray tracing approaches tailored for static
scenes. [PGS07] is a packet ray traversal implementation on
GPU via CUDA, which mainly tracing primary rays, shadow
rays and plane reflection rays for static scenes, and shows that
tracing secondary rays will be much slower than for primary
rays and shadow rays.
Purcell et al. [PBMH02] pioneered the exploration of GPU
ray tracing by storing the scene database. We provide an
alternative methodology for rendering ray tracing effects
for fully dynamic scenes: instead of storing scene geometry into a database and building acceleration data structures, we stream down scene geometry and render them
via GPU polygon rasterization (a philosophy pioneered by

Figure 1: Non-linear beam tracing on a GPU. This scene contains a mirror teapot reflecting complex objects in fully dynamic
motions. Such kinds of scenes can be efficiently rendered via our polygon rasterization based approach. See Section 9 for
detailed performance statistics. Quality-wise, even though our approach is only an approximation to ray tracing, the image
quality remains visually similar; for each pair of images the ray tracing result is on the left while our result is on the right.
The small images are close-ups around high curvature areas, including spout, lid and handle, rendered under different view
points.
c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2158

B. Liu et al. / Non-linear Beam Tracing on a GPU

[CHH02]). This allows us to naturally handle fully-dynamic
and large-polygon-count scenes. Furthermore, despite the
theoretical advantage of ray tracing over polygon rasterization in terms of depth complexity, so far polygon rasterization
on a GPU still outperforms either CPU- or GPU-based ray
tracers for rendering primary rays. Our analysis in Section 9
demonstrates that this performance advantage of polygon
rasterization carries over to secondary rays for large smooth
surfaces as well.
2.2. GPU techniques
Approximating ray tracing effects on a GPU has a long trail
of research efforts, as surveyed in [AMH02].
For reflection, the ultimate goal is to render curved reflection of nearby objects with fully dynamic motion in real
time. However, previous work either assumes distant objects
(i.e. environment map), planar reflectors [HH84], static scene
[YYM05] or limited motion for frame-to-frame coherence
[EMDT06]. Another possibility (e.g. [UPSK07, PMDS06])
is to approximate near objects as depth imposters from which
reflections are rendered via a technique similar to relief mapping. However, handling disocclusion and combining multiple depth-imposters can be difficult issues since they cannot
be resolved via hardware z-buffering.
To render fully dynamic reflections without the depth
sprite issues mentioned earlier, one possibility is to perform
fine tessellation of scene geometry as in [OR98, MPS07],
allowing curved mirror reflection of near objects with unlimited motion. However, because these techniques require fine
tessellation of scene geometry, they often maintain a high geometry workload regardless of the original scene complexity,
which is often quite simple for interactive applications such
as games.
Refraction poses a bigger challenge than reflection, as
even planar refraction can produce non-linear beams [HH84].
Common techniques often handle only nearly planar refractors [Sou05] or only far away scene objects [GLD06], but not
refraction of near objects through arbitrarily-shaped lens.
One possibility to approximate such effects is via imagebased heuristics [Wym05]. This is similar to [UPSK07], in
which the technique represents refracted objects as depth
sprites and shares a similar problem in depth compositing.
Ihrke et al. [IZT*07] renders refraction effects via propagating wave-fronts through pre-computed volumetric textures.
This technique can produce impressive refraction effects,
but it requires pre-computation of incoming radiance. Our
technique is complement to [IZT*07] as we can efficiently
compute incoming radiance from a complex and dynamic
scene. Caustics results from light focused by curved reflection or refraction and can be rendered by a standard two-pass
process as demonstrated in [WD06].
Hou et al. [HWSG06] is a multiperspective GPU technique, which can render limited non-linear effects. However,

this is not for general non-linear beam tracing as it only handles single-bounce reflections and refractions. In addition, the
technique is limited to a simple C0 parameterization and can
only render simple scenes in real-time due to the huge pixel
overdraw ratio caused by a rough bounding triangle estimation. Our technique, in contrast, supports full non-linear beam
tracing effects including multiple reflections/refractions and
a general math framework that supports parameterizations
with more than C0 continuity as well as a tight bounding
triangle estimation with a roughly constant overdraw ratio.
This non-linear projection idea is also utilized in [VC06]
for non-photorealistic rendering, [JMY*07] for a 360-degree
light field display, [LGT*07] for logarithmic shadow maps
and [GHFP08] for single-centre non-linear projections.
Dachsbacher et al. [DSDD07] introduces the notion of
anti-radiance to avoid explicit visibility computation for
global illuminations on a GPU. The technique can be considered complementary to our technique as it is most effective
for diffuse objects in static scenes, whereas our technique is
most effective for mirror reflections and refractions in dynamic scenes.
2.3. Linear beam tracing
Linear beam tracing has been applied for computing visibility [ORM07] or stitching together multiple pinhole cameras
for interesting effects [PRAV09]. These methods are complementary to our approach, aiming at different applications.
3. Overview of Our Approach
For clarity, we first provide an overview of our algorithm
as summarized in Algorithm 1. We present details for the
individual parts of our algorithms in the subsequent sections.
Algorithm 1: Overview of our system
function NonlinearBeamTracing()
B0 ← eye beam from viewing parameters
{Bij } ← BuildBeamTree(B0 ) // on CPU; Section 7
// i indicates depth in beam tree with j labels nodes in level i
foreach (i, j) in depth first order
RenderBeam(Bij ) // on GPU; Sections 4, 5.1, 6, 8
end
function {Bij } ← BuildBeamTree(B0 ) // Section 7
trace beam shapes recursively as in [HH84]
but allow non-linear beams bouncing off curved surfaces
function RenderBeam(B) // Sections 4, 5.1, 6, 8
foreach scene triangle
˜ ← BoundingTriangle( , B) // vertex shader; Section 6
RenderTriangle( ˜ , , B) // fragment shader; Section 5.1
end

Our non-linear beam tracing framework is divided into two
major parts: building the beam tree on CPU and rendering
beams on GPU, as illustrated in NonlinearBeamTracing().
All our beams take triangular cross sections and we compute

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2159

B. Liu et al. / Non-linear Beam Tracing on a GPU

(a)

p1

(b)

(c)

(d)

(f)

(e)

(i)

p2

(g)

(k)

(h)

(j)

Figure 2: Non-linear projection. Left panel: Rendering one scene triangle within one beam. The scene triangle is shown in
RGB colours whereas the beam base triangle is shown in yellow. Because of non-linear projection, the projected region may
have curved edges. Our rendering is performed by a combination of a vertex program that estimates the bounding triangle
(shown in black outline), and a fragment program that shades and determines if each bounding triangle pixel is within the true
projection. In this example, p1 lies within the true projection, but p2 does not. Right panel: Different cases of non-linear beam
projection. The projections are coloured so that the same RGB colours are assigned from a scene vertex to the corresponding
projected vertices. The number of projection regions are one for cases (a)–(e), two for cases (f–h, j, k) and three for case (i).
Our algorithm can handle all these cases correctly.
and record the three boundary rays associated with each beam
in BuildBeamTree().
The core part of our algorithm renders scene triangles
one by one via feed-forward polygon rasterization, as in
RenderBeam(). As illustrated in Figure 2, due to the nonlinear nature of our beams, straight edges of a triangle might
project onto curves on the beam base plane and the triangle might project onto multiple and/or non-triangular components. Such a non-linear projection cannot be directly
achieved by the conventional linear graphics pipeline. We
instead implement a non-linear graphics pipeline on a GPU
by customizing both the vertex and fragment programs as follows. For each scene triangle, our vertex program estimates
a bounding triangle that properly contains the projected region (BoundingTriangle()). This bounding triangle is then
passed down to the rasterizer; note that even though the projected region may contain curved edges or even multiple
components, the bounding triangle possesses straight edges
so that it can be rasterized as usual. For each pixel within the
bounding triangle, our fragment program performs a simple
ray–triangle intersection test to determine if it is within the
true projection; if so, the pixel is shaded, otherwise, it is
killed [RenderTriangle()].
Rendering results for each beam is stored as a texture on
its base triangle. The collection of beam base triangles is then
texture mapped onto the original reflector/refractor surfaces
for final rendering. For clarity, we call the collection of beam
base triangles corresponding to a single reflector/refractor
object the S (simplified) mesh. The resolution of the S mesh
trades off between rendering quality and speed. We associate
a S mesh with a reflector/refractor interface through either
mesh simplification or procedural methods, depending on
whether the interface is static or dynamic.
4. Beam Parameterization
As described in Section 3, we utilize beams with triangular
cross sections for easy processing. For each point on the

d3
d

d3
P3

d2

O
P

d1

d Q

P2

d1

(a) parameterization

d Q

d2

O
P

b
P1

d’ d 3
P3

P2

d1
H

b
P1

(b) projection

P3

d2

O
P

P2
b

P1

(c) transfer

Figure 3: Illustrations for several basic definitions in our
algorithm. In (a), the point P on the beam base b has a
−
→
−
→
corresponding ray origin Q and direction d . In (b), P is
−
→
the projection of scene point Q under the beam parameter−
→
ization in (a). In (c), H is the transfer of Q in the direction
−
→
d , regardless of the beam parameterization.

beam base triangle, our beam parameterization defines an
associated ray with a specific origin and direction. Our beam
parameterization for ray origins and directions must satisfy
the following requirements: (1) interpolating vertex values,
(2) edge values depend only on the two adjacent vertices,
(3) at least C0 continuity across adjacent beams to ensure
pattern continuity, (4) no excessive undulations inside each
beam, (5) easy to compute in a per pixel basis for efficient
fragment program execution and (6) easy bounding triangle
estimation for efficient vertex program execution. We provide
two flavours of parameterizations: a smooth parameterization
for coherent beams and a bump-map parameterization for
perturbed beams.

4.1. Smooth Sn parameterization
→ −
→
−
→ −
Given a beam B with normalized boundary rays { d1 , d2 , d3 }
defined on three vertices of its base triangle P1 P2 P3
(Figure 3a), our smooth parameterization of the ray origin
and direction for any point P on the beam base plane is
defined via the following formula:

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2160

B. Liu et al. / Non-linear Beam Tracing on a GPU

−
→
O =
−
→
d =

i j k
−
a→
ij k ω1 ω2 ω3 (ray origin),
i+j +k=n

−→ i j k
bij k ω1 ω2 ω3 (ray direction),

i+j +k=n

ωi =

area( P Pj Pk )
,
area( P1 P2 P3 ) (i,j ,k)=(1,2,3),(2,3,1),(3,1,2)

(1)

−→ −
→ −
→ −
→
−
a→
ij k = Lij k ( P1 , P2 , P3 ),
−→ −→ −
→ −
→ −
→
bij k = Lij k ( d1 , d2 , d3 ),

−
→
−→
where each Lij k belongs to a general function class L satisfying the following linear property for any real number s,
→ −
→
−
→ −
→ −
→
−
→ −
{ P1 , P2 , P3 }, and { d1 , d2 , d3 }:
−
→ −
→
−
→ −
→
−
→
−
→−
→
L ( P1 + s d1 , P2 + s d2 , P3 + s d3 )
−
→−
→ −
→ −
→
−
→−
→ −
→ −
→
= L ( P1 , P2 , P3 ) + s L ( d1 , d2 , d3 ).

(2)

We term this formulation our Sn parameterization. Note that
this is an extension of triangular Bezier patches [Far86] with
two additional constraints: (1) the identical formulations of
−→
−
a→
ij k and bij k in Equation (1), and (2) the linear property for
−
→
L as in Equation (2). These are essential for an efficient
GPU implementation and a rigorous math analysis as will
be detailed later. Even with these two restrictions, our Sn
parameterization is still general enough to provide desired
level of continuity. In particular, the C0 parameterization
in [HWSG06] (Equation 3) is just a special case of our Sn
parameterization with n = 1, and a S3 parameterization can
be achieved using cubic Bezier interpolation (see our derived
equations in Appendix, which is similar to the equations
−
→ −
→
proposed in [VPBM01]). Note that in general O = P unless
1
under the S parameterization.
−
→
−
→
−
→
−
→
O = ω1 P1 + ω2 P2 + ω3 P3 ,
−
→
−
→
−
→
−
→
d = ω1 d1 + ω2 d2 + ω3 d3 .

Figure 4: Comparison of beam parameterizations. Left
panel: S1 parameterization. Middle panel: S3 parameterization. Right-top/right-bottom panel: close up views around
the reflections of left/middle panel. For the S1 case, notice
the sudden direction change of the pencil reflection over the
teapot.

(3)

Even though in theory general Ck continuity might be
achieved via a large enough n in our Sn parameterization,
we have found out through experiments that an excessively
large n not only slows down computation but also introduces
excessive undulations. Consequently, in our current implementation, we have settled down for our S1 and S3 parameterizations. The tradeoff between S1 and S3 parameterizations
is that S1 is faster to compute (both in vertex and pixel programs) but S3 offers higher quality as it ensures C1 continuity
at S mesh vertices. This quality difference is most evident for
scene objects containing straight line geometry or texture, as
exemplified in Figure 4 .

4.2. Bump-map parameterization
−
→
−
→
For perturbed beams, we compute d and Q via our S1 parameterization followed by standard bump mapping applied

−
→
onto d . Due to the nature of perturbation we have found a
parameterization with higher level continuity unnecessary.
For efficient implementation and easy math analysis, we
→ −
→
−
→ −
assume all rays { d1 , d2 , d3 } point to the same side of the
beam base plane. The exception happens only very rarely for
beams at grazing angles. We enforce this in our implemen→ −
→
−
→ −
tation by proper clamping of { d1 , d2 , d3 }, that is setting the
−
→
z-component dz of d (in the local frame of the beam base)
to max(δ z , dz ) for a small positive value δ z .
5. Core Computations and Definitions
Our beam parameterization above is utilized in two major
computations of our algorithm: intersection, where a point
−
→
Q is found by intersecting a scene triangle with a ray pa−
→
rameterized via a point P on a beam base, and projection,
where an inverse computation is performed to find the corre−
→
−
→
sponding P on a beam base for a space point Q (Figure 3b).
We also define an additional operation, termed transfer, that
does not depend on the beam parameterization (Figure 3c).

5.1. Intersection
−
→
For each point P on a beam base, we first compute its ray ori−
→
−
→
gin OP and direction dP via Equation (1) (followed by bump
map if necessary). We then perform a simple ray–triangle
→
−
→ −
intersection between the ray (OP , dP ) and the space triangle
in question. This intersection operation can be easily imple−
→
mented in our fragment program to render each point P
˜
within the bounding triangle computed by our vertex program, as illustrated in Figure 2. If the ray does not intersect
−
→
the scene triangle, the pixel associated with P is killed immediately. Otherwise, its attributes (depth, colour or texture
−
→
co-ordinates) are determined from the intersection point Q
−
→
and P is shaded followed by frame-buffer write.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2161

B. Liu et al. / Non-linear Beam Tracing on a GPU
d3

d3

H3
t

(t)
a

P3

d2
t

d1
H1

t
P1

d2

H2

Q

(t )
a2
(t )
a1
P2

P3
d1

P2

b

b
P1

Figure 5: Geometric interpretation of our beam parameterization. The beam is defined by its base triangle P1 P2 P3 and
−
→
boundary directions { di }i=1,2,3 . (Left panel): An affine trian−
→ −
→
−
→
gle a (t) is defined by {Hi = Pi + t di }i=1,2,3 . In this case,
−
→
{ di }i=1,2,3 point away from each other so a (t) with different
t values will never intersect each other. (Right panel): When
−
→
{ di }i=1,2,3 not pointing away from each other, a (t) with different t values might intersect each other, such as a (t1 ) and
−
→
a (t2 ), causing any Q ∈
a (t1 ) ∩
a (t2 ) to have multiple
projections on the beam base b . The threshold plane T ,
not shown here, is below b in the left case and high on the
−
→
sky in the right case (when { di }i=1,2,3 eventually diverge).

5.2. Projection
−
→
Computing P on a beam base for a given space point
−
→
Q would require solving a cubic polynomial for the S1
parameterization (and higher order polynomials for our general Sn parameterization). This can be computationally expensive, but fortunately we never need to compute projections on a GPU as it is only needed for determining the bound−
→
ary directions { di }i=1,2,3 of each beam during the beam tree
construction phase on a CPU (Section 7).

to have at most one projection within the beam base triangle P1 P2 P3 . Affine planes/triangles have this ‘barycentric
−
→
invariance property’ that if H ∈ a has the same set of
−
→
barycentric coordinates (relative to a ) as P ∈ b (relative
−
→
−
→
to b ), then P is a projection of H on b . This property
allows us to solve the projection operation geometrically as
−
→
follows. Say we want to compute the projection P of a space
−
→
point Q . We start with a (t = 0) ≡ b and gradually lift
a (t) away from
b (by increasing t) until
a (t) contains
−
→
−
→
Q . We can then compute the barycentric coordinates of Q
−
→
relative to a (t), and use that to interpolate the projection P
from b (t).
−
→
When the three boundary directions { di }i=1,2,3 of a beam
point away from each other, a (t) will rise monotonically
from b with increasing t (Figure 5, left). Thus, each space
−
→
−
→
point Q has a unique projection P on the beam base. Otherwise, a (t) may go up and down several times (at most 3
for S1 ) before it eventually goes away, and this is the exact
cause for multiple projections, as illustrated in Figure 5 right
−
→
where Q can have different barycentric coordinates relative
−
→
−
→
to a (t1 ) and a (t2 ) (and thus different P1 and P2 on b ).
Fortunately, it can be shown that when a (t) is above the
threshold plane T , it will only rise monotonically.
5.5. Threshold plane

T

We define the threshold plane T of a beam B as an affine
plane a (tres ) with a particular value tres so that any space
−
→
point Q above T is guaranteed to have at most one projection within the beam base triangle P1 P2 P3 . Intuitively, the
beam frustum above the threshold plane T has diverging
ray directions.
We now define the threshold plane for our S1 parameteriza→
−
→
−→ −
tion (Equation 1). Let Pi,t = Pi + t di , i = 1, 2, 3. Define
the following three quadratic functions

5.3. Transfer
In our bounding triangle estimation algorithm later, we utilize
a specific operation termed transfer, defined as follows. A
−
→
−
→
scene point Q has a transfer H on a plane b in the direction
−
→
−
→ −
→
−
→
d if there exists a real number t so that Q = H + t d . In
other words, unlike our projection operation, transfer is a
pure geometric operation and has no association with the
beam parameterization.

5.4. Geometric interpretation
Even though we compute intersections and projections algebraically, we have found the following geometric interpretation very useful for understanding our algorithm. We
rely on the following two math quantities for computing
intersections and projections (Figure 5): (1) affine plane
a (t), which is in a t distance away from the beam base
plane b and contains affine triangle a (t) with vertices
→
−
→
−
→ −
Hi = Pi + t di , i = 1, 2, 3, and (2) threshold plane T ,
which is a special affine plane a (tres ) with a particular
−
→
value tres so that any space point Q above T is guaranteed

−→ −→
−→ −→ −
→
f1 (t) = (P2,t − P1,t ) × (P3,t − P1,t ) · d1 ,
−→ −→
−→ −→ −
→
f2 (t) = (P3,t − P2,t ) × (P1,t − P2,t ) · d2 ,
−→ −→
−→ −→ −
→
f3 (t) = (P1,t − P3,t ) × (P2,t − P3,t ) · d3 .
Solving the three equations separately f 1 (t) = f 2 (t) =
f 3 (t) = 0 we obtain a set of (at most 6) real roots . Denote tmax = max(0, ). The threshold plane T is then
defined as the affine plane a (tres ) so that tres > tmax
−→ −−→
−→
−−→
and min( Pres − Pmax , ∀Pres ∈ a (tres ), Pmax ∈ a (tmax )) >
0. Note that a (tres ) always exists as long as tres is large
enough. Intuitively, for t > tres and t > 0, we have
−
→ −→
−
→
−→
−
→
−→
P1,t + t · d1 , P2,t + t · d2 and P3,t + t · d3 on the same
side of a (t).
6. Bounding Triangle Estimation
Here, we describe our bounding triangle estimation algorithm as implemented in our vertex program. Our bounding

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2162
d1

B. Liu et al. / Non-linear Beam Tracing on a GPU
Q
H 12 1 H 21 H 22
H 11

Q

2

d2

d2

d1
Πt

Πa
Q1
Q

2

Πb
P1

P2

Πb
P 1 P 11

P 21 P 12 P22 P 2

Figure 6: 2D illustration of our bounding triangle algorithm. Here we plot two scenarios of diverging (left panel)
and converging (right panel) ray directions (corresponding
to the unique-projection and general cases in Algorithm 2).
The green points indicate projections of the scene triangle
−
→
−
→
vertices Q1 and Q2 . [HWSG06] computes the bounding triangle from the blue points while our algorithm from the red
points. Note that in the left (diverging) case, the overdraw
ratio can be arbitrary large for [HWSG06] when the scene
triangle is small or far away from the beam base plane. Our
algorithm, in contrast, always maintains a roughly constant
overdraw ratio. On the right (converging) case, our algorithm
reduces to [HWSG06] and the overdraw ratio is limited due
to the nature of converging rays. (In the left case a should
−
→
really pass through Q1 instead of the middle of the scene
triangle, but we plot it this way for clarity of illustration.)
triangle algorithm needs to satisfy two goals simultaneously:
(1) easy computation for fast/short vertex program and (2)
conservative and yet tight bounding region for low pixel
overdraw ratio, defined as the relative number of pixels of
the bounding triangle over the projected area.
In the following, we first present our bounding triangle algorithm for S1 , and then extend it to Sn and bump-map parameterizations. Our algorithms rely on the two math quantities
affine plane and threshold plane as described in Section 5.4.
6.1. Bounding

for S1 parameterization

Our bounding triangle algorithm follows a similar philosophy
of [HWSG06] so we begin with a brief description of their
−
→
algorithm. For each scene triangle with vertices {Qi }i=1,2,3 ,
[HWSG06] compute nine transfer (defined in Section 5.3)
points {Pij }i,j=1,2,3 on the beam base plane b where Pij
−
→
−
→
is the transfer of Qi in the direction of dj . A bounding
˜
triangle is then computed from {Pij }i,j=1,2,3 . We term this
algorithm general case as in Algorithm 2. It can be easily
shown that ˜ computed above is guaranteed to be a bounding
triangle for the projection of the scene triangle as long as
its three vertices completely lie within the beam. However,
the bounding triangle computed by [HWSG06] is too crude;
as the scene triangle gets smaller or further away from b ,
the overdraw ratio can become astronomically large, making
rendering of large scene models impractical. The reasoning
behind this phenomenon is illustrated in Figure 6.

Algorithm 2: Bounding triangle estimation algorithm for our S1
−
→
parameterization. ( P , ) indicates the computation of
−
→
barycentric coordinates of a point P on a triangle , and
−1 (ωk
,
)
is
the
inverse
operation
of computing a point
k=1,2,3
k
from a set of barycentric coordinates ωk=1,2,3
and a triangle .
function ˜ ← BoundingTriangle( , B) // vertex shader
−
→
{ di }i=1,2,3 ← boundary rays of B
−
→
{ Pi }i=1,2,3 ← base vertices of B
b ← base plane of B
b ← base triangle of B
T ← threshold plane of B
// all quantities above are pre-computed per B
−
→
{Qi }i=1,2,3 ← vertices of scene triangle
−
→
if {Qi }i=1,2,3 not all inside or outside B
// brute force case
˜ ← entire base triangle of B
else if not above T
// general case, e.g. Figure 6 right
foreach i,j = 1,2,3
−
→ −
→
Pij ← Transfer(Qi , dj , b )
˜ ← BoundingTriangle({Pij }i,j=1,2,3 )
else
// unique projection case, e.g. Figure 6 left
a ← affine plane of B passing through the top of
a ← affine triangle on a
foreach i,j = 1,2,3
−
→ −
→
Hij ← Transfer(Qi , dj , a )
E1 E2 E3 ← BoundingTriangle({Hij }i,j=1,2,3 )
foreach i = 1,2,3
−
→
{ωk }k=1,2,3 ← ({Ei }, a ) // barycentric coordinates
−
→
−1
{ Fi } ←
({ωk }k=1,2,3 , b )
end for
˜ ← F1 F2 F3
end if

We provide a better algorithm as follows. As illustrated
in Figure 6, the large overdraw ratio in our general case
algorithm only happens when the rays are diverging. We address this issue by first computing nine transfers {Hij }i,j=1,2,3
and the associated bounding triangle E1 E2 E3 similar to our
general case algorithm, but instead the base plane b these
computations are performed on an affine plane a that passes
through the top of the scene triangle . (If there are multiple affine planes a (t) that pass through the top, we choose
the one with largest t value.) We then transport vertices of
E1 E2 E3 on a to the vertices of F 1 F 2 F 3 on b so that
they share identical barycentric coordinates. It can be proven
that F 1 F 2 F 3 is guaranteed to be a proper bounding triangle as long as the scene triangle
is above the threshold
plane T . (Intuitively, the algorithm works because of the
barycentric invariance property of affine planes as described
in Section 5. This ensures that F 1 F 2 F 3 is a bounding triangle as long as E1 E2 E3 is also one.) Furthermore, since a
is much closer to than b , the bounding region is usually
much tighter and thus the overdraw ratio can be reduced.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2163

400
350
300
250
200
150
100
50
0

600
our method
[Hou et al. 2006]

overdraw ratio

overdraw ratio

B. Liu et al. / Non-linear Beam Tracing on a GPU
500

our method
[Hou et al. 2006]

400
300
200
100
0

20 40 60 80 100 120 140
scene triangle size

5 10 15 20 25 30 35 40 45 50
distance to beam base

Figure 7: Overdraw ratio comparison. Here, we plot the
pixel overdraw ratio of a single scene triangle being rendered
within one beam. On the left, we fix the scene triangle in space
but vary its size. On the right, we fix the scene triangle size
but vary its distance to beam base.
Because this algorithm is faster than our general case algo−
→
rithm but works only when each space point Q ∈
has
at most one projection in the base triangle (due to the fact
that is above T ), we term it our unique projection case
algorithm.
Since our unique projection case algorithm is not guaranteed to work when the scene triangle is not above the
threshold plane T , in this case, we simply keep our general
case algorithm. Unlike the diverging case, the region below
the threshold plane is mostly converging and the volume is
bounded, so the overdraw ratio is usually small as illustrated
in Figure 6 right.
A comparison between [HWSG06] and our algorithm is
demonstrated in Figure 7. Ideally, the overdraw ratio should
remain constant with varying scene triangle size and distance to beam base. Note that our algorithm remains low
and roughly constant while [HWSG06] exhibits exponential
growth. Consequently our algorithm runs much faster than
[HWSG06] as demonstrated in Section 9.
6.2. Bounding

for Sn parameterization

In addition to affine plane and threshold plane, our algorithm
for Sn parameterization depends on two additional quantities:
(1) extremal directions {d1 , d2 , d3 }, which are expansions
→ −
→
−
→ −
of the original beam directions { d1 , d2 , d3 } so that they
−
→
‘contain’ all interior ray directions d , and (2) maximum
offsets μb and μd , which are maximum offsets between the
Sn and S1 parameterizations of ray origin and direction for
−
→
all P on the base triangle b .
6.2.1. Extremal directions { di }i=1,2,3
Given a beam B with base triangle P1 P2 P3 , its extremal
directions is a set of ray directions {d1 , d2 , d3 } emanating
from {P1 , P2 , P3 } so that for each P within P1 P2 P3 there
exists a set of real numbers ω1 , ω2 , ω3 , 0 ≤ ω1 , ω2 , ω3 and
−
→
ω1 + ω2 + ω3 = 1 that can express d (P ) as follows:
−
→
d (P ) = ω1 d1 + ω2 d2 + ω3 d3 .

Extremal directions exist as long as all the beam directions
point away to the same side of the beam base plane. Note
−
→
that for S1 parameterization we could have di = di , but this
n
is not true for general S parameterization and bump mapped
surfaces. Extremal directions can be computed analytically
for Sn parameterization and from the maximum perturbation
angle δ for bump map parameterization.

6.2.2. Maximum offsets μb and μd
We define μb and μd as the maximum offsets between the Sn
and S1 parameterizations of ray origin and direction for all
−
→
P on the base triangle b :

μb = max

3

−
→
OP −

−
→
ωi Pi

−
→
,∀P ∈

−
→
ωi di

−
→
,∀P ∈

b

,

i=1

μd = max

3

−
→
dP −

(4)
b

,

i=1

−
→
−
→
where OP and dP are ray origin and direction com−
→
puted according to our Sn parameterization, 3i=1 ωi Pi and
−
→
3
1
i=1 ωi di are same quantities computed via our S parameterization, and {ωi }i=1,2,3 are the barycentric coordinates of
−
→
P with respect to b .

6.2.3. Threshold plane

T

We also need to modify the threshold plane definition from
our S1 parameterization (Section 5) for our Sn parameteriza−→
tion (Equation (1)) by replacing the condition min( Pres −
−→
−−→
−−→
Pmax , ∀Pres ∈ a (tres ), Pmax ∈ a (tmax )) > 0 with min
−→
−−→
−→ −−→
( Pres − Pmax , ∀Pres ∈ a (tres ), Pmax ∈ a (tmax )) > μb +
tmax μd (μb and μd are defined in Equation 4).
The algorithm described in Section 6.1 only works for
S1 parameterization but we can extend it for our general Sn
−
→
(n > 1) parameterization by replacing dj with dj followed
μb +tμd
,
by a proper -expansion of ˜ with distance ε = sin(θ
min )
where t is the t value of a relative to b , and θ min =
min(angle(di , a ), ∀i = 1, 2, 3).

6.3. Bounding

for bump-map parameterization

We pre-compute the maximum perturbation angle δ formed
by each ray and its unperturbed cousin sharing the same
origin as determined by our S1 parameterization. Given a
scene triangle Q1 Q2 Q3 , we first compute its S1 bounding
triangle via our S1 algorithm. We then extend this S1 bounding
triangle outward via a distance D computed via the following

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2164

B. Liu et al. / Non-linear Beam Tracing on a GPU

formula:
τ = δ + φ,

−→ → −→ −
−→
→
θ1 = arcsin(min(−
n · dQ1 , −
n · dQ2 , →
n · dQ3 )),
−
→ → −
→ → −
→
→
θ2 = arcsin(min(−
n · d1 , −
n · d2 , −
n · d3 )),
⎧
θ2
θ2 > τ,
⎪
⎪
⎨
θ2 ≤ τ and θ1 > τ,
θ3 = θ1
⎪
⎪
⎩τ
otherwise,
D=

max( Qi Pij , ∀i, j = 1, 2, 3) · sin τ
,
sin(θ3 − τ )

(5)

where φ is the maximum angle formed between any two rays
→
within the same beam, −
n is the normal of the beam base,
−
→ −→
P
−
Q
−→
−→
i
Qi
dQi = −
→ −→ with PQi being the un-perturbed projection
Qi −PQi
−
→ −
→
−
→
−
→
−
→
of Qi , di the direction at Pi , and Pij the transfer of Qi at
−
→
direction dj . Intuitively, this algorithm extends the original
S1 bounding triangle by an amount D so that to cover all
shifted projection locations caused by bump maps.

7. Beam Construction
Here we describe how we construct beam-related data structures: S mesh, beam tree and textures. All these operations
happen on a CPU.

Figure 8: Comparison of BART animation scenes between
ground truth via ray tracing (left) and our technique (right).
From top to bottom: museum, robots and kitchen. Please refer
to Table 1 for scene statistics.
7.3. Texture storage for multiple bounces

7.1. S mesh construction
For a static reflector/refractor, we build its S mesh via mesh
simplification [Hop96]. We have found this sufficient even
though in theory a view dependent technique might yield superior quality. For a dynamic reflector/refractor, care must be
taken to ensure animation coherence. Even though in theory
this can be achieved via [KG06], in our current implementation we simply use a procedural method to ensure real-time
performance.

7.2. Beam tree construction
We build a beam tree via a method similar to [HH84]; for
each primary beam B that interacts with a S mesh triangle
Q1 Q2 Q3 , we derive the spawned secondary beam(s) with
−
→
−
→
projection direction(s) {dij } for each Qi with respect to B.
Note that unlike [HH84] which handles only linear beams
−
→
−
→
and consequently each Qi has a unique value di , we have
dealt with general non-linear beam interactions which may
−
→
produce multiple values {dij }. The reason behind this is simple; similar to the diverse situations that might happen when
a scene triangle is projected onto a non-linear beam (as il−
→
lustrated Figure 2), a secondary beam vertex Q might also
have multiple projections onto the primary beam B.

For single-bounce beam tracing we simply record the rendering results over a texture map parameterized by a S mesh.
However, for multibounce situations (e.g. multiple mirrors
reflecting each other), one texture map might not suffice
as each beam base triangle might be visible from multiple
beams. For the worst case scenario where M beams can all see
each other, the number of different textures can be O(M n ) for
n-bounce beam tracing. Fortunately, it can be easily proven
that the maximum amount of texture storage is O(Mn) if we
render the beam tree in a depth-first order. In our current implementation we pack all beams belonging to one (bounce)
level into one large texture, and keep n textures for n-bounce
beam tracing.
8. Acceleration and Anti-aliasing
A disadvantage of our approach is that given M scene triangles and N beams, our algorithm would require O(MN) time
complexity for geometry processing. Fortunately, this theoretical bound could be reduced in practice by view frustum
culling and geometry LOD control, as discussed later. Furthermore, unlike [OR98, RH06, MPS07], which require fine
tessellation of scene geometry for curvilinear projections,
our approach achieves this effect even for coarsely tessellated reflector/refractor geometry due to the nature of our
method.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2165

B. Liu et al. / Non-linear Beam Tracing on a GPU
Table 1: Comparison using the BART animation scenes on 8800 GTX.

Kitchen

Robots

Museum

Hou et al. [HWSG06]

517 fps (1st)
0.042 fps (2nd)

157 fps (1st)
0.06 fps (2nd)

647 fps (1st, 10K )
1.76 fps (2nd, 10K )
512 fps (1st, 26K )
0.36 fps (2nd, 26K )
273 fps (1st, 76K )
0.10 fps (2nd, 76K )

Horn et al. [HSHH07] static,
with shadow, no antialiasing

120 fps (1st)
1.79 fps (2nd)

122 fps (1st)
2.92 fps (2nd)

130 fps (1st, 10K )
3.51 fps (2nd, 10K )
128 fps (1st, 26K )
2.43 fps (2nd, 26K )
120 fps (1st, 76K )
2.03 fps (2nd, 76K )

Umenhoffer et al. [UPSK07] 1
depth layer, no shadow

0.63 fps (2nd)

0.25 fps (2nd)

13.64 fps (2nd, 10K )
6.44 fps (2nd, 26K )
3.02 fps (2nd, 76K )

Zhou et al. [ZHWG08]

7.4 fps (2nd)

6.2 fps (2nd)

9.9 fps (2nd, 26K

Our method

517 fps (1st)
7.1 fps (2nd)

157 fps (1st)
6.6 fps (2nd)

647 fps (1st, 10K )
17.3 fps (2nd, 10K )
512 fps (1st, 26K )
9.36 fps (2nd, 26K )
273 fps (1st, 76K )
3.34 fps (2nd, 76K )

)

Note: The technique [HSHH07] assumes static scenes. The performance of [UPSK07] depends on the number of depth layers (produced via
depth peeling) used; here, we use the fastest possibility of just 1 depth layer, which is usually not enough to resolve all dis-occlusions but the
performance is still slower than ours. The performances of the museum scene depend on the number of triangles of the brown object. When
possible we also cite the performances according to the different number of ray bounces (primary) 1st, secondary 2nd, etc). For all scenes, our
technique renders shadows via cubemap shadow maps. When comparing with these previous methods, we obtained their source codes from the
corresponding authors, and we measured their performance on the same platform that we used for our own method.

For a linear view frustum, culling can be efficiently performed because each frustum side is a plane. However, in
our case, each beam has a non-linear view frustum where
each frustum side is a non-planar surface. To resolve these
issues, we adopt a simple heuristic via a cone which completely contains the original beam viewing frustum. Please
see Appendix for more details.
Geometry LOD can be naturally achieved by our framework; all we need to do is to estimate proper LOD for each
object when rendering a particular beam, and send down the
proper geometry. This not only reduces aliasing artefacts but
also accelerates our rendering speed.
We also perform anti-aliasing for each rendered beam. The
computed beam results are stored in textures, which in turn
are anti-aliased via standard texture filtering.
9. Results and Discussion
9.1. Quality and speed
We compare the quality and speed of our method against previous methods via (1) the BART animation scenes [LAM01],

a commonly used benchmark for dynamic ray tracing and (2)
a collection of scenes designed by ourselves that highlight
potential usage of our technique in gaming and interactive
applications.
Figure 8 compares rendering quality of BART animation
scenes between ground truth (via ray tracing) and our technique. As shown, even though our technique does not yield
identical results to ray tracing, the image quality remains
perceptually similar.
Table 1 compares performance of BART animation scenes
between our approach and a variety of previous techniques,
including both ray based [HSHH07, UPSK07, ZHWG08]
and polygon based [HWSG06]. As shown in Table 1, our
technique outperforms [HSHH07], one of the fastest ray tracers existing today. Our method is also faster than ray tracing
depth-sprite method [UPSK07], which would have problems
handling scenes with many depth layers such as reflecting
the fractured brown objects upon the mirror tube object in
the BART museum scene. Thus, the fact that our method
outperforms [HSHH07],[UPSK07] and [HWSG06] on the
same hardware via a non-trivial benchmark (BART) clearly
demonstrates the advantage of our method.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2166

B. Liu et al. / Non-linear Beam Tracing on a GPU

Table 2: Scene statistics.

Scene

No. scene

Figure 1
Figure 10
Figure 11a
Figure 11b
Figure 12a
Figure 12b
Figure 12c
Figure 12d
kitchen
robots
museum

78.1K
5.3K
4.0K
2.9K
4.3K
14.3K
10.2K
28K
110.5K
71.7K
26K

No. beam

S texture

FPS CPU/GPU

941
20482
11.2
1/88
960
10242
14.0
1/70
958 + 2458
20482
11.0
25/65
105 + 802
20482
15.0
11/55
450
10242
21.0
1/46
450
10242
17.0
1/57
1922
10242
16.0
1/61
800
10242
29.0
1/33
336
10242
7.1 16/125
52
20482
6.6 13/139
1281
512 × 2048 9.36 2/105

Figure 10: Bump map effects via beam tracing. These two
images demonstrate different amounts of bumpiness; however, because they share identical δ value, they have the
same performance as well.

Note: For multiple reflection and refractions we indicate the number
of beams per tree level (the root level containing a single eye beam
and is not shown for brevity). All frame rates are measured on 8800
GTX with a viewport size 1280 × 1024. The last column shows the
timing breakdown between CPU/GPU in ms. For fair comparison
with other techniques, we have turned off geometry LOD control for
timing measurement.
a

We also compared our performance with [ZHWG08],
a highly optimized GPU kd-tree ray tracer using CUDA.
As shown in Table 1, our approach performs similar to
[ZHWG08], but is faster for scenes containing large smooth
interfaces, for example the BART robots scene. Another advantage of our method over ray tracing [ZHWG08] is that
it is much easier to integrate into a polygon-rasterization
pipeline, for which many interactive applications such as
games belong to.

Figure 11: Multibounce beam tracing for multiple reflections (a) and refractions (b).
cause of the use of polygon-streaming, our performance is
relatively insensitive to viewport resolution. To further analyse the performance of our algorithm, we have also measured
our algorithmic dependence on triangle count, beam count,
and texture resolution via a simplified version of the BART
museum scene. As shown in Figure 9, our algorithm performance is most sensitive to number of scene triangles. The
performance also depends on number of beams but to a lesser
degree, because frustum culling would prevent the worst case
O(MN) situation from happening.

The performance of our algorithm primarily depends on
the number of scene triangles and number of beams (vertex
workload), as well as the S mesh texture resolutions (pixel
workload). (See Table 2 for detailed scene statistics.) Be90
# triangles 64K
128K
256K
512K
1024K
2048K

70
60

60
40

8
7.5

40

0

20

6

10

5.5

0
0

200

400

600

800
# beams

1000

1200

1400

1600

5
0

120

200

400

600

800 1000 1200 1400 1600 1800 2000 2200
# triangles (in thousands)

0

triangle # 64K
128K
256K
512K
1024K
2048K

80
70
60
fps

80
60
40

50

8
7.5

40

400

600

800 1000 1200 1400 1600 1800 2000 2200
# triangles (in thousands)

fixed: 10242 texture resolution

1000

1200

1400

1600

7

6

5

0
200

800
# beams

5.5

10
0

600

6.5

20

0

400

# beams 128
288
512
800
1152
1568

8.5

30

20

200

9

90
# beams 128
288
512
800
1152
1568

100

7
6.5

30

20

fps

50

texture resolution 128^2
256^2
512^2
1024^2
2048^2
4096^2

8.5

fps

fps

80

9
texture size 128^2
256^2
512^2
1024^2
2048^2
4096^2

80

fps

100

fps

120

b

0

500

1000

1500

2000

2500

3000

texture resolution (# pixels per side)

fixed: 512 beams

3500

4000

4500

0

500

1000 1500 2000 2500 3000 3500
texture resolution (# pixels per side)

4000

4500

fixed: 1024K triangles

Figure 9: Performance analysis showing algorithmic dependence on triangle count, beam count and texture resolution. For
each column of figures, we fix one of the three variables while vary the other two.
c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2167

B. Liu et al. / Non-linear Beam Tracing on a GPU

a

b

c

d

Figure 12: Dynamic interface with a variety of phenomena: a waving lake reflecting seagulls flying by (a), refraction of a group
of fish in the lake (b), caustics cast over a whale and lake bottom (c) and non-linear shadow of a seagull cast onto the lake
bottom (d).
9.2. Applications
Besides smooth surfaces, our technique can also be used to
render reflections off bump-mapped surfaces via our perturbed beam parameterization, as demonstrated in Figure 10.
Unlike [HWSG06] which offers only single bounces, our
technique allows rendering up to two bounces of reflections
and refractions as demonstrated in Figure 11. A potential
issue is that the number of beams may increase exponentially
with the increasing number of beam bounces. Fortunately,
we have found two levels of beam-bounces usually enough,
as they provide bigger bang for the buck than subsequent
levels. Furthermore, due to the use of depth-first traversal,
the memory usage of our algorithm only increases linearly
with the level of beam bounces.
Our technique can also be used to render dynamic reflectors and refractors as demonstrated in Figure 12. In this
demo, the motion of the water surface is procedurally generated via sinusoidal motions and consequently our S mesh
simply moves along with that. We render caustic effects by
a two pass algorithm similar to [WD06, HWSG06]. In the
first pass, we render the caustics-receiver co-ordinates via our
non-linear beam tracing, and use this information for photon
gathering in a second pass. Non-linear shadows happen when
an object cast shadows through a reflection or refraction interface, such as a flying bird over a water surface, which can
be easily generated as a by-product of caustics.

10. Conclusions
The basic premise of ray tracing is to treat the scene as a
(more passive) database and the rays as (more active) queries,
and the key to acceleration is on how to organize the scene
database and/or ray queries for efficiency. Our approach essentially attempts to swap the active/passive roles of rays and
scenes, by treating rays as a more passive entity (e.g. stored
in textures) while the scene a more active entity (e.g. streaming instead of database storage). As shown in our results, our
methodology offers both advantages and disadvantages over
ray tracing. In particular, our approach is most suitable for

rendering reflections/refractions off large/smooth interfaces
with at most one or two levels of ray bounces. Beyond that,
the excessive number of beams would render our approach
inefficient.
Acknowledgements
The authors thank Yuan Tian, Junzhi Lu and other artists
for their help on building models and animations, Ligang
Liu for answering questions on Cn continuity parameterizations, Daniel Horn for answering questions about his GPU
ray tracer, Pat Brown for fixing NVIDIA OpenGL driver issues, Shay Barlow for lending his voice to our video making,
Dwight Daniels for help on writing, Xin Sun and Kun Zhou
for help on performance comparing, and the anonymous reviewers for their comments. Enhua Wu was supported by National 973 Research Grant (2009CB320802) and NSFC Grant
(60833007). Baoquan Liu was supported by the Marie Curie
project GAMVolVis (FP7-PEOPLE-IIF-2008- 236120).
References
[AMH02] AKENINE-MOLLER T., HAINES E.: Real-Time Rendering (2nd edition). A.K. Peters Ltd., Natick, MA, USA,
2002.
[CHH02] CARR N. A., HALL J. D., HART J. C.: The ray engine.
In Proceedings of the HWWS ’02 (Saarbrucken, Germany,
2002), pp. 37–46.
[DSDD07] DACHSBACHER C., STAMMINGER M., DRETTAKIS G.,
DURAND F.: Implicit visibility and antiradiance for interactive global illumination. In Proceedings of the SIGGRAPH
’07 (San Diego, California, 2007), p. 61.
[EMDT06] ESTALELLA P., MARTIN I., DRETTAKIS G., TOST D.:
A gpu-driven algorithm for accurate interactive reflections on curved objects. In Proceedings of the EGSR ’06
(Nicosia, Cyprus, June 2006).
[Far86] FARIN G.: Triangular bernstein-bezier patches. Computer Aided Geometric Design, 3, 2 (1986), 83–127.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2168

B. Liu et al. / Non-linear Beam Tracing on a GPU

[GHFP08] GASCUEL J.-D., HOLZSCHUCH N., FOURNIER G.,
PEROCHE B.: Fast non-linear projections using graphics
hardware. In Proceedings of the I3D ’08 (Redwood City,
California, 2008).
[GLD06] GENEVAUX O., LARUE F., DISCHLER J.-M.: Interactive refraction on complex static geometry using spherical
harmonics. In Proceedings of I3D ’06 (Redwood City,
California, 2006), pp. 145–152.
[HH84] HECKBERT P. S., HANRAHAN P.: Beam tracing polygonal objects. In Proceedings of the SIGGRAPH ’84 (Minneapolis, Minnesota, 1984), pp. 119–127.

Graphics Theory and Applications (Barcelona, Spain,
2007).
[OR98] OFEK E., RAPPOPORT A.: Interactive reflections on
curved objects. In Proceedings of the SIGGRAPH ’98
(Orlando, Florida, 1998), pp. 333–342.
[ORM07] OVERBECK R., RAMAMOORTHI R., MARK W. R.: A
real-time beam tracer with application to exact soft shadows. In Proceedings of the EGSR ’07 (Grenoble, France,
June 2007).

[Hop96] HOPPE H.: Progressive meshes. In Proceedings of
SIGGRAPH ’96 (New Orleans, Louisiana, 1996), pp.
99–108.

[PBD*10] PARKER S. G., BIGLER J., DIETRICH A., FRIEDRICH
H., HOBEROCK J., LUEBKE D., McAllister D., McGuire M.,
MORLEY K., ROBISON A., STICH M.: Optix: a general purpose ray tracing engine. ACM Transactions on Graphics,
29 (July 2010), 66:1–66:13.

[HSHH07] HORN D., SUGERMAN J., HOUSTON M., HANRAHAN
P.: Interactive k-d tree gpu raytracing. In Proceeding of
the I3D ’07 (Seattle, Washington, 2007).

[PBMH02] PURCELL T. J., BUCK I., MARK W. R., HANRAHAN
P.: Ray tracing on programmable graphics hardware. ACM
Transactions on Graphics, 21, 3 (July 2002), 703–712.

[HWSG06] HOU X., WEI L.-Y., SHUM H., GUO B.: Realtime multi-perspective rendering on graphics hardware.
In Proceedings of the EGSR ’06 (Nicosia, Cyprus, June
2006), pp. 93–102.

[PGS07] POPOV S., GUNTHER J., SLUSALLEK P.: Stackless kdtree traversal for high performance gpu ray tracing. In Proceedings of the EG ’07 (Prague, Czech Republic, 2007).

[IZT*07] IHRKE I., ZIEGLER G., TEVS A., THEOBALT C.,
MAGNOR M., SEIDEL H.-P.: Eikonal rendering: efficient
light transport in refractive objects. In Proceedings
of the SIGGRAPH ’07 (San Diego, California, 2007),
p. 59.
[JMY*07] JONES A., McDowall I., YAMADA H., BOLAS M.,
DEBEVEC P.: Rendering for an interactive 360-degree light
field display. In Proceedings of the SIGGRAPH ’07 (San
Diego, California, 2007), p. 40.

[PMDS06] POPESCU V., MEI C., DAUBLE J., SACK E.:
Reflected-scene impostors for realistic reflections at interactive rates. EG ’06, 25, 3 (2006), pp. 313–322,
doi:10.1111/j.1467-8659.2006.00950.x.
[PRAV09] POPESCU V., ROSEN P., ADAMO-VILLANI N.: The
graph camera. In Proceeding of the SIGGRAPH Asia ’09
(2009), pp. 1–8.
[RH06] ROGER D., HOLZSCHUCH N.: Accurate specular reflections in real-time. EG ’06, 25, 3 (2006), pp. 293–302,
doi:10.1111/j.1467-8659.2006.00948.x.

[KC07] KELLER A., CHRISTENSEN P.: Proceedings of the Symposium on Interactive Ray Tracing 2007. (Ulm University,
Germany, Sept 2007).

[Sou05] SOUSA T.: Generic refraction simulation. In Proceedings of the GPU Gems II (2005), pp. 295–305.

[KG06] KIRCHER S., GARLAND M.: Editing arbitrarily deforming surface animations. In Proceedings of the SIGGRAPH ’06 (Boston, Massachusetts, 2006), p. 71.

[UPSK07] UMENHOFFER T., PATOW G., SZIRMAY-KALOS L.:
Robust multiple specular reflections and refractions. GPU
Gems 3 (2007), pp. 387–408.

[LAM01] LEXT J., ASSARSSON U., M¨oller T.: A benchmark
for animated ray tracing. IEEE Computer Graphics and
Applications, 21, 2 (2001), 22–31.

[VC06] VALLANCE S., CALDER P.: Rendering multiperspective images with trilinear projection. In ACSC ’06:
Proceedings of the 29th Australasian Computer Science
Conference (Hobart, Australia, 2006), pp. 227–235.

[LGT*07] LLOYD B., GOVINDARAJU N. K., TUFT D., MOLNAR
S., MANOCHA D.: Practical logarithmic rasterization for
low-error shadow maps. In Proceedings of the Graphics
Hardware ’07 (San Diego, California, 2007).

[VPBM01] VLACHOS A., PETERS J., BOYD C., MITCHELL J. L.:
Curved pn triangles. In Proceedings of the I3D ’01 (San
Francisco, California, 2001), pp. 159–166.

[MPS07] MEI C., POPESCU V., SACKS E.: A hybrid backwardforward method for interactive reflections. In Proceedings of the Second International Conference on Computer

[Wal10] WALD I.: Fast construction of sah bvhs on the
intel many integrated core (mic) architecture. IEEE
Transactions on Visualization and Computer Graphics.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

B. Liu et al. / Non-linear Beam Tracing on a GPU

IEEE Computer Society Digital Library. IEEE
Computer Society: http://doi.ieeecomputersociety.org/10.
1109/TVCG.2010.251(07 Dec. 2010).
[WD06] WYMAN C., DAVIS S.: Interactive image-space techniques for approximating caustics. In Proceedings of I3D
’06 (Redwood City, California, 2006), pp. 153–160.
[WIP08] WALD I., IZE T., PARKER S. G.: Special section: parallel graphics and visualization: fast, parallel, and asynchronous construction of bvhs for ray tracing animated
scenes. Computer Graphics, 32, 1 (2008), 3–13.
[WMG*07] WALD I., MARK W. R., G¨unther J., BOULOS S.,
IZE T., HUNT W., PARKER S. G., SHIRLEY P.: State of the art
in ray tracing animated scenes. In Proceedings of the EG
’07 STAR (Prague, Czech Republic, 2007).

2169

−
→
proven that this cone properly contains every point Q inside
the original view frustum and it works for both Sn and bumpmap parameterizations.
Once the bounding cone is computed, we can use it for
frustum culling as follows. For each scene object (or each
scene triangle), we compute a bounding sphere with centre
−
→
O and radius R, and use the following steps to judge if the
−
→
sphere and cone intersect. First, we check if PC is inside the
R
sphere. If not, let θS = arcsin( −
→ −
→ ), which is the critical
| O −PC |)
−
→ −
→
angle formed between O − PC and the cone boundary when
the sphere and cone barely touch each other. Then, it naturally
follows that the sphere and cone intersect each other if and
only if
−
→ −
→
O − PC −
→
−
→ −
→ · dC > cos(θC + θS ).
O − PC

[Wym05] WYMAN C.: Interactive image-space refraction of
nearby geometry. In Proceedings of GRAPHITE 2005
(Dunedin, New Zealand, 2005).
[YYM05] YU J., YANG J., McMillan L.: Real-time reflection mapping with parallax. In Proceedings of I3D ’05
(Washington, DC, 2005), pp. 133–138.
[ZHWG08] ZHOU K., HOU Q., WANG R., GUO B.: Real-time
kd-tree construction on graphics hardware. ACM Transactions on Graphics, 27, 5 (2008), 1–11.

A.2. S3 parameterization
Our formula for S3 is derived directly from [VPBM01]:
→ −
→ −
→
−→ −
Lij k (X1 , X2 , X3 ) =

3! −
−
→ −
→ −
→
c→
ij k (X1 , X2 , X3 ),
i!j !k!
−
→ −→ −
→ −→ −
→
−
c→
300 = X1 , c030 = X2 , c003 = X3 ,
−
→ −
→ → −
−
→
ni , →
ni indicates normal at Pi ,
wij = (Xj − Xi ) · −

Appendix: Detailed Math Formulas

−
→ −
→
−
→
−
c→
210 = (2X1 + X2 − w12 n1 )/3,

A.1. Bounding cone
−
→
−
→
The cone center PC , direction dC and angle θ C are computed
as follows:

−
→ −
→
−
→
−
c→
120 = (2X2 + X1 − w21 n2 )/3,

(d1 + d2 + d3 )
−
→
,
dC =
(d1 + d2 + d3 )
−
→
−
→
−
→
θC = arccos(min(d1 · dC , d2 · dC , d3 · dC )),
→
−
→
−
→ −
→ −
→ max( OP − PG , ∀ P ∈ P1 P2 P3 )
−
→ −
,
PC = PG − dC ·
sin θC
−
→
where {d1 , d2 , d3 } is the set of extremal directions and PG
is the barycenter of beam base P1 P2 P3 . It can be easily

−
→ −
→
−
→
−
c→
021 = (2X2 + X3 − w23 n2 )/3,
−
→ −
→
−
→
c−→
012 = (2X3 + X2 − w32 n3 )/3,

(A.1)

−
→ −
→
−
→
−
c→
102 = (2X3 + X1 − w31 n3 )/3,
−
→ −
→
−
→
−
c→
201 = (2X1 + X3 − w13 n1 )/3,
−→ −→ −→ −→ −→
−
→
e = (−
c→
210 + c120 + c021 + c012 + c102 + c201 )/6,
−
→ −
→ −
→
−
→
v = (X1 + X2 + X3 )/3,
−
→
−
→ −
→
c−→
111 = e + ( e − v )/2.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

