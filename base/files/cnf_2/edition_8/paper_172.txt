DOI: 10.1111/j.1467-8659.2008.01178.x

COMPUTER GRAPHICS

forum

Volume 27 (2008), number 6 pp. 1647–1663

Image-Based Surface Compression
Tilo Ochotta1 and Dietmar Saupe2
1 University
2 University

of Utah
of Konstanz

Abstract
We present a generic framework for compression of densely sampled three-dimensional (3D) surfaces in order to
satisfy the increasing demand for storing large amounts of 3D content. We decompose a given surface into patches
that are parameterized as elevation maps over planar domains and resampled on regular grids. The resulting
shaped images are encoded using a state-of-the-art wavelet image coder. We show that our method is not only
applicable to mesh- and point-based geometry, but also outperforms current surface encoders for both primitives.
Keywords: height fields, 3D model coding, mesh coding
ACM CCS: I.4 [Coding and Information Theory]: Data compaction and compression

1. Introduction
Surface compression has become an important field of study
in graphics with the increasing demand on accessing threedimensional (3D) content. A surface encoder transforms an
explicit surface representation into a compact bit-stream,
which is then decoded at the receiver to generate a surface
reconstruction. Since the direct encoding of the standard representation of 3D surfaces that are given by lists of (x, y, z)coordinates and connectivity, leads to highly correlated data
streams, more sophisticated methods are needed to decorrelate the data. In particular, the requirements of a practical and
versatile compression scheme are as follows:
Effectiveness – the encoder should produce bit-streams as
compact as possible.
Efficiency – encoding and decoding should be efficient;
especially the capability of fast decoding is required when
the data need to be accessed in real time.
Simplicity – the algorithmic design of both, encoder as well
as decoder, should be simple and easy to implement
on common platforms.
We propose a compression scheme that is capable of processing densely sampled surfaces. To approximate a given
surface, the encoder decomposes the model into a set of
c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and
Blackwell Publishing Ltd. Published by Blackwell Publishing,
9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main
Street, Malden, MA 02148, USA.

patches, each of which is parameterized as an elevation map
over a planar domain and resampled on a regular grid. The
resulting representation consists of a set of images with arbitrary regions of support. The encoder compresses the shapes
by using a binary image coder and the elevation images
using a state-of-the-art shape-adaptive wavelet coder. The
decoder extracts the binary images and reconstructs the elevation images by wavelet synthesis, followed by performing the deparameterization. Figure 1 shows the Igea model
with 134k points with a partition of seven patches and reconstructed models. Figure 2 summarizes our compression
pipeline.
Our compression framework fulfills the mentioned requirements as follows:
Effectiveness – in our system, we are able to profit from well
developed existing image compression methods. Our
method outperforms current approaches for compression
of point-sampled geometry and is competitive with stateof-the-art mesh compression schemes, such as normal
mesh compression.
Efficiency – due to the simplicity of the parameterization and
efficient implementation of the wavelet synthesis, we
obtain the decoded model in short time. We can reconstruct the Dragon model of about 460k points within less
than 2 seconds on a common PC platform.

1647

Submitted November 2006
Revised February 2008
Accepted April 2008

1648

T. Ochotta and D. Saupe / Image-Based Surface Compression

2. Related work
2.1. Mesh geometry compression
3D model compression was studied since Deering [Dee95]
proposed to quantize all components of input mesh data to
a certain number of bits. In practice, most computer graphics systems focus on polygon meshes, for which there are
three kinds of data to transmit: geometry, connectivity and
attributes. Methods are needed to compress all three types of
data. However, research has focused mainly on the geometry
of the models, since it makes the largest part of the data. We
now give a very brief overview of mesh coding techniques,
for a more detailed review see [AG04].

Figure 1: Point-surface compression of Igea; top row: original model, partition and the base planes for parameterization; bottom row: reconstructed models at various bit-rates
in bits per point (bpp) and decoding fidelity in dB.
Simplicity – all stages in our pipeline rely on simple algorithms that can easily be implemented. In particular,
we use an elementary parameterization in order to represent 3D surface patches as 2D images. The mapping of a point in parameter space into 3D space is performed by a single matrix multiplication.
Moreover, our method has the following features: Firstly,
the capability to compress mesh- and point-based model using a unified framework, which allows selecting the appropriate primitive for each application. Secondly, the capability to
produce approximations that rely on regular grids, whose resolution can be adjusted by the user or by the encoder itself.
This allows for choosing the appropriate sampling density
for a specific application. Thirdly, the capability to reduce
the problem of 3D geometry compression to 2D compression; consequently, we can utilize existing high-performance
methods from image coding.
In this work, we extend our method [OS04] in that we report much improved results for point-sampled geometry and
extend our approach to mesh coding to demonstrate a surface coder that is independent of the underlying primitive.
Additionally, we incorporate the variational shape approximation scheme [AG04], which improves the overall performance. We start with reviewing previous work in mesh- and
point-based compression of 3D geometry in Section 2. In
Section 3 and 4, we detail our pipeline. Results are presented
in Section 5, followed by conclusions in Section 6.

In Taubin and Rossignac [TR98, Ros99] and Touma and
Gotsman [TG98], the connectivity of the mesh is encoded
separately. Vertex locations are predicted and the prediction residuals are entropy-coded or vector-quantized [CM02].
Connectivity coding was greatly improved [KPRW05], and
out-of-core geometry coding for extremely large meshes was
developed [IG03]. Karni and Gotsman [KG00] transform the
geometry into a frequency domain by performing Laplacian
analysis, followed by quantization of spectral coefficients.
The reconstructed models contain less high-frequency components but still guarantee a high visual quality. Sorkine et al.
[SCOT03] introduce an analog spectral quantization scheme,
but they rather focus on removing low-frequency components
that are not visible to the observer.
In some papers, it was recognized that one could achieve
excellent coding results when well-developed methods from
image wavelet coding are applied [KSS00, KG02, LCB03,
SRK02]. Since images are given in pixels on a regular grid,
the surface must be regularly resampled in order to apply
image-coding algorithms to geometry compression. The coordinate data at the semi-regular mesh is 3D by nature.
Nonetheless, the construction of the multi-resolution semiregular mesh can be organized so that only normal displacements are necessary to define the newly generated vertices
of the next finer level, yielding so-called normal meshes
[KG02]. In practice, however, the subdivision is algorithmically complex and not all detail displacement vectors can be
expressed by a single scalar value.

2.2. Point-based methods
In 3D model acquisition [LPC∗ 00], range scanners generate
point clouds that can be rendered directly without triangulation, i.e. using surface elements (surfels, [PZvBG00]), which
correspond to disks that locally approximate the surface for
hole-free point-based renderings. Rusinkiewicz and Levoy.
[RL00] and Botsch et al. [BWK02] proposed real-time software renderers that provide efficient encodings of the data
based on approximating the original points through centroids
of occupied octree cells. Nevertheless, these approaches rely
on the capability of rapid decoding to allow fast rendering

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Ochotta and D. Saupe / Image-Based Surface Compression

1649

Figure 2: Overview of our compression pipeline; the original model is given either as a point-based model or as a mesh; the
encoder constructs a parameterization of the input model and processes the resulting shaped images with a shape-adaptive
image coder; the model is reconstructed by decoding the bit-stream and combining the elevation maps to a complete surface;
the output model can be reconstructed and rendered either as a point-based model or again as a mesh.
that forbids employing enhanced and complex encoding
strategies in order to achieve a compression performance
comparable to that of mesh coders, e.g. [KSS00, KG02].
Some of the mesh compression strategies that are based on
quantization of vertex coordinates without using connectivity
are also suitable for compression of point sets, e.g. [Dee95,
PK05]. However, the best performing methods [KG02] rely
on a complete resampling of the surface, which requires an
appropriate surface representation if the input data is only a
point set. A direct meshing of the input point set is too difficult
to construct or in most cases too expensive and thus motivates
methods that achieve comparable compression performance
for point sets without meshing.
In [FCOAS03], a multi-resolution compression scheme
for point-sampled geometry was proposed that is based on
an embedded sequence of point sets, starting with a base
point set, which is encoded using a standard mesh compression method. Point set refinement is performed using
so called moving least squares (MLS) approximation over
local coordinate frames. Points are inserted combining estimated point positions and encoded -values. Waschb¨usch
et al. [WGE∗ 04] parameterize a given point-based model in
a hierarchical tree structure in order to recursively predict
point positions from corresponding tree nodes. The tree is
processed by the SPIHT encoder [SP96], yielding compact
bit-streams. In addition, point positions, attributes, e.g. normal vectors and colour can be compressed.
A joint compression and rendering framework was proposed in DuoDecim by Kr¨uger et al. [KSW05]. The point
coordinates were quantized and encoded relatively to each
other using a decomposition of the point set into runs. The resulting bit-stream was decoded on graphics hardware, which
makes this method perfectly appropriate for real-time applications and for processing of very dense data sets.
2.3. Patch-based parameterization
Surface parameterization is an active field in geometry processing, for an overview see [FH05]. A parameterization of

a surface is a mapping of a parameter space (subset in R2 )
to the 3D space. Lee et al. [LMH00] presented displaced
subdivision surfaces, where the original data set is approximated by a smooth control mesh, which in turn is displaced
by a scalar valued map. Sander et al. [SWG∗ 03] proposed
to represent a mesh by an atlas of charts that are designed
to minimize distortion. The detail coefficients are 3D and
thereby, the parameterization is more complex, since it does
not merely displace points orthogonally to a plane.
Ivanov and Kuzmin [IK01] proposed spatial patches as
rendering primitive for meshes. The model is approximated
by an atlas of regularly sampled charts that correspond to
height fields, properly placed in 3D space. A similar representation for point-based models has been proposed by
Pauly and Gross [PG01] for the purpose of spectral filtering.
Because their height fields are resampled to complete rectangular images, which results in heavy chart overlapping, a
blending technique is necessary when merging the processed
patches for surface reconstruction.
Focusing on efficient surface partitionings, Cohen-Steiner
et al. [CSAD04] proposed to cluster the surface and approximate it through a set of polygonal-shaped proxies, each
of which is equipped with a normal vector and a centroid,
yielding an arbitrarily shaped flat surface region. Considering various error metrics, they discussed a strategy to minimize a global surface approximation error based on concepts from vector quantization theory. As shown later, we
utilize this method to obtain height fields with possibly low
distortions.

2.4. Shape-adaptive image coding
Our compression method is based on approximating a given
geometric model by a number of encoded elevation maps
with arbitrary regions of support. Recently, wavelet-based
shape-adaptive image coding has been proposed, in which
wavelet coefficients corresponding to transparent image regions are considered insignificant [LL00]. We utilize shapeadaptive coding with binary set splitting [Fow04], which is

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1650

T. Ochotta and D. Saupe / Image-Based Surface Compression

easy to implement and yields the best rate-distortion performance in comparison to other approaches.
3. Compression framework
We consider a given 3D geometric model as a finite point
set P ⊂ R3 , e.g. points that were acquired by a 3D scanner.
Given P, we consider an associated surface S = S(P) that
provides a continuous approximation of the original surface.
When the input model is a mesh, S is given by the piecewise
linear interpolation of the points in P. In cases where the
model is only a set of points, we define S to be the MLS surface of P. The MLS surface is a smooth surface depending
on the input point set P that is defined without piecewise parameterization. Computationally, a point p ∈ S(P) is found
by applying a projection operator
to an arbitrary point
q ∈ R3 , p = (q). To compute the projection, a nonlinear
equation must be solved by applying an iteration procedure.
A point p ∈ S projects onto itself. Thus, the complete MLS
surface is given by the set of fixed points, S = {p | p =
(p)}. For details see [ABCO∗ 03].
3.1. Error measure
When compressing a surface given by a point set P, an
encoder produces a bit-stream that is converted back into another point set Pˆ at the decoder. In order to evaluate the quality of the approximation, we follow previous work [KG02,
LCB03] in the spirit of Metro [CRS98] and define a symmetric root-mean-square (rms) error and a peak-signal-to-noise
ratio (PSNR) based on surfaces S and Sˆ that are given by P
ˆ respectively.
and P,
The (one-sided) L 2 -distance of Sˆ as an approximation of
S is given by
ˆ S) =
d(S,

1
ˆ
area(S)

Sˆ

d(p, S)2 dp

1
2

,

(1)

where d(p, S) denotes the distance of the point p to the
surface S. The symmetric rms error is given by
ˆ S) = max d(S,
ˆ S), d(S, S)
ˆ .
Erms (S,

(2)

ˆ S) is computed by sampling the surface
In practice d(S,
Sˆ and averaging squared distances. The actual distances
d(p, S) are estimated by applying the projection operator,
d(p, S) ≈ ||p − (p)||, or are computed explicitly when S
is defined by a mesh.
, where d B is the
The PSNR is given by 20 log10 E dB(S,S)
rms ˆ
diagonal length of the bounding box of the surface S.
3.2. Parameterization
Given a subset S ⊆ S (a surface patch of S), the goal of the
parameterization is to find a mapping from a planar domain

Figure 3: Our parameterization corresponds to a mapping
of points in parameter space (base plane) to the surface;
three vectors e 1 , e 2 and n D build an orthonormal basis for
each point over the plane, (u, v, f (u, v)); the orthogonal
projection of the surface S onto the plane D provides a region
Q, which defines the support of the regularly sampled map
that is constructed in our framework.

to 3D space, such that its graph approximates the patch.
Given patch S, we consider a reference plane D = D(S)
defined by a normal vector n D and a reference point rD , D =
{x ∈ R3 | nD , x − rD = 0}, where ·, · denotes the scalar
product. A mandatory condition for S to be a height field is
that the angles between the normal vectors n S (p), p ∈ S and
the plane normal n D do not exceed 90◦ ,
A(S) = inf 1 −
p∈S

1
nS (p) − nD
2

2

≥ 0,

(3)

which follows from the cosine theorem. The aperture A(S) is
the cosine of the aperture angle of the normal cone defined
by the normals on S. In practice, we allow surface patches S
with a decreased aperture angle, A(S) ≥ , with = cos α,
0◦ < α < 90◦ . For computation of n D , we use smallest
enclosing balls [G¨ar99], which can be utilized to compute
the normal cone, consisting of the normal vector n D and an
aperture angle φ. We set n D to the normalized output of the
algorithm in [G¨ar99], arg minn∈R3 maxp∈S nS (p) − n .
Given the plane D = D(S), we consider the orthogonal
projection of the surface patch S onto D, yielding Q ⊂ D,
which we refer to as the support Q of surface patch S over
plane D, see Figure 3. We now define a local coordinate
system with origin in r D and two span vectors e 1 and e 2
on D that form an orthonormal system, providing (u, v)(or parameter-) coordinates for any point on D. We may
choose scaling factors l 1 , l 2 , such that l 1 e 1 , l 2 e 2 and the
reference point r D define a bounding box of the support Q
on the plane. Additionally, we define Qˆ as the set of (u,
v)-parameter coordinates with corresponding points in Q.
The local coordinate system allows to express any point
p on the surface patch S in parameter coordinates (u, v),
ˆ → R. Any point (u,
applying an elevation function f : Q

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Ochotta and D. Saupe / Image-Based Surface Compression

1651

ˆ can be thought of as an elevated
v, f (u, v)) with (u, v) ∈ Q
point over the plane that corresponds to a point on the original
surface patch S, see Figure 3.

3.3. Partitioning
The goal of the partitioning procedure is to decompose the
surface S into a set of patches, such that each patch can be
parameterized as an elevation map. The patches are collected
in a partition R = {Si }, ∪i Si = S, Si ∩ Sj = ∅, i = j .
We interpret the problem of finding such a partition as a
clustering problem, where each cluster has to fulfill condition
(3). In many clustering scenarios, the input data set is grouped
such that for a given number of clusters, a local minimum of
a certain global cost function is found [GG92]. In our setting,
the cost function J for a given partition R should capture the
encoding cost. We integrate the squared differences ||nS −
nDi ||2 , taking over the normal vectors nSi (x) belonging to
all points x on each surface patch S i , and where n D(S) again
denotes the normal vector of the corresponding reference
plane D(S),
J (R) =

J (S),

with

S∈R

J (S) =

(4)

Figure 4: Split-merge partitions: a large patch number after
splitting leads to irregular boundaries after merging, (a,b); a
smaller patch number after splitting straightens the boundary
shapes but slightly increases the final patch count, (c,d).

nS (x) − nD(S) 2 dx.
x∈S

The motivation behind this definition is the expectation that
large normal variations lead to height fields, which are rough
and, consequently, require a higher encoding rate. For the
evaluation of J(S), we approximate the integral by a finite
sum of appropriately weighted contributions at uniformly
distributed sample points on the surface patch S.
In the following, we describe and compare two partitioning
methods that we utilize to appropriately cluster the input
surface into patches.
Split-merge clustering. In the first version, we adopt the
split-merge approach from our previous work [OS04]. In a
nutshell, the complete surface initially belongs to one cluster,
which is recursively split using principal component analysis
until all clusters satisfy the normal cone condition. We use
some fixed threshold α < 90◦ as indicated in Section 3.2.
After the splitting phase, adjacent clusters are merged as long
as the the resulting merged surfaces fulfill the normal cone
condition. In contrast to Equation (3), where the parameter
α is chosen to adjust the maximum cone aperture angle, we
choose a relaxed threshold β during the merging process,
β ≥ α. The merging operations are implemented using a
priority queue, which sorts possible merging candidates after
increasing error increments. For details, see [OS04].
The parameters α and β are used to adjust the properties of
the final partition. In general, a small α (e.g. α = 10◦ ) leads to
a partition with many small clusters after splitting. Applying

a large angle β (e.g. β = 70◦ ) during merging leads to a
smaller number of clusters in the final partition. However,
the patches in this partition show irregular boundary shapes,
see Figure 4(b), which may lead to increased coding costs.
Increasing the parameter α reduces the number of clusters in
the initial partition and produces final partitions where the
patches have straight boundary curves, see Figure 4(d).
Generalized Lloyd’s algorithm (GLA). In [CSAD04], it
was shown that excellent partitionings can be obtained using
methods from vector quantization. In particular they applied
Lloyd’s algorithm [Llo82] to find a local minimum of the
cost function (4). We adopt their method and briefly describe
the procedure.
The search for the optimal partition is initiated by a number of randomly chosen seed triangles (or points) on the
surface that correspond to cluster centroids, each of which
is defined by a location on the surface and a normal vector
and coherently defines a plane (similar to the reference plane
from Section 3.2). The minimization of (4) is implemented
in two steps that are iteratively applied and work as follows.
Given the set of triangles that represent the cluster centroids, the goal is to assign all remaining triangles of the
model to clusters in order to build a surface partition. This
is achieved by employing a flooding technique, in which
triangles are subsequently assigned to the clusters, using
a priority queue. Specifically, for each centroid triangle

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1652

T. Ochotta and D. Saupe / Image-Based Surface Compression

Figure 6: A partition with unoptimized shape shows noisy
edges (left); triangle reassignemts based on boundary length
minimization reduces these artefacts (right).
Figure 5: Partitions obtained using the Generalized Lloyd’s
algorithm for surfaces [CSAD04] for 259 patches (a) and 14
patches (b); in contrast to the split-merge, the patches do not
show straight boundary shapes.
consider all (three) adjacent triangles and sort each of them
into a global priority queue according to the distance of its
normal vector to the normal vector of the centroid. Then,
remove the first triangle from the queue and assign it to the
respective cluster, and insert further (at most two) adjacent
triangles into the priority queue that have not yet been visited.
This procedure is repeated until the queue is empty, meaning
that all triangles were assigned to a cluster.
Once a partition is found, the cluster proxy (plane) of
each patch is refitted in the spirit of Lloyd iterations [Llo82].
Specifically, given a cluster of triangles, i.e. a surface patch
S, the normal n D of the plane D(S) is set to the average of
the normal vectors of the triangles in S, weighted according
to the triangle areas. The plane reference point is formally
set to the barycenter of S, although it does not affect the
approximation error (4).
The procedure of cluster flooding and centroid fitting is repeated until convergence or after a given number of iterations
is exceeded. Results of this partitioning method are shown in
Figure 5. In contrast to the split-merge approach, the GLA
distributes the patches on the surface more evenly, compare
Figure 4(c) and Figure 5(a). We furthermore observed that
for most-tested models, the GLA achieves a smaller number
of patches than the split-merge approach under a given patch
aperture constraint, Equation (3).

Boundary shape optimization
The overall compression performance depends on the structure of the elevation maps in two ways. Firstly, elevation
maps with large total variance in elevation values are disadvantageous for compression. This issue is addressed by
minimizing (4). Secondly, complex and irregular boundary
shapes produce higher coding costs for the binary masks than
shapes with more regular boundaries.
To complete the surface partitioning, we remove noise artefacts at patch boundaries by decreasing the total boundary

length. For any triangle with an edge along a patch boundary,
we compute the decrease in boundary length when reassigning the triangle to the adjacent patch. In an iterative procedure, we reassign triangles as long as there are candidates that
decrease the boundary length. Note that each reassignment
step may affect further reassignments of boundary triangles
in a local neighbourhood. For efficient processing, we maintain a priority queue that holds the candidates according to
their length decreases.
In addition to reassigning single triangles, we allow reassignments of connected triangle pairs, which improves the
boundary denoising. This method may also be extended to
triangle clusters of more than two triangles. Figure 6 shows
that the shape optimization adequately removes noise artefacts at the patch boundaries.
3.4. Resampling
After constructing the partition, we consider the projections
of the original points onto their respective patches, which
yields an arbitrary point distribution for each patch. In order
to apply the shape-adaptive wavelet coder, however, each
surface patch needs to be resampled on a regular grid.
Given patch S ∈ R, we define a sampling interval
consider grid points in (u, v)-space,
GS = {q = (

S i,

S j )|(i, j )

S

and

ˆ
∈ Z2 , q ∈ Q}

that belong to the patch S. For each patch S, we compute
the sampling interval 0S such that the number of resampled
points, |G S |, is roughly equal to the number of original points
in the patch. In order to control the total number of resampled
points in the model, we set the sampling interval in each
patch S to S = μ 0S , where μ > 0 is a scaling parameter.
Consequently, a global parameter of μ = 1 adjusts each grid
such that the number of points in the resampled model is
roughly equal to the number of original points.
In the resampling procedure, we compute for each grid
point q ∈ GS a corresponding elevation value f (q). If the
original surface S is a mesh, this can be achieved by projecting the original triangles onto the patch plane and identifying
the triangle, which contains the grid point on the plane. The

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Ochotta and D. Saupe / Image-Based Surface Compression

1653

Figure 7: Overview of the MLS-based resampling technique for point-based input models; the original point that is closest to
grid point p is projected onto the line through p and normal of the base plane, yielding p 1 (left); the tangential plane of the
surface at the projection (p 1 ) is intersected with the line, providing p 2 (middle); the procedure is iterated until convergence,
yielding the projected point (p) over the point p (right).

elevation value can then be directly interpolated from the
elevation values of the triangle vertices. In cases where S
is a MLS surface, a more sophisticated method has to be
employed as follows.
Similar the work in [AA03], the elevation values f (q) are
computed iteratively with a Newton-like method using the
MLS projection operator (see Section 3). Figure 7 illustrates the procedure. For a given point q on the grid G S , we
consider the corresponding point p ∈ R3 on the plane and
the line r through p and orthogonal to the plane. To initialize
the iterative procedure, we project the nearest neighbour of p
in P onto r, yielding p 1 . Then, we project p 1 onto the MLS
surface by applying and compute the tangential plane of
the surface at the projection. Then the intersection of this
tangential plane with the line r defines the new point p 2 ,
which is used for the next MLS projection. This procedure
is iterated until convergence, yielding the point (p) on the
MLS surface and the desired functional value f (q) for the
grid point q. Applying this procedure to all grid points and
ˆ
patches yields the complete resampled point model P.
3.5. Encoding
The foremost goal of the encoding is to enable the decoder to
reconstruct the elevation data for each patch parameterization
from the encoder output. Thus, for each patch, we encode side
information, binary masks and the scalar height field values
as follows.
Side information – consisting of the base plane rectangle and its grid sampling resolution for each patch. The
rectangle can be encoded using eight quantized floating
point numbers defining the coordinates of the origin and
two vectors corresponding to the sides of the base plane
rectangle.
Binary masks – defining the support of the height field
on each base plane rectangle. There are many methods for
bitmap encoding. In our implementation, we have used an
adaptive context-based arithmetic coder [LR81] for the binary masks of the height field supports.

Elevation maps – consist of the scalar height field values.
Our height fields are regularly sampled but have irregularly shaped supports. Thus, they can be regarded as grey
scale images with irregular boundaries. In image compression, efficient methods for shape-adaptive image coding were
developed [LL00]. The most successful methods are waveletbased using bitplane coding with embedded tree-structured
significance mapping as in the image coder SPIHT [SP96].
Fowler proposed such a coder, called BISK [Fow04], that
showed superior performance and has been made available
as part of QccPack library [Fow00]. We used this coder in
our implementation for image-based surface compression.
In a nutshell, the BISK coder operates as follows. The
input consists of a rectangular height field with the binary
mask defining the support of the height field as side information. The height field is wavelet transformed using
the 4-scale shape-adaptive 9/7 bi-orthogonal filter [LL00].
For the ‘opaque’ coefficients, an embedded bitplane code
is produced applying the paradigm of significance and refinement passes, which is common to most wavelet-based
image coders. Initially, all coefficients are taken as insignificant. In a significance pass, coefficients with a magnitude
greater than a threshold corresponding to the current bitplane are identified, marked as significant, and the sign of
each of these coefficients is coded. In the refinement pass,
for a bitplane, one additional bit for each previously marked
significant coefficient is coded. In significance passes, entire
sets of coefficients are tested jointly, and significant sets are
recursively subdivided. In the BISK coder, this subdivision
is a binary one, yielding a kd-tree subdivision of each coefficient band, and thus, it adapts to the shape of the image
(respective height field). For details see [Fow04].
3.6. Bit allocation
Given a set of resampled patches, approximating the original
surface, the shape-adaptive wavelet coder yields an embedded bit-stream for each individual patch. Consequently, for
each patch the output bit-stream may be truncated providing
a certain reconstruction quality at the decoder.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1654

T. Ochotta and D. Saupe / Image-Based Surface Compression

The straightforward bit allocation strategy is to distribute
the bits among the patches evenly. Specifically, each resampled patch PˆS , S ∈ R receives a total number of
|PˆS |
B
ˆ
|P|

bits,

Table 1: Approximation errors E rms obtained with the least-squares
fitting method for Venus, Rabbit and Sphere at various resolutions μ;
the numbers in brackets correspond to approximation errors without
applying the fitting method.

(5)

1/μ = 0.6

1/μ = 1.0

1/μ = 1.4

1.97 (2.35)
1.15 (1.37)
1.80 (2.27)

0.86 (1.12)
0.53 (0.63)
0.72 (0.98)

0.56 (0.70)
0.32 (0.37)
0.53 (0.66)

where B is the total bit budget given by the user and |PˆS |
ˆ denote the number of resampled points in patch S
and |P|
ˆ respectively. This
and the complete reconstructed model P,
allocation approach is straightforward, and has the advantage
that the complete bit-stream can easily be designed as an embedded stream by interleaving the individual patch streams.

4. Postprocessing for meshes

A more sophisticated bit allocation method is given by
rate-distortion optimization, which optimally distributes the
bits among the patches such that the overall reconstruction
quality is optimal. We achieve this by employing standard
discrete Lagrangian optimization [Eve63].

Our unified framework allows processing of mesh- and pointbased geometry. For mesh input models, however, we consider an optimization scheme to improve the approximation
quality, and a gap closing method, which provides hole-free
reconstructed meshes.

(i)
For each resampled patch PˆS , S ∈ R, let (R (i)
S , D S ),
i = 1, 2, . . . denote the (finitely many) rate-distortion points
achievable by truncating the coder output at different positions numbered by i = 1, 2, . . . . The rates R (i)
S include bits for
the base plane parameters, for the binary masks, and the truncated wavelet coefficient bit-stream. The distortion is given
by the one-sided L2 distance between the reconstructed surface patch and the full original model, see Equation (1). The
truncation index i = i(S) for patch S is given by

i(S) = arg min

i=1,2,...

RS(i) + λ · DS(i) ,

where λ ≥ 0 is the Lagrangian parameter. This yields a solution with total rate i RS(i(S)) with minimal total square
distortion i DS(i(S)) [Eve63]. In order to meet a prescribed
total rate constraint, we vary the Lagrange parameter λ, using
the bisection method.
(j )

The evaluation of the distortion D S for all j and S ∈
R is computationally expensive. We therefore propose to
approximate the surface distortions by an image-based error
measure. Specifically, we redefine
2

(j )

(j )
fS (q) − fˆS (q) ,

DS =

(6)

q∈GS

where f S (q) is the elevation value of the original patch S
(j )
at position (m, n), and fˆS (q) denotes the corresponding
reconstructed elevation value at bit-stream truncation index
j. Equation (6) can easily be evaluated at any truncation index
of the bit-stream and guarantees fast rate-distortion analysis.
While the bit allocation computes the number of bits that
is assigned to each patch, we also utilize the method in order
to compute the optimal resampling density. This is achieved
by including the rate-distortion curves for prescribed grid
resolutions and selecting the resolution of the identified ratedistortion point on the curve for each patch.

Venus
Rabbit
Sphere

4.1. Linear least-squares fitting
If the input model is a mesh, the elevation values of resampled points are directly read off from the original surface S.
Together with the canonical meshing of the resampled points,
this yields an interpolation of points on S. In [HDD∗ 93] the
resulting average squared interpolation error was reduced by
adjusting the resampled points appropriately, thereby, turning
the interpolation into an approximation. We also apply this
linear optimization technique, adjusting the elevation values
of the resampled points.
The optimization corresponds to the minimization of a
quadratic term Ax − b 2 over x. The vector x holds the
elevation values of the patch vertices that have to be adjusted.
The vector b holds the true elevation values f (p i ) of a larger
number of sample points p i that are regularly distributed
over the patch plane. The matrix A holds in each line i the
barycentric coordinates of sample point p i with respect to
the triangle that contains p i . Thus, Ax contains the elevation
values of the sample points p i from the interpolation, and
Ax − b 2 is the sum of the squared errors of all of these
interpolated elevations. For an accurate approximation, the
number of sample points has to be chosen sufficiently large
(e.g. dim b = 5 dim x).
Least-squares optimization results are listed in Table 1.
The numbers show the surface errors E rms for various patch
resolutions μ. The numbers in brackets are surface errors
that correspond to the approximation errors before applying the least squares optimization. We observe a consistent improvement in approximation error for all resampling
densities.
4.2. Topological reconstruction
If the model has to be reconstructed as a mesh, the regular
structure of the elevation maps can be utilized to construct

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Ochotta and D. Saupe / Image-Based Surface Compression

1655

a mesh for each patch. We implement a canonical meshing
by connecting four adjacent points in each patch by two
triangles. However, the resulting mesh suffers from patch
gaps due to the missing connectivity between the patches.
In the literature, there are several methods for repairing
meshes with gaps that are comparable to ours. Borodin et al.
[BNK02] proposed a gap-closing scheme, which iteratively
projects vertices on the gap boundaries towards the opposing
boundary and creates a triangle for each projection. In the
algorithm of Sander et al. [SWG∗ 03], a mesh is zippered by
projecting boundary vertices onto cut paths that correspond to
gap defining curves. In consecutive projection and unification
operations, opposing boundary curves are projected to each
other without inserting new triangles.
Both methods produce watertight high-quality models,
while they rely on moving vertices on the boundaries of
the patches. For the application of compression, however,
we prefer to keep vertices fixed on their reconstructed positions, since they have been computed and transmitted under
rate and distortion aspects. Hence, we propose a gap triangulation method, which is entirely based on triangle insertion
operations under the constraint that vertices are fixed.
In this paragraph, we continue to extend the description of
our gap closing method in [OH06]. We initially construct the
canonical mesh of the set of reconstructed points Pˆi of each
individual patch i, yielding surfaces Sˆi , i = 0, 1, . . . , |R|.
Then we consider the boundary ∂ Sˆi , which is the set of points
on edges that share only one triangle in the triangulation
of Sˆi and also the boundary vertices that are incident to
these edges. Our goal is to connect vertices on the border of
each patch with edges on another patch. This provides new
triangles that connect the patches. Similar to [BNK02], we
consider projections of boundary vertices v ∈ ∂ Sˆi onto ∂ Sˆj .
Specifically, for each Sˆi and vertex v ∈ ∂ Sˆi we compute
pj (v) := arg min p − v .
p∈∂ Sˆj

(7)

Since p j (v) is a point on the boundary ∂ Sˆj , this point will
either be on a boundary edge or a boundary vertex. In the first
case, we consider the pair v1 , v2 ∈ ∂ Sˆj of incident vertices
to this edge. In the second case, we choose one incident edge
to the vertex p j (v) and also consider the pair of incident
vertices v 1 and v 2 . We now form and insert a new triangle
between the three vertices v, v 1 and v 2 , see Figure 8. In a
consecutive step, we update the border information for edges
and vertices that have been affected by this insertion step.

Figure 8: Triangle insertion in our gap-filling method; upper left: projection of border vertex v onto an opposing edge
(v 1 , v 2 ); lower right: triangles are inserted as long as they
do not intersect with an existing part of the surface.
• The vertex v is no longer a border vertex, but an inner
vertex, once all edges incident to v have become inner
edges.
When inserting a new triangle (v, v 1 , v 2 ), we furthermore
check for intersections with triangles incident to v, v 1 and
v 2 to avoid overlapping triangles. This is mandatory, since in
(7) there is no topological condition that prevents the triangle
(v, v 1 , v 2 ) to intersect with an existing part of the surface.
It should be noted that we also allow projections of border
vertices v ∈ ∂ Sˆi onto the same patch boundary ∂ Sˆi , as long
as the edge that contains the projected point p i (v) is not
incident to v. In our implementation, we additionally extend
the formulation in Equation (7) to projections onto more than
one nearby edge, which increases the number of candidates
for each border edge and improves the overall performance,
e.g. triangles with a large obtuse angle are not inserted.
Figure 9 shows the reconstructed Rabbit mesh without
and with applying our gap-filling method. We obtain robust
and fast gap closing. To process the Rabbit model of 69420
vertices and 133129 faces, our algorithm needs less than 3
seconds for inserting 6852 faces on a 2.8 GHz PC platform.
5. Results and discussion
We present results for the models Venus, Igea, Rabbit,
Balljoint, which are available at the Cyberware repository
[Cyb00], and Dragon, David, XYZRGB Dragon and Lucy,
which were taken from the Stanford 3D Scanning Repository [Sta00, LPC∗ 00]. The model of the Shakyamuni statue
is available at the Konstanz 3D Model Repository [KN00].

• The edge (v 1 , v 2 ) becomes an inner edge since it is incident to two triangles.

5.1. Partitioning

• We have two new border edges, namely (v, v 1 ) and (v,
v 2 ), if there is no previously inserted triangle with an
edge (v, v 1 ) or (v, v 2 ), respectively.

Table 2 lists the number of patches and the approximation error for the split-merge approach (Section 3.3) and the
GLA clustering (Section 3.3). The partitions were obtained

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1656

T. Ochotta and D. Saupe / Image-Based Surface Compression

Figure 9: Results that are obtained with our gap-filling method; Rabbit without and with gap filling (left); and corresponding
close-up views (upper and lower right).

Table 2: Comparison between the split-merge partitioning and the
GLA clustering, (Section 3.3); the patch aperture angles are bound
to 80◦ ; the GLA method outperforms the split-merge approach by
providing partitions with a smaller number of patches at a smaller
approximation error, (4).

Model

Igea
Balljoint
Dragon
Shakyamuni
David

# Points

134345
137062
437645
996956
3614096

Split-merge

In the following, we present results for point- and meshbased models using the GLA partitioning, where we applied
the aperture threshold of β = 80◦ . For some large models, however, we applied an extremely relaxed threshold, i.e.
β = 89◦ in order to reduce the total number of patches, which
leads to better compression results at low bit-rates.

GLA [CSAD04]

# patches

error

# patches

error

14
26
367
262
878

0.42
0.41
0.56
0.51
0.51

7
12
150
133
692

0.32
0.29
0.22
0.23
0.09

restricting the maximum aperture of each patch to 80◦ . Comparing the two partitioning methods, we observe that the
GLA [CSAD04] outperforms the split-merge method. We
obtain partitions with less patches that at the same time show
a smaller approximation error, Equation (4).

5.2. Point model compression
Table 3 lists detailed coding results for point models. For each
model, the bit-stream consists of, firstly, a header, which
holds side information and plane mapping parameters for
each patch (the respective amount of bits is denoted by r 0 );
secondly, the coding costs of the binary images which define the elevation map supports; and thirdly, the bits for the
wavelet coefficients, which constitute the largest part of the
stream. The bit-rate is the total amount of bits divided by
the number of input points. The error E rms is the surface-tosurface distance of the original model and the reconstruction,
see Section 3.1. We follow Equation (2) and sample both
surfaces with a number of points that are projected onto the

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Ochotta and D. Saupe / Image-Based Surface Compression

1657

Table 3: Detailed compression results for various point models; the
bit-stream consists of header bits (denoted by r 0 ), bits for the binary
images (masks), shown as percentage of the entire code length, and
bits for wavelet coefficients; the last column shows the decoding and
reconstruction time.

Reconstructed Mask bits Total rate
E rms
PSNR T d
points
[%]
[bpp]
[10−4 d B ] [dB] [sec]
Igea, 134345 points, 7 patches, r 0 = 1218
87526
17.0
0.50
2.35
136638
7.5
1.51
0.76
197012
4.7
3.01
0.44

72.6
84.4
87.1

0.3
0.3
0.5

Balljoint, 137062 points, 12 patches, r 0 = 1893
49422
15.0
0.51
2.52
137558
9.0
1.51
0.73
269581
6.7
3.01
0.43

72.0
82.7
87.3

0.2
0.3
0.4

Dragon, 437645 points, 150 patches, r 0 = 20253
154189
58.4
0.29
13.3
431633
21.5
1.55
1.03
848723
16.6
3.05
0.62

57.5
79.7
84.2

0.8
1.2
1.7

Shakyamuni, 996956 points, 133 patches, r 0 = 18224
268089
24.3
0.22
2.50
72.0
947262
12.6
1.02
0.32
89.9
1366866
7.9
2.02
0.14
97.1

1.6
2.2
2.8

David, 3614096 points, 692 patches, r 0 = 93693
562,419
33.0
0.18
1.30
1,270,573
24.9
0.38
0.45
3,544,249
18.0
0.98
0.21

77.7
86.9
93.6

4.6
8.2
12

Lucy, 14020068 points, 621 patches, r 0 = 84243
5184445
63.3
0.09
2.89
9225736
26.9
0.31
0.19
14422702
10.7
1.01
0.07

70.8
94.4
103

23
34
58

opposite surface. All errors in this work are expressed in units
of 10−4 d B , where d B is the diagonal length of the bounding
box of the original model.
Comparing the results in Table 3, we remark two major observations. Firstly, the number of required patches is widely
independent from the sampling density in the model, e.g. the
Shakyamuni model with 996956 points is twice as big as
the Dragon model with 437645 points, but needs only 133
patches, while the Dragon model takes 150 patches. This is
obvious, since the patch layout reflects the complexity of
the geometry rather than the sampling density. Secondly, the
sampling density dominates the ratio between bit-rate and
reconstruction quality, e.g. we observe a bit rate of about 0.4
bpp for the David model at a reconstruction fidelity of 87 dB,
while at the same time, Igea takes a significantly higher rate
of about 3 bpp at a lower reconstruction quality of 84.4 dB.
Hence, our method aggressively removes redundancies when
the model has a very dense sampling relative to the surface
complexity, e.g. as it is the case for David.

Figure 10: Point-based compression of Lucy; global view:
reconstruction at 0.47 bpp, which corresponds to a file size of
804 kByte; the close-up views on the left show reconstructions
at 0.09 bpp (E rms = 2.89), 0.15 bpp (E rms = 0.62) and
0.47 bpp (E rms = 0.14) (from top to bottom); the head on the
bottom right corresponds to the original model.

Figure 10 shows the compressed Lucy statue at 0.47 bpp
at the global view. The close-up views show reconstructions
at 0.09 bpp, 0.15 bpp and again 0.47 bpp. A close-up view
of the original is shown at the bottom right. For Lucy we
obtain visually pleasant results already at very low bit-rates,
e.g. around a half bit per point (compare the two close-up
views at the bottom). This indicates that the original shows
a high point density compared to the geometric complexity
and our coder removes redundancies adequately.
Figure 11 shows the compression performance for the
Stanford Dragon point model. The top row shows the partition with 150 patches (left) and rate distortion curves
using the error measure Equation (2). The two curves
are obtained using the split-merge partitioning and the
GLA clustering. The right curves compare our results to

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1658

T. Ochotta and D. Saupe / Image-Based Surface Compression

Figure 11: Point-based compression of Dragon; top row: partition, performance curves for split-merge and GLA partitioning
and comparison of our method to [FCOAS03] and [WGE∗ 04]; bottom row: decoded models at various bit-rates with decoding
ˆ and visualizations of the distances of the original surface to the MLS surface of the reconstructed point set.
errors d(S, S)
previous coders [FCOAS03] and [WGE∗ 04], using the error
measure in [FCOAS03]. A significant performance improvement can be observed. The bottom row in Figure 11 shows
close-up views of the decoded Dragon at various bit-rates.
The images below visualize the distance of the original to the
reconstructed surface, where blue corresponds to a distance
of zero and red correspond to distances greater or equal to
0.1% of the bounding box diagonal of the original model.
Figure 12 shows compression of David (3614098 points).
The rate distortion curves indicate an excellent approximation quality at already 0.5 bits per point, which confirms that
our method adequately removes redundancies for large models as for Lucy in Figure 10. Artefacts become visible in the
global view only when reducing the bit-rate drastically, e.g.
down to 0.11 bpp.
The close-up views in Figure 12 show that the rendering
of the reconstruction is more blurry than that of the original.
This is explained by the fact that our method degrades the
sampling of the original model (which may be adaptive to
surface features). This drawback can be overcome by considering the sampling density during the patch layout construction, e.g. the patches are forced to show similar sampling
densities. Although, an improvement in visual quality would

be expected, the total number of patches will increase, which
may worsen the rate-distortion performance due to increased
costs for side information and binary images.
5.3. Mesh geometry compression
In the following, we present results for mesh models. To
evaluate the surface error, Equation (2), we adopt the Metro
tool [CRS98]. The rates are given in bits per vertex (bpv).
Figure 13 shows results for the Shakyamuni mesh. The
close-up views show the partition and reconstructions at
various bit-rates. We observe that the reconstruction at 1 bpp
is already close to the original model (E rms = 0.3 · 10−4 d B )
and provides good visual quality. The corresponding L ∞
ˆ supp∈Sˆ d(p, S)) = 4 ·
error is E∞ = max(supp∈S d(p, S),
−4
10 dB , which is still within a tolerance of, e.g. 0.5% of the
bounding box diagonal length. The top right curve shows the
rate-distortion performance, while the bottom curve shows
the reconstruction error at 1 bpp for a varying number of
patches. We found that the error is proportional to the number
of patches down to a certain optimal number. This behaviour
is explained by the fact that a large number of patches leads
to increased costs for side information, e.g. base plane parameters and binary images, which in turn reduces the bit

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Ochotta and D. Saupe / Image-Based Surface Compression

1659

Figure 12: Point-based compression of David; top: partition and two reconstructions; bottom: rate-distortion graphs without
and with applying the optimal bit allocation scheme and close-up views of the original and a reconstruction.

budget for the wavelet coefficients and leads to larger errors.
Further reduction of the patch number leads to elevation map
artifacts, which worsens the distortion. We found that this
behaviour is typical. In general, there is an optimal number
of patches for which the rate-distortion curve is the lowest.
Figures 14 and 15 show compression results for the commonly used meshes Rabbit and Venus. The curves compare
the performance of our coder to the state-of-the-art methods [KSS00] and [KG02]. Basically, the method in [KG02]
provides a better rate-distortion performance at very low bitrates. Nevertheless, our method provides results that are similar or slightly better than in [KG02] at bit-rates of 1 bpp and
above. At these rates, the errors are sufficiently small, meaning that the reconstructions are close to the original model
and are suitable for a practical application.
Table 4 lists all tested meshes and gives detailed coding
results, including the amount of side bits r 0 and bits for the
binary images, compare Table 3.

Comparison to multi-chart geometry images. Our method
is similar to the approach proposed by Sander et al.
[SWG∗ 03], however, their parameterization is more complex than ours, since the detail coefficients are 3D. Their
work does not focus on compression, hence, we compare
our method to theirs in terms of parameterization quality. To
make an appropriate comparison, we reconstruct our model
with three times as many vertices as in [SWG∗ 03], since
our detail coefficients are only one-dimensional. We reconstruct the Dragon model at 158009 vertices and a reconstruction quality of 80.6 dB, which is comparable to the result in [SWG∗ 03], where the Dragon model is reconstructed
at 79.4 dB (with 52059 vertices, leading to 156177 detail
coefficients).
Computational costs. Table 5 shows timings for the individual steps in the pipeline for various models. For point models, we observe high computational costs for the resampling
procedure, which is due to the fact that a number of MLS projections have to be applied to construct the elevation value

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1660

T. Ochotta and D. Saupe / Image-Based Surface Compression

Figure 13: Mesh compression of Shakyamuni, original model (left); the close-up views show the partition and reconstructions
at 0.2 bpp and 1 bpp (middle); reconstruction errors for various bit-rates (top right); reconstruction errors for varying number
of patches at 1.0 bpv show that there is an optimal number of patches for each model (lower right).

Figure 14: Mesh compression as in Figure 13 for Rabbit (left); the close-up views show that boundary artefacts become visible
at very low bit-rates (middle); the compression performance compared to the coders PGC [KSS00] and NMC [KG02] (right).
for each sample point. The resampling is fast for meshes,
since the elevation values can be read off directly from the
original mesh. Moreover, there is a significant amount of
run time needed for the rate-distortion optimization, since it
includes many encoding and decoding steps to sample the
rate-distortion curve for each patch.
The last column of Table 3 and Table 4 show timings that
are required for decoding and reconstruction. For meshes, the

gap closing has to be applied after decoding, which results
in slower reconstructions than for points.
Boundary artifacts. The weakness of our framework is
the occurrence of discontinuities near the patch boundaries.
These become visible especially at very low bit-rates, where
the reconstruction error becomes large, Figure 16(a). Although the patch discontinuities do not worsen the compression performance with respect to the geometric error, they

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1661

T. Ochotta and D. Saupe / Image-Based Surface Compression

Figure 15: Mesh compression as in Figure 14 for Venus with 50002 vertices.
Table 4: Compression results for meshes, compare Table 3.

Reconstructed Mask bits Total rate
E rms
PSNR T d
vertices
[%]
[bpv]
[10−4 d B ] [dB] [sec]
Venus, 50002 vertices, 8 patches, r 0 = 1353
18392
28.9
0.32
9.06
51519
12.8
1.27
1.91
132437
8.8
2.96
0.85

60.9
74.4
81.4

1.3s
2.7s
3.2s

Rabbit, 67039 vertices, 6 patches, r 0 = 1083
24587
18.2
0.42
2.70
68572
12.3
1.02
1.01
134832
7.3
2.49
0.46

71.3
79.9
86.8

1.5s
2.8s
3.4s

Balljoint, 137062 vertices, 11 patches, r 0 = 1758
50,344
15.4
0.51
2.01
140,619
13.4
1.01
1.01
202,110
8.2
2.01
0.55

73.9
79.9
85.2

2.6s
3.8s
5.2s

Dragon, 437645 vertices, 162 patches, r 0 = 22143
175003
31.1
0.35
3.86
288677
17.6
0.95
0.91
452047
10.7
2.05
0.46

68.3
80.8
86.8

8s
12s
15s

Shakyamuni, 996,956 vertices, 161 patches, r 0 = 22008
308527
34.4
0.22
2.35
72.6
369113
20.9
0.52
0.74
82.6
1481024
14.4
1.27
0.26
91.7

10s
14s
28s

XYZRGB Dragon, 3.6M vertices, 534 patches, r 0 = 72363
1370951
43.1
0.22
1.56
76.1
3295725
27.5
0.52
0.49
86.2
7304838
9.2
2.52
0.11
99.2

57
82
120

Table 5: Timings for the following steps in the pipeline: GLA clus(p)
tering (T clu ), parameterization (T par ), resampling (T res for the
(m)
point-based model and T res for the mesh), rate-distortion optimization (T opt ) and encoding (T enc ).
(p)

Model

T clu

T par

T res /T res

(m)

T opt

T enc

Venus
Rabbit
Balljoint
Dragon
Shakyamuni

0.6s
1.1s
2.6s
12s
34s

0.03s
0.08s
0.1s
0.3s
0.6s

5.6s/1.1s
12s/1.5s
34s/2.4s
2.7m/10s
6.2m/23s

14s
26s
43s
2.2m
8.6m

0.1s
0.1s
0.3s
0.9s
1.6s

Figure 16: Compressing models at very low bit-rates leads
to visible artifacts near the patch boundaries, E rms = 3.91
(left); applying a smoothing filter in these areas improves the
visual quality, E rms = 3.86 (right).

Results of this procedure are shown in Figure 16. We observe that the discontinuities are widely removed, while the
geometric approximation quality is preserved.
may lead to unpleasant visual results. To fix this problem, we
propose to apply a smoothing filter to blend the patch boundaries in a post-processing step after decoding. Specifically,
we identify all points on the patch boundaries and set their positions to the respective Gaussian-smoothed version, which
is obtained by computing the weighted average of point positions in a local neighbourhood around each boundary point.

6. Conclusions
We presented a framework for 3D surface compression. Our
coder applies an elementary parameterization to decompose
the surface into a number of regularly sampled height fields,
which are encoded using state-of-the-art shape-adaptive

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

1662

T. Ochotta and D. Saupe / Image-Based Surface Compression

image wavelet coding, thus, mapping the problem of 3D
geometry coding to 2D image coding.
Our method is applicable to mesh- and point-based geometry, since our pipeline relies only on an appropriate surface
representation of the 3D model. We obtained an improved
rate-distortion performance in comparison with state-of-theart point-based surface compression. If the reconstructed
model is a mesh, our coder still achieves results that are
comparable to those of the best mesh compression algorithms, e.g. normal mesh compression. However, in contrast
to this method, our framework is based on an elementary
parameterization, leading to simple implementations.
The limitation of our method is the occurrence of patch artifacts, which is common with region-based coding schemes,
such as JPEG for images. Moreover, the resampling procedure widely destroys the (perhaps adaptive) sampling of
the original model. Finally, our approach is not suitable for
encoding diffuse and irregular objects, e.g. plant models. It
rather aims at densely sampled surface-like 3D objects, i.e.
models that were acquired by a laser range scanner.
In future work, we may compare our method to previous
coders with respect to error measures that model human visual perception more appropriately than the L 2 metric does.
We may also embed our system in a out-of-core framework,
in that we apply out-of-core partitioning as in [IG03], and
perform the resampling step and the rate-distortion (RD)optimization on each individual patch (in-core).

Acknowledgements
We thank the reviewers for their helpful comments to improve the paper. This work was supported by the DFG
Graduiertenkolleg 1042 ‘Explorative Analysis and Visualization of Large Information Spaces’.

References
[AA03] ADAMSON A., ALEXA M.: Ray tracing point set
surfaces. In Proc. Shape Modeling International (2003),
pp. 272–279.
[ABCO∗ 03] ALEXA M., BEHR J., COHEN-OR D., FLEISHMAN
S., LEVIN D., SILVA C.: Computing and rendering point set
surfaces. IEEE Transactions on Computer Graphics and
Visualization 9, 1 (2003), 3–15.

[BWK02] BOTSCH M., WIRATANAYA A., KOBBELT L.: Efficient high quality rendering of point sampled geometry. In Proc. Workshop on Rendering (2002), pp. 53–
64.
[CM02] CHOU P., MENG T.: Vertex data compression
through vector quantization. IEEE Transactions on Visualization and Computer Graphics 8, 4 (2002), 373–
382.
[CRS98] CIGNONI P., ROCCHINI C., SCOPIGNO R.: Metro:
Measuring error on simplified surfaces. Computer Graphics Forum 17, 2 (1998), 167–174.
[CSAD04] COHEN-STEINER D., ALLIEZ P., DESBRUN M.:
Variational shape approximation. ACM Transactions on
Graphics 23, 3 (2004).
[Cyb00]

http://www.cyberware.com/samples/.

[Dee95] DEERING M.: Geometry compression. In Proc.
ACM SIGGRAPH (1995), pp. 13–20.
[Eve63] EVERETT H.: Generalized lagrange multiplier
method for solving problems of optimum allocation of
resources. Operations Research 11 (1963), 399–417.
[FCOAS03] FLEISHMAN S., COHEN-OR D., ALEXA M., SILVA
C. T.: Progressive point set surfaces. ACM Transactions
on Graphics 22, 4 (2003), pp. 997–1011.
[FH05] FLOATER M. S., HORMANN K.: Surface parameterization: A tutorial and survey. In Advances in
Multiresolution for Geometric Modelling. Springer, 2005,
pp. 157–186.
[Fow00] FOWLER J. E.: QccPack: an open-source software
library for quantization, compression and coding. In Applications of Digital Image Processing XXIII, Proceedings
SPIE 4115 (2000), pp. 294–301.
[Fow04] FOWLER J. E.: Shape-adaptive coding using binary
set splitting with k-d trees. In Proc. ICIP (2004), vol. 2.
[G¨ar99] G¨ARNTER B.: Fast and robust smallest enclosing
balls. In Proc. Symposium on Algorithms (1999), pp. 325–
338.
[GG92] GERSHO A., GRAY R. M.: Vector quantization and
signal compression. Kluwer Academic Publishers, 1992.

[AG04] ALLIEZ P., GOTSMAN C.: Recent advances in compression of 3D meshes. In Advances in Multiresolution for
Geometric Modelling. Springer, 2004.

[HDD∗ 93] HOPPE H., DEROSE T., DUCHAMP T., MCDONALD
J., STU¨ TZLE W.: Mesh optimization. In Proc. ACM SIGGRAPH (1993), pp. 19–26.

[BNK02] BORODIN P., NOVOTNI M., KLEIN R.: Progressive
gap closing for mesh repairing. In Proc. Computer Graphics International (2002), pp. 201–213.

[IG03] ISENBURG M., GUMHOLD S.: Out-of-core compression for gigantic polygon meshes. ACM Transactions on
Graphics 22, 3 (2003), 935–942.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

T. Ochotta and D. Saupe / Image-Based Surface Compression

[IK01] IVANOV D. V., KUZMIN Y.: Spatial patches - a primitive for 3D model representation. Computer Graphics
Forum 20, 3 (2001), 511–521.
[KG00] KARNI Z., GOTSMAN C.: Spectral compression
of mesh geometry. In Proc. ACM SIGGRAPH (2000),
pp. 279–286.
[KG02] KHODAKOVSKY A., GUSKOV I.: Compression of normal meshes. In Geometric Modeling for Scientific Visualization. Springer Verlag, Heidelberg, Germany, 2002.

1663

geometry with elevation maps. In Proc. Shape Modeling
International (2006), pp. 45–56.
[OS04] OCHOTTA T., SAUPE D.: Compression of pointbased 3D models by shape-adaptive wavelet coding of
multi-height fields. In Proc. Point-Based Graphics (2004),
pp. 103–112.
[PG01] PAULY M., GROSS M.: Spectral processing of pointsampled geometry. In Proc. ACM SIGGRAPH (2001).

[KN00] http://www.inf.uni-konstanz.de/cgip/projects/
surfac/.

[PK05] PENG J., KUO C.-C. J.: Geometry-guided progressive lossless 3D mesh coding with octree (ot) decomposition. ACM Transactions on Graphics 24, 3 (2005),
609–616.

[KPRW05] K¨ALBERER F., POLTHIER K., REITEBUCH U.,
WARDETZKY M.: FreeLence - coding with free valences. Computer Graphics Forum 24, 3 (2005), 469–
478.

[PZvBG00] PFISTER H., ZWICKER M., VAN BAAR J., GROSS
M.: Surfels: surface elements as rendering primitives. In
Proc. ACM SIGGRAPH (2000), pp. 335–342.

[KSS00] KHODAKOVSKY A., SCHRO¨ DER P., SWELDENS W.:
Progressive geometry compression. In Proc. ACM SIGGRAPH (2000), pp. 271–278.

[RL00] RUSINKIEWICZ S., LEVOY M.: QSplat: a multiresolution point rendering system for large meshes. In Proc.
ACM SIGGRAPH (2000), pp. 343–352.

[KSW05] KRU¨ GER J., SCHNEIDER J., WESTERMANN R.:
DUODECIM - a structure for point scan compression and
rendering. In Proc. Point-Based Graphics (2005).

[Ros99] ROSSIGNAC J.: Edgebreaker: connectivity compression for triangle meshes. IEEE Transactions on Visualization and Computer Graphics 5, 4 (1999), 47–61.

[LCB03] LAVU S., CHOI H., BARANIUK R.: Geometry compression of normal meshes using rate-distortion algorithms. In Proc. Symposium on Geometry Processing
(2003), pp. 52–61.

[SCOT03] SORKINE O., COHEN-OR D., TOLEDO S.: High-pass
quantization for mesh encoding. In Proc. Symposium on
Geometry Processing (2003), pp. 42–51.

[LL00] LI S., LI W.: Shape-adaptive discrete wavelet transforms for arbitrary shaped visual object coding. IEEE
Transactions on Circuits and Systems for Video Technology 10, 5 (2000), 725–743.

[SP96] SAID A., PEARLMAN W. A.: An image multiresolution representation for lossless and lossy image compression. IEEE Transactions on Image Processing 5, 9 (1996),
1303–1310.
[SRK02] SZYMCZAK A., ROSSIGNAC J., KING D.: Piecewise
regular meshes: Construction and compression. Graphical
Models 64 (2002), 183–198.

[Llo82] LLOYD S. P.: Least squares quantization in PCM.
IEEE Transactions on Information Theory 28 (1982),
129–137.

[Sta00]

[LMH00] LEE A., MORETON H., HOPPE H.: Displaced subdivision surfaces. In Proc. ACM SIGGRAPH (2000), pp. 85–
94.

[SWG∗ 03] SANDER P., WOOD Z., GORTLER S., SNYDER J.,
HOPPE H.: Multi-chart geometry images. In Proc. Symposium on Geometry Processing (2003), pp. 146–154.

[LPC∗ 00] LEVOY M., PULLI K., CURLESS B., RUSINKIEWICZ S.,
KOLLER D., PEREIRA L., GINZTON M., ANDERSON S., DAVIS
J., GINSBERG J., SHADE J., FULK D.: The Digital Michelangelo Project: 3D scanning of large statues. In Proc. ACM
SIGGRAPH (2000), pp. 131–144.

[TG98] TOUMA C., GOTSMAN C.: Triangle mesh compression. In Proc. Graphics Interface (1998), pp. 26–34.

[LR81] LANGDON G. G., RISSANEN J.: Compression of blackwhite images with arithmetic coding. IEEE Transactions
Communications 29, 6 (1981), 858–867.
[OH06]

OCHOTTA T., HILLER S.: Hardware rendering of 3D

http://graphics.stanford.edu/data/3Dscanrep/.

[TR98] TAUBIN G., ROSSIGNAC J.: Geometric compression through topological surgery. ACM Transactions on
Graphics 17, 2 (1998), 84–115.
[WGE∗ 04] WASCHBU¨ SCH M., GROSS M., EBERHARD F.,
LAMBORAY E., W¨URMLIN S.: Progressive compression of
point-sampled models. In Proc. Point-Based Graphics
(2004).

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

