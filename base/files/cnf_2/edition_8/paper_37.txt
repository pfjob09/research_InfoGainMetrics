Volume 27 (2008), Number 2

EUROGRAPHICS 2008 / G. Drettakis and R. Scopigno
(Guest Editors)

Automatic Conversion of Mesh Animations into
Skeleton-based Animations
Edilson de Aguiar1† , Christian Theobalt2 , Sebastian Thrun2 , Hans-Peter Seidel1
1 MPI

Informatik, Saarbruecken, Germany
University, Stanford, USA

2 Stanford

Abstract
Recently, it has become increasingly popular to represent animations not by means of a classical skeleton-based
model, but in the form of deforming mesh sequences. The reason for this new trend is that novel mesh deformation
methods as well as new surface based scene capture techniques offer a great level of flexibility during animation
creation. Unfortunately, the resulting scene representation is less compact than skeletal ones and there is not yet
a rich toolbox available which enables easy post-processing and modification of mesh animations. To bridge this
gap between the mesh-based and the skeletal paradigm, we propose a new method that automatically extracts
a plausible kinematic skeleton, skeletal motion parameters, as well as surface skinning weights from arbitrary
mesh animations. By this means, deforming mesh sequences can be fully-automatically transformed into fullyrigged virtual subjects. The original input can then be quickly rendered based on the new compact bone and skin
representation, and it can be easily modified using the full repertoire of already existing animation tools.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism:Animation I.4.8 [Image Processing and Computer Vision]: Scene Analysis:Motion

1. Introduction
Recently, a variety of deformation-based approaches have
been proposed that enable skeleton-less generation of computer animations [BS08]. This change of paradigm in animation production has been flanked by the appearance of
new tracking approaches in computer vision that enable optical motion capture of arbitrary subjects using deformable
meshes rather than rigid body hierarchies [dATSS07a]. Both
of the above developments suggest that purely mesh-based
animation representations have developed into an ever more
popular alternative to kinematic hierarchy-based animations.
Unfortunately, although mesh-based approaches provide greater flexibility at the time of animation creation
than skeleton-based algorithms, they output more spaceconsuming dynamic scene representations comprising of independent position data streams for every vertex. A further

† This work was done while the author was a visiting researcher
at the Stanford University and it was sponsored by the Max-Planck
Center for Visual Computing and Communication
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

disadvantage is that animators are used to a large repertoire
of tools for editing and rendering traditional skeletal animations, but yet lack the same set of tools for working with
mesh-based dynamic scene representations.
Xu et al. [XZY∗ 07] propose to close this gap by introducing a set of mesh-based operations to post-process surface
animations in a similar manner as kinematic representations.
Although their method allows for flexible post-processing of
time-varying surfaces, it requires a fundamental redesign of
existing animation tools and also does not explore data compaction possibilities. The latter was the focus of the work
by James et al. [JT05] who aim at extracting a skinning
representation from mesh sequences that is well-suited for
rendering on graphics hardware but not meant to be used for
editing.
In contrast, we propose a method that enables the fullyautomatic conversion of an arbitrary mesh animation into a
skeleton-based animation. Given as input a deforming mesh
with constant surface connectivity, our algorithm first extracts a plausible kinematic bone hierarchy that closely resembles a skeleton hand-crafted by an animator, Sect. 4 and

390

de Aguiar et al. / Automatic Conversion of Mesh Animations into Skeleton-based Animations

Figure 1: From left to right: Input animation, color-coded distribution of blending weights, and two poses of the input regenerated based on our skeleton-based version.
Sect. 5. Thereafter, our algorithm automatically infers joint
motion parameters, Sect. 6, and estimates appropriate surface skinning weights, Sect. 7, to attach the skeleton to the
surface. The output of our algorithm is a fully-rigged skeletal
version of the original surface-based input. We show results
obtained with a variety of mesh animations, and also prove
the faithfulness of the reconstructed skeletal representations
to ground truth input, Sect. 8. In summary, our algorithm
• enables fully-automatic extraction of skeleton structure,
skeletal motion parameters and surface skinning weights
from arbitrary deforming mesh sequences, and
• thereby enables easy post-processing and fast rendering of
mesh animations with standard skeleton-based tools without having to modify them.
As opposed to related methods our approach jointly produces a compact and easily modifiable skeletal version
(Fig. 1), enables fast and accurate rendering of the original
input, enables easy generation of new pose sequences for
the input subject, and achieves all this without requiring any
modification to already existing animation tools.
2. Related Work
In our work we jointly solve a variety of algorithmic subproblems by extending ideas from the research on mesh segmentation, skeleton reconstruction, motion estimation, character skinning and pose editing. For the sake of brevity, we
refer the interested reader to overview articles on motion estimation [Pop07] and mesh segmentation methods [Sha08],
and in the following highlight selected related papers from
the other categories.
2.1. Skeleton Reconstruction
Different ways for performing skeleton extraction, each of
them tailored to a specific data type and application, have
been proposed in the literature. Some approaches extract
skeletons from static meshes [KT03, LKA06, SLSK07] to
gain information on topology or to perform segmentation.
Thus, extraction of an animation skeleton is not the goal.
The accurate extraction of kinematically and biologically
plausible animation skeletons is of great importance in optical motion capture, where the skeletal structure needs to

be inferred from marker trajectories [KOF05, dATS06] or
shape-from-silhouette volumes [dATM∗ 04]. Similarly, kinematic skeletons can be reconstructed from a set of range
scans of humans [AKP∗ 04] or CT scans of the human
hand [KM04]. Recently, Aujay et al. [AHLD07] and Schaefer et al. [SY07] have proposed methods to extract plausible
human animation skeletons from a static surface mesh of a
character, for instance by using prior knowledge about the
anatomy of humans and animals.
In contrast, in our setting we explicitly make use of motion information to extract kinematic hierarchies more robustly. Our algorithm creates a plausible animation skeleton
that best explains the data and that closely resembles a skeleton designed by an animator (in case such a skeleton exists
for the data set).
2.2. Character Skinning
Skinning or enveloping is the problem of mapping the articulated motion of the skeleton to deformations of the character’s surface. Due to its efficiency, ease of implementation
and support in many commercial packages, the most widely
used enveloping method is linear blend skinning [LCF00].
Unfortunately, standard linear blending has a limited modeling power and cannot reliably reproduce certain effects, such
as muscle bulging. More recent blending weight estimation
schemes look at several example poses and suggest methods to overcome the limitations inherent to linear blending.
Wang et al. [WP02] suggest the use of multi-linear blending weights, Mohr et al. [MG03] suggest selective adding of
bones to increase reconstruction faithfulness, and James et
al. [JT05] suggest to use affine transformations.
In a different line of thinking, researchers recently suggested to use the motion of a kinematic skeleton to express constraints for a surface deformation method like
a Laplacian deformation [YBS07] or a Poisson deformation [WPP07]. By this means, convincing animations can be
obtained as one can capitalize on the full modeling flexibility
of a more complex mesh deformation approach.
Recently, Baran et al. [BP07] have proposed a new approach that bears some similarity with our method. They
jointly fit a template skeleton to a static character mesh and
automatically compute appropriate skinning weights. In contrast to their method, our approach extracts a subject-specific
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

de Aguiar et al. / Automatic Conversion of Mesh Animations into Skeleton-based Animations

SEGMENTATION

MESH
ANIMATION

SKELETON

SKINNING
WEIGHTS

391

SKELETON-BASED
ANIMATION

Figure 2: Overview of our algorithm: using an animated mesh as input, our approach segments the model into plausible
approximately rigid surface patches (shown in different colors), estimates the kinematic skeleton (joints shown in red) and its
motion parameters, and calculates the skinning weights connecting the skeleton to the mesh. The output is a skeleton-based
version of the input mesh animation.
skeleton without knowing the structure a priori, and it does
so by analyzing the input motion of the mesh. Still, we capitalize on their blending weight computation, but in contrast
to their original idea, apply it to a whole series of poses in
order to obtain weights that more faithfully approximate the
input.

2.3. Pose and Animation Editing
Similar in spirit to our algorithm are methods that edit meshbased deformations directly based on spatio-temporal mesh
editing operators rather than by transforming the input into
a skeletal representation [XZY∗ 07, KG06]. While the flexibility of these methods is very high and the resulting edited
animations are of high visual quality, these methods require
a fundamental redesign of existing animation tools and they
don’t explore data compaction possibilities. However, in particular when the mesh animation can well be explained by a
skeleton, our approach to transform a mesh animation into
a skeletal one is advantageous, as it enables fast and easy
post-processing using the full spectrum of already existing
software.
A first step in this direction was taken in [SY07] where
a skeleton and skinning weights are estimated from a set of
example poses. The main differences to our method are that
we exploit the full motion information in the input to robustly learn a skeleton by means of spectral clustering, that
we get a full range of skeletal motion parameters for the input animation which gives us much greater flexibility during
post-processing, and that we fit our skinning weights to the
entire range of animation frames which leads to more reliable estimates.
In summary, while many related approaches from the literature propose alternative methods to solve subproblems of
our setting, our method outputs a fully-parameterized skeletal version of the input mesh animation that can either be
rapidly rendered without modification or easily be modified
using standard tools.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

3. Overview
An overview of our approach is shown in Fig. 2. The input
to our algorithm is an animated mesh sequence comprising
of N frames. We represent an animated mesh sequence by
a mesh model M = (V = vertices, T = triangulation) and
position data pt (v j ) = (x j , y j , z j )t for each vertex v j ∈ V at
all time steps t. By using the coordinate sets Pt = {pt (v j )}
and the mesh M we are able to completely describe our timevarying animated model.
In the first step of our algorithm we employ spectral clustering to group seed vertices on the mesh into approximately
rigid segments. By using the clustered seed vertices we are
then able to segment the moving mesh into kinematically
meaningful approximately rigid patches, Sect. 4. Thereafter,
adjacent body parts are determined and the topology of the
kinematic structure of the mesh is found, Sect. 5. Using the
estimated topology, joint positions between interconnecting
segments are calculated over time. In order to eliminate temporal bone length variations due to per-time step fitting inaccuracies, joint positions are updated at all time steps and an
inverse kinematics approach is applied to determine the subject’s joint parameters over time, Sect. 6. In a last step we
calculate appropriate skinning weights to attach the learned
skeleton to the surface, Sect. 7. This way, we produce a complete skeleton-based new version of the original input.
4. Motion-driven Segmentation
The first step of our algorithm segments the animated input
mesh (given by M and Pt ) into spatially coherent patches that
undergo approximately the same rigid transformations over
time. We initialize our approach by selecting a subset of l
vertices that are distributed evenly over the mesh M. For the
selection of the seeds we only consider a reference pose Ptr
(typically tr = 0), and employ a curvature-based segmentation method [YGZS05] to decompose the model into l surface patches. The seed vertices are chosen as the vertices
closest to the centers of the patches. We typically choose l
to be in the range of 0.3 − 1.0% of the total vertex count of

392

de Aguiar et al. / Automatic Conversion of Mesh Animations into Skeleton-based Animations

the model, which enables reasonably fast decomposition of
even large meshes.
Similar to [KOF05, dATS06] in the context of optical motion capture and [TRdAS07] for volumetric segmentation,
the motion trajectories of the seed vertices throughout the
whole sequence form the input to a spectral clustering approach [NJW02] which automatically groups the l seeds into
k approximately rigidly moving groups. We capitalize on
the invariant that mutual distances between points on the
same rigid part should only exhibit a small variance while
the mesh is moving.
In order to use spectral clustering we first construct a
spatial affinity matrix A. We developed an affinity criterion
specifically for our setting that defines the entries of A as
follows:
−

Ai, j = e

√ρ

σi, j +

S2

i, j

,

(1)

where ρi, j = N12 ∑t δ(vi , v j ,t) is the mutual Euclidean
distance δi, j,t between seed vertex vi and seed vertex
v j over time and σi, j is its standard deviation. S =
√
1
(σi, j + ρi, j ) is a scaling term controlling the conN 2 ∑i, j
vergence behavior. We construct the entries of A such that
the affinity values of vertex pairs with large average mutual
distance is reduced, which forces our spectral clustering algorithm to put spatially far apart groups of vertices with similar motion into separate clusters.
Spectral clustering is our method of choice as it can robustly infer complex cluster topologies as they are typical
for our motion segmentation problem. Instead of grouping
the vertices directly based on the individual values Ai, j , spectral clustering uses the top eigenvectors of matrices derived
from A to cluster the vertices (for details see [NJW02]). This
makes the clustering more robust against outliers and leads
to a more robust and kinematically more meaningful segmentation. As an additional benefit, the optimal number of
clusters k can be automatically calculated based on the data
set’s eigen-gap. In our system, we automatically cluster the
seeds into k groups such that around 99.0% of the total variation of the data is explained.
Using the k optimal vertex clusters, we create triangle
clusters T0 . . . Tk−1 = T by assigning each triangle ∆ =
(w0 , w1 , w2 ) ∈ T to the closest seed vertex class considering the average Euclidean distance from a seed vertex vu
to w0 , w1 , and w2 . The resulting clusters divide the mesh
into k approximately rigid surface patches, Fig 3. Note that
although a structurally motivated distance measure like the
geodesic distance could also be used for clustering the triangles and to determine affinities in Eq. (1), our experiments
show that similar results can be achieved using simple Euclidean distance which reduces the algorithm’s computation
time considerably.

Figure 3: Approximately rigid surface patches shown in different colors.
5. Automatic Skeleton Extraction
Given the list of body segments, their associated seed vertices and triangle patches, we extract the kinematic skeleton structure of the animated mesh by first finding its kinematic topology (i.e. find which body parts are adjacent) and,
thereafter, by estimating the positions of the interconnecting
joints for the whole sequence.
To determine which body segments are adjacent, we analyze the triangles at the boundaries of the triangle patches.
Body parts A and B are adjacent if they have mutually adjacent triangles in their respective patch boundaries. Unfortunately, in practice a patch may be adjacent to more than one
other patch. If more than two patches are directly connected
(e.g. head, torso and arm), we need to decide which segments are truly kinematically connected and which are not.
Here we take a heuristic approach and consider only those
patches to be adjacent that share the longest common boundary (in terms of the number of adjacent boundary triangles).
For instance, if head, arm and torso are connected we calculate the number of neighboring triangles for all combinations
of patch pairings (e.g. head-torso, head-arm and torso-arm)
and do not assign the pair head-arm as an adjacent segment
since it has less neighbors in comparison with the other two
options. For any adjacent pair of patches, a joint has to be
found later. Note that in our system we assume that the body
part in the center of gravity of the mesh at the reference time
step is the root of the hierarchy.
In order to estimate the joint positions between two adjacent body segments A and B quickly, we only consider the
information from the sets of seed vertices VA and VB located
on these segments, and not the information from all vertices
of V . Instead of solving for the complete sequence of joint
positions, we significantly reduce the problem’s complexity
by first aligning the segment poses to a reference time step
tr (usually tr = 0), then solving for a single optimal joint position at ctr in the reference pose, and finally retransforming
ctr into the original poses of A and B. To serve this purpose,
for each time step t we first compute two rigid body transforms TAt→tr and TBt→tr that align the positions of the seed
vertices in both sets with the positions of the seed vertices
VA at the reference time step [Hor87].
For finding ctr we follow an idea proposed in [KOF05]
and assume that a good estimate for the correct sequence of
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

de Aguiar et al. / Automatic Conversion of Mesh Animations into Skeleton-based Animations

393

joint positions is the sequence of locations that minimizes
the variance in joint-to-vertex distance for all seed vertices
of the adjacent parts at all frames. Using this assumption,
[dATS06] solves for the joint location at the reference time
ctr by using a distance penalty based on the average Euclidean distance to regularize the solution. Alternatively, we
use the regularization term proposed by [AKP∗ 04], which
makes the estimated joint position come closer to the centroid position bt of the boundary curve between the two adjacent body parts at all time steps t. Therefore, we solve for
ctr by minimizing:
1
1
∗
σa (ctr ) + ∗ ∑ σb (ctr ) + α ∗ d(ctr , btr )
2 va∑
2
∈VA
vb ∈VB
(2)
where σa (ctr ) and σb (ctr ) corresponds to the Euclidean distance variance over time between the joint position ctr and
the vertex va and between ctr and vb , respectively. d(ctr , btr )
is the Euclidean distance between ctr and btr at the reference
time step. The coefficient α is used to regularize the solution,
making the joint position be located as closed as possible to
the interior of the mesh. The results in Sect. 8 were generated
using a value of α = 0.05 (which we found to be satisfactory
in our experiments).
J(ctr ) =

After solving Eq. 2 and finding the optimal joint location
ctr , the joint positions at all other frames can be easily com∗ ctr . By applying the above procedure
puted by ct = TA−1
t→tr
to all adjacent body parts we reconstruct all joint positions
for the whole sequence (see Fig. 4).
6. Motion Parameters Estimation
A consistent parameterization of the skeletal motion in terms
of joint parameters is only feasible in case the skeleton structure has constant dimensions over time. However, due to
possible errors generated by aligning the body parts in the
reference frame (mostly caused by subtle (non-rigid) relative
motion between vertices on the same segment) the lengths
of the bones in the skeleton may slightly vary over time. We
enforce the bone lengths to be constant by stepping through
the hierarchy of the estimated skeleton from the root down
to the leaves and correcting the joint positions for each pair
of subsequent joints in the kinematic chain separately.
Let cti be the position of a joint i and cti−1 the position of
its parent joint at time t. We are able to calculate the optimal
value for the length of the bone connecting joint i − 1 and i,
li−1,i , over time and the new positions for the joints i, ncti ,
by minimizing the following energy:
S(nci ,li−1,i ) =
N

∑

t=0

cti − ncti

(3)
2

+ ( ncti − cti−1 − li−1,i )2 .

The first term in Eq. (3) keeps the new joint position ncti as
close as possible to the old position, while the second term
constrains the bone length to be the same in all frames.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Figure 4: (upper) Visual comparison between the reconstructed joint positions and the true positions (shown as
white spheres) at four different frames. (lower) Plot of the
low difference in z-coordinate between the reconstructed left
elbow joint position and the ground truth position over time
(biped walking sequence).
After solving this equation for each pair of subsequent
joints in the hierarchy we obtain a consistent kinematic
structure of the mesh M. To infer joint motion parameters,
i.e. a rotational transformation Tti for all joints i at all times
t, we first specify the skeletal pose at the first time step as reference pose (this is the best we can do given no explicit reference). Thereafter, we apply a Cyclic-Coordinate-Descent
(CCD) like algorithm [Lue73,BMB87] to infer all Tti relative
to the reference using Euler angle parameterization. To this
end, we optimize one joint variable at a time by calculating
the positional error w.r.t the estimated joint positions found
for that time step. Since we have all in-between joint positions of each kinematic sub-chain, our method converges
quickly and reliably. Finally, the translation of the root is
stored as additional parameter for each frame.
7. Skinning Weight Computation
Skinning is the process of attaching the skeleton to the surface in such a way that changes in skeletal pose lead to plausible surface deformations. Although more advanced deformation schemes exist, Sect. 2.2, we decided to use the
standard linear blend skinning method [LCF00] (also called
skeletal subspace deformation method - SSD) since it is
widely supported in games and animation packages.
Let p0 (vi ) be the position of the vertex vi of M in the
reference pose (or rest pose), let Ttb be the transformation
of the bone b from the reference to time t, and let wb (vi )
be the weight of the bone b for the vertex vi . Note that the
j
bone transformation Ttb equals the transformation Tt of the
preceding joint j from the hierarchy. SSD expresses the new
position of vertex vi at time t as pt (vi ) = ∑b (wbi Ttb p0 (vi )).

394

de Aguiar et al. / Automatic Conversion of Mesh Animations into Skeleton-based Animations

Therefore, in order to use the SSD method to re-animate the
sequences using a more compact representation, we need to
determine the bone weights wbi for each vertex vi , i.e. we
need to know how much each bone influences each vertex.
Although alternative approaches for skinning weight
computation are available in the literature, we employ
the method proposed in [BP07] to determine the skinning
weight distribution for each bone. This method computes the
distribution based on the results of a heat diffusion process
rather than based on simple vertex proximity to the bone,
which makes the estimation process more robust. In contrast
to their work, however, we consider the entire sequence of
mesh and skeleton poses from the input when finding optimal weights. In particular we first solve for the weight distributions wbf of each frame f separately, and thereafter average
them to obtain the final distributions wb .
When computing the weight distribution wbf we regard the
character volume as an insulated heat-conducting body and
force the temperature of the bone b to be 1 and the temperature of all others to be 0. The weight wbf (vi ) equals the
equilibrium temperature at vi . For computational simplicity,
the equilibrium equation is only solved on the mesh’s sur∂wb

face yielding ∂t f = ∆wbf + H f (pbf − wbf ) = 0. In our discrete
case this can be reformulated as
(−∆ f + H f )wbf = H f pbf .

(4)

In this equation, ∆ f is the discrete Laplacian operator at
frame f [WMKG07], pbf is a vector where pbf (vi ) equals 1 in
case b is the nearest bone to vertex vi and 0 otherwise. H f is
a diagonal matrix with entries Hii = 1/dist(vi )2 representing
the heat contribution weight of the nearest bone to vertex vi
at frame f . Here, dist(vi ) is the Euclidean distance between
vertex vi and the nearest bone in case it is contained in the
character’s volume and 0 otherwise. The final weight distributions wb for each bone is the average of the weights wbf
for all frames.
The heat diffusion solution provides smooth and realistic
blending weight distributions since it respects geodesic surface proximity during weight assignment rather than errorprone Euclidean proximity [BP07]. Furthermore, our experiments show that by computing the optimal weights from all
available poses our skeletal animation more faithfully reproduces the entire original mesh animation.
8. Experiments and Results
To demonstrate and validate our algorithms, we applied it to
a set of mesh animations generated in a variety of different
ways, see Tab. 1 for a list. Some synthetic sequences, such as
the horse sequence, Fig. 1, were generated with a mesh deformation method [SP04]. Other synthetic sequences like the
bird and the walking biped (see video) were originally created with a skeleton-based method which conveniently gives

Figure 5: Visual comparison between pre-defined skeleton
embedded using [BP07] (l) and the skeleton extracted by our
approach (r). Despite slight differences in the torso area, our
estimated skeleton closely resembles the fitted template.
us ground truth data. We also have captured performances
of humans at our disposition. The dancing and capoeira sequences, Fig. 6 and Fig. 8, were reconstructed by a meshbased marker-less motion capture approach [dATSS07a].
The cartwheel and posing sequences (Fig. 8 and video) were
obtained by using raw motion capture marker trajectories as
constraints in a deformation approach [dATSS07b].
Fig. 8 shows one original input mesh, the color-coded
skinning weight distributions, and some poses of the
original animation re-rendered using our skeleton-based
reparametrization. A visual comparison between our result
and the input, see Fig. 6 and several examples in the accompanying video, shows that our result is visually almost indistinguishable from the original mesh animation and exhibits
very natural surface deformations. Furthermore, visual inspection already suggests that the estimated kinematic structures are nicely embedded into the subjects at all frames and
possess biologically plausible dimensions and hierarchies.
Here, we would like to note again that all these results were
obtained fully-automatically. Our new representation is very
compact. For the horse animation, for instance, we only need
to store geometry and skinning weights once, and for each
time step only store 60 motion parameters (3-DOF per joint)
rather than approx. 25000 coordinate values.
To get a quantitative impression of the faithfulness and
quality of our results, we analyze individual aspects of our
method more closely.
Skeleton Reconstruction Since for most synthetic data we
know the true sequence of joint positions, we are able to
provide a quantitative estimate of the accuracy of our skeleton extraction and motion capture approach. Fig. 4(upper)
shows a visual comparison between the joint positions estimated by our approach and the true joint positions, shown as
white spheres, for the walking biped sequence. Fig. 4(lower)
illustrates the accuracy of the reconstructed motion parameters over time. The plot shows a comparison between the
z-coordinate of the true and estimated positions of the left
elbow joint for the same walking sequence. The difference
between true and estimated joint positions is very small and
consistent which illustrates the robustness of our method.
Similar low errors could be observed in all our sequences
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

de Aguiar et al. / Automatic Conversion of Mesh Animations into Skeleton-based Animations

395

Figure 6: (left) Two pairs of images comparing the input (left sub-image) to our skeleton-based result (right sub-image). In
either case the renderings are visually almost indistinguishable. (right) Plot showing the low average Euclidean distance error
between input and reconstructed vertex positions for the human-sized model.
with available ground truth. The column JACC in Tab. 1
shows the average Euclidean distance error between estimated joint position and true joint position for all joints over
time. Due to the lack of absolute coordinates, the error is
given in percent of the longest bounding box (B.B.) dimension of the model. In all our examples, major misalignments
between the original and the reconstructed skeleton could
only be found if the part of the input corresponding to that
joint was not prominently articulated.
Our automatically computed bone hierarchies closely
match the skeletons that would typically be created by animators. This is shown in the comparison in Fig. 5 where
we show side-by-side our result and the result of fitting
a template kinematic model with the method of Baran et
al. [BP07]. Only the topology of the root/spine area slightly
differs.
Accuracy of reparameterized animation Fig. 6(left)
shows a comparison between two frames of the input dancing sequence (left sub-images) and the generated skeletonbased animation (right sub-images). Visually, almost no difference can be seen. The faithful reproduction of the original
input is also shown in the accompanying video.
Fig. 6(right) plots the consistently low average difference
between the vertex positions of the input and the reparameterized output over time for the dancing sequence. For the
human-sized figure, the error is mostly below one centimeter which shows the high reconstruction quality also quantitatively. Column ACCU of Tab. 1 shows that similarly low
errors are observed in the other test sequences.
Pose and animation editing Using our system, we are able
not only to recreate the original input based on a more compact representation, but can straightforwardly produce novel
postures of the input mesh, see Fig. 7. To this end, we only
need to modify the joint parameters which can easily be done
in any standard animation package. Since we have a complete set of motion parameters for the input, we can also easily modify aspects of the original animation by altering the
joint parameter curves of selected joints (see accompanying
video).
Computation time Tab. 1 lists the run times of each processing step in our algorithm. The second and third columns
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Figure 7: New poses for the input mesh can be easily generated by simply changing the joint parameters of the extracted
skeleton.
show the number of frames in each sequence (FRAMES)
and the number of triangles in each model (NUMTRI). The
column SEGM lists the time needed for clustering and mesh
segmentation. Column SKEL lists the time needed to build
the skeleton and estimate all motion parameters, and column SKIN lists the time needed to find the blending weights.
With the exception of SKIN which shows per-frame times,
all times given are for processing entire sequences. All run
times were measured on a Laptop featuring an Intel Core
Duo CPU with 1.7 GHz.
Discussion Our approach is subject to a few limitations.
During skeleton extraction, it is impossible to locate a joint
if there is no relative motion between adjacent body parts.
Therefore, in some of our sequences hands and feet are
missed due to insignificant relative motion. However, we
consider this to be a principal problem of any data-driven
skeleton extraction method, and user interaction is feasible
in this case.
Most remaining reconstruction inaccuracies are due to
non-rigid deformation components in the input that are not
well explainable by a rigid skeleton and linear skinning
weights. As part of future work, we plan to investigate if
alternative skinning methods lead to an even further reduc-

396

de Aguiar et al. / Automatic Conversion of Mesh Animations into Skeleton-based Animations

SEQUENCE
Horse
Bird
Biped
Cartwheel
Posing
Capoeira
Dancing

FRAMES
47
60
80
120
280
150
220

NUMTRI
17K
55K
32K
7K
7K
106K
106K

SEGM
4s
5s
5s
4s
9s
44s
37s

SKEL
17s
49s
36s
63s
261s
244s
312s

SKIN
7s/frame
10s/frame
14s/frame
3s/frame
4s/frame
28s/frame
28s/frame

JACC
N/A
2.9% of B.B.
1.7% of B.B.
N/A
N/A
N/A
N/A

ACCU
0.56% of B.B.
0.42% of B.B.
0.52% of B.B.
0.81% of B.B.
0.43% of B.B.
0.47% of B.B.
0.37% of B.B.

Table 1: Given an animated mesh sequence with T triangles (NUMTRI) and N frames (FRAMES), the processing times for
segmenting the mesh into triangle patches (SEGM), to extract its kinematic structure and reconstruct its motion parameters
(SKEL), and to calculate the skinning weights based on the input data (SKIN) are shown. Also, the low average difference
between estimated joint positions and true joint locations - in percent of the maximal side length of the overall bounding box
(JACC) and the average difference between original and reconstructed vertex position (ACCU) are indicated.
tion of the residual errors, e.g. [MMG06, KCZO07, AS07,
WPP07].

from 3d range data. In In Proc. of AUAI ’04 (Virginia, USA,
2004), pp. 18–26.

Furthermore, skeletal reparametrization works very well
for subjects whose motion is largely due to a skeleton such
as humans and most animals. In largely non-rigidly moving
animations, such as a deforming piece of cloth, our algorithm would still determine a skeleton, but it is not physically
plausible. Therefore, mesh-based editing approaches might
be preferred in this case [XZY∗ 07, KG06].

[AS07] A NGELIDIS A., S INGH K.: Kinodynamic skinning using
volume-preserving deformations. In Symposium on Computer
Animation (2007), pp. 129–140.

Despite these limitations, we have presented a simple,
practical, robust, and easy-to-implement approach to automatically transform an arbitrary mesh animation into a fullyrigged kinematic skeleton animation.
9. Conclusion
We presented a fully-automatic method to extract a kinematic skeleton, joint motion parameters, and surface skinning weights from an animation of a single triangle mesh.
The result is a compact representation of the original input that can be easily rendered and modified in standard
skeleton-based animation tools without having to modify
them in the slightest. This way, we are able to preserve the
great modeling flexibility of purely mesh-based approaches
while making the resulting skeleton-less animations straightforwardly available to the animator’s repertoire of processing tools. Our results show that the efficient combination of
skeleton learning and temporally-coherent blending weight
computation enables us to effectively bridge the gap between
the mesh-based and skeleton-based animation paradigms.
Acknowledgments: This work is supported by EC within
FP6 under Grant 511568 with the acronym 3DTV.
References
[AHLD07] AUJAY G., H ÉTROY F., L AZARUS F., D EPRAZ C.:
Harmonic skeleton for realistic character animation. In SCA ’07
(2007), pp. 151–160.
[AKP∗ 04]

A NGUELOV D., KOLLER D., PANG H.-C., S RINI VASAN P., T HRUN S.: Recovering articulated object models

[BMB87] BADLER N. I., M ANOOCHEHRI K. H., BARAFF D.:
Multi-dimensional input techniques and articulated figure positioning by multiple constraints. In SI3D (New York, NY, USA,
1987), pp. 151–169.
[BP07] BARAN I., P OPOVI C´ J.: Automatic rigging and animation
of 3d characters. ACM Trans. Graph. 26, 3 (2007), 72.
[BS08] B OTSCH M., S ORKINE O.: On linear variational surface
deformation methods. IEEE Transactions on Visualization and
Computer Graphics 14, 1 (2008), 213–230.
[dATM∗ 04] DE AGUIAR E., T HEOBALT C., M AGNOR M.,
T HEISEL H., S EIDEL H.-P.: M3 : Marker-free model reconstruction and motion tracking from 3d voxel data. In PG 2004
(Seoul, Korea, October 2004), pp. 101–110.
[dATS06] DE AGUIAR E., T HEOBALT C., S EIDEL H.-P.: Automatic learning of articulated skeletons from 3d marker trajectories. In Proc. ISVC (2006), pp. 485–494.
[dATSS07a] DE AGUIAR E., T HEOBALT C., S TOLL C., S EIDEL
H.-P.: Marker-less deformable mesh tracking for human shape
and motion capture. In Proc. IEEE CVPR 2007 (June 2007).
[dATSS07b] DE AGUIAR E., T HEOBALT C., S TOLL C., S EIDEL
H.-P.: Rapid animation of laser-scanned humans. In IEEE Virtual Reality 2007 (Charlotte, USA, 2007), pp. 223–226.
[Hor87] H ORN B.: Closed-form solution of absolute orientation
using unit quaternions. Journal of the Optical Society of America
4(4) (1987), 629–642.
[JT05] JAMES D., T WIGG C.: Skinning mesh animations. ACM
Transactions on Graphics (Proc. SIGGRAPH) 24, 3 (2005).
[KCZO07] K AVAN L., C OLLINS S., Z ARA J., O’S ULLIVAN C.:
Skinning with dual quaternions. In ACM SIGGRAPH Symposium
on Interactive 3D Graphics and Games (2007), pp. 39–46.
[KG06] K IRCHER S., G ARLAND M.: Editing arbitrarily deforming surface animations. In SIGGRAPH ’06 (2006), ACM Press,
pp. 1098–1107.
[KM04] K URIHARA T., M IYATA N.: Modeling deformable human hands from medical images. In SCA ’04 (Aire-la-Ville,
Switzerland, 2004), pp. 355–363.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

de Aguiar et al. / Automatic Conversion of Mesh Animations into Skeleton-based Animations

397

Figure 8: Results obtained with different input animations (left to right in each row): One frame of the input animation, colorcoded blending weight distribution, and two poses of the input recreated with our new skeleton-based representation. Our
method efficiently and fully-automatically converts mesh animations into skeleton-based animations.
[KOF05] K IRK A. G., O’B RIEN J. F., F ORSYTH D. A.: Skeletal
parameter estimation from optical motion capture data. In CVPR
2005 (June 2005), pp. 782–788.

[SLSK07] S HARF A., L EWINER T., S HAMIR A., KOBBELT L.:
On–the–fly curve-skeleton computation for 3d shapes. In Eurographics (Prague, Czech, September 2007), pp. 323–328.

[KT03] K ATZ S., TAL A.: Hierarchical mesh decomposition using fuzzy clustering and cuts. In SIGGRAPH ’03 (New York, NY,
USA, 2003), pp. 954–961.

[SP04] S UMNER R. W., P OPOVIC J.: Deformation transfer for
triangle meshes. In SIGGRAPH ’04 (2004), ACM Press, pp. 399–
405.

[LCF00] L EWIS J. P., C ORDNER M., F ONG N.: Pose space
deformation: a unified approach to shape interpolation and
skeleton-driven deformation. In SIGGRAPH ’00 (New York, NY,
USA, 2000), pp. 165–172.

[SY07] S CHAEFER S., Y UKSEL C.: Example-based skeleton extraction. In SGP ’07 (Aire-la-Ville, Switzerland, 2007), pp. 153–
162.

[LKA06] L IEN J.-M., K EYSER J., A MATO N. M.: Simultaneous shape decomposition and skeletonization. In SPM ’06 (New
York, NY, USA, 2006), pp. 219–228.
[Lue73] L UENBERGER D. G.: Introduction to Linear and Nonlinear Programming. New York, NY, USA, 1973.
[MG03] M OHR A., G LEICHER M.: Building efficient, accurate
character skins from examples. In SIGGRAPH ’03 (New York,
NY, USA, 2003), pp. 562–568.
[MMG06] M ERRY B., M ARAIS P., G AIN J.: Animation space:
A truly linear framework for character animation. ACM Trans.
Graph. 25, 4 (2006), 1400–1423.
[NJW02] N G A. Y., J ORDAN M., W EISS Y.: On spectral clustering: Analysis and an algorithm. In Proc. NIPS (2002).
[Pop07] P OPPE R.: Vision-based human motion analysis: An
overview. Computer Vision and Image Understanding 18, 1
(2007).
[Sha08] S HAMIR A.: A survey on mesh segmentation techniques.
Computer Graphics Forum (2008).
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

[TRdAS07] T HEOBALT C., R ÖSSL C., DE AGUIAR E., S EIDEL
H.-P.: Animation collage. In Proc. SCA (2007), pp. 271–280.
[WMKG07] WARDETZKY M., M ATHUR S., K AELBERER F.,
G RINSPUN E.: Discrete laplace operators: No free lunch. In
Symposium on Geometry Processing (2007), pp. 33–37.
[WP02] WANG X. C., P HILLIPS C.: Multi-weight enveloping:
least-squares approximation techniques for skin animation. In
SCA ’02 (New York, NY, USA, 2002), pp. 129–138.
[WPP07] WANG R. Y., P ULLI K., P OPOVI C´ J.: Real-time enveloping with rotational regression. ACM Trans. Graph. 26, 3
(2007), 73.
[XZY∗ 07] X U W., Z HOU K., Y U Y., TAN Q., P ENG Q., G UO
B.: Gradient domain editing of deforming mesh sequences. ACM
TOG 26, 3 (2007), 84.
[YBS07] YOSHIZAWA S., B ELYAEV A. G., S EIDEL H.-P.:
Skeleton-based variational mesh deformations. In Eurographics
(Prague, Czech, September 2007), pp. 255–264.
[YGZS05] YAMAUCHI H., G UMHOLD S., Z AYER R., S EIDEL
H.-P.: Mesh segmentation driven by gaussian curvature. The
Visual Computer 21, 8-10 (2005), 649–658.

