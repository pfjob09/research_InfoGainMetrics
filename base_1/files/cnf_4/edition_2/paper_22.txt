Motion Track: Visualizing Variations of Human Motion Data
Yueqi Hu ∗

Shuangyuan Wu†

Shihong Xia‡

Jinghua Fu§

Wei Chen¶

Abstract

This paper proposes a novel visualization approach, which can depict the variations between diﬀerent human motion data. This is
achieved by representing the time dimension of each animation sequence with a sequential curve in a locality-preserving reference 2D
space, called the motion track representation. The principal advantage of this representation over standard representations of motion
capture data - generally either a keyframed timeline or a 2D motion map in its entirety - is that it maps the motion diﬀerences along
the time dimension into parallel perceptible spatial dimensions but
at the same time captures the primary content of the source data.
Latent semantic diﬀerences that are diﬃcult to be visually distinguished can be clearly displayed, favoring eﬀective summary, clustering, comparison and analysis of motion database.

(a)

(b)

1 Introduction

Visualizing the variations of human motion data can help people understand the motion more quickly and more clearly. For instance,
we need to view the entire motion clip before we can make a judgment about the characteristics of the walking sequence of a general
character, including speed, style, whether it’s a child or an adult, a
male or a female. If we can visualize the motion sequence in a 2D
flatland with extracted features, and show its diﬀerences from other
sequences, the characteristics can be discerned immediately. Visualizing the variations of human motion data can also be applied to
motion recognition and gait analysis [7].
Motion capture denotes the process of recording movement and
translating that movement onto a digital model [15, 17]. With the
capability of creating a three-dimensional representation of a live
performance, it has become a popular tool for the generation of
realistic human motion. Typical applications include editing and
blending animations from multiple capture sessions, mixing and
matching with keyframed animation, and controlling the style and
quality of the final output [2, 3, 16, 19].
The broad use of motion capture devices produces a large
amount of motion data, and makes the large scale motion databases
essential for both computer animation and human motion analysis [13, 14, 24]. Exploring the motion capture database, however,
is not a trivial task. It is apparent that manual annotation and classification to each dataset is intractable, and an automatic indexing
and retrieval system is far away from perfect [9, 22]. This diﬃculty
is magnified by the high dimension of each motion capture dataset.
Also, each motion frame is expressed by the position information,
∗
e-mail: aaron.huyq@gmail.com, State Key Lab of CAD&CG, Zhejiang
University
† e-mail:wushuangyuan@ict.ac.cn, Institute of Computing Technology,
Chinese Academy of Sciences, Graduate School of the Chinese Academy
of Sciences
‡ e-mail:xsh@ict.ac.cn, Institute of Computing Technology, Chinese
Academy of Sciences
§ e-mail:jinghfu@zju.edu.cn, State Key Lab of CAD&CG, Zhejiang University
¶ e-mail:chenwei@cad.zju.edu.cn, State Key Lab of CAD&CG, Zhejiang University

IEEE Pacific Visualisation Symposium 2010
2 - 5 March, Taipei, Taiwan
978-1-4244-6686-3/10/$26.00 ©2010 IEEE

(c)

(d)

(e)

Figure 1: Representing a motion capture data with diﬀerent approaches. (a) The motion map approach clusters and shows key
frames in a 2D grid map [19]; (b) The action synopsis technique
illustrates a motion data with a 3D proximity-preserving curve associated with detected key poses [1]; (c) The motion belts representation focuses on the selection of key frames [23]; (d) The motion
overview approach produces a video clip for eﬀective summary and
depiction [2]; (e) The motion track proposed in this paper embeds
a motion sequence in a 2D reference space that is built from the
motion database. Selected key frames from the database are shown
as reference points.

while the mapping from the position to categorical properties like
motion types, health condition and ages is still unclear [5, 8, 10].
Often, researchers rely on clustering techniques to classify a motion database. Successful systems include the motion belts [23],
the motion map [19], and motion overview [2]. However, applying those methods to datasets containing latent diﬀerences (e.g.,
two walking examples of two persons) proves to be ineﬀective,
as demonstrated by some results in Figure 1 (a-d). Challenges in
this include how to determine appropriate metric for diﬀerent motion sequences, and how to expressively visualize their diﬀerences
which are essentially high dimensional.
This paper presents a novel diﬀerentiation and visualization

153

Adjusted Map

SOM

LLE

A new frame

The motion
representation
M o track
tio n T ra
ck
Motion database

Figure 2: The conceptual overview of our approach.

scheme that takes an analyze-and-visualization routine, where prior
to visualization, it extracts key frames from the motion database
with a clustering process, and builds a reference space by employing a locality-preserved low-dimensional embedding to the key
frames. The two-dimensional layout of an input motion sequence
with respect to the pre-computed reference space actually forms
a new representation, called a motion track. Here, the key frames
shown in the 2D space provide a clear hint to the underlying motion
sequence, and act as the reference points to the embedded curve.
Figure 1 (e) shows a motion track example, where the input motion
sequence is shown in the bottom.
The new representation is static - like a motion map - but is organized to convey continuity and directionality - like an animation.
It is a curve in a 2D space, and thus eﬀectively exploits the parallel perception capability of human visual system. One distinctive
advantage of this scheme is that a long motion capture sequence
can be expressed in a 2D reference space which is pre-computed
based on the input motion database. In contrast, using a motion
map [19] loses the continuity, and placing a motion sequence along
a straight timeline [23] can not present the directionality. In addition, our approach is particularly well-suited for situations where
latent semantic transitions present like two walking persons, or even
two walking samples of one person, in which their diﬀerences are
almost visually indistinguishable with the existing representations.
Although some previous work like the action synopsis approach [1]
also formulates the sequence as a curve in a low-dimensional space
(3D), its focus is on the selection of key frames by locating significant points in a 3D curve for the purposes of eﬀective motion
illustration. Likewise, the motion overview approach [2] generates
an overview video clip associated with a carefully designed camera path. The result is a dynamic video sequence instead of a still
visualization.
The rest of this paper is organized as follows. Section 2 reviews
the related work concerning the motion data representation, index
and retrieval and visualization. Our approach is elaborated in Section 3. Experimental results and analysis are given in Section 4.
Section 5 concludes this paper and highlights the future work.

154

2 Related Work

Motion Data Data-driven motion synthesis is an important technique to create new, realistic motions from motion capture data.
Eﬃcient analysis, editing, and reuse of motion capture data has became a challenging task with the rapid increasing of motion capture
data. There are many approaches for illustrating motions in a static
image. For instance, Simon Bouvier et al. [4] proposed to use directed arrows, noise waves, and stroboscopic motions to represent
motions. To eliminate the influence of unnecessary data such as
the bone length, the position information from the markers is represented with quaternion [6].
Index and Retrieval With the growth of available motion database,
much attention has been paid on the indexing and retrieval of motion data. Muller et al. [16] presented a method for the feature extraction and matching of motion data by calculating the geometric
information of joints positions and achieved good results for some
types of motion data. This method requires the users to select different features according to the characteristic of the motion types.
Principal components analysis, as a widely used dimension reduction and feature extraction method, is also used in the processing
of motion capture data. Forbes et al. [9] used weighted PCA in the
searching of motion data. Barbic et al. [3] use PCA technique to
cut long motion data stream into single behavioral segments.
Visualizing Motion Capture Data Visualizing motion capture data
is tightly related to an appropriate representation. To respect the
sequentiality of the motion capture data, a visualization approach
typically employs a sequential representation. The motion belts approach [23] displays a motion capture data as a short clip of selected key frames. By preserving the proximity measures among
the individual frames in a high-dimensional space in a 3D space
with the multi-dimensional scaling (MDS) technique, key poses can
be detected in locations where local variations are sharp, yielding
an abstractive motion illustration. Following this scheme, a video
clip associated with a camera path is generated for eﬀective motion
overview [2].
Another category of visualizing motion capture data seeks to
characterize the relationships among diﬀerent poses in a motion

database. The results are commonly shown in a 2D space. For
instance, Sakamoto et al. [19] presented a motion map representation that uses the self-organizing map technique [12] to cluster the
motion data and index the clips by the resulting SOM nodes. The
hierarchical indexing and retrieval approach takes a similar way in
that the motion data is first classified with the SOM algorithm and
then represented with a hierarchical tree [21].
3 Our Approach

To analyze a large amount of multivariate motion capture data, we
propose an analyze-and-visualization approach that first constructs
a 2D reference space, and then embeds a source dataset in this
space. The pipeline is shown in Figure 2.
A 3D motion capture dataset shows its complexity in both space
and time. Basically it consists of a list of skeletal poses, which
are defined by a number of skeleton parts connected by joints. The
rotation information is described with either Euler angles or quaternion numbers. We choose the quaternion representation for better
interpolation result. A typical skeleton model contains 19 joints
and 23 parts (see Figure 3), of which each joint is represented with
a 4-component quaternion [6]. The set of these parameters encodes
a motion frame as a point in a high-dimensional space.
The directionality of motion is not taken into consideration because the styles and features will not change with the orientation.
By neglecting the translation and orientation information of the
root joint, each frame becomes a 72-dimensional vector, and a sequence of motion data can be represented by a matrix Mm×n , where
m is the number of frames, and n equals 72.

Joint Node
Root Node
Site Node

Figure 3: A joint-based skeleton model.
3.1

Constructing the Reference Space

3D human motion data is a type of high-dimensional and nonlinear
data. A clip of motion data are usually made up of hundreds or
even thousands of frames. So we employ the self-organizing map
(SOM) algorithm [12] to cluster the input data and compute representative frames. SOM is an unsupervised non-linear approach, and
can preserve the topological properties of the input space by using
a neighborhood function [11]. The extracted key points are located
on a regular low-dimensional (2D) grid in an ordered fashion. Figure 4 displays the SOM result of a walking motion sequence, where
each grid represents a 72-dimension pose dataset, and these vectors
are adjacent to each other by a neighborhood relation in Euclidean
distance.
This preprocessing can be viewed as a kind of re-sampling,
which is diﬀerent from traditional keyframing. Through SOM,

Figure 4: The SOM of a walking motion sequence.

we can preserve the continuity in the extracted frames that is
needed in the following dimensionality reduction process, while
other keyframing methods cannot guarantee such continuity.
The clustering results actually form a reference space for all
frames in the database, e.g., any frame is supposed to be a combination of certain samples in the SOM. Thus, a straightforward visualization approach would be placing every frame in the SOM with
weighted positions, and sequentially connecting them to get a 2D
curve. This map-based curve representation is simple, and has been
widely adopted in [19, 22]. However, a canonical space formed by
the SOM may be constrained by the grid resolution, and can hardly
express the proximities among diﬀerent frames. In terms of motion
visualization, SOM is well-suited for data abstraction, but is not an
optimal solution for data representation.
Note that, each frame in a motion database is encoded with a
high-dimensional vector, and its relationships with other frames are
non-linear. Thus, a non-linear dimension reduction technique is
needed for displaying the multivariate motion capture data. The
Local Linear Embedding (LLE) [18] technique meets this requirement. By name, the LLE algorithm is an unsupervised learning algorithm that computes a low-dimensional, neighborhoodpreserving embedding for a set of high-dimensional points.
LLE maps its input into a global coordinated space of lower dimensionality. It first finds k neighbors for each point based on
the specified proximity measure. In our approach the Euclidean
distance is employed. Then, it computes the weights for a lowdimensional representation by minimizing a reconstruction error:
N

min ε(Y) =

k

wi yi j |2

|yi −
i=1

(1)

j=1

where yi is a high-dimensional point, yi j denotes its neighbors, and
wi is the embedding weight. With LLE, the key frames computed
with the SOM are projected into a 2D embedding space, as shown
in Figure 5. Embedding points here are bijective related to high
dimension vectors, which are actually skeletal poses shown in Figure 5. The set of these 2D projections serves as the reference frames
for representing a new motion capture dataset.

155

Further considering the range of the data space yields:
wi =

i
[ R−h
Rhi ]

p

i
[ R−h
Rhi ]

(3)

p

where R is the distance from the source point to the most distant
reference point.
A more sophisticated solution would incorporate a Gaussian
smoothing process:
wi =

e[

R−hi p
]
R

e[

(4)

R−hi p
R ]

Figure 5: Applying LLE to the key frames shown in Figure 4.

By exploiting the local symmetries of linear reconstructions,
LLE is capable of preserving the global structure of non-linear manifolds. Even for datasets that exhibit strong coherence like the face
and gait, LLE still performs quite well [18]. In contrast, other popular low-dimensional embedding approaches like PCA and MDS are
suitable for datasets whose distances are adequately large to distinguish, which are compared in Figure 10 and Figure 16. On the other
hand, the self-organizing map representation can not guarantee the
global optimality or convergence.
3.2

Generating the Motion Track

An ideal visualization of a motion capture dataset would be both
spatial and temporal sequential, or say, it is inherently a continuous
curve. On the other hand, the key frames characterize the primary
content of the motion database, and the LLE preserves the local
structure of the key frames. Thus each frame of a new motion capture dataset can be represented as the combination of the projected
key frames. Connecting the frames sequentially yields a long 2D
trajectory, called a motion track.
The position of each frame in the 2D reference space is computed
by weighting the positions of related key frames. The weights are
computed with respect to the proximity measure of frames in the
original high-dimensional space. Various weighting schemes can
be used. The simplest one uses this nearest key frame (see Figure 6
(a)). A more reasonable solution is to compute the weights according to the proximity measures among diﬀerent frames. For instance,
the weight associated with each reference frame can be linearly proportional to the inverse of its distance to the source frame, as shown
in (see Figure 6 (b)).
In practice, we employ the Shepard inverse distance weighted
(IDW) interpolation approach [20]. It assumes that the influence
from a reference point to the source point is inversely proportional
to their distance, and the sum of weights is 1. A general form of the
inverse distance weighting form is as follows:
−p

wi =

hi

−p

hi

(2)

where hi is the distance from the source point to the ith reference
point in the high-dimensional space.

156

(a)

(b)

(c)

(d)

Figure 6: Results for a walking motion sequence shown in Figure 7
with diﬀerent weighting schemes. (a) Nearest neighboring; (b) The
inverse distance weighting with Equation 2, where the parameter
p = 8; (c) The inverse distance weighting with Equation 3, where
the parameter p = 2; (d) The inverse distance weighting with Equation 4, where the parameter p = 2.

Figure 7: A short walking motion sequence.
With an appropriate weighting scheme (e.g., Equation 4), a motion track is smooth and continuous (e.g., Figure 6 (d)), like the
original motion sequence (see Figure 7). It is a static curve in a 2D
space, and eﬀectively exploits the parallel perception capability of
human visual system. Moreover, it is organized in a pre-computed
2D reference space so that both the continuity and directionality are
conveyed, making it a natural depiction of an animation. Furthermore, the reference frames provide a clear hint to the underlying
motion sequence, and act as the reference points to the embedded
curve. All these features favor eﬀective summary, abstraction, comparison and analysis of a motion database.
3.3 Implementation Details

By definition, the key frames computed by SOM do not necessarily
belong to the input database. It is very likely that an unreal pose is

produced. Thus, an additional adjustment stage can be employed
to replace each frame in the SOM with the one that is closest to
it in the input database. Figure 8 compares two sequences with or
without the adjustment.

Jumping
Marching
Walking

SOM Result

Real Frames

Figure 10: Results for three motion sequences: jumping (red),
marching (blue), and walking (purple).

Figure 8: The frames computed with SOM and the original frames.
In the low-dimensional embedding stage, the number of neighborhood k must be smaller than the dimension. Finding an optimal k for any kind of data, however, is not trivial, and is applicationdependent. Current implementation in our approach allows the user
specify an appropriate k. Automatic optimization of k is left for future study.
In our motion model, this value is indicated by the joint angular
speed, an important parameter used in motion analysis. We visualize it with the brightness of the motion track. The white point on
track is the interpolation position of each frame in LLE embedding
space. This track can reveal the order of the motion sequence, as
shown in Figure 9

Overall the three motion tracks display distinctive trajectories.
This verifies the capability of the motion track representation for
diﬀerentiating diﬀerent motion types. It also reveals that the walking and marching sequences are intersected with each other because
they share some poses. Meanwhile, they are completely separated
from the jumping sequence because a jumping sequence does not
contain any frame from the walking and marching sequences. In
addition, the marching sequence shows more complex and longer
trajectory than the walk sequence, and contains more characteristic
key frames. All these observations conforms to our general sense.
A more detailed study on the motion tracks can find some representative features for each motion type. For instance, the leftmost
pose (in the blue circle) with lifting legs, is a recognizable pose for
the marching sequence, and the rightmost pose (in the red circle)
characterizes the jumping sequence.
In another experiment (see Figure 11), four sequences from four
individual persons yield two clusters. This also verifies the robustness of our approach.
What is more, the motion speed is obviously revealed by the
brightness of this motion track. Figure 10, the tracks of marching
and walking are brighter than jumping. And in Figure 11, the tracks
for running are darker than for walking. It means jumping moves
more quickly than walking and marching, and running is quicker
than walking, which is in line with common sense.
An interesting example is shown in Figure 12, where the tracks
come from very similar kinds of motions: walking, running and
marching. It is easy to find that the size of the motion track circle
for marching suggests the range of motion.

Figure 9: The final motion track with speed and frame sequence.
4 Results

We implemented an interactive visualization system based on the
Processing language (http://processing.org). All results and the
companion video were generated on a PC with an Intel 2.0 GHZ
CPU and 2.0G RAM. The experiments were performed on the
CMU motion capture database (http://mocap.cs.cmu.edu). Five
types of motions were used: walking, running, marching, jumping
and picking. All datasets were transformed to the quaternion representation without the information of the bone length and discard
the translation and orientation information of the root joint.
4.1

Examples with Different Motion Types

An SOM at the resolution of 16 × 16 (256 frames) is trained based
on a database that include 5 motion types, 205 motion sequences
and 84864 frames. In the LLE stage, the parameter k is set as 27.
Figure 10 visualizes three motion sequences: jumping, walking and
marching.

Run09_02:
Run104_04:
Walk16_32:
Walk35_01:

Figure 11: Results for four motion sequences which are from different persons.

157

4.2

Examples with the Identical Motion Type

The motion characteristics are diﬀerent from person to person, but
the diﬀerence between two motion sequences with the same motion
type is much smaller than that of two sequences with diﬀerent motion types. Thus, an appropriate approach should capture and distinguish the subtle diﬀerences. We tested our approach on a motion
database with 86 walking sequences (36880 frames). They were
collected from 18 diﬀerent persons according to the identification
numbers built in the CMU database. A 16 × 16 SOM is trained, and
the neighboring number k in LLE is 22. The results are depicted in
Figure 13.
Figure 13 (a) shows 10 motion tracks which are from two persons. In Figure 13 (b), 5 motion tracks are from 5 persons. It is
easy to see that the tracks from the same sources are well clustered,
while the tracks from diﬀerent sources exhibit distinctive patterns,
even though they are totally separated. Notice that all these tracks
represent the walking motion, and the overall shapes of their trajectories are similar. Figure 14 compares two motion sequences from
these two totally separated parts of Figure 13 (a). In this example,
their diﬀerences that makes a diﬀerence can be visually observed,
e.g., the leg bending and arm swinging parts.
A comparative example is shown in Figure 15. Two motion sequences are from one person, and correspond to two motion tracks
shown in the top of Figure 13 (a). Notice how they are visually
similar, and how they are distinguished with the motion track representation. The motion track representation not only shows the
diﬀerences among diﬀerent motion sequences, but also displays the
degree of their diﬀerences with the proximity measures among motion tracks.

4.3 Discussion and Comparison

There are many low-dimensional embedding approaches like PCA,
MDS IsoMap, and Locally Linear Projection (LLP). We choose to
use LLE because it nicely preserves the locality structure in a nonlinear fashion. Figure 16 shows the result by means of MDS for
three motion sequences displayed in Figure 10.
However, some good features of this motion track are only available on motions of high stability. Besides walking, we also have
experiments on other kinds such as running and marching, and
reach similar and convincing results as those on walking sequences.
While tracks on some motion kinds like jumping, picking and dancing are chaos. Figure 17 shows 5 motion tracks from the same
person, with two original motion sequence (the red one and the
blue one) displayed on the bottom. Even from the same person, the
sequences themselves are totally diﬀerent, and so are the motion
tracks. However, some moments are still hold the feature of cluster
as circled. It is when a person prepares to jump.
5 Conclusion and Future Work

This paper introduces a novel diﬀerentiation and visualization approach that is capable of characterizing a motion database, comparing multiple motion sequences, and analyzing one motion sequence. We call it the motion track representation. It is automatically constructed without any manual indexing and retrieval. One
distinctive feature of our method lies in similar motion sequences
diﬀerentiation. Motion features of diﬀerent persons are obviously
exhibited in our experimental results, which suggests that our approach is quite promising in many motion-related applications like
gait analysis and motion editing.
Currently only the rotation angles of the joints are considered.
We expect to incorporate other information such as the bone length
and the motion speed for more complicated reasoning tasks. We
also plan to examine many other types of motion sequences like
embracing.
Acknowledgements

This paper is supported by 973 program of China (2010CB732504),
NSF of China (No.60873123), NSF of Zhejiang Province
(No.1080618), HTRDPC-2007AA01Z320 and NSFC-U0935003.
References

Running
Walking
Marching

Figure 12: Motion tracks for Marching, running and walking.

158

[1] J. Assa, Y. Caspi, and D. Cohen-Or. Action synopsis: Pose selection
and illustration. ACM Press, 2005.
[2] J. Assa, D. Cohen-Or, I.-C. Yeh, and T.-Y. Lee. Motion overview of
human action. ACM Transactions on Graphics, 27(5), 2008.
[3] J. Barbic, A. Safonova, J. Pan, C. Faloutsos, J. K. Hodgins, and N. S.
Pollard. Segmenting motion capture data into distinct behaviors. In
Proceedings of Graphics Interface 2004, pages 185–194, 2004.
[4] S. Bouvier-Zappa, V. Ostromoukhov, and P. Poulin. Motion cues for
illustration of skeletal motion capture data. In Proceedings of NPAR,
pages 133–140. ACM, 2007.
[5] C. Chiu, S. Chao, M. Wu, S. Yang, and H. Lin. Content-based retrieval
for human motion data. Journal of Visual Communication and Image
Representation, 15(3):446–466, 2004.
[6] E. Dam, M. Koch, and M. Lillholm. Quaternions, interpolation and
animation. Technical report, 1998.
[7] J. W. Davis and H. Gao. An expressive three-mode principal
components model of human action style. Image Vision Comput.,
21(11):1001–1016, 2003.
[8] M. Field, D. Stirling, F. Naghdy, and Z. Pan. Mixture model segmentation for gait recognition. In ECSIS Symposium on Learning and
Adaptive Behaviors for Robotic Systems, pages 3–8, 2008.
[9] K. Forbes and E. Fiume. An eﬃcient search algorithm for motion
data using weighted PCA. In Proceedings of the 2005 ACM SIGGRAPH/Eurographics symposium on Computer animation, pages 67–
76, 2005.

[10] K. Grochow, S. L. Martin, A. Hertzmann, and Z. Popovi´c. Stylebased inverse kinematics. In SIGGRAPH ’04: ACM SIGGRAPH 2004
Papers, pages 522–531, New York, NY, USA, 2004. ACM.
[11] T. Kohonen. Self-organized formation of topologically correct feature
maps. Biological Cybernetics, (43):59–69, 1988.
[12] T. Kohonen. Self-Organizing Maps. Springer, 2001.
[13] L. Kovar and M. Gleicher. Automated extraction and parameterization of motions in large data sets. ACM Transactions on Graphics,
23(3):559–568, 2004.
[14] J. Lee, J. Chai, P. S. A. Reitsma, J. K. Hodgins, and N. S. Pollard.
Interactive control of avatars animated with human motion data. ACM
Transactions on Graphics, 21(3):491–500, 2002.
[15] M. Muller and T. Roder. Motion templates for automatic classification
and retrieval of motion capture data. In Proceedings of the 2006 ACM
SIGGRAPH/Eurographics symposium on Computer animation, pages
137–146, 2006.
[16] M. Muller, T. Roder, and M. Clausen. Eﬃcient content-based retrieval
of motion capture data. ACM Transactions on Graphics, 24(3):677–
685, 2005.
[17] K. . Pullen and C. Bregler. Motion capture assisted animation: texturing and synthesis. In Proceedings of ACM SIGGRAPH, pages 501–
508, 2002.
[18] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally
linear embedding. Science, 290(5500):2323–2326, December 2000.
[19] Y. Sakamoto, S. Kuriyama, and T. Kaneko. Motion map: image-based
retrieval and segmentation of motion data. In Proceedings of the 2004
ACM SIGGRAPH/Eurographics symposium on Computer animation,
pages 259–266, 2004.
[20] D. Shepard. A two-dimensional interpolation function for irregularlyspaced data. pages 517–524. ACM, 1968.
[21] S. Wu, Z. Wang, and S. Xia. Indexing and retrieval of human motion
data by a hierarchical tree. In Proceedings of ACM Symposium on
VRST, 2009.
[22] S. Wu, S. Xia, Z. Wang, and C. Li. Eﬃcient motion data indexing and
retrieval with local similarity measure of motion strings. The Visual
Computer, 25(5-7):499–508, 2009.
[23] H. Yasuda, R. Kaihara, S. Saito, and M. Nakajima. Motion belts. In
Proceedings of ACM SIGGRAPH Sketches.
[24] V. Zordan, A. Majkowska, B. Chiu, and M. Fast. Dynamic response for motion capture animation. ACM Transactions on Graphics,
24(3):697–701, 2005.

(a)

(b)
Figure 13: Results for a set of walking motion sequences. (a) Motion tracks from two persons; (b) Motion tracks from ten persons.

159

Jump:
March:
Walk:

Figure 16: Using MDS for low-dimensional embedding may lead
to overlapped motion tracks. In contrast, Figure 10 demonstrates
the advantage of using LLE.

Figure 14: Two walking sequences from two persons.

Figure 15: Two walking sequences (bottom) from one person, and
their motion track representations (top).

160

Figure 17: Five jumping motion tracks from one person, with the
original motion sequences of the blue one and the red one shown at
bottom.

