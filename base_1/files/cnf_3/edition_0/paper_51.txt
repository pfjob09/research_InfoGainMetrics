2012 16th International Conference on Information Visualisation

Adding a Semantic Layer to Flickr Images Search Service
D. Barbuto, G. Contaldi, S. Senatore
Dipartimento di Informatica - Università degli Studi di Salerno
via Ponte don Melillo – 84084 Fisciano (SA)
email: d.barbuto@studenti.unisa.it, g.contaldi@ studenti.unisa.it, ssenatore@ unisa.it

Abstract—The growing amount of images on the Web, the
diffusion of social media sharing web sites demand effective
tools for searching targeted images. In general, the
performance of Web image search depends on the quality of
images annotation, but often the keywords (or tags) associated
to an image are given without relevance information, strictly
connected to a subjective feeling of the taggers and far from
the objective description of the image. In this paper, we
propose a simple approach for social media sharing web sites
such as Flickr, Zooomr, etc. to support users to retrieve images
semantically correlated to a given tagged image. In this paper,
we present an application scenario for Flickr: in general
Flickr returns all the images that meets the input tags, without
no semantic analysis and evaluation of the effectiveness of the
search results. Our approach adds a semantic layer on the
Flickr output: processes the tags associated to the retuned
images to discover the appropriate semantics of them, by
arranging the results in a more user friendly view.

processing is applied. After the analysis of tags, our
application returns relevant images, organized in semantic
categories (through filtering/merging of redundant or blend
information discovered in these categories). Each semantic
category has associated a tags-cloud, and, when possible, it is
hierarchically described by means of its own subcategories.
The remaining of the paper is organized as follows:
Section II provides an overview about Flickr and its main
uses and functionalities exploited by web communities;
Section III is devoted to describe the add-on provided by
our application, through a formal description and modeling
of the approach. Then, Section IV shows the application at
work, in order to evidence the new functionalities.
Experimental results and conclusion close the paper.

Keywords: tagging, similarity measure, semantic clusters,
Flickr images, tf-idf.

Flickr can be considered the most popular photo sharing
website that allows users to upload their own photos into
customizable albums that can then be tagged, organized, and
publicly posted. In August 2011, Flickr reported that it was
hosting more than 6 billion images organized by more than 9
million registered Web-users. The annotation is achieved
manually and generally provides information about the
content of the photos or contextual and semantic
information, so often most of the tags associated to an image
by Flickr users are imprecise and not informative. In
addition, the average number of tags for each Flickr image is
quite small [4], because generally users cannot think of many
words [7] in a short moment and do not like to spend some
time thinking about the additional or more precise tags.
Moreover, the tags are given without an importance order
and then they are saved according to the input sequence. This
way, the tags are not always representative of an image, or
worst, quite unrelated. Some studies reveal, indeed there are
only around 50% tags actually related to the image [2] and
only less than 10% of the images have the most relevant tags
at the top position in their attached tag list [3].
The nature of Flickr tagging has moved some
communities to study and analyze the users’ behavior,
evidencing that the main motivation that moves user to tag
photos is to make them better accessible to the public [4].
The study presented in [5] reveals the modalities by which
the users tag their photos and what type of tags are usually
used: in general, users prefer to tag the photos with more
than one word, increasing the spectrum of meaning
associated to each photo, even though, this way may
contribute to augment the amount of noisy [6].

I.

II.

INTRODUCTION

The images discovery on the World Wide Web needs
sophisticated content-based image retrieval techniques, still
far from bridging the semantic gap between human concepts
that are expressed by keyword-based queries, and visual
features that are extracted from the images [1]. Yet, the wide
diffusion of Web 2.0 applications and semantics associated
to the resources prove the users are willing to provide
contextual information through annotation. The success of
social media web sites such as Flickr is due to easily share
user-generated tags, to describe the image contents on the
Web, according to personal feeling and motivation. Recent
user-behavior analysis outlines the annotation of Flickr
images is driven by personal idea, intention, and the desire to
improve the photos accessibility and retrieval to the general
public [4]. Thus, the users are used to tag their photos to
share them with family, friends, and online community at
large, and they use their own terms to describe the images
that can be different by other users that tag similar images
[8]. Due to the self-centered human nature in the tagging
task, the images search and retrieval represent tricky
activities that need preliminary processing to capture
objective aspects in the human generated tag, associated to
an image. To face this issue, our approach aims at returning
Flickr photos semantically related to a given one. We start
giving as an input an already existing Flickr photo. The tags
associated to a given photos are taken into account to
discover similar photos; no further technique of image
1550-6037/12 $26.00 © 2012 IEEE
DOI 10.1109/IV.2012.62

331

TAGGING IN FLICKR

The freedom of tagging has sparked the growth of the
community contributed multimedia content available online,
but, as the other side of the coin, has limited the access of the
social media, making the photos search and retrieval
ineffective. Just querying Flickr tag-based image search
service 1 and discovering it cannot provide the option of
ranking the tagged images; the result of a query, indeed is a
sequence of images, each one contains in the tag list the
query-word, even though the returned images do not meet
the meaning of the query, the effective user request.

clusters related to a given tag. We call collection the set of
these clusters and head the tag associated to the clusters of
the collection. Formally:
Definition 1: Given a tag t, a collection Cl(t) = {C1, C2,
…Cn} is a set of the clusters obtained by t. The tag t is
called head of Cl: head(Ci) = t, with i=1, …,n.
Our goal is to find out clusters that can be merged,
because describe the same semantic concept. We evaluate
similarity/relatedness among clusters belonging to different
collection.
Definition 2: Let X and Y be two clusters such that head(X)
≠ head(Y). X is related to Y, rel(X, Y) if there is a tag t ∈
X that appears as head in Y, head(Y) = t. Note that
rel(X, Y) ≠ rel(Y, X).
Definition 3: Let X and Y be two clusters such that head(X)
≠ head(Y). X and Y are mutually related and are denoted
as sim(X, Y) when it holds:
sim(X, Y) ⇔ rel(X, Y), rel(Y, X).
(1)
Definition 3 strictly depends on the reciprocal inclusion
of head-tags; it emphasizes that if there are two clusters
whose heads are included in one another, then they are
considered similar and are candidate to be merged.

Figure 1. Flickr clusters returned for the tag bottle

Flickr is not sensitive to the order of tags associated to an
image, but maintains Flickr clusters, which provided a
popular tag, give related tags grouped into clusters. For
example exploring Flickr clusters for the word apple2, four
different categories are returned: laptop, fruit, smartphone
and NYC. Yet, the most of the times that we call for clusters
associated to a word, the results are not very meaningful.
Figure 1 shows the Flickr clusters returned with tag bottle:
there are more than one cluster whose meaning is quite
confused and uncertain. The clusters seem partially
overlapped, including uncertain and blend information. This
is due to the fact that a word appears as a tag in the photo,
even though it is not related with the photo content.
III.

Figure 2. A Flickr photo and the relative XML-based tags.

Example 1: let us suppose that after an API call, Flickr
returns the photo shown in Figure 2 and the associated tags:
android, froyo and google. Then, for each tag, a call for
clusters is launched. The result is:
− Cl(“Android”)
=
{(“paranoid”,
“radiohead”,
”marvin”), (“google”, “htc”, “tmobile”,…), (robot,
scifi, cyborg, …), (starwars, c3po)}
− Cl(“Froyo”) = {(“frozenyogurt”, “yogurt”, “food”,
…)}
− Cl(“Google”) = {(“g1”, “android”, “phone”, …),
(“yahoo”, “flickr”, “art”), (“maps”, “street”, ... )}

FORMAL BACKGROUND

Our approach aims at discovering photos similar to a
given one, by exploiting the semantics behind the tags
associated to those photos. To do this, we consider all the
tags associated to the input photo, and, for each tag, we
consider the related Flickr clusters. Some basic formalism is
given, in order to better describe the approach. Particularly,
a cluster-based similarity measure has been defined and
then, it is expressed in term of tag-based similarity measure.
As said and shown in Figure 1, Flickr maintains all the

From Definition 2 and Definition 3 we get the cluster
(“google”, “htc”, “tmobile”,…) of Cl(“Android”) and the
cluster (“g1”, “android”, “phone”, …) of Cl(“Google”) are

1

http://www.flickr.com/search/
2
http://www.flickr.com/photos/tags/apple/clusters/

332

In (3) and (4) |.| represents the length of a word (tag);
urns the longest common
maxsstr is the function that retu
substring between two (stemmeed) tags; let us note that
syn(.,.) = 0 if maxsstr(.,.) < 3. The function edit
computes the edit (also called Leevenshtein) distance.

similar. Now, let us suppose that the necesssary implication
in (1) does not hold; that means one of the tterms on the right
is not true. That means that, considering foor simplicity two
collections, there is a head-tag of a coollection that is
included in a cluster of the other collectioon, but the viceversa is not true, i.e., the head of the latter collection is not
included in one of the clusters of the first collection. Thus,
to evaluate some similarity between these two collections,
we know having the partial relation from (1): we call cluster
pivot, the cluster that includes the tag-headd from the other
collection. In order to get the other partt of relation, we
evaluate the similarity between the cluster pivot and all the
clusters in the other collection (i.e., the ccollection whose
head is in the cluster pivot). More formally:

gs is computed as the
The similarity between two tag
syntactical similarity (4) between them,
t
discarding a factor
that represents the error (5). Equation (3) without the error
provides the same value of similarity when the tags are
identical or when a tag is own substtring (i.e., it is completely
included) of the other one. The introduction
i
of the error
balances this result.
ogurts”, “fruit”} and
Example 3: Let X= {“yo
Y={“frozenyogurt”, “ice”, “candy””} be the two clusters. X
is the cluster pivot. The matching (sstemmed) pairs are:
(“yogurt”, “frozenyogurt”) (“yogurtt”, “ice”) and (“yogurt”,
“candy”); (“fruit”, “frozenyogurt””), (“fruit”, “ice”) and
(“fruit”, “candy”).
According to Definition 5, the simiilarity at level of word is
equal to 0 for all the pairs ex
xcept for “yogurts” and
“frozenyogurt”. Let us notice by ap
pplying the stemming to
them, we get “yogurt” (and not “yogurts”) and
yogurt”|, |“frozenyogurt”|)
“frozenyogurt”. The values min(|“y
= 6, maxsstring(“yogurt”, “frozzenyogurt”) = 6 and
edit(“yogurt”, “frozenyogurt”) = 6. The similarity is as
follows:
s(“yogurt”, “frozenyogurt”) = 1 – 0.132 = 0.868.
Then from (2), the similarity sim(X, Y) = 0.434.

Definition 4: Let Cl1 and Cl2 be two collections with two
clusters X ∈ Cl1 and Y ∈ Cl2 respectiveely. Let t be a tag
such that head(X) = t and rel(Y, X); thhen Y represents
the cluster pivot.
Example 2: considering the previous eexample, let us
suppose that the cluster (“google”, “htc”, “tmobile”…) is
missing. Then, the cluster pivot is the first ccluster with head
“Google”, that is (“g1”, “android”, “phone”, …).
The similarity is evaluated between the clusster pivot and one
of the clusters in the other collection. The ggoal is to find out
the cluster more similar to the cluster pivot.
Definition 5: Given X={t1, t2, …, tn} and Y ={r1, r2, …, rm}
two clusters of tags, such that X is the ccluster pivot. The
similarity between X and Y, sim (X, Y) is:

Let us remark that the similaritty between two tags has
been evaluated considering only th
heir syntactical meaning,
with no further support of external lexicon, such as
WordNet3, in order to guarantee lan
nguage independence.
Finally, to better describe the clusteers, a ranked sequence of
meaningful tags is associated to eaach category. Let us call
category each merging of clusters (merged
(
or not) after the
computation of similarity. For each category, all its
associated tags are considered. Wee define an adapted tf-idf
measure. More formally:

n

¦ max
sim(X, Y) =

1< j < m

s (t i' , r j' )

i =1

(2)

min(| X |, | Y |)

where |.| stands for the size (cardinnality) of a set
(cluster), A={ (t i' , r j' ) ) | s(t i' , r j' ) > 0}, ii.e., the set of all
the lexical similarity greater than zero,, t i' and r j' are the
stems of the tags ti, and rj, respectivvely. The value
s(t i' , r j' ) is computed as follows:

s(ti' , r j' ) = syn(t i' , r j' ) − err (t i' , r j' )
It is 0 when

syn(t i' , r j' )

=

syn(t i' , r j' )

<

err (t i' , r j' )

Definition 6: Let C ={C1, C2, …Cn} be a category
composed of Ci merged clusterrs, with 1  i  n. Then,
given a cluster Ci let {t1, t2, …tk} be the whole collection
of tags associated to the photos included in Ci. The tf-idf
measure is computed as follows:

(3)

. Speecifically:

1
(min(| t i' |, | r j' |) − | max sstr (t i' , r j' ) | +1) 2

tfidf (t k , C i ) = tf (t k , C i ) log

(4)

where tf (t k , Ci ) is the frequency of the tag tk in Ci.

edit (t i' , r j' )

err (t i' , r j' ) =

e

|ti' |+|r j' |

3

|C |
| {C i : t k ∈ C i } |

−1

(5)

3 http://wordnetweb.princeton.edu/perl/webwn

333

(6)

are mutually related (see Definitions 2-3) and they can be
merged. By clicking on the bar of this cluster, the first
similar images appear (we consider only a small subset of
photos in order to make acceptable the loading time).
Clicking on a head of remaining categories, the relative
clusters are in turn organized for semantic meaning
(exploiting the Flickr semantic grouping). Let us notice that
in Example 1, there are four clusters with head “Android”,
while in Figure 3 there are three ones: one disappears.
Indeed, the cluster (“google”, “htc”, “tmobile”,…) as well
as the cluster (“g1”, “android”, “phone”, …) with head-tag
“google” compounds the new merged cluster, whose
semantic content deals with cellular phones. Moreover,
clicking on the category bar, the main tags representing the
semantic category are shown in a sort of tag-cloud
representation.
By the interface (Figure 4), the similarity as well as the
merging computation can be modified by some “Advanced
Settings”. Selecting a threshold similarity, a filtering on
clusters merge is applied.

Figure 4. Application interface: advanced setting

As a further example, consider the setting shown in
Figure 4 where the threshold similarity is fixed to 75% and
the number of clusters to merge is until to 8. Notice the
checkbox with label “Fully Similarity” is set too: it allow
the retrieval of all the mutually related clusters (otherwise
they are not returned). After the processing, the input image
(given by its URL) and the computed categories are shown
in . Note that the first category is computed as a merge of
three clusters with similar meaning; the name of this
category is describe by the combination of the heads and
relative tags, respectively for each involved cluster: PARIS:
eiffel, tower – EIFFEL: paris, france – TOUR: paris, tower.
Moreover, Figure 5 outlines a “smart view”: it provides a
synthetic description of the semantic categories computed
for the photo-query. It is evident that two main categories
are remarked: one describing the photos inherent Paris and
the Eiffel tower, the other one is relative to pictures taken a
known camera. This is due to some tags (nikon, d90)
associated to the photo.

Figure 3. Categories returned for the input photo (shown at the top). The
first category is explanded and the relative tag-cloud appears.

IV.

THE APPLICATION AT WORK

The application exploits the Flickr API4, to get the tags
associated to a photo and the list of clusters for each tag
(that has been called head). It takes as an input the photo
URL. The interface of our application is inspired to the
starting Google page: the URL can be inserted in a central
textbox and then, clicking on Search button, the application
starts processing. The application returns as output the
photo associated to the input URL and the computed
semantic categories (see Section III). For example, let us
input the URL of the photo in Figure 2; The application
returns photos and categories as listed in Figure 3.
Specifically, there are four categories, described by the
following heads:
1) Android:g1, google – Google: g, android; 2) Android, 3)
Froyo, 4) Google. The first one is from merging two
clusters: one with head “Android” and the other one with
head “Google”, according to Example 1. The two clusters
4

V.

EXPERIMENTAL RESULTS

The performance of our approach has been assessed through
traditional Information Retrieval measurements. Yet, no
direct comparisons with Flickr have been carried out,

http://www.flickr.com/services/api/

334

because Flickr returns “plain” search ressults that strictly
reflect the input sequence of keywords. Theen, Flickr clusters
are based on single head-word: no ccombinations of
keywords is admissible to directly get merged clusters.
The experimentation consists of submittinng a Flickr photo
(with the relative tags) to the application aand evaluates the
results, through the precision, by varyinng the similarity
threshold (supposed checked the “Full similarity” option in
the Advanced Setting of the interface). T able I shows the
results of five query-photos.

For each photo identified by a lab
bel and URL, the retuned
semantic categories are listed, by
y varying the similarity
threshold. Each category is composed of at least two merged
clusters, whose heads are shown in uppercase.
m
of the precision for
Each point is calculated by the mean
each category, to obtain a unique value of precision for a
t
given threshold. Note in Table I that
photos labeled Tour
Eiffel1 and Froyo have the same semantic categories, by
varying the threshold. This is du
ue to the fact that the
merging occurs among very similaar clusters (i.e, with high
similarity value). In particular, thee photo labeled Froyo is
the same of Figure 3 and its simillarity value is computed
exploiting the full similarity. For th
his reason, the merging is
invariant with respect to the thresho
old.
The photo labeled Skate evideences a decrease in the
precision as soon as the threshold
d value becomes greater
than 0.5, then it stabilizes (see Figu
ure 6). This could be due
to the fact that many similar clusteers introduce some noisy
(including wrong tags) with respect to less similar clusters.
d value, only the very
Thus, increasing the threshold
similar clusters participate to the merging,
m
by evidencing a
possible precision reduction. At th
he same time, increasing
the threshold, the variance of categ
gories precision tends to
decrease. Moreover, varying the thresholds, the clusters
participating to the merging are different; just an example:
semantic categories for photo Tourr Eiffel2 are different, by
setting the threshold to 0.25, 0.5 an
nd 0.75.
TABLE I.

SEMANTIC CATEGORIES CO
OMPUTED FOR
VARYING THE SIMILARITY THRESHOLD

5 PHOTOS BY

Apple:
http://www.flickr.com/photos/alek
ktoeumenides/30934960
Threshold
Semantic Categories
FOOD: dessert, choco
olate - PIE: food, dessert
0.01
GREEN: flower, flowers - FO
OOD: fruit, red - APPLES: fruit,
red - COLOUR: macro, green
GREEN: nature, red - APPLES: fruit, red
FOOD: dessert, choco
olate - PIE: food, dessert
0.25
GREEN: flower, flowers - FO
OOD: fruit, red - APPLES: fruit,
red - COLOUR: macro, green
GREEN: nature, red - APPLES: fruit, red
FOOD: dessert, choco
olate - PIE: food, dessert
0.5
GREEN: nature, red - APPLES: fruit, red - FOOD: fruit, red
GREEN: flower, flowers
s - COLOUR: macro, green
FOOD: dessert, choco
olate - PIE: food, dessert
0.75
FOOD: dessert, choco
olate - PIE: food, dessert
0.90

Figure 5. Smart view: a hierarchical structure evideences the different
semantics associated to the retuned phhotos.

Tour Eiffel 1:
http://www.flickr.com/photos/psr0
0071/481571183
Threshold
Semantic Categories
TOUR: paris, tower - PARIS:: eiffel, tower - FRANCE: tower,
0.01
night - EIFFEL: paris, france - FRANCE: paris, louvre PARIS: frrance, seine
TOUR: paris, tower - PARIS:: eiffel, tower - FRANCE: tower,
0.25
night - EIFFEL: paris, france - FRANCE: paris, louvre PARIS: frrance, seine
TOUR: paris, tower - PARIS:: eiffel, tower - FRANCE: tower,
0.5
night - EIFFEL: paris, france - FRANCE: paris, louvre PARIS: frrance, seine
TOUR: paris, tower - PARIS:: eiffel, tower - FRANCE: tower,
0.75
night - EIFFEL: paris, france - FRANCE: paris, louvre PARIS: frrance, seine
TOUR: paris, tower - PARIS:: eiffel, tower - FRANCE: tower,
0.90
night - EIFFEL: paris, france - FRANCE: paris, louvre PARIS: frrance, seine

Figure 6. Precision curve of the query-pphotos

335

Tour Eiffel 2:
http://www.flickr.com/photos/ayingha/6569791883/in/ph
otostream
Threshold
Semantic Categories
PARIS: eiffel, tower - EIFFEL: paris, france - TOUR: paris,
0.01
tower - NIKON: portrait, sky - D90: nikon, bw
NIKON: portrait, sky - PARIS: france, seine
PARIS: eiffel, tower - EIFFEL: paris, france - TOUR: paris,
0.25
tower - NIKON: portrait, sky - D90: nikon, bw
NIKON: portrait, sky - PARIS: france, seine
PARIS: eiffel, tower - EIFFEL: paris, france - TOUR: paris,
0.5
tower
NIKON: portrait, sky - PARIS: france, seine - D90: nikon, bw
PARIS: eiffel, tower - EIFFEL: paris, france - TOUR: paris,
0.75
tower
NIKON: portrait, sky - D90: nikon, bw
PARIS: eiffel, tower - EIFFEL: paris, france - TOUR: paris,
0.90
tower
NIKON: portrait, sky - D90: nikon, bw

EMILIA: italy, italia - ROMAGNA: italy, italia
Froyo:
http://www.flickr.com/photos/niallkennedy/4627466723/
Threshold
Semantic Categories
ANDROID: g1, google - GOOGLE: g1, android
0.01
ANDROID: g1, google - GOOGLE: g1, android
0.25
ANDROID: g1, google - GOOGLE: g1, android
0.51
ANDROID: g1, google - GOOGLE: g1, android
0.75
ANDROID: g1, google - GOOGLE: g1, android
0.90

VI.

CONCLUSIONS

Tags given to a Flickr photo are often not much
descriptive of the visual content of the photo, limiting the
effectiveness of the tags in the retrieval. Discovering photos
that are relevant to a given query and, at the same time, are
well described by relevant associated content could support
the users in search activity.
Our application aims at reaching this issue: it supports
users in the photo retrieval by proposing an efficacy
semantic classification of the images retrieved by Flickr
search service and elicits the most relevant and meaningful
tags, associated to these images. Experimental results
evidence the effectiveness of our application, which
exploited as a front-end of Flickr, represents an additional
semantic layer targeted to “interpret” Flickr plain results.

Skate:
http://www.flickr.com/photos/blind_beholder/485386553/
Threshold
Semantic Categories
TWILIGHT: sunset, dusk - DUSK: sky, evening - SUNSET:
0.01
clouds, ocean
DUSK: sunset, silhouette - SUNSET: clouds, ocean
SKATEPARK: skate, skateboard - SKATE: skateboard,
skateboarding - SKATEBOARD: skate, skateboarding SKATEBOARDING: skate, skateboard - SKATER: skate,
skateboard - SKATERS: skate, skateboard - SUNSET: sun,
beach
EMILIA: italy, italia - ROMAGNA: italy, italia - CESENA:
romagna, italy - TRAMONTO: sunset, mare - SUNSET: sun,
beach
SUNSET: sun, beach - TRAMONTO: sunset, mare JURASSIC: coast, dorset - CESENA: tramonto, sunset
TWILIGHT: sunset, dusk - DUSK: sky, evening - SUNSET:
0.25
clouds, ocean
DUSK: sunset, silhouette - SUNSET: clouds, ocean
SKATEPARK: skate, skateboard - SKATE: skateboard,
skateboarding - SKATEBOARD: skate, skateboarding SKATEBOARDING: skate, skateboard - SKATER: skate,
skateboard - SKATERS: skate, skateboard - SUNSET: sun,
beach
EMILIA: italy, italia - ROMAGNA: italy, italia - CESENA:
romagna, italy - TRAMONTO: sunset, mare - SUNSET: sun,
beach
SUNSET: sun, beach - TRAMONTO: sunset, mare JURASSIC: coast, dorset - CESENA: tramonto, sunset
TWILIGHT: sunset, dusk - DUSK: sky, evening - SUNSET:
0.5
clouds, ocean
DUSK: sunset, silhouette - SUNSET: clouds, ocean
SKATEPARK: skate, skateboard - SKATE: skateboard,
skateboarding - SKATEBOARD: skate, skateboarding SKATEBOARDING: skate, skateboard - SKATER: skate,
skateboard - SKATERS: skate, skateboard
EMILIA: italy, italia - ROMAGNA: italy, italia - CESENA:
romagna, italy - SUNSET: sun, beach
SUNSET: sun, beach - TRAMONTO: sunset, mare CESENA: tramonto, sunset
TWILIGHT: sunset, dusk - DUSK: sky, evening - SUNSET:
0.75
clouds, ocean
DUSK: sunset, silhouette - SUNSET: clouds, ocean
SKATEPARK: skate, skateboard - SKATE: skateboard,
skateboarding - SKATEBOARD: skate, skateboarding SKATEBOARDING: skate, skateboard - SKATER: skate,
skateboard - SKATERS: skate, skateboard
EMILIA: italy, italia - ROMAGNA: italy, italia
SUNSET: sun, beach - TRAMONTO: sunset, mare CESENA: tramonto, sunset
TWILIGHT: sunset, dusk - DUSK: sky, evening - SUNSET:
0.90
clouds, ocean
DUSK: sunset, silhouette - SUNSET: clouds, ocean
SKATEPARK: skate, skateboard - SKATE: skateboard,
skateboarding - SKATEBOARD: skate, skateboarding SKATEBOARDING: skate, skateboard - SKATER: skate,
skateboard - SKATERS: skate, skateboard

REFERENCES
[1]

[2]

[3]

[4]

[5]

[6]

[7]

[8]

336

A. W. M. Smeulders, M. Worring, S. Santini, A. Gupta, and
R. Jain., “Content-based image retrieval at the end of the early
years”. IEEE Trans. Pattern Anal. Mach. Intell., 22(12):1349–
1380, 2000.
L. S. Kennedy, S. F. Chang, and I. V. Kozintsev. “To Search
or To Label? Predicting the Performance of Search-Based
Automatic Image Classifiers,” In Proceedings of ACM MIR
2006.
D. Liu, X.-S. Hua, L. Yang, M. Wang, and H.-J. Zhang.. “Tag
ranking”. In Proceedings of the 18th international conference
on WWW 2009. ACM, New York, NY, USA, 351-360.
M. Ames and M. Naaman. Why We Tag: Motivations for
Annotation in Mobile and Online Media. In Proc. of the
SIGCHI Conference on Human Factors in Computing
System, 2007.
B. Sigurbjornsson and R. V. Zwol. Flickr Tag
Recommendation based on Collective Knowledge. In
Proceeding of ACM International World Wide Web
Conference, 2008
L. S. Kennedy, S. F. Chang, and I. V. Kozintsev. To Search or
To Label? Predicting the Performance of Search-Based
Automatic Image Classi¿ers. In Proceedings of the 8th ACM
International Workshop on Multimedia Information Retrieval,
2006.
S. Sen, S. K. Lam, A. M. Rashid, D. Cosley, D. Frankowski,
J. Osterhouse, F. M. Harper, and J. Riedl. Tagging,
communities, vocabulary, evolution. In CSCW 2006.
Y. Yang, Z. Huang, H. T. Shen, X. Zhou, Mining multi-tag
association for image tagging, World Wide Web, v. 14 (2),
2011, Springer.

