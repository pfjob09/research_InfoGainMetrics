Evaluating a Geovisualization Prototype with Two Approaches:
Remote Instructional vs. Face-to-Face Exploratory
Stephanie Larissa Marsh, Jason Dykes, Fenia Attilakou
giCentre, Department of Information Science, City University, London, EC1V 0HB
{s.l.marsh@soi.city.ac.uk, jad7@soi.city.ac.uk, fenia.attilakou@legalservices.gov.uk}
Abstract
Two evaluations of a prototype designed to help expert
users visualize key census statistics are conducted. The
results yielded are compared in terms of usability issues,
task completion (interaction) and ideation facilitated.
Ways in which this information may be affected by the
use of different data collection techniques, participants
and tasks are considered. We report differences in the
results of the evaluations in each of the three areas and
suggest that flexible and non-disruptive methods be used
to investigate whether geovisualization tools can support
knowledge construction. We recommend using
exploratory tasks, employing 'think aloud' strategies,
requiring users to suggest and explain hypotheses and
using screen capture to contextualise the data collected.
Keywords geovisualization, usability, think aloud,
exploration, ideation, interaction.

1. Introduction
Evaluating geovisualization applications with
techniques from usability engineering has become
increasingly important in the last few years [1]. Here we
review two different usability evaluations of a
geovisualization prototype. The tool provides access to
key statistics from the 2001 census for the county of
Leicestershire, UK as part of an ongoing effort to use
geovisualization techniques to deliver evidence-based
policy.
Established paradigms for conducting cognitive and
usability research do not map well to highly interactive
visual
environments,
particularly
when
these
environments are designed to support ill structured tasks
such as knowledge construction or decision making [1].
Thus one purpose of this investigation is to establish
whether different usability tests on the same software
yield different results relating to usability concerns,
strategies for task completion (interaction) and any
ideation facilitated. We also consider the suitability of
particular combinations of data collection techniques for
evaluating geovisualization applications. This study was

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

conducted as part of wider research on the effective use
of HCI techniques in geovisualization.
The first evaluation (E1-RI) was a remote usability
test with 8 participants from Leicestershire County
Council (LCC) Research Information Group using
instructional tasks. It was undertaken to investigate the
responses of practitioners within LCC to dynamic maps
designed to support visualization. The results of this
remote instructional evaluation prompted questions
relating to the suitability of usability evaluation
techniques in geovisualization [2]. Consequently, a
second face-to-face evaluation was conducted (E2-FE)
with 9 participants from the giCentre at City University
using exploratory tasks. Results on usability, interaction
and ideation gained from each were compared in terms
of the number of usability issues yielded, task
completion data and richness of ideation data. Each task
in E2-FE used a different combination of data collection
techniques including note-taking and think aloud to
capture usability problems, interaction and ideation
facilitated. The effectiveness of the different
combinations of data collection techniques were
observed and recorded throughout E2-FE, and
participants were quizzed as to their preferences of how
data were collected whilst they used the prototype.
Suitable means of gathering data during geovisualization
usability evaluations are suggested as a result.

2. Geovisualization prototype
The prototype was created in consultation with LCC
to provide an example exploratory interface to ward level
data from the 2001 census. It uses scalable vector
graphics (SVG) and JavaScript to provide linked
interactive choropleth maps, population cartograms [3]
and parallel coordinates plots (PCP) [4] that graphically
depict numeric values for 40 variables for the 133 wards
of Leicestershire. The prototype contains two map
interfaces, each including a PCP of all attributes / wards
and a non-continuous population cartogram. One
interface has a single choropleth map in which data can
be mapped through grey scale or colour composite
representations (Figure 1). The second interface maps the
data through 4 shaded small multiples (Figure 2). The

components of each interface are dynamically linked
through brushing, allowing multiple perspectives of the
multivariate census data. LCC’s successful use of the
‘cdv’ software [5,6] with 1991 census data provided
impetus for the geovisualization prototype.

were asked to document their objectives, the queries they
performed, problems encountered and any positive
aspects of the software by making comprehensive notes.
Pre and post evaluation questionnaires were also utilised.
Task
1

Map
1 ward

Attributes
1

2

several
local
wards
whole
map

1

4

1 ward

4

5

1 ward

all
attributes

3

Figure 1 - single map view with PCP and
cartogram (data Crown Copyright 2003 [7])

1

Notes
read data
about a
place
interpret
data about
an area
assimilate
data about
a region
interpret
data about
a place
interpret
data about
a place

Function
single map
view
single map
view
single map
view
small
multiples
view
parallel
coordinates
plot

Table 2 - Task format of 'E1-RI'

2.2. Face-to-face exploratory usability evaluation
Eight members of the giCentre at City University
and one representative of LCC participated in E2-FE,
which was conducted face-to-face at the University in
London. All nine users completed the tasks listed in
Table 2. Six of these individuals were familiar with the
study area and seven had familiarity with census data.

Figure 2 - small multiples view with PCP and
cartogram (data Crown Copyright 2003 [7])

2. Methods
The evaluations shared an introductory tutorial, but
differed in terms of the tasks set and length of time used.
Data collection methods differed due to the remote /
face-to-face nature of the evaluations but common data
analysis methods were employed.

2.1. Remote instructional usability evaluation
E1-RI was conducted at LCC without the presence
of the evaluator or software designer [2]. The exercise
was designed to investigate the degree to which the
geovisualization prototype allowed expert users to
explore the census data. Participants spent 10 minutes
freely exploring the prototype, then five instructional
tasks were then completed - the functionality that should
be used being clearly specified in each case. Table 1
shows how these five tasks were structured. Participants

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

Task
1
2

Map
2 wards

3

1 ward

4

4 wards

5

4 wards

6

2 wards

Attributes
free exploration
population

relating to economic
activity, health &
qualifications
1 attribute

relating to economic
activity, health &
qualifications
religious composition
& health

Notes
geographic
distance
similar
populations
compare and
describe
describe
variation
across wards
compare and
describe
geographic
proximity
hypothesis

Table 2 - Task format of 'E2-FE'
Each task in E2-FE was dataset directed and openended. No time limit was placed on the participants and
no instructional detail was provided, requiring certain
choices to be made in terms of the attributes that would

be considered and the graphical tools to be utilized to
carry out the task.

2.3. Data collection techniques
In E1-RI data were captured through continual notetaking on the part of the participants and their answers to
the tasks. Pre and post evaluation questionnaires were
completed to record information such as the features that
participants considered to be the best and worst aspects
of the prototype and anticipated use of the prototype.
E2-FE used a different combination of data
collection techniques for each task the users completed
to record data on the three aspects of the use of the
visualization software under consideration. The various
ways of capturing data from participants could thus be
informally compared in the context of exploratory task
completion over an extended period of time. Our
observations suggest that exploratory tasks place a higher
cognitive load on the user compared with instructional
tasks and so evaluations that employ them may require
different data collection techniques to those that do not.
The combination of techniques used is shown in Table 3:
Task
1
2
3

4
5

6

Techniques
Think aloud - interaction / usability
Notes – ideation
Think aloud - interaction / usability / ideation
Think aloud - ideation and usability
Notes - usability report form
Onscreen recorder – interaction
Notes - ideation / interaction / usability
Notes - ideation / usability
(usability report form)
Onscreen recorder – interaction
Notes - major ideation
Think aloud - interaction / usability / ideation
Onscreen recorder – interaction

Table 3 - 'E2-FE' data collection techniques

2.4. Data analysis techniques
Content analysis [8, 9] was used to analyze and
interpret the qualitative data collected in the two
evaluations – the participants’ notes and the transcribed
audio data. Data were categorized and links established
between and within categories. Four general analytical
schema were used. Organic schema were then developed
to analyze further. Table 4 shows the coding schema
used in E2-FE.
Any ideation recorded was analyzed for observation
and hypothesis complexity using an established schema
shown in Table 5 [10]. This schema was used on one
task in E1-RI and all tasks in E2-FE. This analysis took
place to investigate whether the prototype could support
hypothesis generation / ideation.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

Coding Schema
Sub-Schema
Positives
Negatives
Suggestions/Improvements
Ideation / learning
Prototype ideation
Data ideation
Interaction and
Positives
exploration
Negatives
Other
Prototype issues
Data collection technique issues
Data issues
General Schema
Usability

Table 4 - Content analysis

Hypothesis / Observation Coding Schema
Schema
Sub-Schema
Hypothesis / Observation
Domain Knowledge
Basis
Visual information from
display
Both domain knowledge
and visual information
Hypothesis / Observation
Low
Complexity
Medium
High
Hypothesis / Observation
Identify
Goal
Characterize
Generalize
Predict
Explain
Hypothesis / Observation
None (existence of a
Relationship
feature)
Pattern
Association
Interaction
Cause
Table 5 - Types of observation / hypothesis

3. Results
Usability - E1-RI yielded 29 usability concerns.
Only eight of these were highlighted by more than one
participant (28%). An example is a repeated request for
larger parallel coordinate plots. E2-FE yielded 62
usability concerns, though 38 (61%) of these issues were
identified by only one participant. An example is a single
indication that the small multiples were too small. E2-FE
identified 47 unique problems (62%) compared with 13
unique problems (17%) in E1-RI. The two evaluations
shared 15 usability concerns - 20% of the total. The
difficulty of finding particular named wards was
considered a problem in both evaluations, though it was
thought to be more severe in E2-FE. This could be due
to the nature of the tasks given in E2-FE, but may also
be influenced by the participants knowledge of the area.

Differences in usability problems identified are due to a
combination of the differences in the user groups and the
structures of the evaluations.
The evaluations used similar numbers of participants
and so the differences in number of usability problems
identified cannot be attributed to this. The data collection
techniques used may explain and reveal possible
shortcomings of using note-taking - the relative lack of
detail resulting from the time taken to make
comprehensive and meaningful notes whilst participating
in visualization may have meant that some usability
problems were not captured during E1-RI, where notetaking was the predominant data collection technique. In
addition, the majority of participants in E1-RI had high
levels of map-use experience though they were
unfamiliar with statistical graphics (parallel coordinate
plots). All participants in E1-RI had prior knowledge of
the study area. Participants in E2-FE however were all
familiar with maps and statistical graphics, and six of the
nine had prior knowledge of the study area. Previous
studies [11] have found that less than 50% of usability
problems were detected with 3 participants, whilst 80%
have been detected with 4 and 5 participants, and 90%
with 10 participants [12]. This study suggests that these
heuristic evaluation findings do not always hold true in
geovisualization usability evaluations, perhaps due to the
exploratory and open ended nature of the tasks being
supported by the software.
Interaction - In E1-RI little data was gained on task
completion strategies due to the remote nature of the
evaluation and the framework used. But, as it consisted
mostly of instructional tasks, completion strategies
would be relatively similar. For E2-FE data was
recorded on the diversity of interaction and strategies
used to complete tasks. Between 4 and 6 different
strategies (using different combinations of functionality)
were employed by the 9 participants for each of the
tasks, and often participants would undertake work that
was beyond that asked of them. The differences in types
of interaction recorded are largely due to the contrasting
natures of the two evaluations. It is hypothesized that
differences in task completion strategies will also
influence the number of usability problems uncovered,
with more problems being identified with increasingly
diverse interaction (i.e. using a variety of techniques).
Behaviour consistent with this hypothesis was observed
during this study and has also been reported in other
usability evaluations [13]. E2-FE highlights that there
are many possible combinations of interaction that can
lead to task completion. Excluding specific instructions
in the evaluation allows for greater diversity in
interaction. People are likely to choose functionality they
are more familiar with to undertake the task, or select a
different combination of tools, depending on what they
think the task requires, and depending upon their existing
mental schemata. The behaviour observed in E2-FE
indicates that the sequence of steps taken to complete an
open-ended task is not consistent and is likely to depend
of prior knowledge and experience - drawing attention to
the complexities associated with using usability for

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

exploratory geovisualization and emphasizing the need
to capture data about task completion strategies.
Ideation - The kinds of observation and hypotheses
formulated by the participants were recorded along with
task answers in both evaluations. However, the types of
observations made, and the degree of success with which
tasks were accomplished was recorded with less
precision in E1-RI due to the nature of data collection
(remote and relying upon note-taking). Nevertheless,
sufficient detail has been gained to show variation
between the two user groups and the two evaluations.
Tasks in E1-RI with predefined answers all resulted in
correct responses. Where questions from the two
evaluations had similar scope for recording ideation this
was compared using the schema shown in Table 5. This
revealed that ideation in E2-FE was more sophisticated
than that recorded in E1-RI, which typically consisted of
a feature being identified in isolation or the identification
of a simple pattern. Observations were required of E1-RI
rather than hypothesis generation and no participants
ventured to formulate hypotheses or suggest causes for
the patterns they were identifying, make predictions or
offer explanations. Using the schema to analyse and
compare the ideation data taken from E2-FE does not
indicate a direct relationship between particular
interaction strategies and ideation. A combination of
visual attention, interaction and prior knowledge seems
to contribute to the type, quality and quantity of ideation
recorded. A tendency for increased complexity of
ideation from tasks 1 through to 6 was noted. A common
occurrence observed in E2-FE was for 'think aloud' to
capture detail in which participants refined or completely
changed hypotheses as they interacted with the data,
without specifically verbally acknowledging that this had
occurred. Hypotheses were frequently developed through
a partially verbalised stream of consciousness that
focused on ideas rather than process.
Data collection - reviewing the techniques used
revealed that during E2-FE, think aloud was the richest
source of data - providing the most information on
usability, interaction, ideation and, reasons for
interaction and ideation. Onscreen recording is useful for
precise information on what is being done, but this is
needed together with think aloud to provide context.
Participants in E2-FE were reluctant to take
comprehensive notes for the evaluation, as it was time
consuming and was observed to disrupt the visualization
process. Brief note-taking however was regarded by
participants as useful for providing a summary / log of
their activities. One participant stated that:
“I preferred the thinking aloud. It’s quite
informative to have to describe what you are
doing, and makes you aware of the previous
knowledge and assumptions you bring to any new
situation. I think the screen capture is a good idea
since it is difficult to describe exactly what you
are doing .... I find myself going down blind
alleys, then taking different approaches, which
may make it hard to follow the overall flow.

Nevertheless, it seems much closer to how
software gets used in reality, which is not always
in a logical, sequential way.”
This supports the rationale behind the tasks
formulated for E2-FE - going down blind alleys and
taking different approaches to complete a task should be
supported by geovisualization software as this is an
important part of data exploration. The tasks used in E2FE did not include instructions for completion and
participants frequently went beyond the specific task to
explore further - indicating that interaction generates
interest in data and the prototype software allowed users
to generate ideas and follow their trains of thought. In
this context of exploratory software a combination of
'think aloud' techniques and screen capture was deemed
most suitable.
The amount of information gained about the process
of visualization through comprehensive note-taking
varied greatly between participants. Quantity and quality
also vary in think aloud but are less significant as the
information gained is richer in than that obtained from
notes. Comprehensive and systematic note-taking
requires considerable effort and can interrupt the task
flow, whereas think aloud and interaction with the
software occur simultaneously. The sophisticated nature
of geovisualization – whereby graphics and software are
used to reduce cognitive load when large amounts of
complex structured data are being analysed - suggest that
lengthy interruptions to task flow may be particularly
significant in exploratory visualization. The levels of
engagement with the software and speed of thought
involved in visualization negate against comprehensive
and systematic documentation of activities by the user.
Think aloud seemed to hinder users less and has been
shown to actively improve task performance during some
usability evaluations [14]. We are unaware of any such
findings in relation to note-taking. Note-taking and
diaries tend to be used in studies that take place over
several months and are conducted in the work
environment, recording everyday interaction, rather than
during initial exploration of data during a ‘lab based
evaluation’. E1-RI revealed that brief note-taking was
useful to gain information on task answers and usability
problems but little or no information was gathered for the
evaluator on interaction or ideation. In contrast,
observation indicated that occasional note-taking may
help users deal with issues of cognitive (over)load in
geovisualization as the physical artefacts produced could
be used to ‘ground’ the exploratory process - interactions
between these effects require further analysis.
Whilst think aloud was the richest data source
available from E2-FE, it also has its limitations and may
well alter the learning / hypothesis generation processes
[15,16,17]. The use of note-taking for anything but brief
notes for the user to ‘ground’ the exploratory process is
likely to be disruptive, altering / interrupting the process,
with little benefit to the evaluator. Taking brief notes
may however help the user as they navigate through the
process of geovisualization.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

5. Conclusions and discussion
Conducting different usability evaluations with two
user groups on the same software has yielded different
results in three areas: usability - problems identified;
interaction - task completion strategies; ideation - ideas
generated.
It is unsurprising that different results were gained
for task completion as E1-RI used directed interaction
(instructional tasks), and E2-FE used free interaction
(exploratory tasks). The data collection methods used in
E1-RI were also less conducive to recording interaction
than those employed in E2-FE. More significantly,
interaction / task completion strategies seemed to affect
the number of usability concerns identified in this
geovisualization application. In this particular study, the
differences between problems identified in each
evaluation due to interaction cannot be isolated from the
differences due to the variations in the evaluations
themselves (including the user group variation).
However, other studies suggest that some of these
differences can be attributed to variation in levels of
interaction, with more usability problems being
identified when interaction is more diverse - as is likely
to be the case in situations such as this where exploratory
software supports visualization [13]. In this case only 15
out of 75 usability issues were identified in both
evaluations. This could demonstrate the importance of
having different user groups evaluate interactive systems
designed for experts, as so many unique problems were
found. But, these problems were often not only unique to
an evaluation, but also to a single participant. This
suggests that reports of 80% of usability problems being
detected with 5 participants, and 90% with 10 users
[11,12], do not always hold true in the case of
exploratory tools for geovisualization.
Though it can be considered advantageous to have
found many wide-ranging usability issues, it also poses
greater difficulty in identifying which problems are most
crucial and persistent as tools are developed. Certain
issues highlighted by the participants may be eliminated
due to the circumstances under which they were found underlining the need for contextual data such as that
obtained using think aloud techniques and through screen
capture in this case.
Ideation was supported by the prototype in each of
our experiments, with more convincing evidence
presented by E2-FE than E1-RI where users participated
in exploration beyond that required to complete the tasks.
The level of ideation achieved can be considered to be
partly influenced by the evaluation format: the
exploratory nature of the tasks posed and the fact that
E1-RI did not require participants to offer hypotheses for
patterns they found. The suggestion is that certain
evaluation strategies may be more likely to result in
ideation in ‘artificial’ tests such as these than others.
Whilst recorded variations in ideation can only be
acknowledged as being due to a combination of the
different user groups and the different evaluation
methods used, in our experience exploratory tasks appear

to facilitate higher levels of ideation than instructional
tasks and so should be applied in geovisualization
evaluations. The results of the open ended tasks
employed in E2-FE show that when exploratory
software is used to support ideation very different forms
of interaction (different 'results') may take place both
within evaluations and within groups - all of which may
involve task completion. The higher numbers of usability
problems identified by E2-FE also suggest that
longitudinal studies and evaluations that more closely
reflect ‘real world use’ of a system are desirable in
geovisualization.
The findings presented here highlight the need for
flexible tools for exploring geospatial data that allow
users to draw upon prior experiences and existing mental
schemas when taking part in geovisualization. They
support the need for the development and use of specific
usability frameworks that employ non-disruptive
methods for evaluating geovisualization applications in
realistic scenarios - where well-informed users have an
investment in the software and the data. We
recommended using exploratory tasks when the purpose
of an evaluation is to investigate whether the tool can
support knowledge construction / hypothesis generation.
In our experiments usability data is also most detailed
during think aloud, but analysis of onscreen interaction is
essential adding context that was not reported by the
participant. We recommend using 'think aloud' methods
an integral part of the data collection methodology if
analysis of interaction strategies and ideation support are
a focus of an evaluation - inferring a need for face-toface data collection (or at least the recording of verbal
reactions). Screen capture should be used to
contextualise the information collected - onscreen
recording is most useful when combined with think
aloud for context, capturing what the participants are
doing and why. Comprehensive note-taking to capture
the process is deemed less appropriate. When exploring
data the cognitive load is already high - and this is
particularly the case when exploring new software.
Whilst occasional / brief note-taking may help ‘ground’
the exploration, adding to this load by asking the
participants to take comprehensive and systematic notes
on their progress was found to be disruptive to the
visualization process, affecting how a task is completed
and the knowledge gained from the task. It will thus also
be detrimental to the process of identifying usability
problems and may thwart efforts to identify strategies for
task completion.

Acknowledgements
Robert Radburn of the Research Information Group at
Leicestershire County Council; participants from LCC
and the giCentre at City University London; David Lloyd.
Census Data Source: 2001 Census Area Statistics; 2001
Census, Output Area Boundaries. Crown copyright 2003.
Crown copyright material is reproduced with the permission of
the Controller of HMSO.

Proceedings of the Information Visualization (IV’06)
0-7695-2602-0/06 $20.00 © 2006

IEEE

References
[1] MacEachren, A. M. and Kraak, M.-J. Research
Challenges in Geovisualization. Cartography and
Geographic Information Science, 28(1). 1-11. 2001.
[2] Attilakou, F. SVG Maps for Visualization of
Geographic Information: Development and Evaluation of
a Prototype. Information Science, City University. MSc
Dissertation. 2005.
[3] Dorling, D. Cartograms for Visualizing Human
Geography. In Visualization in GIS. (Eds, Hearnshaw, H.
M. and Medyckyj-Scott, D.). 1994.
[4] Inselberg, A. The Plane with Parallel Coordinates.
Visual Computer, 1:69-97. 1985.
[5] Dykes, J. A. Cartographic Visualization for Spatial
Analysis, Proceedings International Cartographic
Conference, ICA Barcelona, pp. 1365-1370. 1995.
[6] Dykes, J. A. Cartographic Visualization: Exploratory
Spatial Data Analysis with Local Indicators of Spatial
Association using Tcl/Tk and cdv, The Statistician,
47(3), pp. 485-497. 1998.
[7] Original Data Depositor, 2001 Census : Digitised
Boundary Data (England and Wales) [computer file].
ESRC/JISC Census Programme, Census Geography Data
Unit (UKBORDERS), EDINA (University of
Edinburgh)/Census Dissemination Unit, MIMAS
(University of Manchester)
[8] Patton, M. Q. Qualitative Evaluation and Research
Methods, Sage, London. 1990.
[9] Silverman, D. Interpreting Qualitative Data:
Methods for Analysing Talk, Text and interaction, Sage,
London. 2001.
[10] Griffin, A. L. Understanding How Scientists Use
Data Display Devices for Interactive Visual Computing
with Geographical Models. Penn State University. PhD.
2004.
[11] Nielsen, J. and Molich, R. Heuristic Evaluation of
User Interfaces. In Conference on Human Factors in
Computing Systems Seattle, pp. 249-256. 1990.
[12] Virzi, R. Refining the Test Phase of Usability
Evaluation: How Many Subjects Is Enough? Human
Factors, 34(4). 457-468. 1992.
[13] Marsh, S. L. and Dykes, J. A. Using Usability
Techniques to Evaluate Geovisualization In Learning
and Teaching. In "Internet-Based Cartographic Teaching
and Learning: Atlases, Map Use, and Visual Analytics"
(Eds, Zentai, L., Nunez, J. J. R. and Fraser, D.). ICA,
Madrid, pp. 29-34. 2005.
[14]Dumas, J. S. and Redish, J. C. A Practical Guide to
Usability Testing, intellect. 1999.
[15]Ericsson, K.A. & Simon, H.A. Protocol Analysis:
Verbal Reports as Data. Revised edition. Cambridge,
MA: The MIT Press. 1993.
[16] Cotton, D. and Gresty, K. Reflecting on The ThinkAloud Methods for Evaluating e-Learning. British
Journal of Educational Technology, 37(1). 45-54. 2006.
[17] Calderhead, J. Stimulated Recall: A Method for
Research on Teaching. British Journal of Educational
Psychology, 51211-217. 1981.

