From Visual Semantic Parameterization to Graphic Visualization
Xin, Zeng, Q. H. Mehdi and N. E. Gough
Games Simulation and AI (GSAI) Centre,
Research Institute of Advanced Technologies (RIATec)
University of Wolverhampton, Wolverhampton, WV1 1EQ, UK
E-Mail: x.zeng@wlv.ac.uk

Abstract
Visualizing natural language description is a difficult
and complex task. When dealing with the process of
generating images from natural language descriptions, we
firstly should consider the real world and find out what
key visual information can be extracted from the sentences
which represents the most fundament concepts in both
virtual and real environments. In this paper, we present
the result of a prototype system called 3DSV (3D Story
Visualiser) that generates a virtual scene by using
simplified story-based descriptions. In particular, we
describe the methodology used to parameterize the visual
and describable words into XML formatted data structure.
Then we discuss how to interpret the parameterized data
and create an interactive real-time 3D virtual
environment.

1. Introduction
The work presented in this paper is an extension of
previous work found in [11, 12, 13, 14, 15], which is to
generate a 3D virtual scene by using story based natural
language (NL) input. The approach is based on the
premise that it is unnecessary to involve a complicated
description in order to reconstruct the 3D scene. We start
with children’s picture books to understand the structure
of the story and simplify the complexity of the natural
language processing tasks [13]. We extend our original
architecture by expending the knowledge base component
and using graphic constraints for simplifying the entire
visualization process. This work has been most influenced
by [10], [1] and [2] and we also draw on spatial theory
from [9], [4] and [5]. All natural language based graphical
applications that generate 3D virtual environments (VE)
from story descriptions can be divided into the natural
language processing and virtual scene representation
tasks. The main challenges of this work are to encapsulate
the natural language understanding, incorporate real world
knowledge, and generate appropriate graphical output. In
contrast to other authors, the originality of the approach
described here lies in the generation of a 3D virtual
environment by integrating Java, VRML and XML
technologies, which enables the user to manipulate the
visual features of virtual scene in real-time.

2. System Overview
The core of the development of this system is the
construction of tools to translate natural language input
into its graphic presentation. There are three main parts in
the system shown in Figure 1: Language Component,
Knowledge Base Component and Graphical Component.
There are these main processing layers associated with
these three components, namely File Layer, DOM
(Document Object Model) Layer and Control Layer, and
each layer comprises a specific controller module to
control the sub-modules. The system is highly pipelined
and incremental and the input text is processed through
these layers before finally being visualized. The system
functions as follows: Once the text has been inputted, the
language component analyzes it to extract the information
and output as XML a formatted semantic representation;
then the output is parameterized by the knowledge base
component and is converted into a low-level data structure
using a process termed visual semantic representation; and
finally the graphic component uses the information to
visualize the scene.
VRML World

Story Visualiser
Graphic Controller

File Controller

DOM Controller

File Layer

DOM Layer

Control Layer

Language
Component

Knowledge Base
Component

Graphical
Component

Figure 1. The Architecture of 3DSV
The system is developed in Java and XML is used as
the primary data structure and it provides a link between
these components. VRML was used as a graphic
presentation engine. The system comprises a natural
language analyser in XML and Java, the VRML world,
and a Java applet to form the input interface and finally
create links between natural language and the VRML

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

world. The NLS Java natural language processing tool
developed by the National Library of Medicine is used as
the main language parser in this work. The language
component of this system produces a dual representation
during the language parsing process, with both conceptual
and visual levels. One is to convert the syntactic structure
of sentence into a semantic representation to present the
meanings and relations inherent from the input sentences;
the other is to allow the graphic component to begin to
assemble the original virtual scene once the 3D objects
(entities) associated noun phrases have been extracted.

3. Visual Semantic Definition
People can accurately describe images and perform
classification or identification tasks based on the visual
appearance of the objects. When dealing with the process
of generating images from natural language descriptions,
we should consider the real world and find out what
common visual information can be extracted first from the
sentences because this represents the most fundamental
concepts in both virtual and real environments.
As natural languages often describe visual scenes at a
high level, which means that more details that have to be
specified and translated into a program language that the
computer can understand. Most graphics community
research in language-based control has involved
associating some appropriate words with program
functions [10]. This involves converting the various
semantic elements into parameterized data, and defining
inferences of the visual features through objects and their
context from the input text. The values assigned to these
parameters affect the generation of the 3D scene, such as
colour, material, sizes, etc. The meaning of the words is
considered as a primitive resource for visualization, and
this direct association word-concept-visual is used for
knowledge representations. A representation formalism
based on word-concept-visual information has been
proposed by linking between the levels of meaning and
visualization. Hence, an extendable dictionary called a
“Descriptionary” was created and predefined in the
knowledge base component to present the visible
information. The Descriptionary is a dictionary that uses
XML based word frames to parameterize a few “visual or
describable words” into low-level data in order to bridge
the gap between natural language and graphic component.
In the current stage of development, the vocabulary has
been semantically parameterized and classified into three
XML files: Virtual Object Definition (VOD) describes
nouns; Virtual Object Attribute (VOA) defines adjectives,
and Virtual Object Spatial-relation (VOS) associates
prepositions. Some constraints have been added according
to the applicability and specifications of VRML in order
to suit the task of scene generation.

3.1. Virtual Object Definition (VOD)
In a general sense, the object type is defined explicitly
to represent a physical object, each object in the
environment corresponding to a related noun and
associated with a geometry model in virtual object
database. The VOD uses a simple and computationally
feasible representation to define the 3D objects along with
a list of elements. The objects are idealized and simplified
by defining their default size, the orientation constraints,
and dimensions. Inspired by existing methods for
computer graphical collision detection and the geometric
theory of [4], this semantic definition of a 3D object’s
associated noun uses a bounding box to simplify complex
object geometry. The semantic definition is contained
several parts, for example, the parameterization for the
noun box is showed in Figure 2.
<object_frame>
<transform>
……
<box>
<objectType type="3D"
x="0.0" y="0.0" z="0.0" angle="0.0"/>
<volume w="1.0" h="1.0" d="1.0" />
<eventIn method="set_translation"
x="0.0" y="h/2" z="0.0"/>
<eventOut method="translation_changed" />
<spatial_value>
<top> y="h / 2" </top>
<base> y="-h / 2" </base>
<left> x="-w / 2" </left>
<right> x="w / 2" </right>
<back> z="-d / 2" </back>
<front> z="d / 2" </front>
</spatial_value>
</box>
……
</transform>
</object_frame>

Figure 2. Semantic Parameterization
for Noun Object box

The word ‘box’ is classified into transform area and
belongs to the object frame class. The first element,
objectType contains several attributes. The value of type
indicates what kind of geometry the object belongs to. The
object is divided into two different types, 2D surfaces and
3D solids. The values of x, y, z define the initial position
of the object. The value of angle describes its orientation.
The volume element presents its width, height and depth
values, which help to define the object’s spatial relations.
The values that are defined in the eventIn method are used
to position the object in the virtual scene when it first
appears in the scene. The use of the bounding box enables
rapid parameterization of the 3D object’s spatial_values,
such as top, base, left, right, back and front. The values of
these attributes also determine the position of the object in
spatial descriptions.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

3.2. Definition of Virtual Object Attributes (VOA)
From the perspective of syntactic categorization, the
function of adjective is primarily associated with
modifying nouns. A conventional classification of
adjectives is divided into two major classes: descriptive
adjectives and relational adjectives [3]. According to both
the human visual perception perspective and the
specification of VRML, the VOA currently focuses on
handling three kinds of descriptive adjectives, i.e.
lightness, colour and part of material. The basic idea for
the definition of the VOA was derived from the syntax of
the Appearance node of VRML. Figure 3 shows the data
structure of VOA for the material-related adjective ‘gold’.
There are seven parameters elements with their own
specified attributes to describe the words respectively.
Each method attribute provides various methods and
additional values, which can be used by the graphic
component to set these visual attributes to 3D objects. The
combination of these elements is sufficient to visualize the
materials of a real world in the current stage of
development.
<preposition_frame>
<position>
…
<on>
<place x_subject ="0.0"
y_subject ="+ h_subject / 2 op(+) + h_object / 2"
z_subject="0.0"/>
</on>
<left-of>
<place x_subject="- w_subject / 2 op(-)
+ w_object / 2"
y_subject="+ h_subject / 2 op(-) + h_object / 2"
z_subject="0.0" />
</left-of>
…
<in-front-of>
<place x_subject="0.0"
y_subject="+ h_subject / 2 op(-) + h_object / 2"
z_subject="+ d_subject / 2 op(+)
+ d_object / 2" />
</in-front-of>
…
</position>
</preposition frame>

Figure 4. Semantic Definition of Prepositions

3.3. Analysis and Definition of Visual Object
Spatial-relations (VOS)
When we describe spatial relationships with natural
language we often use spatial prepositions. [6] states that
the differences in language systems between object
identification (nouns) and object localization (spatial
prepositions) is attributable to the underlying organization
of the “what” (identification) and “where” (localization)
channels. Objects are not located in terms of absolute
space, but always in terms of figures placed against a

background [5]. This background is a region of space
whose organization is determined by reference objects.
Currently we concentrate on a rigid object representation.
[9] notes that the spatial properties specify the relationship
between objects. Spatial properties of one object to
another depend on geometric relations (i.e. near, in, on).
<adjective_frame>
<appearance>
<material>
…
<gold>
<parameters method="set_diffuse_color"
r="0.22" g="0.15" b="0.0"/>
<parameters method="set_emissive_color"
r="0.0" g="0.0" b="0.0"/>
<parameters method="set_transparency"
t="0.0"/>
<parameters method="set_ambient_intensity"
a="0.3"/>
<parameters method="set_specular_color"
r="0.71" g="0.70" b="0.56"/>
<parameters method="set_shininess"
s="0.16"/>
<parameters method="set_url" texture="…
\maps\grayscale.jpg"/>
</gold>
…
</material>
</appearance>
</adjective_frame>

Figure 3. Semantic Parameterization of
Visible and Describable Adjectives
While all the structural properties are specified and
defined in the VOD, in order to parameterize the
prepositions, there is a need to figure out what kind of
spatial information is necessary for the semantic
parameterization. It is also necessary to discover a
common way to compute the information that enables
graphic component to operate on it. Herskovits’
geometrical idea indicates that spatial prepositions have
ideal meanings associated with them in their lexical
entries [4]. The ideal meaning itself is defined as a relation
between “ideal geometric objects” such as point, line,
surface, etc. together with a set of constraints. These
geometric functions determine what the preposition
contributes to the meaning of a particular situation. Take a
simple sentence like A book is on the desk. There are two
objects being described—book and desk, the desk is inside
the preposition phrase and acts as a reference object to the
book. The reference object helps to define a region of
space (i.e. surface) and to support the book. The
bounding-box is the boundary to all the objects, and the
definition of the spatial area of an object is obtained by
expending the idea of [7] from 2D to 3D.
Figure 4 shows that prepositions such as on, left of and
in front of are handled by semantic functions that look at
the left and right dependents of the preposition (i.e.
subject and object) and construct a semantic spatial

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

parameterization depending upon their properties. The
element of the prepositions consists of three attributes;
x_subject, y_subject and z_subject represent the x, y, and
z axis respectively, op(-) and op(+) indicate minus and
plus operators. Each attribute takes different values to
indicate that the position of the subject is determined by
both subject and object associate objects’ structural
properties, and the position of object is the key reference
for the subject. For instance, A ball is left of a box, the
position of the ball is determined by the reference box.
Then the ball should be located in the same level as the
object and the equation of the y axis of subject should be
y_subject ="+ h_subject / 2 op(-) + h_object / 2"
which means the y-value of the translation vector has to
be at least half the height of the ball minus half the height
of the box. Furthermore the x axis value of the ball should
be half the width of the box minus half the width of the
ball as defined in equation to ensure non-overlapping.
x_subject = "- w_subject / 2 op(-) + w_object / 2"

spatial relations of the objects in the virtual scene. The
remaining DOM handlers are designed for handling output
from semantic representation and X3D files respectively.

4. Graphic Visualization
The graphic generation stage follows the visual
semantic parameterization in our visualization process. It
specifies and translates the parameterized data into
arguments of Java classes to call corresponding objects,
and manipulates them by sending events into an existing
VRML scene to construct a VE accordingly. The graphic
component consists of three parts: a Control Layer that
forms an input interface; an EAI (External Authoring
Interface) that links to the VRML world; and a VRML
Object Database that stores the 3D models. The structure
of the graphic component is shown in Figure 6. The
Graphic Controller consists of two main controllers, each
controller having several different sub-modules. Each
module handles the different tasks (i.e. modify the
attributes of the objects, placement of the objects, etc.)
and determines the final visual representation.

The reason for half of the value parameter is that all the
pivots of objects are located at the center. The
parameterization of a preposition in VOS is only defined
for the most common situation. As the key to visualizing
spatial prepositions is to note that the visualization is not a
one-to-one association, one preposition may stand for a
different geometric configuration in different objects.
Thus some extra spatial algorithms are defined to solve
this problem, the detail being described in the graphic
component.

DOM Controller

SentenceHandler

input.xml

VRML2X3DHandle

output.x3d

VOAHandler

voa.xml

VODHandler

vod.xml

VOSHandler

vos.xml

Graphic Component
VRML Object Database

VRML
World

Control Layer EAI

Graphic Controller

EAI Controller

RealTime Controller

EAIHandler SpatialLocator SVListener SVRealTimeHandler

Figure 5. Data Diagram of DOM Layer
Figure 5 illustrates the data flow of the DOM
Controller Module. During the initialization phase of a
simulation, the DOM controller module handles all the
XML files and exchanges the data between the different
components. Three DOM handler classes have been
created to parse the VOD, VOA and VOS associated
XML files respectively. Each DOM handler reads the file
and produces a DOM tree; then the trees are restored in
the memory, and they can be accessed by the graphic
component to set the attributes of the object or change the

Figure 6. Structure of Graphic Component

4.1. Changing the Visual Attributes of the Objects
Following the visual semantic parameterization in the
knowledge base, the output data has to be specified and
translated for modifying the elements of the virtual scene.
The EAIHandler module contains the methods to handle
two tasks. The first is to create a virtual objects container
to store the name of the object for further manipulation.
The other is to access the VOA and obtain the specific
data to change the attributes of the virtual object.

4.2. Handling the Spatial Relations of the Objects
Considering the complexities of the task of visualizing
natural language description, there is a need to find a way
to restrict both natural language and graphic visualization.
Story based natural language has been used as an input

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

source to simplify the task of natural language processing.
In addition, we introduce object-oriented geometric
constraints into our prototype system. The essence of our
approach is as follows:
Each object is defined by the real value of its
spatial attributes through prescribing the detail of
the geometric constraints.
Meanings of the prepositions are regarded as the
constraints among the spatial objects by
interpreting these constraints as a set of equations.
Access the numerical constraints among the
parameters and calculate the values.
The SpatialLocator module was created to establish the
methods for handling the spatial relations of the virtual
scene. The visual semantic definitions of prepositions
called VOS reflecting our theory provide the basis for
spatial construction. The SpatialLocator extracts the
sequence of the information references from the semantic
frame that are used as cues for matching the
corresponding parameters. It then extracts spatial
constraints about the objects and the preposition phrase by
accessing the related nodes from the VOS. Finally, the
extracted qualitative constraints are interpreted as the
numerical constraints among the objects. Once the
numerical constraints have been extracted, the layout of
the virtual scene can be constructed through the Story
Visualiser module by updating the position of the objects.
However, not all of the prepositions are one-to-one
associations and some problems may appear if a
preposition contains more than one geometric description.
These will cause the system to present the spatial relations
incorrectly. Thus, a filter is created in the SpatialLocator
module for handling some specific prepositions, like on,
in, under, etc. The filter contains different operations
corresponding to the different situations. The use of this
spatial filter enables the system to cope with both common
and specific spatial relations.

4.3. Integration of Spatial Reasoning
The integration of an inference technique into the
current system is required to address the need for spatial
reasoning about rigid objects. It involves the use of a real
world knowledge rule base to deduce spatial relations of
the objects. The theory underlying this approach is the
integration of the decision structure with implicit
geometries and words constraints to handle the spatial
reasoning [13]. A spatial inference filter, which is located
in the SpatialLocator module was developed in order to
implement this. Two types of spatial inferences have been
implemented in current system: The first is to infer the
spatial relations directly from the given text, which may
involve simple or multiple object manipulations. The
second is to reason the spatial relations from the context
of the descriptions, such as information that has been

omitted or indirect expression, and this is achieved by
adding more constraints in the VOS.

4.4. Real-Time Interaction
As in all language based visualization systems, there
does exist a best instantiation that satisfies all the required
constraints of the system, from both natural language
description and graphic visualization viewpoints.
However, there also exist imprecise or unexpected visual
scenes that are mainly caused by inherent under
specificity or semantic ambiguity in the given text [8]. For
example, it is relatively easy to apply other visual
parameters like “right of a small ball”, “on the red box”,
but the system will confuse if there are two more same
visual feature objects appearance in the scene. The system
obtains the constraints from the knowledge base and then
updates the position of the objects by considering the
current location of the objects from the virtual scene. In
this way the system can handle the spatial relations of the
objects in a dynamic environment and makes the real time
interaction possible. The prototype system offers two
types of enhancements: One is to allow the user to
rephrase the input sentences to obtain the correct virtual
scene. The other is to enable user to manipulate the scene
in real time by combining pointing device (e.g. mice) and
language instructions. The VRML world can be driven by
a 2D interface Java applet in real time through the EAI.
This means we can take advantage of Java features like
keyboard input and manipulate the VRML world directly
by using natural language instructions. The functions of
the manipulations are defined in the SVRealTimeHandler
module, and we simplify the process by using the
language template. This comprises two kinds of real time
instructions, one for setting the attributes of the objects
and the other for changing the spatial relations of the
objects. All these commands can be typed in the interface
of the Story Visualiser module.

5. Example
We now present an example to illustrate how the
system operates in practice. Currently, the prototype
system enables the user to generate the 3D scenes and
manipulate the properties of 3D virtual objects by using
natural language input. Consider a storyline that includes
the following sentences to describe the environment:
A brick wall is right of a room. A red rug is in the room.
A wooden table is on the rug. A lake picture is on the wall.
A blue vase is on the table. A gold ball is next to the vase.

These sentences are interpreted and presented one by
one. After the language component analyzes and extracts
the sentences, it generates two outputs with both
conceptual and visual levels: one output is an XML
formatted semantic representation, the other is to call the
3D object database to generate the prime scene; Consider

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

the last sentence, which has the semantic output shown
below:
<?xml version='1.0' encoding='UTF-8'?>
<sentence-encoding>
<!-- ================= sentence 6 ====================
-->
<np reference="ball_1" adjective="gold"/>
<np reference="vase_1"/>
<verb-frame
verb="is" subject="ball_1"
preposition="next to"/>
</sentence-encoding>

object="vase_1"

The semantic output is parameterized by the knowledge
base component and is converted into a visual semantic
representation.
Following
the
visual
semantic
parameterization, the output data is specified and
interpreted by the graphic component for manipulating the
objects (e.g. attributes and spatial relations, etc.), and
ultimately enables the Story Visualiser to generate the
virtual scene. Figure 7 shows the 3D scene that was
generated automatically from the above descriptions. It
consists of two parts, the first part is a Java application
that processes natural language analysis, and the other
contains a VRML world and a Java applet interface to
perform real time interaction using natural language
instructions.

Figure 7. Final Virtual Scene Output from
the Storyline

6. Conclusion
In this paper, we presented a methodology for
parameterizing the words that contain a visual feature into
an XML formatted data structure. A Descriptionary has
been created and used to define the different types of
words in different constructions. It bridges the gap
between language and graphic components. We also
discussed the use of graphic constraints to assist in object
definitions and for spatial construction. This it makes the
conceptual visualization possible and simplifies the entire
process. Furthermore, the system provides an interactive

virtual environment and application-programming
interface that combines natural spatial relations and object
manipulation for placing virtual objects in a 3D virtual
environment. An example using the prototype was
presented. However, the construction of the Story
Visualiser system is still work in progress. Future work
will concern improving the language component and
expanding the Descriptionary for more complex tasks.

7. References
[1] R, Clay, and Wilhelms, J. “Put: language-based interactive
manipulation of objects.” IEEE Computer Graphics and
Applications, 31–39, March. 1996.
[2] B, Coyne. and Sproat, R. “WordsEye: An automatic text-toscene conversion system.” Proc 28th SIGGRAPH Annual Conf.
Computer Graphics and Interactive Techniques. 2001.
[3] D, Gross and Miller, K. “Adjectives in WordNet.”
International Journal of Lexicography 3(4): 265-277. 1990.
[4] A, Herskovits. Language and Spatial Cognition. An
interdisciplinary Study of the Prepositions in English.
Cambridge University Press, Cambridge, MA. 1986.
[5] R, Jackendoff. Pattern in the Mind: Language and Human
Nature. Harvester Wheatsheaf Press. 1993.
[6] B, Landau and Jackendoff, R “What” and “where” in spatial
language and spatial cognition. Behavioural and Brain Sciences,
v.16: 217-265. 1993.
[7] G, Logan. and Sadler, D A. Computational Analysis of the
Apprehension of Spatial Relations, in Language and Space,
edited by Paul Bloom, et. al. MIT Press. 1996.
[8] A, Mukerjee. “Conceptual description of visual scenes from
linguistic models.” Journal of Image and Vision Computing,
Special Issue on Conceptual Descriptions, v.18(2): 173-187.
2000.
[9] L, Talmy. How language structures space. Spatial
Orientation: Theory, Research, and Application, ed. by Herbert
Pick and Linda Acredolo, 225-282. Plenum Press. 1983.
[10] J, Zara and Krivanek, J. “Graphics Performance
Benchmarking Based on VRML Browsers.” In: VRIC 2001
Proceedings. Laval: ISTIA Innovation. 2001.
[11] Q, Mehdi., Zeng, X., and Gough, N.E. “Story Visualization
for Interactive Virtual Environment.” ISCA 12th International
Conference on Intelligent and Adaptive Systems and Software
Engineering. California. 2003.
[12] Q, Mehdi., Zeng, X., and Gough, N.E. “Interactive Speech
Interface for Virtual Characters in Dynamic Environments.”
Proc. 10th International Conference on Cybernetics and
Information Technologies, System and Applications. ISAS.
Florida, USA, 243-248. 2004.
[13] X, Zeng., Mehdi, Q and Gough, N.E. “Generation of a 3D
virtual story environment based on story description.” Proc. 3rd
SCS Int. Conf. GAME-ON 2002, London, 77-85. 2002.
[14] X, Zeng., Mehdi, Q and Gough, N.E. “An Inference
Methodology for Reasoning Visual Information for a Virtual
Environment.” Proc. 4th SCS Int. Conf. On Intelligent Games
and simulation, GAME-ON 2003, London, 53-57. 2003.
[15] X, Zeng., Mehdi, Q and Gough, N.E. “Implementation of
VRML and Java for story visualization tasks.” Proc. 5th SCS Int.
Conf. On Intelligent Games and simulation, GAME-ON 2004,
Reading, UK, 122-126. 2004.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

