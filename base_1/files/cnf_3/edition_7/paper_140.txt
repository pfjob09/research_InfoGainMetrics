The Application of Adaptive Beam Tracing and Managed DirectX
for the Visualisation and Auralisation of Virtual Environments
Dr I. A. Drumm
The School of Computing, Science and Electronic Engineering
The University of Salford, U.K.
Abstract
The paper will outline the generation of virtual
environments using the author’s own adaptive
beam tracing algorithm for the prediction of
room acoustics. Details of this novel algorithm
were first published in the Journal of Acoustics
Society of America in 1999. Essentially the
algorithm simulates the propagation of sound
within an environment using reflected beams
whose shape is governed by the shape of
reflecting surfaces. Hence the adaptive beam
tracing avoids spurious and missing reflections
or large computation times associated with
other methods whilst providing the prediction
of both specular and diffuse sound
propagation. The current implementation uses
the new managed DirectX API for simulating
visually and aurally virtual environments and
explores possibility of real time acoustic
rendering using predicted acoustics in
conjunction with the 3D rendering classes
offered by Managed Direct X.

1. Introduction
Early techniques to predict room acoustics
used empirical formulae (e.g. Sabine and
Eyring equations) that estimated reverberation
time based on it’s correlation with the sound
absorption properties of surface materials, the
area of reflecting planes and the volume of the
room space. For more detailed location,
direction and frequency information about the
predicted room acoustics, techniques such as
ray tracing [2], the image method [2], conical
beam tracing [3], triangular beam tracing [4]
and hybrid methods [5] simulated the actual
propagation of sound fields and their time
dependant reflection behaviour to build up a
room’s reverberant profile.

the Image Method) is computationally
expensive for high orders of reflection as the
number of possible paths grows exponentially.
Alternatively the emulation of propagating
sound fields with rays and beams is prone to
errors because such geometrically simple
theoretical entities can only roughly describe
the shape of any given wave front. Inevitably
gaps in the wave front appear following
reflection from surfaces of different shapes
causing missing images. If gaps are to be
compensated for then such models can add
spurious images [6]. The adaptive beam
tracing algorithm [7] avoids holes in sound
fields by making sure each new reflected beam
has a shape given by the area of illumination
of the previous beam (Figure 1). If a beam falls
on several reflecting planes then several new
reflected beams are generated. Each new beam
spawned has a direction that is the reflection of
the incident beam’s direction with respect to
illuminated plane’s normal vector. Clearly one
beam may spawn multiple reflected beams in
multiple directions hence a beam stack is used
to keep track of beams that need to be traced.
The result is a geometrically perfect
description of the sound field.

Figure 1: The adaptive beam tracing algorithm
describes sections of illumination by an
incident beam. These are in turn used to form
reflected beams.

Predicting reflected images directly by
considering all possible reflection paths and
hence ruling out those that are invalid (as in

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

2. A closer look at the Adaptive
Beam Tracing Algorithm
Based on the application of vectors in 3D
world space the algorithm uses an object
orientated paradigm. Key theoretic entities that
are defined by classes and instantiated as
objects include….

 x, y , z 

x

Vectors of the form v

x

Rays of the form r ( s, c, t )
where s is the source vector for the
ray, c is a unit direction vector and t
is the time the ray travels between
beam source or reflecting plane and
the intersection with another plane,
such that for example r s  ct .

x

Beams of the form

b

The algorithm starts by creating an initial
omni-directional spread of beams. Each is
pushed onto and then popped off a stack and
hence tested for intersection with the plane list.
The nearest intersect is hence used as a starting
point for describing sections of illumination of
the beam with all planes illuminated.
Describing section of illumination is by far the
most difficult aspect of the model to
implement. This process notes that sections of
illumination are bounded by the edges of the
beams and illuminated planes. The author
describes these edges as ‘edge planes’ and uses
them to map the progress of a ‘descriptor ray’
who direction is governed by edge plane
normals, reflecting plane normal and the sense
of the beam and reflecting planes. Figure 2
shows the edge planes associated with beams
and reflecting planes which are used to
describe the section of illumination.

r1 , r2 , r3 , sb , E[ f ]

consist of bounding ray objects
r1 , r2 , r3 , a beam source vector sb
and an array of frequency dependant
energy values. Beam energy is lost at
reflection due to surface absorption
and a siphoning off of some energy
into a diffuse prediction algorithm.
x

Planes of the form

p

§ v1 , v 2 , v3 ,...v n , n p ,
·
¨
¸
¨ D, A[ f ], D[ f ], E[ 't ] ¸
©
¹

Figure 2: Edge planes bound the progress of a
descriptor ray

are described in terms of their
constituent vectors v1 , v 2 , v3 ...v n , a
plane normal

np

(v3  v 2 ) u (v 2  v1 )

plane coefficient

D

and

n p .sb .

Plane objects also have associated a
frequency dependant array of
absoption and diffusion coeffiecients
and a special array containg a time
dependant description of diffuse
energy incident during the beam trace
to be used in calculating a diffuse
exchange of energy between planes.

These
sections
of
illumination
are
subsequently used to spawn new reflected
beams which are hence pushed onto the beam
stack to be traced later.
Note sections of illumination are also used to
determine the proportions of diffuse energy
incident on planes with respect to time. Each
plane effectively builds up a history of diffuse
energy that can hence be collated to construct a
diffuse decay profile for the room.
Every beam spawned also has the potential to
illuminate a detector which can be easily tested
for to construct a specular impulse response
for the room.
The Figure 3 illustrates the basic beam tracing
process.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

Push beam onto beam stack

While Pop
beam off
stack

Log beam energy and time if detector
illuminated
Build a list of relevant ‘edge planes’

While
adjacent
planes

Describe Section of Illumination
Log beam diffuse energy and time of for plane
Reflect beam and push onto beam stack

Figure 3: Flowchart showing the basic
processes in the adaptive beam tracing
algorithm.

3. Virtual Environments
Adaptive Beam Tracing.

and

There are a number of ways to develop virtual
environments with 3D graphics and 3D sound,
most notably application programming
interfaces such as OpenGL/AL, Sun's Java 3D
API and Microsoft's DirectX. Traditionally
Direct X was the most daunting to program
involving a lot of pointers and esoteric object
management. However, with Microsoft's new
.NET platform, Visual Studio 7 and range of
new languages (e.g. C#); developing virtual
environments with Direct X has now become
surprisingly straight forward. The latest
invocation of Direct X supports the
development of managed applications.
Managed applications are applications that
compile to generic byte code to be run by a
generic virtual machine. You can think of a

virtual machine as software representation of a
computer that sits on top of the computer's
own operating system and hardware. As far as
the program in concerned it will see the same
virtual computer regardless of whether you are
using a Windows PC, a Mac, a Linux Box, etc.
The concept is not new being somewhat
similar Sun's implementation of Java.
In the case of .NET applications the virtual
machine is called the 'Common Language
Runtime' and can be downloaded and installed
for a variety of computer platforms.
The beauty of such 'managed' applications is
that they are not only a write once run
anywhere solution, but also insulate the
programmer from many traditional worries
such memory management and hardware
addressing. Another boon of managed
applications is that they make deployment
from web sites as smart clients easy.
With regards 3D graphics Managed DirectX
provides the Direct3D API for constructing
objects in 3D world space for rendering with
associated textures and lighting. It is
surprisingly easy to implement allowing the
developer to construct vertex buffers
representing the planes of the given
environment in which a user can hence
navigate by changing viewer position and
view-to-point appropriately.
Managed DirectX implements audio via
instances of the DirectSound API’s Device,
BufferDescription
and
SecondaryBuffer
classes. Essentially wav file data can be
manipulated and played through headphones
or speakers. These objects can also be
associated with instances of Buffer3D,
Listener3D and Listener3DOrientation classes
to render the sound in 3D. The beauty of the
API is that the developer can implement in real
time the sensation of localisation of sound by
invoking the above classes to emulate distance
attenuation and binaural cues such as the
frequency dependant level differences and
phase differences. These cues characterise the
way sound is heard differently at different ears.
This binaural effect is analogous to the way we
can when can localise objects with stereo
vision. Such differences can be given by at set
of angle dependant ‘head related transfer
functions’ resulting from differences in arrival
time at ears and the way our head, pinner and
body filter sound - as shown in figure 4.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

of
reverberation
times.
Hence
the
WavesReverbEffect’s
frequency
ratio
parameter can determined appropriately.

Figure 4: Binaural cues giving sensation of
sound localisation

The rendering of simple room acoustics can be
achieved with an instance of the
WavesReverbEffect class. The object can be
associated with a secondary sound buffer and
hence 3D buffer output. WavesReverbEffect
objects have adjustable parameters such as
mix, reverberation and frequency ratio – all of
which can be determined from adaptive beam
tracing.
The adaptive beam tracing algorithm finds
reverberation time from the predicted diffuse
only impulse response (or specular impulse
response with diffuse tail). Essentially the
impulse response is backwards integrated [8]
(Figure 6) and the resulting curve for decay
can be used to find T60 by employing simple
automated linear regression.

However WavesReverbEffect is relatively
crude, especially given the beam tracing model
can easily calculate more subtle acoustic
parameters such as Early Decay Time, Clarity
Index, D50, Early Lateral Energy Fraction and
Centre Time – as well as impulse responses
that can be directly convolved with wave files
for a very realistic rendering of room acoustics
(though not in real-time).
More sophisticated real-time rendering can be
achieved utilising DirectSound’s partial
conformance to the Interactive 3-D Audio
Level
2
standard
(I3DL2).
The
IDirectSoundFXI3DL2Reverb8 interface is
used to access effect parameters on a buffer
that supports I3DL2. Such parameters are
shown in the structure definition for
DSFXI3DL2Reverb (figure 7) and include
attenuation of early and late reflections, the
rate at which reflected signals become
attenuated with distance, the time between
direct sound and early reflections, the time
between early and late reflections, proportion
of diffuse echoes in late reflection and modal
frequencies.
typedef struct _DSFXI3DL2Reverb {
LONG lRoom;
LONG lRoomHF;
FLOAT flRoomRolloffFactor;
FLOAT flDecayTime;
FLOAT flDecayHFRatio;
LONG lReflections;
FLOAT flReflectionsDelay;
LONG lReverb;
FLOAT flReverbDelay;
FLOAT flDiffusion;
FLOAT flDensity;
FLOAT flHFReference;
} DSFXI3DL2Reverb,
*LPDSFXI3DL2Reverb;

30
20

Time(s)

10
0
-10 0

0.2

0.4

0.6

0.8

-20
-30
-40
-50
SPL(dB)

Figure 6: Schroeder backwards integration
based on specular and diffuse impulse
responses. The result is a decay profile from
which reverberation time can be evaluated.
Given every beam object spawned has energy
values for a range of frequency bands
(125,500,1k,2k,4k,8k) and all planes have
frequency banded absorption and diffusion
coefficients the adaptive beam tracing
algorithm gives a frequency dependant profile

Figure 7: Structure representing parameters
provided by Direct X for utilising a more
sophisticated real time emulation of
reverberant spaces.

4. Conclusions
So far the adaptive beam tracing algorithm has
been successful as a fast prediction tool to
provide data to feed into reverberation
algorithms supplied by DirectX. Development
of the complete application and testing is
ongoing.
The
author
envisages
the

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

combination of fast, detailed and accurate
room acoustic prediction, the utilisation of real
time reverberation and the potential to deliver
code as a smart client online will find
applications in virtual prototyping; particularly
where public engagement is required e.g.
trying out the acoustics of a new shopping
centre, concert hall, etc). Simple examples of
the online potential of DirectX smart clients
demonstrating 3D graphics/audio can be seen
via the www.acoustics.salford.ac.uk [9].
Eventually the development of smart clients
accessing remote hall data hall data will be
explored utilising XML web service
technology.

[8] M. R. Schroeder, “New Method for
Measuring Reverberation Time,” J. Acoust.
Soc. Am. 37, 409-412, (1965)
[9] I. A. Drumm, Development of Virtual
Environments,
Encyclopaedia,
www.acoustics.salford.ac.uk

The author has also explored real time audio
capture in Managed Direct X using instances
of the Capture and CaptureBuffer classes. This
will lead to the implementation of simulations
where users can talk and interact within virtual
environments.

5. References
[1] A. Kulowski, “Algorithmic Representation
of the Ray Tracing Technique,” Applied
Acoustics 18, 449-469, (1984)
[2] J. Borish, “Extension of the Image Model
to Arbitrary Polyhedra,” J. Acoust. Soc. Am.
75 (6), 1827-1836, (1984)
[3] D. Van Maercke & J. Martin, “The
Prediction of Echograms and Impulse
Responses within the Epiduare Software”,
Applied Acoustics 38, 93-114, (1993)
[4] T. Lewers, “A Combined Beam Tracing
and Radiant Exchange Computer Model of
Room Acoustics,” Applied Acoustics,” 38,
161–178, (1993)
[5] M. Volander, “Simulation of Transient and
Steady State Sound Propagation in Rooms
using a New Combined Ray Tracing / Image
Source Algorithm,” J. Acoust. Soc. Am., 86
(1), (1989)
[6] H. Lehnert, “Systematic Errors of the RayTracing Algorithm,” Applied Acoustics 38,
207-221, (1993)
[7] I. A. Drumm, Y. W. Lam, “The adaptive
beam tracing algorithm”, J. Acoust. Soc. Am.
107, 1405 (2000)

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

