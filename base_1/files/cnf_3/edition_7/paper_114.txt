Augmented Character Motion for Manipulation Tasks
Satoshi Y
to
Department of Information Science
Kyushu
University
2-3- Matsukadai Fukuoka 8 138503 Japan
kyusan-u.

Rin-ichiro
Department of Intelligent Systems
Kyushu University
6-1 Kasuga-koen Fukuoka 8 168580 Japan
is.
Here, the blobs mean a set of coherent image regions,
which are mainly observed as effective visual features
Since the user should have no restrictions for
the body representation, we realized the motion
capture technique for seamless interaction without any
artificial markers.
Our avatar motion control method is a physically
constrained motion synthesis to generate natural
motion from a limit number of blob positions
The
reconstructed human-like character motion should
obey original motion of the user in detail. However,
stable acquisition of human motion is a very difficult
problem in the computer vision researches
Therefore our approach is based on motion synthesis
from a limited number of perceptual cues, which can
be stably estimated. We use these methods to represent
basic postures of the avatar in the virtual
environments.
As the next challenge, we extend this approach into
automatic motion generation to represent minute tasks
such as manipulation tasks. We propose to utilize
virtual scene contexts as a priori knowledge. In order
to make rich virtual scene, we can augment the
character motion in the virtual scene by simulating
various events of the real world. A typical example of
such augmentation is physical simulation
The
physics law is defined, and in obedience to it, the
virtual objects are moved realistically. In real-virtual
interaction, every task of the character is strongly
related to the associated objects in the virtual
environments. In the case of virtual object
manipulation, the virtual environments can provide
action information for the character. In other words,
we assume that virtual objects can afford the
character's action. The idea corresponds to simulating
affordance in virtual environments. Though the
affordance is essentially defined in the real world, we
use it to relate the virtual objects with the character's

Abstract
We have developed an automatic motion
generation method for human-like characters. In
previous works, we have already developed
based human motion capture technique and avatar
motion control method toward realization of seamless
interaction between users and virtual world. In this
paper, we extend the avatar motion control method
into augmented motion generation method for the
support of animating manipulation

1. Introduction
In this paper, we describe automatic motion
generation method for human-like characters. In
recent years, 3D human figure models are controlled
by various modeling techniques such as
based modeling and retargeting techniques from
motion capture data
Though those
approaches enable realistic motion generation, they do
not consider real-time interaction with virtual objects.
It indicates that the generalized motion must be
adapted by augmenting the related motion with the
virtual objects. For such purpose, the autonomous
agents [3] are needed to inspire virtual worlds which
are represented in 3D video games and in several
virtual reality systems.
Our research goal is to generate rich virtual worlds
which human-like characters should be centered. In
previous works, we have already developed human
motion analysis (capturing) and avatar motion control
method
In this approach, we have tackled
realization of seamless interaction between users and
virtual
In such situation, the user actions need
to be directly presented.
Our motion capture technique is a novel real-time
human motion capture by skin-color blob tracking 13.

1550-6037105 $20.00

2005 IEEE

785

can be also added to the iteration process by the
equation 4.
Using a position of each joint pair, multi-part
model fitting is achieved. Rotation and translation
parameters for each link between adjacent joints are
calculated. One of the results of whole body model
fitting is shown in Figure 1 (down).

actions. In this paper, we describe the augmentation
method in manipulation tasks.

2. Physically-constrained Motion Synthesis
Only the translation of a head, hands and feet are
employed to perform the manipulation task such as
grasping of the virtual objects. As an approximate
approach, we have introduced a physically constrained
motion synthesis, where only the skeletal model can
obey the physics law such as point-to-point constraints
between adjacent joint points. The method is realized
by simulating Hooke’s law (Figure 1). Then, we fit a
part model for the adjacent joint points to estimate the
pose parameters such as rotation and translation.
Here, we assume that a link between adjacent joint
points (or material points), is represented by a strong
spring model so as to keep the length of the link
constant. Moreover, we assume that the velocities of
the joint points can be approximately calculated from
the displacements of the joint positions.
The estimation algorithm of skeletal model is as
follows:
For all pairs of joint i and joint j, the following
procedure is iterated until it converges.
1. Calculate the following d, f.

d

-

where,
and
are joint positions, and
is the
original rest length between
and
and K is a
spring constant value.

2. Modify

and

by a step size

Figure 1: (up) Upper body model. (down) An
example of the reconstructed skeletal model
of the whole body

.

3. Augmented Character Motion
3.1. Affordance simulation

Several joint positions
are modified, based on
is
force vector
force by the step size
which corresponds to sum of several force vectors for
joint k. An example of is a gravity component.

We describe our framework to utilize virtual scene
contexts as a priori knowledge. In order to make the
virtual scene more realistically, we can augment the
character motion in the virtual scene by simulating
various events of the real world. We assume that each
virtual object affords additional information about the
character tasks by simulating the idea of
in
the virtual environments. Interaction among the
virtual objects and the character should be properly
performed by making each virtual object give rise to

are acquired.
Finally, the updated parameters {
joint positions keep the balance according to the
change of positions of head and hands. The other
constraints such as collision against the other objects

786

We address how to animate several manipulation
tasks for the character using several examples of the
tasks. In this case, we assume that the character tasks
are described as user-specified tasks in advance. The
typical tasks are reaching and grasping.

the related actions. Both the states of the virtual
objects and the states of the character are picked up
whether the virtual objects are handled or not, for
example, according to the distance between the virtual
objects and the character. For example, in grasping a
virtual cup, finger motions are afforded as the related
actions. Then the state of the object is changed static
into move. These motions are not represented by
physically-constrained motion synthesis but provided
from the virtual object which is connected with
augmenting the virtual scene.
Moreover, we can consider the scene constraints in
the virtual environments to simulate the scene events
realistically. The virtual scene is completely
recognized by the system. To reason the scene contexts
in the interaction through the virtual environments is
not more serious than in the real world sensing.

Figure 2: Example of augmented character
motion: one hand grasping

3.2. Upper body case
Table 1 shows the driving methods for the case of
the upper body. In this table, the upper two rows (a)
(b) indicate several parts which should be driven by
the character tasks, for example, which the user can
specify in advance. The lower row (c) indicates the
rest parts driven by the motion synthesis. The method
TK means task command such as reaching motion
from a position A to a position B. This is also
controlled by direct motion capture data. The joint
positions by TK are described as white circles in
Figure 1. PH and AF mean physically-constrained
motion synthesis and afforded motion synthesis
respectively. The joint positions by PH are described
as gray circles in the Figure. Detailed motions in
object manipulation task are driven as the afforded
actions.
Part
(a) Head position
Hand positions
Shoulders
Elbows
Neck, torso
Face direction
(c) Fingers
Hand states

Figure 3: Example of augmented character
motion: both hands grasping

4.1. Reaching motion

Method
TK
TK
PH
PH
PH
AF
AF
AF

We used an arm trajectory model [6] for generating
reaching motion. This model is well known as
representing natural reaching motion.
The trajectory is given by the following equation:

=
where,

,

indicates the start position, and
indicates the goal position.
For example, user specifies a starting position A, a
goal position B and/or an interesting object for

4. Manipulation Tasks

787

grasping. This model is used to perform task
command TK.

Manipulation tasks can be semi-automatically
animated when the user specifies only the target
positions and the interesting object. This approach is
the most suitable in the following situation: direct
controls of avatar posture are limited on only primary
positions such as face, hands and foots. Such cases are
often caused in the use of vision-based motion capture
system or in the simplified motion capture systems as
substitute for character handling tools. By using our
approach, detailed motions can be automatically
generated from the relationship between the virtual
objects and the avatar actions. Each of the virtual
objects manages the action information with the avatar.
The limitation of our approach is that: When a lot of
virtual objects are closely allocated, augmented
motions may be added against the user’s will. In
particular, it is a challenging problem for
machine systems to recognize the interesting object.
Future works include extending our framework into
more complex tasks for various avatars.

4.2. Grasping motion
In our framework, grasping motions are
automatically generated according to the virtual scene
situation. Concretely, when the hand position is close
to the object, the hand state is changed, in equal, the
finger and palm motions are augmented as afforded
action. Face direction is also controlled based on the
hand states and the focusing object.

4.3. Examples of the grasping tasks
Figure 2 and Figure 3 show the example shots in
reaching and grasping tasks. In Figure 2, one hand
grasping is realized. In Figure 3 , both hands grasping
is realized. Though the user specifies the same pair of
the start position and the goal position, the different
animation tasks are automatically realized. The
discrimination is determined by the interesting virtual
object. In these experiments, either a cup or a cube is
defined. The cup object is related with both hands
grasping. Table 2 and Table 3 indicate the flowchart
of the driving methods in the grasping tasks
respectively.
RF LH, and LF represent right hand,
right fingers, left hand and left fingers respectively.

4.4. Application to real-virtual system
Next, we have applied the augmented character
motion techniques to real-virtual application, where
the user can handle an avatar and it realizes virtual
object manipulation tasks. The presentation effects of
the augmented motion generation are certified by both
the user and the other participants connected through
the virtual worlds. In particular, it was pointed out
that face direction control of avatar is an important
effect to understand avatar’s action.

Table 2: Flowchart of driving methods in one
hand grasping tasks

RF

LH

LF

A: Start

Reaching

PH

AF

TK

TK

TK

TK

B: Target

Grasping
Reaching
C: Goal
Releasing

5. Conclusions
We have proposed augmented motion control
method for the purpose of animating human-like
character. We employed physically-constrained
motion synthesis for the skeletal model construction.
Then we can augment the character motion in the
virtual scene by simulating various events of the real
world according to the idea of affordance which is
simulated in the virtual world.

AF
PH

AF

AF

Table 3: Flowchart of driving methods in both
hand grasping tasks

788

and M.W. Green, The Dynamics of
Articulated Rigid Bodies for the Purposes of
Animation', The Visual Computer, vol. pp.23 1-240,
1985.

Acknowledgements
This work has been partly supported b y the
programs of the Grant-in-Aid for Scientific Research
from the Japan Society for the Promotion of Science
(15700172). This work has also been partly supported
by " Scientific Research on Priority Areas: Informatics
Studies for the Foundation of IT Evolution; A03
Understanding Human Information Processing and its
Application", one of the programs of the Grant-in-Aid
for Scientific Research from the Japan Society for the
Promotion of Science (15017270).

Flash, T., Hogan, N., The coordination of arm
movements:
An
experimentally
confirmed
mathematical model', The
of Neuroscience,
1985.
Segmentation and
Motion
Estimation by Region Fragments, In International
Conference on Computer Vision, 192-199, 1993.
C.Wren,
Real-Time Tracking of the Human Body,
Transactions on Pattern Analysis and Machine
'Intelligence,
1997.

References
Satoshi Yonemoto, Daisaku Arita and Rin-ichiro
Taniguchi, Real-Time Human Motion Analysis and
Human Figure Control, in Proceedings of
Workshop on Human Motion
154,2000.

"understanding
Human Motion, in Fourth IEEE
conference on Automatic Face and
Recognition, 2000.

Satoshi Yonemoto and Rin-ichiro Taniguchi,
based 3D Direct Manipulation Interface for Smart
Interaction, in Proceedings of International
Conference on Pattern Recognition, pp.655-658, 2002.

[1

Jr., Autonomous Agents for Real-timeAnimation',
thesis Stanford University, 1999.
Y
Planing
Proceedings of

Motions

with Intentions',
pp.24--29, 1994.

Gesture

Learning and Recognizing Human
Dynamics in Video Sequences, in Computer Vision
and Pattern Recognition, pp.568-574, 1997.
Physics-based Deformable Models
Applications to Computer Vision, Graphics and
Academic Publishers, 1997.
Medical Imaging,

[

In

Virtual

789

Humans,

Morgan

2000.

