Hierarchical Indexing for 3D Head Model Retrieval Based on Kernel PCA
Hau-san Wong*, Bo Ma*, Yang Sha*, Horace H-S Ip*,+
Dept. of Computer Science, City University of Hong Kong, Hong Kong
+
Centre for Innovative Applications of Internet and Multimedia Technologies (AIMtech) Centre,
City University of Hong Kong, Hong Kong
*

Abstract
In this paper, a novel 3D head model retrieval
framework is proposed. First, Kernel PCA is adopted
both to reduce the data dimension and to extract
features for model characterization. Second, based on
the derived features, a hierarchical indexing structure
for 3D model database is constructed using the
Hierarchical Self Organizing Map (HSOM). Third, an
efficient search approach is presented based on the
established indexing structure that requires only
feature matching between the query model and a small
number of SOM nodes. The main advantages of our
approach include high retrieval precision due to the
discrimination capacity of kernel PCA, and low
computation cost due to the hierarchical indexing
structure and data dimension reduction. In addition,
the topology-preserving property of HSOM also
facilitates the exploration of the model database with
the possibility of further knowledge discovery.

1. Introduction
With the ready availability of 3D scanners, we have
witnessed a large increase in the number of 3D models,
most of which are available on the Internet. Because of
the time and effort involved in creating 3D models,
considerable resources could be saved if these models
can be reused. As a result, the need for the ability to
retrieve 3D models from large databases or the Internet
has gained prominence. In particular, human head
model retrieval is the focus of attention in view of its
many potential applications to constructing virtual
characters in computer animation as well as to
modeling cranial structures in medical imaging [1, 2].
There are a number of methods proposed for the
representation and matching of 3D models. They can
roughly be classified into four approaches: 2D-view
based, geometry based, frequency based, and topology
based. It is difficult to say which method is best among
others and it very much depends on the application at
hand [1, 2]. Although there are a variety of
representation schemes for 3D head models in the
literature, PCA is virtually overlooked as a feature

extraction approach for 3D model, and its most
frequent use is restricted to model alignment instead of
categorization [3]. The main advantage of using PCA
is that it provides an elegant mechanism for describing
the 3D head model in multiple resolutions, through the
ordering of the eigenvalues and the selection of the
embedded projection subspaces for the different
resolution levels, apart from data dimension reduction.
This enables the performance of multi-scale analysis of
the 3D head models, based on which a hierarchical
structure for the retrieval and management of
multimedia database can be developed. One possible
reason that PCA approaches are often overlooked as
suitable feature representations for 3D head models is
that in the original feature space, PCA may not be able
to successfully extract the salient characteristics of the
dataset appropriate for accurate classification. This
problem can be relieved by use of its nonlinear
counterpart, i.e. kernel PCA [4, 5].
Kernel-based subspace analysis has been applied to
different signal processing and computer vision
problems as an efficient approach for both
dimensionality reduction and feature extraction.
However, to our knowledge, no related works in the
literature have discussed its application to 3D model
retrieval. Toward this goal, we propose an indexing
structure based on the Hierarchical Self Organizing
Map (HSOM). The main idea of our approach is to
organize the database into a hierarchy so that head
models are initially partitioned by coarse features at the
upper levels, and then by finer scale features at the
lower levels. At each level, a two-dimensional grid of
nodes is adopted to cluster the 3D models [6, 7]. To
further enhance the performance, a fuzzy metric
between the query and the feature vector associated
with each node is adopted [7, 8]. Only nodes that
possess high fuzzy measure values will be considered
further for retrieval. In this way, the fuzzy measure
approach is able to pick up potential relevant models
even though they may be partitioned into neighboring
nodes, and the premature commitment to a possibly
incorrect class is avoided.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

2. Nonlinear Feature Extraction Using
Kernel PCA
In this section, we apply kernel based principal
component analysis to extract the nonlinear features for
3D head model retrieval.

2.1.

Nonlinear Feature
Kernel PCA

Extraction

Using

Given M mean-centred observations x k , which in
our case correspond to the original representation
vectors of the 3D head model, in the form of the
complete set of concatenated 3D vertex coordinates,
PCA needs to solve the corresponding eigen-system
λv = Cv where C denotes the covariance matrix. Let
y = φ( x ) denotes a nonlinear mapping. We can apply
PCA to the transformed vectors y in the new feature
space F in a similar way.
N
1
~
Let φ ( x m ) = φ( x m ) −
φ( x n ) , we denote its
=1
n
N

¦

~

corresponding covariance matrix as C , and the
~
corresponding eigen-system satisfies λ~v = C~v . Since
the eigenvectors lie in the same subspace as the feature
~ to make
vectors do, there exist coefficients α
n
N
~
~
~v =
¦n=1α nφ ( x n ) hold. After simple algebraic
manipulations, we obtain an equivalent eigen-system
~
~
~
~
~
~
λN~Į = K~Į with K = [ k mn ] , k mn = φ ( x m ) T φ ( x n ) and
~Į = [ α
~ ,...,α
~ ] T . In order to compute the dot product
1

N

in the nonlinear feature space, we can use a kernel
representation of the form [4]

k (x m , x n ) = φ (x m )T φ (x n )

(1)
Such a representation allows us to compute the value
of the dot product in the nonlinear feature space F
without having to carry out the mapping φ explicitly.
By defining K = [ k mn ] and k mn = k( x m , x n ) , we can
~
calculate K . Thus we can compute the solution to the
eigensystem, and obtain N eigenvectors ~Į and N
n

corresponding nonnegative eigenvalues λ n . Let x be a
vector in the input feature space. We can now extract
its N nonlinear features as follows [5]:

~
cn = ¦mN=1Į~n,mk (xm , x)

(2)

2.2. Multi-level Representation of 3D Models
Thus for every input feature vector x , we can
compute the principal component vector c with each

element corresponding to the operation in Eq. (2).
However, in practice, the feature vectors may reside in
a subspace of F only, and as a result only a subset of
the principal component vectors is required for
satisfactory representation. Suppose that the
eigenvalues
are
arranged
in
descending
ˆ
order, λ1 ≥ λ 2 ≥ " ≥ λ M and let φ l ( x ) denote the
reconstructed data in the nonlinear feature space by
keeping only the first l principal components, we have

E [|| φ ( x ) − φˆ l ( x ) || 2 ] =

N

¦ l′= l +1 λ l ′

(3)

From the above equation, it is seen that PCA provides
a multi-resolution interpretation of the feature vectors.
Specifically, we can judiciously choose the value of l
to obtain a nested set of truncated principal component
vectors c l that correspond to reconstructed feature
vectors φˆ l ( x ) of different resolutions. A larger l
value corresponds to a smaller reconstruction error or a
finer scale. In the next section, the set of feature
vectors c l will be used as input at different levels of
the hierarchical SOM for 3D model retrieval.

3. Hierarchical Indexing Structure Based
On HSOM
A self-organizing map represents a set of training
data using a small set of n clusters, with the j-th
cluster characterized by a weight vector w j . In
addition, a neighborhood relationship (in the form of a
2-D grid) is defined between these clusters such that
similar clusters are located in close proximity to each
other [6]. The hierarchical SOM is an enhancement of
the original architecture such that the set of data
associated with a particular weight vector is further
partitioned using a secondary SOM into a finer set of
clusters [7].
Given the multiscale approximation property of the
nonlinear feature vectors for the 3-D head models
obtained by kernel PCA, where a model can be
approximately represented by the sub-vector and the
representation can be further refined by the inclusion
of additional vector components, a corresponding
HSOM architecture is defined as follows: A set of subvectors c

lp

= [ c1 ,!, c l p ] ∈ R

lp

are extracted from the

full nonlinear feature vector c ∈ R M for each 3-D
model that have the associated dimensionalities
1≤ lp < lp+1 ≤ M, p =1,!, L . In other words, there are L
levels of hierarchies in the indexing structure. The
sub-vectors with the smallest dimensionality are then
used to form the clusters with associated weight

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

M

vectors w 1,r ∈ R l1 in the first hierarchical level. For
each of the selected clusters in the first level, the subvectors with the next smallest dimensionality are
adopted to form the set of secondary clusters in the
next hierarchical level. This process can be repeated in
a recursive way. An indexing structure of 3 layers is
shown in Fig. 1.

2.

Select the parameters θ > 0 and m > 1

3.

for i = 1,..., N ,

4.

for j = 1,...n , update the weight vectors using the
following equations
wj =
h ws
s∈ Neigh ( j )

j = 1,...n

compute U ji

¦

s, j

N

wj

¦ U c
:=
¦ U
m
ji

i =1

N

i =1

5.

i
m
ji

+ θw j
,

+θ

(4)

Terminate the process when the overall difference in
the U ji ’s between the current and the previous
iteration is smaller than a given threshold; otherwise
go to step 4.

In Eq. (4), m is the parameter which characterizes the
degree of fuzziness, Neigh ( j ) is the neighborhood set
of node j ; w j is the weighted average of the
Fig. 1. HSOM architecture for 3-D model retrieval

3.1. Fuzzy SOM
The self organizing map was first proposed by
Kohonen as an example of an unsupervised learning
approach for neural networks. The overall criterion is
to locate the prototype vectors in such a way that the
average distance from the prototypes to the feature
vectors belonging to their own Voronoi compartment
is minimized [6]. We can consider the SOM as a neural
network implementation of this clustering procedure,
where each prototype is the weight vector of a neural
unit. Here the neurons are arranged in a twodimensional grid such that a neighborhood is defined
for each neuron. The goal of learning is both to find
the most representative set of weight vectors, in a mean
square sense, for the tessellation of the input space, and
to realize a topology-preserving mapping from the
input space to the grid of neurons. Instead of adopting
the hard decision process, an algorithm that
incorporates fuzzy c means clustering into SOM was
adopted by minimizing a smoothly distributed fuzzy c
means functional [8]. Let c i , i = 1,..., N denote the
nonlinear feature vector and w j , j = 1,...n denote the

neighboring weight vectors, and θ is a parameter
which determines the contribution of the neighborhood
weight vectors to the final updated vector. In practical
implementations, the size of the neighborhood and the
value of m should be allowed to decrease gradually to
facilitate convergence.

3.2. Hierarchical Indexing Using Fuzzy SOM
Based on a modified version of the fuzzy selforganizing map algorithm, we can establish a
hierarchical indexing structure for 3D head model
retrieval. The corresponding algorithm is as follows
1. Start at the first level and set p = 1 . Take c l1 as the
2.

weight vectors. A fuzzy membership function U ji
which is defined as Uji =

and satisfies 0 ≤ U ji ≤ 1 ,

V =−

1
n

2/( m−1)

¦k=1(||ci −wj||
n

¦ j =1U ji = 1

2/( m−1)

/||ci −wk||

)

is adopted to

3.

incorporate fuzziness into the clustering process [8].
The fuzzy SOM algorithm can be described as follows

4.

1.

input feature vector to the first layer SOM.
For every possible size of the map within a given
range, carry out the following procedure:
a Apply the algorithm described in subsection III.A
to determine the weight vectors for the SOM.
b Assign each feature vector to its corresponding
node based on fuzzy membership value.
c
If the number of feature vectors assigned to a node
is smaller than a given threshold T I , delete this
node, and re-assign these vectors to the remaining
nodes.
d Compute a validation index V of the map using the
following entropy measure:

Initialize the weight vectors w j

1
N

R

N

¦ j =1 ¦i=1U ji log 2 U ji ,

(5)

where R denotes the number of valid nodes on the
current map.
Determine the corresponding submap size with the
minimal index value.
For each node I p ,r in level p :
a

Consider the set of feature vectors c

lp

∈ S p ,r

associated with this node and their extended

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

l

counterparts c p +1 . Based on these vectors, we
compute the trace of its conditional covariance
matrix C p , r :
l

l

l

l

Cp,r =E[(c p+1 −E[ c p+1|Sp,r])(c p+1 −E[ c p+1|Sp,r ])T|Sp,r] (6)
b

If the value is greater than a given threshold TC ,
generate a new submap for this node at the level
p + 1 according to steps 2 and 3 using the

5.

extended feature vectors c
p = p +1 .

l p +1

.

If p = L , end; else go to 4.
According to this algorithm, several important
modifications to the original SOM learning algorithm
are proposed, i.e.
6.

•
the deletion of the relatively less important nodes
associated with small number of vectors;
•
the use of a validation index to choose the optimal size
of the submap;
•
Selective expansion of SOM nodes to form the second
level submaps;
•
Adoption of a statistical measure to identify the subset
of nodes to be expanded.

The motivation of the latter two modifications is
that, in general, not all nodes in the first SOM layer
require the support of a second layer submap. Only
those that correspond to the class boundary between
different model categories, in other words, those nodes
that are associated with training examples of different
classes instead of a single class, require a second layer
map to further refine the classification. Here, we need
to point out that the concept of category is related to
scale: a single class at a coarser level may be able to be
divided into several subclasses when finer features are
taken into account. These nodes can be identified by
evaluating the statistical measure which quantifies the
class diversity of the training examples assigned to a
particular node.

3.3. Query Matching Method
Once the hierarchical indexing structure is
established, we can perform 3D head model retrieval
for a given query. The matching procedure can be
summarized as follows:
1. Extract the feature vector of the query model using kernel
–based PCA.
2. For the first level of the hierarchical SOM, compute the
fuzzy membership of the query model for each node
3. Select the subset of nodes where the membership value is
greater than a given threshold Tr for further query
processing.
4. For other levels, perform feature matching for those sub
maps associated with the selected nodes until the final level
is reached

5. Collect all the selected samples in the database and rank
them according to the distance function.

It can be seen in the above description that retrieval
can be performed in a highly efficient manner since
comparison is performed on only a small subset of
models instead of the complete set of database entries,
and at the upper levels, only comparisons between
low-dimensional feature sub-vectors are involved. In
addition, the fuzzy formulation avoids the pre-mature
commitment to a particular category by selecting,
based on the fuzzy membership value, a subset of
nodes for further exploration at each level for further
exploration, instead of just a single node.

4. Experiment
We have applied our proposed approach to a 3D
head model database to evaluate its performance. In the
database, there are ten categories of heads, and each
category contains 80 models. For each class, 40 models
are included as the training set, while the remaining
models are used for testing.
The Gaussian kernel function is adopted in our
experiments, which can be expressed as
|| x m − x n ||2
k( x m , x n ) = exp( −
)
(7)
2σ 2
In addition, we have also included results based on
applying PCA in the original feature space, which can
be considered as a special case of kernel PCA in which
the mapping φ is linear. In all the experiments related
to kernel PCA, the value of the parameter σ is set to 2.
The fuzzy parameter m is set to 2.5 initially, and
then gradually being decreased to 1.5, which falls
within the optimal interval suggested in [9]. In other
words, at the initial stages, a large degree of fuzziness
is encouraged, and with the onset of convergence, the
degree of fuzziness is decreased to consolidate the
results. As mentioned in the previous sections, a
validation index can be used to choose the optimal
sizes of the different levels. In current implementation,
a 2-layer HSOM is considered. For the PCA feature,
the dimensions are 4 and 7 for the first level and the
second respectively. For the Kernel PCA, the
dimensions are 8 and 25 for the first level and the
second respectively.
In Fig.2, the first layer of the established
hierarchical structure is illustrated. We can
immediately observe that this first layer has
successfully identified those main head categories with
distinct facial characteristics. While the more
prominent head categories are identified in the first
layer, there may be cases where the differences
between certain classes are very subtle, or that, within
a certain head category, there may exist hidden class

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

substructures which are previously unsuspected. In
these cases, we expand the associated first layer node
into a second layer submap which serves to provide a
further elucidation of the underlying class
substructures. An example of a second layer submap is
shown in Fig 3. It can be observed that the learning
process has automatically discovered the underlying
substructure of this particular head category, in the
form of a set of different facial expressions.

Fig. 3. A typical submap for one node in the first layer of
the hierarchical SOM

Fig. 2. The first layer of the hierarchical SOM

The recall-precision graphs of retrieval are given in
Fig. 4-Fig. 7 using both the HSOM method and a nonhierarchical nearest neighbor matching (NN) approach.
The definitions of recall and precision are as follows:
|A
∩ Aretrieved | ,
|A
∩ Aretrieved | ,
Recall = relevant
Precision = relevant
| Arelevant |

| Aretrieved |

where Aretrieved denotes the set of retrieved samples,
Arelevant the set of all relevant samples in the database,
and | ⋅ | the cardinality of a set. Fig.4 gives the results
using the non-hierarchical approach. According to
these graphs, we can see that the kernel PCA approach
can achieve a more satisfying retrieval result than PCA
for both the training set and test set. With the proposed
HSOM method, the same conclusion can be drawn
from Fig.5, i.e. kernel PCA features can result in a
significant improvement in retrieval performance. The
comparison results between HSOM and NN are shown
in Fig.6 and Fig.7 using the features obtained by PCA
and kernel PCA respectively. It is seen that the
adoption of HSOM results in a significantly higher
precision values for the same recall rates in all cases. In
particular, for kernel PCA, a 98% precision value for
the test data set is achieved with the HSOM indexing
structure even when the recall value approaches 100%
(Fig.7), and the original PCA features still manage to
achieve an 88% precision value for the same recall
value (Fig.6).

Another motivation of adopting HSOM is the
possibility of reducing the set of candidate entries
during retrieval. To verify this, we have measured the
average size of the candidate model subset for HSOM
retrieval across the sample query set, as a percentage of
the complete database. In general, with the adoption of
the HSOM indexing structure, the average size of the
candidate model subset is only about 10% of the
complete database, which translates into a
corresponding savings in search cost.
For the sample query shown on the upper left of
Fig. 8, the top five retrieval results are shown in the
same figure. It is seen that the retrieved models are all
visually similar to the query model, which indicates
that the HSOM has correctly modeled the underlying
class structure of the head model data set.

Fig. 4. Precision-recall graph for non-hierarchical
approach: left: training set; right: test set

5. Conclusion
In this paper, we propose a new 3D head model
retrieval framework based on hierarchical SOM.
Descriptive features for the 3D models are extracted by
kernel PCA. Given these features, a hierarchical
indexing structure for 3D model retrieval is proposed

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

based on the Hierarchical SOM. The proposed
indexing structure clusters the database into a hierarchy
so that head models are partitioned by low resolution
features initially and then by finer scale features in the
lower levels, which naturally correspond with the
multi-resolution description of the models in the form
of a set of nested feature sub-vectors. A fuzzy metric
between the query and the feature vector associated
with each node is adopted. Only nodes that possess
high fuzzy measure values will be considered further
for retrieval, with the result that the computation cost is
greatly reduced. In addition, the fuzzy measure
approach is able to pick up potential relevant models
even though they may be partitioned into neighboring
nodes, and the premature commitment to a possibly
incorrect class is avoided.

Fig. 8. Partial retrieval result for a given query

6. Acknowledgement
The work described in this paper was fully
supported by grants from the Research Grants Council
of Hong Kong Special Administrative Region, China
[Project No. CityU 1197/03E and CityU 1150/01E]

7. References

Fig. 5. Comparison between kernel PCA and PCA using
HSOM: left: training set; right: test set

Fig. 6. Comparison between HSOM and NN using PCA:
left: training set, right: test set

Fig. 7. Comparison between HSOM and NN using kernel
PCA: left: training set, right: test set

[1] T. Funkhouser, P. Min, M. Kazhdan, J. Chen, A.
Halderman and D. Dobkin, “A Search Engine for 3D
Models”, ACM Trans. on Graphics, vol. 21, no.3, pp. 83-105,
2003.
[2] D. Keim. “Efficient geometry-based similarity search of
3D spatial databases”. In proceedings of ACM SIGMOD, pp.
419-430, 1999.
[3] D. V. Vranic, D. Saupe, and J. Richter, “Tools for 3Dobject Retrieval: Karhunen-Loeve Transform and Spherical
Harmonics”, Proc. IEEE 2001 Workshop Multimedia Signal
Processing, Cannes, France, pp. 293-298, Oct. 2001.
[4] M H Yang, "Kernel Eigenfaces vs. Kernel Fisherfaces:
Face recognition using Kernel Methods”, Proc 5th IEEE Int
Conf on Automatic Face and Gesture Recognition, pp. 215220, 2002.
[5] Bernhard Scholkopf, Alexander Smola, and KlausRobert Muller, “Nonlinear Component Analysis as a kernel
eigenvalue problem”, http://www.mpik-tueb.mpg.de/bu.html.
[6] T. Kohonen, Self-Organizing Maps, Springer-Verlag,
Heidelberg. 2nd Ed. 1997.
[7] J. Lampinen and E. Oja, “Clustering properties of
hierarchical self-organizing maps”, Journal of Mathematical
Imaging and Vision, vol. 2, no. 2-3, pp. 261-272, 1992.
[8] R D Pascual-Marqui, A D Pascual-Montano, K Kochi, J
M Carazo. “Smoothly distributed fuzzy c-means: a new selforganizing map”, Pattern Recognition, vol. 34, pp. 2395-240,
2001.
[9] N. R. Pal and J. C. Bezdeck. “On Cluster Validity for
the Fuzzy c-Means Model”, IEEE Trans. On Fuzzy Systems,
Vol. 3, No. 3, pp. 370-379, 1995.

Proceedings of the Ninth International Conference on Information Visualisation (IV’05)
1550-6037/05 $20.00 © 2005 IEEE

