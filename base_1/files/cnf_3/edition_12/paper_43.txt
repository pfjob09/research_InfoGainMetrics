Structuring the Visual Content of Digital Libraries Using CBIR Systems
Marion Borowski, Lars Brocker, Stephan Heisterkamp, Jobst Loffler
GMD - German National Research Center for Information Technology,
Schloss,,Birlinghoven,D-53754 Sankt Augustin
Emai 1: { marion.borow ski, lars.broecker, stephan.heisterkamp, jobst .loeffler} @ gmd.de
library might prove their research theories. Journalists and
publishers need to query image archives for the
illustration of newspapers, books and articles. Within the
scope of ,The Digital Beethoven House‘ - a digital library
project of the Beethoven House Association, the City of
Bonn, and GMD [2] - a large number of digitized images
will be made available via the Internet, not yet structured
with additional data. Retrieving all pictures with
Beethoven sitting at the piano is one exemplary query.
Librarians usually rely on text-based retrieval using their
bibliographical catalogue as reliable metadata, but all too
often - especially for archival material - the manual
attachment of textual metadata to the images is too timeconsuming. These examples illustrate the need for
retrieval systems working only on image content.
We used CBIR not only for retrieval reasons but
especially to organize the content of general purpose
image databases of a digital library e.g. to build clusters
consisting of instruments or handwritings and letters. We
tested our new approach to structure databases using three
commercial content-based image retrieval systems. We
focused on still images.
This paper is structured as follows: Chapter 2 describes
the principles that form the basis of CBIR. Chapter 3
outlines our new approach to structuring image databases
by the means of CBIR. The effectiveness of commercial
CBIR systems for this task - to organize image databases
- is the focus of chapter 4. The paper concludes with a
description of possibilities and limits of a practical
application of CBJR systems in digital libraries.

Abstract
In this article we describe the benefits of CBIR - content-

based image retrieval - systems for retrieval and
organization of visual content in databases of digital
libraries. Unlike text-based retrieval systems which work
with manually annotated keywords as metadata, CBIR
systems use automatically extracted numerical
representations of perceptual features like color, texture
or shape. This technique enables users of digital libraries
to retrieve visual content without the help of textual
metadata, which in some cases may be either not existent
or not suficient for a special purpose. It is our approach
to support maintainers of digital libraries in organizing
large image volumes using CBIR systems. Finding
clusters of similar content or providing clusters with
suitable keywords are new application contexts of CBIR.
We propose a method for structuring visual content and
evaluate retrieval results of CBIR systems with regard to
their applicability to this method.

Keywords
Visual information retrieval, image databases, contentbased image retrieval, clustering, digital libraries

1. Introduction
Content-based image retrieval (CBIR) is a technique for
retrieving images on the basis of automatically-derived
features such as color, texture and shape [ 11. An image or
some drawn user input serves as a query example and all
similar images should be retrieved as results.
Digital libraries - especially their visual content - may
highly benefit from CBIR systems. Archaeologists may
need visual comparisons of their archaeological finds at
some stage of their examination. The access to the visual
content of a museum’s, an art gallery’s or library’s digital

2.

Visual information retrieval in large image collections is a
complex task because different types of data can be
associated with the images. Together with the images
metadata are stored in the database. They allow searching
in the image collection. Two major groups of metadata
can be distinguished: content-independent metadata, like
author or date of creation, and content-dependent

288
0-7695-0743-3100$10.00 0 2000 EEE

Principles of CBIR

Sample image

Color histogram

Feature vector

Frequency of
occurrence

t
Pixel color

e

b

Automatic extraction

Figure 1. Automatic extraction of color features using a color histogram
distributions match those of the query example most
closely.
Texture features: Textures are characterized by
differences in brightness with high frequencies in the
image spectrum. They are useful in distinguishing
between areas of images with similar color. To
evaluate texture features a common approach is to use
second-order statistics which allows to calculate
measures of image texture such as the degree of
contrast, coarseness, directionality, regularity and
randomness [4]. A texture is usually represented
through a numerical vector, holding measures of texture
features.
Shape features: These features can characterize either
the global form of the shape - like the area, the
extension and the major axis orientation - or local
elements of its boundary like corners, characteristic
points or curvature elements. The degree of similarity
between two shapes is calculated through standard
mathematical distances as, for example, the distance
between two points. The ability of shape features to
bear semantic meanings can be used for semi-automatic
extraction of high-level content-dependent metadata by
providing characteristic shapes for special real-worldobjects in the CBIR system.
Spatial features: Spatial relationships can be expressed
by spatial entities like points, lines, regions and objects
and their distribution in an image. They can be
classified into directional relationships - like right, left,
above, below together with a distance - and topological
relationships - like disjunction, adjacency, containment
or overlapping of entities. Spatial information can be
used to improve image retrieval with local features.
An image query is performed by generating a weighted
combination of features as signature for the example
image and by directly comparing it with the signatures
stored in the database. A similarity metric (e.g. the
Euclidean distance in the feature vector space) is then
used to find the nearest neighbor of the query example

metadata. This second group of information associated
with images refers to the visual content of images. Data
of this type can again be divided into metadata, which
refer to low-level perceptual features like color, texture,
shape and spatial relationships, and metadata, which
refer to high-level content semantics like relationships
to real-world entities or meanings [3].
When storing an image collection into a database,
metadata of both groups can be added by manual
annotation or automatic extraction (see Fig. 1). Most of
today’s systems for CBIR use numerical representations
of low-level perceptual features, so-called feature
vectors, to allow content-based searching in large image
collections. A content-based visual retrieval system
processes the information contained in the image data
and creates a numerical abstraction of its content in
terms of visual attributes by computing feature vectors.
Any database query operations deal solely with this
abstraction rather than with the image itself. For every
image in the database feature vectors are extracted
automatically and stored in a signature for this image as
content-dependent metadata.
Feature extraction is the crucial step in content-based
image retrieval. The choice and combination of proper
features is responsible for the quality of an image
query. The most common perceptual features used in
CBIR systems are color, texture, shape and spatial
relationships. Evaluation of spatial information
contained in images can improve local and global
representations of features for image retrieval [4].The
following paragraphs describe the mentioned features in
more detail [3]:
Color features: Color is related to chromatic attributes
of an image. The distribution of colors is given by a
color histogram which is obtained by discretizing image
colors and counting how many pixels belong to each
color. The color histogram of each image of the
collection is analyzed and stored in the database. The
matching process retrieves those images whose color

289

II

I

I

I.

I

I

I

M

m - n

.

>

3

signatures

b. Generated subintervals according to distribution of images
Figure 2. Comparison of the generated partitions
in the feature vector space. Query types may be
classified into three levels of complexity. Level 1
represents queries on the basis of low-level features
(Find the pictures that look similar to my query image),
level 2 comprises queries for logical features (Find a
picture of the White House) and level 3 stands for
queries including abstract features (Find a picture that
expresses sad feelings) [ 11. CBIR mostly covers level 1
queries whereas level 2 and level 3 queries in most
cases need a combination of image and textual queries.

Semi-automaticclustering
If the maintainers of a database have good knowledge
about the image content, they can decide on the desired
clusters beforehand. For each of the clusters, one
representative image is used as query input for the
CBIR system. Assuming that the vector containing the
weights assigned to the different features is set to the
appropriate values, the system retrieves a set of images
with a varying degree of similarity. Every image up to a
certain threshold representing the maximal allowed
difference to the example is considered as part of a
cluster. Note that there will probably be some images
that are incorrectly considered part of a cluster. This
approach does not generate perfect results, but it
reduces the necessary manual work, since it is easier to
remove images from a selection than to create different
sets out of a great mass of images.

3. Structuring image databases using
CBIR
Much work is necessary during the process of preparing
an image database for further use. In many cases the
images will cover a great variety of topics and most of
them will not be accompanied by textual metadata. The
expenditure required to capture the data, e.g. to divide
them into clusters of similar content or providing them
with keywords can be immense. A database
management system including CBIR facilities can help
to speed up this process.

3.1

Automatic clustering
If the image content of the database is not known in the
degree necessary to predefine clusters, the CBIR system
an still provide further information about the database
content.
The signatures of the images are a string of bits
representing a very large number. So all the signatures
can be seen as part of an interval. To get a first
knowledge of the content of the database, maintainers
could partition this interval into even sized bits. After
doing this, there will be some intervals containing no
signature, while others will be containing one or more
of them (see Fig. 2a). The median of the signatures of
each of these sub-intervals can then be considered as a
representative for the interval. The maintainers can gain
a starting insight into the content of their database by
browsing these representatives. This is only a very
simple approach.
A more rewarding approach would be to take the
distribution of the signatures across the interval into

Clustering

Clustering (in our application context) is the process of
ordering the images of an image database into sets of
similar content. The main benefit of clustering in a
CBIR context is the ability to reduce the amount of
images to be searched for likely retrieval candidates. As
a result the response time of the system is faster since
less comparisons have to be made. But even in the stage
of setting up the database, maintainers can benefit from
clustering.
There are two approaches to clustering, depending on
the amount of prior knowledge of the content of the
database.

290

account before creating the partition. This gives the
ability to modify the size of the sub-intervals according
to the actual image content (see Fig. 2b). As a side
effect, empty intervals are avoided.
The clusters gained through application of automatic
clustering can either be used as a basis allowing the
selection of the truly relevant clusters by semiautomatic clustering or as the final clusters themselves.
Images which seem to form a cluster of their own
should be grouped into one ‘miscellaneous’ cluster in a
final step when choosing the latter, in order to avoid
cluttering the database with clusters.

3.2

photographs, paintings, holiday pictures and business
charts. We were not only interested in the overall
strength of the retrieval engines, but also in the process
itself: How much knowledge of the images is needed to
get good results? Is it difficult to get the right values for
the different search parameters and what impact do
adjustments to the search parameters have on the
results?

4.1

All three commercial CBIR systems were tested with
our image database. The retrieval results of the three
systems are rather similar. It is not possible to explain
the algorithms used by the three CBIR systems in detail
since the source code of these commercial systems is
not freely available and no related information is
published. Due to this restriction it is very difficult to
explain the retrieval results and the effects of the
different parameters. The following feature extraction
functions are supported by Visual Retrievalware 171:
Color Content: This feature represents the color
histogram of the image.
Shape Content: This feature represents shapes in
the image. Only the direction, curvature and
consistency of edges is measured, the position in
the image is not considered.
Texture Content: This feature represents
information about the texture content of the image.
The position in the image as well as differences in
color, brightness and rotation are irrelevant.
Brightness Structure: This feature represents
contrasts in brightness in the image.
Color Structure: This feature represents contrasts in
the color structure of the image.
In our test application it is possible to vary the value of
each parameter between 0 and 5. As an example figure
3 - figure 6 present the retrieval results from Visual
Retrievalware [7].
The result of the retrieval process are groups of images
which can be used to create clusters. The two groups of
images generated by the CBIR system with parameter
set 1 [Fig. 31 and parameter set 3 [Fig. 51 are
appropriate for the attachment of textual metadata to all
images in this group. Nevertheless there is a problem
using CBIR systems for Even minor changes of
parameters cause highly different retrieval results. As
shown with parameter set 2 [Fig. 41 and parameter set 4
[Fig. 61 the retrieval results depend directly on the
selected parameters. A parameter set that achieves good
retrieval results for a specific comparison image is not
necessarily useful for a different comparison image.
Depending on the used CBIR system there is a different
number of parameters which need to be adjusted during
the retrieval process. The selection of a suitable

Attaching textual metadata

CBIR should not be seen as opposed to retrieval using
keywords, but rather as augmenting the possibilities to
retrieve data from multi-purpose databases. Using
CBIR, the work required for attaching metadata to
images can be reduced, too. If the image database has
been divided into different clusters using the
approaches outlined in 3.1, the task of adding metadata
[ 5 ] becomes relatively simple. Since the images of a
cluster share the same general keywords, adding these
keywords to the collection of images can be done using
a single update query. This saves much time that
otherwise would have been spent adding the same
keywords again and again.
Although current CBIR systems are not yet able to fulfil
level 2 or level 3 queries, the ability to attach keywords
to large amounts of images simultaneously allows
textual retrieval engines to fulfil such queries on
images, e.g. if a cluster of images showing sunsets by
the sea is labeled ‘romantic’, a query concerning
romantic images can be satisfied quite easily. This way
some limitations of current CBIR systems can be
avoided by using the strengths of CBIR.

4.

Test results

Proof of concept using commercial
CBIR systems

We concentrated our efforts on three commercial CBIR
systems. These are the Virage Search Engine by Virage
Corporation [ 6 ] , the Visual Retrievalware by Excalibur
Technologies Corporation [7] and QBIC by IBM
Corporation [SI.
These systems are general purpose CBIR systems. We
do not want to restrict ourselves to special kinds of
images (e.g. Faces [9]). Many research projects at
universities deal with content-based visual information
retrieval software as well but have not yet left the
prototype status.
The image database for our tests consists of roughly
140 images, taken from various sources and with highly
different content. Among those images were passport

29 1

Figure 3. Example 1, Parameter Set 1
Color Content: 1, Shape Content: 0, Texture Content: 0, Brightness Structure: 3,Color Structure: 3

Figure 4. Example 1, Parameter Set 2
Color Content: 0, Shape Content: 0, Texture Content: 0, Brightness Structure: 2, Color Structure: 4

Figure 5. Example 2, Parameter Set 3
Color Content: 0, Shape Content: 0, Texture Content: 3,Brightness Structure: 4, Color Structure: 0

Figure 6. Example 2, Parameter Set 4
Color Content: 2, Shape Content: 0, Texture Content: 0, Brightness Structure: 0, Color Structure: 5

292

parameter set for a comparison image is a rather complex
task and takes even the experienced user 5 - 10 queries
with different parameters. As a result of the complexity of
this task it is not possible to automate it with present
CBIR systems.

In our future work as part of digital library projects we
will elaborate alternative models to optimize the results of
automatic clustering. Secondly we plan to implement
different graphical representations for our clusters, e.g.
topic maps [lo].

5. Conclusion

References

The amount of visual content in digital libraries is
increasing, in most cases this information is not
structured. In consequence, tools are needed to assist in
structuring image databases, for example in the
production of content-dependent metadata.
We suggest a new approach using commercial CBIR
systems to organize databases by grouping similar images
to clusters in content. This may be a first step in the
course of capturing visual data. Providing a whole cluster
with the same fitting keywords - applicable to all images
of the cluster - could be the second step to attach
additional textual metadata to large archives. This
approach is no replacement for a detailed hierarchy of
bibliographic metadata but is helpful if there are no
resources available to provide each single image with
descriptive information.
We are aware of the limits and the strength of commercial
CBIR systems. Forming the optimal query for the semiautomatic clustering demands knowledge of the CBIR
system on the one hand and knowledge of the database
content on the other hand. More information about the
algorithms used could facilitate the work even more. Both
types of clustering, the semi-automatic as well as the
automatic clustering highly depend on the offered features
of the CBIR systems. Nevertheless the results of CBIR
systems are useful for experienced users and appropriate
for maintainers of image databases to structure their
content. After clustering the digital library the users can
get a quick overview of the visual content and are able to
reduce their search for an image to selected clusters to
improve the quality of the query’s outcome.

J. Eakins, M. Graham, Content-based Image
Retrieval, University of Northumbria at Newcastle,
JISC Technology Application Program, Report 39,
1999
M. Bogen, C . Bonkowski, M. Borowski, J. Loffler,
Digitizing a Cultural Heritage - The Key Issue for
Preservation and Electronic Publishing, submitted
to WebNet 2000
A. Del Bimbo, Visual Information Retrieval,
Morgan Kaufmann Publishers, ISBN 1-55860624-6,1999
J.R.Smith, S.F.Chang, VisualSEEK: a fully
automated content-based image query system,
Proceedings of 4* ACM International Multimedia
Conference, pp. 87-98, 1996
S. F. Chang et a1 (1998) “Semantic visual
templates: linking visual features to semantics” in
IEEE International Conference on Image
Processing (ICIP’98), Chicago, Illinois 53 1-535
The Virage Home Page: httu://www.virage.com/
http://www.oracle.com/database/options/index.
htm
l?/database/or,tions/visual.html
The Excalibur Home Page:
httr,://www.excalib.com/
The QBIC Home Page:
httv://wwwqbic.almaden.ibm.com/
httv://www.viisage.com
ISO/IEC 13250: September 2 1, 1998: Topic
Navigation Maps

293

