Internet-Based Teleoperation Using VE Modelling Based on Single Images
JiaCheng Tan,Gordon J. Clapworthy
Department of Computer & Information Sciences, De Montfort University,
Mdton Keynes MK7 6HP,United Kingdom
jtan G? dmu.ac.uk, gc C? dmu.ac.uk
interventions, or simply because the environment
evolves in an unpredicted way - such dynamic factors
potentially exist in any teleoperation system. Obviously,
proper interpretation of these dynamic factors is crucial
to subsequent task operations.
For these reasons, VR is seldom used alone for
teleoperation purposes. It is usually combined with a
certain amount of video transmission because video
images naturally represent dynamic changes to the
environment. By using the added information provided
by the video images, the teleoperation operator can
easily identify any significant change of the environment
and adjust his operations accordingly.
However, simply putting together the VR and video
image displays does not make much sense, for as long as
the changes are not appropriately integrated into the VE,
the part of environment that has changed cannot be
manipulated in the same sense as the remaining parts of
the VE, and the advantages afforded by VR cannot be
utilised in these regions. This is a crucial problem when
VR is used for dynamic environments such as those
encountered in teleoperation.
At present, augmented reality (AR) is frequently
applied to tackle such problems, Rastogi & Milgram [3],
State et al. [4]. Here, the model-based graphics contents
are not affected by any unexpected environmental
changes - real images of the changed parts of the
environment are imposed on to the VE and are presented
to the teleoperation operator through stereoscopic
display. However, the constraints associated with image
transmission and display mentioned previously make AR
difficult to apply in Internet-based teleoperation.
In our work, we have pursued a light-weight
modelling method based on a single image to capture the
distinctive features of environmental changes so that online VR editing can be realised. To achieve this, we need
a fast, but effective, method for image analysis and for
object modelling. This paper introduces the work we
have undertaken in this area.

Abstract
Virtual Environment (VE) techniques provide a
powerful tool for the visualisation of the 3 0 environment
of a teleoperation work-site, particularly when “live’’
video display is inadequate for the task operation or its
transmission is constrained, for example by limited
bandwidth.
However, the ability of VE to cope with the dynamic
phenomena of typical teleoperation work-sites is
severely limited by its pre-defined model-based nature.
Thus, an on-line composing mechanism is needed to
make it environment compliant. This paper describes a
method for this purpose, based on workspace analysis, in
which objects are constructed from simple 2 0 images,
and VEs are modelled interactively. Experiments have
shown that the method is convenient and effective for online VE editing.

1. Introduction
There exist severe restrictions on video display when
used for Internet-based teleoperation, particularly in
relation to the frame rate (that is, the rate at which the
view is updated with information from the remote site).
To overcome such restrictions, virtual environment
(VE) techniques have been employed to re-create the
remote work scene and present it to the teleoperator,
Belousov et al. [l], Tan et al. [2]. By using virtual reality
(VR),the opetator can experience the task operation as if
he were present at the work-site. One advantage over
live video is that a teleoperation system using VR has
greatly-reduced requirements relating to communication.
However, despite this advantage, the model-based
nature of conventional computer graphics makes VR
inadequate to handle the uncertainties of changes to the
environment which may take place during teleoperation.
Such changes may be caused by accidents, by external

485
0-7695-0743-3100
$10.000 2000 EEE

necessarily derived directly from the object itself, can be
obtained by the study of linear object classes.
Relying on image irradiance analysis and working on
either single or multiple images, the shape from the
shading method, Haddon 8z Forsyth [ 1I], can produce a
rough 3D model. However, exact reconstruction requires
precise knowledge of the scene, such as the positions arid
reflectance of all surfaces, including those out of t k
field of view of the camera.
Tour into the Picture, Horry et al. [12], recover:; the
structure of the 3D world by specifying the vanishing
point of the scene. The method is able to create a simple
3D model of the scene so that a 3D feeling of “walking
or flying through” the 2D picture can be achieved in
animation.
Unfortunately, none of these methods can be used in
our application, because the modelling of a VE requires
both qualitative and qualitative descriptions of the
objects. Further observation suggests that, to recover 3D
structures successfully from a single image, either the
object to be recovered possesses special geometric
properties, such as symmetries, or adequate knowledge
about the object can be acquired for a specific scenario
such as that of teleoperation. In this way, some
compensation can be made for the information lost in the
imaging process.
In this respect, this paper describes a technique
similar to that of Debevec et al. [ 131, but rather than first
defining a model for the object and then solving its
parameters, we choose to take the object as consisting of
a number of geometrical primitives. With the help of
workspace analysis, we are able to obtain the parameters
of these primitives, and thus assemble the model using
the primitives. As the method does not involve explicit
object modelling, it is more efficient for on-line use.

2. Related Work
In general, recognition of a 3D object requires two or
more appropriately-defined 2D images, and many
methods have been proposed for this purpose, such as
structure from motion, Seitz 8z Dyer [5], Taylor 8z
Kriegman [6], stereo correspondence matching, Mayhew
& Frisby [7] and shape from silhouettes, Szeliski [8].
The methods in this category, and their variations, can be
described as a four-step process.
a feature point of interest is identified in one image.
the correspondence of the feature is found in the other
images.
the disparities between the image points are measured
by using the parameters of the camera systems with
which the images are taken, the actual depth of the
feature point is computed
In an on-line application, these methods are often
subject to one or more limitations. There may be a
difficulty in finding the correspondence between one
image and the others. A trade-off between efficiency and
accuracy is often necessary, so that sufficient accuracy
can be achieved while avoiding large computational
overheads.
Another limitation is that the methods are very
sensitive to errors in correspondence matching or in the
calibrated camera parameters. In most cases, accurate
calibration is hard to achieve. If the camera is mobile, for
example mounted on the robot arm or other mobile base,
it is perturbed as it moves through the environment, so
accurate calibration may be difficult to maintain.
Further, in the absence of adequate knowledge and
reasoning about the scene, recovering the full 3D
information of a complex object potentially requires
dozens of images of the object from different view
positions. In this sense, some authors even doubt the
feasibility of using methods such as stereo matching to
produce 3D information that matches the object models
sufficiently accurately, Grimson [9]
Considering the above limitations and the scarcity of
the source images in our application, we sought to
produce a modelling method based on single images. As
one dimension is lost in the imaging process, it is
impossible fully to recover a general 3D object from a
single image. For this reason, single-image-based
methods are usually applied where exact explicit 3D
models are not absolutely necessary, such as in human
face recognition in computer vision and in the generation
of new views of a 3D scene in computer graphics.
There exist several different methods suited to such
purposes. The linear class casting method, Vetter &
Poggio [ 101, concerns the generation of new views of an
object from a single image by exploiting known
information about the object. The prior information, not

3. On-line Camera Calibration
To extract the actual geometric information of an
object from a single image, we require the camera used
to be calibrated explicitly. For a given camera, the
intrinsic parameters can be found beforehand using
standard calibration methods, Tsai [14], so it is not
necessary to expose the teleoperation operator to the
intrinsic calibration process.
In general, the form of the external calibration
depends upon how the camera is mounted in the working
environment.
If the camera is rigidly fixed in the teleoperation
environment, its location is known, so the external
calibration, just like the intrinsic calibration, has to be
done only once.
However, if the camera is mounted on the robot
manipulator, and thus moves as the robot manipulator
moves, permanent calibration is not applicable. A

486

parameters, to find these parameters from Equation (2),
we need to know at least 6 world points and their
projections on to the image plane.
However, Equation (2) leads to a set of homogeneous
linear equations, so only eleven parameters are
obtainable, while the twelfth parameter remains as a
unknown scale factor of T,,,c,but this factor does not

mechanism for on-line adjustment of the external camera
parameters will then be needed. Sometimes, even recalibration may be necessary if the calibration accuracy
decreases as the camera moves in the environment or if
the camera is not permanently mounted and thus subject
to unexpected external perturbation.
Because a single image is used in our system, any
external calibration methods that relate to stereo
correspondence or require more than one image are
excluded, and the calibration has to be carried out with
the current image of the environment. Generally, finding
the external parameters of a camera system from a single
image requires sufficient absolute knowledge about the
scene. This requirement is usually difficult to meet for
many application scenarios that occur in computer
vision.
Fortunately, for teleoperation systems, we can often
find some objects about which we have prior knowledge,
for example the robot manipulator or part of it. By
assigning the feature points on the robot manipulator as
the calibration landmarks, finding the external
parameters of the camera is straightforward by using the
techniques described in Section 4.

4. Reverse Perspective Mapping
Workspace Analysis

affect the perspective projection.
To ensure that the resulting linear equations are
solvable, Equation (2) requires that the 6 points selected
are not co-planar on any plane that is parallel to one of
the coordinate planes, and that they should have distinct
projections on to the image plane.
Equation (2) defines a unique mapping from the
world points to image points, but what we need for 3D
reconstruction is the reverse, i.e., to find points in the
workspace from their perspective projections. As the
reverse mapping process is not one-to-one, to determine
a 3D point from its 2D image we have to reduce the
dimension of the workspace.
Rather than seeing an image point p(xim,U,) as the

p ( x , y , z ) in the 3D workspace,
we take it as the image of a point pn(x,y,Z)on a

projection of a point

and

plane

Il

n : a,x+b,y+c,z+d, = o

Suppose that a point in the world coordinate system

p ( x , y , z ) is expressed as p(x,,y,,z,) in the
camera coordinate system , and that p(x,, ,yim) is its

or a point

p L(t)

x = x,

prospective projection on the image plane. For the pinhole camera model, the following equations hold:

on a line

(3)

L

+ k,t
(4).

z = z, + k,t

Y

x, =-*im

f

zc

By using the chosen reference planes or reference
lines, the reverse mapping can be found. If the reference
planes are chosen, Equation (2) produces two linear
equations about x, y and z . These equations, together
with the equation of the reference plane, give the world
coordinate of the image point p(xim,yim) .

Y im

=fZ'
where f is the focal length of the pin-hole camera.
Further, we suppose that the external parameters of
the camera are expressed by the transform between the
world coordinate system and camera coordinate system,
Tw,c= [ u ~ ] ~With
, ~ .TW,,,the perspective projection

+ b,y + c,z + d, = 0
a,x+b,y + c,z + d, = 0
a3x + b,y + c,z + d, = 0

a,x

can be defined as

If the reference line is chosen, the image plane
degenerates to image lines, according to Equation ( 2 ) , a
mapping from 2D line Lim

TW,,can
calibration.

be obtained through the external camera

As

TW,, contains twelve unknown
.

487

This rendering engine works as a VR editor. When an
image is downloaded from the server at the robot
worksite, the modeller performs the external calibration
for the camera if it is necessary. The result of the
calibration is a workspace model.
Superimposed on to the image, the workspace model
makes the 2D image 3D navigable, allowing the
reconstruction work to be done. All the work concerned
with object generation and deformation, and the
selection of position and orientation, is performed in this
model. The appearance of the model, as generated,
depends upon the visual characteristics of the object. If
the object is uniformly coloured, that colour is assigned
as the colour of the model, otherwise a texture mapping
is performed using the portion of the image occupied by
the object.
Once the geometric properties, the appearance
attributes, or the motion characteristics of an object are
determined, the object is converted into a standard
Java3D node and is inserted into the living scene graph
for rendering. Because the VR model and the modeller
work in an interactive manner, we can either choose an
object from the living VE for editing, or generate an
object to be inserted into the living VE.

exists to the 3D line

x = x0

+ k,t

Y = Yo + kyt
z = zo + kzt

(7)

As the reverse mapping is based on establishing a
series of references, if a single image is used we require
that the object to be reconstructed should possess some
geometric references within the workspace and should
not be freely floating somewhere within it.
With the reverse mapping available, the 3D
interpretation of the image is straightforward. In our
application, the teleoperation environment consists of a
number of static and non-static objects. By static we
mean that the location and the geometric attribute of the
objects are not changing or tractable at run time. The
robot manipulator and any permanently-mounted
accessories belong to this category.
In addition to these, the environment may contain a
number of objects to be manipulated. As it may be
possible that their attributes may change - their position,
their number, their shapes, or even other attributes
beyond the control of the teleoperation operator - the
objects are reconstructed at run time.
According to the approach described in this section, a
typical reconstruction process includes:
1. find a suitable reference for the object so that its
location can be determined;
2. from the model set, choose the most appropriate
geometric primitive for the object or part of it;
3. adjust the parameters of the primitive so that it suits
the object;
4. assign the appearance or motion attribute to the
object.

6. Experimental Results
To verify our reconstruction method and the
principles upon which the modelling system is
established, an off-line modelling experiment using a
sample image has been conducted.

5. Implementation
Basically, the system consists of a VE model and an
image-based modeller. To make the system accessible
over the Internet, it is implemented in pure Java, using
the Java3D and the Java Advanced Imaging (JAI) APIs.
The VE model is a standard Java3D application that
provides the facilities for manipulation control, view
platform control and manipulator trajectory planning.
The modeller is implemented as a separate self-defined
rendering engine for the rendering of 3D wire-frames.
To achieve a rapid system response, the objects
defined for this rendering engine are highly simplified.

Figure 1. Sample image used for object
reconstruction

488

In the sample image shown in Figure 1 (captured
from a Java3D-based VE), we have some simple objects
such as blocks and a cylinder, and a complex object - the
robot manipulator. To test the reconstruction method,
these objects will be reconstructed by using the modeller
(we do not need to model the robot manipulator at run
time, because it is well modelled as a basic element of
the VE).
It has proved to be extremely easy to reconstruct an
object for which a reference is clearly defined, and
which can be easily found, such as for the blocks on the
floor, or for the base and trunk of the manipulator
(vertical cylinders).
In other cases, the reconstruction process is .rather
complex, as for the shoulder (horizontal cylinder on the
top of the trunk) and the arm components. Here, we have
to find the references for these objects upon an object
that has been reconstructed. Sometimes, additional
information is needed, for example, to determine the
position of the shoulder, we have to presume that its axis
is at a right angle to the axis of the trunk. As expected,
the method is unable to reconstruct the two suspended
balls because we cannot find location references for
them within the workspace - their point of attachment is
outside the view and, thus, it is impossible to determine
their depth.

objects rather than static image patches. Once they are
registered in the VE, they are “sensible” to the VE, and
the teleoperation operator can interact with them and the
system can track them in the trajectory planning for the
robot manipulator.

Figure 3. VE obtained from a single image

This trial application demonstrates that the modeller
is effective and applicable for on-line modelling of the
VE if the object to be modelled is reasonably simple. In
fact, by using the flexibility of choosing reference planes
or lines, the user can easily construct complicated
objects as long as the relationships between the objects
are clear and the geometric details of the objects can be
reasonably reflected in the image.

7.Discussion
The work undertaken has established that the method,
based on image analysis of the workspace and
employing on-line VR editing, is a feasible way of
dealing with a fairly wide range of dynamic phenomena
within teleoperation. Compared with approaches such as
image augmentation (AR) or automatic modelextraction, the method is more effective, more reliable
and computationally more economical for this particular
application.
By integrating the environmental changes into the
VE, the method supports a fully model-based VE for
teleoperation, so that the changes can be properly
presented in task operation and in task planning. Besides
the use in teleoperation, the method is also suitable for
constructing virtual architectural structures where the
relationships between various objects are more clearly
defined.

Figure 2. Constructed wireframe objects

The results of the modelling process are shown in
Figure 2 where the objects are expressed in the wireframe form. For clarity, the background (sample image)
has been filtered out.
Figure 3 shows the corresponding 3D VE
reconstructed. As there is an explicit model for each
object in the navigable VE, these objects are “live”

489

C.J. Taylor and D.J. Kriegman, “Structure and Motion
from Line Segments in Multiple Images”, IEEE Trans.
on Pattem Analysis. Machine Intelligence, 17(1 I),
November 1995.
[7] J.E.W. Mayhew and J.P. Frisby, 30 Model Recognition
from Stereoscopic Cues, MIT Press, 1991.
[8] R. Szeliski, “From Image to Model (and beyond): A
Personal Retrospective”, Vision Interface 97, Keloma,
B.C, May 21,1997.
[9] W.E.L. Grimson, Why Stereo Vision is Not Always About
3 0 Reconstruction. MIT A.I. Memo No. 1435, July

Several improvements and extensions can be made to

[6]

our current work and we hope to proceed with these in
the near future. A major one should be the integration of
automatic alignment. At present, this is totally reliant on
the operator’s observation and experience. The problems
associated with single-image-based methods, for
example, the projection ambiguity problem and the
occlusion problem, suggest the use of two or more
images.

References

1993.
[lo] T. Vetter and T. Poggio. “Linear Object Classes and
Image Synthesis from a Single Example Image”, IEEE
Trans. on Pattem Analysis and Machine Intelligence
(PAMI), 19:7, 1997, pp733-742.
[l 11 J. Haddon and D. Forsyth, “Shading Primitives: Finding
Folds and Shallow Grooves”, lntemational Conference
of Computer Vision, Bombay, India, January 1998.
[12] Y. Horry, K. Anjyo and K. Arai, “Tour into the Picture:
Using a Spidery Mesh Interface to Make Animation from
a Single Image”, ProcSIGGRAPH’97, Los Angeles,
Califomia, August 3-8, 1997, pp .225-232.
[13] P.E. Debevec, C.J. Taylor and J. Malik, “Modeling and
Rendering Architecture from Photographs: A Hybrid
Geometry- and Image-based Approach”, Proc.
SIGGRAPH’96, New Orleans, Louisiana, August 1996,
pp 11-20.
[14] R. Tsai, “A Versatile Camera Calibration Technique for
High Accuracy 3D Machine Vision Metrology Using
Off-the-shelf TV Cameras and Lenses”, IEEE Journal of
Robotics &Automation, 3(4), August 1987, pp 323-344.

I.R. Belousov, J.C. Tan and G.J. Clapworthy,
“Teleoperation and Java3D Visualization of a Robot
Manipulator over the World Wide Web”, Information
Visualization 99, IEEE Computer Press, 1999, pp 543548.

J.C. Tan, I.R. Belousov and G.J. Clapworthy, “A VirtualEnvironment-Based User Interface for Teleoperationof a
Robot Using the Internet”, Proc. 6‘* UK-VRSIG ConJ
Salford, England, 14-16 September, 1999, pp 145-154.
A. Rastogi and P. Milgram, “Augmented Telerobotic
Control: A Visual Interface for Unstructured
Environments”, KSB Robotics ConJ October 16-18,
1995.

A. State, G Hirota, D.T. Chen, W.F. Garrett, M.A.
Livingston “Superior Augmented Reality Registration
by Integrating Landmark Tracking and Magnetic
Tracking”, Proc. SIGGRAPH’96, New Orleans, 1996,
pp 429-438.

S.M. Seitz and C.R. Dyer, “Complete Scene Structure
from Four Point Correspondences”, Proc. Sth Int. Conf
on Computer Vision, Cambridge MA, 1995, pp 330-337.

490

