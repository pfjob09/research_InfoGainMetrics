Wayfinding in Virtual Environments Using an Interactive Spatial Cognitive Map
Rameshsharma Ramloll & Darren Mowat
Computing Science Department, University of Glasgow, G12 8QQ
{ ramesh,mowatd@dcs.gla.ac.uk}

Abstract

landmarks that can be interactively viewed. They have
been proposed to deal with the lack of depth cues and
context needed to reliably recognise landmarks. In our
opinion, the strength of this approach lies mainly in the
fact that the user can have access to landmarks from
multiple vantage points thereby increasing their
likelihood to be recognised, and thus to become useful.
We address wayfinding problems in large-scale
virtual environments with an approach based on
interactive spatial cognitive maps to the wayfinding
problems in large-scale virtual environments. A spatial
cognitive map [5] has been defined as the body of
knowledge of a large-scale environment constructed by
integrating observations gathered over time, in order to
find routes and determine the relative position of places
[ 1 I]. The cognitive map of space is held in the mind, a
physically unobservable structure of information that
represents spatial knowledge [IO]. This is distinct from
the geometric model of space which results mainly from
direct measurements, is public and directly accessible for
mathematical manipulations. Our strategy relies on ( I )
making this spatial cognitive map as public as possible
and (2) enabling its direct manipulation to facilitate
wayfinding. The strategy we propose is centred on the
concept of defining place as a sensory snapshot. A place
is “wherever you are when you experience a given
sensory image” [ 121 rather than simply a set of Cartesian
coordinates. We wish to stress at this early stage that we
are making no claims about the cognitive processes that
really happen in the mind when spatial information is
captured or stored. Rather, what we are proposing is an
aid, intended to be situated between real world
navigation processes, and all those hidden processes
inside the mind.
Typically, a large-scale 3D virtual environment,
such as a hyperlinked VRML world, is explored using a
3D browser such as CosmoPlayerTM(Figure 1 , arrow 1).
During this exploratory phase, the navigator constructs a

We illustrate how an interactive spatial cognitive
map representation of a large-scale virtual space under
navigation can be constructed in order to help solving
navigation queries and formulate navigation commands.
We explain our design, describe its implementation and
present the evaluation results from a sample of 6
navigators. Our balanced-within experiment shows that
our implementation results in a signijicant decrease in
the physical demand experienced by participants to
tackle set navigation tasks. This paper also introduces a
novel algorithm for the capture and organisation of
spatial knowledge resulting from navigation in a virtual
environment. This algorithm is the basis of our
navigation aid.

Keywords
Large-scale virtual worlds, cognitive map, wayfinding,
direct manipulation, subjective workload

1. Introduction
Large-scale virtual environments [ 131 include
detailed simulations of complex architectural constructs
such as oilrigs and multi-storey buildings; others are
more abstract in nature. Examples of the latter include
spatialised databases such as populated information
terrains [ l][S]. Designers propose tackling wayfinding in
these environments using strategies based on studies of
navigation in the real world [14]. It has been argued that
making virtual environments legible is a promising
solution to wayfinding problems [3][4] experienced in
such spaces. This has led to attempts to automate the
process of making abstract large-scale virtual worlds
legible [ 9 ] . A more recent attempt to facilitate
wayfinding is based on the concept of worldlets [ 6 ] .
These are 3D representations of virtual environment

0-7695-1195-3/01 $10.00 0 2001 IEEE

574

inter-relationships between them. We show shortly the
importance of this abstract construct in describing the
organisation of sensory snapshots into the cognitive web.
Two main constructs of SSDs are space-nodes and
transit boundaries.
(i) Space-node
Space may be subdivided into functionally distinct
regions, each of which is referred to as a space-node.
Space-nodes, rather than focusing on parvsubpart
relationships between spatial regions, emphasise more
the transitory relationships between them. The strategy
for dividing a large-scale environment into space-nodes
depends on the designers of the virtual environments.
For example in our case, the size of the file, the
complexity of the environment to be represented and the
meaningful organisation of 3D objects in the virtual
space determined the choice of space-nodes.
(ii) Transit-boundary
This is the interface between two space-nodes. For
example, a door may be regarded as being a transit
boundary between two spaces ascribed to different
functions, such as two different rooms or a room and a
corridor. A transit boundary is constrained to be
unidirectional. Thus, a door, which allows bi-directional
navigation between two space-nodes, according to this
model, should be considered as two opposite transit
boundaries. A transit boundary is represented by a
directional edge with the additional property that it
cannot loop back to the node it is leaving. In the example
described in Figure 2 , rooms A, B, C, and Outside are
chosen to represent 4 functionally distinct spaces. Figure
3 shows its corresponding SSD.

private representation of the place 'in the mind'. This is
the so-called spatial cognitive map of the place (Figure
1 , arrow 2). Our aim is to capture, store and represent
the latter. This is achievcd by capturing sensory
snapshots and relevant navigational inter-relationships in
order to create a hypermedia representation of the
cognitive map, which we here after refer to as a
cognitive web of the navigated space (Figure I , arrow
3). The latter is not useful if the information it contains is
not easily accessible. A cognitive web browser is needed
to access such spatial information about a 3D virtual
environment (Figure I , arrow 4). The browser fulfils the
following functions: the maintenance and repair of the
cognitive map (Figure 1 , arrow 5) and the formulation of
navigation commands for autonomous navigation
(Figure 1, arrow 6).

,
Outside

j

Figure 1 Overall strategy to tackle wayfinding
problems in large-scale virtual worlds

2.Creating a cognitive m a p representation
A mechanism for capturing and organising sensory
snapshots is developed to create the cognitive web. The
latter can then be accessed and directly manipulated for
information retrieval purposes and for the formulation of
navigation commands or queries. Before describing the
mechanism, it is necessary to define a few abstract
constructs to facilitate the description of our approach.

Figure/;! Floor plan of a very simple building
./'

',

2.1 Necessary abstractions for explaining our
spatial information gathering strategy
We propose Space Structure Diagrams (SSDs) to
represent large-scale spaces using graphs. A Space
Structure Diagram is a directed graph used to represent
distinct spatial regions in 3D worlds and the navigational

Figure 3 SSD representation of the section
described by the floor plan in Figure 1

575

We now construct a browsable representation of
the cognitive map based using:
(i) Entry transit boundaries: Transit boundaries that
allow a space-node under consideration to be visited
when crossed.

a

(ii) Exit transit boundaries: Transit boundaries that
allow a given space-node to be exited when crossed.

P

(iii) Views: Special point of views that reveal salient or
landmarked [sic] sensory data of interest to the
navigator within a given space-node.
In our current implementation, 4x4 cm2 pictures
represent each of these entities which constitute the
building blocks of the cognitive map representation.

Figure 4 Space-node Sa linked to space-node

2.2 Representing transit boundaries and
Views using pictures

S, through a transit boundary

Entries,
Exits
and Views are pictorial
representations of entry transit boundaries, exit transit
boundaries and views respectively. Each space-node Si
has 3 sets associated with it, namely an Entry set N j ,
Exit set X i and View set
Space-node, S i : is
defined as a functionally distinct spatial region; Entry
set, N i : contains sensory snapshots of entrances to
space-node
; Exit set,
contains sensory
:
snapshots of exits from space-node S, ; View set,
contains salient sensory snapshots specific to a
navi ator's interest within space-node S i , where

On traversing the transit boundary of

ea,,,l

s,

xi:

vp.h '

where

k is the index of the view captured in

space-node S, to illustrate his particular interests. These
are then collected into the View set,

p

boundary where G ! , p , / Z E {0,1,2
,...} and a #
(recall that no self-referencing transit boundary i s
allowed in our model). More specifically, the latter is a
sensory snapshot of the transit boundary between
S, and
from within S, (Figure 4). Since there may

vp , of the relevant

space-node (Figure 4 rule 3). In Figure 4, the View was
created in S , .

2.3 Example illustrating the capture and
organisation of pictorial information about the
virtual environment

s,

S, and

N,

(Figure 4 rule 2). When carrying out explorations within
a given space-node, the navigator can create Views,

v,

(Figure 4) define a transit

be more than one transit boundary between

1) and a copy of the most recently

generated Exit is made and stored in the Entry set

i E f0,1,2,
...I.
Pictures,

is created and collected in its Exit set,

x, (Figure 4 rule

vj.

S,, an Exit

s, ,

We use a simple navigation scenario to show how
the contents of the relevant sets for each space-node
evolve according to the rules described earlier. Figure 5 ,
shows a navigator moving through four chosen spacenodes, Outside, room A, then B, and finally into C,
through 3 transit boundaries, Door A, Door B and Door
C. We examine the generation of Exits, Entries, and
creation of Views to demonstrate our method of
capturing and organising pictorial representations of the
virtual environment. Note that in this scenario, the
navigator moves through points P, to Px,creating Views
at P3 and P6, interacting with transit boundary at P4 etc ...
Please note that Entries and Exits are generated only
during the first traversal of a transit boundary in a given
direction.

we introduce an additional index variable /Z to
distinguish between them.
While Views are created at will by the user, Entries
and Exits are produced automatically when the navigator
proceeds through a transit boundary from one spacenode to another. These pictures are collected in three sets
according to the following rules. Say, a navigator is
to another one, S,
moving from a space-node,
through the transit boundary
meta-represented by
(Figure 4).

sa,

576

2.4 Cognitive Web Construction (CWC)
algorithm
The CWC algorithm describes a simple procedure
for capturing the spatial knowledge of a navigator during
his exploration of a given 3D virtual environment
represented as a SSD.
Initial conditions Consider a navigator in a given
starting space-node,

so.

N o = {arbitray} where

the arbitrary Entry Picture

illustrates the point of entry into the 3D virtual
environment; X , = { }; V , = { }.

siis currently being navigated{
(1) IF View
is captured THEN v , . is
~
added to the View set vi,i.e.
v,= vi n bi.o
,..., ,..}where

WHILE space-node
Figure 5 Navigating a very simple building

At Point PI,in space-node, Outside (0)
Entry set, N o = {arbitraq}An arbitrary picture is
used to represent the point of entry into the 3D virtual
environment. This represents the transition from the real
world to the virtual world. ]Exit set,
= {eo,4,0),

k

x[,

v,= { }.

THEN an Exit ei,,i,n
is generated and added to
the Exit set of space-node

Point PI, in space-node, S A ,Room A

={

AND a copy of this exit mark is inherited by the
Entry set of space-node
, N i , i.e.

s,

1.

N~ = N~ n { e i , i , i ) .

Point P3, in space-node, Room A

1;

(3) IF exploration is over STOP.

{v,

= {eO,A,O); ‘ A
{
‘A =
0 1 ’ VA,O is
created by the user to indicate his interest in that
particular part of the virtual environment.
NA

)//While navigator is in current space-nodc

‘A

X , = {eA,B,,3)
from Figure

A navigator can interact with the captured spatial
information through a cognitive web browser in various
modes. Recall that Views are captured during navigation
within a 3D scene while Entries and Exits are created
when moving through a transit boundary. In the ‘learn
mode’ the navigator uses the browser to maintain, repair
or query her own cognitive map. She may also rehearse
navigation in a novel environment, if the cognitive web
is borrowed from a peer. In the ‘command’ mode, the
representation of the cognitive map is accessed in order
to formulate navigation commands which are
subsequently satisfied through autonomous navigation,
with the navigator being transported to the destination
specified. In practice, the navigator will have to shift
frequently from one mode of interaction to another
according to her navigational needs.

4 rule 1;

= {vA,O/’

Point P5,in space-node, Room B

NE=

from Figure 4 rule 2;

s,

3. Interacting with the cognitive map
representation

Point P4, in space-node, Room A

N, =

s,, x,, i.e.

X i = X i n{ei,,,,}wherelE {0,1,2,...}

N , gets a copy of the most recently generated Picture
in x, (Figure 4 rule 2) N,, = {eo,, o}; x, = { };
‘A

{0,1,2,...} .

s,

is generated as result of the navigator’s interaction
with Door A (Figure 4 rule I); View set,

E

(2) IF a transit boundary is crossed for the first
time to reach an adjacent space-node
,

X , = { };

v, = { 1.
Similar operations and spatial information capturing
processes occur through P6,P7and P8 We generalise the
above scenario with The Cognitive Web Construction
algorithm which describes our sensory snapshot
accretionary mechanism more formally as follows.

577

language which provides the navigator with information
describing the current space-node, its constituent
landmarked locations and possible entries and exits. Just
as a reminder, Entries provide information on how I did
get there and Exits on how can I leave from here. The
cognitive web browser also allows the navigator to
rehearse navigation by directly manipulating its
interface. For instance, while her real position is static in
the actual 3D environment, she can click an Exit to
update the scrollable frames with the appropriate
pictures of the space-node that the clicked Exit leads to.
Altematively, she may click an Entry to load the pictures
of the space-nodes that lead to the current one. There is
also provision for the user to undo or redo these
operations as well by clicking the UNDO or REDO
buttons. In short, the interface is designed in such a way
that directly manipulating it allows the navigator to
discover,
question or check the interrelationships
between pictures thereby allowing him to get an idea of
the general topological structure of the virtual
environment.
We show how by interacting with the interface of
the cognitive web browser alone, the navigator is
allowed to visualise the corresponding navigation in the
actual 3D virtual environment. Figure 7 shows the
sequence of changes in the state of the interface as the
navigator clicks on appropriate pictures. Clicking
(a)
will cause the pictures of the space-node, Room B to be
loaded. In turn, clicking eBccauses that of space-node,
Room C, to be loaded (b). Note that this navigation
operation can be reversed by clicking eBc(c) and eAB
(d). Figure 8 describes the visualised navigation in the
actual 3D virtual environment if steps (a) and (b) are
followed. Another point to bear in mind in this example
is that the View V, has been created earlier during a tour
of space-node B. Thus, interaction with the interface
provides the navigator with a spatial language that gives
her an insight into the explored space.

In the following, we describe an example cognitive
web browser (Figure 6, arrow 4) to describe each mode
of interaction. Its interface consists of 3 scrollable
frames, the Views (Figure 6, arrow i), the Exits (Figure
6, arrow 2), and the Entries (Figure 6, arrow 3) frames
each containing members of the View, Exit and Entry
sets respectively. At any one time, all the pictures
belonging to only one given space-node are loaded into
their respective frames. We now show how the navigator
can interact with the interface of the cognitive web
browser in various ways.
1-

11

1

,/”

I

6

7

8 9 1 0

Figure 6 A cognitive web browser (5) next to a
traditional 3D world browser (11)

3.1 Capturing and storing cognitive map
Capturing and storing of the cognitive map occurs
during the manual navigation of the 3D virtual
environment using control buttons (Figure 6, arrows
6,7,8,9) of the 3D virtual environment Browser (Figure
6, arrow I I). During this operation, as explained earlier,
the system captures the cognitive map of the navigator
by generating appropriate Entries and Exits based on the
interplay between the navigator and the transit
boundaries. In addition, Views are also created as
representations of locations landmarked by the navigator
by clicking the View button (Figure 6, arrow IO). In this
way, an evolving cognitive web describing the current
status of her spatial knowledge is produced.

3.2 Repairing and querying spatial knowledge
To repair or query the captured spatial knowledge,
the navigator clicks the toggle button (Figure 6, arrow 5)
to toggle into the leam mode. In this mode, the position
of the navigator in the 3D environment remains fixed.’
The navigator can browse the captured cognitive web to
reinforce, correct, or query her own cognitive map. The
pictures ‘in the scrollable frames of the Cognitive Web
Browser together constitute a spatial descriptive

Figure 7 Direct manipulation of the cognitive
web browser

578

Figure 8 Navigator moving from A to B, touring
B, then moving to C

3.3 Transporting navigator to desired position
and orientation
In the command mode, accessed by clicking the
toggle button, interactions with the pictures drive
navigation in the 3D virtual environment. For example,
clicking a picture causes the navigator to be transported
automatically to the location where the 'sensory
snapshot' represented by the picture can be experienced.
Thus, if a navigator wishes 1.0 visit a particular location
of interest in the 3D virtual environment, she can d o so
by finding the appropriate picture while in the learn
mode. Once this is found, she can switch to command
mode and click the picture to be autonomously navigated
to the corresponding target location. In other words,
switching to the command mode by toggling the
appropriate button (Figure 6, arrow 4) informs the
system that from that poi.nt onwards, clicking the
pictures will cause the scene in the 3D virtual
environment browser to change to reflect the automated
navigation towards the location described by the clicked
picture.

,

.

4. Implementation
The Cognitive Web Browser and the 3D browser
were implemented using the Java3Dr" API and the
NCSA Portfolio which is a collection of utility objects to
use with Java 3 D programs. The development platform
was a Pentium 111 machine with 256 Mbytes of RAM
running W i n d 0 w s 9 8 ~ ~The
.
3D scenes were created
using 3ds m a x 8 and exported to a suitable format (.OBJ
format) that the object loader we were using
understands. This was much easier said than done
because of the difficulties we experienced in using 3D
objects that were not rendering correctly in our 3D
browser. In addition our colourful 3 0 scenes produced
in 3ds m a x 8 were rendered rather disappointingly in

579

monochrome within our 3D browser. This of course led
the captured cognitive map to be represented using
monochromatic images. To ensure that the capturing and
storing of pictures did, not inhibit,navigation in the 3D
world, a separate low priority thread was dedicated to
this task.
Users navigate within the 3D scene using the
cursor keys, Up arrow key for ,forward motion, Down
arrow key for backward motioq,. Left and' Right keys for
turning left and right. ,Hitting the.Enter.liey creates a
View, a thumbnail of the current scene displayed in the
3D world browser. Clicking 'hot-spot' objects e.g. doors
in a. scene causes ( I ) the creation of a d Exit, another
"thumbnail of the current sce,ne in the 3 D world browser,
for the Exit set of the current space node, (2) the creation
of an Entry, a.copy of the fast thumbnail produced, for
' the Entry set of the space-node to be visited and (3) the
loading of the new space-node in the 3D world browser.
Figures 9 and I O illustrate the look and feel of our
application. It is .to be noted that the implemented
interface is not identical to the one described in the
theoretical section of the paper. For example, the current
interface does not provide the undo-redo functionalities
for undoing and redoing manipulations of the cognitive
web browser because of limited time available for the
implementation and evaluation of the aid. We felt that
these features were not critical for evaluating our
approach to way finding problems.
The 'reset view button simply settles the navigator
at some appropriate position and orientation in a 3 D
scene. It functions as a panic button for users to press if
they are totally lost and cannot orient themselves
properly' in the virtual .space. The. .,'show the scene's
snapshots' button (see Figures 9 and I O ) was enabled
only in the learn mode and could be clicked to
synchronise the state of the cognitive web browser with
the space-node under navigation. For example, the user
may have emerged in a space-node, and then wandered
off for quite a while in the learn mode and wants to
synchronise the state of the cognitive web with that of
the 3D scene.

Figure 9 A view of the 3D browser (left) and cognitive web (right) where user, in the command mode,
navigates within a space-node representing a city

Figure 10 A view of the 3D browser (left) and cognitive web (right) where user, in the command mode,
navigates within a space-node representing a transport museum

580

participants in the aided condition (Ts = 2.449, p =
0.029 < 0.05). Hypothesis H1 is thus confirmed.
The mental demand experienced was higher in the
aided condition than in the unaided condition. However,
this.effect was not significant (Ts=-0.93, p = 0.196). H2
is'neither confirmed nor disconfirmed.
The overall workload experienced by participants
in the aided condition was less than in the unaided
condition. However, since this effect was not significant
(Ts = 0.059, p = 0.477), H3 is neither confirmed nor
disconfirmed.
While the average time to locate an object in the
first, second, third, and fifth tasks were less in the aided
condition; this was not true for the fourth task. Also on
the average, objects were located significantly faster in
the third task (T5=2.46, p = 0.028 < 0.05). It appears that
on the whole, locating the objects was faster with the aid
than without the aid. But formally we cannot draw any
conclusions because significance was not achieved either
way.

5. Pilot study
Participants: 6 participants took part in this pilot study
(5 males and 1 female, all with no special training in
navigating virtual 3D worlds, (averageage = 22.5 years).
Experiment Design: Participants had to navigate a
large-scale world consisting of 5 space-nodes. 3D
objects (45 ranging from animals to transportation
vehicles) obtained from public 3D repositories were
distributed among the space-nodes. Before the
administration of the tasks, pahcipants were allowed 20
minutes to navigate the world, to familiarise themselves
with the interface and to take snapshots whenever they
wanted to. Participants were also allowed to extend their
cognitive web by taking more snapshots during the
tackling of the navigation tasks. After completing the
latter, participants were administered the standard NASA
TLX [7] to determine the workload they experienced.
The time they took for completing every task was also
captured. No time limit was imposed on participants for
completing the tasks.

Navigation Tasks: Each participant had to navigate the
virtual environment in order to locate 5 objects under the
aided condition and another 5 different objects in the
unaided condition. 3 participants tackled the tasks in the
unaided condition first and the other 3 tackled the tasks
in the aided condition first. In the aided condition, they
had access to the cognitive web browser. In the unaided
condition, they could not use !he cognitive web browser.
This was done to minimise m y effects resulting from
fatigue and the learning of (1) the interface, (2) the
environment. Note that, the ortier in which objects had to
be located was random. Participants were also prevented
from talking to each other. A participant was deemed to
have located an object after the lowest tip of the object
was made to meet the bottom edge of the 3D browser
window.

20

18
16
p 14
12

E

a,

2

OAided

10

0
I

ElUnaided

8

VI

2 6
4

2

0Ph

M

T

E

Pe

F

Worldoad categories
e m r bars are shown)

Hypothesis H1: Participants experience less physical
demand while tackling the tasks in the aided condition.
Hypothesis H2: Participants experience less mental
demand while tackling the tasks in the aided condition.

W
(Standard

Figure 11 Comparing workload categories (M:
Mental, Ph: Physical, T: Temporal, E: Effort, Pe:
Performance, F: Frustration) and overall
workload (W)

Hypothesis H3: Participants experience less overall
workload while tackling the tasks in the aided condition.
Hypothesis H4: Participants lackle the navigation tasks
faster in the aided condition than in the unaided
condition.

5.1 Results
Our results show that there was a significant
decrease in the physical (demand experienced by

I

l M IPh

IT

I E be

I

1

I

-0.93

1

2.449

0.584

0.343

I

0.560

I F

lw

I

I

I

1

0.117

0.059

Table 1 Paired two means t-test for workload
categories and overall workload

583

0Aided

Unaided

2

1

3

4

5

Order of localising target object
(standard error bars are shown)

Figure 12 Comparing average time to complete
tasks
31

I

I

I

I

I

I

Table 2 Paired two means t-test for time taken
to complete tasks
We express some reservation about the strict
validity of the statistical manipulations especially
because of the small size of participant population.

5.2 User feedback and discussion of results
We have collected here a few comments
participants made while trying out the navigation aid.
positive comments: “Using the navigation tools was
noticeably faster”, “Snap shots were very ‘handy’ for
quicker movement”, “Can position and reorient myself
more easily and quickly”; “It helps me to remember
more about the environment, I can remind myself of
things in there without navigating through it again ”

Entries. The understanding of what Entries and Exits are
has an important impact on the usability of the cognitive
web.
Although we implemented the system in such a
way that while snapshots were being created on the fly,
participants were still able to navigate normally, in
practice this was unfortunately not the case. On a
number of unpredictable occasions navigation would
slow down or stop while pictures were being created or
stored. This probably had to do with the varying
complexities of different scenes and the idiosyncratic
behaviours of Java thread implementation. We believe
that this must have impacted negatively our evaluation
certainly regarding the time taken by subjects to locate
objects in the aided condition, the frustration and
temporal demands experienced by participants. Also our
implementation did not render the 3D scenes in colour.
This resulted in the pictures captured to be less easily
distinguishable from each other. We believe that an
implementation which allowed a colour 3D scene to be
navigated would also probably improve the results we
obtained from our navigation aid. In short, this
experiment may have had a bias against the strength of
our approach.
Another interesting observation is that even for this
small pool of participants the strategy for creating
snapshots varied widely. Some created Views based on
their interests rather than on other concerns. This
strategy is closer to what we had in mind but we were
pleasantly surprised by participants whose strategies
were more task-oriented. For example, some would also
take snapshots from a distance to get as many objects as
possible in the picture. This allowed them to identify
very quickly the location of a target simply by flicking
through the cognitive map representation rather than
navigating manually through the world.

6. Future work
Our pilot study shows that we have achieved a
significant decrease in the physical demand experienced
by users when solving the set tasks. This is a valuable
achievement but we believe that this design can be
improved to achieve significant gains in other areas,
especially mental demand. For example, we can provide
opportunities for users to annotate snapshots as they are
created. These annotations can be made available in a
way similar to tool-tips as the cursor is moved over the
pictures. We anticipate this would help to disambiguate
entries or exits that look similar.
Currently, navigators are teleported from one
position to another when users interact with the
cognitive web browser in the command mode. It may be
useful to look into ways to autonomously navigate [ 2 ] a
user through the scene. Extensive teleportation in a

criticisms: “I was confused between the entries and
exits when leaming to use the system”, “Pictures
describing some entries or exits are sometimes not easily
distinguishable”, “System slows down while snapshot is
being created.. .”
We observed that some participants understood the
interface of the cognitive map representation almost
immediately. However, one participant in particular was
lost on a number of occasions because he or she was not
able to understand the difference between Exits and

582

virtual environment can have a significant negative
impact on the construction of the cognitive map of the
spatial environment because of the unavailability of cues
that will disrupt the relationship .of one place with
another [ 141.
In our opinion, it is not only the navigator
constructing the cognitive web who will find it useful.
Other prospective navigators could use the information
that has been gathered aboul: a given virtual world in
various ways. The cognitive web of an experienced
navigator could be shared wi.th virtual tourists who just
want to have a rough idea of how such and such worlds
are before choosing one that suit them best. Cognitive
webs could be used to determine the interest profile of a
given navigator. In dynamic virtual environments, such
as one shared by multiple avatars, the cognitive web can
also function as a tool for recording events in that
environment. So far we have captured only visual
information which represents !places. In principle, sounds
heard at those places could also be captured. We hope
that our approach will open new lines of enquiry in one
area, which has unfortunately not received as much
attention as it deserves, namely collaborative wayfinding
in large-scale virtual worlds.

7.Conclusion
This paper has presented an initial attempt to
capture the cognitive map of a navigator in a virtual
environment. The captured infomation is publicly
accessible. We have illustrated how directly
manipulating such cognitive webs allows navigators to
tackle wayfinding problems in large-scale worlds. Our
initial evaluation results indicate that interacting with
such an approximate representation of the spatial
knowledge is promising. It can certainly complement
traditional techniques such as increasing the legibility of
an environment to improve wayfinding. It may also be
useful in cases where legibility approaches may fail e.g.
the navigator may not understand the language the labels
are written in. We also suggest that there is a lot of room
for improving the current interface.
Designers need to experiment with other ways of
organising spatial information derived from a
navigator’s knowledge of the environment in order to
create an interactive publicly and intuitively accessible
cognitive map of a navigator to tackle a wide range of
wayfinding needs. We hope our work is a step in that
direction.

8. References
Ingram, R. and Benford, S. Improving the legibility of
virtual environments in proceedings of the 2nd
Eurographics Conference on Virtual Environments, MonteCarlo, JanFeb 1995.
Ingram, R. and S. Benford, Legibility Enhancement for
Information Visualisation, Proc. Visualization 95, IEEE
Press, 209-216.
IO. Kuipers, B. J. Modelling spatial knowledge, Cognitive
Science, 2, 1978, 129-153.
1 1 . Kuipers. B.J. The Cognitive Map: Could it have been any
other way? Spatial orientation: theory, research, and
app‘plicarion,Herbert L. Pick, Jr and Linda P eds. Acredolo
New York: Plenum, 1983, 345-359.
12. Pinette, B. linage-based Navigation through Large-scale
Environments. PhD thesis. University of Massachusetts
Amherst, Department of Computer Science, May 1994.
13. Ruddle, R. P., Payne, S. J.. and Jones, D. M. Navigating
buildings in “desktop” virtual environments: Experimental
investigations using extended navigation experience.
Journal of experimental Psychology: Applied, 3, 2, 1997,
143-159.
14. Vinson, N. G. Design Guidelines for Landmarks to Suppcrt
Navigation in Virtual Environments. CHI’ 99 Proceedings,
Pittsburg, 1999, 278-285.

1. Benford, S. and Mariani. J. Virtual Environrnentsfor Darn
Sharing crtidVisualisation - Populated Inforniation
Terrains, In P. Sawyer ed. Inirerfaces to Database Systems,

Springer-Verlag. 1994.
2. Brooks, Rodney A. Solving the Find-Path Problem by
Good Representation of Free Space, IEEE Transactions on
Systems Man, a n d Cyber-ncfics, Vol. SMC-I 3, No.3.
March/April 1983, 190-197.
3. Darken, P. D. Wayfinding in Large-scale Virtual Worlds.
CHI’ 95 Proceedings, 1995, 4.5-46.
4. Darken, R. P., Sibert, J. L. A Toolset for Navigation in
Virtual Environments, UIST’ 93 Proceedings, 1993, 157165.

5. Downs, M. Roger and Stea, D. Maps in Minds. Reflections
on Cognitive Mapping. Harper and Row, publishers, 1977.

6. Elvins, T. T., Nadeau, R. N., and Kirsh, D. Wordlets - 3D
Thumbnails for wayfinding in Virtual Environments,

UIST’ 97 Proceedings, 1997, 21-30,
7. Hart, S.G. & Wickens. C. (1990). Workload
assessment and prediction., In Booher, Eds.,
MANPFUNT, an approach to systems integration.
New York: Van Nostrand Reinhold.

583

