2011 15th International Conference on Information Visualisation

The Implications of David Hockney’s Thesis for 3D computer graphics
Theodor Wyeld
Flinders University, Australia
{twyeld@gmail.com
Abstract

Hockney claims the projected image from mirrors
and lenses allowed artists to capture fleeting expressions
directly by tracing its projection thus accounting for what
seems to be a dramatic shift to realism in the fifteenth
century. It is this same realism that is captured in today’s
computer algorithms. But, how the algorithms are
configured is subject to the same interpretations that the
Renaissance Masters made of what they observed.
Hence, scientific fact and artistic technique are conflated
in the space of their mutual representation – each
informing the other. Although not well documented, the
terms used to describe the various ‘realism’ algorithms
used in 3D computer graphics seems to confirm this
connection.

David Hockney’s 2001 book Secret Knowledge of
the Masters, reignited the debate on the use of optical
devices for constructing perspective images in the
Renaissance. In it, he brings his insights as an artist to
the debate. This paper explores his thesis in terms of its
implications for 3D computer graphics. Just as
technology informed the Renaissance artist on ways of
seeing and representing natural phenomena, 3D
computer graphics today uses algorithms to simulate
these same phenomena. For both, various techniques are
used to make the images produced seem real or at least
real enough. In the case of the Renaissance artist,
painterly techniques were used to generate the illusion of
clarity. For 3D computer graphics, mathematical
algorithms are used to simulate many of the same effects.
Striving for realism is a common theme. However, while
the Renaissance artist never lost site of their role in
interpreting what they see, 3D computer graphics is
supposed to be underpinned by the certainties of its
apparent scientific veracity. But is this certainty
deserved or is it merely that science and art are
intertwined in ways that mean one is reliant on the
other?
Keywords--Hockney,
Renaissance,
Computer Graphics, Perspective.

The Projected Image
Da Vinci is perhaps the earliest clear link we have
with the direct use of optics in the construction of
Renaissance perspective images [1, 2, 13]. Da Vinci’s
lens-based system involved not just the technology
available but an aspiration to a ‘naturalism’.
Conceptually, artistic representation was redefined as
literal imitation. According to Hockney this was based
on a developing culture that optical devices created
images using the geometry of light to deliver a more
objective imitation than the human eye could perceive. It
provided a model for an 'objectification' of nature. In
turn, Renaissance artists caused a profound change in the
way the world was visually understood and organised –
eventually influencing Galileo, Harvey, Descartes, and
Newton’s views [4].
While many authors [2, 4, 13-16] tend to agree that
the availability of optics influenced the production of the
perspective procedure in the Renaissance, Hockney goes
further, claiming that studies of optics both provided a
mechanism for, and the basis for establishing the rules
of, its construction. Among other examples, as evidence
for his thesis, Hockney points out how Jan van Eyck’s
Arnolfini Wedding of 1434 contains a convex mirror.
This type of mirror, if silvered on the reverse side, would
provide the very concave mirror required to generate the
series of projected images necessary for the painting’s
production. Using a concave mirror, and its subsequent
projection, each object in the scene could have been

3D,

Introduction
In Hockney’s [1] book, Secret Knowledge,
Rediscovering the Lost Techniques of the Old Masters,
he claims the Renaissance Masters transformed their art
from Gothic symbolic spiritualism to a naturalistic
realism using optical devices. This is not a new concept –
it has been reported by others [2 - 7]. However, in a very
practical way, Hockney recreates the conditions for
optically projected images to apply his theories directly.
What is interesting is how contemporary 3D computer
graphics uses many of the Masters’ terms.to describe
their algorithmic corollary. It is the Masters’ techniques
for producing depth, light and shade that seems to inform
the 3D computer algorithms used in creating computer
generated realistic scenes today.
1550-6037/11 $26.00 © 2011 IEEE
DOI 10.1109/IV.2011.98

409

individually rendered in a sharply realistic manner
directly onto the canvas (see Figure 1).

Figure 2. Left: The Death of Cleopatra (1658)
Cagnacci [19]. Right: Cagnacci’s The Death of
Cleopatra reduced to black-and-white using
Photoshop.
According to Edgerton [4, p17], constructed using a
mirror or lens method, linear perspective made
“metaphysical subject matter appear more tactile and
therefore empirically believable,…[demanding that space
should] be perceived as having the same physical
properties and…[obey] the same geometric rules” as
optics. In turn, this, would have furthered the notion of a
rigorous scientific application to the ‘observed’ world.
Today, the optical imitation of nature in the realm of 3D
computer graphics, and in particular three-dimensional
computer modelling follows the same perspectiveconstruction algorithms. Pioneered in the 1960’s, by
Roberts and Cooms [18], their algorithms ushered in an
era of efficient, human error free, perspective
construction. The methodology, though digital, is
essentially a re-configuration of the pre-existing
renaissance-founded perspective paradigm.
While the advent of photography in the nineteenthcentury meant the time consuming process of manually
constructing perspectives was eliminated, the automation
of perspectival imaging using 3D computer graphics
algorithms has now completed the perspectival process
first realised in the Renaissance. Computers which allow
the projection of three-dimensional points in space onto a
two-dimensional plane using perspective-generating
mathematical algorithms reduce real objects to virtual
objects consisting of Alberti’s planes, surfaces and lines.
The coordinates of each vertex is stored in computer
memory. So too is the virtual camera, its point of view,
direction of sight and relationship to a projection plane.
Point by point a perspectival image is generated by the
frustum matrix. As colour was added in the 70’s and 80’s
other algorithms were developed adding depth cues such
as hidden line and surface removal, shading, texture,
shadows, reflections, and so on.

Figure 1. Left: Arnolfini Wedding (1434) van
Eyck [20]. Right: Detail of Jan van Eyck’s
Arnolfini Wedding (1434) [19].
Indeed, a number of devices can be used to
mechanically construct a perspective: the camera
obscura, the camera Lucida, a concave mirror, grids,
screens, Brunelleschi’s peephole, pinhole projections and
so on. What they all tend to do is to map the world onto a
flat plane. The interest in mapping space to the
Renaissance artisan-engineer, coincided with its military
and general surveying use.
This use of optical devices in the construction of
perspectival pictures heralded a fundamental shift in
artists’ otherwise naive appreciation of the structure of
their environments. In particular, they were able to
represent the world in a more sophisticated way because
of optics and its implied geometry. Using such optical
devices artists would no longer just look at the world
around them but represented a projected view of it.

The Photographic Image
Where perhaps Renaissance optics most informs
today’s 3D computer graphics methods is their common
striving for a ‘photographic’ realism. These photographic
qualities are evident, for example, in Cagnacci’s The
Death of Cleopatra (1658). If we reduce his painting to a
black and white reproduction, its photographic-like
qualities become more apparent (see Figure 2). The
lighting is similar to that we associate with photography.
Hockney’s use of the analogy of a black-and-white
photograph to demonstrate his point is further confirmed
by Gombrich’s [17, p91] assertion that using a blackand-white photograph-like reproduction of an artist’s
work helps “separate the code from the content” thus
revealing, in this case, the overarching emphasis on
reproducing the tonal quality of the projected image.
Perhaps just as computers today remove much of the
drudgery in ‘setting-up’ a perspective, Renaissance
artists simply used optical projections to overcome the
difficult mathematical construction of their perspective
compositions – an artist could merely trace a ‘live’ image
and fill in the surfaces later. A tool which would have
helped make images that were intensely realistic.

3D Realism
How much of 3D computer graphics imagery
algorithmically replicates natural phenomena and how
much is artistic impression? For example, to project a
decent image onto an adjacent wall in the Renaissance
full sun was required. Nevertheless, the lack of contrast
in such projected images meant the final painted
production required interpretation of and reference to the
actual scene. In a similar manner, many of the computer
algorithms used to generate shading and highlighting in

410

3D computer graphics requires some interpretation as
neither can approach the same level of light intensity as
that reflected by real physical objects in full sun. Hence
it follows that what is promoted as ‘scientifically
adduced as real’ in 3D computer graphics is often merely
interpreted as ‘real enough’. However, unlike the
sameness that pervades 3D computer graphics, the
absolute subjectivity of vision is reflected in how
painters made use of the Renaissance projected image.
Velasquez, for example, used the rich colours and tones
that a projected image does not provide (see Figure 3).
Painters made choices like this consciously; hence, the
differences between painters’ subjective interpretation of
what they saw in the projected image. And, Cotan’s
objects are almost super-real (much like the tonal
accuracy and brilliance achieved in 3D computer
graphics) (see Figure 3) – “the camera obscura does just
that – making objects assume an uncanny look of
concentrated ‘real-ness’” [1, p245]. But what is this
‘realness’? In painters' quest for a super-realism they
used techniques which encourages one’s perception to
conceive a realness which does not exist. Tricks and
techniques such as shading and highlights enter the
artist's repertoire of representation to which over time we
have become accustomed. They establish a metaphor for
the seen thing. Similarly, in 3D computer graphics, the
lens flare – something photographers try to avoid – is
used to suggest a physical camera was used; raytraced or
mapped shadows are used to indicate strong or soft light;
fog is used to indicate depth; or, blur to introduce motion
and so on – not things we consciously see in reality but
merely metaphors or artistic techniques for generating a
super-realism.

Masters for defining the elements to be constructed
algorithmically.

Where 3D computer graphics Emulates the
Masters’ Techniques
Just as the Renaissance painters “found ways to
render effects of light on surfaces with increasing
exactitude… da Vinci later made them seem quite naïve
and unconvincing,” and, in time, the algorithmically
generated Lambert, Gourard, and Phong shaders will be
supplanted by more powerful reality renderings [8,
p161]. However, where 3D computer graphics and
traditional art differs is in the complex dialogue implied
by the artist’s personal deviations from the computer’s
attempts at mathematically precise imitations of Nature.
The artist’s dialogue, by contrast, includes looking at,
and representing Nature beyond, and in addition to, the
scientifically adduced principles of optical devices.
Unlike 3D computer graphic’s mathematically
constructed images, artists may not necessarily proceed
in the same manner in every instance. For example,
where Dutch realist painters, such as Van Eyck, relied on
Nature reflected mirror-like to teach its perfect imitation,
Italians like Piero della Francesca and others used
precise geometrical rules for remaking Nature based on a
scientific appreciation of ‘natural law’. Van Eyck viewed
humans’ understanding of Nature by its expression as a
surrogate reality created in his projected images [1].
Despite a lack of a singular geometric precision, Van
Eyck saw mirrors as surrogate realities facilitating a
conceptual shift to lens-based, mirror-based, or devicebased naturalism. It is in this device-based naturalism
that we find the paradigm which most closely informs
3D computer graphics today.
If we consider three-dimensional 3D computer
graphics as simply an extension to a pre-existing
representation of three-dimensional space on a twodimensional surface (discounting developments in CADCAM, interactive games and so on) this produces the
unsurprising result that many of the techniques of the
Masters’, and indeed photography, appear as discrete
rendering algorithms. Algorithms used to generate
shading, shadows, and so on, designed to look like their
hand rendered chiaroscuro counterparts are hence,
inherently not ‘real’ but rather appear to be literal
referents to their traditional artistic metaphors –no more
real, therefore, than the artistic metaphors they attempt to
emulate.
A brief review of leading texts on 3D computer
graphics [8 – 12, 20] confirms the assertion that, in
relation to perspective, light, colour, surface, shade,
reflection, and texture, most algorithms are indeed,
typically derived from their traditional artistic
equivalents.
Mitchell and McCullough [9] and Foley et al [10]
discuss the ‘visual effect’ of perspective projection used
in 3D computer graphics as similar to that of
photographic systems, and the common ‘hidden-line
rendering’ is referred to as being generated by the

Figure 3. Left: Untitled (1618) Velazquez [20].
Middle: Still Life with Game Fowl, Fruit, and
Vegetables (1602), Cotan [19]. Right: Computer
generated architectural interior modelled by the
author in 1999.
Highlights and tones are simplified by optical
projection. The Hue, Saturation, and Volume (HSV)
algorithms for lighting in a 3D computer graphics
program are derived from a scientific analysis of how
light behaves. Yet, despite claims that three-dimensional
3D computer graphics follow natural laws, the Masters’
chiaroscuro techniques appear to have influenced how
3D computer graphics images are actually constructed.
For example, wire-frame, point construction, primitives
(cone, sphere, cube, cylinder) and carved solids are also
the essential elements Alberti applied in his fifteenthcentury treatise [8]. This suggests much is owed to the

411

‘Painter’s Algorithm’. The hidden-line algorithm follows
Alberti’s intellectual program, reconstructed in software,
it mimics his descriptions of edge lines, outlines of
surfaces and curvatures (see Figure 4).

the, impossible to absolutely reproduce, incidental
intensity of reflected light [9, 2].

Figure 6. Left: Lambert shading. Middle:
Gourard shading. Right: Phong shading.

Figure 4. Left: Hidden line rendering. Middle:
Flat shaded rendering. Right: Shadow-mapped
rendering (modelled by the author).

Where most of the authors discussed here define an
achievable ‘realism’ in 3D computer graphics what they
are actually describing is illusions in the creation of a
conceptual pipeline [10] which generates an acceptable
realism that we have become accustomed to through an
acculturation to its conventions and metaphors since the
Renaissance.
The dichotomy of both a reference to artistic quality
and scientific fact is common among the exponents of
‘realistic’ 3D computer graphics. Indeed, most rendering
packages approximate reality at best and positively fudge
it at worst. Yet, the 'real' world is still typically described
by 3D computer graphics programmers in terms of its
ability to be simulated: “A fundamental difficulty in
achieving total visual realism is the complexity of the
real world. Observe the richness of your environment….
The computational costs of simulating these effects can
be high” [10, p607]. Here it could be suggested that the
deft stroke of a paint brush generates a greater
complexity in both its visual quality and its quality as a
tactile surface. This is something which a computergraphic image is yet to achieve (neither should it
necessarily). Indeed, while modern 3D computer
graphics images may rely for their realism on the
complexity of textures, often derived of photographic
sources, the surfaces they are painted onto are ostensibly
orthogonal hinting at the trade-off between visual
complexity and operational performance. For example, if
we compare the complexity of Rubens’ work with that of
a 3D computer graphics generated image, what is most
striking about the painting is its organic tactility (though
we cannot generate such a tactile surface in this
document) this is in contrast to the more typical sterile
and orthogonal (i.e. artificial) 3D computer graphics
image (see Figure 7). Moreover, the space used to exposé
3D computer graphics tend to be orthogonal because of
the perspective algorithms used for their generation.

Even the application of perspectival composition
informs the content of 3D computer graphics. Tufte [12]
identifies the strength of Renaissance techniques in the
construction of perspectival images in digital media such
as the common tiled grid in a Renaissance perspective
(see Figure 5). For example, in the case of a simulation
of a cloud mass turning into a storm, the use of an
exaggerated tiled grid as a perspectival cue is considered
necessary to indicate its volumetric quality (see Figure
5).

Figure 5. Left: Nativity (1490), Fernando Gallego
[12, p22]. Right: Study of a Numerically Modeled
Severe Storm National Centre for
Supercomputing Applications [12, p20].
To produce more painterly images, Mitchell and
McCullough [9] claim a standard architectural
convention is used whereby light appears to arrive from
over the viewer's shoulder (see Figure 6). Where colour
is used in both light source and on surfaces, Foley et al
[10, p590] claim, the use of “Smith's HSV (hue,
saturation, value) model…(also called the HSB model,
with B for brightness) is user-oriented, being based on
the intuitive appeal of the artist's tint, shade, and tone.”
Mitchell and McCullough [9] extend this notion of a
surface’s artistic quality by referring to da Vinci’s
chiaroscuro techniques; Lambert’s cosine shading is
described as similar to old-fashioned drawing books
which used tonal media such as charcoal and watercolour
to demonstrate depth on a surface; and, successive
rendering algorithms have attempted to emulate the
artist’s chiaroscuro techniques with increasing precision
such as Gourard and Phong (see Figure 6).
While Lambert’s eighteenth-century cosine law is
now used as a scientific basis for its computer-graphic
algorithmic corollary – specularity – it reduces to a
precise formula a fact long known by painters regarding

Figure 7. Left: Adoration of the Magi (1618-19),
Rubens [19]. Right: Computer-rendered

412

traditional Japanese house using radiosity
[www.radiosity.tripod.co.jp, 10-10-2003].

[3]
[4]

Conclusion
If we accept Hockney’s assertions that the
Renaissance Masters painted directly onto images
projected on a two-dimensional surface from mirrors or
other optical devices then what painters were illustrating
was not always Nature directly but a projected vision of
Nature – a Nature that could be observed and traced in
detail. However, the quality of the image was by
circumstance filtered and altered in ways that also
affected the way artists approached the task of
replicating and interpreting what they could see. By
contrast, while 3D computer graphics technology appears
to have borrowed from the lessons of the Masters in its
representation and interpretation of Nature, it is
underpinned by a scientific method. Yet, there also
seems to be an accord between science and art in the
construction of the 3D computer generated image. This
lies at its code – where the underlying subjectivities of
art should be anathema to the code’s logical construction.
Nonetheless, the terms used to describe the algorithms
for constructing 3D images suggest that its art and
technological invention coexist in a symbiotic
relationship. The software engineer can claim to have
constructed an algorithm which emulates the rules of
natural phenomena, but when it is described in terms
used by the quattrocento Masters then the net effect is
something both less than its Nature-inspired corollary
and more than its retinal impression. It becomes a
convention we agree to use to describe that phenomenon.
Indeed, in more general terms, it is from the symbiosis of
Art and Science that emerge the conventions and rules
we use to describe the natural world around us. Hence,
once recognised as such, 3D computer graphics may
finally be accepted as simply the contemporary artists’
aide in the same manner that the lens was to the
Renaissance artist.

[5]

References

[19]
[20]

[1]

[2]

[6]

[7]

[8]
[9]
[10]

[11]

[12]

[13]

[14]

[15]
[16]
[17]

[18]

Hockney, D., Secret Knowledge, Rediscovering the Lost
Techniques of the Old Masters, Thames & Hudson, UK,
2001.
Kemp, M., The Science of Art: Optical Themes in
Western Art from Brunelleschi to Seurat, Yale
University Press, New Haven and London, 1990.

[21]

413

Kemp, M., The Oxford History of Western Art, Oxford
University Press, Oxford, New York, 2000.
Edgerton, S. Y., The Heritage of Giotto’s Geometry: Art
and Science on the Eve of the Scientific Revolution,
Cornell University Press, Ithaca, London, 1991.
Ruskin, J., The Elements of Perspective, Routledge and
Sons Ltd, London, 1907.
Gombrich, E. H., The Image and the Eye: Further
Studies in the Psychology of Pictorial Representation,
Phaidon, Oxford, 1982.
Gombrich, E. H., Art and Illusion: A Study in the
Psychology of Pictorial Representation, Princeton
University Press, Princeton and Oxford, 2000.
Mitchell, W. J., The Reconfigured Eye: Visual Truth in
the Post-photographic Era, MIT Press, London, 1994.
Mitchell, W. J., and McCullough, M., Digital Design
Media, Van Nostrand Reinhold, NY, 1995.
Foley, J. D., van Dam, A., Feiner, S. K., and Hughes, J.
F., 3D computer graphics: Principles and Practice,
Addison-Wesley, NY, 2002.
Woo M., Neider J., and Davis T., OpenGL Programming
Guide, Second Edition, The Official Guide to Learning
OpenGL, Version 1.1, Addison-Wesley, Massachusetts,
1997.
Tufte, E. R., Visual Explanations: Images and Quantities,
Evidence and Narrative, Graphics Press, Cheshire,
Connecticut, 1997.
Perez-Gomez, A. and Pelletier, L., Architectural
Representation and the Perspective Hinge, MIT Press,
Cambridge, Mass, London, England, 1997.
Coyne, R., Designing Information Technology in the
Postmodern Age, Cambridge, Massachusetts, The MIT
Press, 1995.
Lefebvre, H., The Production of Space, Blackwell,
Oxford UK & Cambridge USA, 1991.
Panofsky, E., Perspective as Symbolic Form, Wood, C.,
(Trans.), Zone Books, New York, 1991.
Gombrich, E. H., Meditations on a Hobby Horse and
other Essays on the Theory of Art, Phaidon, London and
NY, 1978.
Manovich, L., The Mapping of Space: Perspective,
Radar,
and
3-D
3D
computer
graphics,
http://jupiter.ucsd.edu/~manovich/text/mapping.html,
1993, [05-06-2002].
www.kfki.hu/~arthp [10-10-2003].
Richens, P., Computer Aided Art Direction, in (eds.). F.
Penz and M. Thomas, Cinema & Architecture: Méliès,
Mallet-Stevens, Multimedia, British Film Institute,
London,
http://www.arct.cam.ac.uk/mc/cadlab/index.html, 1997,
[05-06-2002].

