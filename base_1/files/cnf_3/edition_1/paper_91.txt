2011 15th International Conference on Information Visualisation

Cultural Data Sculpting: Omnispatial Visualization
for Cultural Datasets
Dr Sarah Kenderdine,
Dept Chinese, Translation and
Linguistics
City University of Hong Kong
Hong Kong
email: skenderd@cityu.edu.hk

Dr Oscar Kin Chung Au,
School of Creative Media City
University of Hong Kong
email: kincau@cityu.edu.hk,

limiting factor in human cognition of data visualizations
[23]. The increasing trend towards research requiring
‘unlimited’ screen resolution has resulted in the recent
growth of gigapixel displays. Visualization systems for
large-scale data sets are increasingly focused on effectively
representing their many levels of complexity. These include
tiled displays such as HIPerSpace at Calit2 [20] and, next
generation immersive virtual reality systems, StarCAVE at
UC San Diego [9] and Allosphere at UC Santa Barbara [2].
In general, however the opportunities offered by
interactive and 3D technologies for enhanced cognitive
exploration and interrogation of high dimensional data still
need to be realized within the domain of visual analytics for
digital humanities [26]. The four projects described in this
paper take on these core challenges of visual analytics
inside the Advanced Visualization and Interaction
Environment (AVIE) [1] [41] (also see Section 2.1) to
provide
powerful
modalities
for
an
omnispatial/omnidirectional (3D, 360-degree) exploration
of heterogeneous datasets responding to the need for
embodied interaction; knowledge-based interfaces,
collaboration, cognition and perception [46]. The projects
are developed by the Applied Laboratory for Interactive
Visualization and Embodiment (ALiVE), CityU, Hong
Kong [3]. A framework for ‘enhanced human higher
cognition’ [16] is being developed that extends familiar
perceptual models common in visual analytics to facilitate
the flow of human reasoning. Immersion in threedimensionality representing infinite data space is
recognized as a pre-requisite for higher consciousness,
autopoesis [39] and promotes non-vertical and lateral
thinking [43]. Thus, a combination of algorithmic and
human mixed-initiative interaction in an omnispatial
environment lies at the core of the collaborative knowledge
creation model proposed.
The four projects discussed also leverage the potential
inherent in a combination of ‘unlimited screen real-estate’,
ultra-high stereoscopic resolution and 360-degree
immersion to resolve problems of data occlusion and
distribution of large-scale data analysis in networked
sequences
revealing
patterns,
hierarchies
and
interconnectedness.
The
omnidirectional
interface

Abstract—This paper presents four research projects
currently underway to develop new omnispatial visualization
strategies for the collaborative interrogation of large-scale
heterogeneous cultural datasets using the worlds’ first 360degree stereoscopic visualization environment (Advanced
Visualization and Interaction Environment - AVIE). The
AVIE system enables visualization modalities through full
body immersion, stereoscopy, spatialized sound and camerabased tracking. The research integrates work by a group of
international investigators in virtual environment design,
immersive interactivity, information visualization, museology,
visual analytics and computational linguistics. The work is
being implemented at the newly established research facility,
City University’s Applied Laboratory for Interactive
Visualization and Embodiment – ALIVE) in association with
partners Museum Victoria (Melbourne), iCinema Centre,
UNSW (Sydney), ZKM Centre for Art and Media
(Karlsruhe), UC Berkeley (USA), UC Merced (USA) and and
Europeana (in association with Israel Museum of Jerusalem).
The applications are intended for museum visitors and for
humanities researchers. They are: (1) Data Sculpture
Museum; (2) Inside Europeana; (2) Rhizome of the Western
Han; (4) Blue Dots AVIE (Tripitaka Koreana).
Keywords-3D; immersive; information visualization;
interactive narrative; museum collections; archaeology;
corpora

I.

INTRODUCTION

Research into new modalities of visualizing data is
essential for a world producing and consuming digital data
at unprecedented rates [24, 40]. Existing techniques for
interaction design in visual analytics rely upon visual
metaphors developed more than a decade ago [25] such as
dynamic graphs, charts, maps, and plots. Currently,
interactive, immersive and collaborative techniques to
explore large-scale datasets lack adequate experimental
development essential to the construction of knowledge in
analytic discourse [46]. Recent visualization research
remains largely constrained to 2D small-screen based
analysis and advances interactive techniques of “clicking”,
“dragging” and “rotating” [24, 48]. Furthermore, the
number of pixels available to the user remains a critical
1550-6037/11 $26.00 © 2011 IEEE
DOI 10.1109/IV.2011.102

Prof Jeffrey Shaw
School of Creative Media
City University of Hong Kong
email: j.shaw@cityu.edu.hk

570

A.

prioritizes ‘users in the loop’ in an egocentric model [23].
The projects also expose what it means to have embodied
spherical (allocentric) relations to the respective datasets.
These hybrid approaches to data representation also allow
for the development of sonification strategies to help
augment the interpretation of the results. The tactility of
data is enhanced in 3D and embodied spaces by attaching
audio to its abstract visual elements and has been well
defined by researchers since Chion and others [7].
Sonification reinforces spatial and temporal relationships
between data (e.g. the objects location in 360degrees/infinite 3D space and its interactive behavior (for
example, see [57]). The multi-channel spatial array of AVIE
platform offers opportunities for creating a real-time sonic
engine designed specifically to enhance cognitive and
perceptual interaction, and immersion in 3D. It also can
play a significant role in narrative coherence across the
network of relationships evidenced in the datasets.
II.

Advanced Visualization and Interaction
Environment

Applied Visualization Interaction Environment (AVIE)
is the UNSW iCinema Research Centre’s landmark 360degree stereoscopic interactive visualization environment
spaces. The updated active stereo projection system
together with camera tracking s installed at ALiVE. The
base configuration is a cylindrical projection screen 4
meters high and 10 meters in diameter, a 12-channel
stereoscopic projection system and a 14.2 surround sound
audio system. AVIE’s immersive mixed reality capability
articulates an embodied interactive relationship between the
viewers and the projected information spaces. [41]
III.

TECHNIQUES FOR CULTURAL DATA ANALYSIS AND
VISUALIZATION

The intersection of key disciplines related to the projects
in this paper includes multimedia analysis, visual analytics,
and text visualization. An excellent review of the state of
the art for multimedia analysis and visual analytics
appeared in IEEE Computer Graphics and Applications [6].
The research projects also responds to core challenges and
potentials identified in Visual Analytics [51, 25] and to key
emerging technologies for the coming years such as Visual
Data Analysis and Gesture Based Computing [22]. Visual
Analytics includes the associated fields of Human
Perception and Cognition where 3D technologies and
immersive and interactive techniques hold significant
potential for enhanced research applications [23].
Computational linguistics is providing many of the
analytics tools required for the mining of digital texts (e.g.
[48, 50]) The first international workshop for intelligent
interfaces to text visualization only recently took place in
Hong Kong, 2010 [35]. Most previous work in text
visualization focused on one of two areas, visualizing
repetitions, and visualizing collocations. The former shows
how frequently, and where, particular words are repeated,
and the latter describes the characteristics of the linguistic
“neighborhood” in which these words occur. Word clouds
are a popular visualization technique whereby words are
shown in font sizes corresponding to their frequencies in the
document. It can also show changes in frequencies of words
through time [19] and in different organizations [8] and
emotions in different geographical locations [17]. The
significance of a word also lies in the locations at which it
occurs. Tools such as TextArc [44], Blue Dots [5, 29, 30,
31, 32] and Arc Diagrams [56] visualize these “word
clusters” but are constrained by the small window size of a
desktop monitor. In the digital humanities, words and text
strings is the typical mode of representation of mass
corpora. However, new modes of lexical visualization such
as Visnomad [50] are emerging as dynamic visualization
tools for comparing one text with another. I another
example the Visualization of the Bible by Chris Harrison
where each of the 63,779 cross references found in the
Bible are depicted by a single arc whose color corresponds
to the distance between the two chapters [17].

EXPERIMENTAL PROJECTS

The four experimental projects included in this paper
draw upon disciplines such as multimedia analysis, visual
analytics, interaction design, embodied cognition,
stereographics and immersive display systems, computer
graphics, semantics and intelligent search and,
computational linguistics. The research also investigates
media histories, recombinatory narrative, new media
aesthetics, socialization and presence in situated virtual
environments and the potential for new psychogeography of
data terrains. Each work takes place in AVIE system. The
datasets used in these four works are:
• Data Sculpture Museum: over 100,000 multimedia
rich heterogeneous museological collections
covering arts and sciences derived from the
collections of Museum Victoria, Melbourne and
ZKM Centre for Art and Media, Karlsruhe, for
general public use in a museum contexts.
• Inside Europeana: 5000 objects from the Israel
Museum of Jerusalem, collaborative searching, live
from Internet.
• Rhizome of the Western Han: laser-scan
archaeological datasets from two tombs and
archaeological collections of the Western Han,
Xian, China culminating in a metabrowser and
interpretive cybermap, for general public use in a
museum contexts.
• Blue Dots AVIE: Chinese Buddhist Canon, Koryo
version (Tripitaka Koreana) in classical Chinese,
the largest single corpus with 52 million glyphs
carved on 83,000 printing blocks in 13th century
Korea. The digitized Canon contains metadata that
links to geospatial positions, contextual images of
locations referenced in the text, and to the original
rubbings of the wooden blocks. Each character has
been abstracted to a ‘blue dot’ to enable rapid
search and pattern visualization. For scholarly use
and interrogation.

571

Visual Analytics is closely related to HCI and the
development of gesture based computing for data retrieval
[22]. Microsoft’s Project Natal and Pranav Mistry (MIT)
Six Sense are examples of increasing use of intuitive
devices that promote kinesthetic embodied relationships
with data.
In the analytics domain of the humanities, Cultural
Analytics as developed by UC San Diego, offers us
visoinary trends in large screen immersive system
visualization. Cultural Analytics researches visualization of
large-scale heterogeneous data in immersive system
displays. It uses computer-based techniques from
quantitative analysis and interactive visualization employed
in sciences, to analyze massive multi-modal cultural data
sets on gigapixels screens [37]. This project draws upon
cutting-edge cyberinfrastructure and visualization research
at Calit2 (including the aforementioned new generation
CAVE and Powerwall).
IV.

made resulting is a database of 24,000 clips of approx. 4
seconds each. Four researchers were employed to hand tag
each 4 second clip with somewhat idiosyncratic metadata
related to the images shown including emotion; expression;
physicality; scene structure; with metatags including speed;
gender; colour and so on. The result is 500 simultaneous
video streams looping each 4 seconds, and responsive to a
users search (Figures 2 & 3).

RELATED WORKS BY RESEARCHERS

Previous embodied and interactive systems visualization
by the researchers collaborating on projects in this paper
includes T_Visionarium I & II [49]. T_Visionarium I was
developed by iCinema Centre, UNSW in 2003. It takes
place in the Jeffrey Shaw’s EVE dome, an inflatable (12
meters by 9 meters). Upon entering the dome, the viewer
places a position-tracking device onto their head. The
projection system is fixed on a motorized pan tilt apparatus
mounted on a tripod. The database used here was recorded
during a week-long period from 80 satellite television
channels across Europe.
Each channel plays
simultaneously across the dome however, the user directs or
reveals any particular channel at any one time. The matrix
of ‘feeds’ is tagged with different parameters - keywords
such as phrases, color, pattern, and ambience. Using a
remote control, the viewer selects options from a
recombinatory search matrix. On selection of a parameter,
the matrix then extracts and distributes all the
corresponding broadcast items of that parameter over the
entire projection surface of the dome. For example, by
selecting the keyword "dialogue" all the broadcast data is
reassembled according to this descriptor. The viewer, by
moving their head in different directions and thus the
position of the projected image, shifts from one channel's
embodiment of the selected parameter to the next. In this
way, the viewer experiences a revealing synchronicity
between all the channels linked by the occurrence of
keyword tagged images. All these options become the
recombinatory tableau in which the original data is given
new and emergent fields of meaning (Figure 1).
T_Visionarium II in AVIE (produced as part of the ARC
Discovery, ‘Interactive Narrative as a Form of
Recombinatory Search in the Cinematic Transcription of
Televisual Information’) [49] uses 24 hours of free to air
broadcast TV footage from 7 Australian channels as its
source material. This footage was analyzed by software for
changes of camera angle, and at every change in a particular
movie (whether it be a dramatic film or a sitcom), a cut was

Figure 1. T_Visionarium I © UNSW iCinema Research Centre.

An antecedent of the T_Visionarium projects can be
found in Aby Warburg’s, Mnemosyne, a visual cultural
atlas, a means of studying the internal dynamics of imagery
at the level of its medium rather than it content, performing
image analysis through montage and recombination.
T_Visionarium can be framed by the concept of aesthetic
transcription, that is, the way new meaning can be produced
is based on how content moves from one expressive
medium to another. The digital allows the transcription of
televisual data, decontextualising the original, and
reconstituting it within a new artifact. As the archiving
abilities of the digital allow data to be changed from its
original conception, new narrative relationships are
generated between the multitudes of clips and meaningful
narrative events emerge because of viewer interaction in a
transnarrative experience where gesture is all defining. The
segmentation of the video reveals something about the
predominance of close-ups, the lack of panoramic shots, the
heavy reliance on dialogue in TV footage. These aesthetic
features come strikingly to the fore in this hybrid
environment. The spatial contiguity gives rise to news ways
of seeing, and of reconceptualising in a spatial montage [4].
In T_Visionarium the material screen no longer exists. The
boundary of the cinematic frame has been violated, hinting
at the endless permutations that exist for the user. Nor does
the user enter a seamless unified space but is confronted
with the spectacle of hundreds of individual streams.
Pannini’s picture galleries also hint at this infinitely large
and diverse collection, marvels to be continued beyond the
limits of the picture itself.

572

and recorded in the architecture of data archive and
metadata and witnessed [11]. This project builds upon the
exploration and gains made in the development of
T_Visionarium I and II.
The datasets used include over 100,000 multimedia rich
records (including audio files, video files, high resolution
monoscopic and stereoscopic images, panoramic
images/movies and, text files) from Museum Victoria and
the media art history database of the ZKM [60] that include
diverse subject areas from the arts and sciences collections.
The data is collated from collection management systems
and from web-based and exhibition-based projects.
Additional metadata and multimedia analysis will be used
to allow for intelligent searching across datasets.
Annotation tools will provide users with the ability to make
their own pathways through the data terrain, a psycho
geography of the museum collections. Gesture-based
interaction will allow users to combine searches, using both
image-based and text input methods. Search parameters
include:
• Explicit (keyword search based on collections data
and extra metadata tags added using the project
accessible through word clouds)
• Multimedia (e.g. show me all faces like this face;
show me all videos on Australia, show me
everything pink!)
• Dynamic (e.g. show me the most popular search
items; join my search to another co-user; record my
search for others to see; add tags).
• Abstract (auto generate a flow of content based on
my search input which results from an algorithm
running through the data and returning abstract
results)
This project seeks understanding in the developments of
media aesthetics. Problems of meaningful use of
information are related to the way users integrate the
outcomes of their navigational process into coherent
narrative forms. In contrast to the interactive screen based
approaches conventionally used by museums, this study
examines the exploratory strategies enacted by users in
making sense of large-scale databases when experienced
immersively in a manner similar to that experienced in real
displays [33]. In particular, evaluation studies will ask: i)
How do museum users interact with an immersive 360degree data browser that enables navigational and editorial
choice in the re-composition of multi-layered digital
information? ii) Do the outcomes of choices that underpin
editorial re-composition of data call upon aesthetic as well
as conceptual processes and in what form are they
expressed? [10]
Recent advent of large-scale immersive systems can
significantly altered the way information can be archived,
accessed and sorted. There is significant difference between
museum 2D displays that bring pre-recorded static data into
the presence of the user, and immersive systems that enable
museum visitors to actively explore dynamic data in realtime. This experimental study into the meaningful use of
data involves the development of an experimental browser

Figure 2. T_Visionarium II in AVIE © UNSW iCinema Research Centre.

Figure 3. Datasphere, T_Visionarium II © UNSW iCinema Research
Centre.

V.

CURRENT WORK

A. Data Sculpture Museum
This project is being developed as part of the Australian
Research Council Linkage Grant (2011 – 2014) “The
narrative reformulation of multiple forms of databases using
a recombinatory model of cinematic interactivity” (UNSW
iCinema Research Centre [60], Museum Victoria [53],
ALiVE City University [3], ZKM Centre for Built Media)
[59]. The aim of this research is to investigate recombinatory search, transcriptive narrative and multimodal
analytics for heterogeneous datasets through their
visualization in a 360° stereoscopic space [10]. Specifically,
the exploration of re-combinatory search of cultural data (as
a cultural artefact) as an interrogative, manipulable and
transformative narrative, responsive to and exposing of
multiple narrations that can be arranged and projected
momentarily [10] over that which is purposefully embedded

573

capable of engaging the user by enveloping them in an
immersive setting that delivers information in a way that
can be sorted, integrated and represented interactively.
Specifications of the proposed experimental data browser
include:
• immersive 360-degree data browser presenting
multi-layered and heterogeneous data;
• re-compositional system enabling the reorganization and authoring of data;
• scalable navigational systems incorporating Internet
functions;
• collaborative exploration of data in a shared
immersive space by multiple users;
• intelligent interactive system able to analyze and
respond to user's transactions.

Figure 5. 3D data distribution of IMJ Europeana dataset in AVIE.
Image: Tobias Gremmler © ALiVE, CityU

B. Inside Europeana
Another prototype project under development at ALiVE
which forms the basis for upcoming projects is focused on
providing a multi-user interactive visualization of the online
cultural collection portal Europeana [13]. Around 1500
institutions have contributed to Europeana and their
assembled collections of over 14 million records in multiple
languages (Figure 4). The recently released future directions
report for Europeana [14] emphasized the need to look for
innovations in delivery of content. Our prototype uses 5000
objects coming from the collection of Israel Museum of
Jerusalem, inside AVIE (Figures 5, 6). We are using the
limited five-field metadata that is the basis for Europeana
portal for this visualization, and the data itself will come
live from the internet repository using the API.

Figure 6. iPad interface and 3D data distribution of IMJ Europeana
dataset in AVIE. Image: Tobias Gremmler © ALiVE, CityU

C.

Rhizome of the Western Han

This project investigates the integration of highresolution archaeological laser scan and GIS data inside
AVIE. This project represents a process of archaeological
recontextualization, bringing together remote sensing data
from the two tombs (M27 & The Bamboo Garden) with
laser scans of funerary objects, in a spatial context. This
prototype builds an interactive narrative based on spatial
dynamics, and cultural aesthetics and philosophies
embedded in the archaeological remains. The study of Han
Dynasties (206 BC 220 A.D.) imperial tombs has always
been an important field of Chinese archaeology. However,
only a few tombs of the West Han Dynasty have been
scientifically surveyed and reconstructed Further, the
project investigates a reformulation of narrative based on
the application of cyber mapping principles in archaeology
[15, 28].
The application engine has been developed in order to
be completely dynamic and not dependent on the
application data. Every environment, information, models
and behaviors are specified and loaded from a configuration
file. When the application starts, the user is surrounded by
an introductory 3D level. This scenario allows the user to
select between various real 3D archaeological

Figure 4. Europeana online portal - current search return © Europeana.

574

reconstruction scenarios through intuitive iconic
representations. This Scene Browser is dynamically created
according to the total amount of models available for the
application (in the Western Han case of study it is possible
to select between two different tomb reconstructions and an
3D objects browser). The engine is able to generate two
type of scenario with different behaviors and user
experiences (Figure 7).
The second type of environment (the Object Viewer)
displays multiple virtual reconstructions of objects around
the user in a circular manner. The user can browse, magnify
and manipulate every object independently. The object
browser experience is also improved thanks to the
visualization of a facultative cloud of point where the
objects are floating in.

Figure 8. Visualizations of CAVE 220 inside AVIE. Image: Tobias
Gremmler © ALiVE, CityU/Dunhuang Academy

This work is in collaboration with the Dunhuang Academy
and the Friends of Dunhuang. Initially data from CAVE
220 will be incorporated into AVIE, together with multiple
interactive features (Figure 8). The work uses ultra high
resolution imaging data to tell stories about the
extraordinary wealth of paintings found in the caves at
Dunhuang, a nexus of cultural interchange via the Silk Road
between China, India, Persian, Greco-Roman and Central
Asia. The site is world renown for its art treasures and has
been subject to extensive digital imaging for conservation
and preservation.

Figure 7. Rhizome of the Western Han: inhabiting the tombs at 1:1 scale
© ALiVE, CityU.

At the nexus of this work is the embodiment of the user
in 360-degree 3D space. There is ample discourse to situate
the body at the forefront of interpretive archaeology
research as a space of phenomenological encounter. Postprocessual frameworks for interpretive archaeology
advance a phenomenological understanding of the
experience of landscape. In his book, Body and Image:
Explorations in Landscape Phenomenology, archaeologist
Christopher Tilley for example usefully contrasts
iconographic approaches to the study of representation with
those of kinaesthetic enquiry [52]. Tilley’s line of reasoning
provides grounding for the research into narrative agency in
large-scale, immersive and sensorial, cognitively
provocative environments [26]. This project examines a
philosophical discussion of what it means to inhabit
archaeological data ‘at scale’ (1:1). It also re-situates the
theatre of archaeology in a fully immersive display system,
as ‘the (re)articulation of fragments of the past as real-time
event’ [45].
This prototype has led to a new project to build an
interactive installation (Inside Dunhuang) using laser scan
data from the UNESCO World Heritage site of the
Dunhuang Caves (Magao Grottoes), Gobi Desert, China.

D.

Blue Dots AVIE

This project integrates the Chinese Buddhist Canon,
Koryo version Tripitaka Koreana into the AVIE system (a
project between ALiVE, CityU Hong Kong and UC
Berkeley). This version of the Buddhist Cannon is inscribed
as UNESCO World Heritage enshrined in Haeinsa, Korea.
The 166,000 pages of rubbings from the wooden printing
blocks constitute the oldest complete set of the corpus in
print format (Figure 9). Divided into 1,514 individual texts
the version has a complexity that is challenging since the
texts represent translations from Indic languages into
Chinese over a 1000-year period (2nd-11th centuries). This
is the world’s largest single corpus containing over 50
million glyphs and it was digitized and encoded by Prof
Lew Lancaster and his team in a project that started in the
70s [29, 30, 31, 32].

575

1)
•
•
•
•
•
2)
•
•
•

time, creator, and place. The Blue Dots method of
visualization is a breakthrough for corpora visualization and
lies at the basis of the visualization strategies of abstraction
undertaken in this project. The application of an omnispatial
distribution of this text solves problems of data occlusion,
and enhances network analysis techniques to reveal
patterns, hierarchies and interconnectedness (Figures 11 &
12). Using a hybrid approach to data representation
audification strategies will be incorporated to augment
interaction coherence and interpretation. The data browser
is designed to function in two modes: the Corpus Analytics
mode for text only, and the Cultural Atlas mode that
incorporates original texts, contextual images and
geospatial data. Search results can be saved and annotated.
The current search functionality ranges from visualizing
word distribution and frequency, to other structural patterns
such as the chiastic structure and ring compositions. In the
Blue Dots AVIE version, the text is also visualized as a
matrix of simplified graphic elements representing each of
the words. This will enable users to identify new linguistic
patterns and relationships within the matrix, as well as
access the words themselves and related contextual
materials. The search queries will be applied across
classical Chinese and eventually English, accessed
collaboratively by researchers, extracted and saved for later
re-analysis.
The data provides an excellent resource for the study of
dissemination of documents over geographic and temporal
spheres. It includes additional metadata such as present day
images of the monasteries where the translation took place,
which is will be included in the data array. The project will
design new omnidirectional metaphors for interrogation and
the graphical representation of complex relationships
between these textual datasets to solve the significant
challenges of visualizing both abstract forms and close-up
readings of this rich data (Figures 13 & 14). In this way, it
we hope to set benchmarks in visual analytics, scholarly
analysis in the digital humanities and, the interpretation of
classical texts.

Amount of content
1.504 texts
160.465 pages
52.000.000 glyphs
1 text includes 107 pages (34674 glyphs)
1 page includes 324 glyphs arranged in 23 rows and
14 columns
Contextual information
1.504 colophons with titles, translators, dates,
places, and other information.
202 people names (translators, authors, compilers)
98 monastery names

The Blue Dots [5] project undertaken at Berkeley as part
of the Electronic Cultural Atlas Initiative (ECAI) which
abstracted each glyph from the Canon into a blue dot, and
gave metadata to each of these Blue Dots allowing vast
searches to take place in minutes which would have taken
scholars years. In the search function, each blue dot also
references an original plate photograph for verification. The
shape of these wooden plates gives the blue dot array its
form (Figure 10).

Figure 9. Tripitaka Koreana © Korean Times
(http://www.koreatimes.co.kr/www/news/art/2010/03/293_61805.html)

As a searchable database, it exists in a prototype form
on the Internet. Results are displayed in a dimensional array
where users can view and navigate within the image. The
image uses both the abstracted form of a “dot” as well as
color to inform the user of the information being retrieved.
Each blue dot represents one glyph of the dataset. Alternate
colors indicate position of search results. The use of color,
form, and dimension for a fast understanding of the
information is essential for large data sets where thousands
of occurrences of a target word/phrase may be seen.
Analysis across this vast text retrieves visual
representations of word strings, clustering of terms,
automatic analysis of ring construction, viewing results by

Figure 10. Blue Dots: abstraction of characters to dots and pattern arrays
© ECAI, Berkeley.

576

Figure 14. Visualization of BLUE DOTS AVIE. Image: Tobias
Gremmler © ALiVE, CityU.
Figure 11. Prof Lew Lancaster interrogates the Prototype of Blue Dots
AVIE © ALiVE, CityU.
Image: Howie Lan

VI.

CONCLUSION

The four projects described begin to take on core
challenges of visual analytics, multimedia analysis, text
analysis and visualization inside AVIE to provide powerful
modalities for an omnidirectional exploration of museum
collections, archaeological laser scan data and multiple
textual datasets. The research is responding to the need for
embodied interaction and knowledge-based interfaces that
enhance collaboration, cognition and perception and,
narrative coherence. For instance, through AVIE, museum
users and scholars are investigating the quality of narrative
coherence of abstract and multimedia data, through
interactive navigation and re-organization of information in
360-degree 3D space. There will be ongoing reporting
related to the Data Sculpture Museum, which has recently
commenced as part of a three-year project, and the Blue
Dots AVIE. The upcoming work on the interactive
installation Inside Dunhuang will also be the subject of
separate reports.

Figure 12. Close up of blue dots & corresponding texts, Prototype of Blue
Dots AVIE © ALiVE, CityU. Image: Howie Lan

ACKNOWLEDGEMENTS
This paper contains information appearing in
Kenderdine & Hart 2011 [26]. The principle author would
like to acknowledge the contribution of colleagues at
ALiVE: Prof Jeffrey Shaw, William Wong and Dr Oscar
Kin Chung Au and Tobias Gremmler. Also the
contributions of members of the Department of Chinese,
Translation and Linguistics, CityU, in relation to textual
analytics, Prof Jonathan Webster and Dr John Lee. The title
‘Cultural Data Sculpting’ is inspired Zhao & Vande Moere
[54]. Data Sculpture Museum: The narrative reformulation
of multiple forms of databases using a recombinatory model
of cinematic interactivity. Partners: UNSW iCinema
Research Centre, Museum Victoria, ZKM, City University
of Hong Kong. Researchers: Assoc Prof Dr Dennis Del
Favero, Prof Dr. Horace Ip, Mr Tim Hart, Assoc Prof Dr
Sarah Kenderdine, Prof Jeffrey Shaw, Prof Dr Peter Wiebel.
This project is funded by the Australian Research Council
2011-2014. [10]. Rhizome of the Western Han. Partners:
ALiVE, City University of Hong Kong, UC Merced,

Figure 13. Visualization of BLUE DOTS AVIE. Image: Tobias Gremmler
© ALiVE, CityU.

577

[21] Horizon Report, 2010, Four to Five Years: Visual Data Analysis.
Available from (http://wp.nmc.org/horizon2010/chapters/visualdata-analysis/). Consulted Nov 30, 2010.
[22] Johnson, L., et al. 2010. The 2010 Horizon Report. Austin, Texas:
The New Media Consortium. <http://wp.nmc.org/horizon2010/>.
[23] Kasik, D. J., et al. (2009), Data transformations & representations
for computation & visualization. Information Visualization 8(4), pp.
275–285.
[24] Keim, D. A., et al. (2006), Challenges in Visual Data Analysis. Proc.
Information Visualization (IV 2006), pp. 9-16. London: IEEE.
[25] Keim, D. A., et al. (2008), Visual Analytics: Definition, Process, &
Challenges. Information Visualization: Human-Centered Issues and
Perspectives, pp. 154-175. Berlin, Heidelberg: Springer-Verlag.
[26] Kenderdine, S. (2010), ‘Immersive visualization architectures and
situated embodiments of culture and heritage’ Proceedings of IV10 14th International Conference on Information Visualisation,
London, July 2010, IEEE, pp. 408-414.
[27] Kenderdine, S. & Hart, T. 2011, Cultural Data Sculpting:
Omnispatial Visualization for Large Scale Heterogeneous Datasets,
In D. Bearman & J. Trant (Eds.) Museums and the Web, Selected
papers from Museums and the Web 2011. Philadelphia: Archives &
Museum
Informatics
(http://conference.archimuse.com/mw2011/papers/cultural_data_scu
lpting_omni_spatial_visualiza). Consulted April 30, 2011.
[28] Kurillo, G. Forte, M. Bajcsy, R. (2010), Cyber-archaeology and
Virtual Collaborative Environments, in Forte, M. (ed) 2010, BAR
S2177 2010: Cyber-Archaeology.
[29] Lancaster, L. (2007), The First Koryo Printed Version of the
Buddhist Canon: Its Place in Contemporary Research. Nanzen-ji
Collection of Buddhist Scriptures and the History of the Tripitake
Tradition in East Asia. Seoul: Tripitaka Koreana Institute.
[30] Lancaster, L. (2008a), Buddhism & the New Technology: An
Overview. Buddhism in the Digital Age: Electronic Cultural Atlas
Initiative. Ho Chi Minh: Vietnam Buddhist U.
[31] Lancaster, L. (2008b), Catalogues in the Electronic Era: CBETA
and The Korean Buddhist Canon: A Descriptive Catalogue. Taipei:
CBETA (electronic publication).
[32] Lancaster, L. (2010), Pattern Recognition & Analysis in the Chinese
Buddhist Canon: A Study of “Original Enlightenment”. Pacific
World.
[33] Latour, Bruno. (1988). Visualization and Social Reproduction. In
G.Fyfe and J. Law (Eds.). Picturing Power: Visual Depiction and
Social Relations. London: Routledge, pp. 15-38.
[34] Lee, H., et al. 2010. Integrating Interactivity into Visualising
Sentiment Analysis of Blogs. Proc. 1st Int. Workshop on Intelligent
Visual Interfaces for Text Analysis, IUI’10.
[35] Liu, S., et al. (eds.) (2010), Proc. 1st Int. Workshop on Intelligent
Visual Interfaces for Text Analysis, IUI’10.
[36] Manovich, L. (2008), The Practice of Everyday (Media) Life. In R.
Frieling (Ed.), The Art of Participation: 1950 to Now. London:
Thames and Hudson.
[37] Manovich, L. (2009), How to Follow Global Digital Cultures, or
Cultural Analytics for Beginners. Deep Search: They Politics of
Search beyond Google. Studienverlag (German version) and
Transaction Publishers (English version).
[38] Many
Eyes.
(http://www.manyeyes.alphaworks.ibm.com).
Consulted Nov 30, 2010.
[39] Maturana, H. & Varela, F. (1980), Autopoiesis and cognition: The
realization of the living, vol. 42, Boston Studies in the Philosophy of
Science, Dordrecht: D. Reidel.
[40] McCandless, D. (2010.) The beauty of data visualization [Video
file].
(http://www.ted.com/talks/lang/eng/david_mccandless_the_beauty_
of_data_visualization.html). Consulted Nov 30, 2010.
[41] McGinity, M., et al. (2007), AVIE: A Versatile Multi-User Stereo
360-Degree Interactive VR Theatre. The 34th Int. Conference on

Researchers: Assoc Prof Dr Sarah Kenderdine, Prof
Maurizio Forte, Carlo Camporesi, Prof Jeffrey Shaw. Blue
Dots: Tripitaka Koreana. Partners: ALiVE, City University
of Hong Kong, UC Berkeley, Researchers: Assoc Prof Dr
Sarah Kenderdine, Prof Lew Lancaster, Howie Lan, Prof
Jeffrey Shaw, Tobias Gremmler
REFERENCES
[1]

[2]
[3]

[4]
[5]
[6]

[7]
[8]

[9]

[10]

[11]
[12]
[13]
[14]

[15]

[16]

[17]
[18]

[19]

[20]

Advanced Visualization and Interaction Environment (AVIE)
(http://icinema.unsw.edu.au/projects/infra_avie.html).
Consulted
Nov 30, 2010.
Allosphere, Available (http://www.allosphere.ucsb.edu/). Consulted
Nov 30, 2010.
Applied Laboratory for Interactive Visualization and Embodiment –
ALiVE, CityU, Hong Kong (http://www.cityu.edu.hk/alive).
Consulted Nov 30, 2010.
Bennett, J. 2008, T_Visionarium: a Users Guide, University of New
South Wales Press Ltd.
Blue Dots (http://ecai.org/textpatternanalysis/). Consulted Nov 30,
2010.
Chinchor, N, Thomas, J., Wong, P. Christel, M. & Ribarsky, W.,
2010, Multimedia Analysis + Visual Analytics = Multimedia
Analytics, September/October 2010, IEEE Computer Graphics, vol.
30 no. 5. pp. 52-60.
Chion, M., et al. (1994), Audio-Vision, Columbia University Press.
Collins, C. Carpendale, S. & Penn, G. (2009), DocuBurst:
Visualizing Document Content using Language Structure. Computer
Graphics Forum (Proceedings of Eurographics/IEEE-VGTC
Symposium on Visualization (EuroVis '09)), 28(3): pp. 1039-1046.
DeFanti, T. A., et al. 2009. The StarCAVE, a third-generation
CAVE & virtual reality OptIPortal. Future Generation Computer
Systems, 25(2), 169-178.
Del Favero, D., Ip, H., Hart, T., Kenderdine, S., Shaw, J., Weibel, P.
(2009), Australian Research Council Linkage Grant, “Narrative
reformulation of museological data: the coherent representation of
information by users in interactive systems”. PROJECT ID:
LP100100466
Deleuze, G. 1989 Cinema 2: the Time Image. Translated by Hugh
Tomlinson and Robert Galeta, Minnesota: University of Minnesota.
Electronic Cultural Atlas Initiative (www.ecai.org). Consulted Nov
30, 2010.
Europeana (http://www.europeana.eu). Consulted Nov 30, 2010.
Consulted March 30, 2011.
Europeana, 2011, Comité des Sages, The New Renaissance,
Europeana
Report
(http://ec.europa.eu/information_society/activities/digital_libraries/c
omite_des_sages/index_en.htm). Consulted April 30, 2011.
Forte, M. (2010), Introduction to Cyberarcheology, in Forte, M (ed)
Cyber Archaeology, British Archaeological Reports BAR S2177
2010.
Green, T. M., Ribarsky & Fisher (2009), Building and Applying a
Human Cognition Model for Visual Analytics. Information
Visualization, 8(1), pp. 1-13.
Harris, J. & Kamvar, S. (2009), We feel fine. New York, NY:
Scribner.
Harrison, C. & Romhild, C. (2008), The Visualization of the Bible.
(http://www.chrisharrison.net/projects/bibleviz/index.html).
Consulted Nov 30, 2010.
Havre, S., et al. (2000), ThemeRiver: Visualizing Theme Changes
over Time. Proc. IEEE Symposium on Information Visualization,
pp. 115-123.
HIPerSpace CALIT2. 2010. Research Projects: HIPerSpace.
(http://vis.ucsd.edu/mediawiki/index.php/Research_Projects:_HIPer
Space). Consulted Nov 30, 2010.

578

[42]

[43]

[44]
[45]
[46]
[47]
[48]

[49]

[50]

Computer Graphics & Interactive Techniques, SIGGRAPH 2007, 59 August 2007.
Museum Victoria, example collections include Social History
(http://museumvictoria.com.au/collections)
&
Bio-security
(http://www.padil.gov.au). Consulted Nov 30, 2010.
Nechvatal, J. (2009), Towards an Immersive Intelligence: Essays on
the Work of Art in the Age of Computer Technology and Virtual
Reality (1993-2006) Edgewise Press, New York, NY.
Paley, B. 2002. TextArc (http://www.textarc.org). Consulted Nov
30, 2010.
Pearson, M. & Shanks, M. (2001) Theatre/Archaeology, London:
Routledge.
Pike, W. A., et al. (2009), The science of interaction. Information
Visualization, 8(4), pp. 263–274.
Ricoeur, P. (2004), Memory, History, Forgetting, University of
Chicago Press.
Speer, R., et al. (2010), Visualizing Common Sense Connections
with Luminoso. Proc. 1st Int. Workshop on Intelligent Visual
Interfaces for Text Analysis (IUI’10), pp. 9-12.
T_Visionarium
(2003-2008),
(http://www.icinema.unsw.edu.au/projects/prj_tvis_II_2.html).
Consulted Nov 30, 2010.
Thai, V. & Handschuh, S. 2010. Visual Abstraction and Ordering in
Faceted Browsing of Text Collections. Proc. 1st Int. Workshop on
Intelligent Visual Interfaces for Text Analysis (IUI’10), 41-44.

579

