Compact Visualisation of Multimedia Interaction Records
Saturnino Luz
Department of Computer Science
Trinity College, University of Dublin
Dublin 2, Ireland
luzs@cs.tcd.ie

Abstract
This paper presents an approach to building compact
and effective information visualisation interfaces to allow
users to browse and access content stored in continuous media. We introduce a content mapping framework which supports time-based indexing of recordings of computer supported collaborative activity. In order to illustrate the temporal aspect of content mapping, we present HANMER,
a system designed to provide fast access to audio-textual
meeting recordings on Personal Digital Assistants (PDAs).
HANMER embodies techniques that exploit the processing
capabilities and functionality of modern PDAs for visualisation and retrieval of multimedia meeting data.

1. Introduction
Storage of information in digital format has become
ubiquitous. The widespread availability of digitally encoded content has given rise to a large variety of interesting
applications and created challenging research problems. As
any e-mail user knows, humans are ill-equipped to deal with
large volumes of data. With the advent of the world-wide
web, a great deal of research has focused on ways of overcoming the so called information overload problem. Information retrieval, text categorisation and other natural language processing techniques have achieved some success in
managing information overload in text data. However, increasing amounts of data being made available nowadays
are stored in continuous media such as audio and video.
Locating and accessing relevant information in these data
repositories is a much harder task.
The general consensus in the research community appears to be that effective retrieval of information from continuous media should be content-based, supporting operations such as search, ﬁltering, summarisation, and alerting [12]. The content-based approach therefore focuses
on modality translation (speech to text, for instance) and

Masood Masoodian
Department of Computer Science
The University of Waikato
Private Bag 3105, Hamilton, New Zealand
m.masoodian@cs.waikato.ac.nz

related techniques. Typical content-based information retrieval applications include indexing of broadcast news and
digitalised records of meetings [8, 14]. These applications
involve developing speech recognisers capable of coping
with noisy environments as well as large-vocabulary spontaneous speech, detecting communicative acts, and tracking prosody, gestures and facial expressions. Each of these
sub-tasks present considerable difﬁculties. State-of-the art
speech recognisers still have fairly high word error rates
(as high as 20 − 60% for large vocabulary conversational
speech). More abstract levels of content indexing are also
problematic. In [13], for instance, a system is described
which attempts to recognise dialogue acts (i.e. to group utterances into classes such as statements, yes-no-questions,
wh-questions, quotations etc) in spontaneous speech. The
maximum accuracy achieved by that system was 65%, compared to a chance baseline accuracy of 35%. Even if recognition and dialogue labelling were perfect, there is no guarantee that users of a meeting browser based on labelled dialogue acts would be able to use those labels effectively
for information retrieval. This is illustrated by the fact that
when dialogue act classiﬁcation is done by humans, interannotator agreement is far from perfect ([13] reports an 84%
agreement rate among linguistics student annotators) which
shows that dialogue acts may not necessarily be intuitive
starting points for meeting browsing.
In what follows we present an alternative to the contentbased approach which places greater emphasis on information visualisation and pattern recognition. Our approach,
whose general framework we call content mapping [6] targets mainly information visualisation and retrieval from audio recordings accompanied by some form of textual interaction. Content mapping consists essentially of timebased indexing, feature selection, and determination of text
and speech neighbourhoods. We argue that content mapping can provide the basis for effective visualisation of data
stored in continuous media, and illustrate our claim by presenting a system designed for visualisation of audio-textual
meeting recordings on Personal Digital Assistants (PDAs).

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

2. Multimedia interaction records
This paper focuses on visualisation of recordings of interactive meetings. We will therefore describe the nature
of the data, the scenarios in which they were collected, and
the recording method used, before presenting the information visualisation techniques we have developed to handle
these data. The data have been recorded from small-group
meetings mediated through (at least) two communication
modalities: speech and text. We will henceforth refer to
these modalities as the speech channel and the text channel, and to the generic class of meetings in which they are
employed as non-collocated speech-and-text meetings.
We have chosen computer-mediated, synchronous collaborative writing activities supported by a speech channel
as the main data source in our pilot studies because this
scenario is typical of interactive multimedia recordings in
many ways. The features which distinguish speech-enabled,
synchronous collaborative writing from other types of
computer-mediated work are the immediacy of textual interactions, and the ubiquity of transient contributions (such
as discussions and back channels) conveyed by the speech
mode. The following types of collaborative activities can be
seen as specialisations of speech-and-text meetings:
• informal, non-collocated encounters where a shared textual tool or whiteboard acts as a focal point and serves
as a medium for exchange of low level details (such as
web or email addresses, formulae, etc),
• formal meetings, where the shared textual component
may act as a focal point, or as collaboratively built lasting record of the transient (speech) interaction,
• document evaluation and revision meetings, particularly
those supported by shared real-time editors, in which
text is loaded onto the collaborative tool at the beginning
of the meeting, and modiﬁed as the meeting progresses,
• the variants of collaborative writing described in [9] as
joint writing in which several group members compose
the text together, and even small components of the text
are decided by group effort,
• shared projects, in particular in the initial phases during which group members discuss and agree on an interpretation of the problem, deﬁne their goals and plan
their work, as well as the integration phase during which
group members integrate their individual inputs.
The prototypical case also bears a relationship with more
generic types of collaborative meetings.
In order to collect data, we have set up a software environment in which synchronous collaborative writing is fully
supported. The conﬁguration we have implemented should
also be ﬂexible enough to cover the more specialised scenarios described above. In our current setup, communication
takes place through native IP Multicast on local area networks (LAN), and through the MBONE [5] on the Internet.

The architecture consists essentially of a meeting memory
server and workstations which support real-time shared editors and multi-party audio conferencing. The protocol used
for both text and audio is the Real Time Protocol, RTP [10].
RTP provides the timestamps needed for mapping temporal
neighbourhoods (see section 3) as part of its built-in packet
delivery mechanism. During a meeting, both text and audio are multicast among the meeting participants, and the
server acts as a passive RTP listener, decoding RTP packets
and recording audio and text on disk. Further processing of
RTP audio and text is done mostly off-line.
A typical RTP payload deﬁnition splits the transported
streams into 10 millisecond packets. Timestamps play the
role of helping multicast clients reconstruct and synchronise
these streams. This is specially important for media such
as audio and video, but less relevant in text, and obviously
too ﬁne-grained for the purposes to temporal mapping. For
simplicity we assumed text segments to be paragraphs (including headings) and generated annotation relating units of
text, time and user action using an annotation scheme coded
in the Extensible Markup Language (XML).

3. Content mapping
At the core of our approach is the notion of content mapping between temporal neighbourhoods (TN) and contextual neighbourhoods (CN). TN and CN build on the assumption that text and speech can be clustered into natural segments, providing an intuitive starting point for information visualisation. Different levels of analysis will determine different types of text and audio segments. These
may vary from segments derived exclusively from formatting and markup meta-information to techniques such as the
ones presented in [14] and [13] which aim at producing segments modelled on high-level theories of human communication and cognition. Examples of text segments include
paragraphs, document sections, items in a list, etc. Examples of speech segments include communicative turns, audio intervals delimited by silences, speech acts, etc. As we
are mainly interested in investigating relationships between
audio and text neighbourhoods, we will leave the segmentation method unspeciﬁed for the moment, and concentrate
on deﬁning TN and CN at an abstract level. Thus we say
that a segment of audio recording is in a temporal neighbourhood of a text segment if that audio segment (i) was
recorded while the section was being created, changed, or
discussed by the participants, or (ii) is in a temporal neighbourhood of a related text segment. There could be multiple
audio segments in the temporal neighbourhood of a document section, each corresponding to different time intervals
during which that section was active. A segment of the audio recording is in a contextual neighbourhood of a document segment when it shares a certain number of keywords

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

(or key-phrases) with that segment. Once again, a document section can have multiple audio segments in its neighbourhood. These notions can be more precisely described in
terms of functions and sets. Given a set T = {t1 , ..., t|T | } of
text segments, and a set S = {s1 , ..., s|S| } of audio (speech)
segments, temporal neighbourhoods are determined by interval overlaps stated in Deﬁnition 1.

The system described below
is based on the MBone, an
Internet multicast
technology that has spread
considerably in recent
years.
First, we will describe the
system architecture, showing
how RTP data can be used
in segmenting and accessing
recorded audio data.

where ts and te denote the start and end time of segment t.
If we let keyword(ti , tk ) indicate that segments ti and
tk share at least one keyword (or phrase), tn(ti ) could
be extend to recursively include intervals sj such that
keyword(ti , tk ) ∧ sj ∈ tn(tk ), thus implementing a form
of text clustering. Once tn has been constructed, one can
also retrieve speciﬁc texts segments using audio as a starting point by simply inverting the mapping, or deﬁning an
audio-text mapping tna : A → 2T , such that:
tna (si ) = {tj : si ∈ tn(tj )}

(1)

Similarly, one can describe a contextual text-audio mapping cn as in Deﬁnition 2. The deﬁnition of its audio-text
counterpart cna is analogous to that of equation (1).
Deﬁnition 2 A contextual text-audio mapping is a function
cn : T → 2A deﬁned as follows:
cn(ti ) = {sj : keyword(ti , sj )}
where keyword(ti , sj ) denote pairs of text and audio segments which share at least one keyword (or phrase).
The relation T ⊆ A × T induced by tn is what we call a
temporal neighbourhood. A contextual neighbourhood is a
relation C ⊆ A × T induced by cn.
From an implementation perspective, the most attractive
feature of temporal neighbourhoods is the fact that their extraction from a meeting record does not require any speech
recognition at all. Time stamps and keyword spotting are
all that is needed. Figure 1 illustrates text and audio components linked via temporal mappings. The time line widget
indicates presence of speech by means of horizontal bars
stretching along the time axis. The words and phrases highlighted on the text pane are the starting point of the browsing activity. When related clusters of text are selected, the
speech event viewer highlights the relevant segments.
Contextual neighbourhood inference, on the other hand,
demands either full meeting transcription or, at the very
least, keyword extraction from text followed by word spotting on audio via speech recognition. Manual speech transcription would be impractical, while automatic transcription of conversational speech tends to suffer from a number

Temporal maps

Audio segments with
matching timestamps.

Deﬁnition 1 A temporal text-audio mapping is a function
tn : T → 2A deﬁned as follows
tn(ti ) = {sj : tsi ≤ ssj ≤ tei ∨ tsi ≤ sej ≤ tei
∨ssj ≤ tsi ≤ sej ∨ ssj ≤ tei ≤ sej }

text cluster
paragraph 10
paragraph 35
...

Figure 1. A temporal neighbourhood
of problems, including noise, disﬂuency, false starts, and
overlaps [11] which result in high word error rates for even
the best recognisers currently available. The situation is further complicated if one considers the fact that proper names
and other named entities, which are likely to constitute an
important class of the information one would need to extract from the audio track, tend to be out of the vocabulary
of most systems. A combination of keyword extraction and
automatic word spotting appears therefore to be the best
way to exploit the intrinsic characteristics of speech-andtext meetings. In this paper, however, we will focus solely
on methods for inferring temporal mappings, and information visualisation techniques derived from these methods.

3.1. Meeting indexing and browsing
Once the text produced during the meeting has been
properly timestamped, temporal mappings can be easily determined by linking text segments to partially co-occurring
audio segments (Deﬁnition 1). Partial co-occurrence can
be immediately extracted from XML markup. If we want
to extend TNs by recursively linking audio to clusters of
keyword-related text segments things get more complicated.
Not all nouns (and phrases) can be regarded as keywords. If
they were, nearly all text segments would be interrelated,
rendering the temporal mapping technique useless. All relevant segments would be recalled but the user would be
overloaded with information of very low precision. In order to select the most relevant words and phrases we use
a module comprising part-of-speech tagging (POS), stopword removal, collocation analysis and feature extraction.
POS tagging assigns each word in the text a grammatical
category. This phase is necessary as a pre-processing stage
to collocation analysis and the removal of very common
and closed class words, such as determiners, auxiliaries and
conjunctions. We use the transformation-based algorithm
of [1] which yields an overall tagging accuracy of 96.5%.
Stop-word removal consists simply of table lookup. Collocation analysis aims at ﬁnding phrases which may be selected as representative features of a text segment. The
approach used in this module is a POS ﬁltering algorithm
adapted from [2]. It consists of selecting POS sequences

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

that are likely to form phrases. Good candidate patterns include nouns followed by nouns (e.g. “speech recognition”,
“meeting browser”), adjectives followed by nouns or proper
nouns (e.g. “partial information”, “passive RTP”), adjectives followed by adjectives and nouns (e.g. “Gaussian random variable”), and many others.
Finally, the problem of selecting those terms that best
characterise a text segment can be recast as a problem
widely studied in the machine learning literature: feature
selection [7]. The task can be described as a reverse classiﬁcation task where each text segment represents a category,
and one wishes to ﬁnd the main features of that category.
We have employed an information theoretic measure known
as expected mutual information, or information gain which
allows aggressive reductions of feature sets while preserving classiﬁcation accuracy. The output of the information
extraction module is a word table containing keywords and
phrases, along with the text segments in which they occur.
TN-based meeting indexing is largely a matter of cross referencing word tables and the timestamps extracted from audio and text media in the manner prescribed by Deﬁnition 1.

3.2. Interleave factor
The approach to meeting indexing described above rests
on the assumption that co-occurrence of events may provide
valuable clues for information retrieval. In this section we
present a metric based on the degree of “interleaving” of
text and speech in collaborative meetings which attempts to
elicit such clues. We call this metric Interleave Factor (IF )
[3]. IF is an essential part of the visualisation interface
presented below.
The meeting browser software keeps track of speech and
text events and stores its records on the meeting server.
Based on those records one may draw charts of speech and
text on a time axis, grouping them into clusters of, say, 10
seconds in length. Figure 2 shows a snapshot of a speechand-text interaction between two participants P1 and P2.
Solid boxes represent voice events and dotted boxes represent text events. In this section we use point charts of small
intervals in order to explain the IF formula. Real meeting
charts exhibit a much more complicated picture, as will be
seen in the next section. IF scores, however, should be insensitive to meeting duration.
We propose a metric founded on a continuous probability
distribution, where the probability of a speech event a in a
meeting m is given by equation (2), where superscripts s
and e denote start time and end time, respectively.
P (a) =

ae − as
me − ms

(2)

Similar probabilities should be estimated for text events.
The basic idea behind IF is that the interleave factor will be

audio
P2
text

I1

I2

s11

s12

t11

...

s21

I3
s31

s22

t31

audio
P1
text

L

20

10

30
time (secs.)

Figure 2. Meeting activity intervals
determined by the probability that a speech and a text event
overlap in a certain time interval. In order to estimate this,
we split the meeting into small (say, 10s) intervals, estimate
a partial IF for each sub-interval separately, and then add
them up to arrive at the ﬁnal value. Let’s assume a meeting
of total duration M seconds split into sections of L-second
long intervals I1 , ..., Ii , where 1 ≤ i ≤ M/L, as shown in
Figure 2. For each of these intervals, equation (3), where m
is the number of text segments and k the number of speech
segments, gives the probability that speech and text events
(respectively Si and Ti ) in that interval will overlap.

P (Si ∩ Ti |Ii ) =

k
j=1 sij

m
j=1 tij

L2

(3)

The probability thus expressed gives a measure of overlapping for a single participant with respect to text and
speech channels. The formula can be further generalised
to cover an arbitrary number of activity channels, and an
arbitrary number of participants. The former is a theoretical possibility which we have not investigated empirically
so far. The latter is necessary for calculating the overall IF
for speech-and-text meetings. Generalising (3) to an arbitrary number of channels would be useful if, for instance, a
video or a collaborative web browsing channel were added
to the set of meeting support tools. A straightforward generalisation is obtained by assuming audio, video, text and
other channels to be members of a set C = {c1 , ..., cn } and
multiplying over interval activity as shown in (4).

P(

Ci |Ii )

=

n
l=1

k
j=1 clij
n
L

(4)

There are two ways of calculating P (Si ∩ Ti |Ii ) for two
or more participants, each of which reﬂects a particular aspect of interaction proﬁle. The ﬁrst involves arranging the
intervals for each stream and each participant serially, and
the resulting (extended) intervals as in (3) except that the
denominator will now be pL2 , where p is the number of
participants. This gives rise to what we call serial IF , or

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

IFs for short. IFs reﬂects the average degree of concurrency per participant. The second way of calculating event
overlap probabilities involve merging active intervals for all
participant streams before summing them up. We call the
metric based on this way to estimate event overlap parallel
IF , or simply IF . This gives us a measure of global activity interleave. IF is obtained by combining the conditional
probabilities and normalising the result though the number
of intervals, as shown in (5).

IF

=

M/L
i=1

P (Si ∩ Ti |Ii )
M/L

(5)

4. Mobile meeting browser
The concepts of temporal neighbourhood and interleave
factor have been implemented in a Handheld Meeting
Browser (HANMER) prototype. HANMER consists of a
text and a graphical component which allow the user to
browse text segments, and at the same time visualise regions
in temporal neighbourhoods of those segments against the
overall meeting proﬁle. The graphical component draws a
2D step-function chart along a time line which represents
a highly compact visual summary of meeting activity. The
user can interact with the chart by zooming into particular
areas, and selecting intervals to be played back (audio) and
segments to be displayed (text) by the server. Visual patterns provide an indication of the level of interleaving of a
particular sub-interval.

B/text, and B/audio along the vertical axis indicate text and
audio produced by meeting participants A and B respectively. Continuous horizontal lines represent stretches of
non-overlapping activities, dense areas of broken vertical
lines indicate intervals with high levels of interleave, and
thick continuous lines represent speech exchanges accompanied by little text activity or vice-versa. In Figure 3, for
instance, there is clear predominance of speech during the
initial stages (ﬁrst 250 seconds) of the meeting, and a more
balanced pattern of speech and text from that point on. This
is explained by the fact that participants spend a few minutes establishing contact, exchanging greetings, ﬁne-tuning
the audio hardware, etc before task execution actually starts.
HANMER’s PDA interface uses similar charts. Initially,
the user is presented with a bird’s eye view of the meeting
(Figure 4, on the lower halves of the screen shots), and a
text component which usually drives the search. The user
can also scan the meeting by zooming into sections, and requesting retrieval of text segments, or audio playback. If
the user is interested in a subject referring to a particular
text segment, they may select it thereby causing all intervals of the meeting in which that subject was active to be
highlighted on the meeting proﬁle (Figure 4, right). This
narrows the search down to a few segments. However, since
browsing also involves audio, listening to even a small number of segments might turn out to be a very time-consuming
task. We address this issue by ranking the candidate segments according to the IF relevance metric.

5. Conclusions and further work

A/text

A/audio

B/text

B/audio

IF A= 0.12291 , IF B= 0.0073991 , Inter−participant IF= 0.06

0

500

1000

1500

2000

Time (in seconds)

Figure 3. Step plot of a speech-audio interval
Figure 3 shows a 37-minute meeting represented as an
IF chart. The top of the vertical line deﬁnes the point at
which an event occurred. The ticks labelled A/text, A/audio,

We have presented an approach to structuring and accessing multimedia meeting records which, unlike recent
approaches based on speech recognition technology, views
time rather than space as the unifying dimension for speech
and text indexing. Content mapping provides a stepwise
strategy for implementation as well as a conceptual framework for experimental research. It builds on the notions
of contextual and temporal neighbourhoods. While contextual neighbourhoods encompass the elements necessary
to incorporate speech and language technologies into the
content mapping framework, simpler temporal neighbourhood techniques can play a surprisingly effective role in
meeting indexing and visualisation. In this paper we described HANMER, a system designed for meeting browsing on small screen devices which illustrates an application
of temporal neighbourhoods.
Preliminary evaluation of information search tasks on
student-supervisor meeting records [6, 4] suggest that IF
provides a reliable indicator of segment relevance and
greatly improves search effectiveness. In addition, the fact
that HANMER uses lightweight temporal neighbourhood
indexing techniques coupled with a highly compact visual-

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

Text component

Hi IF region

Time neighbourhoods
for the selected text
segment (highiligted)

Audio component
Temporal navigation

Activity overview

Figure 4. HANMER on a Zaurus SL-5500 PDA: text-driven browsing (left) and meeting proﬁle (right)
isation interface makes it particularly suitable for operation
on small devices.
Future research will involve collecting larger amounts of
data in order to further establish the relationship between
IF and effectiveness of information access under varied circumstances, as well as the characteristics of temporal mapping in the presence of richer interaction logs. Techniques
for recovering reliable contextual mappings from meeting
data and improving key word and phrase detection, the use
of limited speech recognition, and more structured information extraction from textual records are currently being
investigated.

References
[1] E. Brill. Some advances in transformation-based part of
speech tagging. In Proceedings of the 12th National Conference on Artiﬁcial Intelligence. Volume 1, pages 722–727,
Menlo Park, CA, USA, July 31–Aug. 4 1994. AAAI Press.
[2] J. S. Justeson and S. M. Katz. Technical terminology: some
linguistic properties and an algorithm for identiﬁcation in
text. Natural Language Engineering, 1(1):9–27, 1995.
[3] S. Luz. Interleave factor and multimedia information visualisation. In H. Sharp, P. Chalk, J. LePeuple, and J. Rosbottom, editors, Proceedings of Human Computer Interaction
2002, volume 2, pages 142–146, London, 2002.
[4] S. Luz and M. Masoodian. Supporting online meetings between research students and supervisors. In Proceedings of
E-Learn 2002, pages 1870–1873, Montreal, 2002.
[5] M. Macedonia and D. Brutzman. MBone provides audio
and video across the Internet. IEEE Computer, 27(4):30–
36, 1994.

[6] M. Masoodian and S. Luz. COMAP: A content mapper
for audio-mediated collaborative writing. In M. J. Smith,
G. Savendy, D. Harris, and R. Koubek, editors, Proceedings
of HCI International, pages 208–212, New Orleans, 2001.
[7] T. M. Mitchell. Machine Learning. McGraw-Hill, 1997.
[8] N. Morgan, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
A. Janin, T. Pfau, E. Shriberg, and A. Stolcke. The meeting
project at ICSI. In Procs. of Human Language Technologies
Conference, San Diego, 2001.
[9] I. R. Posner and R. M. Baecker. How people write together.
In Readings in Computer Supported Collaborative Work,
pages 239–250. Morgan Kaufmann, 1993.
[10] H. Schulzrine, S. Casner, R. Frederick, and V. Jacobson.
RTP: A transport protocol for real-time applications. IETF
Internet Draft draft-ietf-avt-rtp-new-04, February 1999.
[11] E. Shriberg, A. Stolcke, and D. Baron. Observations on
overlap: Findings and implications for automatic processing of multi-party conversation. In Proceedings of EUROSPEECH, vol 2, pages 1359–1362, Aalborg, 2001.
[12] A. F. Smeaton. Indexing, browsing, and searching of digital video and digital audio information. In M. Agosti,
F. Crestani, and G. Pasi, editors, 3rd European Summer
School on Information Retrieval, vol. 1980 of Lecture Notes
in Computer Science, pages 93–110. Springer-Verlag, 2001.
[13] A. Stolcke, K. Ries, E. Shriberg, R. Bates, D. Jurafsky,
P. Taylor, R. Martin, C. Ess-Dykema, and M. Meteer. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics,
26(3):339–373, 2000.
[14] A. Waibel, M. Brett, F. Metze, K. Ries, T. Schaaf, T. Schultz,
H. Soltau, H. Yu, and K. Zechner. Advances in automatic
meeting record creation and access. In Procs. of the International Conference on Acoustics, Speech and Signal Processing, 2001.

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

