Shape of the Story: Story Visualization Techniques
X, Zeng, Q. H. Mehdi and N. E. Gough
Multimedia & Intelligent Systems Research Group
School of Computing and Information Technology
University of Wolverhampton, Wolverhampton, WV1 1EQ, UK
E-Mail: x.zeng@wlv.ac.uk

Abstract
There is now a growing awareness of the possibilities
for application of natural language in the computing
domain by incorporating knowledge of human-to-human
interaction. In this paper we present an approach that
aims to bridge the gap between scripting/storyboarding by
a non-technical creative writer and rendering characters
and scenes by a graphics specialist. Natural language
processing (NLP) and 3D graphic presentation is used to
manipulate 3D scenes in real time. The proposed
technique offers a flexible and easy way to generate an
interactive 3DVE as compared with traditional 3D
packages.

1. Introduction
The basic plot of a story does not change very much
but the way that we tell it does. The ways of storytelling
are challenged and changed by text, image and sound
technology developments. Unlike text-based format
storytelling, the readers and listeners convey the words
into a virtual environment in their own mind. Story
visualization focuses on using visual metaphors to
represent non-visual contents and relationships of the story
[22]. Picture books, theatre, television and movies, etc
have one thing in common: they extract abstract
information of the story into concrete visual scenes to
enhance the entire experience and evoke emotions of the
audience. The multimedia computer with its capacity for
handling text, sound and images has a profound effect on
this process [14]. The increasing development of Three
Dimensional Virtual Environment (3DVE) as a popular
concept has produced a new medium for storytelling. The
power of realistic images and interaction give participants
more freedom to interact with the story, but how to build
this kind of VE still lies in the domain of highly skilled
professionals. Menu-based interface design tools (i.e.
Maya, 3DMax, SoftImage, etc) appear as mainstream for
building a 3DVE. However, learning how to use them
requires an enormous amount of time and effort; even the
most expert 3D animators need to spend considerable time

to create interactive 3DVE that could be described in just a
few sentences from a story. The awareness of possibilities
for integrating a Natural Language (NL) interface and
computer graphics by incorporating knowledge of humanto-human interaction has been considered in improved VE
tool user-interfaces. NL is the common tool that people are
most familiar with as it allows them to describe visual
ideas and visual scenes in a straightforward manner.
Generation of a 3DVE by using NL input does not require
substantial training and could speed up the whole process.
In this paper we introduce an approach that enables
non-professionals to create an interactive 3DVE through
manipulating Visual Features of the aspects of
environment by using story-based NL input. The next
section investigates related work. Section 3 explains the
basic idea of our approach, presents the proposed
methodology of the system and focuses on how to
integrate NLP and computer graphics to generate a story
based VE. A simple example is provided in Section 4 and
finally Section 5 draws conclusions and also mentions
future work.

2. Related Work
There has been some research to apply NL in character
animation and behaviour, such as the implementation of a
command interpreter based on NLP techniques to control
or guide animated characters to achieve a task in VE [2, 6,
19]. Some attempts have been made to synthesize NL
instruction to dynamically alter the behaviours of agents
during the graphical output of the simulations [4, 16].
Authors [5] and [15] have developed behavioural
animation systems to generate appropriate facial
movement, gesture and body posture by synthesising
speech and 3D character animation. Research on automatic
construction of visual scenes based on NL input can be
dated back to the early 70s since the SHRDLU system
[20]. SPRINT [21] and Put [7] are language-based systems
that focus on spatial relationships to reconstruct 3D
models of the world, and output the corresponding image
on the graphic display. CarSim [9] implemented NL

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

descriptions to generate animation scene and replay of
accidents. [13] used multi-dimensional fuzzy functions to
match the linguistic description to present scenes
reconstruction from conceptions of 2D urban parks. More
recently, [8] presented a system called WordsEye for
automatically converting text into representative 3D
scenes by using linguistic analysis and depiction
techniques. This system relies on a large database of 3D
models and poses to depict still scenes.
These efforts either focus on implementation of NL in
character animation and behaviour or represent the still 2D
images and scenes without creating real-time interactive
3DVE. Some research has been carried out on story
understanding [11], but concentrates on natural language
understanding without integrating these ideas with
physical representation. In contrast, we describe a
methodology that enables non-professionals to generate a
3DVE by using simplified story-based NL input.
Furthermore, the system allows the user to manipulate the
visual features of VE in real-time.

3. System Overview
Our system is based on the system developed by [12]
which consists of several modules that are written in Java,
XML (eXtensibe Markup Language) and VRML. The
system adopted in this work is shown in Figure 1. The
sentence is tagged and output to a knowledge base system
and then the output XML-tagged parse tree is interpreted
into a semantic representation. Finally the semantic
representation is converted into a set of parameterized data
by a Descriptionary (a dictionary that uses XML based
word frames to parameterize visual or describable words),
which can be used by the graphic engine to generate the
atmosphere or objects and ultimately to construct a virtual
environment.

Sentence

Language
Tagging
Graphic
Engine
Knowledge
Base

3DVE

Language
Engine

Figure 1. The overview of the system

3.1. Language Engine
Arbitrary text is abstract and inherently non-spatial, a
story contains enough detailed information to clearly
portray the temporal, spatial information and events that
can be understood and analysed by people. We argue that
it is unnecessary to involve a complicated description to
reconstruct the 3D scene. For creating a 3DVE by storybased NL input, we should just focus on the direct use of
visual features of the environment, such as colour, size,
material, direction and spatial features, etc. Hence we may
start with children’s picture books to understand the
structure of the story. Moreover, it includes constraints and
context to help avoid ambiguity. This affords the ability to
deal with limited syntax and semantics by ignoring many
of the problems of unknown or meaningless vocabulary
and unconstrained grammar.
NLP uses known data about words and predefined
rules that show authorized word structures to give the
meaning of what was said to enable communication
between people and computers [18]. While the technique
of NLP is still immature, [7] suggested that dealing with
NL and graphical complexities requires finding a way to
restrict both the input language and the conceptual domain
of the systems. Because our main goal is to represent 3D
graphics by text input rather than create some new
language algorithms, currently available computer
linguistic techniques will be sufficient to solve the
problem of semantic presentation. The methodology
includes concepts from [17] and [18]. For our system,
additional constraints are also added as the structure of the
story (i.e. follow information in order, time to location,
object to attribute, static scene to dynamic episodes). XML
has been used as the primary data structure in the language
engine. Within the NLP community, XML is accepted as
the standard for data representation, especially for
purposes of interchange and software interoperability [1].
As XML can be extended and embedded in Java, objects
of different data types can be passed between them. This
allows conditions to return the various test results as either
a Boolean type or a Java string. XML and the Java based
language engine have two tasks in this system: one is to
find matched VRML objects from text and the other is to
translate object depictions (e.g. adjective, prepositions and
verbs) into parameterized data definitions for use by
graphic engine to create a 3DVE (i.e. object’s attribute,
spatial relationships and actions or events).
Our language engine consists of language tagging and
knowledge base modules. Each processes the text in a
different way and the output is used by the next
components without loss of initial information. The
construction of the language engine is shown in Figure 2.
The first component is the language tagging, which
utilizes annotation rules to perform lexical lookup, and

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

finds semantic role-definitions of words and phrases in the
sentences. The words are grouped and categorized as
follows: nouns to denote objects, adjectives to modify
Language
Tagging

Semantic
Presentation

Lexicon

Grammar
Rules

Knowledge Base
Semantic
Parameter

on having a large set of visual features that affect the
environment specified. The values assigned to these
parameters affect the generation of the 3D scene, such as
lighting conditions, colour, material, sizes, etc. Hence, an
extendable dictionary called “Descriptionary” was
created and predefined in knowledge base module. The
Descriptionary uses XML based word frames to
parameterize a few “visual or describable words” such as
nouns, adjectives, adverbs and prepositions into low-level
data to communicate with the graphic engine. For instance,
the semantic definition for adjectives red, shiny are shown
below:

Descriptionary

<Adjective_Frame>
…
<Material>
…
<Type Name=red Class=colour>
<diffuseColor>
<R> 0.5 </R>
<G> 0 </G>
<B> 0 </B>
</diffuseColor>
</Type>
…
<Type Name=shiny Class=shininess>
<shininess> 0.5 </shininess>
</Type>
…
</Material>
…
</Adjective_Frame>

Figure 2. Construction of language engine
nouns, verbs to denote actions, and adverbs to modify
verbs, prepositions to portray the special relationships of
the objects, and articles or digits to show the number of the
objects, etc. The second component is the knowledge base,
which uses grammatical rules to map tagged output into
semantic representations and finally parameterize
semantically elements into data definitions. The main
advantage of using the knowledge base is that it requires
less commitment (and thus less work) and it helps to
decide what objects and relations are worth representing,
and which relations hold among which objects [17]. Some
basic knowledge of grammar and understanding of the
structure of sentences can be added for semantic
representation and can be translated into the following
elements, Time and Atmosphere, Location, Object to
correspond nouns; Number to article or digit; Attribute to
adjectives; Action to Verbs and Position to prepositions.
These represent the various meanings and relations
inherent from the input sentence. For example, There is a
blue table in the middle of the room, first the sentence is
parsed and then match the predefined grammars. The
grammar for the sentence is: Verb (be) Determiner (one)
What (blue table) Prep (in (middle)) What (room). The
output of the semantic representation is as follows:
1.
2.

3.
4.

Location: room
Object2: table
a. Number: 1
b. Attribute:
Colour: blue
Action: is
Position: (in (middle)) (Location)

The next step involves converting the various semantic
elements into a parameterized data definition, and defining
inferences of the visual features through object matter and
context from the input text. The construction of the 3D
scene depends not only on a large number of 3D objects in
the database to correspond to the noun depiction but also

This semantic definition of the word contains two
parts. The word red is classified into material area and
belongs to the colour class with a symbolic feature such as
Type. The RGB value is then defined to correspond the
VRML diffuseColor field. VRML uses RBG colours
mixed together to define the virtual colours. The values
between 0.0 and 1.0 mean a colour should be turned on
partially, e.g. the value of dark-blue is 0 0 0.1. All these
data than can be outputted to a graphic engine for
manipulating the atmosphere or objects in the VE.

3. 2. Graphic Engine
When representing a story-based VE, we should follow
the same order of the stories three main aspects:
Environment, Character and Event. Environment presents
the information of where and when the story happens and
it is the setting and stage of the story. This is crucial for
our approach, because it determines what kind of
information from the story could be presented. VRML is
used as our primary object format and rendering engine
since it is a scene graph, which is constructed by a group
of organized nodes in a hierarchical data structure to

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

describe 3D objects and virtual worlds. Its text based file
format and the construction show that it could be the
perfect platform to help us address the visual features of
the VE by NL input. Figure 3 presents the VRML based
scene graph structure and the four types of VRML nodes
are classified by their function. These nodes correspond to
relative describable nouns, adjectives, preposition, etc - the
visual features of the VE.
VRML Scene
Group

based nodes allow the user complete control of the view
position and orientation of the VE, such as the bird’s eyes
view and persona view. They also provide dramatic
camera effects as in movies as the camera can skip to other
episodes or persons. This is extremely useful when dealing
with a conversational episode. Another important nodes
are DEF node and Group node. DEF node lets the user
define a name for any object and node to help to identify
them. This is very important in our system. Group node
provides a basic node grouping feature to specify a list of
the children.

Viewpoint
Object
Database

Atmosphere

VRML
World

Transform
Translation, Rotation, Scale
Java Applet

Object

EAI

Shape
Appearance

Figure 4. Components of graphic engine

Material
Texture
Geometry

Figure 3. VRML scene graph structure

Atmosphere Based nodes can be engaged to portray
atmosphere of VE, such as Background node (the node to
control the sky, ground colours and set up a panorama of
background images to create distant mountain ranges,
etc.); Lights and Fog nodes give some spectacular effects.
Object based nodes depict objects just as in the real world
and objects are built by separate parts with their own
attributes. Shape node is essential to construct an object.
Basically, Shape node includes Appearance node and
Geometry node. Material and Texture nodes are each used
as values in field of Appearance node. Material node can
specify the colour (i.e. RGB), shininess, transparency, etc
attributes of the object. Texture node can wrap 2D images
over and around an arbitrary object and save much time
for creating shapes. MovieTexture allows use of movie
files as the image source. Geometry nodes are the most
basic VRML objects and are staple graphics primitives,
which provide basic building blocks due to their ease to
creation. Transform based nodes comprise Translation,
Rotation and Scale fields. They can position objects
anywhere and objects can be depicted by the their spatial
relations to construct a complex VE. For example, the
Scale field is used to correspond the adjectives such as
small, big, etc. to the present size of the objects. Object
spatial relationships are fundamental to building a VE
world. In NL, spatial relations are often expressed by
prepositions, such as on, in, behind, beside, near, etc. In
this system, the area of objects in the database are
specified and marked with tags [8], for example, “#Top,
#Front, etc” in the VRML object database. Viewpoint

While VRML is not a general-purpose programming
language and Java is not a 3D presentation language,
integrating the two languages gives interactive 3D
graphics, complete programming capabilities and
extensive support for building large-scale virtual
environments. The External Authoring Interface (EAI)
[10] defines a Java and Javascript interface for direct
communication with the VRML world. This interface
contains functions such as searching existing tagged and
named nodes, sending an event into a node of VRML, etc.
The graphic engine consists of two components: a Java
applet that forms the input interface and the VRML world
are linked through the EAI (see Figure 4). The Java applet
provides interaction with the language engine. We use
Cortona from ParallelGraphics, Inc as our VRML browser.
A number of VRML format objects have been classified
and contained in the object database that are all built to
their default size and in neutral states. To design and
validate the 3D objects library, we divide the 3D objects
into two types: static objects and dynamic objects. All
objects are relatives and can be transformed from one to
another [9]. In general, static objects are more stable in
their locations, e.g. grounds, mountains, buildings, plants,
etc. Dynamic objects have a relatively unstable location or
can move, e.g. human characters, cars. In order to make
the system more efficient, the object database uses
common sense relations such as: A room has a door, a
window, four walls and ceiling and ground; A bedroom
has a bed, wardrobe, … A study room has a desk and
chair, etc. For our graphic engine, the system has to
specify and translate several parameters of elements that
are filled in by default value in the language engine (i.e.
atmosphere of the environment, the coordinates of objects,
etc.) as a Java string to call corresponding elements or

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

manipulate them by sending events into an existing VRML
scene to construct a VE accordingly.

4. The Example
A simple example that demonstrates how to construct
3DVE using the method described above is shown in
Figure 5.
It is a sunny summer mid day. In a green field, there is
a blue table on the ground. There is ball on the table.
The ball is made of glass.

1. Get a reference to the VRML browser
Browser browser = Browser.getBrowser(this)
2. Find the node named “BALL”
Node ball = browser.getNode("BALL");
3. Get the “set_transparency” EventIn field
EventInValue_changed transparency =
(EventInValue_changed)
ball.getEventIn("set_transparency");
4. Add the value to the transparency field
transparency.setValue(0.3);
5. Refresh Browser.
If the entered sentences do not contain detailed
information, such as viewpoint, actual table and ball size,
colour of the ball etc the system uses the default size as a
solution.

5. Conclusion and Future Work

Figure 5. The 3DVE generated
from the story input

These sentences are interpreted and presented by
following semantic presentations:
A. Time of when the story happens, It is a sunny
summer mid day.
Time: summer (mid day)
Atmosphere: sunny
This determines the atmosphere of the environment,
such as light condition, sky of the color, etc.
B. Place where the story happens, In a green field.
Location: field
Attribute:
Colour: green
C. Objects on the field:
Object1: table
Attribute:
Colour: blue
…
Consider the last sentence to understand how the Java
applet interacts with the VRML world through the
following steps:

An approach that allows non-professionals to generate
a 3DVE by using story visualization technology has been
introduced. Story is the primary potential source for
applying visualization techniques and the scene graph
based VRML provides the opportunity to manipulate and
modify properties of the environment in real time. We
believe that integrating NL and 3D graphic techniques
provides a flexible and easy way for non-professionals to
generate and interact with the VE in real time as compared
to traditional 3D packages. This project is still under
development. We find there are still various aspects of the
system that need to be improved. Future work includes:
the option of voice input could be more useful instead of
text typing; to evaluate and compare VRML with Java3D
to find which is more suitable for creating realistic and
interactive virtual environments; research will also take
place to develop a event engine and how to embed a BBAI
(behavioural-based AI) character to carry out the
describable tasks in the domains of time, space and motion
in VE.

6. References
[1] 1st NLP and XML Workshop, Tokyo, 2001; 2nd NLP and
XML Workshop, Taipei, 2002.
[2] N, Badler., W, Becket., ,B, Eugenio., C, Geib., L, Levison.,
M, Moore., B, Webber., M, White., X, Zhao. Intentions and
Expectations in Animating Instructions: the AnimNL
Project. Technique Report In Intentions in Animation and
Action. University of Pennsylvania, 1993.
[3] O, Bersot., et al
A Conversational Agent to Help
Navigation and Collaboration in Virtual Worlds, Virtual
Reality, 1998.
[4] R, Bindiganavale., et al. Dynamically altering agent
behaviours using natural language instructions. In
Autonomous Agents, pp. 293–300, 2000.

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

[5] J, Cassell., H, Vilhjálmsson., T, Bickmore. BEAT: the
behaviour expression animation toolkit. Proc. of the 28th
SIGGRAPH Annual Conference on Computer Graphics and
Interactive Techniques, 2001.
[6] M, Cavazza., S, Bandi and I, Palmer. “Situated AI” in Video
Games: Integrating NLP, Path Planning and 3D Animation.
AAAI Symposium on Computer Games and AI, 1999.
[7] R, Clay and J, Wilhelms. Put: language-based interactive
manipulation of objects. IEEE Computer Graphics and
Applications, pp. 31–39, March 1996.
[8] B, Coyne and R, Sproat. WordsEye: An automatic text-toscene conversion system. Proc. of the 28th SIGGRAPH
Annual Conference on Computer Graphics and Interactive
Techniques, 2001.
[9] A, Egges., A, Nijholt., P, Nugues. Generating a 3D
Simulation of a Car Accident from a Formal Description:
the CarSim System. Proc. of The Workshop, on Temporal
and Spatial Information Processing. ACL 2001 Conference,
Toulouse, 2001.
[10] C, Marrin. External Authoring Interface (EAI) Proposal.
Silicon Graphics Inc., Mountain View California, 1997.
[11] E, Mueller. Prospects For In-depth Story Understanding by
Computer. MIT, 1999.
http://www.media.mit.edu/~mueller/papers/storyund.html
[12] Q, Mehdi., X, Zeng and N.E, Gough. Story Visualization for
Interactive Virtual Environment. ISCA 12th International
Conference on Intelligent and Adaptive Systems and
Software Engineering, 2003.

[13] A, Mukerjee., K, Gupta., S, Nautiyal., M, Singh., N,
Mishra. Conceptual Description of Visual Scenes
from Linguistic Models. Journal of Image and Vision

Computing,
Special
Issue
on
Conceptual
Descriptions, 2000.
[14] J, Murray. Hamlet on the Holodeck, Cambridge, MA:
MIT Press, 1998.
[15] J, Piesk and G, Trogemann. Animated Interactive
Fiction: Storytelling by a Conversational Virtual
Actor. Proc. of VSMM'97, IEEE Computer Society
Press, September 1997.
[16] K, Perlin and A, Goldberg. Improv: A system for
scripting interactive actors in Virtual Worlds. Proc. of
SIGGRAPH 96, 1996.
[17] S, J, Russell and P. Norvig. Artificial Intelligence, A
Modern Approach. Prentice-Hall International, Inc.
London, 1995.
[18] T, Samad. A Natural Language Interface for
Computer-Aided Design, Kluwer Academic Pulishers,
Boston, Dordrecht, Lancaster, 1986.
[19] Y, Shinyama., T, Tokunag., H, Tanaka. KairaiSoftware Understanding Natural Language. Third
International Workshop on Human-Computer
Conversation, 2000.
[20] T, Winograd. Understanding Natural Language.
Academic Press, 1972.
[21] A, Yamada., T, Yamamoto and H, Ikeda.
Reconstructing Spatial Image from Natural Language
Texts. COLING 92, Nantes, 1992.
[22] X, Zeng., Q, Mehdi and N.E, Gough. Generation of A
3D Virtual Story Environment Based on Story
Description. Proc. of 3rd SCS Int. Conf. GAME-ON
2002, London, 2002.

Proceedings of the Seventh International Conference on Information Visualization (IV’03)
1093-9547/03 $17.00 © 2003 IEEE

