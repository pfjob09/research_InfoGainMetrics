Interacting with sonification systems: closing the loop
Andy Hunt, Thomas Hermann, Sandra Pauletto
University of York, UK, Bielefeld University, Germany, University of York, UK
{adh@ohm.york.ac.uk, thermann@techfak.uni-bielefeld.de, sp148@ohm.york.ac.uk }
Abstract
This paper stresses the importance of the human user being
tightly embedded within an interactive control loop for
exploring data sets using sound. We consider the quality of
interaction, and how this can be improved in computer systems
by learning from real-world acoustic interactions. We describe
how different sonification methods can utilise the human
feedback loop to enhance the perception and analysis of the
data under investigation. Some considerations are given
regarding systems and applications.

1. Introduction
This paper discusses the way that humans interact with
sound in everyday life. It focuses on how we gain
feedback from a combination of senses, helping us to
obtain our sense of reality, and thus to understand the
world better. We consider the nature of ‘Control
Intimacy’; the quality of interaction that we take for
granted in manipulating everyday objects, but which is
so often lacking in limited-interaction, visual-only
interfaces.
Musical instruments are examined as a particular type of
human-device interface (tried and tested over long
periods of time) which allow such control intimacy to
develop to high levels. We then consider the special case
of computer interaction where the human is allowed to
form intimate control loops with the system, using sound
(and other senses) as feedback.
We outline what can be learnt about integrating
interaction into exploratory data analysis techniques. We
then consider some challenging application areas which
can be tackled in a fresh way specifically using
continuous interaction with sonic feedback.
2. Control Loops in Human Interaction
As human beings, from the moment we are born we
begin to interact with the world. In fact a baby’s first
action in the world is to cry – to make a sound. As we
grow we learn first how to control our bodies, and then
how to interact with objects around us. The way that the
world works – its physical laws, the constants and the
variables – becomes coded into our developing brain.
We learn to take for granted that dropped objects fall to
the ground, and that when we reach for an object we feel
it and see it and hear it as we touch it. Watch a young
child playing with a pile of bricks and you will notice
how she develops her movements by interacting with

objects and obtaining from them instant and continuous
feedback of their position, speed and texture.
Such control loops of human action and continuous
feedback from the world become embedded deep within
our mind-body system.
Therefore it is hardly surprising that, later in life, we
become rapidly frustrated with computer systems that
engage with us in a very different and more limited
manner [1]. Here, too often, the interaction is dictated by
the computer. A prompt is given, or a list of options
presented as icons or a menu. We have to choose from
the selection offered by the computer at every stage of
the process, and thus the interaction becomes a series of
stilted prompt-choice cycles; a far cry from the way that
we have learnt to interact with the everyday world. It is
as if we have designed our computer systems to always
remain outside our control loop. We seem to expect them
always to be under ‘third-party’ control; things to which
we give instructions. The result of this is that we rarely
gain the same intimacy of control with a computer as we
do with objects in everyday life. A common observation
is that much of our time working with computers is spent
in navigating the interface, rather than completing the
task.
It matters too whether or not you are part of the control
loop. Many passengers become travel sick whereas this
condition rarely affects drivers.
When you are
controlling an object you know what to expect, as – by
definition – you are initiating the reactions and can thus
prepare your mental apparatus for the result. Maybe you
have had the experience of being in a room where
someone else is in charge of the TV remote control. You
cannot believe how much they are ‘playing around with
it’, driving to distraction everyone else in the room.
However when you have it, everything is different, and
you are ‘simply seeing what’s on the next channel’. It
matters greatly whether you are in the control loop or
not.
This paper considers how we can bring more real-world
interaction into our computing interfaces, by placing the
human operator firmly in charge of a continuous control
loop wherever possible.
2.1 Control intimacy
A child playing with wooden blocks and a person
operating a typical computer interface are both
interacting with external objects. It is just that the
quality of the interaction is different. The extent to
which the interaction directly affects the object is one

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

aspect of the control intimacy being exhibited; the other
aspect being how well the human manages this control.
Real-world objects seem to exhort us to spend time with
them, and as we do, we subconsciously learn more about
them, and master the skills of manipulating them until
the control becomes almost automatic.
We are all aware of situations where we are controlling
an object and almost forget that we are doing it. Car
drivers often report that they are shocked to find
themselves at their destination, without knowing how
they got there; even though the act of driving is an
extremely complex interactive process. Many good
performing musicians feel that their fingers are somehow
playing the music by themselves.
In musical
performances, their minds appear to be concentrating on
higher-level modes of expression, whilst their bodies are
managing the physical act of manipulating the
instrument. In fact, most musicians will recount the
terrifying feeling of suddenly becoming aware of what
their fingers are doing, and as a result the performance
grinds to a halt!
Csikszentmihalyi [2] called this type of disembodied
interaction flow. He explains how it is found freely in
children as they play, and less so in adult life. Certainly
in most computer interfaces the flow is never allowed to
happen, due to the constant choices, and the stop-start
style of the interaction caused by the emphasis on
reading words, processing and selecting options. To
shed some light on how to improve this state of affairs,
let us consider the special case of interaction where the
goal is to generate sound.
2.2 The special case of interacting with sound
Engineers use sound to deduce the internal state of
engines and complex machinery such as washing
machines. Sound warns us of dangers outside our
relatively narrow field of view. It is also the medium by
which much human communication takes place via
speech and singing.
Whenever we interact with a physical object, sound is
made. It confirms our initial contact with the object, but
also tells us about its properties; whether it is solid or
hollow, what material it is made of etc. The sound
synchronises with both our visual and tactile ‘views’ of
the object. As we move the object, the sounds it makes
give us continuous feedback about its state. Sound is a
temporal indicator of the physical state of the world
around us.
The act of making sound may be satisfying to human
beings precisely because they are in a very tightlyresponsive control loop. This does not by definition
mean that other people find the sound satisfying. Think
of times when a person mindlessly ‘drums’ his fingers on
the table to help him think. He is part of the control
loop, and so is expecting the moment-by-moment sonic
response. The whole process often remains at the
subconscious level, and he is unaware he is doing it.
However, to other people in the vicinity (not in the loop)
the sound can be intensely annoying. Therefore, we see
that there is something special about being the one to

initiate actions, and receive constant and immediate
sonic results.
2.3 Tuning parameters for individuals
An observation about the individuality of interacting
with sound became clear to the first author during his
experiences as an amateur radio operator. It is wellknown to ‘Radio Hams’ that there is quite an art to
‘tuning in’ the radio to pick out a particularly weak
signal. Somehow you need to be able to pick out the
signal you are trying to listen to, in spite of the fact that
there are much louder interfering signals nearby in the
frequency spectrum, and background noise, and all
manner of fluctuating signal levels and characteristics
due to propagation conditions. To do this requires a fine
balance with the tuning control, and the signal
modulation controls, and sometimes even movement of
the antenna. When two people are listening to the same
radio signal, but only one is at the controls, it is quite
common for the signal to be audible only to the person at
the controls.
What can we learn from such an observation? Perhaps
when a sound is made by a system, we ought to consider
who the sound is intended for. Is it just for the person ‘in
the loop’, since s/he is the one controlling the system
parameters? Or, is the sound intended for everyone?
Where data is being portrayed as sound, for example in a
hospital environment, it is important that everyone
recognises the sound. However, where the sound is
being controlled interactively by a person, we might need
to be aware that the operator could be inadvertently
tuning the system for themselves. More complex sounds
(which could appear as annoying or unpleasant) can be
quite acceptable to people who are in the control loop.
The more general point to be inferred from the above
example is that humans can use physical interaction to
control the generation and modulation of sound in order
to extract data from a noisy signal. Section 3 studies this
area in more detail.
Musical instruments are a special case of sound
generating device where the main intention is that other
people do indeed listen to the sound. Having said that, if
you are sharing a house with someone practicing an
instrument (particularly if the player is a beginner), the
observation that ‘it-matters-whether-you-are-in-controlloop’ becomes obvious.
In the next section we look at human interaction with
instruments in more detail.
2.4 Musical instruments as exemplars
The sonic response of physical objects is so deeply
ingrained in the human psyche that sound and music has
been a fundamental part of every known human society.
In this section, we take a closer look at human interaction
with musical instruments; since much can be learned
from this about what makes good quality real-time
interaction.
In a previous paper [3] we described the attributes of
most acoustic musical instruments as follows:
• there is interaction with a physical object.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

•

co-ordinated hand and finger motions are
crucial to the acoustic output.
the acoustic reaction is instantaneous.
the sound depends in complex ways on the
detailed kinds of interaction (e.g. on
simultaneous
positions,
velocities,
accelerations, and pressures).

the purposes of facilitating communication or
interpretation” [6]
Humans must be equipped with several senses for a good
reason: that they are complementary, and are needed in
collaboration to gain a full sense of the world around us.
There are several areas where sound offers
improvements over visual feedback.

The physical interaction with the instrument causes an
instantaneous acoustic reaction. This allows the player
to utilise the everyday object manipulation skills he has
developed all his life. The player’s energy is directly
responsible for activating the sonic response of the
system; when the player stops, the sound dies away. The
mapping of system input to sonic output [4] is complex;
many input parameters are cross-coupled, and connected
in a non-linear manner to the sonic parameters. This can
make an instrument difficult to play at first, but offers
much scope for increased subtlety of control over time.
As the player practices, he becomes better and better.
This allows the control intimacy to increase to a level
where the physical operation of the instrument becomes
automatic. At this point the player often experiences the
‘flow’ of thinking at levels much higher than complex
physical interface manipulations.
We should also not underestimate the importance of
tactile feedback. A good performer will rarely look at
her instrument, but will instead rely on the years of
training, and the continuous feel of the instrument which
is tightly coupled to the sound being produced. The
human operator learns to wrap his mind-body system
around the instrument to form a human-machine entity.
So, it seems from considering how people interact with
musical instruments, that devices intended for
exploration need to have certain characteristics. These
include a real-time sonic response, a complex control
mapping which permits learning, and tactile feedback
tightly coupled to the sonic response.

“The main differences of sound displays over visual
displays are that sound can:
•
represent frequency responses in an instant (as
timbral characteristics)
•
represent changes over time, naturally
•
allow microstructure to be perceived
•
rapidly portray large amounts of data
•
alert the listener to events outside the current
visual focus
•
holistically bring together many channels of
information” [7]
So, the use of sound allows us to gain alternative insights
into the data under examination. Until recently the sheer
computing power required to generate the sound output
has meant that, by necessity, the act of sonification was a
non-interactive process. Data was loaded, parameters
were selected, the algorithm set going, and some time
later the sound emerged. Too often in computing
technology, when this time-lag is eliminated by
improvements in processor speed, the style of interaction
remains; and interaction is limited to setting parameters,
then listening to a completed sound. As stated in section
2, this stilted interaction prevents any form of control
intimacy from developing. In the following section we
examine how to re-introduce interaction into the art of
making sound.

•
•

3. The use of sound in exploratory data analysis
In this section we consider the use of sound in computers
as a way of understanding data taken from the world
around us. We describe how sound can be used to
portray data, and explain the importance of continuous
human interaction with the sound generating process.
3.1 Sonification
The general term Auditory Displays is employed to
describe the use of sound in computers to portray
information. It covers a wide range of topics including
alarm signals, earcons and sonification techniques, most
of which are discussed by the International Community
for Auditory Display (ICAD) [5]. Sonification is the
more specific term used to describe the rendering of data
sets as sound, or:
“. . the transformation of data relations into
perceived relations in an acoustic signal for

3.2 Interacting with Sonification
Now that computers can run fast enough to generate
sound in real-time, we should re-design our data-tosound algorithms to take advantage of the rich
possibilities of continuous human interaction. How are
we to allow a ‘flow’ experience of data sonification to
take place?
This question was examined by the 2004 Interactive
Sonification workshop, organised by the first two authors
[7] and is summarised in [8]. At this gathering
researchers from diverse disciplines described the magic
that occurs when sound is generated in real-time under
human control. Although musical instruments also
generate sound in real-time under human control, their
primary is artistic expression. In contrast, the goal of an
interactive sonification system is to allow humans to
explore and understand the intrinsic properties of a
particular data set. In other words, it is an analysis tool.
In sections 4 and 5, we describe toolkits that we have
developed which enable such interaction to be explored,
and some interactive sonification applications which are
in progress at the time of writing. There are two basic
approaches to the incorporation of interaction into a
sonification algorithm. The first involves taking data
attributes and converting them into sound (so called

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

parameter mapping), whilst allowing the user to interact
with this process. The second involves designing a
sonification model which is inherently interactive.
Where data is time-ordered (for example where it has
been gathered from a time-evolving source) it is sensible
to retain this time order by mapping the data onto sound
variables. Traditionally, the entire data set is converted
into a sound file, which is then listened to noninteractively, rather like a CD. However, interaction can
be built into the process to allow a human being to
explore the data much more freely. For example the
position in the data can be moved continuously,
‘scrubbing’ through the data and instantly hearing the
sonic result. Alternatively the data could play back
continuously in a loop while the sonification algorithm is
tuned by the user, rather analogous to the Radio Ham
example given in section 2.3.
The next section describes how sonification models can
be designed and used for exploring non-time-based data
sets.
3.3 Using model-based sonification to enhance user
interaction
Traditional sonification schemes are based on clearly
separated computation and playback phases, as pointed
out above. In contrast, the framework of Model-based
sonification (MBS) involves “interacting with datadriven virtual acoustic objects” - which is by design
inherently interactive.
This approach is almost
orthogonal to previous techniques: whereas in parameter
mapping sonification the data is used to provide controls
(e.g. playing instructions) for a given instrument (sound
synthesis algorithm), in Model-based sonification the
data is used to establish the instrument or algorithm
itself. This means that with an MBS system the user is
given the responsibility of interacting with the
sonification model, and (only) by this means causes the
sonification to generate sound. MBS is thus different
from parameter mapping sonification, in that there is no
mapping from data to model-parameters - instead the
data become (in most models) part of the model
configuration and thus do not explicitly but implicitly
determine the sonification.
Model-based sonification is a concept in which a virtual
acoustic object is established, dependent on the data
under analysis. It thus provides a method of mediating
between abstract data spaces and the infinite space of
possible instruments. Concrete models usually specify
the laws of dynamics that govern the temporal evolution
of the dynamical elements constituting the 'virtual
instrument'. Typically sonification models are set up
first to be in a state of equilibrium so that they do not
produce any sound without being excited into a nonequilibrium state. Most MBS models are dissipative
(because, for example, sound radiation represents energy
loss) which causes the sound to vanish after interaction
ceases.
Model design is a very creative process. Some example
models have been described in previous work,
[9][10][11], but the best suited models for specific

analysis tasks are still to be invented. Model design
entails wide possibilities for bringing task-oriented needs
into the concrete realisation of a model. Complex sound
responses can occur, but humans respond well to this
type of reaction. The MBS concept and its benefits are
discussed in detail in [9].
In this paper we focus on the aspects of engagement and
flow, which have been shown to play an important role in
the use of interactive auditory systems such as musical
instruments.
The following three aspects of acoustic real-world
interactions cause human users to increase their
engagement with the system:
(i) sound complexity,
(ii) low-latency correspondence to human controls,
(iii) attention.
Concerning (i), the complexity of sounds from realworld acoustic systems is much higher than that of most
sounds used in computer systems. This is because realworld systems typically possess complex dynamic
behaviour involving nonlinearities as well as stochastic
components, whereas synthesised sounds are often
generated by rather ‘sterile’ algorithms such as FMsynthesis. Our auditory system is so well tuned to, and
experienced with, analysing real-world sounds that it
appreciates complexity, often interpreting this as
‘beauty’ of sound. In contrast, even complex stochastic
time series generated in computer contexts (e.g. from
chaotic systems) fail to please or convince the listener.
For instance plucking a guitar string will never lead to
the exactly same sound, whereas sonification systems
typically reproduce sound accurate to the single bit.
Model-based sonification provides exactly this ‘mind of
its own’ to a data-driven dynamical acoustic system.
Since high-quality interactions (those that go far beyond
a simple triggering) are unique excitation patterns, the
resulting sound will also be a unique reaction to this
unrepeatable stimulus. Sound complexity is not granted
automatically by the use of an MBS approach. Instead
we need to learn from real-world acoustics, which
provides inspiration on suitable ways to create complex
sonic dynamics, resulting in sounds where users can rely
on their highly developed listening skills.
Concerning (ii), low-latency is an important factor in
creating engagement and for facilitating the user’s
transition from conscious mode to flow mode. Lowlatency sound generation is useful for guiding
exploratory activities since the immediate response
allows the user to directly refine his control activities. It
is also important for increasing the synchronisation of
other modalities occurring in the interaction, such as
tactile and visual feedback. For example the user hears
the resulting sound at the same time as they experience
the tactile feedback from the control device.
Concerning (iii), attention; users often focus their
attention in order to enhance perception. Think for
instance of a photograph you are looking at, wondering
why you took a picture of a boring landscape. Later you
remember that you were taking a photo of a bird. It is
almost invisible on the picture, but your attention

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

‘magnified’ at the time. Attention is the magnifying lens
through which users experience and explore the world!
Attention is often directed towards correlations between
the user’s activities and a system’s response to it. Even
faint correlations can then receive significant
magnification by attention, but only for the user in the
control loop. Attention is tightly coupled to points (i)
and (ii). Complexity of sound grants the availability of
many possible sources of correlations between sound and
the system feedback. Low-latency is an important factor
for ensuring that these correlations are easily detectable
in the interaction. Attention is thus related to a user’s
engagement with the system, since the occurrence of
structure on multiple complexity levels keeps alive the
user’s interest in practising and improving in the
interaction.
Model-based sonification helps to implement these
aspects automatically since it incorporates an interaction
style which is more like real-world acoustic interactions.
However, the designer has the freedom to refine the
sonification so that the aspects mentioned above come
better into play. For instance:
- by allowing non-linear couplings of the dynamical
elements, so that the sonification model exhibits a
rich acoustic behaviour.
This may be
computationally costly but the evolution of
computation power makes it merely a matter of
time.
- by enhancing the modes of interaction. For instance
a sonification model triggered by a computer mouse
is ‘poorer’ than one in which users bring in the
multi-dimensional controls of a whole articulated
hand, which in turn is poorer than interactions with
tangible interfaces that take the user even closer to
real-world acoustic interaction.
- by designing sonification models so that subtle
changes of the excitation pattern (e.g. of position or
velocity) are directly related to subtle changes of the
sound. As an example think of a sonification model
with which the user can interact by clicking on a
graphical representation of the data points. One
possible paradigm of model excitation would be to
give the entire excitation to the nearest data node. In
this case any click within the vicinity of the node
will cause the same sonification. If, however, the
excitation energy is distributed between the nearest
neighbours according to their distance, then subtle
changes in the activation position will result in
subtle changes in the sound.
In this way, we hope that the above aspects prove helpful
in the design process of engaging exploratory
sonification models.
4. Software for interactive sonic data analysis
Sonification systems which allow us to link data sets to
their acoustic representation face several requirements in
terms of interfaces, structure, and performance. Here we
briefly introduce interactive sonification toolkits (ISTs).
We then step back and regard the general requirements
for software being used in sonification systems.

At the University of York we have been developing an
Interactive Sonification Toolkit [12] which allows rapid
prototyping of the transition from data to sound, coupled
with real-time user interaction. It is constructed in PD
[13] so that the end product is also cross-platform and
open-source. Pd allows real-time sound synthesis,
creation of graphical user interfaces, refinement of the
‘program’ during runtime, easy interfacing with many
sorts of sensors/controllers, e.g. via MIDI or OSC, and
platform independency.
This is similar in concept to an interactive sonification
platform produced in Bielefeld, based on a graphical
simulation system Neo/NST [14], which is particularly
strong in data computation, data mining, data
visualisation, and rapid prototyping. However, it is
weaker in real-time sound synthesis and limited to the
Linux platform. All sonification models mentioned
above have been implemented using Neo/NST, using
Neo displays for graphical data representation and
interaction.
Interactive sonification systems have to consist of several
components, which (a) need tight interaction, (b) are
computationally expensive, or (c) demand special
platforms. These are expanded below.
a) All sonification systems involve data-related
computations. Interaction with the display (such as
selection) requires intermediate representations to be
recomputed. This demands a tight connection between
the controls and the data computation engine. Neo/NST
here provides a good platform, and related alternatives
for powerful data processing are MatLab or Octave.
b) Specifically for sonification models, CPU power is
never enough. It is useful to distribute specialised
rendering routines onto an extra machine. Where
appropriate, sonification models can be divided into a
high-level part (where low-level synthesis instructions
are computed) and a low-level sound engine (where the
sound signal is actually generated). For the first part,
simulation systems such as Neo/NST or languages like
Smalltalk are appropriate. For sound computation, there
are several candidates, e.g. Csound, PD, or Supercollider.
c) The third aspect concerns controls for sonification
models. Many suitable interfaces, such as an audiohaptic ball interface [15] or computer-vision-based
gestural interfaces are works-in-progress and demand
their own machine. Other interfaces (e.g. certain
joysticks, Phantom device, etc.) are only supported on
special platforms/OS and do not allow a tight direct
combination.
In summary, it seems that any isolated platform is so far
inappropriate for solving the whole range of problems
encountered in interactive multimodal displays. Many
different aspects need to come together in order to enable
a quick and effortless design of systems. Heterogeneous
solutions not only offer the chance to distribute the
computation better over several machines, but also to use
optimised components according to the respective needs.
An intelligent architecture for such complex systems is
currently under development at Bielefeld University and
will be presented elsewhere.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

5. Applications
In this section we briefly outline projects that are in the
early stages of development.
5.1 Analysis of time-stamped data
At present the PD-based toolkit described above is being
modified to suit several different projects. Two of these
are funded by EPSRC, allowing advanced data mining of
helicopter flight data and physiotherapy muscle data
respectively. The data produced by a helicopter test
flight cannot be adequately shown on a computer screen
at a reasonable resolution whilst giving an overall
picture; so we are using sonification to allow engineers
to navigate the entire data set in a matter of seconds.
Physiotherapists wish to know more about the qualitative
aspects of the signals produced by the movement of
muscles, and sound has allowed new insights above and
beyond the traditional visual plots [12].
5.2 Landmine detection
The number of anti-personnel landmines buried around
the world is estimated to be between 50 and 70 million.
Their impact on third world countries is devastating in
terms of local economies and their impact on the local
population.
Humanitarian landmine detection and
clearance is currently a slow process, because of the high
false alarm rate associated with current detector
technology
Sponsored by the UK Department for International
Development, ERA Technology have developed a
prototype hand-held detector which uses both groundpenetrating radar (GPR) and metal detection (MD) to
significantly reduce the effect of false alarms and also
detect minimum metal (plastic) landmines.
The key factors in the design of the new detector are
affordability and ease of use. ERA have developed an
audio interface which uses frequencies in the 100Hz to
3kHz range to give continuous feedback to the user of
the GPR detector. The depth of the target is given by the
frequency of the output signal, and the size of the target
is given by the amplitude of the signal.
“A key feature of the design is a special
(patented) man-machine acoustic interface. This
approach utilises the inherent capabilities of
humans to ‘process’ information and keeps the
‘man-in-the-loop’.” [16]
The metal detector also produces its own audio tone, and
together the operator has sonic feedback of the objects on
and below the surface.
The University of York, Department of Electronics, are
planning to work with ERA to investigate ways of
combining the separate signals from the two sensors, and
to optimise the presentation of the audio information for
users from many cultural backgrounds.
6. Conclusions
In this paper we have stressed the importance, for
complex data analysis, of the human user being in a
tightly-coupled intimate control loop.
We have
illustrated this with examples from everyday interaction,

particularly with sound. We have also summarised the
work done in building interaction into sonification
techniques, and the inherently interactive method of
model-based sonification.
In conclusion, the research community needs to be
acutely aware of the quality of interaction that is
provided in human interfaces, in order to maximise the
capabilities of the human mind-body system.
7. Acknowledgements
Thanks to the Sensors Design Consultancy team of ERA
Technology for the input and collaboration regarding
their existing work on audio interfaces for landmine
detection. Thanks to EPSRC for funding the work
described in sections 4 and 5, under the project
‘Improved data mining through an interactive sonic
approach’.
8. References
[1]

A. Hunt, Radical User Interfaces for Real-time Musical
Control, Ph.D. thesis, University of York, 2000,

[2]

M. Csikszentmihalyi, Beyond Boredom and Anxiety:
Experiencing Flow in Work and Play, reprint, Jossey
Bass Wiley, 2000.
Hermann, T. & Hunt, A.D, The discipline of Interactive
Sonification, Proc. 1st Int. Workshop on Interactive
Sonification, Bielefeld, Germany, January 2004.
http://www.interactive-sonification.org
A. D. Hunt, M. Paradis, and M. Wanderley, “The
importance of parameter mapping in electronic
instrument design,” Journal of New Music Research, vol.
32, no. 4, pp. 429–440, December 2003, special issue on
New Musical Performance and Interaction.
International Community for Auditory Display
http://www.icad.org/
Gregory Kramer et al., Sonification Report, National
Science Foundation,1997
http://www.icad.org/websiteV2.0/References/nsf.html
http://www.interactive-sonification.org
Andy Hunt and Thomas Hermann, “The importance of
interaction in sonification,” in Proceedings of the Int.
Conf. on Auditory Display. ICAD, 2004, submitted.
Thomas Hermann, PhD, Sonification for Exploratory
Data Analysis, Bielefeld University, 2002
Hermann, T., Meinicke, P., & Ritter, H. Principal Curve
Sonification, Proc. Int. Conf. on Auditory Display p81—
86, 2000
Hermann, T.& Ritter, H. Crystallization Sonification of
High-dimensional Datasets, Proc. Int. Conf. on Auditory
Display p76--81, 2002
Pauletto, S & Hunt, A., “An interactive sonification
toolkit” in Proceedings of the Int. Conf. on Auditory
Display. ICAD, 2004.
www.pure-data.org
H. Ritter, The Graphical Simulation Toolkit Neo/NST
http://www.techfak.unibielefeld.de/ags/ni/projects/neo/neo_e.html
Hermann, T., Krause, j., & Ritter, H. Real-Time Control
of Sonification Models with an Audio-Haptic Interface,
Proc. Int. Conf. on Auditory Display p82—86, 2002
http://www.era.co.uk/docs/electronics/Minetect.pdf

http://www-users.york.ac.uk/˜elec18/download/adh_thesis

[3]

[4]

[5]
[6]

[7]
[8]

[9]
[10]

[11]

[12]

[13]
[14]

[15]

[16]

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

