Control of Motion in Character Animation
ZhiDong Xiao Jian J. Zhang Stephen Bell
National Centre for Computer Animation
Bournemouth Media School, Bournemouth University
United Kingdom
E-mail:{ zxiao , jzhang , sbell }@bournemouth.ac.uk
• overcome the spatial constraints of motion capture
studios;

Abstract
This paper surveys the numerical techniques developed
in computer graphics for editing and revising data driven
character animation. We evaluate systems that have been
developed for interactive, off-line and on-line animation,
using motion capture data or animator generated data,
that enable the control of complex character animation.

• reuse captured data and adapt them to a different
character, e.g., retargeting a motion from one character to another; to a different environment to compensate for geometric variations;
• combine two motion clips in such a way that the end
of one motion is seamlessly connected to the start of
the other.

Keywords—Character animation, motion blending, motion
control, motion editing, motion retargeting, motion warping,
spacetime constraints.

1 Introduction
In this paper we survey a number of techniques developed in computer graphics for controlling data driven character animation. From the early 90’s, motion capture techniques have been widely used in movies, video games and
virtual reality to synthesize convincing motion of articulated figures for 3D computer animation. By using magnetic or optical sensors it is possible to store the positions
and orientations of points located on the human body. The
motion is captured from a performer. The synthetic skeleton then plays the recorded motion of the real performer.
Although a variety of technologies have been developed
to capture motion data, recorded motion data is often not
quite “right”, e.g., the markers may not have been properly
located on the performer’s body. However, these systems
hold promise as a means of producing very convincing human figure animation.
Much of the recent research in controlling motion has
been devoted to developing various kinds of editing tools to
produce a convincing motion from a pre-recorded motion
clip: captured motion data or animator generated data. The
ability to edit original motion data is vitally important. By
using these editing tools, animators can adjust motion data
to:
• eliminate artifacts to achieve an accurate spatial and
temporal match to the computer generated environment;

This survey provides a comparative study of motion editing techniques developed so far in computer graphics, which
are grouped and organized in the rest of the paper as follows:
• Basic approaches for motion data processing are reviewed in Section 3.
• Techniques for motion blending are detailed in Section 4.
• Approaches for motion editing based on constrained
optimization are presented in Section 5.
• Techniques for retargeting motion to a new character
are presented in Section 6.
Section 7 discusses the techniques and identifies the problems that are still open and that hopefully will inspire future research in the next few years.

2

Background

In the last decade, various approaches to 3D character
animation have been used and developed in the fields of
art, entertainment, video games and virtual reality. In order to produce convincing character animation, much of
the research in motion control of the articulated figure has
been undertaken to reduce the amount of motion specification and to free the animator to focus on other qualities of animation production. An established strategy is
to build a system that contains some knowledge about human motion and the topology of articulated figures, so that

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

it can execute certain aspects of movement automatically.
This has led to the development of control schemes where
the knowledge is frequently specified in the equations of
motion, constraints and optimization expressions. Boulic
and Thalmann [2] and Girard and Maciejewski [8] used
direct and inverse kinematics (IK) combined with biomechanical knowledge to animate human walking. Faloutsos
et.al.[6], Fang and Pollard [7], Komura et al.[15] and Oshita et al.[21] presented methods that control and synthesize physics-based human motion. Bruderlin and Calvert
[3] used a simplified dynamic model and control algorithms
to generate the motion of a walking human. Hodgins et
al.[11] [12] [25] used a direct control scheme to simulate
and adapt character’s motion. Laszlo et al.[16] combined
feedback control in their method to animate human walking. Ko [13] used inverse dynamics to animate human locomotion. Van de Panne [31] and Witkin and Kass [32]
presented human figure animation techniques based on constrained optimization.
All of these techniques have made a significant contribution to the development of 3D character animation.
However, most of these approaches often suffer from lack
of interactivity, as the animator does not always have control over the generated motion. To generate, synthesize
and control realistic movement of articulated characters remains one of the most challenging tasks in computer animation.

3 Motion data processing
In computer graphics, articulated figures are usually represented as rotation hierarchies parameterized by a whole
body (root) translation, a whole body rotation, and a set of
joint angles. By using magnetic and optical sensors, motion capture devices capture and save the 3D coordinates of
the markers at discrete points in time [19]. A specific software has to be used to convert this data into joint rotations
and translations. In order to produce continuous movement based on motion data, animators have to deal with
this discrete motion data. Captured motion parameters can
be treated as discrete samples. After post-processing, the
captured motion data is described by a set of motion curves
each giving the value of one of the model’s parameters as
a function of time.

3.1 Signal processing in frequency domain
Bruderlin et al.[4] applied signal processing techniques
in the frequency domain to reuse, modify and adapt animated human motion. A multi-resolution filtering (Figure 1), multi-target motion interpolation with dynamic time
warping, waveshaping and motion displacement mapping
techniques were presented.

G0
G1

L1
L2
G:Lowpass filter
L :Bandpass filter

G2
G3

L3
L4

G4

Figure 1: Multi-resolution filtering process
where G0 represents the initial signal and
G0 = L1 + L2 + L3 + · · · + GN
To produce new motions, the signal is reconstructed differently by adjusting gains gk for each band and multiplying Lk by their current gain values:
f b−1

G0 = Gf b +

gk L k

(1)

k=0

where frequency band f b = log2 (m), m is the number of
frames of each signal.

3.2

Signal processing in time domain

Witkin and Popovic [33] presented a motion warping
technique based on signal processing in the temporal domain. They consider and modify each single motion curve
θ(t) independently. They warp the values using a transformation of the form (displacement mapping):
∀i

θi (ti ) = a(ti )θ(ti ) + b(ti )
t = g(t )

(2)
(3)

where θ (t) is a warped motion curve, i represents the ith
parameter of the system, a(t) is a scaling function used
to scale the signal, and b(t) is an offset function used to
change the center of scaling of a. The deformation from
time t to t is a constrained interpolation using Cardinal
splines. Thus, the resulting sequence satisfies the constraints of a new key-frame with respect to the pattern of
the initial motion. Moreover, by overlapping an interval at
the end of the first clip with an interval at the beginning
of the second, they concatenate two motion clips together.
The blend is a weighted sum of the two motion curve segments described as:
θblend (t) = wt (t)θ1 (t) + (1 − wt (t))θ2 (t)

(4)

where θ1 (t) and θ2 (t) are the motion curves being blended
and wt (t) is a normalized easy-in/easy-out weight function.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

3.3 Advantages and disadvantages
In the two approaches presented above, the methods are
based on signal processing techniques. They can be used to
create warped motion quickly and interactively, however,
motion trajectories are decoupled so that irregular motion
may be lost in many cases. They do not handle dynamics
effects, and it is consequently difficult to ensure that the
resulting motion is realistic.

4 Motion blendling
Motion blending takes the data received from several
motion capture passes and averages and interpolates it to
provide a smoother form of motion animation. Although
blending is a powerful tool for animation, it will not produce convincing results unless the input motions are chosen very carefully. The range of motions that can be successfully blended depends heavily on how much information about the motion is given to the algorithm. As we
mentioned above, time warping has been an important part
of motion blending (Witkin and Popovic [33]). Rose et
al.[26] also used linear interpolation to form a keyframe
based timewarp curve. Apart from previous blending efforts using linear interpolation to compute root parameters,
Park et al.[22] presented a technique based on a multidimensional scattered data interpolation, and assumed that
root path could be well approximated by a circular arc.
They positioned and oriented root parameters according to
a user-specified path. Blending was then used to determine
a skeletal pose appropriate for the local speed and curvature of the root trajectory and to generate the corresponding
motion at each frame.
Signal processing in the frequency domain is a similar technique to that used in motion blending [4]. Unuma
et al.[30] used Fourier expansions of experimental human
motions to interpolate and extrapolate human locomotion.
The angular trajectories are expressed using Fourier series
in the frequency domain. Rose et al.[27] used spacetime
optimization to create transitions, from one clip to another,
that minimize the joint torque. Lee et al.[17] has generated transitions by concatenating motion clip and eliminating discontinuities with displacement mapping techniques.
Sun and Metaxas [28] presented an approach which lacked
high level controllability to synthesize a curved locomotion with given aspects.

4.1 Registration curve method
Without requiring manual intervention, Kovar and Gleicher [14] introduced a blending method called registration
curve which is an automatically constructed data structure
that encapsulates relationships involving the timing, local
coordinate frame, and constraint states of an arbitrary number of input motions. To build a registration curve, they
constructed a time warp curve S(u). An alignment curve

A(u) is then built around S(u), and constraint matches are
identified by using S(u) to map constraint intervals into a
standard time frame. Function D(F1 , F2 ) simultaneously
determines the “distance” between two frames of motion
and a rigid 2D transformation which align both frames:
n

D(F1 , F2 ) = min

θ,x0 ,z0

wi pi − T(θ,x0 ,z0 ) pi

2

(5)

i=1

where pi and pi are respectively the point of the first and
second set of point coordinate. T(θ,x0 ,z0 ) is a rotation by θ
about the y (vertical) axis followed by a translation in the
floor plane by (x0 , z0 ), and wi may be used to weight different joints. This optimization has a closed-form solution:

θ = tan−1

i

wi (xi zi − xi zi ) −

i

wi (xi xi − zi zi ) −

1
(¯
xz¯
wi
1
(¯
xx¯
i wi
i

¯ − x¯i cos θ − z¯i sin θ
x0 = x
z0 = z¯ − x¯ sin θ − z¯ cos θ
i

where α
¯=

n
i=1

i

− x¯ z¯)
− z¯ z¯)

(6)
(7)
(8)

wi αi .

Dynamic timewarping method [4] was used to find a
minimal-cost connecting path between the corresponding
pair of frames created by using Equation 4. The final timewarp curve S(u) is fitted using a uniform quadratic Bspline as an unconstrained optimization problem. Given
the timewarp curve, the alignment curve A(u) = (A1 (u),
A2 (u)) is simple to construct. Where A1 (u) is the identity
transformation and A2 (u) = {θ(u), x0 (u), z0 (u)} is the
result of the spline fit. Finally, constraint matches are identified based on the heuristic that corresponding constraints
ought to occur at similar points in the motions: each motion must have one constraint. And the union of all the
constraint intervals must form a single continuous interval.

4.2

On-line dynamic filter

Yamane et al.[34] described the concept and implementation of a dynamics filter, an online, full body motion generator that converts a physically infeasible reference motion into a feasible one for a given human figure (Figure
2).
In this approach, motion capture data or animator generated data can be input as a reference. The dynamics filter provides a general framework for online generation of
physically consistent motions using an efficient algorithm
for dynamics simulation of human figures. Their method
is based on computing the dynamics of structure-varying
kinematic chains. The problem of this dynamics filter is
that it is difficult to tune the parameters and they need to
find a set of parameters for each behavior.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

Motion data
Upper level controller
qref
Controller
..

q des
Optimizer

Tak et al. introduced an off-line, planned motion editing
method based on robotics. All of them formulated their
problem as a constrained optimization problem.

5.1

Spacetime constraint

Gleicher [9] used spacetime constraints which consider
the entire motion simultaneously:

..

q
Integrator
.

q,q

Figure 2: Dynamics filter
where θref is the generalized coordinates in reference motion, θ¨des is the desired acceleration, θ, θ˙ and θ¨ are generalized coordinates, velocity and acceleration, respectively.

4.3 Advantages and disadvantages
Most motion blending techniques need an animator to
manually choose proper intervals of different motion clips
and adjust them. The animator’s experience and skill determine how convincing the movement will be. Like most
techniques a registration curve does not take physical constraints into account, it also processes motion data of different clips. However, a registration curve approach automatically generates blends between an arbitrary number
of motions by explicitly addressing differences in timing,
path, and constraint state, expanding the range of motions
which can be successfully blended. The concepts of dynamic filters will give animators a more powerful tool to
produce convincing motion by considering physical laws
in the creation of blending animation.

5 Motion editing with constraints
When animating, the majority of animators want to communicate their ideas to the system accurately. It is a distinct
advantage if they are provided with intuitive, interactive
tools to achieve their goals. To that end, to be able to edit
existing motion data through a graphical user interface has
proved very useful for animators. Editing existing data to
achieve a good motion, the system should help the animator without compromising any nuance they may want to include. Gleicher [9] presented a low level interactive motion
editing tool that creates a new motion which meets some
new constraints while minimizing the distance of the original motion. Lee et al.[18] presented an approach which
combined a hierarchical curve fitting technique with a new
inverse kinematics solver in their interactive motion editor
for fast, coarse-to-fine motion control. Popovic and Witkin
[24] described a method for editing motion in a simplified character while maintaining physical validity. In [29]

minimize g(x) subject to

f (x) = c

(9)

where x is a vector that represents the parameters of the
motion, g is the objective function (a scalar function of x),
c is the given constraint vector, and f is a vector function
of the constraints. These constraints have the form:
f (m(tc , x))♦c

(10)

where m(tc , x) is animated motion, ♦ is one of ≥, ≤, or
=, tc is the time at which the constraint exists. In their
approach, most constraints are kinematics. They place a
restriction on the configuration of the character at a given
instant.
Using motion displacement technique, an animated motion m(t, x) was achieved by treating the initial motion
m0 (t, x0 ) as a constant and adding a new curve with convenient parameters:
m(t, x) = m0 (t) + d(t, x)

(11)

where d is a new motion curve. This provides the freedom
to choose a representation that is convenient for editing.

5.2

Multilevel B-spline approximation method

Lee et al.[18] apply two categories of constraints to specify the desired features of the target motion: the first is
joint limits and an anatomical relationship among joints
of articulated figures; the second are for placing the endeffectors of the figure at particular positions and orientations which are interactively specified by the user or automatically derived from the interaction between the figure
and its environment. Their approach combines a hierarchical curve fitting technique - multilevel B-spline fitting
technique (Figure 3) with a new inverse kinematics solver.
Using the kinematics solver, they can adjust the configuration of an articulated figure to meet the constraints in each
frame. Through the fitting technique, the motion displacement of every joint at each constrained frame is interpolated and smoothly propagated to frames.

5.3

Simplified model method

Instead of solving spacetime constraint optimizations
on the full character, Popovic and Witkin [24] constructed
a simplercharacter using the spacetime constraints dynamics formulation (Figure 4). To find the motion of the simplified character by:

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

f0

f0+f1

f0+f1+f2

that is outside of the supporting area, keeping the rest unchanged. The objective function they used in the motion
correction step was the discrete form:

f2

f1

g(Θ) =

Θ(t) − Θ0 (t)


m11

M =  ...
0

Figure 3: Hierarchical curve fitting
Original Motion

···
..
.
···

(14)

M



0
.. 
. 
mJJ

(15)

where · M is the matrix M -norm, which is defined by
x M = xt M x for a given vector x ∈ RJ . Here x has
the dimension of joint angle space.

Final Motion

Simplification
Reconstruction

Balance Filter

Motion Fitting

1.ZMP Trajectory Calculation
Spacetime
Motion Model

Edit

Transformed
Spacetime Motion

2.ZMP Trajectory Correction
3.Motion Correction
Unbalance
Motion

Figure 4: Simplified motion transformation

Balance
Motion

Figure 5: Motion balance filter

min Ed = [h0 (q0 (ti )) − hs (qs (ti ))]2

qs (ti )



Cp (q(t), t) = 0
subject to Cm (q(t), t) = 0


Cd (q(t), t) = 0

5.5
(12)

Advantages and disadvantages

where h0 (q0 (ti )) and hs (qs (ti )) are the original motion
and simplified motion respectively. Cp , Cm and Cd are
the pose constraints, mechanical constraints and dynamics
constraints respectively. The edition and modification are
then computed on this simplified character. Finally, they
remapped the change of motion introduced by the spacetime edit onto the original motion to produce the final animation.

These approaches based on constraints optimization gave
animators more intuitive and interactive control. However,
the interactive rate was significantly affected by computational cost — formulation of constraints and objective.
Among kinematics based, dynamic based and other biomechanics based constrained optimization methods, determining how to build up a proper model based on physical law
is a challenge and usually is time consuming. Also, balancing is another of the most important dynamic aspects of
human motion. Without it, the motion does not look as if
it is happening in the real world.

5.4 Off-line motion balance filter

6

Based on robotics, different from previous approaches
[1] [21] that deal with the balance of static posture and [23]
interactively controlling balanced figure through kinematics constraints, Tak et al.[29] achieved dynamic balance by
analyzing and controlling the trajectory to the zero moment
point (ZMP) (Figure 5). The entire motion is considered in
the objective function:

Retargeting is a technique applied to adapt existing motion data to another character. There is a growing demand
to retarget motions while ensuring that spatial constraints
are enforced. Gleicher [10] presented a technique also used
in Gleicher [9] (as described in Section 5.1) to re-use created motion data and adapt it to another articulated figure with identical structure (connectivity of limbs, types
of joints, number of degree of freedom) but different segment lengths. Monzani et al.[20] introduced a new method
that they used to try to retarget hierarchically and geometrically different characters. In order to achieve character
performance in real time, Choi et al.[5] presented an online method to retarget the motion of a character to another.

(Θ(t) − Θ0 (t))2 =

g(Θ) =
t

d(t)2

(13)

t

where Θ(t) and Θ0 (t) are the joint angle vectors of the
new and original motions respectively. They activate their
balancing algorithm only for the part of trajectory of ZMP

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

Retargeting motion to new character

q

Anatonic
Binding
End Character Skeleton

Performer Skeleton
Motion
Converter

src

(t)

x(t)

Retarget
Filter

q des(t)

Figure 7: On-line motion retargeting filter

Motion
Compose
Intermediate
Skeleton

Figure 6: Motion retargeting using intermediate skeleton
Interestingly, Gleicher [9] [10] solved different problems
using the same method, Park et al.[22] solve the motion retargeting problem using motion blending techniques.

6.1 Intermediate skeleton method

represents the variations of the set of Cartesian constraints
(main task), and the secondary task ∆z is used to minimise
the distance to the attraction posture.
Monzani et al.[20] solve the IK problems for the posture
of articulated figure as a global problem with an Intermediate Skeleton posture. The resulting posture computed by
the IK gives them a new displacement map that the user
can modify later. Moreover, since the IK is an incremental
method, they ensure continuity by computing the solution
at t starting with the posture they had at time t − 1. Consequently, it is always better to compute the solution over
the whole time range, rather than separately.

It is difficult to adapt a captured motion to a different
character, mainly because of the different topology of the
articulated figure; also the constraints are usually violated.
Even if it is fairly easy to correct one posture by modifying
its angular parameters with inverse kinematics, it becomes
a difficult task to perform this over the whole motion sequence while ensuring that some spatial constraint (e.g.,
left foot always contact with the ground and right hand
holds a glass) is applied over a given time range. Monzani et al.[20] presented a new method for solving the motion retargeting problem by using an intermediate skeleton.
This allows them to convert movements between hierarchically and geometrically different characters (Figure 6).
At time t, they first compute the Intermediate Skeleton matrices by orienting the Intermediate Skeleton bones
to reflect the Performer Skeleton posture through Motion
converter; then, they set the End Character Skeleton matrices to the local values of the corresponding Intermediate
Skeleton matrices by using Motion Composer. A displacement map method works as an offset, added to each motion
curve to correct the motion. In order to maintain the original captured posture, while satisfying the Cartesian constraints that the user defined on the End Character Skeleton
over a specified time range. Inverse Kinematics was used
to define special postures and interpolate between them to
produce the animation.
This is possible as long as the system is redundant. The
general solution provided by inverse kinematics is [2]:

˙ denote the end-effector positional vewhere x(t)
˙
and θ(t)
locity and joint angle velocity at time t, respectively. K
is a positive definite matrix, e(t) = x(t) − xdes (t) is the
displacement error, x(t) is a given reference end-effector
trajectory, xdes (t) is the resulting end-effector position at
time t in the destination motion.
This method computes the changes in joint angles corresponding to the changes in end-effector position:

∆θ = J+ ∆x + (I − J+ J)∆z

∆θ[n] = J+ [n − 1](∆x[n] + Ke[n])

6.2 On-line inverse rate control method
Figure 7 shows the on-line motion retargeting process.
The measured joint angle vectors θsrc in the source motion and the position x of reference end-effector of the animated character at discrete key time are the input streams.
The real time output is a stream of joint angle vectors θdes
of the animated character during the destination motion at
corresponding key time. This technique greatly helps to
get more satisfactory result in motion capturing with fewer
trials by giving the real time feedback to the performer.
Choi and Ko [5] presented this method to retarget the
motion of a character to another in real time. This online motion retargeting technique is based on inverse rate
control method which solve close-loop inverse kinematics
based on Jacobian pseudo-inverse (Equation 16):
˙ = J+ (x(t)
˙
+ Ke(t))
θ(t)

(16)

where ∆θ is the joint variation vector, I is the identity matrix, J is the Jacobian matrix of the set of Cartesian constraints, J+ = JT (JJT )−1 is the pseudo-inverse of J, ∆x

where

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

∆θ[n] = θ[n] − θ[n − 1]

(17)

(18)

and
∆x[n] = x[n] − x[n − 1]
e[n] = x[n] − x

des

(19)

[n − 1]

Substitute Equation (19) into Equation (18), the output
is calculated based on the current and immediately previous input values, but does not depend on the future input.
While tracking the multiple end-effector trajectories of the
original character, their on-line motion retargeting imitates
the joint motion of the original character by exploiting the
kinematics redundancies of the animated model. Moreover, jerky motion is prevented since the next configuration
is dependent on the previous configuration in inverse rate
control. The high frequency details of the original motion,
which carries important characteristics of the motion, are
also well preserved by their algorithm.

6.3 Hybrid method
Based on Motion Blending technique (as described in
Section 4), Park et al.[22] targeted a different size and proportion character and the environment in an on-line, realtime manner while keeping the characteristics of the original motion. First, they provide the target stance foot position as input data at each frame, and represent each of
them in the local coordinate frame of its root segment. The
target foot position f (t) in the global coordinate frame is:
f (t) = q
ˆ (t)f (t)ˆ
q−1 (t) + p
ˆ (t)

(20)

where f (t) is the target foot position in the global coordiˆ (t) and q
ˆ (t) are the position and orientation
nate frame, p
of root segment in the local coordinate frame respectively.
The target foot position f (t) is obtained by blending
fi (t), 1 ≤ i ≤ Ne with respect to their weights wi :
Ne

f (t) =

wi fi (t)

(21)

i

for the ith example motion, the foot position fi (t) at time t
in the local coordinate frame is:
fi (t) = q−1
i (t)(fi (t) − pi (t))qi (t)

(22)

where fi (t) is the foot position of the i example motion at
time t measured in the global coordinate frame, and pi (t)
and qi (t) are the position of the root segment and its orientation respectively.
th

6.4 Advantages and disadvantages
To retarget motion to a different character, the techniques based on constrained optimization lead to more computational costs; the techniques based on IK suffer from the

redundant hierarchy, and every time the IK have to be computed from the root to end-effector. How to choose priorities among multiple targets is another problem. However,
IK based techniques give animator a convincing tool to adjust the character. All of the methods presented above only
consider how to synthesize the motion data to be reused
by another character, they seldom consider the effect of the
virtual environment.

7

Discussion

All of the techniques presented here are responses to
the need of the animator in computer animation. The techniques can be classified in four main categories: motion
data processing, motion blending, constraint based editing and motion retargeting. These approaches assume that
convincing motions can be obtained by editing and tuning of captured motion data. Techniques based on interactive user interfaces can give the animator intuitive and
fast control of character motion. However, the adjustments
made by the animator may lead to the loss of some physical properties from the original motion. Techniques based
on constrained optimization could produce more realistic
motion. But, as much of the information about human motion can not be mathematically encoded, most constraints
based methods still suffer from excessive computational
costs and the inaccuracy of solution. In this case choosing
criteria, e.g., setting the objective function, then becomes
a crucial problem when generating a correct motion sequence. While On-line, real time techniques for animation
need to compute the action or performance very quickly
when it is used in the fields of video games or motion capture.
In conclusion, notwithstanding the fact that animators
may choose to exploit the artifacts of a given approach
to develop personal styles, when it comes to animating
convincing representations of real world movement, we
believe that physically based approaches that reuse original motion data to adapt or retarget to different characters,
even different species, are the most promising trend of research in this area. They are therefore most likely to satisfy
the demands of animators in the future.

8

Summary

We have discussed the motion editing techniques that
have been used in data driven character animation. All of
the methods have proven very helpful for animators to create convincing 3D character animation. At the same time
some explicit unsolved problems remain for researchers to
solve. With the development of the movie industry, video
games, art and other areas including biomechanics, there
are still many undiscovered areas for researchers to explore.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

References
[1] Y. Aydin and M. Nakajima. Balanced control and mass center adjustment of articulated figures in interactive environments. The Visual Computer, 15:113-123, 1999.
[2] R. Boulic and D. Thalmann. Combined direct and inverse kinematic control for articulated figures motion editing. Computer Graphics Forum , 11(4):189-202, 1992.
[3] A. Bruderlin and T. Calvert. Goal-directed, dynamic animation of human walking. In Proceedings of ACM SIGGRAPH 89, pages 233-242, July 1989.
[4] A. Bruderlin and L. Williams. Motion signal processing.
In Proceedings of ACM SIGGRAPH 95, pages 97-104, August 1995.
[5] K. Choi and H. Ko. On-line motion retargetting. In proceedings of Pacific Graphics, pages 32-42, October 1999.
[6] P. Faloutsos, M. Van de Panne and D. Terzopoulos. Composable controllers for physics-based character animation.
In Proceedings of ACM SIGGRAPH 2001. Computer
Graphics Proceedings, pages 251-260, 2001
[7] A. C. Fang and N. S. Pollard. Efficient Synthesis of Physically Valid Human Motion. In Proceedings of ACM Transactions on Graphics, pages 417-426, July 2003.
[8] M. Girard and A. A. Maciejewski. Computational modeling
for the computer animation of legged figures. In Proceedings of ACM SIGGRAPH 85, pages 263-270, July 1985.
[9] M. Gleicher. Motion editing with spacetime constraints.
In Proceedings of the 1997 Symposium on Interactive 3D
Graphics, pages 139-148,1997.
[10] M. Gleicher. Retargetting motion to new characters. In
Computer Graphics, pages 33-42, July 1998. In Proceedings
of ACM SIGGRAPH 1998.
[11] J. K. Hodgins, W. L. Wooten, D. C. Brogan and J. F.
O’Brien. Animating human athletics. In Proceedings of
ACM SIGGRAPH 95, pages 71-78, August 1995.
[12] J. K. Hodgins and N.S. Pollard. Adapting simulated behaviors for new characters. In Proceedings of ACM SIGGRAPH 97, Los Angeles, August 1997.
[13] H. Ko. Kinematic and Dynamic techniques for Analyzing,
Predicting, and animating Human Locomotion. PhD thesis,
University of Pennsylvania 1994.
[14] L. Kovar and M. Gleicher. Flexible automatic motion blending with Registration Curves. Eurographics 2003.
[15] T. Komura, Y. Shinagawa and T. L. Kunii. Creating and
retargetting motion by the musculoskeletal human body
model. The Visual Computer, vol.5, pages 254-270, 2000.
[16] J. Laszlo, M. van de Panne and E. Fiume. Limit cycle control and its application to the animation of balancing and
walking. In Proceedings of ACM SIGGRAPH 96, pages
155-162, August 1996.
[17] J. Lee, J. Chai, P. Reitsma, J. Hodgins and N. Pollard. Interactive control of avatars animated with human motion data.
In the proceedings of ACM SIGGRAPH 2002, pages 491500, July 2002.

[18] J. Lee and S. Y. Shin. A hierarchical approach to interactive motion editing for humanlike figure. In Proceedings of
ACM SIGGRAPH 99, pages 39-48, July 1999.
[19] R. Maiocchi. 3-D Character Animation Using Motion
Capture. In Interactive Computer Animation, MagnenatThalmann and Daniel Thalmann, ISBN 0-13-518309-X,
Prentice-Hall, 10-39.
[20] J. S. Monzani, P. Baerlocher, R. Boulic and D. Thalmann.
Using an intermediate skeleton and inverse kinematics for
motion retargeting. Eurographics 2000, vol. 19, Number 3,
2000.
[21] M. Oshita and A. Makinouchi. A Dynamic Motion Control
Technique for Human-like Articulated figures. Eurographics 2001, vol.20, Number 3, 2001.
[22] S. II Park, H. J. Shin and S. Y. Shin. On-line locomotion generation based on motion blending. In Proceedings
of ACM SIGGRAPH 2002 / Eurographics symposium on
Computer animation, pages 105-111, July 2002
[23] C. B. Phillips and N. I. Badler. Interactive behaviors for
bipedal articulated figures. Computer Graphics, 25(4),
pages 349-358, 1991.
[24] Z. Popovic and A. Witkin. Physically based motion transformation. In Proceedings of ACM SIGGRAPH 99, pages
11-20, August 1999.
[25] M. H. Raibert and J. K. Hodgins. Animation of dynamic
legged locomotion. In Proceedings of ACM SIGGRAPH
91, pages 349-358, July 1991.
[26] C. Rose, M. Cohen, and B. Bodenheimer. Verbs and adverbs: Multidimensional motion interpolation. IEEE Computer Graphics and Applications, vol. 18, pages 32 - 48,
September/October 1998.
[27] C. Rose, B. Guenter, B. Bodenheimer and M. Cohen. Efficient generation of motion transitions using spacetime constrains. In proceedings of ACM SIGGRAPH 96, pages 147154, August 1996.
[28] H. C. Sun and D. M. Metaxas. Automating gait generation.
In Proceedings of ACM SIGGRAPH, pages 261-270, 2001.
[29] S. Tak, O. Y. Song and H. S. Ko. Motion balance filtering. Eurographics 2000, Computer Graphics Forum,vol. 19,
pages 437-446, 2000.
[30] M. Unuma, K. Anjyo and R. Takeuchi. Fourier principles
for emotion-based human figure animation. In Proceedings
of ACM SIGGRAPH 95, pages 91-96, August 1995.
[31] M. Van de Panne. From footprints to animation. Computer
Graphics Forum, 16(4):211-223, October 1997.
[32] A. Witkin and M. Kass. Spacetime constraints. In proceedings of ACM SIGGRAPH 88, vol. 22, pages 159-168,
August 1988.
[33] A. Witkin and Z. Popovic. Motion warping. In Proceedings
of ACM SIGGRAPH 95, pages 105-108, August 1995.
[34] K. Yamane and Y. Nakamura. Dynamics Filter-Concept
and Implementation of on-line Motion Generator for Human Figures. In proceedings of the IEEE Transaction on
Robotics and Automation, June 2003

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

