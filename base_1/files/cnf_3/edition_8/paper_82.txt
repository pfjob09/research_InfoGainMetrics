Estimating the “Mental Image” for Comprehensible Rendering of 3D Objects
Roman Zenka, Pavel Slavik
Czech Technical University, Faculty of Electrical Engineering
Department of Computer Science and Engineering, Prague, Czech Republic
zenkar1@fel.cvut.cz, slavik@cslab.felk.cvut.cz
Abstract
We present a method for comprehensible nonphotorealistic rendering of 3D free-form objects. The
objects are rendered using a small amount of lines such
as silhouettes or hatching strokes. The lines are carefully placed to ensure the user would understand the structure of the rendered object correctly. The user’s “mental
image” of the object is estimated using simpliﬁed methods of computer vision, allowing 3D shape reconstruction
from the rendered image. The difference between the original model and the estimated “mental image” is used
as a feedback for reﬁning the line placement. The process stops when the difference is sufﬁciently low, which
means the users would understand the rendering sufﬁciently well despite its simplicity.

1. Introduction
There are various goals the methods for rendering 3D objects strive to achieve — goals such as photorealism, aesthetic quality, and high speed of rendering. In this paper
our goal is set on creating non-photorealistic “comprehensible renderings” that allow the observer to imagine the original 3D objects correctly, like if they were presented the 3D
model itself.
Renderings of 3D objects used to be done by hand until
the invention of photography. Unfortunately, photographs
are often not the best option when it comes to comprehensibility, since they are often too detailed and do not emphasize the important aspects of the 3D model the way the
hand-made images do. This is the reason why many people
prefer to use hand-made images even for technical, medical or architectural illustrations, where the comprehensibility of rendering is essential.
Methods of Non-Photorealistic Rendering (NPR) strive
to emulate the appearance of hand-made images using a

computer. Comprehensible rendering is achieved by closely
mimicking successful artistic styles used in practice.

1.1. Mental image
Our way of achieving comprehensible rendering goes beyond this scheme. Instead of hoping that a rendering method
copied from the artists will produce good results, we try to
actively measure the difference between what the user sees
in the image and what we wanted them to see. We will consider our rendering comprehensible if the measured difference between user’s “mental image” and the original model
is below a speciﬁed threshold.
The measured difference is fed back into the renderer,
forcing it to reﬁne the image until our goal is achieved. Unfortunately, since we want an automatic method, the user
cannot be a part of our feedback cycle. That means we can
only estimate the user’s “mental image” by implementing
a model of the image recognition that takes place in the
human brain. Such model is extremely complicated to create, it however turns out that we are able to estimate the
user’s response with sufﬁcient precision in case the displayed object does not contain abrupt changes in normal
vectors across its surface. In other words, our method is
mainly useful for illustrating smooth (C1 continuous) freeform surfaces.

1.2. Normal vectors as key information
For our purposes we simplify the “mental image” model
to a ﬁeld of normal vectors. This is indeed a substantial simpliﬁcation, but it was not done without reason.
We assume that if users understand the 3D shape of a
certain object correctly, they would be able to render such
shape lit from any angle (as long as they can draw). To do
this, users obviously need to have some form of knowledge
about the normal vector directions. This observation justiﬁes our choice of normal vectors as a good, yet sufﬁciently
simple detector of correct image understanding.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

2. Previous work

3. Our approach

Johnston’s Lumo [7] directly inspired our work. The system Lumo is capable of producing very convincing shaded
images given only a drawing of silhouette edges. The system uses sparse interpolation to transfer the information
about normal vectors from silhouette lines to every point on
the displayed object surface. Since the shading produced by
Lumo usually matches expectations of its users, we chose
this method as a base for our “mental image” estimation.
Despite this direct inspiration by one particular method,
following research topics are also relevant.

Figure 1 shows an overview of our method. Since the
process is relatively complex, we describe it part by part
within our text.

3D Model

Render

2.1. 3D from 2D reconstruction

feedback

NPR
image

known
normals

feedback

confidence
map

-

ck

estimated
normals

db
a

2.2. Comprehensible rendering
Many new algorithms have been invented to provide
more comprehensible rendering than photorealism offers.
It is surprising that nobody tried to mathematically calculate the comprehensibility achieved by a particular method,
or at least to roughly estimate it. The methods are evaluated directly by users, which is a time-consuming and often
expensive task.
Methods for comprehensible rendering range from lighting models [5, 13], over silhouette and feature edge extraction [1, 9, 12] and principal direction visualisation [4], to
methods for modifying the object geometry for instance using cut-outs [3] – just to name a few.

S.I.

S. I.

fe e

There were many attempts on implementing a tool for
creating 3D objects from 2D images drawn by users. This
problem is unfortunately very complicated. So far, no system able to understand an arbitrary 2D image has been implemented.
Current systems recognize a set of drawing gestures,
which are either symbols for 3D shapes [14] or they deﬁne
the shape directly, for instance by specifying its boundary
[6, 8]. Users can create a 3D model interactively by sketching in 2D. These methods are best for modelling blobby objects, where a sketchy interface turns out to be very natural and where the imprecision of hand-drawn images is not
a problem.
There are also semi-automatic methods for obtaining approximate 3D shapes from already ﬁnished, scanned drawings. Petrovic [10] uses a method based on Teddy’s chordal
axis elevation [6] for automatic adding of 3D shadows to 2D
images. The mentioned system Lumo [7] also offers techniques for automatic processing of scanned images.

normals

difference
map

Figure 1. Overview of our method

The process starts by rendering a 3D model, using NPR
into a image consisting of silhouettes and (in later steps)
also of hatching lines or stippling located at required positions. The process of rendering is described in section 4.
3D information is extracted from the rendered image. For
instance, the silhouette lines directly specify the orientation
of normal vectors. The conﬁdence value (probability, that
the extracted information is correct) is also retrieved and

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

made available for further use. The process of 3D information extraction is described in section 5.
Since the information about normal vectors is not known
for each single pixel of the image (a low amount of lines is
used), it is necessary to interpolate these data for the uncovered parts. We implemented sparse interpolation (marked
S.I. in the ﬁgure below) using dampened spring diffusers,
as described in section 6. This process is being performed
twice. The conﬁdence values are interpolated ﬁrst, yielding
a conﬁdence map. The map is then used for correct interpolation of normal vectors.
At this point we obtain our required “mental image”,
containing estimated normal vectors as the user would probably imagine them. The difference between the “mental image” and the original normal vectors is stored in a difference
map. This process is described in section 7.
Finally, the difference map is fed back into the renderer,
which renders more detail in the areas of high difference.
The renderer basically “explains” the unclear or confusing
parts of the image this way.
The entire process is repeated until the differences are
below experimentally determined threshold.

4. Rendering
We render the objects using a simple non-photorealistic
renderer capable of calculating silhouettes and placing hatching strokes or at requested areas. More advanced
NPR techniques can be used; however, we chose the simple technique because it is relatively straightforward to
reconstruct the 3D information from the data it provides. A lot of future research will have to be done to
cover other rendering methods that are known nowadays.

Figure 2. A hatched image calculated from
shaded model

Since the methods of non-photorealistic rendering are
well described [2, 11], let us continue explaining the more
novel ideas of our work.

5. Clues for determining normals
Even a simple line drawing contains a lot of information
we can use for placing the normal vectors properly. There
are places in the image where we know the normal vector
precisely; on other places we know the normals with lower
conﬁdence. Since we assume our object to be free-form and
smooth, we can use simple interpolation for estimating the
normals for every pixel of the rendered image.
Here are some examples of what kind of information we
can use for deriving the normal vector:

5.1. Silhouettes
A silhouette indicates either a place where the surface
normal becomes perpendicular to the viewing direction. If
we expect the displayed object to be round and consist only
of smooth surfaces, we can determine the normal vector for
silhouettes with very high precision. The only problem is
the orientation of such vector. Two circles in ﬁgure 3 have
two very distinct interpretations because the inner circle orientation is unclear.

(a)

(b)

(c)

Figure 3. Two ways of interpreting a line
drawing (a): a bump (b), a depression (c)

In most cases the user understands the silhouettes correctly. It is however possible to extend the rendering and
make the normal orientation clear to the user (by smudging the silhouettes on the inside, by thickening the lines according to lighting calculation, . . . ).
Clearly readable silhouettes allow us to optimize the performance of our algorithm. Since we can access the original 3D model, we can determine the correct normal orientation directly, without re-interpreting the silhouettes. We can
rely on the user to interpret the silhouettes correctly, without need to algorithmically check whether this is really true.
It is important to realize that only the inner side of a silhouette deﬁnes the normal with absolute conﬁdence. The
outer side of the silhouette on the contrary suggests the lowest amount of information. If the outside of the silhouette

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

does not lie on a boundary of the object, the appearance
of a silhouette edge suggests there are at least two layers
placed above each other (like the bump in ﬁgure 3). The upper layer obscures the geometry of the lower one, which
can have the normals set in any way imaginable. We have
to treat the inside of the silhouette edge differently than the
outside.
Figure 4 displays a conﬁdence matte calculated by interpolating the values between inner side of the silhouettes
where conﬁdence is maximal (white) and outer side where
it is minimal (black). You can see that the system is unsure near the bunny’s muzzle, while near the edges of the
body the normal directions are obvious.

Figure 4. Silhouette lines conﬁdence

shadow-casting light. Symmetry of the object should be also
used, since the user needs just a fraction of information, if
they know the object is symmetric. A lot of research has to
be done to utilize the information provided by shading. Speciﬁc treatment deserve the crease lines, at which the normal
vector interpolation should be broken. We would like to address these topics in our future work.

6. Sparse interpolation
The sparse interpolation algorithm is used in the calculation twice.
In the ﬁrst step we interpolate the known conﬁdence values over the entire image. The conﬁdence values are measure of probability with which we know that the given place
has normal vector aiming in speciﬁed direction. Since we
know these values only for places where we performed
some rendering (drew a silhouette, for example), we need
to estimate the value in the rest of the image by some kind
of interpolation.
We use the same dampened spring diffuser as described
in [7]. The known values of conﬁdence C are relaxed over
the entire image using a set of following equations applied
in several iterations to the entire image (V is a velocity
ﬁeld):
Vx,y

5.2. Marks on the surface
Various marks can be drawn on the surface to suggest
its orientation. We have experimented mainly with so called
hatching strokes.
The hatching strokes can suggest the orientation of the
surface as well as its material properties. We used hatching
strokes obtained by cutting the object by several (three) sets
of parallel planes. The shape and spacing of the hatching
strokes then provides clues about the normal vector orientation. For our calculations, we consider the densely hatched
areas to communicate the correct normals with high conﬁdence of pre-deﬁned value (in our case 75%).
Another possibility we considered was stippling - covering the object with tiny circular points whose density varies
with shading. We wanted the spots to suggest the normal
orientation by changing their shape, as if they were painted
on the surface. The shape change was unfortunately not expressive enough due to their small size.

5.3. Other clues
There are other clues to be used that have not been tested
by us yet. We would like to use shadow borders as normal
vector indication in case the user knows the position of the

Cx,y

= dVx,y + k (Cx−1,y + Cx,y−1 + Cx+1,y +
+Cx,y+1 − 4Cx,y )
= Cx,y + Vx,y

Johnston [7] suggests values d = 0.97 and k = 0.4357
for fast convergence. These values have been tested and
used also by us.
The second sparse interpolation distributes the values of
normal vectors Nx,y using the known conﬁdence map C.
The equations are modiﬁed so the values from areas of
higher conﬁdence inﬂuence their surroundings more, than
values of a low conﬁdence. The set of modiﬁed equations is
following:
wx,y

= Cx−1,y + Cx,y−1 + Cx+1,y + Cx,y+1

Vx,y

= dVx,y + k Cx−1,y Nx−1,y + Cx,y−1 Nx,y−1 +
+Cx+1,y Nx+1,y + Cx,y+1 Nx,y+1 −
− wx,y Nx,y

Nx,y

= Nx,y + Vx,y

7. Difference map
Once the normal vectors are obtained, the feedback part
of our algorithm starts. For the feedback we have to derive
some metrics for determining how much the normal vectors
from the original ones differ. We need to calculate a difference map between the two normal ﬁelds.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

The simplest method is calculating the great arc length
difference. Figure 5 shows an example of this difference
map. It can be clearly seen that the normal vectors were
different mainly near the legs of the bunny, at the face and
on the right ear. Difference also appeared across the breast
since its curvature is higher than the rest of the body.
(a)

(b)

Figure 6. First derivative of original (a), and
estimated (b) normal vectors

(a)

(b)

(c)

Figure 5. Original normals (a), estimated normals (b), and resulting difference map (c).
Darker areas denote higher difference.

Another approach we tested was taking in account the
differences between normal image derivatives. This method
was expected to be more useful than the previous one, however, when we did the testing; we found it to be useless.
When we calculated the derivative of the estimated normal
map, we found out it is near to zero almost everywhere,
due to smooth interpolation. Figure 6 compares the derivatives obtained from the original and the estimated normals.
The circle in (b) shows artefacts appearing due to the use of
8-bit arithmetic for storing the normal vector orientation.

8. Implementation
Our method has been implemented using C# and
managed DirectX. Since our work is a part of larger
project supported by Microsoft Research, it has been
made freely available (also including sources) at
http://www.sourceforge.net/projects/dratem.

9. Results
We tested our method on several free-form models and
proved it produces plausible results. The reﬁnement takes

place in areas, where the users would add hatching lines
themselves. Sometimes an object appears to be comprehensibly rendered to the user even without additional hatching,
however, this is caused by the prior knowledge the users
have about the object (e.g. they recognize a bunny in the image). To solve this problem would obviously require much
more complicated modelling of human perception.
The algorithm runs relatively slow especially due to the
sparse interpolation that needs a lot of iteration to produce
smooth normal vector ﬁeld. We used the amount of iteration
step of about a half of
Resulting images can be seen on the following page (ﬁgure 7). In order to put same amount of detail into the picture
using conventional hatching based on shading (ﬁgure 2 earlier in the text), more lines have to be used, yet the method
does not ensure that speciﬁed light direction would let all
the important details stand out.

10. Future work
There is a large amount of work to be done yet. Basically
every technique for 3D visualisation striving to be comprehensible can be equipped with a feedback technique similar
to ours to further improve its performance. We would particularly like to enrich several shading models, using our
feedback technique for automatic light placement.
The other direction of future research is reﬁnement of
the “mental image” estimation. The model of human cognitive process should be extended by estimations of perceived depth, smoothness, uncertainty, material properties,
etc. Psychological studies of human perception can provide
very valuable addition for the model. Extension of the Visual Difference Predictor into 3D would be very useful.
A large amount of work can be spent on simulating the
human knowledge of commonly appearing shapes. Certain
knowledge is shared (e.g. a desk is ﬂat, a cat has a round
face and sharp ears, etc.) and utilising it can lead to further
simpliﬁcation of the rendering.
From the technical point of view, we would like to perform the calculations on GPU, which we expect to cause a

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

dramatic speedup of the entire method, which is at the moment limited mainly by the sparse interpolation step.

11. Acknowledgements
This project has been supported by Microsoft Research and by the Ministry of Education, Youth and Sports
of the Czech Republic under research program No. Y04/98:
212300014 (Research in the area of information technologies and communications) and by internal CTU grant
13089D/04/A.
We thank the Stanford University Computer Graphics
Laboratory for the bunny model.

References
[1] J. Buchanan and M. Sousa. The edge buffer: A data structure for easy silhouette rendering. Proceedings of the First
International Symposium on Non Photorealistic Animation
and Rendering (NPAR) for Art and Entertainment, 2000.
[2] O. Deussen, S. Hiller, C. van Overveld, and T. Strothotte.
Floating points: A method for computing stipple drawings.
Computer Graphics Forum, 19(3):41–50, Aug. 2000.
[3] J. Diepstraten, D. Weiskopf, and T. Ertl. Interactive Cutaway
Illustrations. In Procceedings of Eurographics Conference
’03 (to appear in Computer Graphics Forum), 2003.
[4] A. Girshick, V. Interrante, S. Haker, and T. Lemoine. Line
direction matters: An argument for the use of principal directions in 3d line drawings. Proceedings of the First International Symposium on Non Photorealistic Animation and
Rendering (NPAR) for Art and Entertainment, 2000.
[5] A. Gooch, B. Gooch, P. Shirley, and E. Cohen. A nonphotorealistic lighting model for automatic technical illustration. Computer Graphics, 32(Annual Conference
Series):447–452, 1998.
[6] T. Igarashi, S. Matsuoka, and H. Tanaka. Teddy: A sketching interface for 3D freeform design. SIGGRAPH 99 Conference Proceedings, pages 409–416, Aug. 1999.
[7] S. F. Johnston. Lumo: illumination for cel animation. In
Proceedings of the second international symposium on Nonphotorealistic animation and rendering, pages 45–52. ACM
Press, 2002.
[8] O. Karpenko, J. F. Hughes, and R. Raskar. Free-from sketching with variational implicit surfaces. Eurographics 2002,
2002.
[9] J. D. Northrup and L. Markosian. Artistic silhouettes: A
hybrid approach. Proceedings of the First International
Symposium on Non Photorealistic Animation and Rendering (NPAR) for Art and Entertainment, 2000.
[10] L. Petrovic, B. Fujito, L. Williams, and A. Finkelstein.
Shadows for cel animation. In K. Akeley, editor, Siggraph 2000, Computer Graphics Proceedings, pages 511–
516. ACM Press / ACM SIGGRAPH / Addison Wesley
Longman, 2000.

Figure 7. Example of the reﬁnement process
[11] E. Praun, H. Hoppe, M. Webb, and A. Finkelstein. Real-time
hatching. In E. Fiume, editor, SIGGRAPH 2001, Computer
Graphics Proceedings, pages 579–584, 2001.
[12] T. Saito and T. Takahashi. Comprehensible rendering of 3D shapes. In Proceedings of the 17th annual conference on
Computer graphics and interactive techniques, pages 197–
206. ACM Press, 1990.
[13] P.-P. J. Sloan, W. Martin, A. Gooch, and B. Gooch. The Lit
Sphere: A model for capturing NPR shading from art. Graphics Interface 2001, 2001.
[14] R. C. Zeleznik, K. P. Herndon, and J. F. Hughes. Sketch: An
interface for sketching 3D scenes. SIGGRAPH 96 Conference Proceedings, pages 163–170, Aug. 1996.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

