Human Figure Control Software for Real-Virtual Application
Satoshi Yonemoto
Department of Information Science
Kyushu Sangyo University
2-3-1, Matsukadai Fukuoka 8138503, Japan
yonemoto@is.kyusan-u.ac.jp

Rin-ichiro Taniguchi
Department of Intelligent Systems
Kyushu University
6-1 Kasuga-koen Kasuga Fukuoka 8168580 Japan
rin@limu.is.kyushu-u.ac.jp

Abstract
We present human figure control software using visionbased human motion capturing. We have already developed
core techniques for vision-based human motion tracking
and motion synthesis. We have provided the first version
software as an open source kit. In this work, we have developed the second version software with modified motion
synthesis. Our software consists of the following software:
the vision software (includes 2D blob tracking modules and
stereo vision module) and the motion synthesis software (includes skeletal structure reconstruction from a limit number of 3D positions, i.e., physically simulated joint position
estimator). This software can be employed for many realvirtual applications such as avatar based video conferencing, virtual human control, and 3D game consoles which
need the user postures as input. Our software requires the
following hardware: more than two firewire cameras for vision processing and more than one PC for vision processing, motion synthesis, and rendering. In our real-virtual system, this software is also employed for virtual object manipulation interface with augmented avatar representation.

module
2D blob tracking

I/O
input:
output:

stereo vision

motion synthesis
applications

input:
output:
input:
output:
input:

input images
background images
2D blob positions,
hand states
2D blob positions,
calibration info.
3D blob positions
3D blob positions
joint positions
joint positions,
hand states

Table 1. Input/Output for all software modules
and its applications.

first version, we have provided open source library for image processing toolkit[1]. In this paper, we have proposed
the second version software with modified motion synthesis. Figure. 1 shows our software modules. The software
consists of the following sub software:

1. Introduction
This paper describes human figure control software for
vision-based human motion tracking and motion synthesis. 3D Human motion sensing without physical restriction,
i.e., unwired approach, is a very challenging problem. Many
methods have been proposed in computer vision researches
by extending video motion analysis. The unwired human
motion capturing is applicable for interactive systems to realize man-machine interaction. In particular, it is very effective to estimate 3D body postures, or 3D positions of head,
arms, feet, etc. since 3D body postures can directly reflect a
user’s intention.
We have already developed core techniques for visionbased human motion tracking and motion synthesis. In the

• the vision software (includes 2D blob tracking modules and stereo vision module)
• the motion synthesis software (includes skeletal structure reconstruction from a limit number of 3D positions, i.e., physically simulated joint position estimator)
This software can be employed for many real-virtual applications such as avatar based video conferencing, virtual
human control, and 3D game consoles which need the 3D
postures of the user as input. Table. 1 shows input/output
for the modules.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

2D blob tracking
camera 1

camera 2

view 1

2D blob tracking
view 2

2D blob tracking
camera n

view n

stereo vision

motion synthesis

real-virtual application

Figure 1. Software modules.
Figure 2. Input images and 2D blob tracking
results (top: input images, bottom: the processed views).

2. The Vision Software
We briefly show the vision software to realize unwired
human motion capturing. This software consists of the following two modules:
• 2D blob tracking modules (image analysis)
• stereo vision module (computer vision)

2.1. 2D blob tracking modules
2-D blob tracking method is realized by skin-color region identification. To extract a rough sketch of a human
body from the image, blobs are mainly used as effective visual features. For example, we can employ skin color regions of a face and hands in the image to acquire the upper
body posture. In our method, we assume that skin colors can
be simply modeled in a parametric form. Color-identified
pixels are classified into several blobs based on the similarity of positions to those blobs detected in the previous
frame. Figure. 2 is an example of input images and an example of the processed views in case of two views. In the
bottom views, three blob regions are overlaid. As an option, we can also realize grasp identification, observing the
change of the hand blobs. The identification is usually employed as a kind of gesture commands for virtual object
handling.

2.2. Stereo vision module
When a 2D blob is detected in two views or in multiple views, the 3D position of the blob can be estimated by
stereo vision. In blob tracking, precise estimation is not required because the blob indicates the rough sketch of the hu-

man body. Therefore, we have employed a simplified stereo
calculation. 3-D position calculation is summarized as follows: Based on camera calibration for each of the views, a
line of sight, or a vector from the origin of the camera coordinate system to the center of mass of the blob, is calculated. Intersecting the lines of sight, the center of gravity
for each blob is calculated (see [2]). We adopt Tsai’s approach as the camera calibration method (The calibration
module is also distributed as an open source kit).
The vision software includes the following routines:
• skin color extraction
• boundary detection
• camera calibration data I/O
• multiview setereo integration

3. The Motion Synthesis Software
In many real-virtual applications, human figure models
are used to construct realistic virtual scene. The human figure models are usually controlled by employing general motion capture equipments with special markers. Our software
is the unwired approach, so only 3D positions acquired by
the vision software can be used as input. As the different approach, we realize the skeletal structure reconstruction from
a limit number of 3D positions.
In the first version of our software, we have already
developed motion synthesis method by analytical inverse
kinematics approach[2]. The method has a merit of fast

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

4. Performance Evaluation and Application
We have evaluated our software by applying for several
real-virtual applications.
Our software is written by C++. It runs under the following hardware condition and needs several programming libraries:
• more than two firewire cameras and more than one
PC (for image and vision processing, motion synthesis, and rendering)
• 1394-based digital camera control library on linux[4]
(for the vision software) and OpenGL library[5]
(a) Skeletal model structure

(b) Example of the skeletal model (part models are fit)
Figure 3. Skeletal model.

computation. However, dynamical motion properties can
not be represented in such methods. For the purpose of more
dynamical motion synthesis, we have introduced a physically simulated motion synthesis, where only the skeletal
model can obey the physics law such as point-to-point constraints between adjacent joint points. The method is realized by simulating Hooke’s law. Figure. 3(a) shows the
skeletal model structure, where white circles correspond to
blob positions and black ones correspond to joint points.
Figure. 3(b) is an example of skeletal model reproduced
from only five blob positions of head, hands and foot. We
fit a part model for the adjacent joint points in order to reproduce as the human figure model. The fitting corresponds
to estimate the pose parameters such as rotation and translation.
The motion synthesis software includes the following
routines:
• physically simulated joint position estimator
• model fitting (viewer)

4.1. Applications
In our related projects, this software is employed for virtual object manipulation interface with augmented avatar
representation[3]. Although the avatar is directly controlled
by following by the user motions, detailed motions which
is difficult to observe from input images are automatically
augmented.
The desktop style system is implemented on a PC (laptop
or desktop). In order to realize high performance rendering,
we employ a workstation (dual Xeon 3.20GHz). The system installs two firewire cameras (SONY DVW-V500) and
a wide 2D display in front of the user. The system can perform in real-time and online from vision process to scene
rendering. The user can only monitor the projected virtual
scene with the 2D display. Figure. 4(b) shows the desktopstyle system setup. In Figure. 4(c), the user handles the virtual objects, controlling the upper body of the avatar. In Figure. 4(d), the user can control viewpoint of the virtual scene
by hand. We have also demonstrated the application on the
distributed situations shared through the internet. Our software can be employed for many real-virtual applications
such as avatar based video conferencing and 3D video game
consoles which need the user postures as input.

4.2. Performance evaluation
We have investigated the merits and demerits of our software. We summarize the following features for image and
vision processing:
• The skin-color detection is easily tuned: The parameterized skin color model is acquired from training data.
We can use GUI for the training.
• Multi-view extension is simplified: We have done basic experiments for the multiple view integration. We
have tried the multiview integration up to 6 cameras.
• The estimation accuracy is not high: The vision modules give the acceptable estimation results because the

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

goal of our vision modules is to acquire a rough sketch
of the human body, or blobs.
• Variation of the postures is limited: it depends on the
vision processing results. For example, when the positions of both hands are very close in the image, the
tracking can not be normally performed.
Next, we summarize the following features for motion
synthesis:
• Fast motion synthesis approach: the computation of
physical simulation is effectively approximated in human figure posture calculation[2].

(a) Example of avatar.

• Many applications must consider the size of the user.
Our approach do not depend on such properties. It corresponds to just re-targeting techniques[13].
• The exact postures of the user are not reproduced: Our
approach focuses on augmenting avatar motion. The
goal is to reproduce realistic postures from the limited cues. In our approach, the ’realistic motion’ implies augmented motion generation.

5. Conclusions

(b) Desktop-style system setup.

(c) Virtual object manipulation.

We have developed human figure control software using
vision-based human motion capturing. Our software consists of the following software: the vision software, which
has 2D blob tracking modules and stereo vision module,
and the motion synthesis software, which has skeletal structure reconstruction from a limit number of 3D positions and
physically simulated joint position estimator. This software
can be employed for many real-virtual applications such
as avatar based video conferencing, virtual reality systems,
and 3D games which need the user postures as input. In our
real-virtual system, this software is employed for virtual object manipulation interface with augmented avatar representation. We have evaluated the performance of our methods
used in the software. Future works include evaluation of the
usability by the demonstration in many real-virtual applications.

Acknowledgments
This work has been partly supported by the programs of
the Grant-in-Aid for Scientific Research from the Japan Society for the Promotion of Science (15700172). This work
has also been partly supported by “Scientific Research on
Priority Areas: Informatics Studies for the Foundation of
IT Evolution; A03 Understanding Human Information Processing and its Application”, one of the programs of the
Grant-in-Aid for Scientific Research from the Japan Society for the Promotion of Science (15017270).

(d) Virtual scene navigation.
Figure 4. Our real-virtual applications.
Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

References
[1] Jun Iio, et.al., http://www.malib.net/applications/
[2] Satoshi Yonemoto, Daisaku Arita and Rin-ichiro Taniguchi,
“Real-Time Human Motion Analysis and IK-based Human
Figure Control”, in Proceedings of Workshop on Human Motion, pp.149–154, 2000.
[3] Satoshi Yonemoto and Rin-ichiro Taniguchi, “Virtual Scene
Control Using Human Body Postures”, in Proceedings of 1st
IEEE Workshop on Computer Vision and Pattern Recognition for Human Computer Interaction, 2003.
[4] H.Yosimoto,
et.al.,
IEEE1394
on
Linux:
http://limu.is.kyushu-u.ac.jp/˜yosimoto/ieee1394/indexe.html
[5] OpenGL: http://www.opengl.org/
[6] M.Turk, G. Robertson, “Perceptual User Interfaces”, in Proceedings of Communication of the ACM, Vol.43, no.1, pp.3234, 2000.
[7] C.Wren, A.Azarbayejani, T.Darrell, A.Pentland, “Pfinder:
Real-Time Tracking of the Human Body”, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol.19,
No.7, pp.780–785, 1997.
[8] C.Wren, A.Pentland, “Understanding Purposeful Human
Motion”, in Fourth IEEE International conference on Automatic Face and Gesture Recognition, 2000.
[9] C.Bregler, “Learning and Recognizing Human Dynamics in
Video Sequences”, in Computer Vision and Pattern Recognition, pp.568–574, 1997.
[10] J.Kuffner Jr., “Autonomous Agents for Real-time Animation”, PhD thesis Stanford University, 1999.
[11] Y.Koga, “Planing Motions with Intentions”, Proceedings of
SIGGRAPH’94, pp.24–29, 1994.
[12] A.Witkin, M.Gleicher, W.Welch, “Interactive Dynamics”, in
ACM SIGGRAPH, Vol.24, no.2, pp.11–21, 1990.
[13] M. Gleicher. Retargeting motion to new characters. in Proceedings of ACM SIGGRAPH 98, Annual Conference Series,
pp 33–42, 1998.

Proceedings of the Eighth International Conference on Information Visualisation (IV’04)
1093-9547/04 $ 20.00 IEEE

