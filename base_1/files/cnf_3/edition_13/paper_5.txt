A Robust Image Mosaicing Technique Capable of
Creating Integrated Panoramas
Yihong Gong

Guido Proiettiy

David LaRose

Robotics Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, U.S.A.
Abstract
Existing featureless image mosaicing techniques do not
pay enough attention to the robustness of the image registration process, and are not able to combine multiple video
sequences into an integrated panoramic view. These problems have certainly restricted applications of the existing
methods for large-scale panorama composition, video content overview and information visualization. In this paper,
we propose a method that is able to create an integrated
panoramic view for a virtual camera from multiple video
sequences which each records a part of a vast scene. The
method further enables the user to visualize the integrated
panoramic view from an arbitrary viewpoint and orientation by altering the parameters of the virtual camera. To
ensure a robust and accurate panoramic view synthesis from
long video sequences, we attach a global positioning system
(GPS) to the video camera, and utilize its output data to provide initial estimates for the camera’s translational parameters, and to prevent the camera parameter recovery process from falling into spurious local minima. Our proposed
method is not only suitable for video content overview, but
also applicable to the areas of information visualization,
team collaborations, disastrous rescues, etc. The experimental results demonstrate the effectiveness of the proposed
method.

1. Introduction
Applications for image mosaicing techniques are becoming more and more common in computer graphics, computer vision, and multimedia systems. In the computer
graphics domain, image mosaicing techniques have enabled cost-effective creation of virtual environments [1] and
super-resolution images [2, 3]. In the computer vision field,
these techniques have been exploited to extract 2-D textures
and 3-D models of the target scenes [4, 5]. In the multimedia
area, the same techniques have been used to create content
overviews and visual indexes of digital video images [6].
Image mosaicing techniques can be mainly divided into
 The author now works for NEC USA, C&C Reseach Laboratories.
EMAIL: ygong@ccrl.sj.nec.com
y The author did this work when he was on leave from University of
L’Aquila, Italy. EMAIL: proietti@univaq.it

two categories: feature-based methods, and featureless
methods. Feature-based methods assume that feature correspondences between image pairs are available, and utilize
these correspondences to find transforms which register the
image pairs. A major difficulty of these methods is the acquisition and tracking of image features. Good features are
often hand-selected, and reliability of feature tracking is often a problem due to image noise and occlusion. On the
other hand, featureless methods discover transforms for image registration by minimizing a sum of squared difference
(SSD) function that involves some parameters. Since featureless methods do not rely on explicit feature correspondences, they bear no problems associated with feature acquisition and tracking. However, methods in this category
typically require that the change (translation, rotation, etc)
from one image to another be small, and that good guesses
for the parameters of the transform be given as initial values to the program. Moreover, since there is no guarantee
that the parameter estimate process will definitely lead to
the optimal solution even when the above requirements are
met, special efforts must be made to prevent the parameter
estimate process from falling into local minima.
Another important thing to be emphasized here is that
both the existing feature-based and featureless image mosaicing methods can only create panoramic image from a
single video sequence, and are not able to combine multiple
video sequences into an integrated panoramic view. This
limitation, in addition to the problems described above, has
certainly restricted applications of the existing methods for
large-scale panorama composition, video content overview
and information visualization.
In this paper, we propose a featureless image mosaicing
technique that is able to create an integrated panoramic view
for a virtual camera from multiple video sequences which
each records a part of a vast scene. The major contribution
of this work includes: (1) The panoramic view is synthesized from multiple, independent video sequences, breaking
the limitation of the existing image mosaicing techniques.
(2) The panoramic view synthesis is seamlessly combined
with the virtual environment creation. More specifically,
each panoramic view is synthesized according to the virtual
camera specified by the user, and can be visualized from an

arbitrary viewpoint and orientation by altering the parameters of the virtual camera. (3) To ensure a robust and accurate panoramic view synthesis from long video sequences,
a global positioning system (GPS) is attached to the video
camera, and its output data is utilized to provide initial estimates for the camera’s translational parameters, and to prevent the camera parameter recovery process from falling
into spurious local minima. We also develop a hardware
GPS encoder/decoder that is able to encode/decode the GPS
data into/from the audio track of the video camera. The GPS
data acquisition, and the synchronization between the GPS
data and the video frames are fully automated without the
need of human assistance. Our proposed method is not only
suitable for video content overview, but also applicable to
the areas of information visualization, team collaborations,
disastrous rescues, etc.

2. Related Work
Existing featureless image mosaicing techniques include cylindrical/spherical panoramas, affine transformbased panoramas, and projective transform-based panoramas. Cylindrical/spherical panoramas [1, 7] are commonly
used by various commercial software products because of
their ease of construction. However, this class of methods require the camera to be mounted on a leveled tripod,
and allow only camera pan and tilt around the tripod. Because of these restrictions, the application domain of cylindrical/spherical panoramas is quite limited.
Affine transform-based panoramas [2, 8] are often used
as an approximation to projective transform-based panoramas. Allowable camera motions include translations,
change of focal length, and rotation about the optical axis.
This class of methods provide adequate image registration if
the camera does not pan and tilt excessively, and if the focal
length is sufficiently large.
Projective transform-based panoramas[3, 9, 5] permit arbitrary camera motions. Methods in this category are able
to register image sequences taken under any camera movements, such as translations, zooming, rotation about the optical axis, panning and tilting. The only constraint is that the
target scene being recorded must be a planar scene so that
no parallax exists. In practice, whenever the target scene is
sufficiently remote from the camera, it can be approximated
as a planar scene.
A genuine projective transform is represented by the following two equations:

xt+1 = mm0 xxt ++mm1 yyt ++m12
(1)
6 t
7 t
yt+1 = mm3 xxt ++mm4 yyt ++m15
(2)
6 t
7 t
where xt ; yt  and xt+1 ; yt+1  denote a point in image It ,
and It+1 , respectively, and m0 ; m1 ; : : : ; m7 are 8 parameters that determine the projective transform. The image

Y
∆Y
ΩY

∆X
O

y

X
ΩX

p

P

x

Z=f

ΩZ
Z

∆Z

Figure 1. The world and the camera coordinate systems.
registration process is the process of discovering the 8 parameter set, which is achieved through an iterative process
to minimize the following sum of squared difference (SSD)
function:

SSD =

X

xt ;yt

It+1 xt+1 ; yt+1  , It xt ; yt  2

(3)

In order for the iterative minimization process to converge
to the optical solution, the change (translation, rotation, etc)
between It and It+1 must be very small, and a good estimate of the 8 parameters must be given as the initial values
to the program. As the 8 parameters do not correspond to
physical movements of the camera, it is not an easy task
to make a good guess for these parameters even when the
physical camera motions are known. In an implementation
reported in [5], the user has to bring each frame into the
vicinity of its final position in the panoramic image, and the
registration program starts from there to find the exact position of that frame. Moreover, as with other image mosaicing techniques, this method provides no means of combining multiple video sequences into an integrated panoramic
view, and thus has a limited capability for the purpose of
content overview and information visualization.
In this paper, we propose a projective transform-based
panoramic view creation method which addresses the problems described above. The following sections provide the
detailed descriptions.

3. Camera Parameter Estimation

Suppose the camera is placed at the origin O of the world
coordinate system X; Y; Z , and the image plane x; y is
located at the focal length Z
f (see Figure 1). The perspective projection of a scene point P
X; Y; Z t onto
t
the image plane at a point p
x; y is expressed by:









=
=
= 

p = xy =

 X 
f
YZ f
Z

 


(4)

Suppose It+1 and It are the two consecutive frames taken
by a moving camera. The camera motion comprises two
components: a translation
X ; Y ; Z and a rotation
X ; Y ; Z . When the field of view is not
very large (a planar scene) and the camera rotation is relatively small, each point xt+1 ; yt+1 in frame It+1 can
be expressed as a transformed version of the corresponding
point xt ; yt in frame It [10]:

 =    




=







xt+1
yt+1



2

X
= xytt + Z1 ,0f ,0f xytt 4 Y
Z
2


+



"





,f 2 +x2t 
f
,xt yt

xt yt
f
f 2 +x2
t
f

f

yt
,xt



4




X
Y 5 (5)
Z

(7)

In summary, the above projective transform consists of
9 parameters: 3 for camera translations, 3 for camera rotations, and 3 for the scene depth. The reason why we adopt
Eq.(5) for panoramic view creation is because this projective
transform model explicitly recovers the scene depth, which
will enable us to project the resultant panoramic images into
any specified camera coordinate systems, and hence will
play a critical role in creating integrated panoramic view
for a given virtual camera. Furthermore, as the remaining
six parameters involved in this model correspond directly to
physical camera movements, it becomes much easier for us
to provide good guesses for the initial values of these parameters. Good guesses for the parameters give great impact in
ensuring the parameter estimate process to converge to the
optical solution.
Clearly, there is a scale ambiguity between =Z and .
More precisely, there is an unlimited number of combinations of =Z and
which produce the same displacement
between xt+1 ; yt+1 and xt ; yt . However, if correct values of
can be provided, the remaining 6 parameters will
be uniquely determined.
In the image capturing process, We attach a GPS device
to the camera to obtain the camera’s approximate translations along the XY Z axes, and then use the GPS data to
initialize . The GPS data acquisition, and the synchronization between the GPS data and the video streams are
fully automated by the hardware GPS encoder/decoder developed by us. Civilian GPS receivers are limited in accuracy to about several centimeters, and consequently it is











It+1 xt ; yt; m , It xt ; yt  2 +

xt ;yt



S
S

X , GP
Y , GP
Y
X
+
+
GP S
GP S
2

X



2



3

using Eq.(4), we have:

1



X

'

Z =A+BX +C Y
(6)
By dividing both sides of Eq.(6) by Z , and replacing X; Y

1

SSDm =

5



1
Z = + x+ y



3

where Z is the scene depth corresponding to image point
xt ; yt . Based on the planar scene assumption, all the scene
points X; Y; Z satisfy a plane equation:



more appropriate to use the GPS data as a good guess for,
rather than the final result of, the parameter set .
In order to further address the local minimum problem of
the iterative minimization process, we add three additional
terms to the SSD function in Eq.(3):

Y

S
Z , GP
Z

2



GP S
Z



m





S GP S GP S are
where
is the 9-parameter set, GP
X , Y , Z
the camera translations given by the GPS device, and
GP S GP S GP S are the standard deviations of the corX ; Y ; Z
responding camera translations, respectively. ' is the user
provided confidence coefficient that specifies how much
confidence should be given to the GPS data. Large values of ' will cause the final values of X ; Y ; Z to
S GP S GP S
be close to GP
X ; Y ; Z , while small values of
' will have the opposite effect. Therefore, as a rule of
thumb, ' should be given a value in proportion to the accuracy of the GPS output. Note that the first term of the
above equation has exactly the same meaning of the righthand side of Eq.(3), but is expressed in a different form.
Because xt+1 ; yt+1 are determined by xt ; yt , and the 9parameter set , It+1 xt+1 ; yt+1 can be also expressed as
It+1 xt ; yt ; .
To perform the minimization of the SSD function, we
use the Levenberg-Marquardt iterative non-linear minimization algorithm [11], which is known for its quick convergence in comparison to the direct gradient descent methods
(the details of the method has to be omitted because of the
space limitation).
To further increase the robustness and accuracy of the image registration, we have employed hierarchical registration
and exact model acquisition in the registration process. In
hierarchical registration, a multi-level Gaussian pyramid is
created for each image in the sequence. Registration begins
from the top level of the pyramid, using the most coarsely
subsampled image pair. Parameters estimated from the top
level are used to initialize parameter estimates at the subsequent (finer) level. The process continues down the pyramid
until the bottom of the pyramid is reached. While this technique is not guaranteed to avoid the local minimum problem, it works well even when image pairs have relatively
large displacements.
Exact model acquisition relates the parameters in Eq.(5)
back to the parameters in Eq.(1) and Eq.(2), which represent the exact projective transform. Here we adopt the “four
point method” proposed in [3] for relating the two parameter
sets, which results in four easy to solve linear equations. The







m
m



  





(8)

advantage of performing exact model acquisition is that the
law of composition applies to the exact projective transform,
so that we can project image Ii into the space of image I1
using equation ij j without sacrificing accuracy. Here,
ij is the registration matrix that projects Ii into the space
of Ij , and j is the global registration matrix that projects
Ij into the space of image I1 .
In summary, the registration of the entire image sequence
can be described as follows:

A P

A

P

= 1, P = I. Input image i from the sequence.
2. Input image i +1, and register i; i +1 using hierarchical
1. Set i

1

image pair registration as described above.

3. Relate the parameters obtained from Step 2 to the parameters of the exact projective transform using the exact model acquisition technique. This is equivalent to
acquiring the registration matrix i+1i .

A

+ 1 to the space of image 1 using
= Ai i Pi.

4. Project image i
i+1i i . Set i+1

A P

P

+1

5. increment i by 1. If i reaches the end of the sequence,
terminate the operation; otherwise, go to Step 2.

4. Integrated Panoramic View Creation
In the previous section, we proposed the robust
panoramic image creation by employing GPS data. In this
section, we move beyond the real panoramic view creation
to create integrated panoramic view encompassing multiple
video sequences for a given virtual camera.
The process of compositing an image sequence into a
panoramic image is equivalent to projecting the rest of the
images into the space of a reference image. The reference
image could be the one that overlaps all other images, or
could be just the first frame of the image sequence.
Suppose that a panoramic image is created for the target scene, and the coordinate system of the reference image
is known. Then, if the coordinate system for a virtual camera is specified, the panoramic image of this virtual camera
can be obtained by transforming into the virtual camera’s
coordinate system. The operation consists of the following
steps:





 

1. Project all the pixels x; y of the panoramic image
back into the 3-D world coordinates X; Y; Z :

8



X = xfZ = f  + xx+ y
Y = yfZ = f  + yx+ y
:Z =
1
f  + x+ y
where

f





(9)

is the focal length of the real camera, and
are the parameters about the scene depth recovered by the proposed image registration method (see
Section 3).

; ;





2. Transform X; Y; Z into the world coordinate system
of the virtual camera:

2

3

2

3 2

3

X0
m11 m12 m13
X
4 Y 0 5 = 4 m21 m22 m23 5  4 Y 5
Z0
m31 m32 m33
Z

(10)

Since the world coordinate systems of both the reference image and the virtual camera are known, elements
mij of the transform matrix can be easily obtained.



M



3. Project X 0 ; Y 0 ; Z 0 into the image plane of the virtual
camera:



x0 = f 0 XZ
y0 = f 0 YZ

0

0
0

(11)

0

where f 0 is the focal length of the virtual camera.
In the above operation, we can analytically alter the parameters of the virtual camera to simulate a change of focal
length or a 3-D displacement/rotation of the virtual camera.
In this way, we can synthesize new panoramic views from an
arbitrary position and orientation, just as a real walk through
a 3-D space.
When there are multiple video sequences, we first create
a real panorama for each sequence, and then project each
of these real panoramas into the space of the virtual camera. The final result is an integrated large-scale panoramic
image as if it was taken by the virtual camera. Theoretically, there is no limitation in the number of video sequences
the proposed method can integrate, as long as there is an
enough memory space in the computer. However, the video
sequence must be closely related to each other (e.g., each
sequence covers a part of the vast scene) in order to create a meaningful integrated panoramic view. This ability
of integrating multiple panoramic images into a collaborative perspective not only considerably extends the content
overview ability, but also enables members in a collaborative team to contribute and share their observations about
the environment under investigation among themselves.

5. Results and Discussions
The proposed image mosaicing technique was implemented with C++, and was tested using three image sequences. The three sequences were taken by the same camera in different days and different locations. The camera was
a SONY digital video camcorder which gives a

resolution and 30 FPS frame rate. A GPS antenna was attached to the video camera to obtain 3-D displacements of
the camera. The GPS device was run in the differential
mode, which gives up to one centimeter accuracy and five
position updates per second. To synchronize between the
video stream and the GPS data, the GPS output was encoded
into the DTMF audio signals using a hardware encoder, and
was recorded onto the audio track of the video tape. The

640 480

GPS data were then recovered by feeding the audio output
from the video camera into a hardware DTMF decoder. Because the GPS data update rate is much lower than the video
frame rate (5 vs 30), only the video frames that have corresponding GPS data were used for the creation of panoramic
images. Our experiments have shown that this 5 FPS frame
rate is sufficient for the proposed technique when the camera motion is not very fast.
Figure 2 shows the three panoramic images which were
created using video sequences 1, 2, and 3, respectively. All
of the panoramic images were fully automatically generated
by the proposed method without any human intervention.
Sequence 1 consists of more than 100 frames, while Sequence 2 and 3 were taken in a cloudy day with remarkable
brightness variations. To let readers better understand the
composition of each panoramic image as well as the brightness changes in the corresponding video sequence, image
boundaries were deliberately left, and no post-processing
was performed to smooth out the uneven brightness in the
panoramic images. It is clear from the figure that the three
panoramic images were all properly composited with no apparent errors.
For comparison, we implemented the image mosaicing
method presented in [5], and tested it using the above three
video sequences. This method uses the exact projective
transform defined by Eq.(1),(2) in combination with hierarchical image pair registration. None of the three sequences
were fully successfully registered by this method. Errors
occurred when geometrical changes or brightness variations
between image pairs were relatively large. When given a
good guess for the parameter set, however, the method was
able to properly register all the frames, and finally create
correct panoramic images from these sequences.
Figure 3 shows three integrated panoramic images for a
given virtual camera. These three images were created by
projecting panoramic images (2) and (3) from Figure 2 into
the coordinate system of the virtual camera, and by altering the orientation and zoom settings of the virtual camera.
The angles of the optical axes of the reference frames of
video sequences 2 and 3 (the first frame in each case) were
measured during the video recording, which were horizon,
east from the north pole, respectively.
tal, and were
The virtual camera was placed at a point around the middle of the gap between the video sequences 2 and 3, and its
orientation and zoom settings were described in the caption
of Figure 3. A blank in these results exists because video
sequences 2 and 3 do not overlap each other, and no information for that part of the scene was captured. Sequence
1 was not projected into the synthesized panoramic images
because it was taken miles away from the rest of the two
sequences.
The above experimental results have demonstrated the
effectiveness of the proposed method. This method is applicable to various application domains such as video con-

60 65

tent overview, information visualization, team collaborations, disastrous rescues, etc.

References
[1] S. E. Chen, “Quicktime VR – an image-based approach to virtual environment navigation,” in ACM
Computer Graphics (SIGGRAPH’95), pp. 29–38,
Aug. 1995.
[2] M. Irani and S. peleg, “Improving resolution by image
registraion,” in Graphical Models and Image Processing, May 1991.
[3] S. Mann and R. Picard, “The virtual bellows: A new
perspective on the rigid planar patch,” in Technical Report 260, MIT Media Lab Perceptual Computing Section, (Cambridge, MA), Jan. 1994.
[4] R. Szeliski and J. Coughlan, “Hierarchical splinebased image registration,” in IEEE Conference on
Computer Vision and Pattern Recognition (CVPR’94),
(Seattle), pp. 194–201, June 1994.
[5] R. Szeliski and S. Kang, “Direct method for visual
scene reconstruction,” in IEEE Workshop on Representations of Visual Scenes, (Cambridge, MA), June
1995.
[6] Y. Taniguchi, A. Akutsu, and Y. Tonomura, “PanoramaExcerpts: Extracting and packing panoramas for
video browsing,” in ACM Multimedia’97, (Seattle),
Nov. 1997.
[7] L. McMillan and G. Bishop, “Plenoptic modeling: An
image-based rendering system,” in ACM Computer
Graphics (SIGGRAPH’95), pp. 39–46, Aug. 1995.
[8] J. Koenderink and A. van Doorn, “Affine structure
from motion,” Journal of the Optical Society of America, vol. 8, 1991.
[9] M. Irani, P. Anandan, and S. Hsu, “Mosaic based
representations of video sequences and their applications,” in ICCV’95, (Cambridge, MA), June 1995.
[10] J. Bergen, P. Anandan, K. Hanna, and R. Hingorani, “Hierarchical model-based motion estimation,”
in Second European Conference on Computer Vision
(ECCV’92), (Santa Margherita, Italy), May 1992.
[11] W. Press and et al., Numerical Recipes in C: The Art
of Scientific Computing. Cambridge, England: Cambridge University Press, 2 ed., 1992.

Figure 2. Three panoramic images: (1) from sequence 1; (2) from sequence 2; (3) from sequence 3.

Figure 3. Three integrated panoramic images created using panoramic images (2) and (3) from Figure 2. The optical axes of the reference frames of video sequences 2 and 3 were horizontal, and
were
,
east from the north pole, respectively. The virtual camera was placed at a point around
the middle of the gap between the two sequences. (1) Integrated panorama for a virtual camera with
the optical axis
east from the north pole; (2) Integrated panorama for a virtual camera with the
optical axis
east from the north pole; (3) Integrated panorama for a virtual camera with a smaller
zoom setting, and with the optical axis
east from the north pole.

60 65

60

50

70

