Video Composition by Spatiotemporal Object Segmentation, 3D-Structure and
Tracking
Ebroul Izquierdo and Mohammed Ghanbari

Department of Electronic Systems Engineering, University of Essex
Colchester, CO4 3SQ, United Kingdom

Fig. 1: Integration of video objects in a computer generated world.

Abstract
In this paper a stereo vision based system for
composition of natural and computer generated images is
presented. The system focuses on the solution of four
essential tasks in computer vision: disparity estimation,
object segmentation, modeling and tracking. These tasks
are performed in direct interaction with each other using
novel and available multiview analysis techniques and
standard computer graphics algorithms. The system is

assessed by processing natural video sequences. Selected
results are reported in the paper.

1

Introduction

Manipulation, composition and integration of natural
video with computer generated environments is essential in
a large variety of advanced computer vision applications.
This process not only requires accurate registration
between the video objects and the synthesized
environment, but also the extraction of single objects from

video sequences representing complex scenes. In this
context advanced technologies for mixing and enhancing
views of the real world with computer generated
environments have to be capable of segmenting natural
images and tracking single video objects. Currently, there
exists a large collection of application domains for these
key technologies including: entertainment, education,
medical imaging, augmented reality, immersive
telepresence, etc.
In this paper a stereo vision based system for
composition of natural and computer generated images is
presented. In Fig. 1 an example of integration of segmented
stereoscopic video objects and computer generated worlds
is outlined. The presented system focuses on the solution
of four essential tasks in computer vision: disparity
estimation, object segmentation, modeling and tracking. To
deal with the first and second tasks a strategy is used in
which segmentation and disparity estimation interact in a
recursive process providing a robust recognition of
physical objects present in the scene and at the same time
improving an initial displacement field. Initial
displacement fields are obtained by applying the estimator
introduced in [4]. An advanced technique based on
nonlinear diffusion is used for object segmentation. In this
approach initial disparity fields are used to define the
conduction coefficients in the nonlinear diffusion process.
Finally, the initial displacement fields are refined using
segment information.
The third task addressed in the presented system is
concerned with the extraction of 3D structure from single
objects in an unrestricted environment and the efficient
modeling of their surfaces. To cope with this task we
exploit some well-established algorithms from both
computer vision and computer graphics, improving and
combining them with new strategies. The goal is to extract
the 3D structure of single objects present in natural scenes
and to give a compact and efficient representation of them,
keeping computational cost low. The basic idea of the
technique presented is to use feature points and relevant
edges in the images as nodes and edges of an initial 2D
wire-grid. Starting from this initial 2D model the final 3D
wireframe is generated by fitting the 2D model to a
previously recovered depth map of the object. Depth is
estimated by triangulation, using refined disparity fields
and camera parameters.
Two different objectives are challenged in the tracking
task: to establish temporal consistency in the generated
model and to maintain accurate registration between the
real and computer generated environments when they are
integrated. Establishing temporal consistency means that
nodes of the wireframes have to follow the natural optical
flow of the analyzed sequence. Maintaining accurate
registration between the real and computer generated

environments means that the background must remain
aligned with the 3D-orientation and perspective of the real
objects. This goals are achieved by tracking the nodes of
the mesh from frame to frame as long as the same faces of
the object of concern are visible. When a new part of the
object becomes visible or occluded, the wireframe is again
initialized taking into account the nodes that remain visible
and expanding the mesh in order to cover the new object
face.
The paper is organised as follows. Next section
describes the used techniques for combined disparity
estimation and segmentation. In section 3 the methods used
for 3D Modelling are described. Section 4 is concerned
with tracking task. Selected results obtained by processing
natural video sequences are reported at the end of each
section. The paper closes with a summary and conclusions
in section 5.

2

Disparity-Driven Object Segmentation

Object segmentation in computer vision consists of
extraction of the shape of physical objects projected onto
the image plane, ignoring edges due to texture inside the
object borders. In most cases the extraction of segment
masks with physical meaning cannot be carried out without
additional information about the structure or the dynamic
of the scene. For this reason robust systems for object
segmentation should addresses different image processing
tasks. In the proposed method object segmentation is
carried out by combining initial disparity estimates with an
advanced nonlinear diffusion technique for accurate
segmentation. The segmentation results are then used to
refine the initial disparity estimates. To obtain initial
disparity fields, we basically use the hierarchical blockmatching method introduced by Izquierdo [4]. This
estimator is based on a hierarchical approach in two levels.
In the first level, global displacements are estimated and
used as potential displacements in the second level. This
scheme copes with arbitrary disparity ranges and performs
very robustly even in the case of low correlation between
the left and right image areas. More detailed description of
this estimator and the assessment of its performance by
several computer simulations is given [4]. In the image at
the left of Fig. 2 the initial disparity field corresponding to
the first frame pair of the sequence JARDIN is displayed.
The original stereoscopic images are shown in Fig. 1.
To extract masks of physical objects, we introduce a
technique based on nonlinear diffusion. In the context of
image segmentation, it is known that the nonlinear
diffusion paradigm leads to impressive results clearly
outperforming well-established filters like the canny
operator [2]. The nonlinear diffusion model for image
segmentation was introduced by Perona and Malik [5] and

extended by several other authors [1, 3]. In this technique
averaging is inhibited at the image edges and the diffusion
velocity is controlled by the magnitude of the gradient
intensity. Within the nonlinear diffusion process a set of
images I ( x , y , t ) is generated, with I ( x , y ,0 ) as the
original image and t as scale parameter, by applying the
parabolic diffusion equation
I t = ∇ ⋅ [c( x, y , t )∇I ] = div( c( x , y , t )∇I ) .
(1)
If c is chosen as a suitable function of the image edges,
the diffusion process should tend to a piece-wise constant
solution representing a simplified image with sharp
boundaries. Using the diffusion equation (1) with
2

c( x , y , t ) = f ( ∇I ( x , y , t ) ) , a set of simplified images is

generated. Different choices for f are proposed in the
literature. In our work we use
f ( w ) = A (1 + w
)
B

(2)

as proposed in [5]. Note that the so-defined equation is of
type forward parabolic if the image gradient is smaller than
B , otherwise it is of type backward parabolic. That is,
the used model smoothes regions where the gradient is
smaller than B but enhances edges with gradient larger
than
B . Fig. 3 shows the nonlinear diffused image
obtained when the left first image of the sequence JARDIN
is taken as initial condition in the model.
To smooth edges due to texture inside of object
contours, we introduce an extended diffusion model in

Fig. 2: Initial disparity field estimated by hierarchical block-matching (left); and enhanced disparity field estimated
after segmentation (right).

which the conduction coefficient is also modulated by the
disparity field. The main idea behind this approach is to
control the diffusion process according to the variations in
the disparity. This strategy has been inspired from the fact
that image areas with smooth disparity variation probably
represent a single physical object of the scene, thus edges
in these regions are due to texture and should be smoothed.
It is expected that the solution to the modified nonlinear
model tends to a piece-wise constant function representing
a simplified image with sharp boundaries. In contrast to the
solution of the conventional Perona-Malik model, in the
solution of the proposed model each image region with
constant intensity should correspond to a physical object in
the scene.

The degree of smoothness ς ( z ) of the disparity field
at any sampling position z=(x, y) is obtained by measuring
the variance of the disparity vectors inside a small
observation
window
W
centered
at
z:

ς ( z ) = σ z2,W = (σ x2 )2 + (σ 2y )2 , where (σ x2 ) and

(σ 2y )

are the variances of the horizontal and vertical components
of the disparity vectors inside the window W.
The conduction coefficient c in (1) is now defined as
function of a ς -weighted image gradient ∇I ς . That is
the magnitude of the image gradient is weighted at each
sampling position by the local disparity variance σ z2,W . Let
2
σ max
be the maximal variance of the considered disparity

2
field and g:[0, σ max
] → [0, 1] be any increasing function
2
satisfying the two conditions g(0)=0 and g( σ max
)=1. Then

we define

∇I

2
ς

=

g (σ z2,W ).

2

∇I . Obviously, there are

several choices for the control function g. The results
reported in this paper have been obtained by choosing
2
) 2 if v ≤ C
(v σ max
g (v) = 
 1
else

Fig. 3:

2

diffusion equation (1) with c( x , y , t ) = f ( ∇I ( x , y , t ) σ )
and f as the Perona-Malik function (2), an iterative
disparity-driven diffusion process is carried out. Fig. 3
shows the result obtained by applying this technique to the
sequence JARDIN.

Smoothed image by nonlinear diffusion using the Perona-Malik model (left); and disparity-dependent
nonlinear diffusion (right).

To finish interaction process between disparity
estimation and segmentation, the initial disparity field is
improved by matching along the object contour and
considering only intensity information corresponding to the
foreground objects. The field shown at the right hand side
of Fig. 2 correspond to the improved disparity field,
estimated by block-matching but using segmentation
results.

3

where C ∈( 0, 1) is a threshold modulating the influence of
ς in the diffusion process. Applying the parabolic

progressively refined at the locations where the triangular
patches do not approximate the surface accurately. The
approximation error is measured according to the distance
of the model to the object surface taking into account the
reliability of the depth estimated from disparity and camera
parameters.

3.1 Generation of a 2D model by constrained
Delaunay triangulation

3D Modelling

Efficient and flexible modelling of arbitrary 3D
objects from stereoscopic views is carried out using the
results of previous processing. The main goal is the
generation of spatially-optimised and temporally-consistent
models which properly reflect the geometrical surface
characteristics of the objects. The basic idea of the
techniques presented is to use feature points, relevant
edges and abrupt variations in the disparity map as nodes
and edges of the wire grid. The method is adaptive in the
sense that an initial rough surface approximation is

To generate a wireframe model approximating the
object surface, a complete set of surface characteristics
reflecting relevant object features is initially estimated. In
this process three different features are considered: Piecewise linear approximation of image edges (denoted by Λ
in the sequel), edgeness and cornerness. These features are
then used to constrain the position of nodes and edges of
the model. To obtain meshes with nodes regularly
distributed over the non-uniform object areas, the object is
split into rectangular blocks of moderate size. For each
block that does not intersect a feature, the point which is

most clearly distinguished from its neighbors is also
considered as a candidate for the mesh nodes. Let us
denote the set of feature points and lines representing
object surface characteristics as SC = Φ ∪ Λ , where Φ is
the set of relevant image points and corners. Moreover let
Ε be the set of end points and vertices of the polygonal
lines Λ . The generation of the projection in the image
plane of an initial surface approximation constrained to the
surface characteristics leads to a 2D triangulation
constrained to SC. There are different ways to triangulate a
2D object. We are interested in a locally optimal
triangulation T in the sense given by the goodness property

g (T ) introduced by Lawson [6]. Here g (T ) is defined as

function of the minimum interior angles of each triangle of
T.
In order to obtain a local optimal triangulation in the
sense of g (T ) , we first construct the Delaunay
triangulation of all points belonging to Φ ∪ Ε . After that,
the remaining lines of Λ that are not edges of the
Delaunay triangulation are inserted in T. The resulting
constrained Delaunay triangulation is then locally
optimized according to the goodness property g (T ) .

a)

b)

c)

d)

Fig. 4: First original left frame of sequence BUGGY a); generated model b); tracked wireframes corresponding to
the twelfth frame; and perspectively views obtained by rendering the 3D model after texture mapping.

3.2 Generation of the 3D model
Depth maps are estimated for each segmented object
using previously estimated disparities and camera

parameters The accuracy of depth estimation is improved
by averaging corresponding depth values of independently
estimated depth maps, taking into account the reliability of
the disparity estimates. In the averaging process, different

combinations of disparity fields are considered, e.g., using
four cameras with optical axes converging to a common
point and lying on the same plane, a total of 12 disparity
fields can be estimated using all possible image
combinations. In this case, 12 depth values may be
calculated for each sampling position. Ideally, all depth
values should coincide, but due to occlusions, matching
errors, sampling perturbations, noise, etc., these values
differ in practice.
Assigning the depth value to each node of the 2D wire
grid an initial wireframe is obtained. Next, a local adaptive
refinement procedure is performed in order to generate a
polyhedral approximation of the object surface within a
preset error tolerance. The approximation error is
estimated independently for each triangular patch by
measuring the distance between the depth map and the
initial wireframe. If the error is greater than a given
threshold, the model is refined locally around this patch.
The refinement is performed by selecting the point in the
triangular patch that gives the worst approximation and
inserting it as a new node in the 3D triangulation. In order
to compute the approximation error of a given triangular
patch a partition of the image domain is defined. A
weighted Euclidean distance between the triangular patch
and the depth map is used as error estimate. The weights
are calculated in proportion to the reliability of the
disparity vectors used to estimate depth. The reliability
measure is defined as a linear combination of two different
criteria. The first one is based on the disparity-uniqueness
constraint. The second is derived from the analysis of the
curvature of the correlation surface obtained during the
matching process.

4

Tracking

Once a wireframe is available for the first frame pair,
its mesh nodes are tracked by analyzing the natural optical
flow of the video sequence. The motion of the nodes is
accurately estimated from frame to frame as long as the
same faces of the object concerned are visible. When a
new part of the object becomes visible or occluded, the
wireframe is again initialized taking into account the nodes
that remain visible and expanding the mesh in order to
cover the new object face. The motion estimation is based
on a model in which each motion vector inside any object
is represented as the sum of a global motion vector and a
local motion vector. The global motion is calculated by
applying a parameter model and a robust statistical
regression approach. The local motion is estimated from
the global motion parameters and accurate matching.
Outliers are simultaneously detected by applying a strategy
which combines the local and global motion estimates with
their reliability. Fig 4 shows results obtained by applying

the modeling and tracking techniques to the stereoscopic
sequence BUGGY. In Fig 4a the segmented foreground
object corresponding to the first left frame is shown. The
wireframe generated for this object is shown in Fig. 4b.
Fig. 4c shows the tracked wireframe corresponding to the
twelfth frame. A perspectively different view of the object,
obtained by rendering the 3D model after texture mapping,
is displayed in Fig. 4d.

5

Summary and Conclusions

A complete framework for stereoscopic image analysis
and synthesis has been introduced. The main challenges
targeted in this work are the segmentation of video objects
and the efficient modeling of their 3D surfaces. These
goals are reached by combining advanced multiview
analysis techniques with some standard computer graphics
algorithms. Applications of the presented system include
manipulation and integration of natural video with
computer generated worlds. Selected results obtained by
processing natural video sequences are reported in the
paper. Experimental results demonstrate that the objectives
challenged in the work can be achieved by the proposed
system.

Acknowledgment This work was supported by the
Virtual Centre of Excellence in Digital Broadcasting and
Multimedia Technology Ltd., U.K.

References
[1] L. Alvarez, P. L. Lions and J. M. Morel, “Image Selective
Smoothing and Edge Detection by Nonlinear Diffusion. II“,
SIAM J. Numer. Anal., Vol. 29, No. 3, 1992, pp. 845-866.
[2] J. Canny, “A Computational Approach to Edge Detection“,
IEEE Transaction on Pattern Analysis and Machine
Intelligence, Vol. PAMI-8, No. 6, 1986, pp. 679-697.
[3] F. Catté, P. L. Lions, J. M. Morel and T. Coll, “Image
Selective Smoothing and Edge Detection by Nonlinear
Diffusion I“, SIAM J. Numer. Anal., vol. 29, no. 1, 1992, pp.
182-193.
[4] E. Izquierdo, “Stereo matching for enhanced telepresence in
3D-videocommunications“, IEEE Transaction on Circuits
and Systems for Video Technology, Special issue on
Multimedia Technology, Systems and Applications, vol. 7,
no. 4, Aug. 1997, pp. 629-643.
[5] P. Perona and J. Malik, “Scale Space and Edge Detection
Using Anisotropic Diffusion“, Proc. IEEE Comput. Soc.
Workshop on Comput. Vision, 1987, pp. 16-22.
[6] C. L. Lawson, “Software for C1 surface interpolation“, In
Rice J. R. Ed. Mathematical software III, Academic Press,
pp. 161-164, 1977.

