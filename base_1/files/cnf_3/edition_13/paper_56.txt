Integration of High-End and Low-End Animation Tools: A
Case Study of the Production Pathway for the Chicago
Bulls Broadcast Animation
Carlos R. Morales
Purdue University
Crmorales@puk.indiana.edu

The broadcast animation division of High
Voltage Software was contracted by the Chicago
Bulls Organization to complete an animated
opening to be shown at the United Center and on
television before each home game. A team of
animators and compositors developed a
production pathway based on the use of motion
capture data to coordinate the use of high-end
NURBS based animation and lower-end
polygonal based animation tools, and
compositing software.

include a series of 3D rendered bulls running
from numerous places in the city toward the
United Center. Along the way, the bulls would
pass Chicago landmarks. Each landmark passed
by the bulls was to "include enough detail so that
the average Chicagoan could recognize it." [2].
The choice of landmarks was left up the HVS,
except for the inclusion of the Art Institute. One
shot was to include a bull stopping in front of
one of the statues in front of the Art Institute
where it would scare one of the lions. The
deadline for the project was set for November 1,
1998.

The Problem

The Solution

In August 1998, the Chicago Bulls
Organization contracted High Voltage Software
(HVS) to develop a 3D animated opening to be
shown on the JumboTron at the United Center
and on television before each home game. The
Bulls Organization had an animation of a single
Gouroud shaded bull running down a generic
city completed in 1991 by Animation Station.
They wanted to update the animation to more
accurately reflect the current state of computer
graphics.

Personnel

Abstract

Client Mandates
Over a series of meetings with the
client, it was decided that the animation should

HVS assigned a total of six people to
the project.
Three would form the core
production team responsible for generating all of
the graphic elements that would actually
comprise the final animation.
The core
production team was comprised of one character
animator, one effects/architectural animator, and
one technical director/compositor. The author
served as the technical director. The remaining
personnel would form the support staff
comprised of one storyboard artist, one art
director, and one project administrator. Table 1
details the responsibility of each person.

Table 1. Production Personnel
Position
Character Animator
Effects/Architectural Animator

Technical Director/Compositor

Storyboard Artist
Art Director
Project Administrator

Project Planning
The art director and storyboard artist
concluded that the entire city should be built
using 3D software instead of using 3D bulls
running through live video shots of the city. The
look would be a hyper-realistic yellowish overcast similar to what film makers call the golden
hour. The bulls were to be fully textured using
bitmap images. Buildings and street elements
would be left untextured and only given

Responsibilities
Model and animate all organic elements to include
the bull, and lion
Model inorganic structures to include buildings,
street elements (garbage cans, traffic lights, etc.),
and cars
Solve technical problems pertaining to use and
implementation of software and hardware tools,
composite animation plates.
Draw storyboards to reflect the client’s vision
Interact with client and develop the look of the
project
Track media elements, and time management
definition though the use of creative lighting and
basic color. The length of the animation was to
be 32 seconds and take place over a series of 18
shots [6]. A grayscale set of storyboards was
prepared showing the sequence of the animation
[figure 1] and a set of three color still images to
depict the look and feel of the animation [figure
2]. Combined these two documents would serve
as an objective reference to which the production
team could look to for guidance.

Figure 1. Sequential storyboards

Figure 2. Art storyboards

Production
While the art director and storyboard
artist finalized the vision for the animation, the
production staff worked on establishing a
production pathway to meet the project needs.
The production pathway would be developed by
animating a sample scene. In the course of
producing the sample scene the production team
would select software and hardware that could
deliver the look established by the storyboards,
test new techniques, and start to create the
production models. In short, the production of
the sample scene would serve to establish the
milestone schedule, map out the production
pipeline, and also secure the final approval from
the client to start production.
Production of the sample scene started
on August 3, 1998. To maximize production
time while maintaining creative control, the
production team decided to render all of the
scenes in multiple passes. Individual elements
would be created in the package of choice by the
expert in his respective area and composited
together by the technical director. This approach
would allow all of the members of the team to
use the software most appropriate for his task
and not have to compromise by using software
that might be known by all of team members, but
not the best for the task at hand.
This
arrangement would also allow the team to make
changes without having to rerender all of the
elements in a scene. If a particular bull looked
too slow, too yellow, or not dark enough only
that bull would have to be rerendered.
The character animator decided that
only a high-end NURBS based animation
package with support for expressions would be
appropriate. He selected SoftImage 3D Extreme
[3]. The architectural animator determined that a
lower-end polygonal based package such as
LightWave 3D would allow him to model,
texture, and animate much faster than using
SoftImage (Schutlz, 1999).
The technical

director decided that After Effects 3.1 Production
Bundle would be used for compositing. Organic
models would be built and animated in
SoftImage 3D, inorganic models in LightWave
3D, and everything put together in After Effects.

Preparing for Compositing
Once team members selected the tools
they would be using for completing the sample
scene, they focused their attention on making the
necessary preparation to ensure a successful
composite. To enable the production team to
composite organic elements rendered in
SoftImage with inorganic elements rendered in
LightWave 3D, all of the objects in a scene
would have to exist in both SoftImage and in
LightWave at the same scale. Second, a texture
that would receive and cast shadows, but not
render would have to be developed. This would
allow the bulls rendered in SoftImage to cast
shadows on the streets and sidewalks rendered in
LightWave. Finally, the movement of the camera
and any lights would have to match exactly in
both scenes.
The team decided to model everything
using real measurements. To create a sense of
power and strength for the bull, its scale was
changed to approximately 4 times the size of a
real bull. The modeling of the bull proceeded
rapidly using standard NURBS modeling
techniques. The bull was modeled in SoftImage
using primarily loft surfaces and proportional
modeling. Construction history and the "select
by U/V" made it possible to make small changes
to the model effortlessly [3]. The character
animator finished the basic model of the bull in
three days.
To prepare the model of the bull for
animation, an Inverse Kinematics (IK) skeleton
was built and assigned to an automatic global
envelope.
The weights of the generated
envelope were then edited manually and nulls
added to the end effector of the IK chains. With

the body of the bull set up for IK animation, it
was time to prepare the head of the bull for shape
or morph target based animation.
The head of the bull was replicated
multiple times and modeled to reflect different
facial expressions. Each instance of the bull’s
head was then assigned to a null object that
would control how much of that instance would
be reflected in the original head. SoftImage’s
expressions and channel drivers were used to
connect the null to the influence of the individual

shapes on the target bull head. This made it
possible to animate the bull’s face by just moving
the null objects [3]. Finally, textures for the bull’s
skin were generated using Metacreations Detailer
and Adobe Photoshop and applied as a UV map.
Figure 3 shows the final bull model with textures
applied.
The entire process, including
modeling, texturing, and enveloping the bull
took approximately one week. When tessellated
for rendering the bull weighed in at
approximately 2 million polygons.

Figure 3. Bull Model

Models of the city streets were created
using LightWave’s standard polygonal tool set.
Photographs
gathered
by
the
project
administrator were used as a guide in modeling
the street elements. Because the placement of
the camera in the scene had not been determined
at this point, it was decided that the all of the
details on the buildings would be modeled and
no bump maps would be used. The Bloom and
Gaffer plug-ins were then used to give the
buildings the look designated in the storyboards.
It took approximately 1.5 days to set up the street
scene [7].
To allow for proper shadow interaction
between the SoftImage and LightWave objects, a
DXF model was made of a bull modeled in

SoftImage. The bull was imported in LightWave
and assigned the "unseen by camera" surface.
This plug-in allows an object to cast a shadow
onto the scene without it rendering [4]. The net
effect was that by placing the dummy DXF bull
object in the same position to where the bull
would be rendered in SoftImage, it was possible
to properly cast a shadow onto the LightWave
background plate without showing the dummy
bull. The same was accomplished in SoftImage
by importing a dummy DXF version of the street
under the bull and assigning it a "shadow object"
shader [8]. This arrangement resulted in four
unique layers rendered from the two packages
[table 2].

Table 2. Rendered layers
Layer
Foreground

Software
Package
LightWave 3D

Content

Example

Bulls

SoftImage 3D

Render of Bulls only.
Alpha channel.

Bull Shadows

SoftImage 3D

Shadow of bulls falling on
dummy LightWave street
elements.
Shadows
rendered into the alpha
channel.

Background
+ Foreground

LightWave 3D

Render of buildings,
pavement, sidewalk, street
signs.
Both
the
background
and
foreground are included
in this layer. No alpha
channel necessary.

Any street element that
needed to be in front of
the bulls. Alpha channel.

The most significant problem in the production
pipeline was figuring out how to match camera
movements in SoftImage to camera movements
in LightWave. The team first tried using a
commercial product called PolyTrans 2.1, but it
proved unsuccessful. It transferred geometry,
but not animation [5]. The motion of the camera
was not transferred. A viable alternative was to
use motion capture. Calls to SoftImage and
LightWave revealed that both packages import
and export motion data, but the motion data was
incompatible [8]. LightWave uses a proprietary
format [table 3] while SI uses Biovision BVA
[table 4].
An analysis of the LightWave motion
data revealed that it closely resembled the
Biovision BVA format [1]. By multiplying the Z
axis values from the LightWave motion dump by
negative one and reordering the file to include
the proper the TAB delimiters and header it was

possible to format the LightWave motion dump
into a BVA motion file.
Further analysis
showed that the SoftImage camera always
reflects a rotation of 0,0,0 because it derives this
quality by the location of null object to which the
camera always faces. The camera object in
LightWave carries the rotation qualities within
the object. The technical director wrote a
program to read in LightWave motion files,
determine the location of the camera and the null
object needed by SoftImage 3D, and write it out
in Biovision BVA format. The program, called
SoftImage to LightWave Converter (SIL), was
written in Macromedia Director.
Table 3. Lightwave motion dump
0.0225 0.12 -1.0837 0 0 0 1 1 1
0.0232557 0.117897 -1.0526 0 0 0 1 1 1
0.024022 0.115758 -1.02118 0 0 0 1 1 1

Table 4. Biovision motion dump
Segment: TEST
Frames: 3
Frame Time: 0.033333
XTRAN YTRAN ZTRAN XROT YROT ZROT XSCALE
INCHES
INCHES
INCHES
DEGREES

YSCALE
DEGREES

ZSCALE
DEGREES

0.0225
0.0232557
0.024022
0.0247978

0
0
0
0

1
1
1
1

0.12
0.117897
0.115758
0.113586

01.0837
01.0526
01.02118
00.989462

With the 3D elements matched, the production
team was able to focus on rendering. SoftImage
elements would be rendered using the Mental
Ray renderer and LightWave elements using
LightWave’s internal renderer. The technical
director decided to field render all elements for
smoother motion at a resolution of 720 pixels by
486 pixel at D1 aspect ratio. Initial tests proved
that while the field dominance could be set for
SoftImage and LightWave, the method that both
programs use to calculate field motion was
incompatible. SoftImage rendered its fields to
separate files and compressed the vertical size of
the images to only contrain the the information
for that field [8]. This generated pictures that
were half the height of the corresponding
LightWave pictures. LightWave interlaced its
fields into one file [4]. The technical director
decided that it would be best to render the scene
at 60 frames per second and then conform it to
29.97 frames per second with the appropriate
field dominance during compositing.
After rendering the 3D elements of the
test scene, the production team directed its
attention to compositing. The 3D elements had
been completed in four days, including building
the scene elements, matching the camera, and
rendering the scene. The production team could
use this figure as a guideline for how long the 3D
portion of each scene would take to complete.
But, before benchmarks could be set for the
entire process and before the production pipeline
could be finalized, the individual layers would
have to be composited successfully.

Compositing
The integration of the discreet 3D
rendered layers was accomplished by using
Adobe
After
Effects
3.1
Production.
Compositing consisted of interlacing the frames,

0
0
0
0

0
0
0
0

1
1
1
1

1
1
1
1

color correcting the layers, masking and keying
out the shadow areas, and finally applying
camera shake.
The main premise of the
compositing phase was to put everything
together while keeping all of the image files
uncompressed.
Before the individual layers could be
manipulated in After Effects, they had to be
interlaced and conformed to proper NTSC video
timing. The SoftImage fields could not be
properly expanded in After Effects.
The
technical director wrote a script using
AppleScript that would rename the SoftImage
pictures into the needed naming convention,
convert the SoftImage PICS into 32 bit Targa
images, and resize each picture to the proper
size. The resultant files could then be imported
into After Effects where they could be
conformed and interlaced to 29.97 frames per
second lower-field dominant files. This was
accomplished via the After Effect interpret
footage option (After Effects, 1998).
With the footage properly interlaced,
the technical director concentrated on color
correcting the layers and adding effects. The
materials rendered with Mental Ray had a
different gamma and more contrast than those
rendered in LightWave. After Effects levels
were used to match the color qualities of the
renders. The bull’s shadow layer was added by
keying the non-shadow elements in the layer via
a Difference Key and softened via a gaussian
blur. The layer with the bulls was then given
motion blur and finally the entire composition
was nested into a second composition where
camera motion could be added via the wiggler
plug-in to simulate the weight of the bulls as
they ran past the camera. Finally, motion blur
was added and the composition was output as an
uncompressed QuickTime movie file at 720
pixels by 486 pixels at D1 aspect ratio lower

field dominant. Figure 4 shows the composite of

the layers shown in Figure 2.

Figure 4. Composited frame

Production Pipeline
The successful completion of the test
scene finalized the production pathway for the
rest of the project. Through the completion of
the scene, the team had derived a production
methodology which each team member could
use a guiding reference for determining his
responsibilities. Not only had they formed a
procedural plan that dictated the ordinal order for
the completion of the task by the team members,
but they also generated a time schedule which
could be used to orchestrate the rest of the

production [see table 5]. This made it possible
for the team to use an assembly line approach in
completing the project, and to accurately parcel
out the time devoted to each task. Compensating
for tasks that would not have to be replicated for
each scene, such as writing the SIL application
or building the bull model, the team estimated
that it would take approximately five days to
complete a single scene. The only unaccounted
event was the final edit, because it could not be
completed until all of the scenes had been
composited.

Table 5. Production pathway

The Final Edit
Once all of the individual scenes had
been completed and rendered as individual
QuickTime files, the technical director focused
on sequencing the clips into a final edit that
could be printed to tape. The individual
uncompressed QuickTime movies for each clip
were imported into Adobe Premiere where they
were sequenced using standard cuts and
dissolves. Once properly sequenced, another
uncompressed QuickTime movie was output at
720 pixels by 486 pixels at D1 aspect ratio,
lower field dominant. The rationale for

outputting a properly sequenced uncompressed
movie was the ability to access an
uncompromised movie without having to go
back to the individual scenes.
The final uncompressed movie was then
imported into a second Adobe Premiere project
where it was output as a M-JPEG TARGA
compressed movie that could be played out in
real-time using one of the Targa 2000 PRO
board at HVS. It was then printed to a Sony
BVW-75 BetaCam SP deck.
The tape was
delivered to the Bulls Organization on October
28, 1998. On November 1, 1998, the animation

was shown for the first time on WGN and on the
JumboTron at the United Center.

Conclusion
By relying on multi-pass rendering, the
HVS production team was able to exploit the
best qualities offered by SoftImage and
LightWave in the production of the Bulls
animated opening. By concentrating on using
the tools best for each task and then relying on
compositing to integrate the results into a
finished animation, the production team was able
to bring the vision depicted on the storyboard to
the screen.
The production model employed by the
HVS production team brings two issues to
surface. First, we must consider the selection of
the two 3D animation packages selected by the
animators in this project.
The character
animator selected SoftImage 3D Extreme, a
package often hailed as being on of the top
animation packages in the world and has a price
tag to prove it. It has capabilities that few other
programs can match. The architectural animator
elected to use LightWave 3D, a very inexpensive
package. When asked why he elected to use
Brian Schultz said, “I could have used
PowerAnimator, SoftImage, or Visualizer for
animating the back plates, but putting price tags
asides, LightWave was just better for what I
needed to do with the amount of time that I had.”
[7]. This choice was made not because of a
limited budget or limited resources, but instead

because LightWave could best deliver inorganic
elements needed by the architectural animator.
Second, we must consider the reliance
on composting and the implications for the
production pipeline employed by the team. Every
scene contains elements completed by all
members of the team. Each scene has bulls
completed by the character animator, street
elements by the architectural animator, and
effects composited by the technical director.
This level of interdependence on scene elements
necessitated a very exact production pipeline as
shown on table five. The 3D animators had to
exchange camera positions before they could
animate their scenes. The character animator
needed DXF versions of the street elements
produced by the architectural animator before he
could render the shadow layers. The technical
director could not composite anything until all of
the layers had been properly rendered. It was
imperative that all of the production members
follow the production model developed while
producing the test scene. If any of the steps in
the pipeline model were skipped or not followed
in order by any of the production members, the
scenes would not composite correctly.
In the end, we can conclude that the
HVS production team subordinated the selection
of tools to the creation of the client’s vision. By
selecting tools that would best create the client’s
vision and then subscribing to a very strict
production model, the production team was able
to meet the client’s content mandates.

References
[1] Biovision Incorporated, Biovision Motion Data
Specifications. http://www.biovision.com, 1999.
[2] Fiore, R. Personal Interview. Art Director at High
Voltage Software. Hoffman Estates: Illinois, 1999
[3] Jeffrey, E. Personal Interview. Character Animator
at High Voltage Software. Hoffman Estates: Illinois,
1999.
[4] Newtek. LightWave 3D 5.5 Users Manual. Austin,
Texas: Newtek, 1998.

[5] PolyTrans. Polytrans 2. Users Manual. Seattle,
WA: Okino,1998.
[6] Russell, P. Personal Interview. Storyboard Artist at
High Voltage Software. Hoffman Estates: Illinois,
1999.
[7] Schultz, B.. Personal Interview. Architectural and
Effects Animator at High Voltage Software. Hoffman
Estates: Illinois, 1999.
[8] SoftImage. SoftImage 3D 3.7 Extreme Users
Manual. Seattle, WA: Microsoft Press, 1998.
[9] Whiteaker, J. Personal Interview. Director of
Creative Services at High Voltage Software. Hoffman
Estates: Illinois, 1999.

