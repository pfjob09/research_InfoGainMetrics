12th International Conference Information Visualisation

Creation and Visualization of User Behavior in Ambient Intelligent Environment
Ivo Malý1, Jan Cuřín2, Jan Kleindienst2, Pavel Slavík1
Czech Technical University in Prague, Czech Republic, 2IBM Czech Republic
{malyi1@fel.cvut.cz, jan_curin@cz.ibm.com, jankle@cz.ibm.com, slavik@fel.cvut.cz}
1

Abstract

components. A reason for traditional approaches is the
inability to setup a complete environment of supporting
technologies as they are still under development or yet
non-existent. Unfortunately, this is the case typical for a
product’s early phases of development. These phases are
the ones where a correct design decision is crucial, because
it influences further development of the product.
In this paper we introduce a concept for testing
applications in ambient intelligent environments. The
concept is derived from the methodologies of desktop
application usability testing combined with evaluation of
user behavior in an ambient environment. It is based on
user behavior recording what is visualized and investigated
in its virtual copy. The visualization serves as a tool for
analyzing complex situations in ambient applications. Such
a situation may be e.g. a complex task or a collaborative
task.
Several visualization techniques for desktop
application usability testing data evaluation exist. A very
popular one is visualization in form of timelines, presented
e.g. in [5] or [6]. We will present a tool and methodology
that transforms data into a virtual environment, where
additional information of application context like user
position, user behavior or object position may be
interconnected with data from user interaction.

Ambient intelligence is a rapidly developing field
producing a new generation of applications and services
demanding new, more complex strategies for usability
testing and evaluation. In this paper, we identify caveats of
traditional methods of application usability testing when
applied to an ambient intelligent environment and we
introduce a method for unified analysis of user behavior
using a virtual environment visualization. For the usability
testing in ambient environments, we propose to transform
data (both data recorded in real settings and data
artificially created by experts) into a virtual environment
to cope with the following issues: a) overwhelming amount
of data, b) proprietary data formats, and c) ethical codices
issues. We describe a tool for creation of expert evaluation
data and for transformation of data from video recordings
and illustrate its use on two cases.
Keywords—Ambient Intelligence, Virtual Space,
Simulation, Usability Testing

1. Introduction
Ambient intelligence is a rapidly developing field that
is led by the necessity of ubiquitous computing. People are
surrounded by computers in most of their daily activities;
i.e. in cars, homes, offices and public areas, and supplied
by various services on mobile phones, PDAs, or interactive
kiosks. New applications using this interconnection are
starting to appear, allowing new procedures in user task
solving. There are several projects that demonstrate
potentials of ubiquitous computing, e.g. in a museum [11],
in a home [8], or in an office [2].
Along with increased complexity of the system and
user workflow in an ambient intelligent environment, the
usability of all the components must be ensured.
Traditional approaches of usability testing are suitable for
testing individual applications or devices in an isolated
manner. However, usability of the whole system can
significantly differ from the usability of individual

1550-6037/08 $25.00 © 2008 IEEE
DOI 10.1109/IV.2008.38

2. Motivation and Goals
Usability testing methodologies for desktop
applications are quite well known and well described e.g.
by [9] or [10]. The methodologies typically use indirect
observation of the test participant, while solving a task
from the task list where the tasks use the tested application.
Indirect observation is recorded and can be replayed
several times. Typically a small amount of participants (412) are tested and mainly qualitative issues are evaluated,
interpreted, and used for suggested improvements. There
may also be other data sources involved in application
usability testing; typically application logs. Information
from such sources can help to thoroughly analyze
particular parts of the application or the test.

497

Many modifications of this methodology appeared.
One that is also significant for our mobile methodology is
remote usability testing [6]. This methodology does not use
test participant observation and usability of the application
is extracted from application log files. This approach is
suitable when observation is not possible due to technical
limitations, e.g. test participant and test evaluator are at
different places, or when observation does not make sense,
e.g. voice application usability testing.
An advantage of the desktop application is that we can
quite accurately estimate context of use, based on target
user group knowledge. This allows us to focus more on the
observation of user interaction, supplemented with user
thoughts acquired by the thinking aloud technique.
On the other hand, applications for ambient intelligent
environments are much more context sensitive. Context
can change several times during a single test and therefore
we have to give the evaluator all the important context
information. Observation of the user in such an
environment is also problematic. Direct observation gives
the evaluator only a little information for usability
evaluation and analysis from the recording of indirect
observation can be complicated when more cameras are
used in a complex test setup. These issues can be solved by
transformation of the participant behavior and interaction
into the virtual environment that is used for further test
analysis and application usability evaluation.
There are also other motivations that support the idea
of transformation of the participant interaction into the
virtual environment such as:

knowledge of tasks and the target user group. Such data
simulates the user behavior during the task solving based
on usability expert experiences. Expert evaluation is
inevitable in situations where no real user is available.
Moreover, thanks to the unified data representation, data
from expert evaluation can be further used as an optimal
user walkthrough in comparison with both types of real
user behavior.

2.3 Ethical Issues
The third motivation is connected with ethical issues.
The process of usability testing obeys strong ethical
codices, which also keep to the laws. Typically, it is not
allowed to publish a video of the user from the test, where
he/she can be recognized, without his/her permission. In
desktop application usability testing the problem can be
easily solved. The video from a desktop application
focuses on the user interaction with the application, which
means that the application is the object being recorded and
the user usually does not appear in the video (the user can
be recorded using another camera). In the case of mobile
application usability testing, the user is part of the
environment and his/her interaction and behavior is being
investigated. Such a video recording is then not usable as
material for application usability issue presentation. Instead
of making the user anonymous in the video recording the
user can be made anonymous by the process of data
transformation into the virtual environment.

The first motivation is the ability to analyze huge
amounts of data that is gathered from the tested application
and from the supporting infrastructure of the ambient
environment. Such data supports the video recording of
indirect observation or the data may be the only source of
user behavior. It is very complicated to analyze the data in
its raw format such as a text file and visualization in the
virtual environment is one of the suitable data
transformations.

In previous paragraphs we presented motivations why
data from usability testing of an application in an ambient
environment should be analyzed in a virtual environment.
Now we present goals that must be reached to support the
concept. The first goal is creation of the tool for user
behavior and interaction transformation into the virtual
environment. Such a tool called USEd is described in
Section 4. The second goal is evaluation that it is possible
to create user behavior transformations using the USEd
tool and to use the transformed data for usability evaluation
e.g. using SitCom. The use cases and results of the pilot
evaluation are presented in Sections 5 and 6.

2.2 Unification of Data Formats

3. Related Projects

The second motivation is unification of different test
data for direct comparison. We distinguish two categories
of data that can be used for application usability testing.
The first category contains the data from real users. This
data can be acquired by converting the video recording into
the virtual environment when the infrastructure of a mobile
environment is not developed or by automatic collection of
data through the infrastructure interpreted directly by the
virtual environment. The second category contains the data
from experts or developers. A usability specialist can
perform expert evaluation of the application based on

Research in the field of ubiquitous computing in a
mobile environment with respect to usability of
applications is focused on different parts. Modeling of
applications in mobile environments is presented in [4].
Models and distribution of the applications on the available
devices in the system is described using models. These
models are visualized in the virtual environment for the
correctness checking
Another virtual environment for ubiquitous systems is
presented in [7]. In this paper the authors present a tool for
testing the correctness of UPnP protocol based appliances

2.1 Data Analysis

498

and rotation of the environment to allow full control during
scenario creation and analysis. At the bottom there are
buttons for starting and stopping of the recording and for
starting and stopping replaying of the recorded scenario.
There is also the timeline showing time of the recorded
scenario and allowing time shift during replaying. On the
left there are person controls for online recording. We can
add or remove a person from the recorded scenario and we
can select which person is controlled in case there is more
then one person in the scene. There is also a list of actions
the person is currently performing and list of things the
person is currently holding. Two buttons are used for
finishing the selected action or for putting the selected
thing. On the right, there are edit mode controls with
button toggling between data recording and editing.

connected into a network. The authors focus on the
presence of the appropriate appliance, controller
functionality and logic instead of the user behavior.
Visualization of the user behavior is not available.
Situation Composer for Smart Environments (SitCom)
is a 3D simulator tool and runtime for the development of
context-aware applications and services developed at IBM
[1]. SitCom supports service developers, technology
providers and context-modeling experts by collecting the
information from various perceptual components (body
trackers, speech recognition engines) in the smart
environments. It provides higher (semantic) abstraction in
the situation modeling module, and presents it to the user
graphically in a 3D scene.

4. USEd – User Scenario Editor

4.1 Real-time Data Transformation

To fulfill the demand for the user behavior data
transformation, we developed the USEd, the User Scenario
Editor. This tool supports the transformation of data from
the application usability testing video recording into data in
the virtual environment and also transformation or creation
of expert evaluation data in the virtual environment.

Recoding is the activity when a new scenario is
created. It is started by pressing the REC button. After that,
timing begins and we are recording a new scenario. We
add a new person by pressing the Add New Person button.
A dialog for setting the name of the person appears and
after entering the name the person appears in front of the
starting point Entrance. Person movement is controlled by
clicking into the scene. Detection collision and path finding
algorithms were implemented to simplify the scenario
creation. Interaction of the person with other objects in the
scene is performed by right mouse button clicking on the
object. After that, a dialog menu with possible actions
appears and we can select the desired action. There are two
types of action; immediate actions and long lasting actions.
The difference is that long lasting actions appear in the
action list of the person and must be explicitly finished
compared to immediate actions that are executed
immediately and do not appear in the action list of the
person.

4.1 Main Principles

Figure 1. Main window of USEd editor with
highlighted areas of controls.
The User Scenario Editor is a Java3D application for
user scenario creation. This editor is data compatible with
SitCom, because it uses a SitCom environment description
file for virtual environment description and one of the
outputs of the editor is a SitCom person description file.
Therefore scenarios recorded in the USEd can be
investigated in SitCom runtime.
USEd supports both scenario creation and verification.
For scenario creation there are two modes; real-time data
transformation and data editing.
The main screen of the USEd is divided into four
parts, see Figure 1. In the middle, the interactive virtual
environment is displayed. USEd allows zooming, panning

Figure 2. USEd editor in data editing mode.
Instead of avatar the movement path is displayed.

499

In this scenario the user is doing inventory in the smart
room with a PDA. The PDA has a barcode reader that
allows for fast recognition of inventoried items. If the item
is not in the correct place or in the correct room, the user
can move the object in the application to the correct place.
The scenario consists of the following steps:
• User enters the room.
• User looks around.
• User goes to the set of three LCD displays.
• User checks the displays and makes appropriate
changes in the application using his/her PDA.
• User leaves the room.

4.2 Transformed Data Editing
Editing is an activity suitable for correction of the
recorded scenario. It is activated by pressing the Edit Mode
button in the upper right corner of the application. In Edit
mode the virtual environment changes (see Figure 2), the
person disappears and an individual person path is
displayed instead. Also new buttons that switch between
different user interaction modes are available. Edit mode
supports movement of the navigation points, adding and
removing points in the path and also adding new points to
the path. Timeline controls the segment of the path that can
be edited. This is extremely important in case the path is
very dense in a particular area and helps that we not
operate with wrong points of the path.

5.1 Data Acquisition Setup
Expert evaluation data were created before the video
of the real user was created. It is based on the scenario
steps with estimated interactions and timings of user
actions. The expert had no limit for data creation to avoid
any influence.
One video of the use case was recorded. Then 5 people
tried to transform the video into the virtual environment
using USEd. The computer setup is shown in Figure 3. The
USEd editor is on the left; the video player with a recorded
scenario is on the right. Each person had to record the
video in real time with an additional 5 minutes for
corrections.

4.3 Collaborative Scenario Transformation
Collaborative scenario transformation can be achieved
by two different approaches. The first approach is one-byone scenario recording, where one person is being recorded
while other persons that were recorded are replayed to help
with synchronization. The second approach is multiple
person recording, when all persons are recorded
simultaneously and using edit mode the actions are shifted
to appear concurrently.

5. Inventory Use Case

5.3 Use Case Results

The Inventory use case was used to evaluate the ability
of the USEd editor to transform data from video recording
into the virtual environment and create expert evaluation.
Then, these two types of transformations were compared
both from ease of creation and a quality of results point of
view.

All the people were able to transform the video into
the virtual environment. There were some small usability
issues of the USEd editor that will be corrected during
future development. All the videos were similar, only a
limited time drift of action start time appeared. Also, the
algorithm for collision detection and path finding helped to
unify the movement of the avatars.

5.1 Use Case Description

Figure 3. USEd editor and video player during transformation of video into the virtual environment.

500

room? Who is the person speaking at the whiteboard? Has
this person been in the room before?
Schemas in Figure 4 correspond to the setup for the
connector service scenario. In this case, the situation model
provides the information about the current state of the
meeting to the service.

6. Meeting Use Case
The meeting use case arose from actual needs in a
project called CHIL (Computers in the Human Interaction
Loop) [2]. Objectives of the CHIL project were to create an
environment in which computers serve humans that focus
on interacting with other humans. The use of expertcreated scenarios at the beginning of the project allowed us
to start development of services prior to setting-up all the
required technologies. Later when the technologies were
ready, it gives us the possibility to compare behavior of the
same service using either expert-created or real data.

6.2 Bootstrapping Situation Models by Scenarios
Created in USEd
When a project is currently in its first stage, the
“emerging” technologies planned to be used are usually
under development, but service providers are already eager
to try prototypes and discuss deployment of feature
functions with possible clients. In such cases, the
possibility to create a virtual space capable of simulating
the technologies helps and speeds-up the whole
development cycle of the project.
This was also the case for the Connector service
introduced in Section 6.1. Schema in Figure 4 shows the
situation model created for a meeting state recognition
task. We have investigated numerous experiments in order
to find a suitable statistical method. According to the needs
of CHIL services, the methods were applied to a task of
detecting the state of the meeting using labels provided by
the perceptual components. We use the facts produced by
perceptual components (about people’s presence, location,
pose, and voice activity in the room) to determine the
meeting states. These facts may themselves be the result of
the analysis of multiple sensors.

6.1 Use Case Description

Figure 4. Schema of data flow in the meeting
scenario.
In our use-case example, we consider a connector
scenario proposed and exploited in [3]. The connector
service is responsible for detecting acceptable interruptions
(phone call, SMS, targeted audio …) of a particular person
in the smart room. During the meeting, for example, a
member of the audience might be interrupted by a message
during the presentation, whereas the service blocks any
calls for the meeting presenter.
The meeting state detector is implemented as a
situation model in the SitCom framework (mentioned in
Section 3). The situation modeling layer is the place where
the situation context received from audio and video sensors
is processed and modeled. The context information
acquired by the components at this layer helps services to
respond better to varying user activities and environmental
changes. For example, the situating modeling answers
questions such as: Is there a meeting going on in the smart

Figure 5. Meeting scenario visualization using
SitCom.

6.3 Use Case Results
The expert-created scenarios (generated by the
precedent of USEd) were used to bootstrap the initial work
and to obtain first results. Identified methods and feature
sets were than later used with seminar data, particularly the
CHIL seminar 2007 corpus for both model training and
evaluation. Figure 5 shows visualization of recorded

501

seminar data in the SitCom tool. A positive experience was
that the feature sets and their parameters built and tuned on
expert-created scenarios needed only minor changes when
applied to real data. The differences were in selected time
thresholds and intervals; the expert-created scenarios were
not as dynamic as the real recording. Yet another minor
difference was in expected and real speech activity among
meeting participants; the expert-created scenarios
contained many more interruptions and noisy
conversations than the real case. We have used five
manually crated scenarios of total length 1 hour 25
minutes; the real recordings comprised 6 scenarios of 1
hour 50 minutes length.

Acknowledgements

Conclusions and Future Work

[1] Pascal Fleury, Jan Cuřín, and Jan Kleindienst. SitCom Development Platform for Multimodal Perceptual Services.
Proc. of 3nd International Conference on Industrial
Applications of Holonic and Multi-Agent Systems
(HoloMAS’07), LNAI 4659, 104-113, Springer-Verlag.
September 2007.
[2] CHIL Consortium. The CHIL reference model architecture
for multi-modal perceptual systems. Web site (2008)
http://chil.server.de/servlet/is/6503/
[3] Danninger, M., Flaherty, G., Bernardin, K., Ekenel, H.,
Köhler, T., Malkin, R., Stiefelhagen, R., Waibel, A.: The
connector: facilitating context-aware communication. In: 7th
Int. Conference on Multimodal Interfaces 2005. ICMI’05,
New York, USA, ACM Press. 2005.
[4] Luyten, K., C. Vandervelpen, and K. Coninx. 2005. Task
modeling for ambient intelligent environments: design
support for situated task executions. In Proc. of the 4th
Workshop on Task Models and Diagrams 2005. TAMODIA
'05, vol. 127. ACM Press, New York, NY, 87-94 . 2005.
[5] Malý, I., Slavík, P.: Towards Visual Analysis of Usability
Test Logs Using Task Models. In Proc. of the 5th Workshop
on Task Models and Diagrams 2006. TAMODIA 2006.
LNCS, vol. 4385, pp. 24–38. Springer, Heidelberg, 2007.
[6] Paternò F., Russino A., Santoro C.: Remote Evaluation of
Mobile Applications. In Proc. of the 6th Workshop on Task
Models and Diagrams 2007. TAMODIA 2007. LNCS, vol.
4849, pp. 155–169. Springer, Heidelberg. 2007.
[7] Nishikawa H., S. Yamamoto, M. Tamai, K. Nishigaki, T.
Kitani, N. Shibata, K. Yasumoto, and M. Ito. Ubireal:
Realistic smartspace simulator for systematic testing. In
Ubicomp, volume 4206 of LNCS, pages 459–476. Springer,
2006.
[8] The Intuitive Interaction for Everyone with Home
Appliances based on Industry Standards (i2home) project.
Web site (2008) http://www.i2home.org
[9] J. Rubin, Handbook of usability testing: How to plan,
design, and conduct effective tests. John Wiley & Sons, Inc.,
New York, NY, USA, 1994.
[10] Nielsen, J. 1994. Usability Engineering. Morgan Kaufmann.
[11] J. A. Gallud, M. Lozano, R. Tesoriero, V. M. R. Penichet.
Using Mobile Devices to Improve the Interactive Experience
of Visitors in Art Museums. In Human-Computer
Interaction. Interaction Platforms and Techniques, volume
4551 of LNCS, pages 280–287. Springer, 2007.

This work has been sponsored by IBM in the frame of
IBM-CTU Student research projects. It has also been partly
supported by the Ministry of Education, Youth and Sports
of the Czech Republic under the research program LC06008 (Center for Computer Graphics) and the research
program 6840770014 (Research in the Area of the
Prospective Information and Navigation Technologies).
We would like to thank student Martin Krecek for his work
on the USEd implementation.

References

In this paper we identified caveats of traditional
methods of application usability testing applied to an
ambient intelligent environment. Three main issues were a)
overwhelming amount of data, b) different data formats,
and c) ethical codices issues. Based on this motivation we
proposed a method for unified analysis of user behavior
using virtual environment visualization. The method uses
the two visual tools, USEd and SitCom. The User Scenario
Editor (USEd) was used for creation of expert evaluation
data and for transformation of data from a video recording
(a typical data source from usability testing in general) into
the virtual environment. The Situation Composer for Smart
Environments (SitCom) was used for building connector
service in a “smart” office environment and for comparison
of the data prepared by USEd and the data collected by real
perceptual technologies.
The two use cases were used for evaluation of the
tools and the process. The Inventory use-case successfully
checked that it is possible to create and edit expert
evaluation data and more importantly that it is possible to
accurately transform the data from a video recording into
the virtual environment. In addition, the Meeting use-case
was focused on the ability to create collaborative expert
evaluation data and comparison of accuracy of the data
with data gathered by context-aware applications and
services. Our experience shows that such generated data
are usable and may be used when real data are unavailable.
Even though we have shown that our tools are able to
efficiently create user behavior recording for application
usability evaluation, there is still space for improvements
e.g. involvement of task models. Currently, the user
behavior is not divided into shorter parts reflecting
particular task execution. This may lead to problems when
we want to compare user behavior between several
persons. We have to edit the recording and extract all
events in all recordings that are not from that particular
task. Such post processing is time consuming. We would
like to also investigate creation of collaborative scenarios.

502

