Improved Boundary and Silhouette Enhancement in Volume Illustration
Qianqian Han, Yang Gao, Jizhou Sun, Jiawan Zhang
Visualization and Image Processing Group, SRDC, School of Electronic and Information
Engineering, Tianjin University, 300072, China
hanqianqian@eyou.com, gaoyang8054@163.com, jzsun@tju.edu.cn, jwzhang@tju.edu.cn

Abstract
Structure cues are the dominant information in a
rendered image. And conveying structure accurately
and automatically is a problem still not fully solved in
volume rendering. The presence of volume illustration
allows a more flexible way in conveying structure
information compared with the traditional transfer
function. This paper presents two improved approaches
in volume illustration, namely the distance weight
boundary enhancement and gradient weight silhouette
enhancement. In these approaches, the enhancing
parameters vary with the samples‚Äô position and local
gradient. As a result, they are more flexible in
rendering various data sets and are able to convey
clear structure cues without missing of small features.

1. Introduction
Scientific visualization has become an important
tool in the research of many fields. Algorithms can be
categorized into two general approaches, surface
algorithms and direct volume rendering. And the key
advantage of direct volume rendering over surface
rendering approaches is the potential to show the
structure of the value distribution throughout the
volume.
A new kind of approaches to volume rendering has
been proposed: the augmentation of a physics-based
rendering process with nonphotorealistic rendering
(NPR) techniques [12] to enhance the expressiveness
of the visualization. One of those approaches called
volume illustration [1] has recently been presented
specifically for the visualization of volume data. The
approach is fully incorporated into the volume
rendering process. Utilizing viewing information,
lighting information, and additional volumetric
properties, many feature enhancement techniques are
implemented. And the resulting image is able to
convey clearer structure cues than direct rendering.
Two improved volume illustration techniques, the
distance weight boundary enhancement and gradient

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV‚Äô05)
0-7695-2392-7/05 $20.00 ¬© 2005 IEEE

weight silhouette enhancement, are proposed in this
paper. In these two methods, enhancing parameters are
varying based on the sample position and gradient
magnitude, instead of keeping constant in rendering
process. As a result, the amount of enhancement
changes in accordance with the volume structure. And
the result image is more ‚Äúproperly‚Äù enhanced, which
clearly conveys data structure, and keeps detailed
information at the same time. In addition, due to the
parameters‚Äô variability according to data structure, the
approaches are capable of adapting to different volume
data sets.

2. Related works
Traditional volume rendering relies on transfer
functions to produce artificial views of the data or
region of interest. These transfer functions, however,
require in-depth knowledge of the data, and it is a
tedious task to modify transfer functions in rendering
different data sets. Efficient transfer function design is
still an active research area [13][14][9].
In recent years, the work in non-photorealistic
rendering (NPR) techniques has been extended by
numerous researchers. Some research is based on
geometry meshes. With the division of silhouettes into
outlines and shape lines, Martin and Torres described
in [10] a virtual light model in obtaining nonphotorealistic images. In [8], McGuire and Hughes
introduced a feature-edge detection algorithm that runs
entirely in hardware.
As for volume rendering, a new era has begun after
Ebert and Rheinqans gave the definition of volume
illustration in [1]. And thereafter, much research
related has been carried out. There are many extensions
to the volume illustration techniques. For example, In
[6], Cs√©bfalvi Morz introduced a technique of fast
visualizing object contours based on NPR. And in [4],
Nagy Z. presented an accurate, interactive silhouette
extraction mechanism for texture-based volume
rendering.
Some approaches combine the volume illustration
enhancement with the traditional DVR. Helwig and

Lukas in [18] introduced a two-level volume rendering
method. Zhou and Hinz designed a method based on
NPR for generating object contours and enhancing
volumetric features to depict context information out of
focal region, and in the focal region, the direct volume
rendering method was used [5].
There are also some algorithms that introduce the
idea of volume illustration in artistic rendering. Lu and
Morris presented a framework for an interactive direct
volume illustration system that simulates traditional
stipple drawing [7]. Nagy and Schneider in [3]
proposed interactive approaches to non-photorealistic
volume illustration. For a number of seed points that
are placed appropriately to represent selected volume
structures, curvature lines are traced and encoded by a
sparse set of control points. These curves are finally
drawn as hatching strokes.

3. Distance weight boundary enhancement
Levoy [15] introduced gradient-based shading and
opacity enhancement to volume rendering. Ebert and
Rheinqans proposed a method to extend the work [1],
which allows the user to selectively enhance the
density of each volume sample by a function of the
gradient. It is described as equation

og

o v ( k gc  k gs (|| ¬í f ||)

Here ov is original opacity and
vector at the sample position.

k ge

)

(1)

¬í f is the gradient

k gc , k gs and k ge are

coefficients to control the range of gradient
enhancement. Specially, k ge controls the slope of the
opacity curve.
This method, taking the structure information of
gradient magnitude into account, is able to enhance the
boundaries between different objects in a volume data.
In order to acquire the depth cues of visualization as
well, we would enhance boundaries of near objects
more and those of far objects less, which is in
accordance with the case in real world: boundaries of
objects close to us are clearer than those of objects far
off. As for the depth cues, previous researchers have
proposed many approaches related, such as intensity
depth cuing [16], tone shading [17], and distance color
blending [1]. These methods are independent of other
volume illustration methods. While we incorporate the
depth cues into the boundary enhancement approach
through the changing coefficient according to the
sample‚Äôs depth, which can be illustrated by equations

ob
d

ov [k f  k c ¬ò d  k d (1  d )(|| ¬í f ||) ke ]
Di
Dmax

(2)
(3)

Here ov is the original opacity; k f is the ratio of
the part that does not participate in boundary
enhancement; k d is the ratio of enhancement; d is
distance coefficient equaling to the depth value at the
sample position divided by the maximum depth of the
data set, with premise that near objects have less depth;
¬í f is the gradient vector; k e has the same meaning
as the coefficient k ge in Equation 1; and k c is a
compensating coefficient, since we want objects with
great depth to be rendered largely in original method,
instead of giving them less intensity as in intensity
depth cuing. k f , k d , k c and k e are free to be set by
users. But generally, k f is less than 1, k c is less than 1,
sum of them is commended to be 1; k d is greater than
1, and k e is greater than 1. In our implementation, we
let k f be 0.0, and k c be 1.0 with intention that the
volume data fully takes part in the enhancement.
So the amount of boundary enhancement in this
approach is changing with the sample‚Äôs depth. In this
changing process we are able to observe clear
boundaries of the data and depth cues at the same time.
Fig.1.b is an image of engine rendered with distance
weight boundary enhancement in which k d 4.0 ,
and k e

1.0 , compared with the image rendered by

DVR (Fig.1.a). We can see boundaries of the engine in
Fig. 1.b are clearer. And there is a gradual change of
intensity from up to down, as the upper part is nearer
and more enhanced. Generally, in a rendered image,
boundaries of near part can be more easily observed,
whereas for the deeper part, detailed features won‚Äôt be
missed with the help of compensating coefficients.
The improved boundary enhancement approach is
different from a combination of original boundary
enhancement and a depth cuing approach such as
intensity depth cuing. In the original enhancement,
enhancing parameters are kept unchanged for all the
voxels. If the parts with great gradient are emphasized,
generally the even parts become more vague. Fig.1.c is
an image rendered with the original boundary
enhancement with intensity depth cuing method, in
which k gc 0.2 , k gs 4.0 , k ge 1.0 , and
intensity of the farthest voxel is half of the original.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV‚Äô05)
0-7695-2392-7/05 $20.00 ¬© 2005 IEEE

(a)
(b)
(c)
Figure 1. Engine rendered with (a) DVR (b) distance weight boundary enhancement (c) original
boundary enhancement and intensity depth cuing
As we can see, although the depth cuing effect is
quite obvious, darkness of the whole image (side effect
of intensity depth cuing) makes it more difficult to
observe the structure cues.

os

4. Gradient weight silhouette enhancement

gm

Silhouettes are useful information for orientation
cues and for rendering a sketch of the feature shape.
The common approach of silhouette enhancement is to
strengthen the areas where the view vector is
orthogonal to the surface normal vector [17]. Ebert and
Rheinqans in [1] implemented the idea in equation

os

ov (k sc  k ss (1  abs(¬í fn ¬ò V )) k se )

(4)

Here ¬í fn is the gradient vector representing the
surface normal vector, V is the view vector, k sc
controls the scaling of non-silhouette regions, k ss
controls the amount of silhouette enhancement, and
k se controls the sharpness of the silhouette curve.
As we can see, areas where gradient vector is
perpendicular with view direction will obtain the most
enhancements. They are addressed as the silhouette
regions. However, this equation doesn‚Äôt take the
magnitude of the gradient into account, and as a result,
areas with same gradient direction despite the
magnitude would acquire the same amount of
enhancement. This may result in a confusing image if
the data set is a complex one with complicated
appearance and shape. So our new approach add
another factor into the silhouette enhancement, shown
in equations

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV‚Äô05)
0-7695-2392-7/05 $20.00 ¬© 2005 IEEE

ov [k f  k c (1  g m ) 
k s ¬ò g m (1  abs (¬í fn ¬ò V )) ke ]

|| ¬ífn ||
|| ¬íf max ||

¬ò ka

(5)

(6)

Here g m is the gradient magnitude coefficient
proportional to the division between the gradient
magnitude at the sample position and the possible
maximum gradient magnitude, k a controls the amount
of the weight, k f controls the part without silhouette
enhancement, k s controls the amount of enhancement,

k e controls the slope of the silhouette curve, and k c is
the compensating coefficient based on our motivation
that areas with little enhancement would be rendered in
traditional DVR method. ov , ¬í fn and V have the
same meaning as in Equation 4. Usually, in
coefficients‚Äô modulation, k f and k c are less than 1,
sum of them had better be 1; k a , k s and k e are
greater than 1, since they represent the effects of
0.0 and
enhancement. In our experiment, k f

kc

1.0 so that the data set is fully enhanced.

The silhouette parts are usually the concave or
protruding edges. With the addition of the gradient
magnitude factor, enhancement of those edges will
become more flexible, since the amount of
enhancement is proportional to the amount of data
value change. For important edges, where data change
is sharp, such as the border of two objects,
enhancement is more; while for small silhouettes,
enhancement is less.

(a)
(b)
(c)
Figure 2. Bonsai rendered with (a) DVR (b) gradient weight silhouette enhancement (c) original
boundary plus silhouette enhancement
Fig.2.b is a CT image of bonsai rendered using this
approach with k s 2.0 , and k e 1.0 , k a 2.0 ,
compared with the DVR image in Fig.2.a. With
silhouette enhancement, the trunk and branches has
more concave and protruding features. We can see the
small holes on the wood surfaces, edge of which are
the light curves in the image. And crown of the bonsai
has more three-dimensional effect.
We benefit from two advantages as we put the
gradient magnitude factor into the silhouette
enhancement: First, we are able to acquire clear
structure cues of the volume data, because for ordinary
samples, opacity computed in this way is similar with
that computed in DVR method; while samples with
great gradient magnitude and view-orthogonal gradient
direction will be considered as silhouette and
emphasized. This differs from the effect of the original
boundary plus silhouette enhancement. In that situation,
though magnitude and direction of the gradient are also
both used, only the boundary and silhouette regions are
enhanced. Without the compensating coefficients,
small features will possibly be missed. As in Fig.2.c,
small features, such as the little holes on the trunk, are
lost. Second, the enhancement parameters‚Äô varying
with the data structure will increase the adaptability to
different data sets. This point will further be discussed
in next section.

5. Flexibility
The presence of volume illustration [1] allows a
more flexible way in conveying structure information
than traditional transfer function design. However,
when rendering a new data set, usually modification of
the enhancing parameters is needed to obtain a
satisfactory image.
In contrast, we adopt changing parameters in the

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV‚Äô05)
0-7695-2392-7/05 $20.00 ¬© 2005 IEEE

enhancement methods. The enhancing parameters
change with the sample position and local gradient, and
have better adaptability for various data sets.
Coefficients‚Äô modulation is infrequent. Compared with
the DVR techniques, the new approaches are able to
‚Äúfind‚Äù and enhance the boundary and silhouette areas,
which are significant for conveying structure
information. This is the main property of our methods.
Generally, we can obtain an image with clear structure
cues and small features included as well, since besides
the enhancement of the boundary and silhouette
regions, samples of inner or even areas can also be
normally displayed in the final image.

6. Experiment and results
We incorporated the new approaches above into a
ray-casting rendering pipeline. Resolutions of the data
sets used are listed in Table 1. Specially, all the
coefficients are kept unchanged to test the new
approaches‚Äô flexibility. In distance weight boundary
0.0 , k c 1.0 , k d 4.0 ,
enhancement, k f

1.0 , and in gradient weight silhouette
enhancement, k f
0.0 , k c 1.0 , k s 2.0 ,
ke

ke

1.0 , k a

2.0 . The images listed below are

rendered with the combination of the two approaches.
Comparison with the ray-casting images is given.
Back of the engine in Fig.3 has a brighter edge than
that in Fig1.b, which is the effect of the silhouette
enhancement. Trunk of the bonsai in Fig.4 is more
three-dimensional with enhanced boundaries, and
keeps small features such as the small holes on it as
well. Also, the two approaches help to enrich the
structure information. For example, in the head image
of Fig.6, the veins on the temple, the nose, ear and eyes

are clearer, which is largely result of the silhouette
enhancement. And the protruding edges (the blue lines)
on the Teddy bear‚Äôs belly, forehead and legs are also
successfully registered in Fig.8.
The two enhancement approaches are quite easy to
implement. They are basically opacity modulation in
the rendering pipeline. So there is almost no
computation cost. But with these approaches, the
improvement of image quality is prominent. And the
improved enhancement algorithms are more flexible
and good at registering the small features. Thus, they
can be valuable tools in analyzing a volume data set.
Table 1. Resolution of the data sets used in
experiment
Volume data sets
Resolution
Engine
256√ó256√ó110
Bonsai
256√ó256√ó128
Head
256√ó256√ó256
Teddy bear
128√ó128√ó62
Function
64√ó64√ó64

7. Conclusion
In this paper, we have introduced two improved
enhancement methods in volume illustration. The depth
information is added to the boundary enhancement
process. As a result, both boundaries and inner areas
can be clearly displayed with depth cues acquired at the
same time. In silhouette enhancement, we take the local
gradient magnitude into account, so that it can register
more appropriately the unevenness of a volume data.
In implementation, the two improved methods show
great flexibility. The reason is analyzed and from the
rendering results we can see our new approaches are
able to convey clearly the structure cues without
missing of small features.

Acknowledgements
We give much thanks to Klaus Engel, for the data
sets we used in our experiments. The research is
supported by the National Science Foundation under
Grants 60373061.

References
[1] Ebert D. and Rheinqans P., ‚ÄúVolume illustration: nonphotorealistic rendering of volume models‚Äù, Proceedings of
the IEEE Visualization Conference, 2000, pp. 195-202+556.
[2] Lu Aidong, Morris Christopher J., Taylor Joe, Ebert
David S., Hansen Charles, Rheingans Penny and Hartner
Mark, ‚ÄúIllustrative interactive stipple rendering‚Äù, IEEE
Transactions on Visualization and Computer Graphics, v 9, n
2, April/June, 2003, pp. 127-138.

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV‚Äô05)
0-7695-2392-7/05 $20.00 ¬© 2005 IEEE

[3] Nagy Z., Schneider J. and Westermann R., ‚ÄúInteractive
volume illustration‚Äù, Proceedings Conference on Vision,
Modelling and Visualization (VMV), Erlangen, Germany,
2002.
[4] Nagy Z. and Klein R., ‚ÄúHigh-quality silhouette illustration
for texture-based volume rendering‚Äù, Proceedings of Journal
of WSCG Vol.12, No. 2, pp. 301-308.
[5] Zhou Jianlong, Hinz Manfred and T√∂nnies Klaus D.,
‚ÄúFocal region-guided feature-based volume rendering‚Äù, 1st
International Symposium on 3D Data Processing
Visualization and Transmission, Padova, Italy, June 19 - 21,
2002, pp. 87.
[6] Cs√©bfalvi B., Mroz, L., Hauser H., Konig A. and Groller
E., ‚ÄúFast visualization of object contours by nonphotorealistic volume rendering‚Äù, Computer Graphics Forum,
v 20, n 3, Sep 3, 2001, pp. 452-460.
[7] Lu Aidong, Morris Christopher J., Ebert David S,
Rheinqans Penny and Hansen Charles, ‚ÄúNon-photorealistic
volume rendering using stippling techniques‚Äù, Proceedings of
the IEEE Visualization Conference, 2002, pp. 211-218.
[8] McGuire Morgan and Hughes John F., ‚ÄúHardwaredetermined feature edges‚Äù, Proceedings NPAR 2004 - 3rd
International Symposium on Non-Photorealistic Animation
and Rendering, 2004, pp. 35-44.
[9] Kindlmann Gordon, Whitaker Ross, Tasdizen Tolqa and
Moller Torsten, ‚ÄúCurvature-based transfer functions for direct
volume rendering: methods and applications‚Äù, Proceedings of
the IEEE Visualization Conference, 2003, pp. 513-520.
[10] Martin D. and Torres J.C., ‚ÄúRendering silhouettes with
virtual lights‚Äù, Computer Graphics Forum, v 20, n 4,
December, 2001, pp. 271-282.
[11] Michael P. Salisbury, Sean E. Anderson, Ronen Barzel
and David H. Salesin, ‚ÄúInteractive pen-and-ink illustration‚Äù,
Proceedings of SIGGRAPH 94, Computer Graphics
Proceedings, Annual Conference Series, pp. 101-108.
[12] Georges Winkenbach and David H. Salesin, ‚ÄúComputerGenerated Pen-And-Ink Illustration‚Äù, Proceedings of
SIGGRAPH 94, Computer Graphics Proceedings, Annual
Conference Series, pp. 91-100.
[13] Nicoletti G.M, ‚ÄúOptimal generation of transfer functions
for direct volume rendering‚Äù, System Theory, 2003.
Proceedings of the 35th Southeastern Symposium, 16-18
March 2003, pp. 367 ‚Äì 371.
[14] Gunther H. and Weber G. S., ‚ÄùTopology-based transfer
function design‚Äù, Proceedings of the Second IASTED
International Conference on Visualization, Imaging, and
Image Processing, 2002, pp. 527-532.
[15] Marc Levoy, ‚ÄúEfficient ray tracing of volume data‚Äù.
ACM Transactions on Graphics, 9 (3), July 1990, pp. 245261.
[16] Foley Jame, Dam Andries Van, Feiner Steven and
Hughes John, ‚ÄúComputer Graphics: Principles and Practice,
Second Edition‚Äù, Addison Wesley, 1996.

[17] Svakhine Nikolai A. and Ebert David S.,
‚ÄúInteractive volume illustration and feature halos‚Äù,
Proceedings of the 11th Pacific Conference on
Computer Graphics and Applications, 2003, pp. 347.
[18] Helwig Hauser, Lukas Mroz, Gian Italo Bischi
and M. Eduard Gr√∂ller, ‚ÄúTwo-level volume rendering‚Äù,

IEEE Transactions on Visualization and Computer
Graphics, Vol. 7, No. 3, July-September 2001.

Figure 3. Engine rendered with improved
boundary and silhouette enhancement

Figure 4. Bonsai rendered with improved
boundary and silhouette enhancement

Figure 5. Head rendered with DVR

Figure 6. Head rendered with improved
boundary and silhouette enhancement

Figure 7. Teddybear rendered with DVR

Figure 8. Teddybear rendered with improved
boundary and silhouette enhancement

Proceedings of the Computer Graphics, Imaging and Vision: New Trends (CGIV‚Äô05)
0-7695-2392-7/05 $20.00 ¬© 2005 IEEE

