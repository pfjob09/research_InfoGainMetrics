Fifth International Conference on Computer Graphics, Imaging and Visualization
Visualisation

Neighbourhood Discriminant Locally Linear Embedding in Face Recognition
Pang Ying Han1, Andrew Teoh Beng Jin2, Wong Eng Kiong 1
1
Multimedia University, Malaysia
2
Yonsei University, Malaysia
{yhpang@mmu.edu.my, bjteoh@yonsei.ac.kr, ekwong@mmu.edu.my}
kernel LDA [8][11]. LLE is proposed by Roweis and
Saul to characterize nonlinear data. This technique
attempts to preserve local properties of the original data
in the low-dimensional representation via neighbourhood
preservation. Many authors utilized this technique in face
analysis. Hadid et al. assessed the performance of LLE
on one candidate’s real face images in different poses
[1]. Wang et al. and Zhang C.S. et al. reported the
application of LLE in multi-pose face synthesis. The
projected points by LLE scattered along a smooth curve
and in the arrangement associated with the head rotation
trend [9] [3].
From the abovementioned works, we can see that
LLE is popular in analyzing face images according
different poses, illuminations or facial expressions for
one subject class. It is developed based on the
assumption that face images from the same subject class
are distributed on a single manifold; hence the process of
neighborhood selection is non class-specific. However,
this is inappropriate to face recognition as face
recognition learns in multiple manifolds, in which each is
contributed by the training images of each subject.
Ridder et al. [4] have proposed a modified LLE,
namely Supervised LLE (SLLE). The algorithm
increases the separability between samples in different
classes by introducing additional distances, while leaving
the points unchanged if they are from the same class.
Then, the minimum-distance k points are selected as the
set of neighbourhood of the data point. The goal is to
ensure that the neighbourhood of each point is from the
class that the point itself belongs to. Hence, higher
locality preservation can be achieved, leading to a better
discrimination power. Inspired by Ridder’s work, we
propose a method to increase the locality preserving
power of LLE, which we call Neighbourhood
Discriminant LLE (NDLLE). Here, we refer one
neighbour for a point as intra-class neighbor if it belongs
to the same class; and inter-class neighbor otherwise. In
our technique, the intra-class neighbors are given priority
to be included as first h nearest neighbouring points
during training phase while k-h between-class neighbors
(h<k) are considered based on Euclidean distance. As we
have proven experimentally in three face databases, i.e.
ORL, PIE and FRGC, the intuition underlying the
increasing of locality preserving power works out
advantageously for achieving higher recognition
performance compared to LLE and SLLE, as well as
PCA.

Abstract
Face images are often very high-dimensional and
complex. However, the actual underlying structure can
be characterized by a small number of features. Hence,
locally linear embedding (LLE) is proposed as a
nonlinear dimension reduction technique to deal this
problem. LLE learns the intrinsic manifold embedded in
the high dimensional ambient space by minimizing the
global reconstruction error of the neighbourhood in the
data set. LLE is popular in analyzing face images with
different poses, illuminations or facial expressions for
one subject class. It is developed based on the
assumption that data that is distributed on a single
manifold is having the same class label; hence the
process of neighborhood selection is non class-specific.
However, this is inappropriate to face recognition as
face recognition learns in multiple manifolds where each
representing data on one specific class. Here, we modify
the original LLE by embedding prior class information
in the process of neighborhood selection. Experimental
results demonstrate that our technique consistently
outperforms the original LLE in ORL, PIE and FRGC
databases.

1. Introduction
The study on face recognition has been devised over
the past few decades. Therefore, a variety of feature
extraction and analysis algorithms have been proposed
for face recognition. One of the well-studied algorithms
is the appearance-based method [7][12]. Eigenfaces (or
Principal Component Analysis- PCA), proposed by Turk
and Pentland, is one of the most popular appearancebased techniques [12]. It seeks the optimal linear
transformation which aims to preserve the maximum
variance of data vector. However, it is unsupervised.
Therefore, a revised version of PCA has been proposed
by Belhumeur et al., namely Linear Discriminant
Analysis (LDA) [14]. LDA is a supervised learning
algorithm. It seeks for the directions on which the data
points of between classes are far apart, while requiring
data points of the same class to be as close as possible.
Recently, several nonlinear manifold learning
techniques were proposed for handling face images.
There are locally linear embedding (LLE) [16], Isometric
feature mapping (ISOMAP) [2], kernel-based methods,
such as kernel PCA [10][11], kernel ICA [15][17] and

978-0-7695-3359-9/08 $25.00 © 2008 IEEE
DOI 10.1109/CGIV.2008.63

223

2. Locally Linear Embedding (LLE)
x−

Locally linear embedding (LLE) is a nonlinear
dimensionality reduction algorithm proposed by Roweis
and Saul [16]. Different from PCA which aims at
preserving the global Euclidean structure, LLE aims at
preserving the local manifold structure. Each data point
is represented as a linear combination of the
neighbouring data points and the combination
coefficients are specified in the weight matrix. Then, the
optimal embedding is sought.
Basically, the computation of LLE algorithm can be
divided into three main steps [16]:
1. Computation of the neighbours of each data point
Γ i .
2.
3.

{ }

Computation of the weights W = Wij

W jη j
∑
j =1

=
=

k

W j ( x −η j )
∑
j =1
k

2

(2

k

W jWi C ji
∑∑
j =1 i =1

)
With a local covariance matrix Cji = (x - ηj)(x – ηi ). To
minimize the error under the constraint that the rows of
W sum to one, a Lagrange multiplier can be used.
k k
⎛ k
⎞
L(W , λ ) =
W jWi C ji + λ ⎜ W j − 1⎟
(3
⎜⎜ j =1
⎟⎟
j =1 i=1
⎝
⎠
)
where λ ≠0. Roweis and Saul recommended solving the

∑∑

that best

∑

∑ j C jiWi = 1 and rescaling the weights

reconstruct each data point Γ i from its neighbours.

linear equation

Computation of the embedding coordinates Yi .

so that they sum to one, which yields the same result.
However, the matrix C might be singular, especially
when k > m or if k << d. To overcome this problem, a
small multiple can be added to C:
(4)
C := C + rI
where r is a small regularization parameter that will have
only a negligible effect on the results.

Let { Γ i ∈ ℜd | i=1,…, m} represents the input data
as d-dimensional points in an Euclidean vector space and
the output embedding vectors are { Yi ∈ ℜt | i=1,…, m},
where t  d .

Step 3: Embedding Coordinates Computation
In this step, a low-dimensional vector Yi is sought by
minimizing the following cost function:

Step 1: Neighbourhood Computation
Calculate the Euclidean distances between all the points,
Γ i , and then choose the k nearest neighbours for each
data point.

Φ(Y ) =

Step 2: Linear Weights Reconstruction
Calculate a weight matrix, W which contains the best
linear approximation of each data point xi from its k
nearest neighbours by minimizing the following cost
function:

Φ(W ) =

m

∑
i =1

xi

−

k

Wij x j
∑
j =1

m

m

2

Yi − ∑WijY j
∑
i =1
j =1

(5)

The minimum value here is invariant under the rotations
and translations of the image points. So, all coordinates
m

are required to be shifted to the origin, i.e.

2

are

(1)

m

constrained

to

have

unit

Yi =0 and
∑
i=1

covariance,

i.e.

1
Y Y T = I . We can rewrite equation ( 5 ):
m i =1 i i

∑

k

st.

2

k

Wij =1
∑
j =1

Φ(Y ) = Y T MY

(6

)

Note that each data point xi is reconstructed only from its
neighbours, i.e. Wij = 0, if xi is not a neighbour of xi. In
other words, every weight matrix Wij contains only at
most k non-zero elements and the W is sparse.
Consider a specific x with k nearest neighbours ηj
and the reconstruction weights Wj, equation ( 1 ) can be
rewritten in the following form:

T

where M = ( I − W ) ( I − W ) . Equation (6) can be
minimized by solving the eigen-decomposition of M.
The eigenvectors corresponding to the 2nd to (M+1)st
smallest eigenvalues form the embedding Y. The
corresponding eigenvector with zero eigenvalue is
discarded since it is representing a free translation mode.
Step 4: Mapping a Test Data Point onto the Embedding
Vectors

 test are selected
k nearest neighbours of the test data Γ
among the training data points. Then, the linear weights
Wtest

224

j

 test from its jth neighbours
that best reconstruct Γ

are

computed.

These

linear

constrained to the sum-to-one,

weights Wtest

∑W
j

test j

j

from the same-class (the class that data point A belongs
to) or from the between-classes (the other classes that
data point A does not belong to), are selected as the
nearest neighbours of data point A. This neighbourhood
is further processed for weight matrix and embedding
vector computations. The limitation of this
neighbourhood selection is that it is unsupervised. On the
other hand, in NDLLE, the same-class samples of data
point A are pre-defined in the neighbourhood of data
point A during training stage. The basic idea of the
neighbourhood selection process in NDLLE is shown in
Figure 2. Assume that there are h same-class samples of
data point A, h < k . Then, the balance, k ' = k − h , is the
k’ minimum-distance samples from the different-classes.
With this, we can ensure that the algorithm can get local
information from the same-class samples for local
geometry learning.

are

= 1 . Finally,

the embedded test data, ϒ test , is computed via

ϒ test = ∑ Wtest jY j , where the sum is over the
j

 test [5].
outputs corresponding to the all neighbours of Γ
3. NDLLE Framework
Basically, NDLLE is a supervised version of LLE.
NDLLE is supervised in neighbourhood selection
process during training stage; but LLE is unsupervised.
In other words, the difference between NDLLE and LLE
is the neighbourhood selection process during training
stage. Figure 1 illustrates the neighbourhood selection
process of data point A in LLE. Firstly, the Euclidean
distances between all data samples and data point A are
calculated. Then the k minimum-distance samples, either

k minimum-distance
samples

1.
2.
.
.
.
k.

Same-class
samples

Between-class
samples

Neighbourhood of data
point A

Figure 1 The basic idea of neighbourhood selection process in LLE during training phase

k’ minimumdistance samples

1.
2.
.
.
.
k.

h same-class
samples

Between-class
samples

Neighbourhood of data
point A

Figure 2 The basic idea of neighbourhood selection process in NDLLE during training phase

225

4.

addressed among the proposed NDLLE, LLE, SLLE and
PCA.

Data Analysis in LLE and NDLLE

Here, we also illustrate the embedding robustness of
NDLLE and compare it with the embedding of the
original LLE in Figure 3. From Figure 3 (a) and (b), we
observe that there is a better separation between the class
structures in NDLLE embedding, compared with those in
LLE embedding. These two embeddings are generated
using small neighbourhood size, which is k=6.
PIE_LLEk6

5.1. Experimental results on ORL
The ORL database contains 400 images from 40 classes
of users. The images have different variations including
facial expression (open or close eyes, smiling or nonsmiling) and facial details (glasses or without glasses).
They were captured with a tolerance for some tilting and
rotation of the face up to about 20 degrees. We conduct
experiment with 4-fold cross-validation strategy. In the
first-fold test, the first five samples per person are used
for training and the rest are used for testing. In the
second-fold test, the training set is formed by the last five
samples per person and the first five samples per person
are used for testing. On the other hand, in the third-fold
test, the odd numbered images of each class are as
training images, while the even numbered images are as
testing images. In the forth-fold test, it is another way
round, in which the even numbered images are in
training set and the odd numbered images are in testing
set. The recognition error is the average error rate (AER)
obtained in four tests.
For LLE and NDLLE methods, small
neighbourhood size, k=6 and large neighbourhood size,
k=60, are tested. Figure 4 shows the plots of the AER
curves versus different feature dimensions. The
performances of LLE and NDLLE are comparable, in
which LLE obtains 10.76% and NDLLE obtains 9.23%
error rates. However, NDLLE takes advantage of simpler
computation due to smaller neighbourhood, k=6, is used.
Table 1 shows the error rates of PCA, LLE, SLLE and
NDLLE. Our algorithm shows the best result with error
rate 9.23%.

class A
class B

2
1.5
1
0.5
0
-2

-1

-0.5

0

1

2

-1
-1.5
-2

(a)
PIE_NDLLEk6

class A
class B

2
1.5
1
0.5
0
-2

-1

-0.5

0

1

2

3

-1
-1.5
-2

(b)
Figure 3 (a) LLE embedding with k=6, and (b) NDLLE
embedding with k=6

5.

Experimental Results and Discussions

Figure 4 Error rate plots of LLE and NDLLE with
different feature dimensions on ORL database

The performance of the proposed method, NDLLE,
is evaluated using Olivetti Research Laboratory Database
(ORL) [6], CMU Pose, Illumination, and Expression
(PIE) database [18] and Face Recognition Grand
Challenge Database (FRGC) [13]. Each database is
partitioned into training and testing sets. Both training
and testing sets contain all user classes, but there is no
same sample image between the training and testing sets.
In this study, a simple Euclidean distance based classifier
is adopted. Besides, the performance comparison is

Table 1 Error rates of PCA, LLE, SLLE and NDLLE on
ORL database
Dimension, d
AER (%)
Method
PCA
50
16.93
LLE
50
10.76
SLLE
50
10.50
NDLLE
50
9.23

226

better recognition performance. Table 3 shows the best
results of each method with the corresponding feature
dimensions. The error rates for PCA, LLE, SLLE and
NDLLE are 40.36%, 35.50%, 30.26% and 28.06%.

5.2. Experimental results on PIE
The PIE database contains 68 subjects with 41,368 face
images as a whole. The images were captured by 13
synchronized cameras and 21 flashes under varying pose,
illumination and expression. Here, we use 10 images per
subjects for the experiment. These images have the same
pose but with significant illumination variations. The
experiment is carried out using 4-fold cross-validation
strategy, just like the previous experiment. Figure 5
shows the plots of the AER curves versus different
feature dimensions. Since the PIE face images contain
many nonlinear components due to illumination
variations, larger neighbourhood, k=60, is considered in
both LLE and NDLLE for better recognition
performance. Besides, the best results of each method
with the corresponding feature dimensions are listed in
Table 2. The error rates for PCA, LLE, SLLE and
NDLLE are 37.97%, 25.75%, 16.03% and 14.81%.

Figure 6 Error rate plots of basline, NPE and sNPE with
different feature dimensions on FRGC database
Table 3 Error rates of PCA, LLE, SLLE and NDLLE on
FRGC database
Dimension, d
AER (%)
Method
PCA
100
40.36
LLE
100
35.50
SLLE
100
30.26
NDLLE
100
28.06

5.4. Discussions
Figure 5 Error rate plots of baseline, NPE and sNPE with
different feature dimensions on PIE database

We summarize the experimental results as below:
1.

Table 2 Error rates of PCA, LLE, SLLE and NDLLE on
PIE database
Dimension, d
AER (%)
Method
PCA
100
37.97
LLE
50
25.75
SLLE
50
16.03
NDLLE
100
14.81

5.3. Experimental results on FRGC

2.

In this database, a subject contains controlled and
uncontrolled still images. The controlled images were
taken in a studio with controlled environment; while the
uncontrolled images were taken at the outdoors, hallways
or atria, with varying “natural” illumination conditions.
These images contain two expressions: smiling and
neutral. A subset of this database with 156 subject
classes is used for experiment. Like the previous
experiments, 4-fold cross-validation strategy is carried
out on this face data. The plots of the AER curves versus
different feature dimensions are demonstrated in Figure
6. Similar to PIE face images, the images of the FRGC
also contain significant intra-class variations, thus LLE
and NDLLE with larger neighbourhood, k=60, achieve

3.

4.

227

The proposed NDLLE approach consistently
outperforms LLE in the all experiments on three
different face databases, which are ORL, PIE and
FRGC. As shown in the analysis in section 4,
NDLLE has better class separability than LLE.
Hence, the proposed algorithm is expected to be
superior to LLE in face recognition. This has been
illustrated through the observation to the
performance comparison between NDLLE and LLE
in face verification.
NDLLE approach also achieves better performance
than PCA and SLLE methods in the databases. This
shows that our proposed method is able to extract
more discriminative features than the both methods.
Both LLE and NDLLE with larger neighbourhood
attain better performance than these with smaller
neighbourhood in PIE and FRGC databases. The
face images from these databases contain
significant intra-class variations, such as
illumination, pose and facial expression. Thus,
larger neighbourhood is considered in computing
the embeddings of LLE and NDLLE without lost of
generalization.
The lower dimensionality of the face subspace
obtained in our experiments, either PCA-, LLE-,
SLEE- or NDLLE- subspaces, shows that

dimensionality reduction step is a very important
process to face recognition.
Workshop on Applications of Computer Vision, Sarasota FL.
1994.
[7] H. Murase and S.K. Nayar. Visual Learning and
Recognition of 3-D Objects from Appearance. International
Journal of Computer Vision, (14), 5-24. 1995.
[8] Jian Yang , Alejandro F. Frangi , Jing-yu Yang , David
Zhang , Zhong Jin. KPCA Plus LDA: A Complete Kernel
Fisher Discriminant Framework for Feature Extraction and
Recognition. IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 27, no.2, 230-244. February 2005.
[9] J. Wang, C.S. Zhang, Z.B. Kou. An Analytical Mapping for
LLE and Its Application in Multi-pose Face Synthesis. In Proc.
14th British Machine Vision Conference. 2003.
[10] Kwang In Kim; Keechul Jung; Hang Joon Kim. Face
Recognition using Kernel Principal Component Analysis.
Signal Processing Letters. IEEE, Volume 9, Issue 2, 40-42. Feb
2002.
[11] M.H. Yang. Kernel Eigenfaces vs. Kernel
Fisherfaces: Face Recognition Using Kernel Methods. In Proc.
of the Fifth IEEE International Conference on Automatic Face
and Gesture Recognition. 215. 2002.
[12] M. Turk, A. Pentland. Eigenfaces for recognition. J.
Cognitive Neuroscience, 3(1). 71-86. 1991.
[13] P.J. Phillips, P.J. Flynn, T. Scruggs, K.W. Bowyer, Jin
Chang; K. Hoffman, J. Marques, Jaesik Min; W. Worek,.
Overview of the Face Recognition Grand Challenge. In Proc of
Computer Vision and Pattern Recognition, 2005. CVPR 2005.
IEEE Computer Society, 947-954. 2005.
[14] P.N. Belhumeur, J.P. Hespanha, D.J. Kriegman.
Eigenfaces vs. Fisherfaces: Recognition using Class Specific
Linear. IEEE Transactions on Pattern Analysis and Machine
Intelligence. (19), 711-720, 1997.
[15] R.B. Francis and I.J. Michael. Kernel Independent
Component Analysis. The Journal of Machine Learning
Research. Volume 3, 1-48. March 2003.
[16] S. Roweis, L.K. Saul, Nonlinear Dimensionality
Reduction by Locally Linear Embedding. Science, 290(5500),
2323--2326. 2000.
[17] T. Martiriggiano , M. Leo , T. D'Orazio , A. Distante. Face
Recognition by Kernel Independent Component Analysis. In
Proc. of the 18th international conference on Innovations in
Applied Artificial Intelligence, .55-58. 2005.
[18] T. Sim, S. Baker, and M. Bsat. The CMU Pose,
Illumination, and Expression Database. IEEE Transactions on
Pattern Analysis and Machine Intelligence, Vol. 25, No. 12,
1615 – 1618. 2003.

Conclusions
Locally linear embedding (LLE) is developed based on
the assumption that face images from the same subject
class are distributed on a single manifold; hence the
process of neighborhood selection is non class-specific.
However, this is inappropriate to face recognition as face
recognition learns in multiple manifolds, in which each is
contributed by the training images of each subject. Thus,
we have proposed a revised version of locally linear
embedding (LLE) for face recognition by introducing
prior class label information during neighbourhood
selection at training stage. This proposed technique is
namely Neighbourhood Discriminant LLE (NDLLE).
During the neighbourhood selection, the same-class
samples of each data point are pre-defined in the
neighbourhood of each data point. So that, the algorithm
can get local information from the same-class samples
for local geometry learning. Hence, NDLLE achieves
better recognition, as shown in our experimental results.

References
[1] A. Hadid, O. Kouropteva, M. Pietikainen. Unsupervised
Learning Using Locally Linear Embedding: Experiments with
Face Pose Analysis. In Proc. the 16th International Conference
on Pattern Recognition. 111-114. 2002.
[2] B. Joshua, Tenenbaum, J. Langford. A Global Geometric
Framework for Nonlinear Dimensionality Reduction. Science,
vol. 290, no. 5500. 2319-2323. 2000.
[3] C.S. Zhang, J. Wang, N.Y. Zhao, D. Zhang. Reconstruction
and Analysis of Multi-pose Face Images based on Nonlinear
Dimensionality Reduction. Pattern Recognition Journal 37.
325-336. 2004.
[4] D. de Ridder and R.P.W. Duin. Locally Linear Embedding
for Classification. Technical Report PH-2002-01, Pattern
Recognition Group, Dept. of Imaging Science & Technology,
Delft University of Technology, Delft, The Netherlands, 2002.
[5] D. Liang, J. Yang, Z.L. Zheng, T.C. Chang. A Facial
Expression Recognition System based on Supervised Locally
Linear Embedding. Pattern Recognition Letters 26. 2374-2389.
2005.
[6] F. Samaria and A. Harter. Parameterisation of a Stochastic
Model for Human Face Identification. In Proc of 2nd IEEE

228

