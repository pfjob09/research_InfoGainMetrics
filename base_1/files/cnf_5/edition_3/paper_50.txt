Fifth International Conference on Computer Graphics, Imaging and Visualization
Visualisation

LEARNING IN THE RECURRENT HOPFIELD NETWORK
Saratha Sathasivam
School of Mathematical Sciences, Universiti Sains Malaysia, Penang, Malaysia
saratha@cs.usm.my

Abstract
propositional logic formulas and energy
functions of symmetric neural networks. Both
methods are applicable in finding whether the
solutions obtained are models for a
corresponding logic program.

There are two ways to calculate synaptic
weights for neurons in logic programming.
There are by using Hebbian learning or by
Wan Abdullah‚Äôs method. Hebbian learning for
governing events corresponding to some
respective program clauses is equivalent with
learning using Wan Abdullah‚Äôs method for the
same respective program clauses. We will
evaluate experimentally the logical equivalent
between these two types of learning (Wan
Abdullah‚Äôs method and Hebbian learning) for
the same respective clauses (same underlying
logical rules) in this paper. The computer
simulation that had been carried out support
this theory.

Previously Wan Abdullah has shown on
see how Hebbian learning in an environment
with some underlying logical rules governing
events is equivalent to hardwiring the network
with these rules [5]. In this paper, we will
evaluate experimentally by carrying out
computer simulations to support this.
This paper is organized as follows. In
section 2, we give the outline of doing logic
programming in Hopfield model and in section
3; Hebbian learning of logical clauses is
described. In section 4, we discuss the
proposed approach for comparing connection
strengths obtained by Wan Abdullah‚Äôs method
and Hebbian learning. Meanwhile, section 5
contains discussion regarding the results
obtained from computer simulations. Finally
concluding remarks regarding this work
occupy the last section.

Keywords: logic programming, Hebbian
learning, Wan Abdullah‚Äôs method, program
clauses.

1. Introduction
Recurrent single field neural networks are
essentially dynamical systems that feed back
signals to themselves. Popularized by John
Hopfield, these models possess a rich class of
dynamics characterized by the existence of
several stable states each with its own basin of
attraction. The Little-Hopfield neural network
[1,2] minimizes a Lyapunov function, also
known as the energy function due to obvious
similarities with a physical spin network. Thus,
it is useful as a content addressable memory or
an analog computer for solving combinatorialtype optimization problems because it always
evolves in the direction that leads to lower
network energy. This implies that if a
combinatorial optimization problem can be
formulated as minimizing the network energy,
then the network can be used to find optimal
(or suboptimal) solution by letting the network
evolve freely.

2. Neuro-symbolic Integration
The Hopfield dynamics is asynchronous,
with each neuron updating their state
deterministically. The system consists of N
formal neurons, each of which is described by
an Ising variable
Si (t ), (i 1,2,....N ) [6].
Neurons then are bipolar, S i ¬è { -1,1 }, obeying
the dynamics S i o sgn(hi ) , where the
field, hi

j

all neurons N, J ij( 2) is the synaptic strength
from neuron j to neuron i, and  J i is the
threshold of neuron i.
Restricting the connections to be
diagonal, J ij( 2) J (ji2) , J ii( 2) 0 , allows one to

Gadi Pinkas [3] and Wan Abdullah [4]
defined a bi-directional mapping between

978-0-7695-3359-9/08 $25.00 ¬© 2008 IEEE
DOI 10.1109/CGIV.2008.14

¬¶ J ij(2)V j  J i(1) , i and j running over

write a Lyapunov or energy function [7],

323

1
(2)
(1)
 ¬¶ ¬¶ J ij Si S j  ¬¶ J i Si
2 i j
i

E

to be minimized. We can observe that the
minima value for E p is 0, and has otherwise

(1)

which decreases monotonically with the
dynamics.

value proportional to the number of unsatisfied
clauses. The cost function (5), when
programmed onto a third order neural network
yields:

The two-connection model can be
generalized to include higher order
connections. This modifies the ‚Äúfield‚Äù into

Table 1: Synaptic strengths for Aƒ∏B, C using
Wan Abdullah‚Äôs method

(3)
....  ¬¶¬¶ J ijk
S j S k  ¬¶ J ij( 2) S j  J i(1)

hi

j k

Synaptic
Strengths

(2)

3)
J [(ABC
]

j

where ‚Äú‚Ä¶..‚Äù denotes still higher orders, and an
energy function can be written as follows:
1
(3)
E .....  ¬¶ ¬¶ ¬¶ J Si S j Sk
3 i j k ijk
(3)
1
(2)
(1)
 ¬¶ ¬¶ J ij Si S j  ¬¶ J i Si
2 i j
i

(3)
provided that J ijk

Clause
Aƒ∏B,C
1/16

2)
J [(AB
]

1/8

2)
J [(AC
]

1/8

2)
J [(BC
]

-1/8

J [(A1)]

1/8

J [(B1)]

-1/8

J [C(1)]

-1/8

3)
J[(ijk
] for i, j, k distinct,

with [‚Ä¶] denoting permutations in cyclic
(3)
0 for any i, j, k equal, and
order, and J ijk

We addressed this method of doing logic
programming in neural network as Wan
Abdullah‚Äôs method.

that similar symmetry requirements are
satisfied for higher order connections. The
updating rule maintains

3. Logical Learning

(4)
In logic programming, a set of Horn
clauses which are logic clauses of the form
A m B1, B2 ,.., BN where the arrow may be
read ‚Äúif‚Äù and the commas ‚Äúand‚Äù, is given and
the aim is to find the set(s) of interpretation
(i.e., truth values for the atoms in the clauses
which satisfy the clauses (which yields all the
clauses true). In other words, we want to find
‚Äòmodels‚Äô corresponding to the given logic
program.

The Hebbian learning for a two-neuron
synaptic connection can be written as
( n)
'J ij
O (2Vi  1)(2V j  1)
(6)

As an example, consider the following
logic program, A m B, C , whose three clauses
translate as A ¬õ ¬ô( B ¬ö C ) A ¬õ ¬ôB ¬õ ¬ôC . The
underlying task of the program is to look for
interpretations of the atoms, in this case A, B
and C which are model for the given logic
program. This can be seen as a combinatorial
optimization
problem
where
the
‚Äúinconsistency‚Äù,
1
1
1
E
(1  S ) (1  S ) (1  S ) (5)

This gives the changes in synaptic
strengths depending on the activities of the
neurons. In an environment where selective
events occurred, Hebbian learning will reflect
the occurrences of the events. So, if the
frequency of the events is deducted by some
underlying logical rule, logic should be
entrenched in the synaptic weights.
Wan Abdullah [5] has shown that the
synaptic strengths obtained using Wan
Abdullah‚Äôs method is similar with Hebbian
learning provided that equation (9) is true.

S i (t  1)

p

2

sgn[ hi (t )]

A

2

B

2

(or, for bipolar neurons, 'J ij

O Si S j ), where

O is a learning rate. For connections of other
orders, we can generalize this to
( n)
'J ij....m O (2Vi  1)(2V j  1)..
(7)
..(2Vn  1)

c

where S A , etc. represent the truth values ( true
as 1) of A, etc., is chosen as the cost function

324

O( n )

1
(n  1)!

We run the relaxation for 1000 trials and
100 combinations of neurons so as to reduce
statistical error. The selected tolerance value is
0.001. All these values are obtained by try and
error technique, where we tried several values
as tolerance values, and selected the value
which gives better performance than other
values. To compare the information obtain in
the synaptic strength, we make comparison
between the stable states (states in which no
neuron changes its value anymore) obtained by
Wan Abdullah‚Äôs method with stable states
obtained by Hebbian learning.
The way we calculated the percentage of
solutions reaching the global solutions is by
comparing the energy for the stable states
obtained by using Hebbian learning and Wan
Abdullah‚Äôs method. If the corresponding
energy for both learning is same, then we
conclude that the stable states for both learning
are the same. This indicates, the model (set of
interpretations) obtained for both learning are
similar. In all this, we assume that the global
solutions for both networks are the same due to
both methods considering the same knowledge
base (clauses).

(9)

We do not provide a detail review
regarding Hebbian learning of logical clauses
in this paper, but instead refer the interested
reader to Wan Abdullah‚Äôs paper [5].

4. Hebbian Learning
Abdullah‚Äôs Method

vs.

Wan

In the previous section, we have
concluded that synaptic weights for neurons
can be calculated either by using Hebbian
learning or by Wan Abdullah‚Äôs method.
Theoretically, information (synaptic strengths)
produce by both methods are similar.
However, due to interference effects and
redundancies [8] synaptic strengths could be
different, but the set of solutions for both cases
will remain the same. Due to this, we cannot
use direct comparison. So, we introduce a way
for making this comparison.
The following algorithm shows how a
logic program can be done in a Hopfield
network based on Wan Abdullah‚Äôs method:
i)
Given a logic program, translate all
the clauses in the logic program into basic
Boolean algebraic form.
ii)
Identify a neuron to each ground
neuron.
iii) Initialize all connections strengths to
zero.
iv)
Derive a cost function that is
associated with the negation of all the clauses,
1
such that
(1  S x ) represents the logical
2
value of a neuron X, where S x is the neuron

5.0 Results and Discussion
Figures 1 - 6 illustrate the graphs for
global minima ratio (ratio= (Number of global
solutions)/ (Number of solutions=number of
runs)) and Hamming distances from computer
simulation that we have carried out.
Meanwhile Figure 7-12 illustrate the graphs
for global minima ratio and Hamming
distances from computer simulation for Wan
Abdullah‚Äôs method. From the graphs obtained,
we observed that the ratio of global solutions is
consistently 1 for all the cases, although we
increased the network complexity by
increasing the number of neurons (NN) and
number of literals per clause (NC1, NC2,
NC3). Due to we are getting similar results for
all the trials, to avoid graphs overlapping, we
only presented the result obtained for the
number of neurons (NN) = 40.

corresponding to X. The value of S x is define
in such a way that it carries the values of 1 if X
is true and -1 if X is false. Negation (neuron X
1
does not occur) is represented by (1  S x ) ; a
2
conjunction logical connective is represented
by multiplication whereas a disjunction
connective is represented by addition.
v) Obtain the values of connection strengths
by comparing the cost function with the
energy.
vi) Let the neural networks evolve until
minimum energy is reached. Checked whether
the solution obtained is a global solution (the
interpretation obtained is a model for the given
logic program).

Besides that, error bar for some of the
cases could not be plotted because the size of
the point is bigger than the error bar. This
indicates that the statistical error for the
corresponding point is so small. So, we
couldn‚Äôt plot the error bar.
Most of the neurons which are not
involved in the clauses generated will be in the
global states. The random generated program
clause relaxed to the final states, which seem

325

Since all the solutions we obtained are
global solution, so the distance between the
stable states and the attractors are zero.
Supporting this, we obtained zero values for
Hamming distance. This indicates the stable
states for both learning are the same. Therefore
they are no different in the energy value. So,
models for both learning are proved to be
similar. Although the way of calculating
synaptic weights are different, since the
calculations revolve around the same
knowledge base (clauses), the set of
interpretations will be similar. This implies
that, Hebbian learning could extract the
underlying logical rules in a given set of events
and provide good solutions as well as Wan
Abdullah‚Äôs method. That‚Äôs way we obtained
identical results for the both methods. The
computer simulation results support this
hypothesis.

[6] Geszti Tamas. (1990). Physical Models of
Neural Networks. Singapore: World Scientific
Publication.
[7] Haykin, S. (2008). Neural Networks and

Learning Machine. Prentice Hall, New Jersey.
[8] Sathasivam, S. (2006). Logic Mining in Neural
Networks. PhD Thesis. University of Malaya,
Malaysia.

8. Figures
Global Minima For NC1
2
1.5
Ratio

also to be stable states, in less than five runs.
Furthermore, the network never gets stuck in
any suboptimal solutions. This indicates good
solutions (global states) can be found in linear
time or less with less complexity.

NN=40

1
0.5
0
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

NC1/NN

Figure 1: Global Minima Ratio for NC1
Global Minima For NC2

6. Conclusion
2

In this paper, we had evaluated
experimentally the logical equivalent between
these two types of learning (Wan Abdullah‚Äôs
method and Hebbian learning) for the same
respective clauses (same underlying logical
rules) using computer simulation. The results
support Wan Abdullah‚Äôs earlier proposed
theory [5].

R atio

1.5
NN=40

1
0.5
0
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

NC2/NN

7. Acknowledgement
Figure 2: Global Minima Ratio for NC2

This research is partly financed by FRGS
grant (203/PMATHS/671185) from the
Ministry of Higher Education, Malaysia.

Global Minima For NC3
2

8. References

1.5
R atio

[1] J.J. Hopfield. (1982). ‚ÄúNeural Networks and
Physical Systems with Emergent Collective
Computational abilities‚Äù, Proc. Natl. Acad. Sci.
USA, 79, 2554-2558.
[2] Little, W.A. (1974). Math. Biosci. 19, 101-120.
[3] Pinkas, G. (1991). Energy minimization and the
satisfiability of propositional calculus. Neural
Computation, 3, pp 282-291.
[4] W.A.T. Wan Abdullah. (1992). ‚ÄúLogic
programming on a neural network‚Äù. Int. J.
Intelligent Sys. 7. 513-519.
[5] Wan Abdullah, W.A.T. (1991). Neural Network
logic. Proc of the workshop held in Elba
International Physics Center, Italy, pp 135-142.

NN=40

1
0.5
0
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

NC3/NN

Figure 3: Global Minima Ratio for NC3

326

Global Minima For NC1

1

2

0.5

1.5
R atio

Ratio

Hamming Distance For NC1

NN=40

0
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

NN=40

1
0.5

-0.5

0
0

-1

Figure 4: Hamming Distance for NC1

2

0.5

1.5
R atio

Ratio

Global Minima For NC2

1

NN=40

0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

Figure 7: Global Minima Ratio for NC1

Hamming Distance For NC2

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
NC1/NN

NC1/NN

1

NN=40

1
0.5

-0.5

0
0

-1

Figure 5: Hamming Distance for NC2

2

0.5

1.5
R atio

Ratio

Global Minima For NC3

1

NN=40

0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

Figure 8: Global Minima Ratio for NC2

Hamming Distance For NC3

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
NC2/NN

NC2/NN

1

NN=40

1
0.5

-0.5

0
0

-1

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

NC3/NN

NC3/NN

Figure 6: Hamming Distance for NC3

Figure 9: Global Minima Ratio for NC3

327

Hamming Distance For NC3

1

1

0.5

0.5
Ratio

Ratio

Hamming Distance For NC1

NN=40

0
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

-0.5

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

-0.5

-1

-1
NC1/NN

NC3/NN

Figure 10: Hamming Distance for NC1

Figure 12: Hamming Distance for NC3

Hamming Distance For NC2
1
0.5
Ratio

NN=40

0

NN=40

0
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

1

-0.5
-1
NC2/NN

Figure 11: Hamming Distance for NC2

328

