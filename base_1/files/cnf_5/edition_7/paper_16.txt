A Review on Level of Detail
Tan Kim Heok
Daut Daman
tkimheok@hotmail.com
daut@fsksm.utm.my
Department of Graphics and Multimedia, Universiti Teknologi Malaysia
UTM Skudai, 81300 Johor, Malaysia.
Abstract
Recent advances in modern 3D scanning
technology and the increasingly size of computer
simulations have led to a rapid increase in the
availability and size of geometry data sets. Gigantic
polygonal data sets, consisting of hundreds of millions
of faces, are becoming quite common. While the
performance of graphics hardware also seen a drastic
rise in these years, however, ability to produce
enormous data sets overload the capabilities of the
state-of-the-art graphics chip. Therefore, automatic
model simplification and run-time level of detail (LOD)
management techniques are introduced. In this paper,
we present a review on level of detail techniques in an
organized structure. Level of detail framework, level of
detail management, level of detail simplification models
and metrics for simplification and error evaluation are
discussed in sequence. Finally, some current issues and
conclusion will be given.

1. Introduction
A 3D interactive application is an extremely
computational demanding paradigm, requiring the
simulation and display of a virtual environment (VE) at
interactive frame rates. Even with the use of powerful
graphics workstations, a complex VE can involve a vast
amount of computation, inducing a noticeable lag into
the system. This lag can severely compromise the
display quality.
Therefore, a lot of techniques have been proposed
to overcome the delay of the display. It includes motion
prediction, fixed update rate, visibility culling, frameless
rendering, Galilean antialiasing, level of detail, world
subdivision or even employing parallelism [25]. Level
of detail is certainly a great way in resolving this
problem.

Since the mid nineteen-seventies, programmers
have used Level of Detail (LOD) techniques to improve
the performance and quality of their graphics systems.
Numerous benefits can be obtained from the
simplification of models, including reduced storage
requirements, the reduction of computational complexity
for scene rendering and more rapid transmission over
network,.
The LOD approach involves maintaining a set of
representations of each polygonal object, each with
varying levels of triangle resolution. During the
execution of the animation, objects deemed to be less
important are displayed with a low-resolution
representation. Where as object of higher importance is
displayed with higher level of resolution (Figure 1).

Figure 1 Three levels of detail of the Ludwig
model. Left, 4998 faces; center, 1250; right 250
faces. [15]
In Section 2, level of detail framework will be
discussed. Following it, Section 3 will show the level of
detail management implemented so far. Then, researches
on simplifying the object will be explained in Section 4.
Next, the error metric calculation methods will be
revealed in Section 5. The current issues will be covered
in Section 6 to discuss some up to date issue in LOD
field. Finally, this work will be summarized.

2. Level of Detail Framework
Currently, there are four different kinds of LOD
frameworks, which are discrete LOD, continuous LOD,
view-dependent LOD and hierarchical LOD.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

2.1. Discrete Level of Detail

2.4. Hierarchical Level of Detail

Discrete level of detail, the traditional approach
creates LOD for each of the object separately in a preprocess. At run-time, it picks each object’s LOD
according to the particular selection criterions.
Therefore, we call this discrete LOD.
The most significant advantage of discrete LOD is
the simplest programming model. Secondly, it fits the
modern graphics hardware well. It is easy to compile
each level of detail into triangle strips, display list,
vertex array and so on. The rendering process is much
faster than unorganized triangles on today’s hardware.
Even the implementation of discrete LOD is
simpler; however, it is not suite for drastic simplification
and not scale well to large object.

View-dependent LOD solves the problem with
large objects. However, it still faces difficulties in
displaying small objects. Hence, hierarchy LOD was
created to solve the problem with small objects. It
merges objects into assemblies. At sufficient distances,
simplify assemblies, but not individual objects.
Hierarchical LOD dovetails nicely with viewdependent LOD. It treats the entire scene as a single
object to be simplified in view-dependent fashion.
Besides, hierarchy can also sit atop traditional discrete
LOD schemes. These discrete LOD will be group into a
hierarchy. It creates a better scalability for large
structured models.

3. Level of Detail Management
2.2. Continuous Level of Detail
Continuous LOD developed at 1976 is a departure
from the traditional discrete approach. Oppose to
discrete LOD, it creates data structure from which a
desired level of detail can be extracted at run time.
The level of detail of continuous LOD is specified
exactly, not chosen from a few pre-created options. Thus
objects use no more polygons than necessary, which free
up polygons for other objects. Therefore, it has a better
resource utilization and lead to better overall fidelity.
Smoother transitions can be created using
continuous LOD. It is because continuous LOD can
adjust detail gradually and incrementally, reducing
visual pops. We can even geomorph the fine-grained
simplification operations over several frames to
eliminate pops. Additionally, it supports progressive
transmission.

2.3. View-Dependent Level of Detail
View-dependent LOD uses current view parameters
to select best representation for the current view. A
single object may thus span several levels of detail. It is
a selective refinement of continuous LOD. It shows
nearby portions of object at higher resolution than
distant portions. Silhouette regions of object are showed
at higher resolution than interior regions do. Viewdependent also take into account the user peripheral
vision.
The advantages of view-dependent LOD include it
has a better granularity than continuous LOD. This is
because it allocates polygons where they are most
needed, within as well as among objects. On the other
hand, it enables drastic simplification of very large
objects.

Level of detail management is an important process
in choosing which level of detail to represent each
object. Traditionally, the system will assign each level
of detail a range of distances. Even though this method
is extremely simple, it doesn’t maintain constant frame
rate and the correct switching distance may vary with
field of view and resolution. A more sophisticated level
of detail management is enquired to enhance the LOD
selection.
a. Size
An object’s LOD is based upon its pixel size on the
display device. It can overcome the weakness of
distance selection criterion.
b. Eccentricity
An object’s LOD is based upon the degree to which
it exists in the periphery of the display. Without a
suitable eye tracking system, it is generally assumed that
the user will be looking towards the centre of the
display, and so non-perceived objects are degraded.
c. Velocity
An object’s LOD is based upon its velocity relative
to the user. Funkhouser and Sequin [1] acknowledge
that the quick moving objects may appear blurred, or
can be seen only for only a short period of time, and
hence the user may not be able to see them clearly.
However, visual importance-biased image synthesis
animation, which incorporating temporal changes into
the models [2]. This research indicated that motion is a
strong attractor of visual attention.
d. Fixed Frame Rate
An object’s LOD is modulated in order to achieve
and maintain a prescribed update rate. It is distinct from
others because it is concerned with computational
optimization rather than perceptual optimization. A
combination of discrete and continuous approach is

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

presented in time-critical rendering technique [3]. It
ensures guaranteed frame rates even for complicated
scenes. Therefore, it provides a convenient framework
for real-time rendering applications.
e. Human Eyes Limitation
Resolution of element depends upon the depth of
field focus of the user’s eyes, i.e. objects out with the
fusional area appear in lower detail. Besides, visual
disruptions, including eye saccade, flicker or blink are
eyes’ weakness. Saccade is a rapid reflex movement of
the eye to fixate a target onto the fovea. Human do not
appear to perceive detail during visual disruption occur.
Change Blindness [4], where portions of the scene
that have changed simultaneously with the visual
disruption go unnoticed to the viewer. Attention is
controlled entirely by slower, highly-level mechanisms
in the visual system, which searches the scene, object by
object, until attention finally focuses on the object that is
changing.
There are two major influences on human visual
attention: bottom-up and top-down processing. Bottomup processing is the automatic gaze direction for human
to lively or colourful objects. In contrast, top-down
processing is consciously directed attention to
predetermined goals or tasks. This technique
demonstrated the principle of Inattentional Blindness
[5], where portions of the scene unrelated to the
specified task go unnoticed.
f. Environment Conditions
Coarsen level of detail thresholds through use of
haze, smoke, fog, clouds, etc because these effects make
the scene blur and hard to perceive the actual detail.
g. Attention-Directed
Models of visual attention work out on where the
user is likely to be looking. Visual attention-based
technique allocates polygons to objects in a scene
according their visual importance [6]. The importance
value is generated by considering size, position, motion
and luminance.

4. Level of Detail Generation
Refinement and decimation are the most common
methodologies in surface simplification. Refinement
algorithm begins with an initial coarse approximation
and details are added at each step. Contrary to
refinement, decimation algorithm begins the original
surface and iteratively removes elements at each step.
Both refinement and decimation share a very important
characteristic: they seek to derive an approximation
through a transformation of some initial surface.
Decimation simplification can be separate into two
parts, whether it is polygonal simplification or nonpolygonal simplification. Non-polygonal simplification

includes, parametric spline surface simplification,
simplification of volumetric models and also
simplification of image based models. More works are
done in polygonal simplification due to its flexibility
and ubiquity. In fact, it is a common to convert other
model types into polygonal surfaces prior to processing.
Practically, all virtual environment systems employ
polygon renderers as their graphics engine.
Polygon simplification can be categorized into two
parts, one is geometric simplification and another one is
topology simplification. Geometric simplification
reduces the number of geometric primitives (vertices,
edges, triangles). Meanwhile, topology simplification
reduces the number of tunnels, holes and cavities.
Aggressive simplification is a combination of geometric
simplification and topology simplification.
Based on [7], polygon simplification can be
categorized as geometry removal (decimation), sampling
and adaptive subdivision (refinement). Sampling is an
algorithm that samples a model’s geometry and then
attempts to generate a simplified model that
approximate the sampled data.

4.1. Geometric Simplification
a.

Vertex Removal
Iteratively remove pieces of geometry perhaps is
one of the most natural approaches in simplification.
One such method is based on “pluck” vertices; each
time a vertex and its incident triangles are removed, a
hole is created, which must then be patched via
triangulation (see Figure 2). Edge swap near the
degenerated vertex prevents the folds in the mesh.
The vertex removal method for arbitrary meshes
was first introduced by Schroeder et al. [14]. The
decision whether to remove a vertex is based on the
distance between the vertex and the plane. Vertices in
flatter regions are preferred for removal. “Simplification
Envelopes” algorithm use vertex removal as the
coarsening operation [16].
b. Vertex Clustering
The original vertex clustering approach for
simplification is proposed by Rossignac and Borrel [8].
Geometrically this corresponds to building a uniform
grid of rectilinear cells, and then merging all vertices
within a cell, or cluster, and computes a representative
vertex to merge all triangles and edges. Hence, only a
single vertex remained after simplification process.
By optimizing the position of each cluster’s vertex,
the geometry can be improved. The “visual importance”
of each vertex is rated by a simple heuristics and it was
used to elect the representative vertex [8]. Subsequently,
a slight variation on this heuristic motivated by
introducing “floating cells” [9].

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

e.
Edge
swap

Vertex
Removal

Edge
split

Edge
collapse

Half-edge
collapse

Face Clustering
This dual of vertex clustering is less popular. This is
because the produced models generally exhibit relatively
poor geometric and visual quality. The idea behind this
approach is to merge nearly coplanar faces into large
clusters of faces. Kalvin and Taylor [10] refer to such
clusters as “superfaces.” The mesh is first partitioned
into clusters. The interior vertices in each cluster are
removed, and the cluster boundaries are simplified.
Lastly, the resulting non-planar superclusters are
triangulated. For others examples, see [11, 12, 13].

4.2. Topology Simplification
Figure 2 Local connectivity operations
c.

Edge Collapse
The edge collapse operation [17, 18] is a quite
popular coarsening operation. The two edge’s vertices
are contracted to a single vertex, thereby deleting the
edge and its incident triangles. The advantages are that
the position of the substitute vertex can be chosen freely,
can be optimized and no triangulation action is needed.
The general edge collapse algorithm involves two
decisions: first, where to place the substitute vertex; and
second, choosing the order of edges to collapse. In
general, this can be made implicitly by specifying an
error metric that depends on the position of the
substitute vertex. Besides, other factors are involved,
including topological constraints, geometric constraints,
handling of degenerate cases and so on.
Half-edge collapse generally results in lower quality
meshes than regular edge collapse since it allows no
freedom in optimizing the mesh geometry, but has the
advantage of having a more concise representation.
Triangle collapse is yet another possible coarsening
operation [19, 20]. Vertices of a triangle are merged to a
single new vertex. It provides little practical benefit.
A simple, fast and effective polygon reduction
algorithm based on edge collapse [21], which utilizes
the “minimal cost” method to calculate a set of LODs in
3D real time virtual environment. On the other hand,
parallel triangular mesh decimation with the edge
contraction can simplify object models in a short time
without sorting [22].
d. Vertex Pair Contraction
Vertex pair contraction is an even more flexible
than edge collapse in that it allows any pair of vertices
to be merged, whether they share an edge or not. In
order to limit the number of possible candidates for pair
contraction, only a subset of pairs, called virtual edges,
which are spatially close, are considered. [23] is the
most outstanding for its dynamic selection of virtual
edges, which allows increasingly larger gaps between
pieces of a model to be merged.

For models with a large number of connected
components and holes, it may be necessary to merge
geometrically close pieces into individual larger ones to
allow further coarsening [24]. Algorithms based on
vertex clustering and vertex pair contraction are by
nature topology modifying [26]. However, few of these
algorithms are rather the byproduct of these coarsening
operations. If preserving the manifoldness of a surface is
important, then the topology simplification is applicable.
However, manifoldness of the surface always less
importance, and vertex pair contraction and its
derivatives are adequate alternatives.

5. Metrics for Simplification and Quality
Evaluation
The simplified model is rarely identical to the
original, and therefore such a metric is needed to
measure how similar the two models are. If we are
interested in the visual quality of a model, then an image
metric may be more suitable. Such metrics have been
developed, for example, to measure the degree of
photorealism in computer generated images, search
image databases, guide algorithms for image generation,
and measure the image compression’s quality.

5.1. Geometry-Based Metrics
Metrics for simplification are commonly used for
two distinct purposes; evaluating the output quality, and
determining where and how to simplify a model. If done
correctly, a metric defined for simplification both
determines the position of new vertices and the order in
which coarsening operations are applied. However, it is
sometimes difficult to express exactly what the metric is
for a given simplification method.
The Hausdorff distance is probably the most wellknown metrics for making geometric comparisons
between two point sets. This metric is defined in terms
of another metric such as the Euclidean distance.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

Quadric error metrics is based on weighted sums of
squared distances [13]. The distances are measured with
respect to a collection of triangle planes associated with
each vertex. The beauty of the quadric metric is that it
can be evaluated very efficiently by representing any set
of planes as a single symmetric 4x4 matrix. This
technique is fast and good fidelity even for drastic
reduction. Besides, it is robust in handling non-manifold
surfaces. Aggregation in merging objects can also be
performed here.
Vertex-vertex distance measures the maximum
distance traveled by merging vertices. While vertexplane distance store set of planes with each vertex, then
errors are calculated based on distance from vertex to
plane. Similarly, vertex-surface distance is distance from
vertex to surface. It map point set to closest points to
simplified surface. Maximum distance between input
and simplified surfaces is used to measure surfacesurface distance.

5.2. Attribute Error Metrics
Similar to geometry error metrics, it can be
categorized into vertex-vertex distance, vertex-plane
distance, vertex-surface distance and surface distance.
Besides, it also include image-driven metric and
perceptually-based metric. Some image metrics are
rather simple and treat each image in a geometric sense.
Probably the most well-known metric for comparing
images is the Lp pixel-wise norm, and in particular the
d2 root mean square error. Perceptually-based metrics
use contrast sensitivity function to guide simplification.

As addition, graphics cards nowadays able to render
millions of triangles per second, hence, a truly scalable
system for high-quality 3D interactive rendering of
enormous data sets is wanted. Future work can also
focus on network-streamed simplification. It can
overcome the limit space in local hard disk problem.
Another popular topic is artist-assisted LOD, where the
user can control the degree of simplification based on
user input.

Conclusions
The paper presented most of the processes related to
level of detail. First, determine what kind of framework
to use. Next, LOD selection criteria will be chosen to
select one of the simplified models, which created from
simplification technique during run-time. Later on, error
metrics is calculated to measure the effectiveness and
quality of generated level of detail. Future works are
likely to focus more on out-of-core simplification and
huge data memory management in various applications.

Acknowledgements
This research work has been supported by IRPA
grant 04-02-06-0048EA240. Thousands of appreciation
dedicate to Dr. Peter Lindstrom and Dr. Martin Reddy
for valuable opinions on latest issue in level of detail
field.

References
[1]

Funkhouser, T. A. and Sequin, C. H. (1993). Adaptive
Display Algorithm for Interactive Frame Rates During
Visualization of Complex Virtual Environment.
Proceedings of the 20th annual conference on Computer
graphics and interactive techniques. 1993. New York,
USA: ACM Press, 247-254.

[2]

Brown, R., Pham, B. and Maeder, A. (2003a). Visual
Importance-biased
Image Synthesis
Animation.
Proceedings of the 1st international conference on
Computer graphics and interactive techniques in
Austalasia and South East Asia. 2003. New York, USA:
ACM Press, 63-70.

[3]

Zach, C, Mantler, S. and Karner, K. (2002). Timecritical Rendering of Discrete and Continuous Levels of
Detail. Proceedings of the ACM symposium on Virtual
reality software and technology. 2002. New York, USA:
ACM Press.

[4]

Cater, K., Chalmers, A. and Dalton, C. (2003). Varying
Rendering Fidelity by Exploiting Human Change
Blindness. Proceedings of the 1st international
conference on Computer graphics and interactive

6. Current Issues
Level of detail still has a lot of attentions in many
applications, especially in games and real-time
environments. In level of detail framework, we can see
the trends switched to view-dependent and hierarchical
level of detail instead of traditional methods. Besides,
human visual ability becomes an important issue in level
of detail management [25].
A newly invented library by Cohen et al. [27],
named GLOD is invented recently. It is a geometric
level of detail system integrated into OpenGL rendering
library. GLOD provides a low-level, lightweight API for
level of detail operations. Hence, it is useful to
simplified LOD developers’ works.
There’s been a big shift lately with out-of-core
simplification models [26]. Because of memory shortage
in dealing with meshes that are significantly larger than
available main memory, conventional simplification
methods, which typically require reading and storing the
entire model in main memory, cannot be used. In-core
simplification is mostly well implemented.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

techniques in Austalasia and South East Asia. 2003.
New York, USA: ACM Press, 39-46.

(WSCG 2001). February, 2001. Plzen: Czech Republic,
299-306.

[5]

Cater, K. (2002). Selective Quality Rendering by
Exploiting Inattentional Blindness: Looking but not
Seeing. Proceedings of the ACM symposium on Virtual
reality software and technology. 2002. New York, USA:
ACM Press, 17-24.

[16] Cohen, J., Varshney, A., Manocha, D., Turk, G., Weber,
H., Agarwal, P. Brooks, Jr. F. P. and Wright, W. (1996).
Simplification envelopes. In: Rishmeier, H. ed.
Proceedings of SIGGRAPH 96. New Orleans,
Louisianna: Addison Wesley. 119-128.

[6]

Brown, R., Cooper, L. and Pham, B. (2003b). Visual
Attention-Based Polygon Level of Detail Management.
Proceedings of the 1st international conference on
Computer graphics and interactive techniques in
Austalasia and South East Asia. 2003. New York, USA:
ACM Press, 55-62.

[17] Hoppe, H., DeRose, T., Duchamp, T., McDonald, J. and
Stuetzle, W. (1993). Mesh Optimization. In: Kajiya, J.
T. ed. Proceeding of SIGGRAPH 93. Anaheim,
California: Addison Wesley. 19-26.

[7]

[8]

[9]

Erikson, C. (1996). Polygonal Simplification: An
Overview. Department of Computer Science, University
of North Carolina, Chapel Hill, NC: UNC Technical
Report No. TR96-016.
Rossignac, J. and Borrel, P. (1993). Multi-resolution 3d
Approximations for Rendering Complex Scenes. In:
Falciendo, B. and Kunii, T. L. eds. Modeling in
Computer Graphics. Springer-Verlag. 455-465.
Low, K. L. and Tan T. S. (1997). Model Simplification
using vertex-clustering. In: Cohen, M. and Zeltzer, D.
eds. 1997 ACM Symposium on Interactive 3D Graphics.
Phode Island: ACM SIGGRAPH. 75-82.

[10] Kalvin, A.D. and Taylor, R. H. (1996). Superfaces:
Polygonal Mesh Simplification with Bounded Error.
IEEE Computer Graphics & Applications. 16(3):64-77.
[11] Hinker, P. and Hansen, C. (1993). Geometric
Optimization. In: Nielson, G. M. and Bergeron, D. eds.
IEEE Visualization ’93. San Jose, Carlifornia: IEEE
Computer Society Press. 189-195.
[12] Garland, M. (1999). Quadric–Based Polygonal Surface
Simplification. Carnegie Mellon University: Ph.D.
Thesis.
[13] Garland, M. and Heckbert, P. S. (1997). Surface
Simplification Using Quadric Error Metrics. In:
Whitted, T. ed. Proceedings of SIGGRAPH 97. Los
Angeles, California: ACM Press. 209-216.
[14] Schroeder, W. J., Zarge, J. A. And Lorensen W. E.
(1992). Decimation of Triangle Meshes. In: Catmull, E.
E. ed. Computer Graphics (Proceeding of SIGGRAPH
92). Chicago: Illinois. 65-70.
[15] Ribelles, J., López, A., Belmonte, O., Remolar, I. and
Chover, M. Variable Resolution Level-of-detail of
Multiresolution Ordered Meshes
Proc. of 9-th
International Conference in Central Europe on
Computer Graphics, Visualization and Computer Vision

[18] Hoppe, H. (1996). Progressive Mesh. In: Rushmeier, H.
ed. Proceeding of SIGGRAPH 96. Computer Graphics
Proceedings, Annual Conference Series. New Orleans,
Louisiana: Addison Wesley. 99-108.
[19] Hamann, B. (1994). A data Reduction Scheme for
Triangulated Surfaces. Computer Aided Geometric
Design. 11(20): 197-214.
[20] Gieng, T. S., Hamann, B. Joy, K. I., Schlussmann, G. L.
And Trotts, I. J. (1997). Smooth Hierarchical Surface
Traingulations, In: Yagel, R. and Hagen, H. eds. IEEE
Visualization ’97. Phoenix, Arizona: IEEE. 379-386.
[21] Wang, H. and Ruan, Q. (2000). Fast Rendering of
Complex Virtual Environment. Proceedings of
ICSP2000. 1395-1398.
[22] Franc, M. and Skala, V. (2001). Parallel Triangular
Mesh Decimation without Sorting. 17th Spring
Conference on Computer Graphics (SCCG '01). April
2001. Budmerice,Slovakia: IEEE, 22-29.
[23] Erikson, C. and Manocha, D. (1999). GAPS: General
and Automatic Polygonal Simplification. In: Hodgins, J.
and Foley, J. D. eds. ACM symposium on Interactive 3D
Graphics. Atlanta, Georgia: ACM SIGGRAPH. 79-88.
[24] Erikson, C. (2000). Hierarchical Levels of Detail to
Accelerate the Rendering of Large Static and Dynamic
Polygonal Environments. University of North Carolina
at Chapel Hill: Ph.D. Thesis.
[25] Reddy, M. (1997). Perceptually Modulated Level of
Detail for Virtual Environment. University of
Edinburgh: Ph.D Thesis.
[26] Lindstrom, P. (2000). Model Simplification using Image
and Geometry-Based Metrics. Georgia Institute of
Technology: Ph.D Thesis.
[27] Cohen, J., Luebke, D., Duca, N. and Schubert, B.
(2003). GLOD: Level of Detail for The Masses. The
Johns Hopkins University and the University of
Virginia: Technical Report.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

