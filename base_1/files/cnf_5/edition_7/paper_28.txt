Real-time Facial Expression Recognition in the Interactive Game Based on
Embedded Hidden Markov Model
Xiaoxu Zhou, Xiangsheng Huang, Yangsheng Wang
Institute of Automation, Chinese Academy of Sciences, Beijing, P.R.China, 100080
{xxzhou@nlpr.ia.ac.cn, xshuang@nlpr.ia.ac.cn, wys@nlpr.ia.ac.cn}
Abstract
Combine the facial expression of the game player with
the game role and the context can highly enhance the
interaction and attractiveness of the interactive computer
game. In this paper, we propose a novel framework to
real-time facial expression recognition in the interactive
computer game environment. There are two main
contributions of this work. First, an embedded HMM
proposed by Nefian et al. for face recognition [1] is used
in real-time facial expression recognition. Second, face
alignment is used in facial expression environment. In
this paper, the embedded HMM uses two-dimensional
Discrete Cosine Transform (2D-DCT) coefficients as the
observation vectors opposite to previous HMM
approaches which use pixel intensities to form the
observation vectors. Our proposed system reduces the
complexity of the training and recognition system.

1. Introduction
Interactive computer games are more popular than
before. A stronger interaction is game designer’s
continuous goal. Facial expressions play an important role
in communications between people. Research in facial
expression recognition can help people build more
intelligent and interactive system. If the facial expression
of the game player can be combined with the game role
and the context, the interaction and attractiveness will be
highly enhanced. To do this, the real-time facial
expression recognition is necessary.
In recent years, many computer vision researchers
have been working on automatic detection, tracking and
recognition of the whole or parts of the face. Wang et al.
use 19 points 2D feature tracker for the recognition of
three emotional expressions [2]. Sako and Smith use color
matching and template matching to find the positions of
important 2D features on the face and use the dimension
and position information about these features for
expression recognition [3]. Essa and Pentland developed a
system for observing dynamic facial motions using model

based optical flow method [4]. Their system is suitable
for coding, analysis and recognition of facial expressions.
The most significant facial features of a frontal face
image include the hair, forehead, eyes, nose and mouth.
These features occur in a natural order, from top to
bottom, even if the images undergo small rotations in the
image plane, and/or rotations in the plane perpendicular
to the image plane. Therefore, the image of a face may be
modeled using a one-dimensional HMM by assigning
each of these regions to a state. This one-dimensional
model was extended by Samaria [5] to a pseudo 2-D
HMM structure.
This paper proposes a novel framework to real-time
facial expression recognition in the interactive computer
game environment. There are two main contributions of
this work. First, an embedded HMM proposed by Nefian
et al. for face recognition [1] is used in real-time facial
expression recognition. Second, face alignment is used in
facial expression environment. In this paper, the
embedded HMM uses observation vectors that are
composed of two-dimensional Discrete Cosine Transform
(2D-DCT) coefficients opposite to previous HMM
approaches which use pixel intensities to form the
observation vectors. Unlike previous facial expression
recognition approach, our proposed system reduces the
complexity of the training and recognition system and
offers a more flexible framework and can be used in realtime applications such as the interactive computer game.
Experimental results demonstrate that the proposed
approach is an effective method to recognize facial
expression in the interactive computer game.
The rest of this paper is organized as follows: The
methods of face detection and alignment of our system
are introduced in Section 2. Section 3 briefly describes
the embedded hidden Markov model. Section 4 presents
embedded hidden Markov model training for facial
expression. Section 5 provides experimental results of
facial expression recognition in a network gobang game.
Section 6 draws a conclusion.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

2. Face detection and alignment
2.1. Face detection based on Haar feature
Recently, many technologies have been proposed to
detect human face from a single intensity or color image.
Generally, these technologies can be classified into four
categories: knowledge-based methods, feature invariant
approaches, template matching methods and appearancebased methods [6]. The experimental results demonstrate
that appearance based methods are more effective than
other measures. Viola et al. have proposed a boosted
cascade of simple classifiers for rapid object detection [7].
Their approach uses Discrete AdaBoost [8] to select
simple classifiers based individual features drawn from a
large and over-complete feature set in order to build
strong stage classifiers of the cascade. At each stage a
certain percentage of background patches are successfully
rejected, while almost all the object patterns are accepted.
The cascade structure is used together with a coarse-tofine strategy in [9] to develop a detector pyramid for
multi-view face detection. In this paper, we use Haar
features as feature space. GentleBoost [10] is used to
select simple classifiers. Haar feature construction and
GentleBoost have been proposed in [10].

2.2. Multi-expression face alignment
Both shape and texture (image pixels enclosed in the
facial outline) provide important clues useful for
characterizing the face. Accurate extraction of features for
the representation of faces in images offers advantages for
many applications, and is crucial for highly accurate
facial expression recognition. The task of face alignment
is to accurately locate facial features such as the eyes,
nose, mouth and outline. Active Shape Model (ASM) [11]
and Active Appearance Model (AAM) [12] are two
popular models for the purpose of shape and appearance
modeling and extraction. The standard ASM consists of
two statistical models: (1) global shape model, which is
derived from the landmarks in the object contour; (2)
local appearance models, which is derived from the
profiles perpendicular to the object contour around each
landmark. ASM uses local models to find the candidate
shape and the global model to constrain the searched
shape. Experimental results show that it is hard to train a
good ASM model for facial expression alignment while
we need to process real-time multi-expression face
alignment in game environment. In this work, separate
model is trained for each facial expression. According to
five basic expressions, we collect 200 samples
respectively for each expression. Five different ASM
models are trained using these samples. During searching,
the five model instances are placed near the centre of the
image and a coarse to fine search performed. The search

starts at level 3 (1/8 the resolution in x and y compared to
original image). After searching one level, one of these
models is selected according to their convergence rates.
Then the selected model is used, and the search
progresses to finer resolutions. The final convergence
gives a good match to the target image.

3. Embedded HMM
A one-dimensional HMM is a Markov chain with a
finite number of unobservable states [13]. Each state of
Markov Model has a probability distribution associated
with the set of possible observations. Generally, the
Markov states are not directly observable. When the
HMM is in state i, the observation is determined
according to a given conditional probability density
function (often Gaussian of a Gaussian mixture). The
state transition probability matrix, the initial state
probability distribution and a set of probability density
functions associated with the observations for each state
is most necessary to statistically characterize an HMM. In
embedded HMM, each state in a one-dimensional HMM
will be a new HMM [1]. In this way, the HMM consists
of a set of super states, along with a set of embedded
states. The super states may then be used to model twodimensional data along one direction, with the embedded
HMM modeling the data along the other direction. This
model is different with a true two-dimensional HMM
since transitions between the states in different super
states are not allowed. The elements of an embedded
HMM include:
x N0: the number of super states;
x S0: the set of super states, S0={S0,i}1İiİN0;
x Ȇ0: the initial super state distribution, Ȇ0={ʌ0,i},
where ʌ0,i are the probabilities of being in super
state i at time zero;
x A0: the super state transition probability matrix,
A0={a0,ij}, where a0,ij is the probability of
transition from super state i to super state j;
x N1(k): the number of embedded states in the kth
super state;
x S1(k): the set of embedded states, S1(k)={S1,i(k)};
x Ȇ1: the initial state distribution of the embedded
states, Ȇ1={ʌ1,i(k)}, where ʌ1,i(k) are the
probabilities of being in state i of super state k at
time zero;
x A1(k): the state transition probability matrix of the
embedded states, A1(k)={a1,jk(k)} that specifies the
probability of transition from state k to state j.
x B(k): the state probability matrix, B(k)={bi(k)(Ot0,t1)}
for the set of observation vector at row t0 and
column t1. In a continuous density HMM, the
states are characterized by continuous
observation density functions. The probability
density function that is typically used is a finite

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

mixture
k 

bi

of
M

O  ¦ c
t0 ,t1

k 
im

the



k 

k 

N Ot0 ,t1 , P im ,U im

form



(1)
where 1İiİN1 ,cim is the mixture coefficient
for the mth mixture in state i of super state k.
N(Ot0,t1,µim(k),Uim(k)) is a Gaussian PDF with mean
vector µim(k) and covariance matrix Uim(k).
(k)
Let ȁ ={Ǚ1(k),A1(k),B(k)} be the set of parameters that
define the kth super state. Using a shorthand notation, an
embedded HMM is defined as the triplet
(2)
Ȝ= (Ȇ0, A0, ȁ)
where ȁ={ȁ(1), ȁ(2),…, ȁ(N0)}.
The state structure of the facial characteristic model
and non-zero transition probabilities of the embedded
HMM are shown in Figure 1. Each state in the overall
top-to-bottom HMM is assigned to a left-to-right HMM.
This model is appropriate for face images since it exploits
an important facial characteristic: frontal faces preserve
the same structure of “super states” from top to bottom,
and also the same left to right structure of “states” inside
each of these “super states”.
m 1

(k)

(k)

4. Training the facial expression models
The observation sequence is generated by using a P×L
window to scan the image left to right, and top to bottom
which is shown in Figure 2. There are two kinds of
overlaps. The overlap between adjacent windows is U
lines in the vertical direction and V columns in the
horizontal direction. The observation vectors were formed
from the 2D-DCT coefficients of each image block. For
our experiments, P=10, L=8 and six 2D-DCT coefficients
from each block are used as observation vectors.

Figure 2. Face image parameterization and blocks

extraction

Normal

Laugh

Anger

Sleep

Surprise

Figure 3. Five basic expressions

Figure 1. Embedded HMM for facial expression

recognition
Compared with the other structure, an embedded
HMM has the following advantages:
x The computational complexity is reduced both in
terms of training and recognition;
x Better initial estimates of the model parameters
that can be obtained;
x The two-dimensional structure of the data is
naturally preserved without using extra frames or
end-of-line states that increase the complexity of
the model.

As shown in Figure 3, we use five basic expressions,
normal, laugh, anger, sleep and surprise, for training and
recognition. Each facial expression is represented by an
embedded HMM. A set of images representing different
instances of the same facial expression were used for
training. The training of the facial expression modes
include:
x Obtain the initial estimates of the model
parameters by uniformly segmenting the data
according to the number of the super states, the
number of the embedded states, and the top-tobottom and left-to-right structure of an embedded
HMM prototype as shown in Figure 1.
x At the next iteration, the uniform segmentation is
replaced by a doubly embedded Viterbi
segmentation algorithm [14]. The doubly
embedded Viterbi segmentation algorithm is
illustrated in Figure 4. The Viterbi segmentation
is applied to each row of the image, and the
probabilities





P Ot0 ,1 ...Ot0 ,T1 , q1,t10  ...q1,tT01 | Ok  ,1 d k d N 0

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

are

calculated, where q1,t1(t0),1İt1İT1 represent the
state of a super state assigned to the observation
Ot0,t1. The probabilities of the states and
observations in a row given the super state model,
obtained from the Viterbi segmentation,
represent the super state probabilities. The super
state probabilities A0 and the initial super state
probabilities Ȇ0, are used to perform the Viterbi
segmentation from the top to the bottom of the
image and to determine:



P O1,1 ...O1,T1 ,..., OT0 ,1 ...OT0 ,T1 , q 0,1 ...q 0,T0 | O



embedded HMM facial expression model is computed.
The model with the highest likelihood is selected and this
model reveals the identity of the unknown facial
expression.
We use 500 images of 20 individuals, 25 images for
each individual for training. All images are captured by a
low-cost videoconference camera and have a 320×240
pixels resolution using 256 grey levels. In each 25 images
of every individual, there are 5 images for each
expression.

or using a shorthand notation P(O,Q|Ȝ).q0,t0,
1İt0İT0 are the super states corresponding to
row t0.
¬

'9LWHUEL
¬

'9LWHUEL

O1, 1……O 1, T1

¬1

'9LWHUEL

¬

(Ȇ0, A0)

'9LWHUEL

P(O, Q|ȁ)

'9LWHUEL
¬

OT0, 1……OT0, T1

'9LWHUEL
¬1

'9LWHUEL

Figure 4. Double Embedded Viterbi Algorithm

x

The model parameters are estimated using an
extension of the segmental k-means algorithm
[15] to two dimensions. So the model parameters
are obtained according to:
number of transitions from S1,ki  to S1,kj
a1,kij
number of transitions from S1,ki 
(k)
µi = sample mean of vector i in super state k
Ui(k)= sample covariance matrix of vectors in
state i of super state k
number of transitions from S 0,i to S 0, j
a 0,ij
number of transitoins from S 0,i

x

The iteration stop, and the HMM is initialized,
when the Viterbi segmentation likelihood at
consecutive iterations is smaller than a threshold.

5. Tests and results

Figure 5. Snapshot of the network gobang game

and web camera
After training the models of each facial expression, the
probability of the observation sequence given an

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

We combined this real-time facial expression
recognition system with a network gobang game. The
game program is set to change the facial expression
picture of the roles in the game based on the recognition
result of our system. As shown in Figure 5, the facial
expression of the roles in the game changed as soon as the
game player’s expression changed.
When recognizing, we use 10 individuals for real-time
recognition. Among the 10 individuals, 5 people are from
the training set, the other 5 people are new individuals.
The result is shown in table 1 and table 2 respectively.
Table 1. Recognition results of the individuals in

the training set
Expression of the game player

Correct

Wrong

Normal

9

1

Laugh

10

0

Anger

8

2

Sleep

10

0

Surprise

9

1

Table 2. Recognition results of the new

individuals
Expression of the game player

Correct

Wrong

Normal

8

2

Laugh

9

1

Anger

7

3

Sleep

10

0

Surprise

8

2

In table 1, the system achieves a 92% of successfully
classified examples from the training set, while an 84% of
successfully classified examples of the new individuals in
table 2. It takes 60~100 mm to detect the face and
recognize the facial expression in a PIII processor
working at 800MHz with 128M RAM.

6. Conclusions
In this paper, we propose a novel framework to realtime facial expression recognition in the interactive
computer game environment using an embedded HMM.
There are two main contributions of this work. First, an
embedded HMM is used in real-time facial expression
recognition. Second, face alignment is used in facial
expression environment. Our approach makes the best of
2D-DCT coefficients and reduces greatly the size of the
observation vectors and, therefore, decreases the
complexity of the training and recognition system.

Experimental results demonstrate that the proposed
method achieves good performance. However, it needs
more training data to train more effective ASM models
and facial expression models.

7. References
[1] Ara V. Nefian and Monson H. Hayes III, “Face Recognition
using an embedded HMM”, IEEE International Conference
Audio Video Biometric based Person Authentication, March
1999, pp. 19-24.
[2] M. Wang, Y. Iwai, and M. Yachida, “Expression recognition
from time-sequential facial images by use of expression change
model”, Proceedings of Third IEEE International conference on
Automatic Face and Gesture Recognition, 1998, pp. 324-329.
[3] H. Sako and A.V.W. Smith, “Real-time facial expression
recognition based on features’ positions and dimensions”,
Proceedings of the 13th International Conference on Pattern
Recognition, 3, 1996, pp. 643-648.
[4] I.A. Essa and A.P. Pentland, “Coding, analysis,
interpretation and recognition of facial expressions”, IEEE
Transactions on Pattern Analysis and Machine Intelligence, 19.
No.7, 1997, pp. 757-763.
[5] F. Samaria and S. Young, “HMM based architecture for face
identification”, Image and Computer Vision. Vol. 12, October
1994, pp. 537-543.
[6] Ming-Hsuan Yang and David J. Kriegman, “Detecting Faces
in Images: A Survey”, IEEE Transactions on Pattern Analysis
and Machine Intelligence. Volume: 24, Issue: 1, Jan. 2002, pp.
34-58.
[7] P.Viola and M.Jones, “Rapid object detection using a
boosted cascade of simple features”, IEEE CVPR, 2001, pp.
511-518.
[8] Y. Freud and R.schapire, “A short introduction to Boosting”,
J. of Japanese Society for AI, 14 (5), 1999, pp.71-78.
[9] Z.Zhang, L.Zhang, S.Li, et al., “Real-time Multi-view Face
Detection”, IEEE The 5th international conference on automatic
face and gesture recognition. Washington, DC, USA, 2002.
[10] R.Lienhart and J.Maydt, “An extended set of Haar-like
features fro rapid object detection”, IEEE ICIP, 2002, pp. 900903.
[11] T.F.Cootes, C.J.Taylor, D.H.Cooper, and J.Graham,
“Active Shape Models: Their Training and Application”,
CVGIP: Image Understanding, Vol.61, 1995, pp. 38-59.
[12] T.F.Cootes, G.J.Edwards, and C.J.Taylor, “Active
Appearance Models”, ECCV98, Vol.2, 1998, pp. 484-498.
[13] L. Rabiner and B. Huang, “Fundamentals of Speech
Recognition”, Englewood Cliffs, NJ: Prentice-Hall, 1993.
[14] S. Kuo and O. Agazzi, “Keyword spotting in poorly printed
documents using pseudo 2-D Hidden Markov Models”, IEEE
Transactions on Pattern Analysis and Machine Intelligence. Vol.
16, August 1994, pp. 842-848.
[15] L. Rabiner, “A tutorial on Hidden Markov Models and
selected applications in speech recognition”, Proceedings of
IEEE. Vol. 77, February 1989, pp. 257-286.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualization (CGIV’04)
0-7695-2178-9/04 $20.00 © 2004 IEEE

