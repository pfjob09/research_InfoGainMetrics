2009 Sixth International Conference on Computer Graphics, Imaging and Visualization

A Review of Scene Visualization Based on Language Descriptions

Zeng, X and Tan, Mling
Information Visualization Art and Design Research Centre, School of Art
Central South University
Changsha, China
e-mail: zengxin@smail.csu.edu.cn
expressed and, ultimately, the scenes that can be created. The
generating and manipulating of virtual elements in 3D virtual
environments based on natural language input has been
investigated by many researchers over the last few decades.
The generating and manipulating of virtual elements in
virtual scenes based on language input focus on how to
identify the nouns associated objects and prepositions
corresponded spatial relations, thus enabling a system to
assemble virtual scenes by manipulating the visual elements
of the environment. SHRDLU is a system that capable of
answering questions and executing commands that allowed
user to interact with a very restricted virtual world of objects
by including as few as 50 words [5]. [6] described a program
SPINT takes natural language descriptions, produces a
sketch of the environment instead of creating a real-time
interactive 3D environment. An object-placement system
called Put that uses a very simple and constrained grammar
(X + Preposition+ Y) for placing rigid objects in 3D scenes
[3]. The limitations of this system are the users cannot adds
more objects into the environment and unable to modify the
attributes of the objects. The Spoken Image system accepts
natural language descriptions of city views and then displays
a view consistent with the description [7]. User can navigate
and interact with the environment through spoken natural
language. The deficiency of SI is the environment lack of the
detail and the objects could not be interacted with in real
time, such as relocates its positions and modifies its
attributes. The goal of CarSim system is to visualize a 3D
traffic accident scene from a rewritten description [8]. But
the system is only implemented the collisions happened only
between two vehicles without considering more elements of
the environment. WordsEye a language based system for
automatically translating text into representative static scenes
with some character actions via the use of the predefined
poses [9]. However, the output is only a static image, and the
background environment need be added manually after the
scene is built up. It also does not provide the interface for
real time interactivity. [10] describes an approach that enable
non-professionals to manipulate visual features of the aspects
of environment in real-time and ultimately create an
interactive 3D virtual environment by using simplified storybased natural language.

Abstract—When dealing with computational models for
integrating natural language and graphic representation, it is
useful to examine theories of human visual perception and
spatial language to discover what kind of processes and
representation lie in between. This paper begins with a
discussion of systems which have been implemented. It outlines
related natural language processing technology and theories
that involve in language based scene visualization. We then
discuss the way can be used to integration of visual
information for scene generation.
Keywords-language description; graphic representation;
visual information; text to visualization

I.

INTRODUCTION

The task of generation of the computerizable virtual
environment could be greatly simplified by integration of
natural language interface. Although the study of visualizing
information has a long history, many efforts have been made
in visualizations of abstract data whilst little has been done in
language visualisation [1, 2]. Because the whole process
involves complex natural language processing and
understanding, the knowledge of visual perception, spatial
theory, and graphic representation can be combined with one
another to accomplish the task of generating a virtual
environment ˈ especially the concepts essential
representation can be the hardest for a computer to
understand and represent (i.e. computer animation for spatial
and temporal relations) [3].
Even the integration of natural language and graphic
application appears a difficult and complex task, and the
technique of natural language processing is still immature,
but general grammars do exist that can be used for many of
the most common natural language constructions [4]. If we
find a way to restrict both the input language and the
conceptual domain of the systems when dealing with natural
language and graphical complexities [3], this may help us
ignore many of the problems of language issues for existing
NLP techniques to produce reliable interpretations. This kind
of approach has been broadly used in many existing system.
Each application system usually has its own textual
language, suitable for its particular objectives. The form of
the language created restricts the concepts then can be
978-0-7695-3789-4/09 $25.00 © 2009 IEEE
DOI 10.1109/CGIV.2009.9

429

From these language based systems, they demonstrate
that language interface can help users to explore or complete
their tasks in a virtual environment. It demonstrates the uses
of language constraints corresponding to default
dependencies in the specific domain, and greatly simplify the
whole conceptual visualization process, i.e. to address the
ambiguities of the linguistic description input, to provide
information to assist visual search, and to solve the difficulty
of the spatial reasoning by ignoring some of the underspecificity.
II.

accurately describe images and perform classification or
identification tasks based on objects’ visual appearance. The
description of an environment is dependent on the viewer’s
perspective and the characteristics of the space to be
described. This involves object identification and
placement, it also appears very important to areas of
computer graphics such as generation of virtual environment
and animation. When dealing with computational models
for integrating natural language and graphic representation,
it is useful to examine theories of human visual perception
and spatial language to discover what kind of processes and
representation lie in between.
The relationship between perception and language is one
of the research fields of cognitive science that concerned
with machine-simulated intelligent behaviour [14]. Studies
in visual perception, such as Marr’s [15] influential work
investigate how to use visual information to construct an
internal spatial representation of visible objects. [16]
describes a theory for correlating words and images on a
lower level. In particular, he outlines a method whereby an
intermediate representation provides the link between
language and Marr’s computational theory of vision. In
order to recognize objects, Jackendoff suggests that object
descriptions similar to Marr’s 3D models are associated
with classes of objects, thus providing a link between the
perceptual system and the conceptual level. Furthermore,
both visual imagery and object recognition share the same
3D representation of objects. He suggests an extension to
Marr’s 3D representation of objects that would include path
information (helpful in describing the motion of objects)
and would extend object-internal coordinate axes to the
space exterior to an object. The latter would prove helpful in
spatial reasoning. The ideas he expressed are powerful,
since his method would establish a correlation between
language and pictures at the conceptual level. Jackendoff
deals with the correspondence problem mainly at the singleword level (both nouns and verbs) but does not extend the
discussion to establishing a correspondence between a
sentence/phrase and the complex scene that it may evoke
[17].
Spatial language has been the specific subject of much
research in cognitive linguistics. This approach considers
the linguistic structure used for describing configurations as
being fundamental for spatial cognition [18]. [19]
hypothesize that there are “what” and “where” channels and
that these are localized in the human brain. One is
responsible for object identification primarily through shape
and the other controls the relative special relation of objects.
They argue that the “what” system is relatively rich in its
ability to describe shape, there are thousands of nouns that
are used to identify objects. The “where” system is
relatively limited because it is encoded mainly via
prepositions and spatial adverbs. Whenever an object
location or orientation is depended on some other object.
[18] and [20] call this relationship Figure/Ground
(foreground vs. background). Figure indicates move/more

EXTRACTING MEANING FROM DESCRIPTION

In order to represent the contents of the natural language
descriptions as virtual scenes, the first and the most difficult
problem encountered is how do we get a computer to
understand natural language. After more than 50 years of
development and research on natural language processing
and understanding, the state-of-art natural language enabled
technology is rapidly growing along with development of
computer processing power. Natural language processing
belongs an area of AI research that attempts to reproduce the
process of the human interpretation of language. It uses
known data about words and predefined rules that show
authorized word structures to give the meaning of
descriptions to enable communication between people and
computers [12]. [11] mentions three types of knowledge
which are used in natural language processing, namely
syntactic knowledge, word-sense knowledge, and world
knowledge. Syntactic knowledge refers to the permissible
structures of sentences. Word-sense knowledge refers to the
meanings of words and the associations between words.
When combined, the above three types of knowledge permit
to construct a representation of the initial meaning of the
sentence.
The general process of NPL can be divided into
following levels: morphological analysis, lexical tagging,
syntactic parsing, and semantic analysis, knowledge of the
language as well as real world information. In the theory of
meaning, natural language syntax defines the structure of the
sentence; semantic determines the meanings of words of the
sentence and to build up complex meanings by combining
elements of a language [13]. The meaning of a sentence is
most often represented as a frames or logical formulas in
order to capture meaning of a sentence and link between
language and external commonsense knowledge of the
world. They are the most prevalent ways of representing the
semantics of natural language and it is based on what kind of
information the system needs. It is one of the most important
phrase in language based graphic representation system and
acts as an intermediate that links the language processing and
visual presentation together.
III.

PERCEPTION THEORY AND SPATIAL
LINGUISTICS

The semantic representation figures out what is the
information that the language conveys. The next problem
we encounter is what kind of information can be represented
or visualized by the simulated virtual world. People can

430

mapped as low-level data and seeded with descriptions of
generic visual features and can be extended for particular
domains and further assignment.
Physical objects along with the other visual information
are central to the description, human can perceive and
identify these physical phenomenon in real world. In general
approach, the object type is defined explicitly to represent a
physical object, each object in the environment
corresponding to a related noun and associated with a
geometry model in virtual object database. The elements are
defined with respect to implicit properties of the object, such
as initial position, size, length, width and height, etc.
From the perspective of syntactic category, the function
of adjective is primarily associated with modifying nouns;
semantically adjectives include many terms that describe
properties and qualities, such for the concepts of colour, age,
and value. A conventional classification of adjectives divides
them into two major classes: descriptive adjectives and
relational adjectives [25]. Descriptive adjectives (such as big
and small, light and dark) are organized into clusters that
represent the values of bipolar attributes to their head nouns.
There are some semantic features of descriptive adjectives
that relate to visualisation. Relational adjectives differ from
descriptive adjectives in that they do not relate to an
attribute, which means their visual semantics are the same as
their corresponding nouns, e.g. swimming pool, tea table, etc.
Therefore, this subcategory of adjectives can be treated as
nouns and represent specified 3D objects defined in the
object database. [26] suggestes 12 semantic classes of
descriptive adjectives that are each further subdivided. There
are 4 common types of descriptive adjectives that may
contain visual information according their classification:

moveable, variable site, path, orientation; Ground denotes
stationary. [21] calls it primary object and reference object
and provides an extensive list of other terms used to
describe the relationship. The location and orientation of a
primary object can be determined in relation to reference
objects. Objects are almost solely characterized by their
spatial properties, such as relevant dimensions, boundary
conditions, symmetry vs. distinguish ability of parts.
IV.

INTERGRATION OF VISUAL INFORMATION
FOR SCENE GENERATION

In order to enable the computer system to represent a
virtual environment through natural language descriptions, it
would require a great amount of computation because the
system must integrate and reflect our own internal
representations of knowledge for natural language
understanding [9, 22]. In particular, the system must be able
to know what kind of visual information could be
represented in the virtual environment which reflect or
correspond with how they appear in the real world; and what
visual information is hidden or not mentioned but could be
inferred by the context of the descriptions. Language
description always contains many clues that enable people
understand the descriptions. If such clues are carefully
recognized and analysed, the amount of computation may be
significantly reduced and the visualization of such a scene
becomes possible. As our observation of the world mainly
depends on the visual information, people usually have no
idea how to draw the picture through non-visual features
depictions, such as adjectives used to portray the character’s
behaviour as friendly, decent, etc. Therefore, we should just
focus on the visual features of the environment and objects,
such as colour, size, material, direction and orientation,
lighting, etc. This limited domain contains simple but
effective information that is sufficient for the graphic
component to create the 3D scenes.
However, as natural language often describes visual
scenes at a high level, thus leaving out many of details that
have to be specified and translated into program language
that the computer can understand. Conveying the abstract
meaning from text narrative to visual representation requires
adding more information [23]. The common approach of
language-based graphic application has involved defining
some specific words with program functions [24]. This
involves converting the various semantic elements into
parameterized data definition, and to define inferences of the
visual features through objects and their context from the
input text. The values assigned to these parameters affect the
generation of the 3D scene. The meaning of the words is
considered as primitive resource for visualization, and this
direct association word-concept-visual is used for knowledge
representations. To achieve such a goal, it is necessary to
build an intermediate representation that bridges the gap
between the language component and graphic component. It
therefore allows us to semantically parameterize the visible
and depictive vocabulary (i.e. the word contains visual
information) and draw inference from the context. The
vocabulary and their associated visual features can be

x
x
x

x

Perceptional
o Lightness, colour, sound, taste, smell,
temperature, touch.
Spatial
o Dimensional, directional, origin, form.
Human-related
o Body-related
 Appearance, sexual, desire, etc.
o Mood-related
 Mood, stimulus
o Behaviour-related
character,
discipline,
 Skill,
relation, etc.
Material-related
o Material, consistence, purity, weather, etc.

The perceptional adjectives class consists of the
adjectives that can be perceived by using different human
senses, such as by hearing, seeing, tasting, etc. Therefore in
this class, only lightness and colour subclasses are
distinguished by whether they can be observed through a
visual sense, i.e. human eyes. The spatially related adjectives
that are associated with the size of an object can be defined
to modify the size of the objects. Human-related adjectives
are not included because it is hard to semantically

431

determined by reference objects. [30] suggested that there
appear to be three distinct ways of talking about space—
three spatial reference frames—that are used to locate a
target object with respect to a background object: the
intrinsic reference frame, the relative reference frame, and
the absolute reference frame. [18] defines the spatial system
according to configurations and interrelationships of
boundaries, masses. We distinguish between structural and
spatial properties of objects. The structural properties stand
to the spatial attributes within the object, meaning that each
object has location, orientation, dimension, and shape as its
internal and independent spatial attributes. Spatial properties
specify the relationships between objects. Spatial properties
of one object to another depend on geometric relations (i.e.
near, in, on). This is the most common way to express object
spatial relations. [20] geometrical idea indicates that spatial
prepositions have ideal meanings associated with them in
their lexical entries. The ideal meaning itself is defined as a
relation between “ideal geometric objects” such as point,
line, surface, etc. together with a set of constraints. These
geometric functions determine what the preposition
contributes to the meaning of a particular situation. Take a
simple sentence like “A book is on the desk.” There are two
objects being described—book and desk, the desk is inside
the preposition phrase and acts as a reference object to the
book. The reference object helps to define a region of space
(i.e. surface) and to support the book. The prepositions can
be divided into several groups: proximity prepositions (near,
far (from), by, next to), directional prepositions (in front,
behind, left of, below), and boundary prepositions (in, on,
between, across, crossing). In the current system, we
concentrate on using the relative reference frame method of
[30] (i.e. viewer-centred coordinate system) and examine the
spatial descriptions that describe the spatial configuration of
a located entity with respect to a reference entity. The
information such as structural properties of the objects
involved, initial position, orientation, axes of rotation etc.
can be used to semantically parameterize the prepositions
according the common situation. [31] points out that a 3D
spatial relation is traditionally represented as volumetric
representation and boundary representation, which often
used in computer graphics.

parameterize them even if they could be observed by human
vision. Some of them may accompany some special actions
or occur in specific situations and involve manipulating
massive data, such as inverse kinematics of human body
movement, facial expression e.g. excited, angry, terrified.
Some of the material-related adjectives such as gold, wooden
can be easily semantically parameterized. According to both
the human visual perception perspective and the
specification of computer graphic, three kinds of descriptive
adjectives can be predefined, such as lightness, colour and
part of material. Colour is one of the most important
attributes of the environment. Lightness adjectives describe
the illumination and estimate the reflectance from the light,
such as light/dark. English has 11 basic color terms: black,
white, red, green, yellow, blue, brown, purple, pink, orange,
and gray [27]. Although the color names are not very
precise, but linguistic colour categories do indeed affect how
people perceive those colours [28]. These terms are based on
differences in hue among the color categories and a great
numbers of colour such as wheat, slate, cobalt, sea green,
etc. exist in natural language for colour description.
In our requirements for a conceptual representation of a
3D virtual scene from given natural language descriptions,
the interpretation and representation of the spatial
relationships of the objects are the most important issues. In
a general sense, objects spatial relations are subject to the
description together with other visual information that
humans can perceive in the real world. Spatial abilities are
cognitive functions that enable people to deal effectively
with spatial relations and orientation of objects in space.
When we describe spatial relationships with natural language
we often use spatial prepositions. These prepositions specify
the spatial relationships between objects and movement
paths in a language visualisation system. [19] states that
object identification (nouns) and object position (spatial
prepositions) of the language system are distinguished by
“what” (identification) and “where” (localization) channels.
It is important to explain the theory of an observer’s view
before discussing the spatial relations. Without this, people
could not imagine the direction such as “left of” and “to the
west of”, etc. and would be unable to figure out where the
object to refer. A viewpoint acts like a camera and is a
predefined viewing position and orientation in the world.
The description of an environment is dependent on the
viewer’s perspective and the characteristics of the space to
be described. There are three ways to describe environments:
gaze, route and survey [29]. Each way reflects the choice of
a perspective and depends on the characteristics of the
environment. To depict the object’s spatial relations in a
relative small space (e.g. a room), people prefer the viewercentred frame of gaze descriptions, for example, “the chair is
to the right of the desk” locates the chair in relation to the
background object from the viewer’s perspective. In
describing apartments, people like to use route descriptions.
In portraying their town, people typically uses object-centred
frame of survey descriptions.
Objects that are not located in terms of absolute space,
but always in terms of placed against a background [16].
This background is a region of space whose organization is

V.

SUMMARY

In order to visualize the contents from language
descriptions, the central issue is the correspondence
problem, namely, how to correlate visual information with
words. It is not a simple matter of correlating pictures with
words (e.g., nouns); it is necessary to associate visual
information with events, phrases or entire sentences thus
making the indexing problem very difficult [17]. Another
issue to be considered is whether the intermediate
representation is flexile enough to handle both visual and
language information. Translating text into graphic
representation cannot be done without an intermediate
representation, that is, a conceptual structure that bridges the
gap between these two expression modes [32]. The key
problems arising in such tasks are how to derive visual

432

semantics from the text and how to map between the visual
information and meaning. This not only involves lexical,
syntactic and semantic processing of text, but also needs to
integrate commonsense knowledge about the real world.
The most important part of visual semantics is to unify
various words meanings and related theories (i.e. perception,
spatial linguistics and real world knowledge), and to
interpret their associated visual information (e.g. nouns
associated objects, verbs related actions, adjectives
associated attributes and spatial prepositions, etc.) into
computer understood low-level data. Thus, a language
understanding system enables us to access visual knowledge
and to generate the visual scene corresponding to the
descriptions.

[15] D. Marr, Vision: a Computational Investigation into the Human
Representation and Processing of Visual Information, San Francisco,
Freemann, 1982.
[16] R. Jackendoff, “On beyond zebra: the relation of linguistic and
visual information,” Cognition, 26(2), 1987, pp.89-114.
[17] K. R. Srihari, and T. D. Burhans, “Visual semantics: extracting
visual information from text accompanying pictures,” In Proceedings
of AAAI-94, Seattle, WA, 1994.
[18] L. Talmy, “How language structures space.” Spatial Orientation:
Theory, Research, and Application, Herbert, P and Linda, A. (Eds.),
Plenum Press, New York, 1983, pp.225-282.
[19] B. Laudau, and R. Jackendoff, “What and Where in spatial language
and spatial cognition,” Behaviour and Brain Sciences 16, 1993,
pp.217-265.
[20] A. Herskovits, Language and Spatial Cognition. An interdisciplinary
Study of the prepositions in English, Cambridge University Press,
Cambridge, MA, 1986.
[21] G. Retz-Schmidt, “Various views on spatial prepositions,” AI
Magazine, 9(2), 1988, pp.95-105.
[22] B. Heidorn, Natural Language Processing of Visual Language for
Image Storage and Retrieval. PhD thesis, University of Pittsburgh,
1997.
[23] W. Wojtkowski, and G. Wojtkowski, “Storytelling: its role in
information visualization,” In Proceedings of the 5th European
System Science Congress, special issue, Vol2. Crete, 2002.
[24] D. Zeltzer, Task-level Graphical Simulation: Abstraction,
Representation, and Control. In Making Them Move, MorganKaufmann, San Francisco, 1991, pp.3-33.
[25] D. Gross, and K. Miller, “Adjectives in WordNet,” International
Journal of Lexicography 3(4), 1990, pp.265-277.
[26] F. Hundsnurscher, and J. Splett, Semantik der Adjektive im
Deutschen: Analyse der semantischen Relationen, Westdeutscher
Verlag, 1982.
[27] B. Berlin, & P. Kay, Basic Color Terms: Their Universality and
Evolution. Los Angeles, University of California Press, 1969.
[28] J. David, (2003) Spatial language and temporal cognition. [online].
[cited 10th Feb 2004]. <www.swarthmore.edu/SocSci/Linguistics/
papers/2003/january.pdf>.
[29] H. A. Taylor, & B. Tversky, “Perspectives in Spatial
Descriptions,”Journal of Memory and Language, Vol. 35, 1996,
pp.371-391.
[30] S.C. Levinson, et al., “Returning the tables: language affects spatial
reasoning,” Cognition 84, 2002, pp. 155–188.
[31] V. McDermott, “Spatial reasoning,” In Encyclopedia of Artificial
Intelligence, volume 2. John Wiley & Sons, 1987.
[32] M. Arnold, “Transcription automatic verbal-image et vice versa,” In
Proceedings of EuropIA-90, Paris, pp.30-37, 1990.

ACKNOWLEDGMENT
This project is sponsored by SRF for ROCS, SEM and NSFC
60873189.

REFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]

[8]

[9]

[10]
[11]
[12]
[13]
[14]

K. R. Srihari, “Computational models for integrating linguistic and
visual information: A survey,” Artificial Intelligence Review, special
issue on Integrating Language and Vision, 8, 1995, pp.349-369.
W. Wahlster, “Text and images,” In: Survey of the State of the Art
in Human Language Technology. Linguistica Computazionale, Vol.
12, 1998, pp.348 – 352.
R. Clay, and J. Wilhelms, “Put: language-based interactive
manipulation of objects.” IEEE Computer Graphics and
Applications, 16(2), 1996, pp.31–39.
XTAG Research Group, “A Lexicalized Tree Adjoining Grammar
for English. Technical Report”, University of Pennsylvania, 1995.
T. Winograd, Understanding Natural Language, Academic Press,
1972.
A. Yamada, T. Yamamoto, and H. Ikeda, “Reconstructing Spatial
Image from Natural Language Texts,” In Proceedings of COLING
92, Nantes, pp.23-28, 1992.
A. Smith, B. Farley, and S. Nualláin, “Visualized models for
language understanding,” In Proceedings of the Workshop
Representation and Processes between Vision and Natural Language,
Twelfth European Conference on Artificial Intelligence (ECAI-96).
Budapest, Hungary, pp.43-50, 1996.
P. Nugues, “Verbal and written interaction in virtual worlds, some
application examples.” In Proceedings of the Fifteenth Twente
Workshop on Language Technology, Universiteit Twente, Enschede,
pp.137-145, 1999.
B. Coyne, and R. Sproat, “WordsEye: An automatic text-to-scene
conversion system,” In Proceedings of 28th SIGGRAPH Annual
Conf. Computer Graphics and Interactive Techniques, Los Angeles,
pp.487-496, 2001.
X. Zeng, , Q. Mehdi, and N.E. Gough, “Shape of the story: story
visualization techniques,” In Proceedings of IEEE 7th International
Conference on Information Visualisation, London, pp.144-151, 2003
J. Allen, Natural Language Understanding, Benjamin/Cummings,
Menlo Park, CA, 1987.
T. Samad, A Natural Language Interface for Computer-Aided
Design, Kluwer Academic Pulishers, Boston, Dordrecht, Lancaster,
1986.
C. Monz, M. Rijke, “Inference in computational semantics,” In
Proceedings of the first workshop on Inference in Computational
Semantics, Amsterdam, pp.151-156, 1999.
W. Maaß, “How spatial information connects visual perception and
natural language generation in dynamic environments: Towards a
computational model,” In Proceedings of the International
Conference COSIT'95, Semmering, Austria. Berlin, Heidelberg:
Springer, pp.223-240, 1995.

433

