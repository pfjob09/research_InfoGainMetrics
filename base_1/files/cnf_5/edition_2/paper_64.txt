2009 Sixth International Conference on Computer Graphics, Imaging and Visualization

/ -Nearest Neighborhood Criterion for Improving Locally Linear Embedding
1

Armin Eftekhari , Hamid Abrishami Moghaddam1, Massoud Babaie-Zadeh2
1
K.N. Toosi University of Technology, Tehran, Iran
2
Sharif University of Technology, Tehran, Iran
{a.eftekhari@ee.kntu.ac.ir, moghadam@eetd.kntu.ac.ir, mbzadeh@ ee.sharif.edu}
Abstract

dimensional structure of data highly depends on the
construction of an accurate adjacency graph that gives a
faithful representation of the local geometry of data [3].
In this regard, though widely used, -NN criterion
suffers from major drawbacks. In fact, since each sample
is connected to its direct nearest neighbors, -NN rule
is generally unable to exclude noisy samples or outliers
in the neighborhood. In addition, -NN criterion
considers a fixed neighborhood size about each sample
on the manifold. In this paper, with our focus on LLE,
-NN criterion is first represented as an optimization
problem, which is then modified to yield / -nearest
neighborhood ( / -NN) criterion. As was the case in NN, new criterion searches for a small subset of samples
in the neighborhood of each data point. However, unlike
-NN, this subset is not limited to -nearest neighbors of
each sample, but instead belongs to a larger
neighborhood within the roughly linear patch on the
manifold centered at that sample. Furthermore, size of
this subset is chosen adaptively to include the minimum
required samples among
nearest neighbors of
each data point, which is often believed to give a more
reliable representation of the manifold [4]. The proposed
criterion involves finding the sparsest approximate
representation of a sample in the dataset, and is realized
by modifying the recently proposed Robust-SL0
algorithm for sparse approximate representation [5]. The
modified spectral method, namely Sparse-LLE is then
experimentally
validated
on
several
datasets,
demonstrating remarkable improvement over the
conventional LLE. The rest of this paper is organized as
follows. Section 2 is devoted to a review of the LLE. In
Section 3, shortcomings of -NN are studied and / NN criterion is introduced and justified. Implementation
details are then discussed in Section 4 and, finally,
experimental results are presented in Section 5.

Spectral manifold learning techniques have recently
found extensive applications in machine vision. The
common strategy of spectral algorithms for manifold
learning is exploiting the local relationships in a
symmetric adjacency graph, which is typically
constructed using
-nearest neighborhood ( -NN)
criterion. In this paper, with our focus on locally linear
embedding as a powerful and well-known spectral
technique, shortcomings of -NN for construction of the
adjacency graph are first illustrated, and then a new
criterion, namely / -nearest neighborhood ( / -NN)
is introduced to overcome these drawbacks. The
proposed criterion involves finding the sparsest
representation of each sample in the dataset, and is
realized by modifying Robust-SL0, a recently proposed
algorithm for sparse approximate representation. / NN criterion gives rise to a modified spectral manifold
learning technique, namely Sparse-LLE, which
demonstrates
remarkable
improvement
over
conventional LLE through our experiments.
Keywords--- Locally linear embedding, sparse
representation, Robust-SL0

1. Introduction
In the recent years, several algorithms have been
developed to perform dimensionality reduction of lowdimensional nonlinear manifolds embedded in a highdimensional space. In particular, due to technical
advantages, local linear embedding (LLE) has found
widespread applications in real-world problems [1, 2].
LLE is based on eigen decomposition of a special Gram
matrix, which is designed to preserve the local structure
of data. This local structure is typically defined using
nearest neighborhood criterion in the Euclidean space by
constructing a symmetric adjacency graph, in which the
nodes represent the training samples and any pair of
nodes are connected iff the corresponding data points are
adjacent. Indeed, successful recovery of the low978-0-7695-3789-4/09 $25.00 © 2009 IEEE
DOI 10.1109/CGIV.2009.81

2. Locally Linear Embedding
In LLE, the local properties of the manifold are
expressed by writing each sample as a linear
combination of its nearest neighbors. LLE then attempts
to preserve these local relationships in the low392

where · is the step function and η is the ℓ -norm
of the vector η, i.e. number of nonzero elements of η. It
is observed that solving
primarily minimizes the
second term of the functional by choosing x
x . Then, keeping x
fixed,
minimizes the
reconstruction error x
Xw
by solving the linear
Xw subject to Supp w
, where
system x
x is the projection of x onto Span x . It is observed
that, despite its importance, minimizing the
reconstruction error does not contribute to the choice of
x in . Furthermore, # x is fixed to in , while it
is generally better to let the algorithm automatically
decide on # x by selecting only necessary samples for
representation of x [4]. To overcome these drawbacks,
the following optimization problem is introduced:

dimensional space [6]. To be more specific, LLE first
constructs the adjacency graph
, , whose nodes
and edges
represent the data samples and
neighborhood relations among samples, respectively.
, we will use x ~x to
Denoting each sample by x
indicate that samples x and x are adjacent by some
criterion, i.e. x x
. Similarly, x
x will indicate
xx
. Furthermore, for each sample x , the subset of
samples x satisfying x ~x will be denoted by x . In
particular, for -NN criterion, we have x
x ,
where
x denotes the subset of -nearest neighbors
would denote the
of x . Additionally,
x .
corresponding subset of indices of
Once the adjacency graph is constructed, each
sample x is written as a linear combination of its
nearest neighbors. This is achieved by solving:
: min x

min x

Xw

s. t.
Supp w
wT
1

s. t.
wT

(1)

,

where w
contains the reconstruction weights and
Supp η denotes the support of η, i.e. subset of all
is nonzero. In addition,
indices , for which
. The weight matrix W
w ,…,w
1, … ,1 T
is then constructed and the embeddings are found
by computing the eigenvectors associated with the
bottom nonzero eigenvalues of M
I W I WT ,
where I is the identity matrix. To be more specific,
denoting the resulting modal matrix by V
v ,…,v
, rows of V contain the embeddings y
. In
fact, up to a scaling factor that depends on the algorithm,
the embedding of x , namely y , is a vector with ,
.
, ,

s. t.
w
wT
w,

Xw

,

x

x

x
w

1,
0
(3)

min w
s. t.
x
∑
wT
,

-NN criterion implies that x ~x iff x
x ,
and is justified based on the notion that local geometry of
x rather
the manifold at x is best represented by
with #
. In
than by any other subset
x
this section, with our focus on LLE, shortcomings of this
notion are discussed.
/ -NN criterion is then
introduced, which, to some extent, overcomes the
shortcomings of -NN.
As shown in the Appendix, (1) is asymptotically
equivalent to:
x

,

where ,
are finite positive scalars. By choosing
∞, we ensure that, in contrast to , minimizing the
Xw
contributes to our
reconstruction error x
choice of x . Moreover, (3) uses the minimum
required number of samples to best represent x , and
hence adaptively selects # x on the manifold. Note
that for every pair and , there always exist a pair
and , for which (3) is equivalent to:

3. / -Nearest Neighborhood Criterion

: min lim

Xw

Xw
,

x

,
x

,

0,
0
(4)

Furthermore, we notice that ∑
w, x
x
sets an upper limit on x
x
for x
x and hence
there exists some
0, for which the second constraint
x
,
in (4) can be safely replaced by x
x
x [7]. A closer look reveals that this in turn
could be safely replaced by Supp w
, for
some integer . Therefore, we can rewrite (4) as follows:
: min w

x

s. t.
x
Xw
Supp w
wT
1

,
1,
0

,
,
(5)

(2)

Let w and x denote the solution of
and the subset
of samples corresponding to the nonzero elements of w ,
393

respectively. Note that, as a result of the second
. In order to preserve the
constraint in , x
computational advantages of working with highly sparse
matrices, we further limit # x to , for some integer
. This is achieved by keeping at most
top
nonzero elements of w and setting others (if any) to
is also modified by discarding the
zero. x
corresponding samples. The new criterion will be
referred to as / -NN rule and is summarized in Fig. 1.
Notice that, in -NN, x is the subset of first nearest
neighbors of x , whereas in , x is the best subset
x with #
, that contains the minimum
required samples to achieve a reconstruction error less
than the error tolerance . When compared to -NN,
/ -NN criterion is able to exclude noisy neighbors and
outliers, which is achieved by the constraint on the
reconstruction error in
. On the other hand, when
compared to -NN, / -NN criterion adaptively selects
(
) to best represent x with the minimum
#x
required number of samples. Now, using / -NN
criterion to construct the adjacency graph, LLE is
modified to obtain an improved spectral algorithm,
dubbed Sparse-LLE. Note that the only difference
between LLE and Sparse-LLE lies in the construction of
the adjacency graph.

whenever such answer exists. Moreover, Robust-SL0
runs significantly faster than the competing algorithms,
while producing answers with the same or better
accuracy [5]. The idea is now to modify , , in a way
that enables using Robust-SL0 algorithm to solve , , .
This necessitates proper modification of the second
. While this may be
constraint in , , , i.e. Supp s
achieved by, for instance, setting
0 for
at each
iteration, we prefer to preserve the studied convergence
properties of Robust-SL0 by replacing Supp s
with
a term in functional that smoothly favors small values for
when
. Therefore, , , is modified to:
lim max
s.t. b

1

/

1

/

As
(6)

where we take 0
,
1. Convergence properties of
(6) are obtained by minimal modifications in the proof
presented in [5]. Note that Robust-SL0 algorithm is now
applicable to (6) by merely using the gradient of the
functional of (6) in the algorithm. The interested reader
is referred to [5] for details.

5. Experiments
The objective of this section is to experimentally
assess the merits of the proposed / -NN criterion for
construction of the adjacency graph. To this end, the
performance of LLE and Sparse-LLE are compared on
several datasets. In each experiment, (and if available
) are experimentally tuned for the best results. Other
parameters of Sparse-LLE are fixed to:
0.05,
0.9. As our first experiment, we compare the
performance of LLE and Sparse-LLE for visualizing the
Frey face dataset, which consists of 1965 gray-level
images of a single individual acquired under different
expression and pose conditions [2]. Few images in this
dataset are depicted in Fig. 2(a). Fig. 2(b) depicts the first
two components of these images discovered by LLE.
Depicted in Fig. 2(c) are the visualization results
obtained by Sparse-LLE, which may be interpreted as
follows. We can recognize four pair of opposite branches
in the embedded space, labeled from 1 to 4. The main
trend of images in each branch is represented in Fig. 3. It
is observed that the main trend in branches 1 and 2
includes left pose or slightly left pose images, whereas
images in branches 3 and 4 are mainly either right pose
or slightly right pose. In particular, while containing
opposite poses, both branches 1 and 3 are similar in that
one of their ends includes happy faces and the other end
includes either sad faces or faces with visible tongue.
As our second experiment, the performance of LLE
and Sparse-LLE is compared in face recognition task on
the extended Yale face database. The dataset includes
2432 cropped frontal images of 38 individuals under
expression and illumination variation [8], where the first
16 images of each individual are considered in this
experiment. After vectoriation, using LLE and Sparse-

Given integers and , with
, solve
for each
x
, and denote the answer by w . Then,
sample x
nodes and in the adjacency graph are connected iff
, is among the top nonzero elements of w .
Figure 1. / -NN criterion for construction of the adjacency
graph.

4. Implementation
In Section 3, / -NN criterion for construction of
the adjacency graph was introduced and justified. In
order to apply this criterion, we shall study the following
s. t. b As
optimization problem:
, , : min s
and Supp s
, where b
and
is a given
T
,…,
. Our
subset of indices of s
implementation assumes
, which fairly happens
almost always in real-world situations. As the starting
point, we first consider the well-known sparse
approximate
representation
problem
s. t. b As
. Among available
, : min s
approaches, we opt for the recently proposed RobustSL0 as a fast and accurate algorithm [5]. Briefly
speaking, Robust-SL0 solves a sequence of problems of
/
s.t. b As
,
the form
, : max ∑
decreasing at each step, and initializing the next step at
the maximizer of the previous (larger) value of . Each
is solved approximately by few iterations of
,
gradient ascent. Convergence analysis of Robust-SL0 has
been thoroughly considered in [5] and it was shown that,
under some mild conditions, the sequence of maximizers
of , indeed converges to the unique minimizer of , ,
394

Table 1. Generalization errors of 1-NN classifiers for different
dimension reduction algorithms.
(a)

Yale face database
Algorithm Parameters

PCA
LLE
SparseLLE

12
5
12

VARIA database

Generalization
Generalization
Parameters
error of 1-NN
error of 1-NN

35.5263
29.9342
23.5197

4
5
7

73.2634
47. 9765
38.1270

dimensional representations is then evaluated similar to
the second experiment (Table 1).

(b)

Conclusions
LLE is a well-known and powerful spectral
dimension reduction algorithm. For successful recovery
of the low-dimensional structure of data, however, LLE
requires an adjacency graph, which is typically
constructed using -NN criterion. In this paper,
deficiencies of -NN for construction of the adjacency
graph were first studied and / -NN criterion was then
introduced to overcome the drawbacks. Implementation
of / -NN involved a variant of Robust-SL0 algorithm
for sparse approximate representation. The modified
spectral method, namely Sparse-LLE, is experimentally
validated on several datasets, demonstrating remarkable
improvement over the conventional LLE.

(c)
Figure 2. A few samples of Frey dataset used in the first
experiment (a). Images of faces mapped into the embedding
space described by the first two coordinates of LLE with
12 (b), and sparse-LLE with
5 and
12 (c).

References
[1] L.K. Saul, K.Q. Weinberger, and J.H. Ham, F. Sha, and
D.D. Lee, “Spectral methods for dimensionality
reduction,” in Semisupervised Learning, O. Chapelle, B.
Schölkopf, and A. Zien (eds), MIT Press, 2006.
[2] S. Roweis, and L. Saul, “Nonlinear dimensionality
reduction by locally linear embedding,” Science, vol. 290,
pp. 2323-2326, 2000.
[3] G. Lebanon, “Riemannian geometry and statistical
machine learning,” Doctoral Thesis, School of Computer
Science, Carnegie Mellon University, 2005.
[4] T. Lin, H. Zha, and S.U. Lee, “Riemannian manifold
learning for nonlinear dimensionality reduction,” in Proc.
ECCV (1), pp. 44-55, 2006.
[5] A. Eftekhari, M. Babaie-Zadeh, C. Jutten, H. Abrishami
Moghaddam, “Robust-SL0 for stable sparse representation
in noisy settings,” Int. Conf. Acoustics, Speech, and
Signal Proc., 2009, Accepted.
[6] L.J.P. van der Maaten, E.O. Postma, and H.J. van den
Herik, “Dimensionality reduction: a comparative review,”
submitted to Neurocomputing, 2009.
[7] M. Bernstein, V. de Silva, J.C. Langford, and J. B.
Tenenbaum, “Graph approximations to geodesics on
embedded manifolds,” Technical Report, Stanford
University, 2000.
[8] A.S. Georghiades, P.N. Belhumeur, and D.J. Kriegman,
“From few to many: illumination cone models for face
recognition under variable lighting and pose,” IEEE Trans.
Pattern Anal. Mach. Intell., vol. 23, pp. 643-660, 2001.
[9] VARIA
database,
available
online
at
http://www.varpa.es/varia.html

LLE, dimension of image data is reduced to 10.
Subsequently, motivated by the well-designed
experimental setup in [6], quality of the resulting lowdimensional representations is evaluated by measuring
the classification errors of 1 nearest neighbor classifiers
trained on the low-dimensional representations using
leave-one-out cross-validation. In other words, class of
each sample is predicted by its nearest neighbor in the
embedded space and the overall classification error is
reported in Table 1.
Retinal biometrics refers to identity verification of
individuals based on their retinal vessel tree pattern (Fig.
4). Our third experiment is conducted on VARIA
database containing 153 (multiple) retinal images of 59
individuals [9]. To compensate for the variations in the
location of optic disc (OD) in retinal images, a ringshaped region of interest (ROI) in the vicinity of OD is
used to construct the feature matrix. To extract the ROI,
using the technique presented in [10], OD and vessel tree
are extracted. Then, a ring-shaped mask with proper radii
centered at OD is used to form the feature vectors
by collecting the pixels along 8 beams of
X
length 6 originating from OD. After vectorization of
feature matrices, dimension is reduced to 10 using LLE
and Sparse-LLE. The performance of the resulting low-

395

w,
, where we have used Lemma 1. Under the
assumption of uniqueness of the answer, it immediately
follows that values of and , as well as w , and w , ,
are closer than any arbitrary positive constant in terms of
Euclidean distance.

[10] Farzin, H., Abrishami, H.: A novel retinal identification
system. EURASIP Jr. on Advances in Signal Proc., 2008.

Appendix
In the following, asymptotic equivalence of and
x is not degenerate, such that
is studied. Suppose
the following optimization problem has the unique
solution w , .
: min x

Theorem 1 demonstrates the asymptotic equivalence
∞.
of and as

Xw

s. t.
Supp w
wT
1

,

Then, we attempt to show the asymptotic equivalence of
and , which is repeated below for convenience.
: min lim
s. t.
w
wT
w,

x

Xw

,

x

x

,
1,
0

Without loss of generality, we assume that columns
of X are normalized to unit ℓ -norm. Without a need for
a rigorous proof, it is observed that
is clearly
Xw
s. t. Supp w
equivalent to min x
1; the solution of which is denoted by
and w T
w , . Before proceeding, we need to define the mutual
coherence
A of matrix A with normalized columns,
as the largest entry in absolute value outside the main
A
1. Now,
diagonal of AT A [5], where obviously
consider the following observation.
with
Lemma 1. Given a symmetric matrix H
diagonal elements equal to unity and off-diagonal
elements less than or equal to in amplitude, the largest
eigenvalue of H does not exceed 1
1 , where
denotes the mutual coherence of H.
Proof. If v is an eigenvector, suppose the largest absolute
v, we
value in v is , occurring at index . From Hv
∑
. Taking into
may write 1
,
account | , |
, we get
for
and | |
1
1
and consequently
1
1 .
w,
is smaller than any arbitrary
Theorem 1. w ,
positive constant.
Proof. First we show that, for any given
0, there
w,
implies
exists
0, such that w
x
Xw
x
Xw ,
, where Supp w
1. Considering the fact that w
and w T
T
|w
0 , Supp w
is an
1
-dimensional linear subspace passing through 0,
#
our claim is shown by choosing
/
,
because
x
Xw
x
1
1
Xw ,
X w
w,
1
1
w
396

1445

1447

1441

638

639

634

436

435

429

1443

1440

1439

630

631

628

437

434

433

1489

1498

1503

645

629

635

430

431

432

1101

1787

1767

1789

1788

1779

1765

1766

1778

1469

1470

1448

1482

1453

1454

1455

1456

1458

1188

1187

1

1175

4
1179

978

1183

1176

983

1185

2

3
3
2
1

445

444

443

448

447

442

449

4

1867

1866

419

446

1652

423

1594

450

425

126

1607

Figure 3. Further study of the embedded space obtained by sparse-LLE in Figure 2(c). Outer boxes are positioned similar to the
distribution of branches in Figure 2(c), where label of corresponding branches are indicated by the arrows. Each outer box contains few
samples of the corresponding branch, which are selected to represent the main trend of the images inside the branch.

397

