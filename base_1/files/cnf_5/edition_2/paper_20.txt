2009 Sixth International Conference on Computer Graphics, Imaging and Visualization

Providing novel and useful data for game development using usability expert
evaluation and testing
Yong Jun Choi
Department of Image Engineering, the Graduate School of Advanced Imaging Science, Multimedia & Film. Chung
Ang University, Seoul, Korea
Telephone: 82-2-82-5771, Fax: 82-2-814-5404
yongjc0602@yahoo.com

can be used as a measure of the face validity of the
methods. Face validity is a crude measure and does not
tell everything about the validity of the usability expert
evaluation and testing in game development. In the
current situation, however, face validity is an interesting
issue and well worth studying.
Desurvire, et al. [2] studied the validity of expert
evaluation. In the expert evaluation the experts evaluated
a game using a list of heuristics specifically developed
for the evaluation of video, computer and board games.
They studied the validity by comparing the results of
expert evaluation to the results gained through a usability
test. They found that the expert evaluation was “…very
useful for creating highly usable and playable game
design, particularly in the preliminary design phase prior
to expensive prototypes”. Yet they concluded that the
expert evaluation does not replace usability testing as
there is no way of knowing how people really behave.
Medlock, et al [3] studied the validity of the rapid
iterative testing evaluation method. The measures they
used included a web diary of the lead game designer,
game reviews, the awards the game claimed and the sales
figures. Based on this data it was concluded that the
method was highly effective in terms of finding fixing
problems and resulted in positive industry reviews for
the part of the game the method was applied to.
A case study about usability expert evaluation and
testing is not enough to establish the validity and more
studies and growing evidence about the usefulness of
these methods in game development are needed [4].

Abstract
A case study is done to study whether usability expert
evaluation and testing are appropriate for game development.
In this study, it identifies if the usability expert evaluation and
testing provide novel and useful data for game development.
For this experiment, a computer game is first evaluated and
then tested. Then game developers are asked to rate the
findings and give other feedback about the methods used and
the results gained. The usability expert evaluation and testing
have considerable face validity in game development because
the results of a case study were both novel and useful for
game development.
Keywords --- Usability method, expert evaluation,
usability test, computer games

1. Introduction
Both computer games and usability have long
histories, but only recently have these two been
combined. The first reported steps to introduce usability
evaluation methods to game development were taken in
1997 at Microsoft [1]. Since then, other companies have
also adopted usability evaluation methods to their game
development, but not all have been convinced [2]. One
likely reason for rejecting the usability methods is that
the game developers, game producers and marketing
departments may be doubtful or ignorant about the
usefulness of the usability evaluation methods. In this
study it is investigated whether there is a good reason for
this or not. It is studied whether usability expert
evaluation and usability testing provide data that game
developers find novel and useful.
The game developers' perception of the usability
expert evaluation and testing is important for very
practical reasons. If game developers think that usability
expert evaluation and testing do not provide new and
useful data they will not use the methods. The same
applies to the problems found with the methods. If the
game developers do not find the problems plausible they
will choose not to fix them.
The game developers’ perception of the usability
expert evaluation and testing is also important because
the game producers and marketing departments can use
this to make up their minds about whether to use the
methods or not.
The game developers’ perception of the usability
expert evaluation and testing is also interesting because it
978-0-7695-3789-4/09 $25.00 © 2009 IEEE
DOI 10.1109/CGIV.2009.51

2. Method and Process
2.1. Design
A case study is conducted to study the
aforementioned issues. A computer game is first
evaluated and then tested. After this, the game
developers answer a web survey where they rate each
usability problem found on a multidimensional scale.
They also give general feedback about the process.

2.2. Participants
The usability expert evaluation is conducted by six
usability specialists. Four of them are classified as
gamers and two as non-gamers.
129

evaluation took varies from two to four hours per
specialist.
After the evaluation, the specialists present their
findings to the evaluation leader and discuss the reasons
behind the problems, severity classifications and the
possible solutions. The leader then collects the problems
to a single list. Once the list is ready the problems are
grouped within predefined categories. After the
categorization, similar problems within each category are
grouped together. This categorized and grouped list serve
as the basis for the final report which is written by the
lead specialist.
In the final report each problem has a title, severity
classification, detailed description of the problem and
suggested solution. A five step scale ranging from
cosmetic to catastrophic is used to rate the severity of
each problem. There are additional categories for the
technical problems and the problems that cannot be
classified. The final report is then delivered to the game
developers, and a results meeting is held where the key
findings are presented and discussed.

The usability test is conducted by one usability
specialist and an assistant. In the usability test there are
six test users. They represent the three target groups of
the game.
The two game developers who answer the web
survey are the lead designer and the project manager.
They are also the contact persons on the game
developers’ side, and the results of both the usability
expert evaluation and test are reported to them.

2.3. Materials
The game evaluated and tested is a computer game
called Midnight Strike. Midnight Strike is an action
game viewed from the top-down perspective.
The usability test is conducted in a standard
usability laboratory. The developers have the opportunity
to observe the tests in a separate room.
In the web questionnaire, seven questions address
each problem found in the usability expert evaluation
and testing. The first two questions measure the novelty
of the problem and its description. The following three
questions are about the relevancy of the problem,
accuracy of the severity classification and usefulness of
the suggested solution. The remaining two questions
probe whether corrective actions are to be taken to fix
the problem and is the problem due to a programming
error. Additional seven open-ended questions are asked
after the problem specific questions. The questions
address the pros and cons of the methods and how to
improve them.

2.4.2. Procedure - Usability test
The usability test is conducted a week after the
usability expert evaluation have been reported to the
game developers. The same version of the game is used
as in the expert evaluation. The usability test is
conducted by one usability specialist and an assistant.
In the usability test the six participants are run
individually. The test is done in a standard usability
laboratory. Each session start with an introduction where
the user is told the basics about usability testing. After
the introduction the user is taken to the laboratory and is
briefed about the game. In the briefing the game’s
background story is told and the setting where the game
started is explained. If the user has no questions the test
begins.
The test consists of two parts. In the first part the
user gets acquainted with the controls and interacting
with the game environment. In the second part the user
plays the game for 1.5 hours.
After the test the user fills in a questionnaire
measuring usability and user experience issues. In total,
each session lasted for approximately two hours.
The think aloud method is used in the test. The user is
instructed to tell what s/he was doing and why. The user
is also encouraged to tell if s/he did not understand
something plus any positive or negative thoughts that
came to his/her mind. During the test the instructor
interrupts the player every now and then either with a
question or to give the user a task.
Each test is recorded, and the recordings are
reviewed. When reviewing the tapes the usability
problems are written down. The analysis and reporting of
the problems is similar to the usability expert evaluation.

2.4. Procedure
The process begins with an initial meeting where
game developers present the game to the usability
specialist who led both the usability expert evaluation
and testing. In the presentation, the focus is on
pinpointing what is supposed to be challenging to the
gamers and what is not supposed to be a challenge.
The goals of the usability expert evaluation and testing
are also defined in the meeting.
2.4.1. Procedure - Usability expert evaluation
First, the six usability specialists evaluate the game
independently of each other. The specialists play the
game and write notes on the usability issues they found
while playing. The findings are based on the Nielsen’s
usability heuristics [5] and the specialists’ knowledge
and experience in human computer interaction. The
specialists are told to evaluate the game like any other
software product. No heuristics specific to computer or
video game are used. Neither is any specific instructions
given on what to focus in the game. Before the usability
specialists start to evaluate the game they are instructed
how to play the game and remind that in games some
issues are supposed to be challenging whereas everything
else should be as easy as possible. The time the

2.4.3. Procedure – Web survey
Two weeks after the final meeting with the game
developers, they are sent a link to the web survey. Two
game
developers
answer
the
questionnaires
130

independently of each other. In the instructions they are
encouraged to be critical about the methods and the
results.

39% (N=105) were new out of all the problems
found. Neither of the game developers knew about them
prior to the usability expert evaluation and test. The
remaining 61% (N=143) were such that at least one
developer had prior knowledge of it.
The game developers were asked if they
believed they would have found the problem without the
usability methods. For 69% (N=84) of these problems
both of the game developers answered that they would
not have found the problem.
The game developers were asked to rate each
problem for relevancy. The mean rating was 3.51 (SD =
1.53) on a scale of 1 (not relevant at all) to 5 (very
relevant). The result is presented in Table 2.

Usability problem
- Title: No feedback is provided if the player cannot
move an item.
- Severity: Severe
- Description: It occurs occasionally that the player
cannot move an item because there is not enough room.
If this occurs, the user is not provided any feedback.
It is big problem as the user may not understand why
s/he cannot move an item. Although the user will find
out eventually, but the confusion and extra effort
required are likely to cause frustration.
- Solution: Providing the user appropriate feedback in
every situation where the user interacts with the
environment. If an item cannot be moved, inform the
user of this with a sound and/or textual feedback.
Questions about the problem
-Did you know this problem before the usability expert
evaluation / test?
Y
N
-If yes, Did the problem description bring you relevant
new information about the problem? Y N
-If no, Would you have found the problem without the
usability expert evaluation / test? Y N
-How relevant was the problem found?
Not relevant at all 1 2 3 4 5 Very relevant
-How accurate was the severity classification?
Not relevant at all 1 2 3 4 5 Very relevant
-How useful was the suggested solution?
Not relevant at all 1 2 3 4 5 Very relevant
-Did you take corrective actions due to the problem?
Y
N
Was the problem caused by a bug in the code?
Y N

1
2
3
4
5
Expert
9%
18%
30%
37%
6%
evaluation
Usability
6%
22%
26%
33%
13%
test
Total
7%
20%
28%
35%
10%
Table 2. The distribution of the relevancy ratings the
game developers gave to each problem found.
When asked whether corrective actions were to
be taken to fix the problems found in the usability expert
evaluation and testing, the answers were as follows. Both
of the developers answered yes for 29 %( N=71 ) of the
problems. One developer answered yes and the other
answered no for 43% (N=106 ). Both developers
answered no for the remaining 28 % (N=71 ) of the
problems.
89 % (N= 220) of the problems were not
because of a programming error according to the game
developers. When asked how useful the suggested
solutions were the mean was 3.88 (SD =1.15) on a scale
of 1 (not useful at all) to 5 (very useful).
The game developers’ answers to the openended questions about applying the usability methods to
game development are as follows. Most of them think
that the findings helped improve numerous small details
in the game and to avoid a couple of potential pitfalls in
designing and implementing new features. One
developer thinks that this report clarified a lot of design
issues that were known to be problematic already in the
design phase. Another developer thinks that discussing
already known issues in more detail before the
evaluation work would save time.

Figure 1. The feedback about the novelty and
usefulness of the individual problems found was
gathered with a web questionnaire. There is an
example of a usability problem found and way
problems were reported in the upper part of the
figure.

2.5. Results
The number of problems found is summarized in
Table1. The same table also shows the distribution of the
problems by severity
Cosmetic Minor Intermediate
Usability
2
35
76
expert
evaluation
Usability
1
29
54
test
Total
3
64
130
Table 1. The total number of usability
found and their distribution by the
classification.

Severe
30

3. Discussion
21

It can be conclude that usability expert
evaluation and testing provide both novel and useful data
for game developers based on the results. 39% of all the
usability problems found were new to the game
developers, which supports the view that the results were
novel. Another novel finding is that game developers

51
problems
severity

131

would not have found 69% of the new problems without
the help of the usability methods used.
The view that the data was useful is supported
by three findings. First, the mean relevancy rating for the
problems found was 3.51, which can also be considered
good. Second, only 28% of all the problems found were
such that the game developers had no intention to
address them. Third, the game developers rated the
usefulness of the suggested solutions high; the mean was
3.88. These findings indicate that the results are useful
for the game developers.
The view that the methods are valid is
supported even further by the finding that the game
developers reported that 89% of the problems found
were not due to a programming error. This is crucial
because the usability expert evaluation and test are not
supposed to replace the conventional quality assurance
methods which are used for finding and fixing bugs.

Acknowledgements
This research was supported by the ITRC
(Information
Technology
Research
Center,
MIC) program and Seoul R&BD program, Korea. This
work also supported by the third phase of the Brain
Korea 21 program in 2009.

References
[1] B. Fulton, R. Romero, User-testing in a Hostile Environment:
Overcoming Resistance and Apathy in your Game Company. Presented
at the Game Developer’s Conference, San Jose CA, March 2004.
[2] H. Desurvire, M. Caplan, J. Toth, Using Heuristics to Improve the
Playability of Games. CHI Conference, 2004, Vienna Austria (In the
collection of Abstracts). 2004.
[3] M. C. Medlock, D. Wixon, M. Terrano, R. Romero, & B. Fulton,
Using the RITE Method to improve products: a definition and a case
study. Usability Professionals Association, Orlando FL July 2002.

Conclusions

[4] B. E. John, A case for cases. In Commentary on "Damaged
Merchandize?" Human-Computer Interaction 13(3), 289-296. 1998

The usability expert evaluation and testing can
provide both novel and useful data for game
development. It can be concluded that expert evaluation
is a fast and effective way to check the usability of a
game. It can also be argued that the usability expert
evaluation and testing have considerable face validity in
game development. The game developers reported that
the usability expert evaluation and test helped them to
improve numerous details in the game, avoid potential
pitfalls when developing new features and to solve issues
that they knew were problematic. The game developers
also found observing the usability test informative
because it gave a real life example how the gamers really
play the game. These positive comments suggest that the
usability expert evaluation and testing have face validity
in game development. Another important and interesting
point is that the reported problems were not due to
programming mistake.
For future studies, studying how the game user
research specific experience affects the quality of the
problems found, comparing the usability methods to the
traditional quality assurance methods used in game
development can be considered. Other game genres and
other usability evaluation methods should be investigated
too.

[5] J. Nielsen, Usability engineering. San Francisco: Morgan
Kaufmann

132

