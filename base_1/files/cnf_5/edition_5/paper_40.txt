Gray Image Recognition Using Hopfield Neural Network
With Multi-Bitplane and Multi-Connect Architecture

Kussay N. Mutter

Imad I. Abdul Kaream
Hussein A. Moussa
Al-Mustansiryah University
College of Education
Baghdad, Iraq.
2006
E-mail: kussaynm@yahoo.com

Abstract
In this work, a method for applying Hopfield Neural
Network (HNN) with gray images is presented.
Hopfield networks are iterative Auto-Associative
networks consisting of a single layer of fully connected
processing elements thus categorizes as an associative
memory. Associative memories provide one approach
to the computer-engineering problem of storing and
retrieving data which is based on content rather than
storage address. HNN deals with the bipolar system
(i.e. -1 and +1) for direct input data, however it is
useful for binary images, but unuseful for gray-level or
color images unless we suppose another way for input
data of such images. To overcome this obstacle, one
can suppose for 8-bit gray-level image that consists of
8-layers (bitplanes) of binaries can be represented as
bipolar data. In this way it is possible to express each
bitplane as single binary image for HNN. The
experimental results showed the usefulness of using
HNN in gray-level images recognition with good
results. Furthermore, there are no limitations to the
number of 8-bit gray level images that can be stored in
the net memory with the same efficient results.

1. Introduction
With the advent of advanced computer technology
few decades ago, automatic pattern recognition is
being increasingly adopted to provide positive
recognition with a high degree of confidence.
Therefore, different techniques have been proposed for
computer classification and recognition of different
patterns.[9]

Artificial neural networks are programs containing
elements that behave somehow like the nerve cells in
the brain. They have been widely applied to fields such
as pattern recognition because of their ability to solve
cumbersome or intractable problems by learning
directly from data. Moreover, it can deal with
information which is noisy, inconsistent, vague, or
probable [5], [8].
In this work we will try to use a type of neural
networks described by J.J. Hopfield in 1982. Hopfield
is a physicist and was working on the magnetic
behavior of solids (spin glasses). This is essentially
determined by the Ising-spin, a property of magnetic
atoms, which is described by two states (+1 and –1).
The interesting part is the magnetic mutual exchange
between the atoms, and this can be described by a
mathematical formula, which finally leads to Hopfieldnetwork [6].

2. Hopfield Neural Network Architecture
Hopfield networks are iterative AutoAssociative networks consisting of a single layer of
fully connected processing elements, thus categorize as
an associative memory. An expanded form of a
common representation of the Hopfield net is shown in
figure (1). All the processing elements (neurons or
nodes) are connected in feedback architecture with the
connection weights specified in a certain way.
Moreover, this net is fully interconnected, that is to

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

say, each unit (node) is connected to every other unit,
and has feedback connections between the units [1].

ª w i1 º
«w »
« i2 »
« . »
Where: w i = «
» is the weight vector containing
« . »
« . »
« »
¬« w in ¼»
weights connected to the input of the i’th neuron, and:

Figure (1): Hopfield net (single-layer) [3].
The net consists of n neurons having
threshold values Ti. The feedback input to the i’th
neuron is equal to the weighted sum of neuron outputs
vj, where j=1,2…n. Denoting wij as the weight value
connecting the output of the j’th neuron with the input
of the i’th neuron. The total input neti of the i’th can be
expressed by:

ª v1 º
«v »
« 2»
«.»
v = « » is the neural network output vector. The
«.»
«.»
« »
¬« v n ¼»
complete matrix description of linear portion of the
system is given by:

net = Wv + i − t --------------(3)

n

net i = ¦ wij v j + ii − Ti

For

j =1

Where:
i=1,2, … n ------------(1)

The external input to i’th neuron has been denoted here
as ii. Introducing the vector notation for synaptic
weights and neuron output, equation (1) can be
rewritten as:

ª net1 º
« net »
« 2»
« . »
net = «
»,
.
«
»
« . »
«
»
«¬net n »¼

ª i1 º
«i »
« 2»
«.»
i=« »
«.»
«.»
« »
«¬i n »¼

are

vectors

containing activations and external inputs to each
neuron, respectively [12].

The threshold vector t has been defined here as:

net i = w i v + i i − Ti
i=1,2…n ------------------(2)

For

ª T1 º
«T »
« 2»
«.»
t=« »
«.»
«.»
« »
«¬Tn »¼

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

Matrix W, sometimes called the connectivity matrix, is
an n×n matrix containing network weights arranged in
rows of vectors, and its expanded form is given by:

ª 0
«w
« 21
« w 31
«
W=« .
« .
«
« .
«w
¬ n1

w12

w13

0

w 23

w 32

0

.

.

.

.

.

.

wn2

wn3

. . . w1n º
. . . w 2n »
»
. . . w 3n »
»
. . .
. »
. . .
. »
»
. . .
. »
. . . 0 »¼

wij: the weight from the output of neuron i to the input
of neuron j.

t: limiting (threshold) value, which equal to zero in
Hopfield network.

In the vector form the energy function is written as:

1
E = − ⋅ v ⋅ (W ⋅ v ) + t ⋅ v
2
or with t=0 we have:

Note that the weight matrix W in this model is
symmetric, i.e., wij= wji, and with diagonal entries
equal explicitly to zero, i.e., wii= 0. In other words, no
connection exists from any neuron back to itself.
Physically, this condition is equivalent to the lack of
the self-feedback in the nonlinear dynamical system. If
the diagonal elements were not 0, the net would tend to
reproduce the input vector rather than a stored vector
[12], [1].

3. The Energy Function
If a Hopfield NN is given by weights and
limiting values, then the network will be in dynamic
equilibrium when one creates a pattern. A network can
define various patterns; one can find them by different
start vectors in the iteration. Corresponding to the spinglass theory of solid state physics, such equilibrium
functions in Hopfield networks are characterized by the
fact, that the total energy (Hamilton function) becomes
minimum. This leads here to a “Lyapunov function or
energy function”, which becomes exactly minimum,
when one creates a pattern. This energy function can
be defined as follows [6]:

n
·
1§ n n
E = − ¨¨ ¦ ¦ w ij v i v j + ¦ t k v k ¸¸
2 © i =1 j =1
k =1
¹

Where

………(4)

n: the number of elements in the vector v.

1
E = − ⋅ v ⋅ (W ⋅ v ) ………(5)
2
One can calculate the energy function E for every input
vector, which can be created in the network. If one
calculates the function E for all the possible input
vectors, an energy landscape with maximums and
minimums can be obtained. The point is that the
minimum is taken when the input is a pattern.

4. Hopfield NN’s Implementations and
Limitations
The main application of Hopfield NN is in pattern
recognition that is able to recognize correctly unclear
(noisy) pictures [6]. However, there are many
limitations associated with Hopfield NN that can be
discussed briefly here.
Associative memories provide one approach
to the computer-engineering problem of storing and
retrieving data are based on content rather than storage
address. Since information storage in a neural net is
distributed throughout the system (in the net’s
weights), a pattern does not have a storage address in
the same sense that it would if it were stored in a
traditional computer. An associative memory net may
serve as a highly simplified model of human memory.
Before training an associative memory neural net, the
original patterns must be converted to an appropriate
representation for computation. However, not all
representations of the same pattern are equally
powerful or efficient. In a simple example, the original

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

pattern might consist of “on” and “off” signals, and the
conversion could be “on”Æ +1, “off”Æ 0 (binary
representation) or “on”Æ +1, “off”Æ -1 (bipolar
representation) [3].

5. Gray-Level images recognition via
HNN

Bitplane
8

1

1

0

0

1

1

1

0

1

1

0

1

0

1

0

1

.
.

1
1
1

.

1

0

0

1

1

0

0

0

1

0

0

1

1

1

1

0

1

1

0

1

1

0

0

0

1

In this work we will try to adapt HNN for GrayLevel images recognition, but before going on further
into details we needs to show some concepts about
image representation.
Image representation is concerned with the
representation of input data, which can be measured
from the objects that can be recognized. The input data
is acquired by digitizing images, which are obtained
through scanner devices [10], [2].
The digital image is represented as twodimensional array of data I(x, y), where each pixel
(picture element) value corresponds to the brightness
of the image at the point (x, y). In linear algebra terms,
a two-dimensional array is referred to as a matrix, and
one row (or column) is called a vector. This image
model is for monochrome (one-color, this is what we
normally refer to as black and white) image data, but
normally we have other types of image data, these are
color images. Gray-level images are referred to as
monochrome images, and they contain brightness
information only, i.e. no color information. The
number of bits used for each pixel determines the
number of different brightness levels. Binary images
are the simplest type of images and can take one of two
values, typically black and white, or ‘0’ and ‘1’. A
binary image is referred to as 1 bit/pixel image because
it takes only 1 binary digit to represent each pixel.
These types of images are most frequently used in
computer vision applications where the only
information required for the task is general shape, or
outline information.[11].
Now, as mentioned previously, HNN deals
with the bipolar system (i. e. -1 and +1) for direct input
data, so it is useful for binary images, but unuseful for
gray-level or color images unless we suppose another
way for input data of such images.
To overcome this obstacle, one can suppose
that 8-bit gray-level image consists of 8-bitplanes of
binaries, each can be represented as bipolar data. In
this way it is possible to express each bitplane as single
binary image for HNN, see fig.(2). Hence, the 8-bit
image consists of 8-bitplanes or 8-subimages arranged
systematically in its proper index in the net memory
(i.e. the net data base).

Bitplane
2

Bitplane
1
Fig.(2) Bitplanes of 8-bit gray level
image.
In this work, multi-connection net of HNN will be used
for learning and converging, as shown in fig.(3)

Figure (3) Hopfield NN with multi-path
architecture.
The net for bitplane k consists of 3 neurons.
The feedback input to the i’th neuron is equal to the
weighted sum of neuron outputs vj. wij is the weight
value connecting the output of the j’th neuron with the
input of the i’th neuron. The total input neti of the i’th
will be:

n

net = ¦ wikj v j + ii
k
i

-------------(6)

j =1

Note that we used the number of neurons here n=3,
which is the simplest and efficient form of the HNN as
used by [1]. Moreover, the number of neuron
connections (paths) in this case will be 23=8, see the
above figure.
Therefore, each bitplane in the input image
can be expressed by blocks of 3-elements, as shown in
fig.(4). The blocks are representing as input vector for
learning process. The weights that results from the
learning process will look like fig.(5). Hence, each

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

input image consists of 8-bitplanes correspond to 8bitplanes of weights. There is no limitation for the
maximum number of stored weights in HNN as it is
proofed empirically.
1

1

0

0

1
Bitplane 8

Bitplane 8

1

1

0

1

1

0

1

0

1

1

1

0

1

.

0

0

ik j

v j + ii

0

0

0

0

0

1

1

1

0

1

1

0

1
1
Bitplane 1

0

0

0

1

0

1

0

1

0

1

1

1

0

0

1

1

.

1

0

0

0

1

0

0

1

1

1

1

0

1

1

0

1

1

0

0

0

1

W3

W4

W5

W6

W7

W8

W9

W10 W1 W11W2 W12
W3

W13

W14

Fig.(6) Bitplanes of 8-bit gray level
unknown image.

6. The Energy Function
The energy function for the new form of HNN
can be rewritten as follows:

·
1§ b n n
E = − ¨¨ ¦ ¦ ¦ wij v i v j ¸¸ -----------(8)
2 © k =1 i =1 j =1
¹

------------(7)

W2

.

1

Bitplane 1

W1

.

0

1

In the same way, an unknown 8-bit gray level
image can be considered as 8-layres each consists of
vectors (blocks) of 3-elements, see fig.(6). Therefore,
taking each vector in each bitplane with the correspond
weight in the net memory; one can get converging
results in same manner that has been done as in binary
images.

.

1

1

j =1

Bitplane 8

1

.

We can generalize the above formula for any number
of gray-level images k=1, 2, .., b as follows:

¦w

1

1

Fig.(4) Blocks in bitplanes
(bolded border).

n

0

Bitplane 2

1
Bitplane 2

net ik =

0

.

1

1

.

1

1

1

.

1

W4

W15
W16
W5
W6
W7

W8

W9

W10

W11

W12

W13

W14

W15

W16

Where n: the number of elements in the vector v.
wij: the weight from the output of neuron i to the input
of neuron j.
k: the number of bits in the gray-level image.
The values of the energy function E for all the
possible input vectors can be calculated here for each
bitplane, and then an energy landscape with maximums
and minimums can be obtained.
In the next section we will illustrate the
comparison between the input learned images and the
converged images with the difference criteria between
them, where the empirical tests of number of 8-bit gray
images show very good results indicate the range of
possibility of using HNN in b-bit gray image
recognition.

Bitplane 1

7. Results
Fig.(5) Saved weights in database

Empirically, a lot of different 8-bit gray-level
images are applied for learning and converging by the
new HNN algorithm. Table(1) shows the converging
ratios of three chosen images that correspond to the
noise ratios acted randomly on 256 gray levels within
images canvas. As can be seen from converging ratio

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

Converging Ratio (% )

values, the noise ratio values are increasing at the
beginning with the converging (restoring) ratios till
(50 %), as average of all the taken images, and
decreasing for each increment in the noise ratio.
Figures(7), (8), and (9) illustrate the relationship
between the noise ratio and the ratio of converging
images, where the doted line represents the real results
of the HNN output, while the solid line is the fitting
function which takes here the following form:

25.00
20.00
15.00
10.00
5.00
0.00

f(x)=-ax2+bx+c -------- (9)

Converging Ratio (%)

Converging Converging Converging
Ratio for
Ratio for
Ratio for
Image-1
Image-2
Image-3
5
6.75
6.54
7.16
10
6.08
9.46
5.03
15
12.97
11.91
12.28
20
14.81
14.10
6.31
25
17.03
16.25
15.56
30
18.00
15.93
6.13
35
19.51
18.04
18.48
40
19.21
17.22
4.45
45
19.86
18.53
18.96
50
18.96
18.30
19.35
55
19.74
18.32
18.32
60
18.43
16.71
16.83
65
19.33
16.60
18.02
70
17.08
14.67
17.93
75
17.72
16.21
16.67
80
16.23
15.38
14.74
85
14.00
14.23
14.78
90
13.66
11.85
12.76
95
14.26
12.99
13.98
Table (1): Converging results for three Gray-Scale
Images

Noise
Ratio

20

40
60
Noise Ratio (%)

80

100

Figure(7): Converging results for image-1
20.00
15.00
10.00
5.00
0.00
0

20

40

60

80

100

Noise Ratio (%)

Figure(8):Converging results for image-2

25.00

Converging Ratio (%)

Where f(x): represents the converging ratio of the
fitting
x: is the noise ratio
a, b, and c: are constants.

0

20.00
15.00
10.00
5.00
0.00
0

20

40
60
Noise Ratio (%)

80

100

Figure (9): Converging results for
image-3

8. Conclusions
Many conclusions are found according to the
results noted bellow:
1. The usefulness of using HNN in gray-level images
recognition which gives good results.
2. Rising the noise ratio to about 50 % as average of
all learned images, will cause an increasing in
restoring ratio (converging ratio). The latter will

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

3.

4.

decrease when increasing in noise ratio. This
relationship looks like the equation (9).
In this work , there is no limitation to the number
of 8-bit gray level images which can be stored in
the net memory with the same efficient results.
Using multi-bitplane gray-level images never
affects the net efficiency because each image is
considered as binary sub images (bitplanes) stored
as weights by learning process in the net database
memory.

9. Future Work
The followings are directions for further works may
develop HNN in gray and color images:
1. Using HNN for color images.
2. Developing HNN for gray images taking into
accounts the variant of Brightness and Contrast.
3. Using HNN for gray-level fingerprint images
classification, which includes information more
than binary fingerprint images.

10. References
[1] Imad Issa Abd Al-Kaream, “Hopfield Neural
Network Using Genetic Algorithm”, M.Sc. thesis,
High studies institute for computer and information,
Iraq, 2000.
[2] David H. Chang, “Fingerprint Recognition Through
Circular Sampling”, Center for Imaging Science,
Rochester Institute of Technology, May 1999.

[5] L. C. Jain and R. K. Jain, “Hybrid Intelligent
Engineering Systems”, World Scientific publishing Co.
Pte. Ltd., 1997.
[6] Werner
Kinnebrock,
Fundamentals, Applications,
publications, Pvt. Ltd., 1995.

“Neural Network,
Examples”, Galotia

[7] Domagoj Kovacevic and Sven Loncaric, “Ct Image
Labeling Using Hopfield Neural Network”,
Department of Electronic Systems and Information
Processing, Faculty of Electrical Engineering and
Computing, University of Zagreb, Croatia, 1998.
[8] Cornelius T. Leondes, “Algorithms and
Architectures, Neural networks systems techniques and
applications”, Academic press, 1998.
[9] Kussay N. Mutter, “Hopfield Neural Network and
Genetic algorithm Techniques for Fingerprint Image
Identification and Reconstruction”, M.Sc. thesis, AlMustansiryah University, College of Education,
Department of Physics, Iraq, 2002.
[10] Julius T. Tou and Rafael C. Gonzalez, “Pattern
Recognition Principles”, Addison-Wesley publishing
company, 1974.
[11] Scott E. Umbaugh, “Computer Vision and Image
Processing: A Practical Approach Using CVIP Tools”,
Prentice Hall PTR, 1998.
[12] Jacek M. Zurada, “Introduction to Artificial
Neural Systems”, Jaico publishing house, 1996.

[3] Laurence Fausett, “Fundamental of Neural
Networks,
Architectures,
Algorithms
And
Applications”, Prentice-Hall, 1994.
[4] Rafael C. Gonzalez, and Richard E. Woods,
“Digital
Image
Processing”,
Addison-Wesley
publishing company, 1992.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

