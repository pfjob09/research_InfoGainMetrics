Visual Registration of Industrial Metallic Object from Local Contour
Descriptors
Steve Bourgeois1,2 , Sylvie Naudet-Collette2 , Michel Dhome1
1
LASMEA - CNRS UMR 6602 - Blaise Pascal Iniversity- France
2
CEA Saclay - DRT/LIST/DTSI/SARC/LCEI - France
steve.bourgeois@cea.fr, sylvie.naudet@cea.fr, michel.dhome@lasmea.univ-bpclermont.fr
Abstract
This article introduces an innovative visual registration process suitable for textureless objects. Because our
framework is industrial, the process is designed for metallic, complex free-form objects containing multiple bores.
This technique is based on a new contour descriptor,
invariant under afﬁne transformation, which characterizes
the neighborhood of a closed contour. The afﬁne invariance is exploited in the learning stage to produce a lightweight model. Moreover, during the learning stage, this
descriptor is combined to a 2D/3D pattern, concept likewise presented in this article. Once associated, the 2D/3D
information wealth of this descriptor allows a pose estimation from a single match. This ability is exploited in the
registration process to drastically reduce the complexity of
the algorithm and increase efﬁciently its robustness to the
difﬁcult problem of repetitive patterns.
Evaluations on a cylinder head, a car door and a binding beam conﬁrm both the robustness and the precision of
the process.
Keywords— Visual Registration, Robust Registration, Industrial Object, Contour descriptor

1 Introduction
Visual registration of 3D objects from a single view,
which consists in computing the pose of the 3D object w.r.t.
the camera, is crucial to many computer vision applications: augmented reality, conformity control, wear control,
robotics, etc. Although such registration has been the topic
of extensive research in the last few decades, it is still a
burning issue.
The ability of existing solutions to successfully register a given object depends on the nature of that object (on
whether or not it is textured, reﬂective, polyhedral, etc.)
and of its environment (lighting conditions, cluttered environment, partial occlusions, etc). In this article, we address the problem of visual registration for textureless 3D
1 Repetitive

objects.
During the last decades, different approaches were explored such as alignment [17, 9, 7], appearance based
[16, 14] or geometric invariants [18] techniques. Nevertheless, when applied to real scenes, these methods have
some limitations. With alignment approaches, the exploration of all possible correspondences become computationaly prohibitive when the image and/or the model is
complex. Faster, appearance based techniques generally
cannot deal with occlusions and cluttered environments.
Geometric invariants are sensitive to missing or imprecise primitives. During the last years, texture-based local descriptors [10, 12, 8, 15] and local shape descriptors
[19, 1, 2, 13] approaches have demonstrated impressive results. The ﬁrst ones, because they are designed for textured object, don’t suit metallic object (metallic objects are
poorly textured and specular reﬂections disturb the object
appearance). Nevertheless, while adapted to poorly texture
objects, local shape descriptors are generally not invariant
under afﬁne transform. This weakness decreases their robustness to viewpoint changes.
The proposed approach is based on an afﬁne-invariant
local shape descriptor (texture instabilities induced by
specular reﬂections prevent the use of texture-based local
descriptors). While the descriptor signature computation is
similar to the Shape Context [1], our approach exploits a
different notion of locality which make the descriptor invariant under afﬁne transform. An inspection of Fig. 1 is
sufﬁcient to reject points of interest and commonly-used
contour-based features (inﬂexion points, extremal curvature points, etc.) : contours are smooth and generally
closed (due to bores). This report motivates the choice of
closed-contour as the characteristic feature. The resulting
afﬁne-invariant descriptor is described in section 2. The
learning stage, which associates a 2D/3D pattern to each
descriptor and produces the object model , is explained in
section 3. A matching process, which handle repetitive patterns1 (cf. ﬁg. 1) is exposed in section 4. Finally, a pose

pattern : a 3D structure which is repeated on the object (eg. bore series)

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

computation process, robust to false matches and repetitive
patterns, is explained in section 5. The full process is then
evaluated on a car door, a binding beam and a automotive
cylinder head in section 6.

whatever the afﬁne transform applied to the associated Ng
[21]. Therefore, if we consider an afﬁne camera, the NgS
is invariant to arbitrary viewpoint changes. The signature
itself is similar to the Shape Context (or SC) [1]. The problem of scale factor determination, and more generally the
problem of afﬁne invariance of the SC, is solved by computing the signature over the NgS. Therefore SC divide the
NgS into N concentric rings and M angular sectors. For
each sector, the number of contour pixels is estimated and
stored in the N × M matrix. This matrix encodes the spatial distribution of contours in the region.

Figure 1: The Automotive cylinder head with its repetitive
patterns. Inside colored ellipses : examples of repetitive
patterns.

2 An Afﬁne-Invariant Local Contour Descriptor
A local descriptor is constituted of a feature, a neighborhood that surround this feature and a signature that characterizes this neighborhood. In this section, we introduce a
local contour descriptor which characterizes the neighborhood of closed-contour.
The feature used in this descriptor is a closed contour.
Closed contours are extracted from contours detected with
a Deriche contour extractor [4]. This extraction process is
improved by locally ﬁltering the contour orientation with
an PCA before the non-maxima contour suppression step.
Because this modiﬁcation prevents contour split at high
curvature points, the number of closed-contour extracted
is increased.
The neighborhood associated to a closed contour is a
neighborhood ellipse. It is deﬁned as the ellipse of inertia
[11] of center g computed from the closed contour [6] and
inﬂated by a constant factor. By closed-contour neighborhood (or Ng) is meant the region of the contour image that
is surrounded by the neighborhood ellipse.
This neighborhood ellipse is charaterized by a signature. To obtain an afﬁne invariant signature, the signature is not computed on the neighborhood itself but on its
shape. We deﬁne this notion of neighborhood shape (or
NgS) as an extension of the ”shape of Jordan curve” introduced by Zuliani et al. [21]. This shape is just the image of
the Ng obtained by the transformation F, which transforms
the neighborhood ellipse into a unit circle (see Fig. 2). The
resulting contour image is invariant, apart from a rotation,

Figure 2: Neighborhood and its shape. Vi : neighborhoods
linked by an afﬁne transform Ti . V : Neighborhood shape
of Vi obtained by the transformation Fi apart from a rotation.
Because there is no assumption available for deﬁning
the origin of the angular divisions, the SC does not remain
invariant under rotation : it depends on the choice of this
origin2. The effects of a rotation on the signature can be
summarized in two cases :
1. if the rotation is less than 2π/M , the impact on the
signature is difﬁcult to anticipate,
2. if the rotation is equal to 2kπ/M with k ∈ N , a
circular permutation of the matrix columns appears.
To solve the ﬁrst case, we noticed experimentally that
the mentioned effects were insigniﬁcant when M is large
enough (typically M=64).
We chose to solve the second case at the signature comparison level (cf. section 4).

3 Model and Learning Stage
The object model is created during an off-line learning
stage using a view-sphere of the object and its CAD model.
During this process, the camera intrinsic parameters and
the object pose are known for each view of the sphere.

2 Various solutions consists in using the principal orientation of the Ng [10] or extremal points [15]. Because bores are usually circular, it’s difﬁcult to
ﬁnd extremal points or a principal orientation. Thus, these these solutions are not adapted to our context.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

The learning process consists in creating the model,
which is the collection of descriptors {Mj }j≥0 extracted
from the different learning views. Each model descriptor
M is associated to:
• a plane Π
• the 3D center G
• the 2D/3D pattern {((x, y, θ), (X, Y, Z))}
We assume the closed contour of the descriptor belongs to
a plane Π in 3D space. The 3D center of the descriptor
matches up with the 3D point G ∈ Π whose projection in
the view is equal to the neighborhood ellipse center. This
position is used in section 5.2.
To present the 2D/3D pattern, let us introduce the following notations :
• (x , y , θ ) : where (x , y ) is a contour point belonging to a Ng and θ is its normal orientation, with x ,y
and θ expressed in the image coordinate system.
• (x, y, θ) : coordinates and orientation of this same
contour point but in the coordinate system of the
NgS.
• (X, Y, Z) : 3D point of the CAD model which is
projected onto (x , y ).
The 2D/3D pattern, associated with the descriptor
M, consists of the set of pairs ((xj , yj , θj ), (Xj , Yj , Zj ))
where (xj , yj , θj ) belongs to the Ng of M (ﬁg. 4).
In practice, only a subset of these pairs (eg. 20 pairs)
constitutes the 2D/3D pattern. Pairs are selected to maximize the distribution of their 2D coordinates and minimize
the distance of their 3D coordinates from Π. In this way,
the planarity assumption of the Ng described by the 2D/3D
pattern is better veriﬁed. One way of doing so is to group
the pairs according to their 2D coordinates by means of a
bucketing technique : the NgS is divided into M buckets. In
each bucket, only the N (eg. N=3) pairs with minimal distance to Π are kept. Among the N ×M remaining pairs, the
twenty that are closest to Π form the 2D/3D pattern. The
strongest asset of the 2D/3D pattern concept is its invariance under afﬁne transformation. Because 2D positions are
expressed in the coordinate system of the NgS, the 2D/3D
pattern is invariant under afﬁne-camera viewpoint changes.
To emphasize the advantage of this property, we take :
• I1 and I2 are two images of a same scene taken from
different viewpoints with an afﬁne-camera.
• V1 = {(x1 , y1 )} a Ng in I1
• N gS(V1 ) = {(x1 , y1 )} the NgS of V1 .

• {((x1 , y1 , θ1 ), (X, Y, Z))} the 2D/3D pattern associated with the descriptor computed from V1 .
• V2 = {(x2 , y2 )} the Ng corresponding to V1 when
observed from the viewpoint of I2 .
Shape invariance under afﬁne transformation implies the
existence of an invertible transformation A such that
A−1 V2 = N gS(V1 ).
From A, we can determine, for each point (x1 , y1 ) from
the 2D/3D pattern, the corresponding point (x2 , y2 ) in I2
(see Fig. 4) :
(x2 , y2 )T = A.(x1 , y1 )T

(1)

Using the 2D/3D pattern pairs and equation (1), it is
possible to compute the position in I2 of the projection of
each 3D point (X, Y, Z) belonging to the 2D/3D pattern.
These relationships will be used in section 5.1 to compute
a pose from a single descriptor match.

4 Matching Process
The ﬁrst step of the visual registration consists in matching descriptors extracted from the image {Ii } with the descriptors {Mi } of the model. In this section, we introduce
the matching process and explain how the notion of repetitive pattern is integrated in this matching process.

4.1 Descriptor Matching
The matching process consists in selecting for each image descriptor Ii the nearest model descriptor Mi to form
a match < Ii , Mi >. To determine Mi , the distance between the signature of Ii and the signature of each model
descriptor is evaluated. Mi is then deﬁned as the model
descriptor with the minimal signature distance. Because
signatures are deﬁned modulo a circular permutation of its
columns, the distance D used in this process is invariant
under circular permutations. This distance consists of a
series of zero-normalized cross-correlations (ZNCC) with
circular shifts of the signature’s columns. The following
notations are required to deﬁne this distance:
• si : the signature of the image descriptor Ii with Q
angular sectors
• sj : the signature of the model descriptor Mj with Q
angular sectors
• sni : the signature si after n circular shifts of its
columns
• corr(si , sj ) : the ZNCC score between the signature
si and sj .

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

The distance D(Ii , Mj ) corresponds to :
D(Ii , Mj ) = 1 − max (corr(sni , sj ))
n=1..Q

We can notice that D doe not only estimate the distance
between Ii and Mj : it also give us an approximation of the
rotation r that superimposes the shape of Ii on the shape
of Mj . The number of shifts n for which corr(sni , sj ) is
maximal give us an approximation of θ:
r=

2π
arg max (corr(ini , mj )))
n=1..Q
M

false matches (more than 50 %), particularly if we consider
the different hypothesis of each multi-match as individual
match. However, Ψ contains more correct matches than Γ.
With such an amount of outliers, even robust registration technique, such as RANSAC-based solutions [5], will
encounter difﬁculties to reach the solution in an acceptable
time due to the problem complexity. To solve this problem, an innovative pose estimation process is proposed.
This process is designed to be robust to false matches and
to handle the non-unicity of matches.

Let R denote the 2D rotation matrix computed from θ. By
combining R with the transformation F (F links the NgS
of the descriptor Ii to its Ng) the transformation A, that
links the NgS of Mj to the Ng of Ii , can be computed (see
Fig. 4) :
A = F−1 R−1

4.2

(2)

Integration of Repetitive Patterns Knowledge
in the Matching Process

The proposed matching process furnishes a set of
matches Γ = {< Ii , Mi >} and the associate afﬁne
transforms. However, industrial objects frequently contain
repetitive patterns (see Fig. 1). A descriptor Mi , which
characterizes such a pattern, shares the same signature with
a set of descriptors {Mil }l≥1 but not the same 3D position or 2D/3D pattern : these descriptors are equivalent to
Mi . Therefore, the matching process may produce a match
< Ii , Mil > instead of < Ii , Mi >. A matching process
that does not integrate notion of descriptor equivalence will
consider such a match < Ii , Mil > as false match. If the
object contain many repetitive patterns (which is the case
of many industrial objects), Γ will include only few correct
match.
To handle this problem, the ﬁrst step consists in identifying for each model descriptor Mi the set of its equivalent
descriptor {Mil } in the model. {Mil } is determined by
a statistic study of the distribution of distance between
Mi and the remaining model descriptors {Mj }j≥1 . A
model descriptor Mj is equivalent to Mi if the distance
D(Mi , Mj ) is below m − 2σ, where D is the distance
introduced in subsection 4.1, and where m and σ are respectively the median and the median absolute deviation
of {D(Mi , Mj )}j≥1 . This process is illustrated in ﬁg.3.
The second step of our solution consists in replacing
each simple match < Ii , Mi >∈ Γ by a multi-match
< Ii , {Mil }l≥1 >. The problem of false matches is therefore replaced by a problem of non-unicity of the match.
Let Ψ = < Ii , {Mil } >, i = 1..n denote the resulting
collection. Generally Ψ contains an important fraction of

Figure 3: Example of distance distribution between a
model descriptor (M37 ) and the remaining model descriptors {Mj }. Descriptors with a distance below the threshold m−2σ (the red line) are equivalent to M37 . The thumbnail images represent the NgS of these descriptors (in the
green box, the NgS of M37 ).

5 Visual Registration
Visual registration aims to compute the position and
orientation of the oject w.r.t. the camera. Our solution
comprises two steps, an initial coarse pose estimation and
a pose reﬁnement process. The solution proposed in this
section exploits the 2D/3D pattern to compute a pose from
a single match (subsection 5.1). Then, a pose can be hypothetized for each match of Ψ. These poses can be ordered
w.r.t a quality criteria (subsection 5.2) which integrates the
notion of descriptor equivalency (subsection 5.3). The best
pose is then used to initialize a pose reﬁnement process
(subsection 5.4).

5.1 Pose from 2D/3D Pattern
Visual registration solutions based on local descriptors generally require three matches between descriptors
to compute a pose. In this article, the original association of the descriptor with the 2D/3D pattern concept allows computation of a pose P<Ik ,Mk > from a single match
< Ik , Mk >.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

of Ψ for a pose P<Ik ,Mk > is deﬁned by:

Indeed, as already seen in section 3, by determining the
transformation A - which transforms the NgS of a model
descriptor Mk into the Ng of an image descriptor Ik - it
is possible to determine, with eq. 1, the projection (x , y )
of the 3D points (X, Y, Z) from the 2D/3D pattern onto
the afﬁne camera image. A is estimated with eq. (2). Using those 2D/3D couples, the pose P<Ik ,Mk > is computed
with a conventional techniques such as POSIT [3].

EP<Ik ,Mk > (Ψ) =

(4)
where K is the matrix of intrinsic camera parameters, δ
is the robustiﬁed euclidean distance (i.e. the euclidean
distance on which a Tukey estimator is applied), gIi the
center of the neighborhood ellipse of Ii and GMi the 3D
position of Mi .

While coarse pose registration is intended for an afﬁne
camera, it can be extended to a pinhole camera by replacing the afﬁne transformation A with a homography H. H
assure the best ﬁt of the 2D points pj of the 2D/3D pattern
with the image edge. H is initiallized to A and its parameters are tuned with a Levenberg-Marquardt algorithm to
minimize the cost function f :
f (H) =

d(H.pj , p
ˆj)

(δ(K.P<Ik ,Mk > .GMi , gIi ))
<Ii ,Mi >∈Ψ

The pose P is obtained with:
P = arg

EP<Ik ,Mk > (Ψ)

(5)

5.3 Extension to Multi-Matches Case
To handle multi-matches, eq. 4 is replaced, for a simple
match < Ik , Mk >, with :

(3)

j

EP<Ik ,Mk > (Ψ) =

where p
ˆj is the nearest edge point found along the normal
of pj = H.pj and d is the euclidean distance (see Fig. 5).
To increase the robustness of this process, p
ˆj can be
replaced with the collection of edge points found along
the normal. A multiple hypothesis Tukey estimator is then
used to compute the distance in a robust way, like proposed
in [20].

<Ii ,Mi >∈Ψ

min

j
<Ik ,M >
k

In this subsection, let us consider that Ψ contains no
multi-macth. In this case, to determine a coarse pose in a
robust way, a pose P<Ik ,Mk > is evaluated for each match
of < Ik , Mk >∈ Ψ with the process described in section
5.1. The pose that minimizes the reprojection error of Ψ is
then selected and referred to as P. The reprojection error

-1

l

P

Robust Coarse Pose Estimation

A=F R

(min(δ(K.P<Ik ,Mk > .GMil , gIi ))) (6)

If we replace < Ik , Mk > with a multi-match <
Ik , {Mkl } >, rather than a single pose, the set {P<Ik ,M j > }
k
is associated to this match. Consequently, these new poses
are also considered in the registration process and eq. 5 is
replaced by:
P = arg

5.2

min

P<Ik ,Mk >

EP

j
<Ik ,M >
k

(Ψ)

5.4 Pose Reﬁnement
The resulting pose initializes a pose reﬁnement process
based on Levenberg-Marquardt algorithm which tunes the
pose parameters P of the camera to minimizes the reprojection error EP (Ψ) (cf. eq. (6)).

-1

(X,Y,Z)

(x,y)
(x’,y’)
Neighborhood
shape of
the image
descriptor
Image to register

F

Neighborhood
shape of
the model
descriptor

R

CAD Model
2D/3D Pattern

Figure 4: 2D/3D Pattern and its use in pose estimation from a single match. F:
transformation that links the neighborhood to its shape. R : rotation that links
the two shapes. A : transformation that links the NgS of the model’s descriptor
to the Ng in the image.

Figure 5: Search for the nearest edge
point. In black : model projection.
In gray : edges. Dashed arrow : the
normal of p j .

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

6 Results
The process is evaluated through two experiments. The
ﬁrst experiment was lead on three different objects: an
automotive cylinder head, a binding beam and a car door
(see ﬁg. 7). The registration quality is expressed as the reprojection error of the CAD model. Since the ground truth
is known, we can determine, for each pixel (x, y), the 3D
point (X, Y, Z) of the CAD model that is projected onto
(x, y). We can thus compute the projection (x , y ) of this
same 3D point (X, Y, Z) with the estimated camera pose.
The euclidean distance between (x, y) and (x , y ) is the
reprojection error for the pixel (x, y). The reprojection error for the entire image is the mean reprojection error over
all the object pixels, i.e. about 350 000 pixels.
For each object, the learning viewsphere, the graph of
the model reprojection error for the different views, and
some registration results are illustrated in Fig. 7. We obtain 100 % of views correctly registred for the cylinder
head and the car door, and about 90% for the binding beam.
Registration faillures can be explained by pose ambiguities
that are due to the object geometry : for these viewpoints,
visible bores are all aligned along an axis. Therefore, the
pose can only be estimated apart from a rotation around
this axis (see Fig. 6).

Figure 6: Explication of registration faillure : Because visible bores are aligned along an axis (the dashed line in
ﬁg. a), the pose estimation problem is under constrained.
Therefore, the pose is estimated apart from a rotation
around this axis. The backprojection of the model in ﬁg.
b illustrates the resulting registration faillure. Bores are
correctly registered but not the whole object.
This experiment underlines the robustness to viewpoint
changes of the process. With a median reprojection less
than 1 pixel for the binding beam and the car door, and less
than 2 pixels for the cylinder head, this experiment shows
that high accuracy can be reached even with a model composed of only 4 to 6 views.
The second experiment tests the robustness of the
process in uncontrolled conditions. Images used in this

experiment where taken with an other camera which is
coarsely calibrated. Images were taken in conditions signiﬁcantly different with respect to the learned views : important changes in scale and lighting conditions , presence
of partial occlusions and cluttered environment. Results,
shown in Fig. 8, are obtained with the learning view-sphere
of the ﬁrst experiment. Because no ground truth data is
available for this experiment, quality can only be assessed
visually. Even under such extreme conditions, registrations
are successful. The loss of accuracy w.r.t the ﬁrst experiment is attributable to poor calibration of the camera.
Videos of these results can be download at
http://membres.lycos.fr/visualreg/.

7 Conclusions and Future Research Directions
This article proposed an original registration method
intended for fabricated bore-containing metal objects and
more generally object containing closed contours. The
method is based on a new closed-contour neighborhood descriptor which is discriminative and invariant under afﬁne
transformation. This descriptor is associated to a ”2D/3D
pattern” - a concept that we introduced in this article which allows pose computation from a single match between two descriptors.
This article described how a learning step extracts these
elements from a registered view-sphere and forms a model.
Finally, a registration process, robust to occlusions, false
matches and repetitive patterns was introduced. The complete process was evaluated on an automotive cylinder
head, a car door and a binding beam. Experimental results conﬁrmed its robustness to viewpoint changes as well
as its ability to achieve good accuracy with a lightweight
model.
A reﬁnement stage will be necessary to achieve highly
accurate pose registration. The solution proposed by Vacchetti et al. [20] could be adapted for this purpose. For future work, a learning stage based on a synthetic viewsphere
computed from the CAD model is being considered. Robustness of the coarse pose estimation process can also be
increased by considering the N best poses, with respect
to E(Ψ), instead of the best one. An extension to object
recognition is also in perspective.

References
[1] S. Belongie, J. Malik, and J. Puzicha. Shape matching
and object recognition using shape contexts. Technical Report UCB//CSD00, UC Berkele, Jan. 2001.
[2] O.T. Carmichael and M. Hebert. Object recognition
by a cascade of edge probes. In BMVC, 2002.
[3] D. DeMenthon and L.S. Davis. Model-based object
pose in 25 lines of code. IJCV, 14:123–141, 1995.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

(a)

(b)

(c)

Figure 7: Registration results in experiment 1 with the cylinder head (a), the car door (b) and the binding beam (c). The synthetic view shows the different viewpoints of the sequence (green cones correspond to the learned views). The graph shows
the reprojection error for each view of the sequence (red bar correspond to registration failure). The images presented show
some registration results (in red : the CAD model reprojection)

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

(a)

(b)

(c)

Figure 8: Cylinder head registrations in uncontrolled environments: signiﬁcant viewpoint change (a), scale change (b) and
presence of occlusions in a cluttered environment (c). In red : the CAD model reprojection.
[4] R. Deriche. Using canny’s criteria to derive a recursively implemented optimal edge detector. IJCV,
1:167–187, 1987.

[13] K. Mikolajczyk, A. Zisserman, and C. Schmid. Shape
recognition with edge-based features. In BMVC,
2003.

[5] M.A. Fischle and R.C. Bolles. Random sample consensus: A paradigm for model ﬁtting with applications to image analysis and automated cartography. Graphics and Image Processing, 24(6):381–
395, June 1981.

[14] S. Nayar, H. Murase, and S. Nene. Parametric appearance representation. Early Visual Learning, 1996.

[6] J. Flusser and T. Suk. On the calculation of image
moments. Research Report 1946, Institute of Information Theory and Automation, 1999.

[16] A. Pentland, B. Moghaddam, and T. Starner. Viewbased and modular eigenspaces for face recognition.
In CVPR, Seattle, WA, June 1994.

[7] D.P. Huttenlocher and S. Ullman. Object recognition
using alignment. In First International Conference
on Computer Vision, 1987.

[17] L. G Roberts.
Machine perception of threedimensional solids. Optical and Electro-Optical Information Processing, pages 159–197, 1965.

[8] S. Lazebnik, C. Schmid, and J. Ponce. A sparse texture representation using local afﬁne regions. PAMI,
accepted in 2005.

[18] C.A. Rothwell, A. Zisserman, D.A. Forsyth, and J.L.
Mundy. Planar object recognition using projective
shape representation. In IJCV, volume 16, pages 57–
99, 1995.

[9] D.G. Lowe. Three-dimensional object recognition
from single two-dimensional images. Artiﬁcial Intelligence, 31(3):355–395, March 1987.
[10] D.G. Lowe. Distinctive image features from scaleinvariant keypoints. IJCV, 60(2):911–110, 2004.
[11] A.I. Medalia. Dynamic shape factors of particles.
Powder Technology, (4):117–138, 1970.
[12] K. Mikolajczyk and C. Schmid. Scale and afﬁne invariant interest point detectors. IJCV, Volume 60(Issue 1):63–86, Oct 2004 2004.

[15] S. Obdrzalek and J. Matas. Object recognition using local afﬁne frames on distinguished regions. In
BMVC, 2002.

[19] A Selinger and R.C. Nelson. A perceptual grouping hierarchy for appearance-based 3d object recognition. CVIU, 76:83–92, 1999.
[20] L. Vacchetti, V. Lepetit, and P. Fua. Combining
edge and texture information for real-time accurate
3d camera tracking. In International Symposium on
Mixed and Augmented Reality, November 2004.
[21] M. Zuliani, S. Bhagavathy, B. S. Manjunath, and
C. S. Kenney. Afﬁne-invariant curve matching. In
IEEE International Conference on Image Processing,
Oct 2004.

Proceedings of the International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)
0-7695-2606-3/06 $20.00 © 2006

