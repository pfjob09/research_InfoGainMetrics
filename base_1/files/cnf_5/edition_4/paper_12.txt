Quaternion-Based Camera Calibration and 3D Scene Reconstruction
Jihun Park
Hongik University
jhpark@hongik.ac.kr
Abstract
This paper deals with determining camera parameters
for three dimensional scene reconstruction given an image
with three known points on a single plane on the image.
The problem is solved using a quaternion based set of nonlinear equations. The advantage of using quaternion is it
can accelerate computation and allows us correct orientation interpolation, if needed, while avoiding degeneration
in camera parameter interpolation.
Keywords— Camera Calibration, Quaternion, 3D Reconstruction

1 Introduction and Related Works
Three dimensional object reconstruction from a set of
images is an important and difficult problem. If we solve
these problems we can get a lot of information useful for
various applications. If image feature correspondences are
given, one of the approach to solve these kinds of problems is by analytic computation based on pure mathematical geometry. Numerically solving a set of solution can
be sometimes very sensitive to input data due to numerical
errors. But we believe mathematical analysis is one of the
basic approaches to better understand the problem, and is
a good starting point. Many authors studied different approaches to compute multi view relations[5,6]. Torr and
Zisserman[7] have proposed a robust approach to the computation of the three view geometry. Hartley[8] proposed a
method to compute the four view geometry.

2 Quaternion and its expansion to translation
2.1

ing quaternion is that we do not need to compute trigonometric equations (but sometimes have to compute some
square roots), which makes computationally quicker. Although quaternion can represent only rotation, we have extended quaternion by adding a translation step[1]. We can
completely eliminate trigonometric equation computation,
while acclerating computation. Resulting motion by interpolating quaternion is natural compared to that of using
three 1-DOF joints, which results in independent 1-DOF
interpolations. Major deficiency with quaternion is it can
only represent rotations/orientation. But we need translation for 6 DOF space. In this paper, we improved quaternion by adding translation part, resulting in what we call
heptanion. With heptanion, we can solve forward kinematics, inverse kinematics as well as differential kinematics,
while getting better forward computational performance.
Even we can completely eliminate trigonometric equation
evaluation. This section deals with extending a quaternion
with translational terms, resulting what we call a Heptanion.
Quaternion is defined by q = (cos θ2 , n sin θ2 ) =
(w, pi + qj + rk) where n represent axis of rotation, θ
rotation angle around n, and cos θ2 = w represents the real
part of a quaternion, while n sin θ2 = pi + qj + rk represents the imaginary part of a quaternion. We denote v
to be a v vector. Let h be an extension of the quaternion,
h = (w, p, q, r, x, y, z). The rotational part, (w, p, q, r)
where w2 + p2 + q 2 + r2 = 1, is the same as that of a
quaternion. The translational part is (x, y, z). Multiplication of two extended-quaternions is defined as follows.

Quaternion and its expansion to translation

Quaternion needs four variables to represent a single rotation or orientation, which has 3 DOFs(degrees of freedom). The reason for using quaternion is its ability to represent 3 DOFs. The advantage of using quaternion is as follows: although Euler angles are frequently used for three
dimensional rotation, they sometimes create problems such
as the Gimbal lock problem, where we lose one degree of
freedom. In this situation, we get infinite number of solutions, leading to singularity. Another advantage of us-

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

h 1 ∗ h2

=

(w1 , v1 , t1 ) ∗ (w2 , v2 , t2 )

=

(w1 ∗ w2 − v1 · v2 ,
w1 ∗ v2 + w2 ∗ v1 + v1 × v2 ,
(w12 − v1 · v1 )t2 + 2v1 (v1 · t2 )
+ 2w1 (v1 × t2 ) + t1 )

where

vi = (pi , qi , ri ), ti = (xi , yi , zi ),
|wi |2 + |vi |2 = 1, i = 1, 2

(1)

2.2

+2. ∗ w ∗ (p ∗ W ay − q ∗ W ax ) + z)

Transformation and Projection

For a single homogeneous transformation, we need
a single heptanion computation. Let the transformation we need is C W T r, which can be represented
using hi .
Let h2 = (1, 0, 0, 0, W Px , W Py , W Pz ).
a position vector defined in W coordinate.
h1
=
(w1 , p1 , q1 , r1 , C Wx , C Wy , C Wz ), where
C
C
C
( Wx , Wy , Wz ) is a W coordinate’s origin position
represented in C. For simplicity, let us denote these as
(x1 , y1 , z1 ) then h1 = (w1 , p1 , q1 , r1 , x1 , y1 , z1 ).

Similarly we can compute for C bX , C bY , C bZ , C uX ,
uY , and C uZ . Then corresponding points on camera image are computed as

C

I

aX

=

aY

=

bX

=

bY

=

uX

=

uY

=

I

I

3 Camera Calibration with Three Known
Planar Points
Let us assume we know three planar points a, b, and u,
where a = [ax ay az ]T etc. without any camera distortion.
These points are represented in world coordinates and the
same notation for these points is W a,W b, and W u, respectively. One pair of vectors out of three position vectors
are perpendicular. Because we need to handle coordinate
transformations, we denote W as world coordinate, C as
coordinates of a camera. W
C T is a homogeneous transformation matrix that transforms data represented in C coordinate to corresponding data in W coordinate. Let three
input points be W a,W b, and W u, represented in real world
coordinate, W. Let I denote 2D image coordinate of the
camera. Input points in image coordinates that are projected points of three real world points are I ax , I ay , I bx ,
I
by , I u x I u y .
The unknowns are transformation matrices between W
and C coordinates, and focal length, f . To solve these
unknowns, we used quaternion to derive a set of nonlinear equations. Let us assume W s = [W sx W sy W sz ]T
is any point in real world. In order to convert points represented in world coordinates to camera coordinate, we compute the following equations. h = (w, p, q, r, x, y, z) is
camera coordinate’s quaternion-based representation in W.
(w, p, q, r) is camera orientation representation in quaternion, while (x, y, z) is camera coordinate origin represented in W. We solve these equations for camera transformation. Following equations are scalar components of
the translational vector part of equation 1.
C

R

=

w∗w−p∗p−q∗q−r∗r

aX

=

(R ∗ W ax + 2. ∗ (p ∗ W ax
+q ∗ W ay + r ∗ W az ) ∗ p
+2. ∗ w ∗ (q ∗ W az − r ∗ W ay ) + x)

C

aY

=

(R ∗ W ay + 2. ∗ (p ∗ W ax
+q ∗ W ay + r ∗ W az ) ∗ q
+2. ∗ w ∗ (r ∗ W ax − p ∗ W az ) + y)

C

aZ

=

−(R ∗ W az + 2. ∗ (p ∗ W ax
+q ∗ W ay + r ∗ W az ) ∗ r

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

(2)

where h = (w, p, q, r, x, y, z)

I

I

I

C

aX
f
C
− aZ
C
aY
f
− C aZ
C
bX
f
−C bZ
C
bY
f
−C bZ
C
uX
f
−C uZ
C
uY
f
C
− uZ

(3)

as well as w2 + p2 + q 2 + r2 = 1. There are seven equations to be solved. Our eight unknowns are w, p, q, r, x, y,
z, and f . As a result, we have above seven equations and
eight unknowns. While moving the origin of camera coordinate along the gaze vector, we can get an infinite proper
combination of x, y, z, f , satisfying the projection. It is
because we can get an infinite set of solutions in mapping
between ground and camera image, by selecting a proper
combination of x, y, z, f . For the numerical solution, we
fix f values and solve a set of nonlinear equations with
seven equations and seven unknowns. The final set of solutions were found, among the infinite set of possible solutions, which gives minimum numerical errors by varying f
values.
For our application, an accurate camera calibration simply means that the 2D image coordinate can be properly
predicted given the 3D location of the object. After the
calibration, we know the homogeneous transformation
W −1
= T and focal length, f .
C1 T

4 3D Object Reconstruction
This section explains how to compute an object’s height
standing normal on a plane, and the result can be extended
for 3D reconstruction. The input to this approach is a single image with three known real world points, and their
corresponding image position on the image. The three
points should be on the plane we are interested, and one
set of the two vectors made by three known points should
be perpendicular. Then based on the three points, we can
compute camera parameters and possibly retrieve plane
equation. After retrieving points on the plane, we can

(
(

W

d

x

, dy ,
W

W

d

z

I

)

)
x, y

d

I

camera

d

object top

(

C

d

,

C

d

,

C

d

)

x
y
z
object top projected

(d,
W

x
object bottom

dy ,0 )

W

Figure 1: Computing height of an object
compute an object’s height based on the assumption we
know the corresponding projected-on-the-plane points of
arbitrary three dimensional points we are interested. By
using this technique, we can construct a three dimensional
world give a single image with three known image points.

4.1

Assumptions in 3D Reconstruction
• The plane we are interested can be any plane in real
three dimensional world. But every plane should be
perpendicular to the base plane which contains initial three know points.
• We need three points for our reference. But the three
initial points should lie on a plane.
• Three real point inputs should be on a single three
dimensional plane while independent each other to
get 6 independent equations for us. Most set of three
points are independent.
• Camera gaze vector should be neither perpendicular
nor parallel to the 3 dimensional plane normal vector. The plane has 3 independent input points.
• In order for us to compute a height of an object, the
object’s height should be in normal direction of the
3 D plane.
• We should be able to see the base and top of the object in the image, and base/top should be able to be
projected on the plane.

4.2

Computing a height of an object on the plane

Assume that the object we are interested is standing normal to the plane of our interest. This kind of situation is
quite often. A human standing on a floor, a building on a
ground. The key assumption is we should be able to know
the contact point, where the object of our interest is in contact with our mapping plane. Mapping between 2 dimensional projection plane and a plane in three dimensional
real world. The ground can be tilted as far as the object
is standing normal to the plane, although this situation is

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

quite unusual for neither a human nor a building, In this
computation, we get an extra image coordinate input. Let
us say the input point is (I dx , I dy ). Our governing projection equation always holds true. But this time, our unknown is three dimensional real world coordinate. But we
can get its base (W dx , W dy , 0) points from previous computation because the base is on the plane. The only value
we do not know is W dz . Unfortunately we get 2 equations and 1 unknown from the following equations. This
can be explained using function mapping. In real world,
we deal with a three dimensional plane. This plane is
mapped to a camera image. This is mapping from 2 dimensional plane space to 2 dimensional image space. But
the three dimensional point we are interested is not on the
same plane. This kind of transformation is not affine transformation. In order to solve the exact points for the three
dimensional point, we used trigonometric rules. Figure 4
shows how to get correct three dimensional points given a
corresponding image point, (I dx , I dy ). Computation steps
are as follows. Given I dx , I dy , find . (C dx , C dy , C dz )
According to assumption, we can observe (W dx , W dy , 0)
on the image. Surely we can compute these values because we have corresponding image values for this point
based on our assumption. Then we know vector from
(C dx , C dy , C dz ) to (W dx , W dy , 0). Also we can compute
vector from (C dx , C dy , C dz ) to camera. Then we can compute C dz . The governing projection equations are
I

I

dX

=

dY

=

C

dX
C
− dZ
C
dY
C
− dZ

f
f.

(4)

5 Experimental Results
Figure 5 shows input image file. On the floor of the
image, you can find a white triangle. The vertices of the
triangles are three input points. We selected pairs of points
both at the floor and at the ceiling to generate walls of the
three dimensional world. Figure 5 shows resulting three
dimensional virtual world in VRML(Virtual Reality Modeling Language). You can find some texture mismatch in

(a) Input image

(b) VRML scene reconstructed

Figure 2: Input image and three dimensional VRML scene reconstructed.
ceiling and on walls. This may be due to either computation error or VRML browser error. In this problem, if
we are just working on three points on a same plane, we
have 6 equations. But our number of unknown in terms
of camera parameters are 7, resulting in infinite number of
solutions. But if we are interested in a three dimensional
point which is not on the same plane, the number of unknowns for the camera is the same. But number of equation changes. Because the point we are interested has extra
unknown value in z resulting in extra equation. Its x and
y points are mapped on the plane, which does not increase
the number of equations. This leads to a unique set of solutions if a non-planar point is involved. Our result shows
the worst height computation error for the above scene is
12 %.

Conclusions
In this paper, we have provided a solution for camera
calibration problems which arise in real world. An object height computation problem using a single camera
was nicely solved, and was demonstrated with an example three dimensional reconstruction. Our solution can be
generalized by including quaternion nonlinear interpolation and by solving a set of nonlinear equations for moving
camera views. The advantage of using quaternion is that
it can accelerate computation and it allows us to avoid de-

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

generation in camera parameter interpolation. Quaternion
is the only solution for three dimensional interpolation
because Euler angles or other similar methods does not
represent full three degree-of-freedom, but they are rather
a collection of three one degree-of-freedoms, which can
fall into degeneration situation.

Acknowledgements
This research was supported by Seoul Research and
Business Development Program (10555).

References
[1] J. Park and S. Kim. Kinematics and constrained joint
design using quaternion. In CISST, June 2002.
[2] R.Y. Tsai. versatile camera calibration technique
for high-accuracy 3d machine vision metrology using off-the-shelf tv cameras and lenses. IEEE Journal
of Robotics and Automation, RA-3(4):323–344, Aug.
1987.
[3] Quming Zhou, J. Park, and J. K. Aggarwal.
Quaternion-based tracking multiple objects in synchronized videos. Lecture Notes in Computer Science
(ISCIS 2003), 2869, Nov 2003.

