Upper Facial Action Units Recognition Based on KPCA and SVM
Chunfeng Yang, Yongzhao Zhan
School of Computer Science and Telecommunication Engineering, Jiangsu University, Zhenjiang
212013, Jiangsu, China
cfyang1981@gmail.com, yzzhan@ujs.edu.cn
Abstract
The existing methods of facial action unit
recognition are always affected by illumination and
individual difference. An upper facial action units
recognition method based on KPCA and SVM is
presented in this paper. In this method, KPCA
algorithm which is formed by choosing and designing
kernel function in terms of visual features of upper
facial action units is used to extract the upper facial
action unit feature and the two features associated with
illumination effect are removed. Then the optimal
kernel function and chastisement factor in SVM
algorithm are determined by experiments. Finally the
SVM is used to classify and recognize action units.
This method is tested on the Cohn-Kanade’s facial
expression image database. The average recognition
rate achieves 90.6% and the recognition speed is also
fast. The experiments show that this method is not
sensitive to illumination and individual difference and
can be used to real time recognition.

1. Introduction
Nonverbal
communication
including
face
expression, gesture and other body languages plays an
important role in our life. Understanding human
emotions through these nonverbal means is one of the
necessary skills both for humans and also for the
computers to interact intelligently and effectively with
their human counterparts. Face expression expressed
by action units (AUs) is a hot spot of research in the
nonverbal communication. Tian et al [1] have
developed a system to recognize sixteen AUs and their
combinations of these. The shapes of facial features
like eyes, eyebrows, mouth and cheeks are described
by multi-state templates. The parameters of these
multi-state templates are used by a Neural Network
based on classifier to recognize the AUs. The result
shows that it has a good performance, but this system
requires that the templates be manually initialized in

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

the first frame of the image sequences, which prevents
it from being fully automatic recognition system.
Kapoor et al [2] have developed a new automatic AUs
recognition system. They extract features of AUs by
PCA and recognize five single AU by linear SVM, but
the recognition rate is only 81.2%. Valstar et al [3]
have developed an automatic AUs recognition system,
which extracts features of AUs by temporal template
and uses the classifier of k-nearest-neighbor (KNN) to
train and recognize. The face profile images are taken
into account in the system, but the recognition rate is
less than 77%. Some researchers have adopted Kernel
Principal Component Analysis (KPCA) to extract
features in face recognition, and got effective features
that can mask illumination variation [4]. So in this
paper, we propose a method that adopts KPCA to
extract upper facial action unit feature and uses
Support Vector Machine (SVM) to recognize upper
facial action unit state. Meantime, we optimize the
corresponding algorithms to achieve the goals that can
mask illumination variation and be suitable for real
time recognition.

2. Preprocessing
segmentation

and

action

units

In this paper, we use the method in [5] to detect face
region and preprocess them. Bilinear interpolation is
used to normalize all face images size into 180×200
pixel. In the grayscale equalization, the formula is used
as follows:
I [ x ][ y ] =

σ0
σ

( I [ x ][ y ] − µ ) + µ

0

(1)

where I ( x, y ) and I( x , y ) are the intensity of pixel (x, y)
before and after grayscale equalization respectively,
μ0 ,σ0 are the mean and the covariance value of the
equalized image, and μ,σ are those values of the
original image.

After preprocessing face region image, we detect
and segment upper facial action units region with the
methods as follows. Let ELeft, ERight, ETop, EBottom
be the left, right, top, bottom position of the rectangle
of eye region. Let EBLeft, EBRight, EBTop, EBBottom
be the left, right, top, bottom position of the rectangle
of eyebrow region. Firstly, we can get a segmentation
point between eye and eyebrow by doing gray integral
projection. Assign the y coordinate value of this point
to ETop and EBBottom. Then EBTop equals to this
value added with 35, and EBottom equals to this value
subtracted with 35. Secondly, we find the position of
the inner, outer corner and pupil of eye by using Harris
corner detection [6]. Then, we can get the value of
ELeft, ERight, EBLeft, EBRight. Finally, we segment
corresponding upper facial action units region through
these values. The final sizes of eye and eyebrow region
image are set to 70 × 35 pixel and 80 × 35 pixel
respectively. The original images of faces and the
segmentation results for the eye and eyebrow regions
are shown in figure 1.

[7]. In addition, KPCA can resolve the problem which
can not be linearly classified in the input space by
mapping them to the feature space, so that the design
of classifier can be simplified. So we adopt KPCA to
extract upper facial action unit features in this paper.
Given a sample set { xi , i = 1, 2,

, n} , we map the

input x to a high dimensional feature space H via a
d

nonlinear function φ : R → H , x → φ ( x ) .
The corresponding eigenvalue problem and the
covariance matrix in H are:

λU φ = C φU φ , i = 1, 2, ..., n
φ

C =

1

n

(2)

∑ φ ( x )φ ( x )
n

T

i

(3)

i

i =1

where φ ( x1 ) , φ ( x2 ) ,..., φ ( xn )

are centered, i.e.,

n

φ ( xk ) ∈ H , ∑ φ ( xk ) = 0, k = 1, 2,..., n

,

λ

is

k =1

eigenvalue,

U

φ

φ

is

φ

eigenvector.

Every

feature

φ

vector U 1 , U 2 ,…, U n with λ1 , λ2 ,…, λn lies in the
span of φ ( x1 ) , φ ( x2 ) ,..., φ ( xn ) . In other words, there
exists coefficient α r (i = 1, 2, ..., n) such that:
i

n

U r = ∑ α rφ ( xi )
φ

(a) Face images

i

(4)

i =1

Consider the following equation:

λ ( φ ( xi )iU

φ

) = (φ ( x )iC U )
φ

φ

i = 1, 2, ..., n (5)

i

By combining equation (3), (4) and (5), we have:
n

λ ∑ α r (φ ( xi )iφ ( xi ) )
i

i =1

(b) Eye and eyebrow region images
Figure 1. Segmentation of eye and eyebrow
action units

3. Upper facial action units
extraction based on KPCA

feature

Principal Component Analysis (PCA) is a method
based on 2-order statistical feature and doesn’t
consider high-order statistical information of image
pixels. Some researches show that higher-order
statistics (HOS) information contains non-linearity
relations, such as edge information, curve information
etc., which are more suitable for recognition. KPCA
algorithm makes full use of the high-order dependence
of pixel directly, and converts non-linear problem to
common eigenvalue problem by use of kernel function

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007



= ∑ α r  φ ( xi )i ∑ φ ( x1 )  ( φ ( xi )iφ ( xi ) )
n i =1 

i =1
1

n

n

(6)

i

Define a n × n kernel matrix K:

K ij = k ( xi , x j ) = ( φ ( xi )iφ ( x j ) ) i , j = 1, 2,..., n (7)

So equation (6) can be written as:
nλ K α = K α
(8)
It is shown in reference [8] that the eigenvalue
problem of equation (8) is equivalent to:
(9)
nλα = K α
where K is half positive matrix, so the eigenvalue of K
is not negative. Let 0 ≤ λ1 ≤ λ2 ≤ ... ≤ λn denote the
2

eigenvalue, and α 1 , α 2 , ..., α n is the corresponding
eigenvector
set
of
matrix
K,

where α r = α r , α r , ..., α r  , ( r = 1, 2,..., n ) . We can
1

T

n

2

φ

get eigenvector U via formula (4). Then we need to
normalize

the

(

φ

eigenvector

U

φ

with

) = 1， r = 1, 2, ..., n .

φ

equation: U r iU r

the

It mentioned above that the data φ ( xi ) in the high
dimensional feature space must be centered. We can do
it by changing the kernel matrix as follows:
Kˆ = K－An K－KAn＋An KAn
(10)
where K is kernel matrix before centered, Kˆ is kernel
matrix after centered, An is the matrix which all the
elements are

1

.
n
For the purpose of extracting the principal
component of test sample x in the high dimensional
feature space, we need to compute φ ( x ) projection on

the eigenvector U r ( r = 1, 2, ..., n ) :
φ

(

g r ( x ) = U r iφ ( x )
φ

n

= ∑ α r (φ ( x i )iφ ( x ) ) = ∑ α r k ( x i , x )
i

i

i =1

(11)

i =1

where g r ( x ) is the r non-linear principal component
th

of corresponding φ . Let g ( x ) = ( g1( x ), g2 ( x ),..., gn ( x ) ) be
kernel principal component of test sample x.
In this paper, we compare KPCA features extracted
with different kernel functions through several
experiments. The results show that the best
performance is achieved when we use radial basis
function
which
expression
is K ( x , y ) = exp{−

x−y

σ

2

3)
4)
5)

Compute the kernel matrix Kˆ centered by
equation (10);
Compute the eigenvalue and eigenvectors of the
kernel matrix Kˆ ;
Choose the necessary eigenvalue number
according to the covariance contribution rate, then
normalize the corresponding eigenvectors;
Compute kernel principal component of test
sample x by equation (11).

4. Upper facial action units classification
and recognition based on SVM
There are lots of special advantages to solve the
problem of small samples, non-linear and pattern
recognition in high dimension space by using SVM
algorithm, such as strong ability of generalization and
fast classification. Its basic idea is that searching and
constructing the optimal hyperplane discriminates
between them in the situation of the minimal margin
for error in two kinds of data given.

( x , y ) , i = 1,

Given a training set

)

n

2)

2

} , so we adopt RBF as

kernel function.
Based on the experiences of reference [9] and our
experiments, we know that the first two principal
components of the features extracted contain mostly
low frequency information of the image, and the
change of illumination corresponds to this low
frequency information. So the first two principal
components that are sensitive to illumination variation
are deleted, so as to decrease the effect of the
illumination variation. The remaining features denote
the change of the state of action units.
In summary, the KPCA algorithm used to extract
upper facial action unit features is described as below:
1) Compute the corresponding kernel matrix K of
every action unit by equation (7);

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

i

i

, n, xi ∈ R

d

,

y i ∈ {+1, −1} is the label of different class. Generally

speaking, we solve the problem of optimal hyperplane
by transforming it into relatively simple dual problem
with Lagrange function. The constraint equation is as
below:

max Q α = n a − 1 n n α α y y x ⋅ x
∑∑ i j i j ( i j )
i
 α ( ) ∑
2 i =1 j =1
i =1

(12)
C ≥ ai ≥ 0, i = 1, , n
s.t

n

∑ ai yi = 0
i =1

where ai is Lagrange multipliers, C is chastisement
factor which is a constant.
So the optimal classification hyperplane is as
below:
f ( x ) = sgn

{∑
n

i =1

where

k

}

ai yi k ( xi , x ) + b

(13)

is a kernel function, b is a threshold value,

sgn { } is a sign function.
The SVM abilities of generalization and fast
classification are depended on the selections of kernel
function and parameter C. In the experiment, we
choose RBF as kernel function, and the chastisement
coefficient C is determined by the experiment result.
SVM is a bivalence classifier. With regard to multiclassification, there are three methods as follows: oneagainst-many method, decision tree method, and one-

against-one method. In this paper, we adopt the oneagainst-one method. Its basic idea is that constructing
hyperplane for any two classes. If having k classes,
k * ( k − 1) / 2 bivalence classifiers must be trained. We
adopt the suffrage rule to decide the test sample due to
the class that gets the most votes. In the
implementation, LIBSVM [10] is used in this paper.

5. Experimental results and analysis
The experiments are done based on the CohnKanade AU-coded face expression image database. For
each AU, 150 images are selected, and there are 10
testers and 15 images for each tester. 100 images form
10 testers are used for training and the remains for
testing. Among these images, there are different
illumination situation.
The effects of different kernel function, kernel
parameter and chastisement coefficient C are
considered in these experiments. Four kinds of kernel
function are applied to SVM, including Linear Kernel
Function, Polynomial Kernel Function, Radial Basis
Kernel Function (RBKF) and Sigmoid Kernel
Function. Many experiments show that the RBKF is
better than any other kernel function and the best
parameter values of σ and c are as follows, σ = 0.3 ,
c = 2 , when the recognition rate is optimal.
The effect of the number of kernel matrix
eigenvalue is considered in these experiments. The
experiment results show as Table 1. From Table 1, we
can see that the recognition rate increases with the
number of eigenvalue selected increasing. When the
number of eigenvalue selected is up to 75, the
recognition rate reaches a saturation point. So finally
the number of eigenvalue is set to 75 in this paper.
Table 1. The effect for AUs recognition by the
number of the features
Number

25

40

50

60

75

80

AU0
AU1

60%
54%

82%
72%

88%
84%

89%
86%

91%
88%

92%
88%

AU2
AU4

56%
52%

74%
74%

86%
78%

90%
81%

92%
83%

92%
83%

AU5
AU7

59%
54%

73%
76%

87%
80%

90%
83%

92%
86%

92%
86%

AU1+2
AU4+5

63%
53%

86%
75%

89%
78%

92%
82%

94%
85%

95%
85%

AU1+2+5

64%

76%

85%

89%

92%

93%

AU

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

In this experiment, PCA is implemented to compare
with the KPCA we used in this paper. The results are
shown as Table 2 and Table 3, from which we can
conclude that the features extracted by KPCA is better
than by PCA. The recognition rate is satisfying for an
automatic recognition system. In addition, we did the
recognition speed test. The time consumption for
recognizing one image is 0.05s, which is suitable for
real time recognition. But, the recognition rate is
relativistically low in some measure for AU4
(Eyebrows lowered), AU7 (Lower eyelids raised) and
AU4+5 (Eyebrows lowered and Upper eyelids are
raised). After analyzing the test images, we can see that
there is only a litter difference existed between
eyebrows down and eyebrows motionlessness (neutral
AU), and also between lower eyelids up and lower
eyelids motionlessness for some cases. They are hard
to be distinguished from the image, so the
misjudgment is easy to happen.
Table 2. Recognition rate for features
extracted by PCA
AUs
Sample
Correct
Misses Recognit
Number Recognition
-ion Rate
AU1
58
49
9
84.5%
AU2
58
52
6
89.7%
AU4
58
47
11
81.0%
AU5
58
52
6
89.7%
AU7
58
48
10
82.8%
AU0
58
53
5
91.4%
AU1+2
58
54
4
93.1%
AU4+5
58
47
11
81.0%
AU1+2+5
58
52
6
89.7%
Total
522
455
67
87.2%
Table 3. Recognition rate for features
extracted by KPCA
AUs
Sample
Correct
Misses RecognitNumber Recognition
ion Rate
AU1
58
52
6
89.7%
AU2
58
54
4
93.1%
AU4
58
49
9
84.5%
AU5
58
54
4
93.1%
AU7
58
51
7
87.9%
AU0
58
54
4
93.1%
AU1+2
58
55
3
94.8%
AU4+5
58
50
8
86.2%
AU1+2+5
58
54
4
93.1%
Total
522
473
49
90.6%

6. Conclusions and future work
In this paper, an upper facial AUs recognition
method is proposed. The methods of gray integral

projection and Harris corner detection are adopted to
accurately locate the sub-region of AUs and segment
the sub-region from the facial image. The features are
extracted by KPCA algorithm designed by choosing
right kernel function. This algorithm can bring down
the dimensions of the image matrix to reduce
computational cost by mapping the image to the
feature space, and remove the features reflecting
illumination variation. The features extracted can not
only reflect the change of the state of the AUs
effectively, but also mask the effect caused by different
individual features and illumination variation. At last,
optimized SVM algorithm is used to train and
recognize upper facial AUs. The recognition rate is
satisfying and the recognition speed is fast, which can
be suitable for real time recognition. Only six single
AUs and three combinations of AUs are concerned in
this paper. Yet 46 single AUs and more than 7000
combinations of them exist in the Facial Action Coding
System (FACS). So the future work should take into
account extracting the features for more AUs and the
combinations of them and recognizing them, so as to
improve our method to achieve high recognition rate
for more AUs and the combinations of them.

Acknowledgements
We would like to express our thanks to Carnegie
Mellon University Robotics Institute, they offer us
Cohn-Kanade’s facial expression image database for
testing our method. This research is supported by
National Natural Science Foundation of China under
Grant No. 60673190.

References
[1] Y. Tian, T. Kanade, and J. Cohn, “Recognizing Action
Units for Facial Expression Analysis”, Pattern Analysis and
Machine Intelligence, February 2001, 23(2).

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

[2] A. Kapoor, Y. Qi, and R.W. Picard, “Fully Automatic
Upper Facial Action Recognition”, Analysis and Modeling of
Faces and Gestures, IEEE International Workshop, October
2003, pp. 195-202.
[3] M. Valstar, I. Patras, and M. Pantic, “Facial Action Unit
Recognition Using Temporal Templates”, Robot and Human
Interactive Communication, 13th IEEE International
Workshop, Sept, 2004, pp. 253-258.
[4] J. Yang, A.F. Frangi, J.Y. Yang, D. Zhang, and Z. Jin,
“KPCA Plus LDA: A Complete Kernel Fisher Discriminant
Framework for Feature Extraction and Recognition”,
Transactions on Pattern Analysis and Machine Intelligence,
IEEE, February 2005, 2(27): 230-244.
[5] J.F. Ye, “Research of Facial Expression Recognition
Based on Video” (Master paper of Jiangsu University),
2004.
[6] C.F. Yang, G.T. Zhou, and Y.Z. Zhan, “Eye Feature
Extraction Based on Gray Information and Harris Corner
Detection”, IeCCS, 2006.
[7] B. Scholkopf, and A. Smola, “Learning with Kernels”,
MIT, 2002.
[8] B. Scholkopf, A. Smola, and K. Muller, “Nonlinear
Component Analysis as a Kernel Eigenvalue Problem”,
Neural Computation, 1998, 10(5): 1299.
[9] P.N. Belhumeur, J. EHespanha, and D.J. Kriegman,
“Eigenfaces VS. Fisherfaces: Recognition Using Class
Specific Linear Projection”, Transactions on Pattern
Analysis and Machine Intelligence, IEEE, July l997, 19(7):
711-720.
[10] C.C. Chang and C.J. Lin, “LIBSVM: a library for
support vector machines”, Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm, 2001.

