Cloning of Facial Expressions Using Spatial Information
De-Hui Kong ,Liang Kang,Bao-Cai Yin
(Multimedia and Intelligent Software Technology Beijing Municipal Key Laboratory, College of
Computer Science and Technoloby, Beijing University of Technology, Beijing 100022 )
{kdh@bjut.edu.cn , kjl8183@yahoo.com.cn, ybc@bjut.edu.cn }

Abstract
In virtual world, it is always a challenge research
issue for an animated human face model to appear
natural. In this paper, we present a novel facial
animation method for the technique of expression
cloning which can directly maps an expression of the
source model onto the surface of the target model. Using
litter time, our method reuses the 3D motion vectors of
the vertices of the source model to create similar
animations on a new target model having the same mesh
structure
Keywords--- facial animation, expression cloning,
deformation

1. Introduction
Facial animation aims at producing expressive and
plausible animations of a 3D face model and is one of the
focuses in computer graphics because of widely
utilization.. Traditional facial animation methods usually
pay much attention to the geometric warping of the
parametric model. Although traditional methods can also
generate various kinds of expression, these expressions
are not expressive and the parametric model needs to
adjust a lot of parameters. Our goal is to produce highquality facial animations quickly by reusing motion data.
In this paper, we present a novel facial animation method
called expression cloning which can directly maps an
expression of the source model onto the surface of the
target model. Our method reuses the 3D motion vectors
of the vertices of the source model to create similar
animations on a new target model having the same mesh
structure.
Contributions: This paper introduces new
techniques for producing facial animations that are
inexpensive, practical and well suited for retargeting to

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

new characters. By using our method, the existing
libraries of high-quality animations created by many
different methods can be reused for new models.

2. Related Work
There have been extensive efforts on the
development of 3D facial animation techniques
beginning with Parke’s pioneering work. Some model
the anatomy of the face, deriving facial animations from
the physical behaviors of the bone and muscle structures
[1]. The facial expression is generated by using the
vertex displacements of an individual facial model based
on estimated muscular contraction parameters. We call
these approaches as Physically-based approaches. In
general these approaches make little use of existing data
for animating a new model and can not meet the demand
of reality. Animation parameters do not simply transfer
between models.
Another important field in facial animation area is
designing systems, which will automatically generate
appropriate facial animation according to a set of rules.
Those rules are based on the work of many
psychologists. MPEG-4, which is an object-oriented
standard of multimedia compression, is typical to these
approaches. An international standard of 3D facial
animation is defined in MPEG-4[2]. MPEG-4 proposes a
facial animation method based on FDP and FAP
parameters which has high versatility and low calculation
complexity. But only a framework is provided in MPEG4. YIN [3] proposed a novel 3D facial animation model
to generate realistic facial animation sequence based on
MPEG-4.
Inspiration of motion capture data, one approach
called Performance-driven animation made a growing
role in film production because it allows actors to
express content and mood naturally, and because the
resulting animation had a degree of realism that is hard

to obtain from Physically-based approaches. Firstly, the
researches should obtain a single-camera tracker that
estimates performance parameters and detailed 3D
geometry from video recordings. Then face transfer
extracted performances from ordinary video footage.
Performance-driven animation acquires expensive
instrument. For reusing existing data, synthesis has
recently been popular. Example-based synthesis is
another stream of research. Pyun [4] gives an input 3D
facial animation for a source face model, and then
generated a similar animation for a target model by
blending the predefined target models corresponding to
the example source face models with extreme
expressions.
Noh [5] presented a novel approach to producing
facial expression animation for new models. They called
this approach “expression cloning”, which can directly
maps an expression of the source model onto the surface
of the target model. Expression cloning makes it
meaningful to compile a high-quality facial animation
library since this data can be reused for new models.
We adopt the expression cloning method which
reuses the 3D motion vectors of the vertices of the source
model to create similar animations on a new target model
having the same mesh structure. That means we require
perfect one-to-one correspondence between all meshes.
Figure 1 illustrates our approach for expression cloning.
The source base-model and source key-model is the
same person of two expressions. The target base-model
is the new character which is used to generate an
expression same to the source key-model’s. To improve
the efficiency, we plot out the model to five spaces:
mouse, eyes, nose, face, and eyebrows in the step of
pretreatment. Then, in each space, we calculate the
displacement vector of the source key-model from the
source base-model. Using the displacement vector, the
cloning arithmetic can directly map an expression of the
source key-model onto the surface of the target basemodel. So our approach comprises three major parts:
pretreatment, displacement calculation, and synthesis of
expression. The first part is done once at the beginning,
and the last parts are repeatedly executed for each source
key-model in runtime.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Figure 1: Overview of our example-based
expression cloning
The remainder of this paper is organized as follows:
In next section, we describe in more detail the
pretreatment of model. Section 4 proposes the method
used to calculate the displacement vector of source keymodel from source base-model. The method, Creating a
cloned expression same to the source key-model, is
discussed in section 5. Finally, a series of conclusions are
drawn and future research issues are discussed in Section
6.

3. Pretreatment
Generally, the motion of each part of the head model
is independent. For example, the motion of eyes is
independent of the motion of mouse and the motion of
eyebrows is independent of the motion of face. So
plotting out the model to many spaces is a common
method to improve efficiency. In MPEG-4 the model is
plotted out to ten spaces showed in Figure 2(A).In our
method, the model is plotted out to five spaces:
eyebrows, eyes, mouse, face, and nose. Note that the
mouse space also includes tooth and tongue.

(a)
(b)
Figure 2: segmentation of function area
Now, we will build a local reference frame for each
model space. But why we should build the local
reference frame? And what is the goal to build it? We all
know that the same expression has the different
impressions for different persons. That is because one
face is always different from the other. Some persons
have bigger mouths and some persons have smaller eyes.
So, to eliminate the relativity between different models,
we should build the local reference frame. There are five
local reference frames corresponding to the five model
spaces. Each local reference frame only has an
immediate effect on the corresponding model space.
Note that we only build local reference frames on the
source base-model and the target base-model. The source
key-model uses the same local reference frames with the
source base-model.

Figure 3: Local reference frame of mouth space.
As proof of concept, we only explain our method in
the mouth space. Figure 3 illustrates our approach for
building local reference frames. Firstly, we choose four
points called base-points: bp1 , bp2, bp3,bp4 . We believe
empirically that one point should be on the bridge of
nose, two points should be on the edge of mouth, and the
last point should on the middle of mouth. Let

bp2 − bp1
,
bp2 − bp1
bp − bp1
β = ( d , e, f ) = 3
, and
bp3 − bp1

α = ( a, b, c ) =

χ = ( u, v, w ) =

α ,β

α ×β
α ×β

.

and χ are the unit direction vectors of X

axis, Y axis, and Z axis. The angle between α and β
implies the width of mouth. Note that the angle is not
always equipped to 90 degree, which means X axis is not
always upright with Y axis. This is a characteristic in our
local reference frames. bp1 is the origin of local
reference frame belongs to mouth.. bp4 is the w-point
used to eliminate the relativity between different models.
By the local reference frames and bp4 , we can ascertain
the main position of mouth space on the model. Other
spaces use the same method to build their local reference
frames. Table 1 illustrates all the base-points which we
choose empirically.

4. Offset of Expression

their displacements to the corresponding points on the
source key-model, and then employ scattered data
interpolation with radial basis function. This method
works well, but maybe lost some details. In this section,
we explain how expresses the difference between the
expression of source base-model and the expression of
source key-model. It is the base of face transfer. We also
demonstrate our method in the mouth space. Figure 4
illustrates the flow chart of offset of expression.
All the points of mouth space can be expressed
as

(p ,p
1

2,

p3,,,,,,,,,,,,,, pn ) . pk is a random point:

pk = ( xk , yk , zk ) , 1 ≤ k ≤ n . N is the number of
points. The bp1 which is the origin in local reference

(

)

frame of mouth space has the coordinate o1, o2, o3 in the
world reference frame. From the section 3, we
the unit direction vectors of X axis, Y axis,
Coursing pk , make three lines upright with X
axis, Z axis. Mathematically, the equations
expressed by:

can get
Z axis.
axis, Y
can be

[xk −( o1 +txk ×a)]×a+[yk −( o2 +txk ×b)]×b+[zk −( o3 +txk ×c)]×c=0

[xk −( o1 +tyk ×d)]×d+[yk −( o2 +tyk ×e)]×e+[zk −( o3 +tyk ×f )]× f =0

[xk −( o1 +tzk ×u)]×u+[yk −( o2 +tzk ×v)]×v+[zk −( o3 +tzk ×w)]×w=0
Where t xk , t yk and t zk are the parameters of point of
intersection. Let ek be the local coordinate of pk :

ek = ( t xk , t yk ,t zk ) .By the Equation (1) ~ (3), we can get
T

the local coordinate of every point of mouth space. Let

(

S Bi = ( e1, e2, e3,,,,,,,,,,,, en ) and Si = e1 , e2 , e3, ,,, ,,,, en

)

be the local coordinates of source base-model and source
key-model.

(

)

Si − SBi = e1 −e1, e2 −e2, e3 −e3,,,,,,,,,,,, en −en = ( vi1,vi2,vi3,,,,,,,,,,,vin )

, 1≤ i ≤ m.

 Tx   ∆x1, ∆x2, ∆x3,,,,,,,,,,, ∆xn 

  
⇒ Si − S Bi =  Ty  =  ∆y1, ∆y2, ∆y3,,,,,,,,,,, ∆yn  .

  
 Tz   ∆z1, ∆z2, ∆z3,,,,,,,,,,, ∆zn 
where n is the number of points of one space, and m
is the number of model spaces. In this paper, m is equal
to 5.
The displacement vector ν i of source key-model

Si from source base-model S Bi is defined as follows:

Figure 4: flow chart of offset of expression.
Tradition approaches usually select a number of
feature points on the source base-model and then extract

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

coordinate corresponding to qk i . Using the method
mentioned in the third Section, we build the local
reference frames on the target base-model. So, in the

 1

 × Tx 
 ωx

 1

ν i =  × Ty 
 ωy

 1

 × Tz 
ω

 z

where

(ω

x ,ω y ,ω z ) is

(4)

th

local reference frames of the i space of target basemode, the unit direction vectors of X axis, Y axis, and Z
axis
can
be
expressed
by:

Vxi ( aTi ,bTi , cTi ) , Vyi ( dTi , eTi , fTi ) , Vzi ( uTi , vTi , wTi ) ;and

the local coordinate of bp4 .

bp4 called w-point is on the middle of the mouth. It
implies the distance from nose to mouse.

5. Synthesis of expression on new model
In this section, we are interested in moving every
point of 3D head model for constructing a new
expression same to the expression of source key-model.
By the Equation (1) ~ (3), we can get the local
coordinate of every point of target base-model. Let

 ℑx 


S T B i =  ℑ y  1≤ i ≤ m
 ℑ 
 z 
Where, STBi is the 3×n matrix which includes all the
th

local coordinates of the i space of the target basemodel., and m is the number of model spaces.

(ς xi,ς yi,ς zi ) is the local coordinate of w-point belongs to
th

the i space f the target base-model. Using the
information mentioned above, we can get all the local
coordinates of target key-model. Mathematically, the
equations can be expressed by:

 ςx

 × Tx 
ω

 ℑx 
 ℑx   x

 
   ςy
(5)
STi =  ℑ y  + f (ν i ) =  ℑ y  +  × Ty 
ω


y
ℑ 
ℑ 
 z
 z 

 ς z × Tz 
ω

 z

Where, STi is the 3×n matrix which includes all the

(

)

the origin can be expressed by OTi oTi1, oTi 2, oTi 3 .Let:

Vxi   aTi
 
Di = Vyi  =  dTi
V   uTi
 zi 

bTi
eTi
vTi

cTi 
fTi  ,
wTi 

ε xki × ( Vxi 2 ) + OTi × VxiT 


2
T

Bi = ε yki × ( Vyi ) + OTi × V yi .


ε × ( V 2 ) + O × V T 
zi
Ti
zi 
 zki


(6)

(7)

We can give the equation:

pk i × Di = Bi

(8)
Using the Equation(6)~(8),we can evaluate the
unknown pk i .

6. Conclusions
Our goal is to produce facial animations by reusing
motion data. In this paper, we present a novel facial
animation method called expression cloning, which can
directly maps an expression of the source model onto the
surface of the target model. Our method reuses the 3D
motion vectors of the vertices of the source model to
create similar animations on a new target model having
the same mesh structure. The next contribution is that the
new target model can have the expression similar partly
to two or three source key-models. That means, by using
our method, the expression of mouth on the new target
model can similar to one source key-model’s, and the
expression of eyebrow on the new target model can
similar to another source key-model’s. Figure 5 and
Figure 6 show the whole expression cloning.

th

local coordinates of the i space of target key-mode..
ν i is the displacement vector of source key-model Si
from source base-model S Bi .Then, the next step is to
translate local coordinate to world coordinate. Let qk i ,

1 ≤ k ≤ n and 1 ≤ i ≤ m , be the local coordinate of
th
point belongs to the i space of target key-mode:
qki = ( ε xki ,ε yki ,ε zki ) . pk i ( xki , yki , zki ) is the world

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Figure 5: cloning expression from a woman to a man.

Figure 6: cloning expression from a man to a boy.
One limitation of our method might be that it
perfect one-to-one correspondence between all meshes.
In future, we are planning to extend our approach to the
different models.

References
[1]
[2]

[3]

[4]
[5]
[6]

[7]
[8]

[1]S.Ahn, S.Ozawa,”Facial Animation Based on
Muscular Contraction Parameters”
[2]J. Kim, M. Song, I. Kim, Y. Kwon, H. Kim, S.Ahn,
”Automatic FDP/FAP Generation from an Image
Sequence”, IEEE International Symposium on Circuits
and Systems,pp.40-42,May 2000.
[3]BAO-CAI YIN, CHENG-ZHANG WANG, QIN
SHI,”MPEG-4 Compatible 3D Facial Animation Based
On Morphable Model”, Proceedings of the Fourth
International Conference on Machine Learning and
Cybernetics, Guangzhou, 18-21 August 2005.
[4]H. Pyun, Y. Kim, W. Chae, “An Example-Based
Approach for Facial Expression Cloning”, SIGGRAPH
2003.
[5]D Vlasic, M Brand, H Pfister, “Face Transfer with
Multilinear Models”, SIGGRAPH 2005.
[6]J Bu, M You, C Chen, “Facial Animation System for
Embedded Application”, Proceedings of the Second
International Conference on Embedded Software and
Systems.
[7]S Ahn, S Ozawa, “Facial Animation Based on
Muscular Contraction Parameters”.
[8]F Dornaika, F Davoine, “Head and Facial Animation
Tracking using Appearance-Adaptive Models and
Particle Filters”, Proceedings of the 2004 IEEE
Computer Society Conference on Computer Vision and
Pattern Recognition Workshops.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

