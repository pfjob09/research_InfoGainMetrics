Shape Recovery Using HDR Images
Zuoyong Zheng1, Lizhuang Ma1, Zhou Zeng1
Department of Computer Science, Shanghai Jiaotong University
{oliver.zheng@sjtu.edu.cn, ma-lz@cs.sjtu.edu.cn, bluecourse@china.com}
1

Abstract
In this paper, an effective method for shape recovery
using HDRI (High Dynamic Range Images) is proposed.
Firstly, an accurate reflectance map for a reference
sphere relating its radiance values and corresponding
normals is established, then the set of candidate normals
of each target point comprising the same material are
found by comparing its radiance to that of each reference
sphere point. At last, a smoothness operation is applied to
the normals to obtain a reasonable result, and the height
values can be recovered. The experiment results show the
performance of our approach.

1. Introduction and related work
Shape recovery from image(s) is an important research
area in computer vision. The approaches which utilize the
shading information fall under the category of Shape from
Shading (SFS). Shading information of a real object in an
image depends mostly on the reflectance property of the
object surface, which can be described by Bidirectional
Reflection Distribution Function (BRDF). Our goal is just
to recover the shape according to the shading information
in an image, by the following basic image irradiance
equation:
(1)
I ( p) = R(n)
Where I(p) is the irradiance value at pixel p and often take
the form of color observed, and n is the normal of the
object point corresponding to that pixel. The right side of
this equation includes all components about imaging,
where BRDF is the main factor.
The BRDF model in recovering methods is often
Lambertian. This non-physical simplification could not be
adapted to all real-world objects, because of it missing
capability of expressing specularities [1]. E. Prados and O.
Faugeras [2] made the SFS problem well-posed utilizing
the state constraints on the object's boundary and
attenuation term of the illumination, leading to a
numerical method of a Partial Differential Equation
(PDE). Z. Gao, et al. [3] proposed an interactive SFS
method which efficiently uses human knowledge in order
to resolve concave and convex ambiguity. They attempted

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

to overcome the intrinsic ill-posedness of SFS, however,
their applicability are limited due to Lambertian model
used. T. B. Chen, et al. [4] described a simple and robust
method for surface mesostructure acquisition using only
specular reflection as a reliable visual cue, however, this
method need user intervention to control the light source.
To express the reflectance property of a real object,
dense BRDF data can be acquired by either direct
goniormeter measurement or image-based technique, and
then fitted to some analytical reflectance models [5] [6] [7]
[8] to estimate the corresponding parameters. These
models can be also used in SFS. Goldman et al. [9]
introduced a concept of fundamental material, and
expressed the intensity of every pixel by the weighted sum
of fundamental materials. This approach finally leads to a
complex minimization problem. Similarly, Georghiades
[10] expressed pixel depth values with a few discrete
cosine basis and substitute them into the TorranceSparrow model [5], also leading to a minimization
scheme. Goldman and Georghiades’s sophisticated
approaches recover not only the object shape but also the
reflectance properties, so they are very time-consuming
and not so applicable for only recovery-oriented
applications. Hertzmann and Seitz [11] introduced a
concept of orientation consistency and formulated a
photometric stereo system by examples. The shape of a
target object could be recovered by compare its pixel
intensities to those of one or more reference object(s)
whose shape is already known.
Just as Matusik et al. [12] indicated, measured BRDF
values are usually not exactly equal to those computed by
analytical models, furthermore, a specific model can only
represent the reflectance property of a specific kind of
material. In this paper, we proposed our radiance-based
method for recovery, having the following features:
• An accurate reflectance map is obtained by dense
radiance data of a reference sphere, and is used to
replace any analytic reflectance models;
• Diffuse and specular component are treated in a
unified manner.
• Only one view point of image is needed rather than
multiple images used in photometric stereo methods.

We make the following assumption in this paper: (1)
The camera projection is orthographic and the light source
is distant and directional; (2) The effects of shadows,
occlusions and inter-reflections are ignored; (3) The
BRDF of the surface material is isotropic.

2. Shape recovery using HDRI

(2)
g ( Z ij ) = ln Ei + ln ∆t j
where exposure time ∆t j and pixel value Zij are known,
and unknowns are irradiance Ei and function g. The
response curve g can be solved in least square sense, and
an additional constraint (or an assumption) g(128) = 0 is
added to localize it. The recovered response curve is as
follows:
4

2.1 Principle of recovery

2
Relative Radiance

A series of photographs of a reference sphere and
targets objects are taken at different exposure time, under
the same lighting environment. Here, all objects
photographed have the same surface material. These
images are used to synthesize corresponding HDR ones.
Because all sphere points cover different orientations
distributed evenly over the sphere, an accurate reflectance
map can be established. The set of normal candidates of
every target point can be acquired by comparing its
radiance value with that of sphere points. By applying
smoothness operation, we can obtain a reasonable needle
map of a target object and can calculate heights from it.
The experiment setup is shown in Figure 1.
Photographing was carried out in a closed room
surrounded by a piece of black felt. The camera used is
Canon EOS 5D with a 300 mm telephoto lens, and was
fixed on a tripod. All objects photographed were placed on
a circular plate. The light source used is a 5W halogen
bulb mounted on a metallic arm, which can rotate around
the plate. This rotation action was precisely controlled by
a remote computer. The distance from photographed
objects to the light source and the camera exceeded 1500
mm. Such a configuration assured an approximate
orthographic camera projection and directional incident
light ray.

0
-2
-4
-6
0

50

100

150
Pixel

200

250

Figure 2. The response curve of Canon EOS

2.3 Establishing the reflectance map
We select a table tennis as the reference sphere, sprayed
with shiny green paint. With the same exposure time series
as used by recovering of the camera response curve, 14
images for the table tennis are photographed.

(a)

(b)
Figure 3: Image series of the reference spheres
(a) and a target object (b) at different exposure
time.

Figure 1. Experiment setup

2.2 Obtaining camera response curve
We use Debevec’s method [13] to compute the camera
response curve. A selected static scene was photographed
with total 14 different exposure time ranging from 1/40 to
10 seconds. The following camera response function g
relates the CCD irradiance Ei at pixel location i and an
exposure time ∆ t j to the corresponding pixel value Zij:

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

The circular silhouette of the sphere is fitted through a
few edge points found by Sobel gradient operator. Now,
we get the center point coordinate (x0, y0) and radius r of
the sphere (in pixels). Because we assume orthographic
projection of the camera, so the height of every internal
point i of the sphere can be calculated simply by
zi = r2 − (xi − xo )2 − ( yi − y0 )2
. The coordinate (xi, yi, zi) is
then normalized to form an orientation vector n.
On the other hand, the irradiance value of point i can be
calculated by the following formula derived from equation
(2):

1 P
(3)
∑ [ g ( Z ij ) − ln ∆t j ]
P j =1
It should be noted that the weight function w(z) actually
plays a role of a filter (weighted averaging), which
removes the impact of image noise.
The mapping from CCD irradiance to actual object
radiance are constant for most modern cameras at f/8 and
smaller apertures [14], so we do not distinguish between
them in the following sections. To establish a reflectance
map of the green paint we used as a surface material, all
candidate normals for any possible radiance value E must
be found. This could be achieved by a radiance linear
interpolation of the triangular mesh which closely
approximates the sphere (Figure 4).
ln Ei =

Figure 4. Radiance linear interpolation. If a
specified radiance value falls between those of
two end points of a triangle edge, we use linear
interpolation to calculate its corresponding
location at the edge, and replace the location
with the nearest end point. Because every
triangle is very small (the sphere has around
over 18,000 pixels), this replacement has little
effect on accuracy.

Using a reference sphere, we eventually obtained an
accurate reflectance map relating a radiance value to its
corresponding normals. This discrete reflectance map can
be used to substitute for any analytic BRDF model and can
handle the diffuse and specular reflectance in an unified
framework. In contrast, almost all BRDF models have a
highly non-linear specular term therefore numerically
difficult to solve. It should be indicated that establishing
this reflectance map is in fact equivalent to a BRDF
measurement under a fixed view point and lighting
direction [15] [16].

2.4 Obtaining the needle map of target objects
The target object was segmented from background
utilizing color variation. The points belonging to the
object form a closed and connected region. The normal of
points at occluding boundary were calculated in advance
and hold constant in the following iteration (Figure 5).

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

Figure 5. Normals at the occluding boundary

The set of candidate normals of each internal target
point could be acquired by comparing its radiance to that
of every reference sphere point, which is a brute-force
searching step already exhibited in Figure 4. Once a
sphere point with the same radiance value is found, its
normal is chosen as one of the optional normals for the
target point.
We fused the acquired set of the candidate normals for
every target point into the shape recovery framework
presented by Worthington and Hancock [17]. The normal
at every internal target point is initialized such that it is the
nearest one among the candidate normals to the opposite
direction to the image gradient. This kind of initialization
implies an assumption that bright regions correspond to
peaks and image gradient direction points toward these
peaks.
Next, we applied a needle-map smoothness operation
using a robust regularizer developed by Worthington and
Hancock [18]. In contrast to classical Horn and Brooks’
quadratic smoothness constraint [19], which make the
reconstructed surface oversmooth, the regularizer can
preserve more surface detail. The robust regularizer
constraint function is defined as:
 ∂n 
 ∂n 
(4)

 + ρσ 
ψ (n, N ) = ρσ 


 ∂x 
 ∂y 
Where n is the normal at pixel location (i, j), i.e., nij. N is
the 4-neighborhood normal about location (i, j). ρσ is the
sigmoidal derivative M-estimator which is define as:
πη
σ
(5)
ρ σ (η ) = log cosh( )
π
σ
where parameter η is the residual and σ the width
parameter. To minimize the robust regularizer constraint
function, the calculus of variation is applied, leading to an
update equation about nij:
 ∂n ( k )   ∂  ∂
 ∂n ( k )  
∂  ∂
  +  ( k ) ρσ 

n ( k +1) =  ( k ) ρσ 
  ∂y  ∂n
 ∂y  
∂x  ∂n x
∂
x
y





(6)
( k +1)
where n
is the current updated normal from the last
(k )
result n . In addition to this smoothness constraint,
image irradiance equation (1) must be satisfied. That is to
say, the radiance value of every target point must be equal
to that one calculated from the series of photographs,
which is a hard constraint. So, these two constraints form
an iteration. Every time the smoothness operation is

applied, the resulting normals may deviate, and violate the
image irradiance equation. These normals are then
adjusted to satisfy the hard constraint (Figure 6), and the
next smoothness is applied. The iteration terminate until
all normals converge.

average, respectively (Table 1).
The number of these target points range from around
6,300 to 22,000. Obtaining final normals require about 11
to 18 seconds, and recovering heights need about 180 to
255 seconds. All the work was done on a computer with
Athlon 3000+ CPU (main frequency 1.8GHz) and 1GB
DDR400 memory.

(a)
Figure 6. Normal adjustment after a pass of
smoothness. The smoothed normal maybe
deviates, and needs to be replaced with the
nearest one to it which satisfy the image
irradiance equation for the point P.

(b)

Because the reflectance model used in Worthington and
Hancock’s paper is just only Lambertian with an explicit
expression of a normal cone, calculating n(k+1) from n(k) is
simple and direct. In our data-driven and discrete
reflectance map including the specular component, n(k+1) is
calculated by searching in the candidate normal set for the
nearest one to n(k):

n

( k +1)

(k )

= arg max(ni ⋅ n )

(c)

(7)

ni

where the operator (.) denotes dot product and subscript i
the element index of the candidate normal set. Obviously,
solving for the nearest normal is replaced with a searching
procedure.

2.5 Recovering heights from the needle map
After the needle map is obtained, the height z of every
target point can be recovered by minimizing the following
objective function [20]:
∂z
∂z
(8)
Ψ ( z ) = (n
+ n )2 + (n
+ n )2

∑

z

∂x

x

z

∂y

y

where (nx, ny, nz) is the normal recovered for each point,
and ∂ z and ∂ z is approximated by finite difference. This
∂x

∂y

leads to a sparse linear equation system and can be solved
by Gauss-Siedel iteration.

3. Experiment results
We recovered four objects, including the reference
sphere itself, another two non-regular objects and a
synthesized vase. Because there exist ground-truth for the
sphere and the vase, we can compute the total normal error
converged with the smoothness iteration. The normal error
for the sphere and vase converge after about 20 and 12
iterations, at the order of 1° and 4.5° per point on

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

(d)
Figure 7. Reconstructed surfaces: the reference
sphere itself (a), arbitrary objects (b), (c) and a
synthesized vase (d). The left, middle and right
column show the original image, reconstructed
triangular meshes and needle maps for the four
objects, respectively.
Table 1. Points number, computational time and average
normal error for the reference sphere and the synthesized
vase.
Object
Sphere (a)
Vase (d)

Points
18,000
6,300

Time (sec)
188
87

Average
Error

Normal
1.12°
4.43°

4. Discussion
Because there exists radiance data for RGB channels,
we compared the surfaces reconstructed using the radiance
of these three channels. Figure 8 shows the candidate

normals (indicated by the corresponding color) for which
the radiance of three channels for the reference sphere is
respectively equal to those of a target point selected
arbitrarily, which are 0.0054(red), 0.0324(green) and
0.0068(blue). The consistency among the normals in the
three figures implies the reconstruction is independent of
which channel of radiance data will be used.

The main reconstruction steps in this paper, including
the segmentation of target objects, the recovering of the
normals and heights can be automated. Furthermore, no
calibration of the light source and camera is needed. Based
upon the above reasons, the method proposed in this paper
is effective and easy to use. Future works include testing
more target objects with various shapes, and trying to use
local curvature consistency as smoothing constraints.

6. Acknowledgments

Figure 8. All optional normals found by
comparing a target point’s radiance values at
RGB channel to those of the reference sphere.

In addition, it should be noted that the same positions
for the camera and light source (called coaxial light source
in industry) will bring great benefit for the recovering of
normal field. In such kind of a situation, the radiance map
of the reference sphere comprises the “radiance loops”
(Figure 9). For each point in such a loop, the radiance
values are identical. The special case means that an
analytical expression of candidate normals for a specific
radiance value can be given and accelerate much the
recovering of target normals, rather than the slow
interpolation-searching scheme used in section 2.3 and 2.4.

Figure 9. Radiance loops.

5. Conclusion
This paper proposes a method for finding all possible
orientations for each point on a reconstructed surface,
based on the physical concept radiance. Whether in
single-view recovery or in photometric stereo, the main
advantage of using HDRI is its capability of handling the
specular and diffuse reflectance component in a uniformed
manner. That means the specular highlight is not an
obstacle for surface reconstruction any more, instead,
providing the reliable cue same as the diffuse reflection.
However, more orientation smoothness constraints used
with single light source need to be tested and evaluated to
approximate the true surfaces.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

This work is supported by the National Basic Research
Program of China (973 Program) under Grant No.
2006CB303105, and partially sponsored by Omron
Corporation, China.

References
[1]

R. Zhang, P. S. Tsai, J. E. Cryer, et al. “Shape from
shading: a survey”, IEEE Transaction on Pattern Analysis
and Machine Intelligence, 1999, 21(8), pp. 690-706.
[2] E. Prados, and O. Faugeras. “Shape from shading: a wellposed problem?”, In Proceedings of the 2005
International Conference on Computer Vision and Pattern
Recognition (San Diego, California). pp. 870-877, 2005.
[3] Z. Gang, M. Yasuyuki, Q. Long, et al. “Interactive shape
from shading”, In Proceedings of the 2005 International
Conference on Computer Vision and Pattern Recognition
(San Diego, California). pp. 343-350, 2005.
[4] T. B. Chen, M. Goesele, and H. P. Seidel. “Mesostructure
from specularity”, In Proceedings of the 2006 International
Conference on Computer Vision and Pattern Recognition
(New York), pp. 1825-1832, 2006.
[5] K. E. Torrance, and E. M. Sparrow. “Theory for offspecular reflection from roughened surfaces”, Journal of
the Optical Society of America, 1967, 57(9), pp. 1105 1114.
[6] R. L. Cook, and K. E. Torrance. “A reflectance model for
computer graphics”, ACM Transactions on Graphics, 1982,
1(1), pp. 7-24.
[7] M. Oren, and S. K. Nayar. “Generalization of Lambert's
reflectance model”, In Proceedings of SIGGRAPH'94, pp.
239-246, 1994.
[8] G. Ward. “Measuring and modeling anisotropic reflection”,
In Proceedings of SIGGRAPH '94, pp. 239-246, 1994.
[9] D. B. Goldman, B. Curless B, A. Hertzmann, et al. “Shape
and spatially-varying BRDFs from photometric stereo”, In
Proceedings of the 10th IEEE International Conference on
Computer Vision, pp. 341-348, 2005.
[10] A. S. Georghiades. “Recovering 3-D shape and reflectance
from a small number of photographs”, In Proceedings of
the 8th IEEE International Conference on Computer
Vision, pp. 816-823, 2003.
[11] A. Hertzmann, and S. M. Seitz. “Example-based
photometric stereo: shape reconstruction with general,
varying BRDFs”, IEEE Transaction on Pattern Analysis
and Machine Intelligence, 2005, 27(8), pp. 1254-1264.
[12] W. Matusik, H. Pfister, M. Brand, et al. “A data-driven
reflectance model”, ACM Transactions on Graphics, 2003,
22(3), pp. 759-769.

[13] P. E. Debevec, and J. Malik. “Recovering high dynamic
range radiance maps from photographs”, In Proceedings of
the 24th Annual Conference on Computer Graphics and
Interactive Techniques, pp. 369-378, 1997.
[14] C. Kolb, D. Mitchell, and P. Hanrahan. “A realistic camera
model for computer graphics”, In Proceedings of the 22nd
Annual Conference on Computer Graphics and Interactive
Techniques, pp. 317-324, 1995.
[15] K. J. Dana, S. K. Nayar. B. V. Ginneken, et al.
“Reflectance and texture of real world surfaces”, ACM
Transactions on Graphics, 1999, 18(1), pp. 1-34.
[16] S. R. Marschner, S. H. Westin, E. P. F. Lafortune, et al.
“Image-Based BRDF measurement including human skin”,
In Proceedings of the Eurographics Workshop on
Rendering (Vienna, Austria), pp. 131-144, 1999.
[17] P. L. Worthington, and E. R. Hancock. “New constraints
on data-closeness and needle map consistency for shapefrom-shading”, IEEE Transaction on Pattern Analysis and
Machine Intelligence, 1999, 21(12), pp.1250-1267.
[18] P. L. Worthington, and E. R. Hancock. “Needle map
recovery using robust regularizers”, Image and Vision
Computing, 1999, 17(8), pp. 545-558.
[19] B. K. P. Horn, and M. J. Brooks. Shape from shading, MIT
Press, Cambridge, Mass., 1989.
[20] D. A. Forthyh, and J. Ponce. Computer vision: a modern
approach, Prentice Hall, 2003.

Computer Graphics, Imaging and Visualisation (CGIV 2007)
0-7695-2928-3/07 $25.00 © 2007

