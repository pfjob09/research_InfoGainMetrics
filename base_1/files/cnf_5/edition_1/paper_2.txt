2011 Eighth International Conference Computer Graphics, Imaging and Visualization

Stabilizing Marker-based Visual Tracking Using Markers with Scattering Materials
and Multiple Cameras

Kikuo Asai
Center of ICT and Distance Education
The Open University of Japan
Chiba, Japan
asai@ouj.ac.jp
Since marker-based visual tracking is processed on the
basis of video images of a camera, brightening the square
and pattern on a marker due to illumination changes is one of
the main causes of the estimation errors [3]. However,
illumination conditions are often difficult to control in
practical environments. Another problem is that the
geometric relationship of the camera and the square marker
affects the tracking stability [11]. The orientation becomes
unstable when the optical axis of the camera is perpendicular
to the surface of the square marker, because the orientation
estimation becomes sensitive to fluctuations on the edges
and corners of a square marker.
Here, we propose using scattering materials for the
square frame and pattern shape on a marker, so that
extraction of 2D features becomes robust to illumination
changes. We tried woolen paper out as the scattering material,
holding down a specular reflection component. This method
is very simple because software does not need to be changed,
though a user has to make markers with woolen paper.
In addition, we use multiple cameras to stably estimate
the position and orientation when a camera is directed along
the axis perpendicular to the marker surface. Multiple
cameras have usually been applied to stereo vision,
calculating depth distance to an object. We tried multiple
cameras out as alternative ballast to stabilize the position and
orientation estimation. We developed a method for
interpolating the transformation matrices for the multiple
cameras on the basis of the level of instability. Then, we
implemented the ballast function through the position and
orientation derived from the ARToolkit [12] in each camera,
which enabled us to easily apply the multiple-camera ballast
function to other systems.

Abstract—The marker-based method in visual tracking is
generally convenient, while the stability of the position and
orientation estimation of the camera depends on the
illumination conditions because it is processed on the basis of
the camera images. However, illumination conditions are often
difficult to control in practical environments. Although some
solutions have been proposed for illumination changes, the
methods based on image processing have unstable estimation
due to specular reflection on markers. Thus, effects on
illumination were avoided by using markers with scattering
materials. In addition, multiple cameras were also used for
stabilizing the orientation estimation when the optical axis of
the camera is almost perpendicular to the marker surface.
Keywords-visual tracking; marker-based cue; augmented
reality; scattering materials; multiple cameras

I.

INTRODUCTION

Image processing technology enables visual tracking to
be widely used for estimating position and orientation of
objects in various applications. Augmented reality (AR)
applications require accurate geometric registration between
virtual objects and the real world, overlaying the virtual
objects onto the real world [1].
Geometric registration is achieved by tracking the
movement of the real scene captured by a camera. The
geometric registration results in measuring the motion of the
camera that takes video images of the real scene. Various
vision-based methods have been proposed for visual tracking
such as marker-based [2, 3, 4], model-based [5, 6], featurebased [7, 8], and sensor-combined [9, 10] methods.
The marker-based methods have a practical advantage
and disadvantage. Markers in scenery make the users aware
of the markers’ presence but blot the appearance. The
marker-based methods are implemented with simple imageprocessing and accurately estimate position and orientation
of a camera.

978-0-7695-4484-7/11 $26.00 © 2011 IEEE
DOI 10.1109/CGIV.2011.32

II.

RELATED WORK

Various methods have been proposed to improve the
stability in marker-based visual tracking. Even ARToolkit
[12] includes some functions for stable tracking. The
position and orientation is calculated at every frame of the
video image captured by a camera, which involves jitter
caused by random noise. ARToolkit tries to reduce jitter
using the previous frame as well as the present frame.
ARToolkit also prepares a multi-marker function for
avoiding occlusion problems that happen when part of the
marker is hidden in a video image. This function can
extrapolate the position and orientation of the hidden
7

markers from that of the one fully captured in the image
frame.
AR applications require not only stable tracking but also
correct identification of markers. Error detection codes have
been used for detecting and correcting identification errors of
markers [3, 13]. The error codes, such as parity checks and
checksums with forward error correction, recover the
identification errors caused by occlusion and illumination
changes.
Recent advances in computer vision have enabled us to
take vision-based approaches to the stable tracking under
illumination changes. Reliable tracking was achieved by
integrating edge extraction with texture analysis [14].
Considering illumination on textures improved tracking
robustness. A similar method for reliable tracking has been
used that involves jointly optimizing the parameters in the
photometric model of illumination changes combined with
the geometric model of a planer surface [15]. A background
subtraction has been achieved under the conditions of sudden
illumination changes by using a statistical illumination
model [16]. This background subtraction technique was
successfully applied to the occlusion segmentation in AR.
In general, marker-based tracking is sensitive to
occlusion and illumination changes, whereas feature pointbased tracking lacks robustness to drastic scene changes. The
complementary performance in visual tracking has been
obtained by integrating marker-based and feature pointbased cues [17]. Another camera was added for improving
estimation reliability. The tracking of a user’s view camera
attached to a head-mounted display (HMD) was made more
stable by tracking the HMD with a bird’s eye view camera
[18]. A filtering technique has also been used for improving
accuracy of marker-based tracking, especially in conditions
where a camera is directed along the axis perpendicular to
the marker surface [11].
Although the above methods largely improved the
performance in visual tracking, they require expensive sensor
devices or complicated algorithms in the systems. We
developed a simple method that uses inexpensive devices for
improving reliability of visual tracking by using markers
with scattering materials and multiple cameras.
III.

Wool paper

Inkjet-printed paper

Figure 1. Highlights reduced more with woolen paper than with inkjetprinted paper.

Woolen paper was chosen as scattering materials because
of its high absorption. Woolen paper scatters light with the
wool fibers on the surface, which is generally produced by
transplanting synthetic fibers straight on drafting paper.
Woolen paper has often been used for preventing
windowpane reflection and controlling illumination at a
photography site. Figure 1 compares the markers of the
woolen paper and the inkjet-printed paper. The woolen paper
reduced highlights on the marker surface more than the
inkjet-printed paper.
IV.

STABILIZATION WITH MULTIPLE CAMERAS

Although a marker using woolen paper stabilizes visual
tracking by reducing highlights on the square frame and
patterns, it requires a user to make markers with woolen
paper. Even though this problem was solved by methods
such as manufacturing square frames and patterns using an
automatic cutter, the method for using scattering materials
for markers is invalid because of another problem: the
orientation is unstable when the optical axis of the camera is
perpendicular to the marker surface.
We designed a ballast system by interpolating position
and orientation among multiple cameras on the basis of the
level of instability of the estimation. In the system, we take
account of 1) tremors of the position and orientation, 2)
geometric relationship between markers and cameras, 3)
deviation from other cameras in the position, and 4) marker
detection. Figure 2 schematically overviews the stabilization
processing.

SCATTERING MATERIALS FOR MARKERS

For paper considered to have rough surfaces, the
relationship between topography and gloss on the surfaces
has been investigated empirically [19, 20]. A paper surface is
approximately expressed as a combination of specular
reflection and diffuse reflection. The component of specular
reflection causes highlights on a marker surface in the video
images in accordance with the geometric relationship of the
light sources, camera, and marker. A square frame and
patterns printed with a laser or inkjet printer become shiny
on the marker, resulting in recognition errors of the patterns
and detection losses of the markers. We tried using scattering
materials for the square frame and the patterns on the
markers to reduce the recognition errors and the detection
losses by suppressing the specular reflection on the marker
surfaces.

8

Camera 1

Camera 2

Figure 3 shows a geometric relationship of a light source,
a marker, and multiple cameras. We calculate the weight
values for each camera to the stable estimation rather than
determine the best estimation among multiple cameras.
Although ARToolkit [12] was used here for estimating
position and orientation of a camera, other libraries can be
implemented easily in the stabilization processing.

Camera 3

Input image
Position
estimation
Tracking

Position
estimation

Position
estimation

Transformation

Transformation

B. Geometry of Camera Tracking
Let us consider geometry of the position and orientation
estimation. The object coordinates are placed at the tracking
object, which is a marker in our case. The camera
coordinates are placed at the viewpoint of each camera. The
screen coordinates are placed at the projection plane.
The screen coordinates are expressed with the object
~ as hu
~ , where C is the
~ =C R |t m
coordinates m
k
k
k

Grouping of marker patterns

[

Composition of matrices for multiple cameras
Tremors
Deviation from
other cameras

Geometric relationship

R k a 3 x 3 rotation matrix, t k a 3
x 1 translation vector, and k the number of cameras. The

camera calibration matrix,

Marker detection

symbol ‘~’ denotes the homogeneous coordinates. Since the
perspective transformation between the camera coordinates

Stabilization

~
~ is formed with C , u
~ is
f and the screen coordinates u
~
~ = C f . The transformation between camera
defined as hu
~
~ , where
and object coordinates is expressed as fk = Pk m
Pk = [R k | t k ] . Thus, we obtain the relationship between
~
−1 ~
the camera coordinates, for example, f2 = P2 P1 f1 at k =1,

Video and CG rendering
Output image
Figure 2. Overview of stabilization processing.

A. Interpolating Position and Orientation
The position and orientation among multiple cameras are
interpolated on the basis of the assumption that the geometric
relationship is fixed among multiple cameras. In principle,
the position and orientation can be derived from video
images of any camera by calibrating in advance. However,
the geometric condition produces a slight difference among
the multiple cameras. One of the multiple cameras might
have the optical axis that is not perpendicular to the marker
surface. Another of the multiple cameras might not capture
specular reflection from the light source on the marker. That
is, either camera is expected to capture video images for
stably estimating the position and orientation.

2.

Defining the object coordinates correctly requires
detecting the accurate shape of the square frame on the
screen coordinates. The multiple-camera environment
moderates the severe requirements under the conditions in
which the geometric relationship among multiple cameras is
given with the transformation matrices.
C. Stabilizing Process
One of multiple cameras is set as datums, and the video
images captured by this camera are used for presentation to
users. However, the images from the main camera are not
directly used to estimate its position and orientation. The
position and orientation are estimated by interpolating them
among multiple cameras, where the camera coordinates of
the other cameras are transformed into those of the main one,
assuming that the geometric relationship is fixed among the
multiple cameras.
The position and orientation are interpolated by a
weighted mean of the transformation matrices between the
main camera and the object coordinates with weight based
on the level of instability. The weighted mean of the
transformation
matrices
is
calculated
as

Light source
Camera 1

Camera 4

]

Camera 2

P = w1P1 + ∑ wk Q k , where wk is the weight of the

Camera 3

k

transformation matrices of the camera at the number
k (k ≥ 2) , and Q k is the transformation from the cameras’

Marker

Figure 3. Geometry of light source, marker, and multiple cameras.

(k ≥ 2) transformation matrices to the main one’s. The

9

weight
as w =

for

each

wt + wg + wd

camera
is
defined
+ wm , where each component of the

the others, it has the lowest reliability. The reliability is
calculated with deviation of the position between one
camera and the others. The weight wd of the deviation is
defined as the distance b between the point of one camera
and the center of points at the other cameras in the
transformation matrices, as shown in Figure 5 (b). The
weight of the deviation is calculated for each camera, and
decreases with distance b for the camera.

weight expresses the level of instability that includes tremors
of the position and orientation, wt , the geometric
relationship between markers and cameras,
from other cameras in the position,

wg , deviation

wd , and marker

detection, wm . The balance among the weight components
is determined by trial and error. Next, we explain how to
calculate each weight.
1) Tremors of position and orientation: The tremors are
often caused on the orientation at the same position. The
reference point is introduced to detect the tremors of the
orientation and taken at the point along the z axis on the
central point, as shown in Figure 4 (a). The weight wt of

y

b

z

x

φ

Marker

the tremors is determined with the angle θ and the
distance d , as shown in Figure 4 (b).
θ is the angle between the line between two points at the
current frame t and the previous frame t − 1 and the line
between two points at the previous frames t − 1 and t − 2 .
When the angle at each frame is close to 180 deg., the
reference point is interpreted to have tremors.
When the camera moves smoothly, the translation of the
reference point should not be very large. That is, the distance
d between the points at the frames t and t − 1 can be a kind
of index for expressing the instability. The thresholds are
determined by trial and error.

Camera

Center of points
at other cameras

Figure 5. Weight based on geometry and deviation.

4) Marker detection: The visual tracking system
sometimes repeats detection and loss of markers for short
periods of time due to the illumination conditions, even if
the markers are presented in the scene image. The frame
number with marker detection during certain time is counted
as the weight wm of the marker detection.
V.

Reference point
Reference point

t-2

t
y

t-1

EXPERIMENTS

We implemented the stabilizing methods into the imageprocessing libraries, ARToolkit on a PC with Windows XP
in OS, Pentium4 3.6 GHz in CPU, 2 GB in memory, nVidia
9800 in GPU, and USB cameras (640x480 pixels). Figure 6
shows the set up of the multiple cameras. The optical axis of
each camera slightly differs. The square frame of the marker
is 40 x 40 mm.
We did an experiment to investigate the performance in
terms of the stability of the position and orientation
estimation using a marker with woolen paper and multiple
cameras. The scattering materials for a marker worked well,
and the camera did not capture the highlights of the specular
reflection on the marker surface.
Figure 7 shows a screenshot of overlaying a wireframe
cube onto the video image at the position and orientation
estimated by four cameras. The video images at the bottom
in the screenshot were captured by all cameras except the
main one.

d

z

Point of one camera

Marker

θ

x
Figure 4. Weight based on tremors.

2) Geometric relationship between markers and
cameras: The orientation estimation becomes unstable when
the optical axis of the camera is roughly perpendicular to the
marker surface. The instability is expected to increase at
around 90 deg in the angle ϕ between the vertical axis (z
axis) on the marker surface and the optical axis of the
camera, as shown in Figure 5 (a). The weight wg of the
geometry relationship is set to change with the angle ϕ . The
perpendicularity is measured with the shape of the square
frame on the marker.
3) Deviation from other cameras in the position: When
the estimation of one camera largely differs from those of

10

X axis

[mm]

60

4040
20

00
-20

One camera

-40
-40

Two cameras

-60
20

Two cameras

Y axis

00
-20
-40
-40
-60

One camera

-80
-80
-100
-120
-120

Two cameras

900
900

Z axis

Figure 6. Set up of multiple cameras.

850
800
800

One camera

750
700
700
650

0

50

100
150
200
Frame number
[frames]

Figure 8. Position of reference point estimated by one camera and two
cameras.

[mm] 8080

One camera

X axis

60

4040
20

00
-20
-40
-40

Four cameras

-60

Figure 7. Screenshot of overlaying cube onto video image at position and
orientation estimated by using four cameras.

-80

4040

One camera

Y axis

20

Figure 8 shows the position of the reference point
estimated by using one camera and two cameras. The camera
was moved to change the orientation so that the optical axis
was around the direction perpendicular to the marker surface.
The position of the reference point estimated from one
camera often suddenly rose and dipped during 40 frames,
whereas the position estimated from two cameras changed
smoothly. Although both positions are not very different
from each other, the difference could largely affect
instability in appearance. The estimation using three and four
cameras obtained results similar to those using the two
cameras, enabling the reference point to more smoothly
change position, but reducing the frame rates because of
heavier loads for processing video images. Figure 9 shows
the position of the reference point estimated by using one
camera and four cameras.

00
-20
-40
-40
-60

-80
-80

Four cameras

-100
-120
900

900

One camera

Z axis

850
800
800
750
700
700
650

Four cameras
0

50

100
150
200
Frame number
[frames]

Figure 9. Position of reference point estimated by one camera and four
cameras.

11

VI.

[6]

CONCLUSION

We have improved stability in marker-based visual
tracking, enabling the position and orientation of a camera to
be reliably estimated. The scattering materials for square
frames and patterns on a marker reduced the specular
reflection component, resulting in robustness to illumination
changes. The multiple cameras were also used for stabilizing
the orientation estimation when the optical axis of the
camera was almost perpendicular to the marker surface. A
ballast system was designed by interpolating the position and
orientation among the multiple cameras on the basis of the
level of instability. Future work includes implementing the
methods into other image-processing libraries and evaluating
the performance on the position and orientation estimation.

[7]

[8]
[9]

[10]

[11]

ACKNOWLEDGMENT

[12]
[13]

This work is partly supported by a Grant-in-Aid from the
Ministry of Education, Culture, Sport, Science, and
Technology in Japan. We are grateful to Mr. Norio Takase
for his efforts in the system implementation and Dr.
Tomotsugu Kondo for his advice about scattering materials
for markers.

[14]

[15]

REFERENCES
[1]

[2]

[3]

[4]

[5]

R. Krevelen, and R. Poelman, A survey of augmented reality
technologies, applications and limitations, International Journal of
Virtual Reality, vol.9, 2010 pp. 1–20.
H. Kato, and M. Billinghurst, Marker tracking and HMD calibration
for a video-based augmented reality conferencing system, Proc.
International Workshop on Augmented Reality, 1999, pp.85–94.
M. Fiala, ARTag, a fiducial marker system using digital techniques,
Proc. IEEE Conference on Computer Vision and Pttern Recognition,
vol.2, 2005, pp.590–596.
D. Wagner, and D. Schmalstieg, ARToolKitPlus for pose tracking on
mobile devices, Proc. Computer Vision Winter Workshop, 2007,
pp.139–146.
V. Lepetit, L. Vacchetti, D. Thalmann, and P. Fua, Fully automated
and stable registration for augmented reality applications, Proc.
International Symposium on Mixed and Augmented Reality, 2003,
pp.93–102.

[16]

[17]

[18]

[19]

[20]

12

D. Kotake, K. Satoh, S. Uchiyama, and H. Yamamoto, A fast
initialization method for edge-based registration using an inclination
constraint, Proc. International Symposium on Mixed and Augmented
Reality, 2007, pp.239–248.
V. Lepetit, P. Lagger, and P. Fua, Randomized trees for real-time
keypoint recognition, Proc. IEEE Conference on Computer Vision
and Pattern Recognition, vol.2, 2005, pp.775–781.
M. Pupilli, and A. Calway, Real-time camera tracking using a particle
filter, Proc. British Machine Vision Conference, 2005, pp.519–528.
A. State, G. Hirota, D. T. Chen, W. F. Garret, M. A. Livingston,
Superior augmented reality registration by integrating landmark
tracking and magnetic tracking, Proc. SIGGRAPH, 1996, pp.429–438.
S. You, U. Neumann, and R. Azuma, Hybrid inertial and vision
tracking for augmented reality registration, Proc. IEEE Conference on
Virtual Reality, 1999, pp.260–267.
Y. Uematsu, and H. Saito, Improvement of accuracy for 2D markerbased tracking using particle filter, Proc. International Conference on
Artificial Reality and Telexistence, 2007, pp.183–189.
ARToolkit, http://www.hitl.washington.edu/artoolkit/
T. Kawano, Y. Ban, and K. Uehara, A coded visual marker for video
tracking system based on structured image analysis, Proc.
International Symposium on Mixed and Augmented Reality, 2003,
pp.262–263.
M. Pressigout, and E. Marchand, Hybrid tracking algorithms for
planar and non-planar structures subject to illumination changes, Proc.
International Symposium on Mixed and Augmented Reality, 2006,
pp.52–55.
G. Silveira, and E. Malis, Real-time visual tracking under arbitrary
illumination changes, Proc. IEEE Conference on Computer Vision
and Pattern Recognition, 2007, pp.1–6.
J. Pilet, C. Strecha, and P. Fua, Making background subtraction
robust to sudden illumination changes, Proc. European Conference on
Computer Vision, 2008, pp.567–580.
D. Marimon, Y. Maret, Y. Abdeljaoued, and T. Ebrahimi, Particle
filter-based camera tracker fusing marker and feature point cues, Proc.
IS&T/SPIE Conference on Visual Communications and Image
Processing, vol.6508, 2007, pp.1–9.
K. Satoh, S. Uchiyama, H. Yamamoto, and H. Tamura, Robot visionbased registration utilizing bird’s-eye view with user’s view, Proc.
International Symposium on Mixed and Augmented Reality, 2003,
pp.45–55.
H. Lipshitz, M. Bridger, and G. Derman, On the relationships
between topography and gloss, Tappi Journal, vol. 73, 1990, pp.237–
245.
M. MacGregor, P. Johansson, and M. Beland, Measurement of smallscale gloss variation in printed paper, Proc. International Printing and
Graphics Arts Conference, 1994, pp.33–43.

