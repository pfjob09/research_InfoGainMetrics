A Visual Workspace for Hybrid Multidimensional Scaling Algorithms
Greg Ross'

Matthew C h a h "
Department of Computing Science,
University of Glasgow,
Glasgow,
United Kingdom

Abstract
Other features of the data set also affect the applicability of
algorithms, such as data distribution. For example, K-means
clustering [MacQueen 19671 is most effective when the data is
distributed in spherical Gaussian clusters [Bradley and Fayyad
19981.

In visualising multidimensional data, it is well known that
different types of data require different types of algorithms to
process them. Data sets might he distinguished according to
volume, variable types and distribution, and each of these
characteristics imposes constraints upon the choice of applicable
algorithms for their visualisation. Previous work has shown that a
hybrid algorithmic approach can be successful in addressing the
impact of data volume on the feasibility of multidimensional
scaling (MDS). This suggests that hybrid combinations of
appropriate algorithms might also successfully address other
characteristics of data. This paper presents a system and
framework in which a user can easily explore hybrid algorithms
and the data flowing through them. Visual programming and a
novel algorithmic architecture let the user semi-automatically
define data flows and the co-ordination of multiple views.

In a working environment, corporate memory and project-specific
databases tend to start off small and gradually evolve into large
information repositories. While it would be feasible to visualise
the inter-object relationships with a force-dirccted layout
algorithm in the infancy of such a database, it would become less
and less effective as the database matures and demands a more
computationally feasible solution. Previous work has shown that
hybrid algorithmic approaches to visualisation scale up to
relatively high-volume data sets, even though some of the
constituent algorithms would be too costly on their own if applied
to the entire set [Morrison et al. 20021. This would suggest that
when applied to a growing database, algorithmic steps could be
bypassed in the repository's infancy and incorporated as it
approaches maturity Or, in the case that volume fluctuates, the
hybrid algorithm could fluctuate and adapt with it.

CR Categories: 1.5.3 [Pattern recognition]: Clustering Algorithms; E.l [Data Structures]: Graphs and networks; D.1.7
[Programming Techniques]: Visual Programming; 1.3.6
[Computer Graphics]: Methodology and Techniques - Interaction
techniques;

We present an implemented system and framework called HIVE
(Hybrid Information Visualisation Environment) that utilises
direct manipulation to allow users to interactively create and
explore hybrid MDS algorithms. Figure 1 shows screen-shots of
the system. Visual programming and a novel algorithmic
architecture are proposed as a means to let the user semiautomatically co-ordinate multiple views and define data flows.

Keywords: Data-flow, visual programming, multidimensional
scaling, multiple views, hybrid algorithms, complexity

1 Introduction
There is a multitude of algorithms available for clustering and
laying out abstract data. The different algorithmic approaches
seem to be tailored to specific types of data. Some algorithms
perform well with data sets of low cardinality and dimensionality,
such as the basic spring model [Eades 19841. Other algorithms
work best with high cardinality data, an example of which is the
self-organising map or SOM [Kohonen el al. 20001. In training, a
substantial training set allows the SOM to reveal complex nonlinear structure in a very large body of data.
..............................................
'e-mail: g@dcs.gla.ac.uk
'c-mdil: matthewadcs.gla.uc.uk

This paper is organised into seven sections. Section two describes
related work including the data-flow model and visual
programming. Section three illustrates the hybrid algorithmic
framework, upon which the system is built. Section four describes
the HIVE architecture and implementation. Early experience of
using HIVE is discussed in section five. Finally, sections six and
seven present future work and conclusions respectively.

2 Related work
The HIVE system permits users to easily create and experiment
with hybrid algorithms for generating visualisations of their data.
This process is a visual one in that algorithms and visualisations
are represented by visual components that afford direct
manipulation. The following sub-sections describe topics in the
literature that have influenced HIVE'S development.

IEEE Symposium on Information Visualization 2003,
October 19-21, 2003, Seattle, Washington, USA
0-7803-8154-8/03/$17.00 02003 IEEE

91

Figure 1. Two screen-shots of the HIVE interface. The image on the left illustrates interconnected components that import,
transform and render multidimensional data. The algorithmic components collectively represent the O(N& hybrid MDS
algorithm of [Morrison et al. 20021. Thick lines that link modules represent data-flows while thin ones, connecting scatterplots
and other visualisations, represent the connections between interlinked interactive views. The image on the right shows the same
scatterplots enlarged and supplemented with a fisheye table component. The data consists of 5000 points sampled from a 3D ' S '
shaped distribution
control was pre-set and the scientists and engineers had to make
do as hest as they could for their tasks. In the words of Haeberli,
2.1 Visual programming
"Instead of the user driving an applicalion, the user is often
driven and constrained by the application. "
At around the time when scientific visualisation was being
established, the concept of visual programming was also
Visual programming addressed a number of these problems,
becoming prominent [Haeberli 1988; Upson et al. 19891.
moving away from these monolithic and static applications and
Conventional programming languages, whether high level or low
providing integrated environments where a user without
level, tend to be built around a vocabulary where the 'words'
programming expertise could customise his or her applications.
consist of primitives (characters). Visual programming languages
Visual programming in the application design cycle takes the
are at a higher level of abstraction than conventional languages.
form of a dattiflow architecture. In this architecture, users are
Haeberli [I9881 states that a visual programming environment is
presented with a library of modules-application componentsany system that has adopted a graphical 2D notation for the
with specific functions. The users can select which modules will
creation of programs. The visual primitives that make up the
be useful in their application and draw, via direct manipulation of
vocabulary of these programs are essentially representations of
graphical representations, a block diagram and create connections
well-defined aggregates and the (direct) manipulation of these
between modules for the data to flow through. This quick and
aggregates means that complex programs can be produced more
easy process meant that scientists and engineers could concentrate
easily than with conventional languages. This is because the
on the problems being studied instead of dealing with the
abstraction allows a greater degree of code or function reuse and
overhead of re-coding and configuring monolithic applications.
the workings of the programs themselves are more readily
understood and communicated due to their visual and spatial
properties. It can also he argued that if the manipulation of the
2.3 Multiple-view co-ordination
visual constructs is flexible enough-for example, the user may
Multiple view cwordination allows two or more related views of
wish to place them arbitrarily on the display surface-then this
data to run concurrently, with views evolving as data flows into
allows greater freedom for externalising the plans and thoughts of
them from some common ancestor in the data flow graph, or as
the user [Hendry and Harper 19991.
the user interacts with one of them. A well-known example of
this is brushing and linking [Becker and Cleveland 19871. By coUsing visual programming for constructing InfoVis algorithms
ordinating multiple views so that changes made in one view are
reinforces our commitment to and interest in graphical interaction
in computing. With regard to the means-end relationship, the
reflected in other views, interaction can he said to flow between
them. This lets the user focus an specific parts of the data set, and
means are a visual process and the end result i s a tool that
see them within the context of other views.
produces the visual information originally sought aflervisualisations are useful for producing other visualisations.
In evaluating their snap-together visualisation system, North and
Shneiderman have found that this enhances user-performance in
2.2 Data-flow model
data analysis tasks [North and Shneiderman 20001. Co-ordination
of activity across multiple views gives the user greater control
Before visual programming was available in scientific
over the visual representations of the data. This ultimately
visualisation tools, the functional components of the tools were
nurtures discovery. In [Buja and Swayne 19961 it is described as
hidden from the users and they had no control of the flow of data
linking " ...a graphical q u e ~
to a graphical response", and in
between them. The stream of data from input through calculation
functions to mapping, filtering and rendering graphics and their

92

in the available screen space. One solution being considered is to
allow the user to dynamically increase the level of abstraction by
aggregating groups of modules, simplifying the graph of
interconnected modules and the programming task.

[Eick and Wills 19951 it is stated that it gives users the impression
that they are touching the data.
HIVE takes advantage of the data-flow model and visual
programming. To create a hybrid algorithm, a user drags
components from the system's tool bar into the drawing region
(see figure 1) and then interconnects them by dragging links
between ports on the components. Not only is the data-flow set up
in this manner, but the view co-ordination can also be defined this
way. Afier connecting visualisation tools such as scatterplots to
the output ports of algorithmic components, 'Select' ports can he
linked between view components to establish 'brush and link'
functionality.

As well as implementing visual programming to steer data-flow

and co-ordinate multiple views, HIVE has at its core a novel
hybrid algorithmic framework, exploring a general approach to
the composition of efficient and flexible hybrid algorithms. The
choice of each algorithmic component is influenced by many
characteristics including computational cost, the cardinality,
dimensionality and distribution of the data, and the other
interaction components that might he used within a larger
workspace, such as scatterplots and fisheye tables. We suggest
that these choices can be made incrementally, so that the user
employs intermediate representations as they work with and
explore their data. We also suggest that the system can assist the
user by using a pre-authored classification of data-based on,
initially, cardinality and dimensionality of data sets-and
a
corresponding classification of available algorithmic components
based on the classes of data each is suited for. This offers us an
incremental and combinatorial approach to the creation of
efficient and informative hybrid visualisations.

Hybrid algorithms can exhibit a lower run-time than spring
models run upon the whole data set, as discussed in [Morrison et
al. 20031, hut they also lend themselves to the production of
intermediate visualisations. The benefits of this hybrid approach
are two-fold: efficiency is enhanced and intermediate views
provide more insight into the data. For example, the hybrid
algorithm depicted in Figure I (left) uses a spring model of a
sample ofthe full data set, to gain an initial small-scale 2D layout.
In the left frame of Figure 1, the sample and the remainder have
both been fed into spring models, to allow for comparison. The
two layouts have been positioned by the user on the right hand
side of the frame. The sample layout is also fed into another
module, which interpolates the remainder of the set to produce a
third and final scatterplot, shown in the middle of the frame. In
the right hand frame in the figure, the fisheye table shows the
layout points sorted on the y dimension. If we then use brushing
to select a range of rows in the table, we highlight the
corresponding points in the scatterplot and reveal more of the
structure of the data.

Our work has focused on data set cardinality, N, and the
dimensionality or number of variables associated with each
object: D. We roughly categorise D and N using an ordinal range
(high, medium and low), and then we can categorise an
algorithmic component with values of D and N for 'good' inputs
and for the component's outputs, effectively stating our opinion
that the component is hest suited to such combinations of D and
N . For example, we consider that the input to K-means clustering
should he medium to high in D and N, whereas a canonical O(Nz)
spring model algorithm can only handle low N and low to medium

3 Hybrid algorithmic architecture

D

HIVE has been inspired by some of the existing data-flow and
visual programming systems that are prominent in the literature
and common in the marketplace. Upson et al's Application
Visualisation System (AVS) [Upson et al. 19891 and North and
Shneiderman's snap-together system [North and Shneiderman
20001 are two good examples. AVS is predominantly aimed at
scientific visualisation, for modelling or simulating physical
processes such as fluid dynamics, and concentrates on channelling
data through algorithmic processes for transformation and
rendering. The emphasis here is on the data-flow. North and
Shneidennan's snap-together system, on the other hand, is
concemed with information visualisation. In this system there is
less emphasis upon the algorithmic processes for transforming
data and more on the transformation of graphical representations
by way of multiple interconnected views. Here the flow of
interaction takes precedence.

Cardinality (N)

b

.....................................................
~

"

"

"

"

Q,,,,

'

p
g

p
J

v......

p
J

"'''.. ............................................

0%;
D''
....

...,..'..'

Figure 2. Data input to components in a hybrid algorithmic
architecture can he categorised by the ranges of
dimensionality and cardinality they are best suited forhigh, medium or low. Each component transforms the data,
effectively moving across the 3x3 grid. Our hybrid layout
algorithm produces a low-dimensional layout of a large
high-dimensional data set i.e. a move from ( X H ) to (L,H)
that involves several steps shown as dotted lines in the
figure: sampling, which reduces N, then a spring model of
the sample, which reduces D, and then interpolation, which
increases N.

HIVE borrows from the data-flow model of AVS to he flexible in
creating efficient algorithms for the visualisations. However, to be
in line with the goal of information visualisation, it concentrates
on exploration rather than simulation. This is achieved by
supplementing the data-flow with interaction flow across multiple
views, rather like the snap-together system. It must be said,
however, that this approach does not come without drawbacks. It
is important to note that if the level of abstraction used in the
visual programming language is too low then there might he too
many visual modules, in that programming would become
complicated and the flow networks too large and hard to manage

93

The graph manager defines three types of components to support
the construction of hybrid visualisations. These are (1) data source
components to allow the import of disparate data sets and perform
the required variable type transformations; (2) algorithmic
components to transform data into metadata and intermediate
representations; and finally, (3) visualisation components for
rendering. It should be noted that this system is not strictly a dataflow model since it is not the original data that is passed between
components through links and ports, hut references to the data and
any transformations that are applied. The primary benefit of this is
the more efficient support for tightly coupled interaction, e.g.
brushing.

As shown in Figure 2, the choice of components and how they are
connected allows one to solve familiar problems in new ways.
The hybrid algorithm o f [Morrison et al. 20031 transforms a large
set of data of high D to low D. It can be thought of as a move
across the grid of combinations of D and N, stepping from (H, H)
to (L, H)-but
taking an indirect route via (H, L) and (L. L) that
involves sampling, spring model layout of the sample, and
interpolation based on that intermediate representation.
Tentative default values for these ordinal categories of data are as
follows. We derived these values from our own experience of
constructing hybrid algorithms, however, HIVE allows the user to
tailor them:

To facilitate extensibility, the visual modules that represent
algorithmic processes and visualisations are all derived from a
common Java class. This means that to accommodate new
algorithms and visualisations, the programmer need only extend
the base class and implement hisiher own specific methods.

LowD<3
3 <= Moderate D <= 100
High D > 100
L o w N < 1000

Visual components ‘listen’ to each other by way of their ports.
When a programmer writes a component, he or she must declare
the ports that are necessary for the functioning and
communication of the component. Ports operate by extending the
Java ‘Observable’ class and implementing the ‘Observer’
interface [http:lijava.sun.cadapi/l, 50 that when a link is made
between two components, the ports at each end of the link register
with each other. This simple approach means that a component
can send a message to another connected component by sending
data through one of its (observable) output ports to the (observer)
input port of the other component.

1000 <=Moderate N <= 25000
High N > 25000
The HIVE system has been designed and implemented with this
hybrid algorithmic approach in mind, and serves to provide a
workspace for experimental algorithm design and exploratory data
analysis. The visual modules that have been implemented 50 far
include a CSV data-importer, Chalmers’ 1996 spring model
[Chalmers 19961; radial interpolation [Morrison et al. 20021, Kmeans [MacQueen 19671, neural PCA [Oja 19821, stochastic
sampling, scatterplot, histogram and fisheye table. These.
components are the ingredients used in an algorithmic
‘cookbook’, in which components deemed to suit particular data
characteristics can he automatically connected to form hybrid
algorithmic paths that span the grid of Figure 2. Examples are
discussed in Section 5 , following the next section’s discussion of
HIVE’S internal structure and component composition model.

There are five types of port that a visual component can
implement. These consist of the one-way data-in, data-out,
trigger-in and trigger-out ports, as well as the two-way ‘select’
port. When declaring ports, this type must he defined. However,
data-in and data-out ports may also define the structure of the data
that will pass through them as well as the variable types
comprising those data. Two forms of data structure that the ports
cater for are high-dimensional feature vectors that can consist of
real, integer, string and date variables, and 2D real-valued coordinate vectors.

4. Implementation
The software has been implemented in Java SDK 1.4. The system
architecture has been designed to let users compose visualisation
tools using modular components for importing data, algorithmic
processing and graphical rendering. In general terms, the
architecture involves a graph manager that supports the user’s
composition of a flow of data through components such as
scatterplots, K-means clustering, spring model layouts, table
views and so forth. A view manager handles linked user
interaction with these components.

The system’s composition model is responsible for laying down
the rules for which ports can be connected, based upon these port
types. These rules comprise the default composition model,
however visual component implementations can override them to
tighten or loosen connection constraints when required. An
overview of these rules is as follows:
polarity
one-way ports can only he connected to their
complement.
~

The graph manager allows the user to incrementally create
It employs a
executable networks of components.
scriptinglcomposition model [Nierstrasr et al. 19911 to impose
constraints upon which modules can be connected and through
which ‘ports’, depending upon factors such as the categorisation
of data type mentioned in Section 3, as well as graph structure and
port polarity (input only, output only, two-way). A user can
manually connect together components, but be uwned of
potentially unsuitable or inefficient connections. Another mode
offers an automatically generated default path through the grid of
Figure 2, instantiating components based on the system’s
classification of the input data set.

self-connection
connected

~

ports on the same component cannot be

fan-In - one input port can only be linked to one output port
fan-out

~

one output port can be linked to many input ports

data-structure compatibility - data-in and data-out ports
can only be connected when they are declared to handle the
same data structure.
data-variable compatibility - data-in and data-out ports
can only be connected when they are declared to handle the
same variable types.

94

scientists at the University of Nottingham. The aim of this is to
learn about carbon cycling processes. The data set was composed
of 2202 probe measurements, each consisting of 16 variables
measured at five-minute intervals between 171h January 2003 and
3 I “ January 2003. This was converted into CSV format before
importing it into HIVE.
Two algorithms were set up in parallel in HIVE and used to
perform dimensional reduction of the data so that they could be
rendered as a point distribution in scatterplots. One algorithm
consisted of a neural PCA component and the other was generated
automatically after the user specified the data set and visualisation
tool, in this case a scatterplot. This latter algorithm was similar to
the hybrid algorithm illustrated in Figure I with the exception that
it used K-means instead of stochastic sampling in initially
reducing the representative cardinality. Both algorithms took less
than five seconds to mn. By setting up these two algorithmic
paths in parallel, it was possible to directly compare the
visualisations produced (Figure 3).
Figure 3. The leftmost scatterplot shows the output of
neural PCA. The middle scatterplot shows the data after
interpolation around the K-means centroids while the
right scatterplot illustrates the output of the final spring
model component. The highlighted cluster is a small
subset of erroneous PAR measurements. These clusters
are much clearer in the hybrid algorithm’s plots than
with PCA. The histogram shows the PAR distribution at
a depth of 10 metres. The outlying peak (far-lefl) has
been selected and this highlights the clusters in the
scattemlots.

One notable difference between the visualisations was a small
cluster made prominent by the hybrid spring model, especially in
the intermediate view after the interpolation phase, which was not
apparent in the PCA output. By linking a histogram to the
scatterplots it was found that this cluster of points represented
data where the photosynthetically active radiation (PAR)
measurements at a depth of 10 metres were invalid. It tumed out
that these erroneous measurements were caused by the light level
exceeding the sensor’s maximum input threshold.
The two algorithms used here are examples of ‘recipes’ that are in
the algorithmic cookbook mentioned in Section 3. Since the data
set used here is deemed to be of moderate cardinality and
dimensionality, K-means is applicable in reducing the
representative cardinality (centroids) to make it low enough for
Chalmers’ spring model to converge very quickly and reduce the
dimensionality to 2 dimensions. From here, the rest of the data set
is interpolated onto the layout to restore the representative
cardinality. A final spring model step is added to tun for a small
constant number of iterations to refine the final layout. This
algorithm was generated by HIVE to span the grid in Figure 2
from (M, M) to (L,
Ifhowever, the cardinality of the data set
was high, the algorithm would have had to span from (M. Hi to
(L, Hi, in which case HIVE would have generated a variant of the
hybrid algorithm. In this case stochastic sampling would be
employed instead of K-means in the initial phase, to speed things
up. The other algorithm used in the exercise, neural PCA, was
composed manually and can be regarded as a direct jump from
(M. Mi to (L, M) with respect to the algorithmic space in Figure 2.

There is one exception to the data-structure compatibility rule
above. This is to facilitate the semi-automatic generation of
hybrid algorithms and occurs when the user connects a high
dimensional output port such as the output of a data source
component, to a 2D input port such as the input to a scatterplot. In
this case HIVE classifies the data on the output port according to
the ordinal ranges of dimensionality and cardinality as described
in Section 3. Once this is complete, HIVE loads the appropriate
algorithm from a default set of hybrid algorithms
the
algorithmic ‘cookbook’. These algorithms have been preclassified in their applicability in spanning the grid of Figure 2,
and are inserted between the two components that the user had
originally connected, thus restoring adherence to the datastructure compatibility rule described above.

m.

~

When HIVE has finished this process the user can run or modify
the algorithm and visualise hisher data. It is suggested that this
functionality might aid inexperienced users of the system, as well
as encourage experimentation with hybrid algorithmic
conjunctions. If the user modifies the HIVE-generated algorithm,
heishe can save it and specify that this should be used the next
time HIVE is prompted to generate an algorithm under the same
circumstances.

This exercise demonstrated the fact that some algorithms can be
more effective than others when employed in MDS. If PCA had
been used alone, the anomalous data might have been overlooked,
whereas the hybrid spring model made the cluster immediately
apparent. Also, the value of the intermediate view after
interpolation boosted the cluster’s separation and made it more
visible.

5 Preliminary experience using HIVE

6 Ongoing and Future Work

Early experience of the HIVE system was gained when exploring
a data set gathered from an eScience project within the Equator
Interdisciplinary Research Collaboration (www.equator.ac.uk).
The eScience team has set up a remote sensing probe at a frozen
lake in the Antarctic, which transmits data including ice thickness,
water temperature, UV radiation levels etc. to environmental

Our ongoing work is focused on implementing further visual
modules to be included in the cookbook of hybrid algorithms that
will span the simple 3x3 space represented in Figure 2.
Algorithms to be considered include SOMs [Kohonen et al. 20001

95

and Random Mapping [Kaski 19981. We are also experimenting
with new algorithmic components such as Morrison and
Chalmers' O(h3'4) hybrid algorithm [Morrison .and Chalmers
20031. These algorithms are being analysed with respect to the
data types they can handle, their complexity in time and space,
whether or not they produce visualisations as useful intermediate
representations, and the order in which they should be applied in a
hybrid conjunction. We will also investigate aggregation of visual
modules, as described in Section 3 , as a means of increasing
abstraction and therefore simplifying visual programming. Given
a larger 'palette' of components, we will then carry out user trials
of the workspace and the framework.

Buja, A., Cook, D., Swayne, U. F. 1996. Interactive high-dimensional data
visualization. hirrnal of Compatatiunal and Graphicill Slotistics 1996,
78-99.

One boundary issue that could impact on the implementation and
usage of the proposed HIVE framework relates to applicable data
formats. There are several well-established standards for encoding
and handling data including the hierarchical data format (HDF)
and others such as the common data format (CDF). For the HIVE
fiamework to be adopted as a feasible information visualisation
workspace in a non-experimental setting, the formats of data that
it should he capable of importing, modifying, and possibly
exporting, should employ these standards.

Eick, S. G., Wills G. J. 1995. High Interaction Graphics. European
Journal ofOp~.rarionalResearch 84,445-459.

Becker, R., Cleveland, W. 1987. Brushing scatterplots. Technomelrics 29,
2 , 127-142.
Chalmers, M. 1996. A Linear Iteration Time Layout Algorithm for
Visualising High-Dimensional Data. in Proceeding.? of IEEE
Visualization 1996, San Francisco, 127-132.
Eades, P. A. 1984. A heuristic for graph drawing. Congressus
Numerantium 42.

Haeberli, P. E., 1988. ConMan: a visual programming language or
interactive graphics. Computer Graphics 22,4, 103-111
Hendry, D.G., Harper, D. J., 1999. An informal information--seeking
environment.Journol of the American Societyfor Iq?ormormalion Science,

48, 11,1036-1048.
Kaski, S. 1998. Dimensionality reduction by random mapping: Fast
similarity computation for clustering. In Proceedings Internutionrrl
Jurnl Conference onNeuralNetx.orks I , 413418.

7 Conclusion
A framework for hybrid algorithmic development has been
described and a system, HIVE, embodying the framework has
been implemented. From our early experience with our prototype,
w e suggest that the hybrid approach has two-fold benefits:
significant improvements in run times of MDS algorithms can be
achieved, and intermediate views of the data and the visualisation
program structure can provide greater insight and control over the
visualisation process. In the near future, we intend to carry out
user trials to test this opinion, and to derive system improvements
and new design ideas

Kohonen, T., Kaski, S., Lagus, K., Salojrvi, J., Paatero, V., Saarela, A.
2000. Self Organization of a massive document collection. IEEE
Transaction NeuralNetworks, 1 I , 3,574-585.
MacQueen, J., 1967. Some methods for classification and analysis of
multivariate observations. in Proceedings of 5th Berkeley Svmpoxium,

281-297.
Morrison, A., Chalmers, M. 2003. improving Hybrid MDS with PivotBared Searching. To appeor in Proceedings ofthe IEEE Symposium on
Information Visualisation.

Overall, we suggest that the growth in the number, variety and
internal complexity of visualisation algorithms is a similar
situation to the growth in the size and complexity of the data we
visualise. While we are not yet at the stage of having hundreds or
thousands of components to visualise, we feel that the task of
constructing, adapting and using information visualisation tools is
becoming a user activity that may benefit from system assistance.

Morrison, A., Ross, G., Chalmers, M. 2002. Achieving Sub-quadratic
Multidimensional Scaling through the Combination of Sampling,
Clustering and Layout Algorithms. in Proceedings of the IEEE
Symposium on Information Visualisation. 152.158.

Morrison, A., Ross, G., Chalmers, M. 2003. Fast Multidimensional
Scaling through Sampling, Springs and Interpolation. Informotion
Visualisalion 2, 1. 68-77.

Visual programming is a promising first step in this direction, as
we hoped to demonstrate in this paper, but there may be rich and
useful work to do in using InfoVis techniques to support the
understanding and use of InfoVis systems.

Nientrasa, O., Tsichritzis D., Vicki de Mey, Stadelmann, M. 1991.
Objects + Scripts = Applications. in Proceedings of ESPRIT
Conference. Kluwer Academic Publishers, 534-552

Acknowledgements

North, C., Shneiderman, B. 2000. Snap-together visualization: can users
~onstmctand operate coordinated visualizations? lnternrrtional Journal
ofHuman-Compuler S t u d i ~ sj3, 715-739.

W e thank Luc Girardin and Dominique Brodbeck (Macrofocus)

for the 'S' data set used in Figure I, Alistair Morrison (U.
Glasgow) for earlier work on the hybrid algorithms in Figures 1
and 3, and Stefan Rennick Egglestone and Chris Greenhalgh (U.
Nottingham) for the Antarctic data.

Oja, E., 1982. A Simplified Neuron Model as a Principal Component
Analyzer,Journal ofMathematico1 Biologv 15,267--273.

References

Upson, C., Faulhaber Jr, T., Kamens, D., Laidlaw, D., Schlegel, D.,
Vroom, J., Gunuitz, R., Van Dam, A., 1989. The application
visualization system: a computational environment for scientific
visualization. IEEE Computer Graphics and Applicolions. 30-42

Bradley, P. S., Famad, U. M. 1998. Refining Initial Points for K-Means
Clustering. in Proceedingr ofthe Ffieenlh International Conference on
Machine Learning 1998. 91-99.

96

