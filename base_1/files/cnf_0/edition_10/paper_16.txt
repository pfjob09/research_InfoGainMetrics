Case Study
An Empirical
Investigation
of
Thumbnail
Image Recognition
C. A. Burton, L. J. Johnston and E. A. Sonenberg
The University of Melbourne, Parkville, Victoria, Australia, 3052.

Abstract

and noise and background texture smoothed [14].
Another approach is to use lossy image compression such as the Joint Photographic Experts Group
(JPEG) method. The objective of modern lossy image compression techniques is to reduce the file size
while maintaining important perceptual information.
Extreme compression of an image will in effect simplify an image by removing many of the colours and
textures. By taking the original image closer to an
abstract iconographic form, it may be expected that
such compression would suit thumbnail creation. In a
perceptual study of icon designs for user interfaces,
icons that were more abstract and visually simpler
were identified as being easier to understand than
complex images or complicated symbols [3]. It is postulated that “simplified” thumbnail images will enhance a user’s ability to browse, as perception has been
shown to play an important role in browsing [S].
In the work described below, EDS, JPEG and a selforganising feature map (SOFM) were used to treat
full-size images as an enhancement step before the
usual subsampling that reduces their physical size.
Forty-eight computer users viewed one hundred and
sixty of these thumbnails in an experiment designed
to explore the value of such thumbnail creation techniques in a context akin to browsing, where the viewer
has some idea of what they are seeking. Section 2
presents the issues central to the selection of pretreatment processes. The design of the experiment
and some key results are provided in Section 3. Section 4 includes our discussion and conclusions.

The use of thumbnails (i.e. miniatures)
in the
user-interface of image databases allows searching and
selection of images without the need for naming policies. Treating parent images prior to reduction with
edge-detecting smoothing, lossy image compression,
or static codebook compression resulted in thumbnails
where the distortion caused by reduction was lessened.
An experiment assessing these techniques found resulting thumbnails could be recognised more quickly and
accurately than thumbnails of the same parent images
This prethat had been reduced without treatment.
treatment in thumbnail creation is offered as an improvement for browsing of image databases.

1

Introduction

The increased popularity of image databases has
encouraged user interface designers to use highly reduced images or thumbnails to facilitate the handling
of images. Thumbnails are displayed in place of fullsized images to allow quick, preliminary visual identification.
Such reduced images require much less
bandwidth for transmission; they are very common
in World Wide Web pages. In databases, thumbnail
use overcomes the difficult process of naming images,
frees image identification from problems of language
and literacy, and encourages browsing and manual organisation by related visual content.
Because thumbnail images are such dramatic reductions of their parents (for example a 64x64 pixel
thumbnail from a 1024x1024 parent) distortions, such
as exaggerated textures and stepped edges, occur during the subsampling used to achieve reduction. This
article investigates whether thumbnail creation could
benefit from image-enhancement technology being applied before subsampling.
One approach is to apply edge-detecting smoothing
(EDS) which enhances images by smoothing around,
but not over, large changes in image intensity, resulting in visually simpler images with edges preserved

O-8186-7201-3/95$04.00 0 1995 IEEE

Proceedings of the Proceedings on Information Visualization (INFOVIS '95)
0-8186-7201-3/95 $10.00 © 1995 IEEE

2

Image Simplification

In evaluating methods for removal of information
from an image while retaining fast accurate recognition, one must consider: factors influencing object
recognition; how to effect improvements on thumbnail
images which might lead to better recognition performance; and the choice of measures to determine
thumbnail quality.

115

See Color Plates, page 150.

2.1

Human

object recognition

study, JPEG, an edge-detecting smoother [12], and a
self-organising feature map compressor [15] were chosen.
Both subsampling and compression techniques
treat an image as a series of blocks. This has its
largest impact on continuous features of the image,
such as the edges of objects. Of the distortion types
identified in [7] that significantly affect edges, blocking was the most easily detected, and is also the main
type of distortion caused by subsampling. It is expected that blocking can be reduced by exploiting the
block-ordering of JPEG to minimise the differences
between image blocks and hence preserve the consistency of image features such as edges. Compressed
images are also visually simpler than their source images. In figure 1, the image butterfly
has more
than 23000 colours, its subsampled thumbnail (figure
2(i)) has 1002 colours, but the same image compressed
with 3% quality in JPEG, then reduced has only 272
(figure 2(k)).

Object recognition relies on the eye successfully detecting certain parts of the object before it can be
identified. Boundaries, the most important perceptual feature of an object, define for the eye where the
object starts and ends and its position relative to other
objects and the background [2]. In images, boundaries
are made up of edges, i.e. changes in local image intensity, which need careful attention [7].
A thumbnail image representing an object need
only contain enough visual information to display the
general concept of that object. It is argned that reducing the number of colours may clarify, rather than
distort, representative images such as thumbnails. A
biid miniature might contain the shape of a bird and a
few colours, but not all the details of any specific bird.
To effect a simplification that takes an object closer to
its exemplifier, one might, for example, remove most
of the texture of the bark from an image of a tree, but
leave a few of the green colours of the leaves and the
brown of the trunk. This abstracted image could be
thought of as a more general exemplifier of the concept
“tree”.
The next section describes existing methods that
not only deal with boundaries and reduce the number
of image colours, but simplify texture as well.

2.2

Implementing

“simpiification”

Digital image reduction is achieved via subsampZ&rg
the image data - where a certain subset of pixels is
chosen from the array of the source image. For example, selecting one pixel in every square block of four
produces a reduced image that is one quarter the area
of the original. There are many ways to subsample
images [9], however, all such methods are p&e1 neighborhood operations. That is, one pixel in the reduced
image represents a block of pixels in the source image. Because of this, reduced images can be rather
distorted, the most visible effect being blocking, where
a straight line appears as a staircase [7]. The effect
may be compounded with further subsampling 191.It
should be noted that such distortion is visible because
the process of reduction removes information irrespective of perceptual features. In the sheep image in
figure 2(e), blocking distortion results in the exaggeration of texture of the sheep’s wool and the soil of the
background, creating (relatively) large bright spots.
This paper proposes the use of compression or
smoothing as a treatment for the parent image before the subsampling takes place. Standard smoothing
alone will not overcome the limitations of neighborhood subsampling, but operations that re-order image
blocks or smooth around image features can. For this

Figure 1: Sourceimage butterfly,
256x256 pixels.

The edge-detecting smoother was written by David
Mumford et al. [la]. In this approach, the image is
smoothed with a kernel that changes shape according
to edges in the image. The algorithm smoothes up to,
but not across, edges. Its design is based on the idea
that noise is less obvious to the eye in regions of the
image with high detail. Strong textures such as wood
or grass constitute high detail, and these are removed
by this algorithm.
The self-organising feature map used for image simplification is a neural network program written by
Lachlan Andrews at The University of Melbourne af-

116

Proceedings of the Proceedings on Information Visualization (INFOVIS '95)
0-8186-7201-3/95 $10.00 © 1995 IEEE

22586 (scaled) colours,

ter the work of Zurada [15]. SOFM’s undergo a type
of “training” which involves a source image being divided into blocks, which are encoded by the nodes
of the neural net. The training process parses image
blocks into clusters of like blocks based on the meansquared error between each block and the exemplar
(centre block) of the cluster. Compression is achieved
by discarding most of the blocks in each cluster, and
replacing them with pointers to the exemplar. The
Andrews method was used as a static codebook compressor (as opposed to the dynamic JPEG). That is,
the neural network was trained on an initial training
image, then all subsequent images were compressed
with the codebook derived from the initial training.
This approach explores another possible simplification
effect-replacing specific blocks from the source image
with “simple” blocks from the training image.
The training image chosen for this method agglomerated some common features of general images. Typically, with decreasing image block size, most image
features such as edges are only local gradations in
colour intensity [13]. Mohan et al. [lo] have defined
human visual invariants of perceived objects to consist only of curves. That is, the unique features of real
world objects are encoded by humans as collections
of curves, with right angles and flat surfaces being
special cases. These assertions together suggest that
a general training image should supply the Andrews
neural net with curved graded edges. As Andrews algorithm operates in grey-scale on colour separations,
a grey-scale image was chosen for training the neural net. A 256x256 image was created by applying
a large Gaussian convolution (blurring) to an image
of a Smith Calibration Chart, a calibration scale for
television transmission.

2.3

Measuring

image perceptual

@I

(4

(f)

k)

(h)

(9

(3

(4

(1)

(4

Figure 2: 32x32 pixel thumbnails (enlarged), comparing
control imageswith those produced as a result of the compressionmethods. (a),(e),(i) controls, (b),(f),(j) Andrews,
(c),(g),(k) JPEG and (d),(h),(l) Mumford methods.
to the source scale then applying MSE tests, but only
for moderate reduction rates of 50% [9]. Thumbnails
assessedin the present study were as small as 2% of
the original image area. In real applications, this reduction could be even more dramatic, as many source
images are larger than 256x256 pixels.
The second reason for using non-numeric methods,
was due to the large amount of image degradation.
Source images are simplified then reduced producing
image degradation at both steps. This high distortion,
however favourable to the effect of simplification, rules
out the use of qualitative distortion measures, such
as Just Noticeable Distortion Threshold (JNDT) [5],
and ratio and ordinal scales of quality [S]. Both approaches involve users ranking several versions of the
same image in order of clarity. Such ordering is not
necessarily an indicator of the perceptual content of
the image, but rather the aesthetic content.
We chose to assess the perceptual quality of the
thumbnails by recording accuracy and time to recognition taken by subjects viewing thumbnails for very
short durations. Detailed in the next section are the
design and results of the experiment undertaken.

quaIity

Because the intended image treatment is effectively
a distortion of the parent image, classical quantitative means of determining transmission fidelity, such
as mean-squared error (MSE) and signal to noise ratio
(SNR), are not appropriate. As our study was particularly concerned with ways to improve the perceptual
quality of reduced images, it was apparent that ways
to assessthe benefit of simplification could not rely on
other numeric methods - for two reasons.
The first is due to the enormous physical scale
difference between a source image and its thumbnail. Even perceptual-numeric methods of comparison (such as perceptual MSE or log MSE) require a
1:l pixel correlation between source and target images,
and hence are not appropriate. Li and Hu found a successful compromise by enlarging reduced images back
117

Proceedings of the Proceedings on Information Visualization (INFOVIS '95)
0-8186-7201-3/95 $10.00 © 1995 IEEE

(4

3 Experiment
3.1 Design

times were used in displaying the images. The thumbnail sizes chosen were 24x24,32x32,48x48 and 64x64
pixels. Display times were 125, 250, 375 and 500ms.
Subject recognition times and accuracy were expected
to be better for thumbnails displayed for greater durations and at larger display sizes.

The thumbnail assessment was designed as a
threshold experiment, where subjects attempted to
identify an image displayed for only a few hundred
milliseconds.
Response times and accuracy are an indicator of
how quickly an image is recognised, not how pleasing
it is to the eye. Bhatia et al. [l] observed this to be
a sound technique for assessing ability to identify images of human faces. Their experimental paradigm focussed on how dramatic image degradation affects human perception of faces. Subjects were presented with
two images at the same time, and indicated which was
a human face by pressing either a left or right button.
Because our study was concerned with general images
rather than faces, it was decided that subjects would
view word-image pairs instead. We believe that this
approach best simulated the browsing environment,
as browsers have at least a vague idea of what they
are searching for [S]. The actual expectation of seeing
objects has been shown to change the perceptual requirements for identification of an image. It is argued
that pictorial perception is a state in which the viewer
has some expectation of how an anticipated object will
appear as a picture. In this way, one might expect a
circular coin to appear as an ellipse. This is contrasted
to the default state of viewing where the viewer just
processes visual information without any expectation.
Words were displayed as black letters on a 128x128
pixel white background, to remove any residual image on the retina between pairs. The word stimulus
was displayed for the same duration as the following
image, between which there was a standard pause of
500 ms. For example, the text “light globe” was displayed in one trial for 125 ms, replaced by a blank
screen for 500 ms, then one of the images from figure
2(a)-(d) was displayed for 125 ms. This constituted a
“‘correct” sequence. An “incorrect” sequence would be
the same image preceded by the word “monkey”. All
word-image pairs were examined for semantic clarity
in the “correct” pairings and semantic non-relatedness
in the “incorrect” pairings.
Word-image pairs were displayed in random order
to subjects. Subjects ranged in age from 16 to 45
years, were literate and spoke English. All stated that
they had either sufficient or corrected vision. The subjects’ exposure to computers ranged from data entry
and word processing to postgraduate degrees in computing. Each subject saw 160 word-image pairs, 80 incorrect and 80 correct pairings, leading to over seven
thousand data points. A variety of both sizes and

3.2

parameters

The control images were produced by straightforward subsampling of the parent image to the desired
size.
JPEG was implemented as a pipe through cjpeg
and djpeg with the “quality” parameter set to 2%
for 24x24 thumbnails, 3% for the 32x32s, 4% for the
48x48s and 5% for the 64x64s.
The Mumford method has a number of parameters,
such as the rate of blurring, the size of the gradient
window (the area searched for features), and the standard deviation of image intensity. For this study, the
same settings were used for all final thumbnail sizes
as recommended in [12], except that a larger standard
deviation was used (s=20, instead of s=lO) to force the
algorithm to ignore very small points and single-pixel
features.
The Andrews method was implemented with four
weight sets generated for the four image sizes to exploit the varying code word sizes that were possible.
The code word sizes were set to 4x4 pixels for the
64x64 pixel thumbnails, 5x5 for the 48x48s, 8x8 for
the 32x32s and 10x10 for the 24x24s. All training was
limited to the construction of a 256 codeword codebook from the image of the Smith Chart.

3.3

Apparatus

An SVGA monitor with a viewing hood and chinrest was used to display images to the subject. The
subject-screen distance was set at 500mm. To record
subjects’ choices, a button box with buttons for “yes”
and “no” answers was placed within reach of the subject. Below the computer screen, in line with each of
the buttons were labels “YES” (left) and “NO” (right)
as reminders. A PC compatible 80486-33 computer
with 24bit graphic card was used to display wordimage pairs and record reaction time in milliseconds
when either button was pressed.
In total, 160 images were obtained from the Visual
Symbols CD-ROM [ll], scanned from National Geographic Magazine or photographed with a CCD camera. All images were cropped to 256x256 pixels and
some noise removal was performed where necessary
with Adobe Photoshop.
Of these images, 88 were “iconographic” images typically one object set on a white background, 72
were “natural” images of scenery and multiple objects.

118

Proceedings of the Proceedings on Information Visualization (INFOVIS '95)
0-8186-7201-3/95 $10.00 © 1995 IEEE

Algorithm

3.4

Results

[ Methods

] Time
I Significance 1
I Vs. Controla I Mumford
Lower
0.000
JPEG
Lower
0.000
Andrews
Higher
0.013
] JPEG vs Mumford 1 No difference I 0.074

Forty-eight subjects (22 female, 26 male) were each
allocated to one of four groups designated by the control and three methods. Seven thousand six hundred
and eighty reaction times and responses were recorded.
The average score over all trials was 82% correct, with
the lowest at 79% and highest at 90% correct.
A 3-level analysis of variance was performed for significance tests on display size, time and compression
method. There was found to be a significant presentation order effect, with response times dropping slightly
over the course of the trial. Response times are skewed
slightly away from the mean toward lower response
times.
Mean reaction times for correct responses were significantly lower for the Mumford and JPEG methods
than the control. Mean response times for the Andrews method, on the other hand, was found to be
significantly higher than for the control. There was
no significant difference between response times for the
JPEG and Mumford methods (figures 3 and 4). All
three methods had a higher proportion of correct responses than the control, with no difference between
JPEG and Mumford (figures 5 and 6). Thumbnail
display size was shown to significantly affect response
times in all methods, with larger display sizes generally resulting in faster and more accurate responses
(figures 7 and 8). Thumbnail display time had no significant effect on response time, but had a significant
effect on total percentage of correct responses.

to mean response time for control subjects.

Methods
Mumford
JPEG
Andrews
JPEG vs Mumford

4

000

-Mean
Medtan

-Mode
JPEG

Significance
0.013
0.012
0.012
0.251

Discussion

An image-treatment scheme was used together with
subsampling to improve the perceptual quality of
thumbnail images. A perceptual threshold experiment showed the benefit of using an edge-detecting
smoother or the lossy compression method, JPEG, to
“simplify” images prior to spatial reduction to thumbnails.
As expected, the performance in terms of the subjects’ response times and accuracy increased with increasing thumbnail size. JPEG and Mumford’s EDS
produced thumbnails that elicited significantly better
perceptual performances than the control thumbnails.
These results support the general claim in icon construction that simpler, more abstract icons are better
understood [3]. Both icons and thumbnails are rep
resentative images and have their content abstracted
from the instance of the objects they depict to the
general classifier of that object. This simplification
was achieved in the present work by removing many
of the textures and colours in the parent image with
a smoother, a lossy image encoder such as JPEG, or
a neural network employed as a static codebook com-

ma

contml

Accuracy
vs. Controlfl
Higher
Higher
Higher
No difference

Figure 5: Relative findings for accuracy method vs control; difference between JPEG and Mumford methods.
Key; /3: Proportion of correct responses with respect to
proportion of correct responses for control subjects.

1100

Andrewa

I

Figure 4: Relative findings for mean response times,
method vs control; difference between JPEG and Mumford methods. Key; (Y: Mean response time with respect

lzoa

-

I

I

Mumford

Figure 3: Mean, median and mode response times for
correct responsesper method.

pressor.

An unusual result is the poor performance of the
Andrews method subjects, whose reaction times were
significantly worse than for the control data. This can

Further, no significant difference was found between
overall performance on iconographic and naturalistic
images.

.19

Proceedings of the Proceedings on Information Visualization (INFOVIS '95)
0-8186-7201-3/95 $10.00 © 1995 IEEE

28

26.0

25.5

25.0

24.5
16
24.0

23.5

23.0

22.5

JPEG

Andrews

MUmford

Figure 8: Proportion of correct responsesper display size
in percentages

Figure 6: Proportion of correct responsesper method in
percentages
1100

accuracy or reaction time suggests that the display
times may have been too long, or alternately masked
by second-order effects. Further work should employ
shorter exposure periods, closer to the actual perceptual threshold of recognition.
The optimum thumbnail size in terms of performance was the 64x64 pixel thumbnail, for which the
fastest reaction times and best accuracy were observed. Where thumbnails are to be directly extracted
from compressed parent images, e.g. with JPEG, decode time could be interleaved with transmission time.
As images are decoded as a byte stream, the receiving
end could commence display of the pixel information
as it arrives. A production system based on the other
methods would presumably require thumbnails to be
stored along with parent images. Otherwise there is
the processing overhead at run time when images are
treated and then sub-sampled.
Another advantage of this scheme is that JPEG is
currently public domain. Only minor adjustments are
need to be made to its decoder to obtain a 118th sized
image instead of the normal full sized image.
It is interesting to note that for thumbnails, at
least, very coarse JPEG compression resulted in better recognition than for the control images. Although
this result seems contrary to the basic intention of the
JPEG “quality” setting, it highlights the relatively
lower distortion caused by JPEG over subsampling
alone. Because the process of reduction so distorts
an image, the usual degradation caused by JPEG is
relatively better for the image when both JPEG and
subsampling are used together.
As an additional study in an effort to under-

700

600

500
24

32

48

64

Figure 7: Mean response times for correct responsesin
milliseconds for the four display sizes in square pixels.
possibly be attributed to the training image and/or
the colour separation used. It was noticed by some
subjects that these thumbnails often had unusual or
unexpected colour noise. We believe that this was
caused by the use of RGB separation. Subsequently,
the effect has not been found with images separated
to YUV.
Contrary to our expectations, there was no significant difference in subject performance between images
that were iconographic or naturalistic. This suggests
that the simplification does not favour either of these
image types.
Overall, this provides strong support for our assertions on image simplification for thumbnail creation.
The absence of an effect of display duration on either

120

Proceedings of the Proceedings on Information Visualization (INFOVIS '95)
0-8186-7201-3/95 $10.00 © 1995 IEEE

stand the relatively poor performance of the Andrews
method, a set of training images for the SOFM was
created that explored various types of circular gradations. It was found that supplying concentric circle
images with differing radii gave results that appeared
to be more perceptually appropriate. Further work
should include an assessmentnot only of these training
images, but also the use of other colour separationsthe more common YUV separation-as employed in
JPEG. Preliminary work on the colour separations
indicated that this was a contributing factor in the
disappointing results from the self-organising feature
map method.
Lately, other lossy image compression schemeshave
attracted attention because of their speed and/or very
high compression ratios attainable with low distortion.
One of these is fractal compression. In this approach
the compressed image is described not only in terms
of copies of codewords, but transformations of these
codewords that best match the original image features
[4]. Because the transforms used to describe the image are iteratively contractive, the fractal compressed
image can be decoded to any spatial scale. Thus it
should be simple to construct a thumbnail from an
image compressed in this way, and the quality of the
images should be similarly investigated.

[51 N. Jayant, J. Johnson, and R. Safranek. Perceptual
coding of images. volume 1913, pages 168-178, 1993.
161 B. L. Jones and L. E. Marks. Picture quality assessment: A comparison of ratio and ordinal scales.
SMPTE Journal, December:1244-506, 1985.
[71 S. A. Karunasekera and N. G. Kingsbury. A distortion
measure for image artifacts based on human visual
sensitivity. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing,
pages 117-121, 1994.

P31B.

PI
m

P21 D. Mumford, M. Nitzberg, and O.Shiota. Filtering,

segmentation and depth, volume 662. Springer Verlag,
1992.

1131 A. P. Pentland. Local shading analysis, chapter 3,
pages 40-77. Albex Pub. Corp Norwood, NJ, 1986.

Thanks to: Dr. David Mumford (Harvard); LachIan Andrews(Unimelb); Prof Micheal Barnsley (Oxford) for direction to David Mumford’s non-linear filter; Sanjiv Bhatia (Seattle Uni.) for the main supporting work for this study; Dr. Tony Hayes, Terry
Robinson and Mike Johnston (Psychology, Unimelb)
for references, advice and software; and Mike Landy
(NYU) for sending a stack of otherwise unavailable
references from the other side of the planet.

[I41 G. Reese. Enhancing images with intensity-dependent
spread functions. In J. P. Allebach and B. E. Rogowita, editors, Proceedings of the International Society for Optical Engineering-Human
vision, visual
processing, and digital display III, volume 1666, pages
253-264, 1993.
[I51 J.M. Zurada. Introduction to artijIcia1 neural systems.
St.Paul, Westminster Inc, 1992.

References
[l] S. K. Bhatia, V. Lakshminarayanau, A. Samal, and
G. V. Welland. Parameters for human face recognition. PhD thesis, University of Missouri - St. Louis,
1994.
[2] M. L. Braunstein, D. D. Hoffman, and A. Saidpour.
Parts of visual objects: an experimental test of the
minima rule. Perception, 18:817-826, 1989.
[3] M.D. Byrne. Using icons to find documents: simplicity is critical. In Proceedings of the International
Interaction,

pages

[4] A. E. Jacquin. Image coding based on a fractal theory
of iterated contractive image transformations. IEEE
fiansactiona On Image Processing, 1(1):18-30, 1992.

121

Proceedings of the Proceedings on Information Visualization (INFOVIS '95)
0-8186-7201-3/95 $10.00 © 1995 IEEE

R. Mohan and R. Nevatia. Segmentation and description based on perceptual organisation. In Proceedings
of the International
Conference on Acoustics, Speech
and Signal Processing, pages 333-341, 1989.

WI Clement Mok. Visual Symbols Sampler CD. Published
by CMCD Inc CA USA, 1993.

Acknowledgments

Conference on Computer-Human
446-453, 1993.

H. Kwasnik. A descriptive study of the functional
components of browsing. In Proceedings of Engineering for HCI-IFIP
WG 2.7 Wo&ing Conference August 10-14, pages 191-202, 1992.
Z. Li and G. Hu. On edge preservation in multiresolution images. CVGIP: Graphical models and image processing, 54(6):461-472, 1992.

