Visual Analytics on Mobile Devices for Emergency Response
SungYe Kim∗

Yun Jang∗

∗ Purdue

Angela Mellema∗

Using mobile devices for visualization provides a ubiquitous environment for accessing information and effective decision making. These visualizations are critical in satisfying the knowledge
needs of operators in areas as diverse as education, business, law
enforcement, protective services, medical services, scientific discovery, and homeland security.
In this paper, we present an efficient and interactive mobile visual analytic system for increased situational awareness and decision making in emergency response and training situations. Our
system provides visual analytics with locational scene data within a
simple interface tailored to mobile device capabilities. In particular,
we focus on processing and displaying sensor network data for first
responders. To verify our system, we have used simulated data of
The Station nightclub fire evacuation.
Keywords: mobile visualization, visual analytics, emergency response
Index Terms:
I.3.6 [Computer Graphics]: Methodology and
Techniques—Interaction techniques; I.3.8 [Computer Graphics]:
Applications—Visual Analytics
I NTRODUCTION

Our goal is to make mobile devices valuable tools for emergency
response by effectively visualizing relevant, selected information
(e.g., images, 3D models, and sensor data streams) on devices with
varying capabilities and resolutions. We are developing a mobile
visual analytic system that processes and displays sensor, location,
and video data for first responders to increase situational awareness
and enable more effective response.
Visual analytics is defined as the science of analytical reasoning facilitated by interactive visual interfaces [20]. Mobile visual
analytics extends the visual analytics process using state-of-theart mobile devices to increase the effectiveness and interactivity of
analysis on-site. Visual analytics provides a solution for first responders and public safety command personnel requiring advanced
analytical insight by allowing them to analyze and understand onscene, active emergency situations through interactive, integrated
data analysis and visualization.
Several factors enable mobile visual analytics to be a valuable
solution for first responders. First, the mobility of the handheld
devices using wireless connectivity can minimize the “fog of war”
environment allowing first responders to attain rapid and actionable
∗ e-mail:
† e-mail:

Timothy Collins†

University Regional Visualization and Analytics Center (PURVAC)
† Purdue Homeland Security Institute (PHSI)
Purdue University, West Lafayette, IN

A BSTRACT

1

David S. Ebert∗

{inside|jangy|amellema|ebertd}@purdue.edu
{tfcollins}@purdue.edu

IEEE Symposium on Visual Analytics Science and Technology 2007
October 30 - November 1, Sacramento, CA, USA
978-1-4244-1659-2/07/$25.00 ©2007 IEEE

on-site decision making. Hence, mobile visual analytic tools can
provide improved situational awareness and support first responders in planning immediate life-saving responses and for prioritizing actions in emergency situations. Second, the rapidly growing
capabilities of these mobile devices, such as PDAs and cell phones,
make it possible for these devices to gain acceptance as useful tools
in a variety of fields. In particular, 2D and 3D graphics-accelerated
mobile devices have become important in the game market, often
delivering PC quality rendered images. However, most mobile devices still have many limitations including small screens, limited
user interfaces, a short battery life, low bandwidth of the system
bus, slow CPU clock speed, limited storage capacity, and a lack of
advanced graphics hardware.
Despite these problems, many researchers and developers have
been exploring the use of mobile devices in various applications.
In particular, visualization on handheld devices has gained increasingly popularity due to their mobility and various functionalities.
Previously, visual analytics of sensor data on mobile devices was
presented by Pattath et al. [16]. They showed the analysis and
visualization of network and sensor data on mobile devices utilizing football games as a testbed. In our work, we extend this mobile analysis and visualization approach to emergency training and
planning, and present a scalable interactive visual analysis system
for emergency response.
This paper is organized as follows: Section 2 discusses the background and Section 3 summarizes visual analytics for emergency
response. Section 4 describes the design of our system and Section 5 presents visualization and analytics on client mobile devices.
Section 6 gives a brief summary of implementation and results of
our system. Section 7 discusses the capabilities and potential of
our system as a visual analytic tool for emergency response. Finally, Section 8 presents conclusions and discusses some possible
extensions for visual analytics.
2

BACKGROUND

In relation to emergency training and planning, we can classify related work into three categories: visualization of sensor data, 2D
and 3D visualization on mobile devices, and visual analytics on
mobile devices.
2.1 Visualization of sensor data
With the increase in applications for sensor networks, manipulation
and visualization of sensor data streams have become a crucial component. Fan and Biagioni [11] described approaches to process and
interpret data gathered by sensor networks for geographic information systems. The approaches combine database management technology, geographic information systems, web development technology, and human computer interactive design to visualize the data
gathered by wireless sensor networks. Their work differs from ours
in that our system is based on a mobile environment, whereas, their
work is web-based. Koo et al. [14] implemented software to analyze multi-sensor data for pipeline inspection of gas transmission.
The information gathered by sensors is parsed and converted before it is saved in a database. They intended to manage sensor data

35

effectively using a database. However, the system was also based
on a desktop PC environment. Pattath et al. [16] implemented an
interactive visual analytic system to visualize sensor network data
during football games on PDAs. However, they did not provide
enough processes and structures for real-time streaming of sensor
data from a server.
2.2 2D and 3D visualization on mobile devices
We often need to visualize complex 3D models for displaying an urban environment. Much research has been conducted in displaying
urban environments effectively through the use of mobile devices.
City models are important factors for visually communicating spatial information related to an urban area. Particularly, we can divide
this issue into two components. First, simplification of a representation [9, 10, 13, 18] makes it possible to visualize complex 3D
models of the environment with a limited graphical capability. For
instance, the simplest way is to extract and draw that feature lines
of 3D models. Second, effective transmission [17, 18] between
a server and a client enables mobile devices to visualize complex
models.
Dollner et al. [10] described visualizations to represent abstract
and comprehensible drawings of 3D models providing line drawing
to enhance edges, tone shading, and simulated shadows. The purpose of their work is to render a large number of models (e.g., a city
scene) in real-time; however, it is not designed for mobile devices.
Diepstraten et al. [9] proposed a remote line rendering technique
between the server and client. The server extracts feature lines of
3D models and transmits them. The clients just draw the results
that were transmitted from the server. The clients do not need to
have high computational capabilities since they only draw 2D lines.
Hekmatzada et al. [13] also described non-photorealistic rendering
of 3D models based on a server and client environment. Their work
provided transmission of meshes from a server progressively and
level of detail (LOD) as well; therefore, it is possible to navigate
in nearly real-time. Quillet et al. [18] presented two optimization
methods to visualize an urban environment on mobile devices interactively. One optimization method extracts feature lines and then
changes the lines into vector lines. The other splits the urban environment into cells in order to transmit them as a stream. Their
work also provides an efficient LOD solution. Pouderoux and Marvie [17] proposed two levels of adaptivity to display a large amount
of terrain data regardless of the devices. The terrain data is partitioned into regular tiles and the tiles around a viewer are transmitted
as a stream. The tiles are rendered using a pre-computed triangle
strip path.
2D visualization can be an alternative to the above two approaches if we compromise and take advantage of 2D visualization
instead of 3D visualization. 2D graphics and visualization is just as
efficient in the case of information visualization. Hence, there are
many applications that utilize 2D capabilities of mobile devices in
fields such as geographic information systems [15], entertainment,
education, business, and industry. Moreover, OpenVG [1] and Mobile 2D Graphics (M2G) are boosting the development of more 2D
applications that are scalable across any screen size.
2.3 Visual analytics on mobile devices
The creation of visual analytics for mobile devices has several challenges. It is different from visual analytics on common desktop
systems because of the restricted display space and computing resources of mobile devices. Sanfilipa et al. [19] introduced InfoStar [2], an adaptive visual analytics platform for mobile devices.
Their work was applied at SuperComputing2004 (SC2004) to provide information such as maps, schedules, an exhibitor lists, and
provided visual exploration to conference attendees. Similarly, the
work by Pattath et al. [16] provided a visual analytic tool for the vi-

36

Figure 1: System overview

sualization of network and sensor data gathered from Purdue RossAde Stadium during football games.
3 V ISUAL A NALYTICS FOR E MERGENCY R ESPONSE
For emergency response, a well-designed visual analytics system is
necessary. We need to tailor the display capability to the responders
and their roles and provide a succinct, quickly understood display
of relevant information extracted from all information acquired. For
example, SWAT (Special Weapons And Tactics) teams are highly
trained groups of police officers whose missions include hostage
rescue, dignitary protection, and high-risk warrant services. These
missions all require successful coordinated information collection
and exchange. In the case of the SWAT team responding to an active shooter in a school, the first and most critical requirement of
the team leader, as well as all responders, is the most accurate situational awareness possible:
• Where are all team members located?
• Where are the locations of responding personnel?
• Where are the secure, neutral and hot zones of the incident?
• What locations provide opportunity or threat information?
In addition, the capability to provide information back to the
emergency operation center, such as indicating rooms cleared or
information contradictory to current situational assessment, is also
vital. As previously indicated, relevant information is specialtydependent. A firefighter responding to a fire at the same building
would need some of the above information, as well as task-specific
information, such as fire spread, potential toxic gases or locations
of dangerous goods stored.
Moreover, the first generation mobile analytics should target
readily available technology, such as PDAs and smartphones. This
display system will be useful in decision support during emergency
response, as well as planning for event response. The system will
allow responders to be free from time spent on information gathering and focus on response actions. Response actions, such as asset
dispersal, will be assisted with a visual display of current information while after-action reviews will also be enhanced by having
the ability to clearly see information such as evacuation routes that
are not used efficiently. After-action review methods are also aided
through real-time analysis of actions taken during response time.
This system will enhance training for response to many unique
emergency situations, rather than simply the current scenario discussed here.
4 S YSTEM D ESIGN
Figure 1 shows the abstraction of our system structure that has a
server-client architecture. Our system focuses on the utilization of
various types of datasets such as images/video, 3D models, sensor

Figure 2: Data structure for streaming data.

data, and text data. All of the streaming data are received from and
are preprocessed by each server in the server group. In Figure 1,
the data converter in a server group converts all input data into the
appropriate types for the mobile visual analytic client.
This conversion is necessary for visualization on mobile devices
and involves determining the appropriate representation of the data
for rapid, in-field cognition on a small-screen mobile device. The
data created for desktop systems cannot be used for mobile devices
without further preprocessing because of the limits of mobile devices in terms of memory, bandwidth, and screen resolution.
The converter preprocessing has components that are customized
for each type of input data stream, but in general it uses a flexible
structure to allow the input of a variety of data based on the given
response situation. Moreover, the structure is designed to allow
tailored processing of the same input data for different response
situations and different roles in the response.
For our initial prototyping, we are using pre-generated simulations so that all of the data can be transmitted to the client initially,
similar to the approach of Pattath el al. [16]. However, the architecture is designed to allow real-time network pulling of data from the
server in actual response and training scenarios.
Our client visualization tool consists of four components: the
2D/3D visualization system, the visualization of streaming personnel location data and sensor data (initially simulated), visual analytics, and the user interfaces. These functions can be utilized for
situational awareness and assessment.
Preprocessing in a data converter
In our current system, we are using four input datasets that can
be roughly categorized by their real-time properties. The abstracted/simplified background images and font data as well as the
3D models are immutable data during processing for visual analytics; whereas the other data sources, the sensor/video data, and
location data are time-varying. Therefore, our data converters are
designed to process both time-varying data and static data.
For the static data (e.g., images/blueprints, 3D models, text files),
conversion to a format that enables execution on a mobile device in
real-time under the management of the server is performed. Using
vector images gives our system a scalable capability independent
of the device screen size and resolution. Hence, all images are converted into vector path data in scalable vector graphics (SVG) format [3]. We provide both two-dimensional and three-dimensional
visualization of incident location, since three-dimensional viewing
can provide various first-person views of the situation that provide
new perspectives to the responders. The two-dimensional view can
often provide the best, overall situational awareness for the responders but may lack the visual cues necessary for navigation in-field.
For the conversion of time-varying data (e.g., sensor/video data,
location data), special processing is needed to provide proper synchronization of the temporal data streams in a networked environ-

Figure 3: Floor plan of the Station nightclub. Image courtesy: National Institute of Standards and Technology.

ment. Moreover, filtering and selection of large data streams is necessary to enable real-time use on processor- and memory-limited
mobile devices.
To effectively utilize large streaming data on mobile devices, we
employ simple compression/transformations of the data to reduce
both network bandwidth and local storage requirements. We also
use data importance characteristics to determine update rates, data
items to skip, and interpolation methods to maintain our performance requirements. Finally, level of detail, level of abstraction,
and level of data aggregation are chosen not only to enable interactive performance but also to reduce visual clutter and enable effective visualization and analysis on the mobile device.
Management of streaming data on the client
To deal with the large size of time-varying data streams, we need
to utilize an appropriate data structure for storage. We use a circular queue, as shown in Figure 2, in order to minimize memory consumption. A circular queue is a particular implementation
of a queue, where insertion and deletion are totally independent.
Though our system focuses on client-based visualization and analytics, such queuing structures can be used for processing of streaming data in server-client architectures. In our work, the size of the
circular queue is set to 30 to provide short-term reference data for
visualization and still fit in the memory of PocketPC phones and
PDAs. In Figure 2, the simulation data manager serves as a communication handler in a server-client system and can be composed
of several types of data managers. The data manager is responsible for updating the appropriate entry type in each element of the
queue (e.g., sensor, location). The application in a main thread can
then take the data from the queue by time stamp without suspension
occurring due to any network traffic.
5 V ISUALIZATION ON MOBILE DEVICES
The main concerns for visual analytics on mobile devices are the
device limitations (screen size, memory) and the appropriate data
aggregation/abstraction level to enable effective decision making.
In our work, the device memory limitations are primarily solved
on the server component during data conversion to an appropriate
representation. Hence, our mobile visualization client mainly deals
with the visual representation to enable visual analytics on mobile
devices for emergency response.
5.1 Example test scenario
We employ the scenario of The Station nightclub fire of West Warwick, Rhode Island, that took place on the February 20, 2003. The
scenario has been completed from an investigation and a computer
simulation by the National Institute of Standards In Technology

37

Figure 4: Visualization of sensor agents in 2D environment; (left) 30
agents in alive status (green) and their movement path by fading line,
(right) four agents in unconscious status (red).

Figure 5: Visualization of sensor agents and 3D environment.

Table 2: Information required for analysis of fire emergency
Table 1: Information involved in simulated datasets
Data
Personnel

Fire

Information
Identifier of the agent (e.g. customer number)
The number of steps an agent took
Position in 2D space (with 640×480 coordinate)
Health levels
Temperature at each position
HRR at each position
Smoke at each position
CO2 , CO at each position

(NIST) [12] after the fire. We used two simulated datasets including
fire data and evacuation data of 419 intelligent agents. The fire was
simulated using the Fire Dynamics Simulator (FDS4) [4] developed
by NIST. FDS is a computational fluid dynamics (CFD) model of
fire-driven fluid flow and provides time-resolved temperature, carbon monoxide, carbon dioxide, and soot distribution throughout the
building for the duration of the fire. These calculations show how
the fire and smoke propagate through the building and the results
were used in the evacuation model for movement of agents. Agents
start to move towards the exits to find the nearest known exit at the
time fire started. All simulation and 3D model data that we used
for this work was provided by Purdue Homeland Security Institute
(PHSI) [8]. Figure 3 shows the floor plan of The Station nightclub.
The 3D model and the background image we used were generated
with same scales and locales such as The Station nightclub based
on the document [12] from NIST.
Table 1 shows information included in the two simulated
datasets. For the agent data, ‘the number of steps an agent took’
in Table 1 provides a time stamp for the simulation. The fire simulation files are combined into a solution dataset by a data converter,
as mentioned in Section 4.
5.2 Visualization of simulated data
For this scenario, as shown in Table 1, we visualize the two simulated datasets, agent and fire data, to provide efficient emergency information. These datasets have different characteristics that are representative of most emergency response datasets. The agent dataset
is agent-centered (e.g., location, health), whereas, the fire dataset is
global, time-centered information (e.g., time-centered fire spread,
carbon monoxide data).
Agent Data: The agent information is displayed using 2D vector

38

Object
Personnel

Environment

Information
Agent identifier
Path of evacuation
Health condition (alive, unconscious, dead)
Change of health level
Exit identifier used for evacuation
The number of evacuated agents per each time
Structure of an environment (building)
Exit area
The number of agents at each exit
Distribution of temperature, HRR
Distribution of toxic gas (CO, CO2 , Smoke)

graphics. The position is shown by a circle and the time-evolving
path is drawn by line segments. Based on the size of data contained
in the circular queue, the paths of the agent movement is visualized
using ghosting as a method of temporal visualization. The color
of each agent’s path can be pre-assigned based on agent/team designation, it can be changed based on the agent’s health, or it can
be randomly assigned. In our dataset, each agent has two health
indicators based on the fractional effective doses (FED) for heat
exposure and for gas concentration. Agents become unconscious
and cannot move anymore when either of these values reaches 0.5.
We nominally visualize healthy agents as green and display unconscious agents using red. A health level between 0.0 and 0.5 is visualized with yellow to orangish colors predefined in a health level
table. Since in this scenario, we are tracking the patrons evacuating
and not responders entering an incident, Figure 4 shows the evacuation of agents visualized in a 2D visualization for global evacuation
analysis. During the visualization, the number of agents with each
health status is displayed in the information window. After analyzing, it is apparent that only four of 30 have become incapacitated. In
addition, we provide a 3D perspective view of the agent movement
within the environment to better understand factors that may have
determined the evacuation paths chosen. Similarly, 3D navigation
and observation can help train first responders to have effective and
intuitive recognition of potential evacuation routes and visual building characteristics that may lead responders to probable alternative
paths taken by people missing during an actual emergency incident.
Figure 5 shows the movement of agents in our 3D environment using two views at different camera positions.
Fire Data: The fire simulation data includes the level of temperature, HRR (Heat Release Rate), smoke, CO2 and CO in the fire.

Figure 6: Visualization of the temperature spread at different time
steps.

Figure 8: Visualization of information related to agents: (left) the
number of agents in each health condition, (right) the number of
agents at each exit.

Figure 7: Visualization of the CO spread at different time steps.

Figure 9: Visualization of the rates for evacuation and crowd.

To visualize this information, we use separate 16-element color and
gray-level tables. Temperature and HRR are displayed using colors, while smokes, CO2 and CO are drawn using gray-levels. The
visualization of each is overlaid on the 2D or 3D environment. Figure 6 and Figure 7 show the results of the visualization of temperature and CO data at different time steps. In Figure 6 and Figure 7,
we use 7×7 pixels to interpolate and visualize temperature and CO
since the data was transformed on a coarse grid for performance.
We also encountered a problem with this data because the fire simulation data we have is not exactly aligned to the provided 2D floor
map because the simulation was conducted for the bounding area of
the building of the 2D map. Figures confirming this misalignment
can be found in the Rhode Island Nightclub Investigation Image
Archive [5] of NIST.
5.3 Mobile Visual Analytics
There is general analytic information that is commonly required for
most emergency response situations. Such information helps first
responders suggest response priorities and plan actions based on
their evaluation of effective situational awareness common operating picture data. Table 2 classifies and lists analytic methods for
our simulated agent and fire data in terms of personnel (agent) and
environment categories.
The most basic information is the location and movement of people and assets (see Figure 4 and Figure 5). In our scenario, to analyze the effectiveness of the evacuation, the number of agents at

each status (alive, unconscious, dead) and the number of agents at
each exit are provided in an information window. Figure 8 shows a
few visual analytic results with numerical values. It shows the number of agents in each health condition and the number of agents at
each exit used for evacuation. When the fire started, most agents
started to run for the exit that was most familiar to them or was
closest to them. However, this is not the exit most heavily used:
most agents ran towards the bar exit. Figure 9 shows the congestion near the main exit that caused the heavy usage of the bar exit.
In Figure 6, Figure 7, and Figure 8, we obtain an unexpected result
from the analysis. Although the kitchen area was safer than others
were in terms of temperature and carbon monoxide, nobody used
this area for the evacuation. Figure 9 shows the rates (the number
of agents per second) of evacuation during the fire. The slope of
the data line decreased when the main exit became blocked by the
crowd. Thus, many agents chose the other exit (bar exit) for evacuation instead of the main exit. This happened 90 seconds after the
fire occurred. The kitchen exit was not used as an evacuation exit
because of its unfamiliarity. Figure 10 shows the information of a
specific agent selected by a user. The selected agent is displayed
using magenta. In Figure 10, the agent with ID = 21 became unconscious near bar exit before evacuation (left). The FED of CO of
the agent is over 0.5, whereas the agent with ID = 23 who evacuates
through the stage exit shows the FED of heat and gases at low levels (right). Moreover, obtaining the change in health levels of each
agent at each time can help first responders (e.g., fire fighters) to

39

Figure 10: Information of a selected agent: (left) the agent located in
front of a bar exit has not yet evacuated, (right) the agent positioned
at a stage exit succeeded in evacuation.

Figure 12: Overall user interfaces for 2D: (left) main menu, time
slider, graph window and text window, (right) change of view direction
and zoom-in.

Figure 11: Change of the health condition of agents.

Figure 13: Views of the agents in 3D wireframe environment.

establish rescue priorities. Figure 11 shows different health levels
of 100 agents by using distinct colors in the middle of the duration
of the visualization.
6

I MPLEMENTATION AND

RESULTS

We have implemented and tested our tool on a Dell Axim X51v
PDA that uses the Intel 2700G graphics processor and 16MB of
video RAM and on a Sprint PCS VisionSM smart device PPC-6700,
which uses the Window Mobile 5.0 and a 416 MHz Intel processor. However, our tool will be run on any PDA using Pocket PC
with sufficient processing capabilities. We use the Hybrid Rasteroid3 for OpenGL|ES and OpenVG library provided by Hybrid
Graphics, which is a reference implementation and provides functions by OpenGL|ES 1.1, OpenVG 1.0 and EGL 1.3 specification
announced by Khronos [1] group. All images in this paper were
captured with the Win32 version of our system. Figure 14 shows
our system running on a Dell Axim X51v PDA and a Sprint PCS
smartphone.
We set the main screen as a 2D orthogonal projection of a building model for global situational awareness since the visualized entities are all in the same 2D plane. In addition, our system does
provide 3D perspective views of all the data within the 3D building
model. All of the user interfaces are represented with transparency
in order to provide a non-invasive interface. The main menu and
information window can be also be hidden to not interfere with

40

situational awareness visualizations. Therefore, this interface can
always guarantee a full main view of the situation to the user. Due
to the small screen size of mobile devices, the problem of an efficient user interface is another challenge for visualization on mobile
devices. GLUT|ES [6] has been developed for WinCE and Win32
systems based on OpenGL|ES as the open source implementation.
However, it can be space-consuming for the visualization of information. Therefore, we implement the API for a user interface based
on OpenVG. Currently, it provides a button, a time slider, a hiding
window, a line graph, and vector fonts. Figure 12 shows the overall user interfaces and camera changes in the 2D environment. All
buttons can be activated and deactivated. We use the buttons for
play, pause, stop, speed-up/down, toggle of 2D/3D, display of exits, viewing of fire spread (e.g., temperature, HRR, CO2 and CO),
and selection mode. The time slider shows the progress of the overall simulation. Menu windows are opened with their own button.
Our tool has two menu windows: one is used for displaying text
information and the other is used for visualizing additional graphic
information, such as graphs. Figure 13 shows some of the viewing
options including a wireframe and a bounding box of a 3D model
in our tool.
As a prototype for mobile visual analytic tools for emergency
response, our tool presents efficient and interactive visual analytic
methods and provides visualization of various types of data. For
situations requiring rapid decisions, such as emergency response
analysis and services, our system can be used as an efficient testbed.

a detailed and extensive assessment and comment on the exercise
with written evaluations that takes place several days or weeks after
the exercise. The AAR does not judge success or failure but rather
focuses on learning what happened, why things happened, and what
tasks and goals were or were not accomplished. Mobile visual analytics adds mobility to common visual analytics and can provide
enhanced situational awareness on site during the hotwash. Such
rapid and appropriate awareness leads to “lessons learned,” which
is intended to guide future response direction in order to avoid repeating errors made in the past. Hence, the effectiveness of analysis
and evaluation to identify strengths and weaknesses of the response
to a given situation can be enhanced.
The capabilities of visual analytics needed for the hotwash include providing integrated visual analytics of additional data. Visual analytics of correlated 3D, 2D, video, and audio data will
be extremely beneficial to created lessons learned from exercises
and enable new insight from the interactive exploration and analysis of all information captured during the exercise. Replaying by
time slider and location, by exercise plan as chapters, and at increased/decreased speeds is demanded. Pause, fast-forward, and
rewind functions are also required. Evaluators and analysts will
request to display various video, 2D/3D scene reconstruction, and
statistical results of the exercise. For requirements of the above aspects, our system has been equipped with enough functionalities
and extendibility to allow its expansion for these activities.
To evaluate the effectiveness and capacity of our system to be
used in real emergency situations or emergency training, we had
several current and former emergency response personnel (state police, fire) use our system and provide informal feedback. Their
feedback indicated that they felt our system could be useful in real
emergency situation when integrated with real time personnel tracking data because accurate situational awareness is a crucial issue in
real situations. Responders also felt that our system will be very
useful for personnel training, including pre-planning scenarios and
site inspections.
8

Figure 14: Photos of our system running on PDA (top) and smartphone (bottom)

Based on the overall visualization and analysis for our test simulation datasets, we observed that some agents evacuated using the
stage exit at the beginning of the fire. Most agents ran about in confusion while they moved to the exits located opposite to the source
of the fire near the stage. Some agents who tried to evacuate out
of the main exit failed because of congestion near the corridor between the main exit and the main bar. Hence, many agents moved
to the bar exit. As a result, 21 out of 100 agents became incapacitated and 24 agents did not find an evacuation exit. Most of the
agents who became unconscious were found near the main exit. At
the completion of the simulation, 35% exited via the bar exit, 0%
via the kitchen exit, 16% via the main exit, 8% via the stage exit,
and 17% of the agents used a window for their evacuation.
7

C APABILITIES & P OSSIBILITIES FOR M OBILE V ISUAL
A NALYTICS FOR EMERGENCY RESPONSE

Visual analytic systems for emergency response can be used not
only during actual emergency events, but also during training, and
for hotwash [7] and After Action Reviews (AAR) of exercises and
incidents. The hotwash is an immediate debriefing and critique conducted immediately after the exercise and incident. The AAR is

C ONCLUSION AND F UTURE W ORK

We have shown a flexible prototype mobile visual analytics system
for emergency response and demonstrated its use for a building fire
evacuation. For situations requiring rapid decisions such as placement and location of public safety assets during a critical incident,
our system can be used as an efficient prototype and testbed.
In the future, we will extend this work to include more analytic
functions to enhance emergency situational awareness and support
rapid decision making. For example, a tool for interactively selecting specific personnel groups and comparing information within
and between them can improve analysis of response asset allocation
and training effectiveness. Moreover, our system can be extended
for actual first responder 3D tracking, visualization, and video information for training and in-field deployment support. The integration of RSS data and social network data (e.g., family, friend
group, local community, police, fire station, hospital, and government department) will provide interesting visual representation and
interrogation challenges, and will further increase the usefulness of
our system for emergency response.
ACKNOWLEDGEMENTS
We wish to thank the Purdue Homeland Security Institute (PHSI)
for supplying the simulation data. This work has been funded by
the U.S. National Science Foundation (NSF) under grants 328984
and 0121288, and by the U.S. Department of Homeland Security
Regional Visualization and Analytics Center (RVAC) Center of Excellence.
R EFERENCES
[1] [Khronos Group] http://www.khronos.org/.

41

[2] [Infostar]
http://www.sc-conference.org/sc2004/
infostar.html.
[3] [W3C: Scalable Vector Graphics (SVG) XML Graphics for the Web]
http://www.w3.org/Graphics/SVG/.
[4] [NIST Fire Dynamics Simulator (FDS) and Smokeview]
http://fire.nist.gov/fds/.
[5] [Rhode Island Nightclub Investigation
Image Archive]
http://www.nist.gov/public_affairs/ri_archive/
RI_imagearchive.htm/.
[6] [GLUT|ES - The OpenGL|ES Utility Toolkit]
http://glutes.sourceforge.net/.
[7] [Wikipedia: Hotwash]
http://en.wikipedia.org/wiki/Hotwash.
[8] A. Chaturvedi, A. Mellema, S. Filatyev, and J. Gore. Dddas for fire
and agent evacuation modeling of the rhode island nightclub fire. In
ICCS ’06: Workshop on Dynamic Data-Driven Application Systemse,
pages 433–439, Berlin, Heidelberg, 2006. Springer-Verlag.
[9] J. Diepstraten, M. Gorke, and T. Ertl. Remote line rendering for mobile devices. In Proceedings of the Computer Graphics International,
pages 454–461, 2004.
[10] J. Dollner and M. Walther. Real-time expressive rendering of city
models. In Proceedings of the 7th International Conference on Information Visualization, pages 245–250, 2003.
[11] F. Fan and E. S. Biagioni. An approach to data visualization and
interpretation for sensor networks. In HICSS ’04: Proceedings of
the 37th Annual Hawaii International Conference on System Sciences
(HICSS’04) - Track 3, page 30063.1, Washington, DC, USA, 2004.
IEEE Computer Society.
[12] W. L. Grosshandler, N. P. Bryner, D. Madrzykowski, and
K. Kuntz.
Report of the technical investigation of the
station nightclub fire.
[NIST NCSTAR 2:
Volume 1]
http://www.nist.gov/public_affairs/releases/
RI_finalreport_june2905.htm/, June 2005.
[13] D. Hekmatzada, J. Meseth, and R. Klein. Non-photorealistic rendering of complex 3d models on mobile devices. In Proceedings of 8th
Annual Conference of the International Association for Mathematical
Geology, volume 2, pages 93–98, 2002.
[14] S. O. Koo, H. D. Kwon, C. G. Yoon, W. S. Seo, and S. K. Jung. Visualization for a multi-sensor data analysis. In Proceedings of International Conference on Computer Graphics, Imaging and Visualization,
pages 57–63, 2006.
[15] M. Masoodian and D. Budd. Visualization of travel itinerary information on pdas. In AUIC ’04: Proceedings of the fifth conference
on Australasian user interface, pages 65–71, Darlinghurst, Australia,
Australia, 2004. Australian Computer Society, Inc.
[16] A. Pattath, B. Bue, Y. Jang, D. S. Ebert, X. Zhong, A. Ault, and
E. Coyle. Interactive visualization and analysis of network and sensor
data on mobile devices. In VAST ’06: Proceedings of IEEE Symposium on Visual Analytics Science and Technology. IEEE Computer
Society, 2006.
[17] J. Pouderoux and J.-E. Marvie. Adaptive streaming and rendering of
large terrains using strip masks. In GRAPHITE ’05: Proceedings of
the 3rd international conference on Computer graphics and interactive techniques in Australasia and South East Asia, pages 299–306,
New York, NY, USA, 2005. ACM Press.
[18] J.-C. Quillet, G. Thomas, and J.-E. Marvie. Client-server visualization
of city models through non photorealistic rendering. INRIA Technical
Report, September 2005.
[19] A. Sanfilippo, R. May, G. Danielson, B. Baddeley, R. Riensche,
S. Kirby, S. Collins, S. Thornton, K. Washington, M. Schrager, J. V.
Randwyk, B. Borchers, and D. Gatchell. An adaptive visual analytics platform for mobile devices. In SC ’05: Proceedings of the 2005
ACM/IEEE conference on Supercomputing, page 74, Washington, DC,
USA, 2005. IEEE Computer Society.
[20] J. J. Thomas and K. A. Cook, editors. Illuminating the Path: The R&D
Agenda for Visual Analytics. IEEE Press, 2005.

42

