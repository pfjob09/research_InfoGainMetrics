Combining Statistical Independence Testing, Visual Attribute Selection
and Automated Analysis to Find Relevant Attributes for Classification
Thorsten May∗

James Davey†

Fraunhofer Institute for Computer Graphics Research

Fraunhofer Institute for Computer Graphics Research

¨ Kohlhammer‡
Jorn
Fraunhofer Institute for
Computer Graphics
Research

A BSTRACT
We present an iterative strategy for finding a relevant subset of attributes for the purpose of classification in high-dimensional, heterogeneous data sets. The attribute subset is used for the construction of a classifier function. In order to cope with the challenge
of scalability, the analysis is split into an overview of all attributes
and a detailed analysis of small groups of attributes. The overview
provides generic information on statistical dependencies between
attributes. With this information the user can select groups of attributes and an analytical method for their detailed analysis.
The detailed analysis involves the identification of redundant attributes (via classification or regression) and the creation of summarizing attributes (via clustering or dimension reduction). Our
strategy does not prescribe specific analytical methods. Instead, we
recursively combine the results of different methods to find or generate a subset of attributes to use for classification.

guarantee that meaningful differences can be calculated.
To combine attributes of different types of scale Johansson [2]
proposed a quantification of categorical data. While this approach
is very generic, the numeric representation of an attribute may not
reflect the semantics of its data type.
Instead, our strategy extends MacEachren’s approach [3], which
uses a discretization of numerical data. The selection of attributes
is supported by a coarse dependency analysis which is used to
control the detailed exploration using a dedicated visualization.
Our extension defines a strategy for feeding the results of the
detailed (visual or automated) analysis back into the dependency
analysis.

Index Terms: High-Dimensional Data, Dimensionality Reduction, Data Clustering, Data Filtering
1 I NTRODUCTION
In numerous applications massive amounts of multivariate,
heterogeneous data are created and stored in archives. With
an increasing number of dimensions, filtering out the relevant
pieces of information becomes a more and more challenging task.
Heterogeneous data sets, consisting of nominal, numerical or even
hybrid types of attributes, pose an additional challenge. Because
important relations might exist between attributes of different types
of scale, automatic methods must be carefully selected and applied.
We propose a strategy supporting classification in a highdimensional data table, consisting of attributes of different types.
Given a target attribute, the general goal is to find a classifier
function which uses other attributes of the data set as its input. We
assume that the attributes which are relevant for the classification
are not known in advance. Hence, our strategy supports a subgoal
of the classification task.
The goal of the strategy is to find or create a candidate subset of
attributes with the following properties; firstly, it must contain all
information necessary to learn the classifier function. Secondly it
must contain as little redundant information as possible. Guyon et
al provide an introduction to this task from the machine learning
perspective [1]. Methods for feature selection or dimension
reduction have been proposed as a solution to this problem for
specific types of data. However, many approaches require that
the attributes are of numerical, commensurable types in order to
∗ e-mail:

thorsten.may@igd.fraunhofer.de
james.davey@igd.fraunhofer.de
‡ e-mail:joern.kohlhammer@igd.fraunhofer.de
† e-mail:

IEEE Symposium on Visual Analytics Science and Technology
October 24 - 29, Salt Lake City, Utah, USA
978-1-4244-9487-3/10/$26.00 ©2010 IEEE

Figure 1: The proposed strategy couples two alternating subprocesses which operate on different levels of detail. The overview
process (left) operates on the complete data set yet provides only
generic results on the dependency between attributes. These results
are visualized to control the detailed analysis (right) interactively. Different methods may be used to identify redundant attributes, to create summarizing attributes or to compute a discretization. All methods for detailed analysis only need to operate on a small group of
attributes. The intermediate results are inserted into the data table
and can be reused in subsequent iterations.

2 O UR A PPROACH
Our strategy consists of two alternating processes (see figure 1).
These processes analyze the dependencies between attributes at two
levels of detail.
The first process provides a coarse overview over the dependencies between all attributes in the data set. We use a stochastic,

239

parameter-free test for independence - the Chi-Square test. This
test has two advantages; firstly, it can be applied to any pair of attributes in the data set regardless of their type. Secondly, stochastic
independence is the most generic representation of independence.
The test statistic is used to calculate a measure of dependency between a selected target attribute and all other attributes. We use a
matrix display which shows the measures for all attributes and for
all discrete partitions of the data set (see figure 2). In the matrix
display the user interactively selects a set of attributes and a set of
partitions which is inspected in the second process.
In the second process the dependencies found with the generic tests
are analyzed in detail. In the context of our strategy the detailed
analysis serves three different purposes:
1. Identifying redundant attributes in the selection,
2. Creating a new attribute which summarizes information from
a given number of attributes, and
3. Computing or refining the discretization of an attribute.
In each iteration one of these tasks is performed as a detailed analysis. The task and the method are chosen by the user, based on the
dependencies found and the types of attributes involved.
Both subprocesses are tightly coupled; if a new attribute is created
in the detailed analysis, it is added to the data table. It is then included in the next test for dependency and can be treated like every
other attribute in the original data set in subsequent iterations. Using this approach we can combine results from very different methods in an effective way.
The recursive combination of attributes was inspired by Yang et al.
[5] and other visual hierarchical clustering methods as presented,
for instance, by Wilkinson and Friendly [4]. However, hierarchical clustering methods only search for similar attributes which is a
special kind of dependency.

for classification then the main task can be considered completed.
In any other case the redundant attribute is eliminated from the candidate subset of relevant attributes.
The second task creating a summarizing attribute applies if no
strong dependencies can be found. In this case attributes may be
created to summarize the information in a group of weakly dependent attributes. Clustering and dimension reduction methods may
be applied to complete this task. Clustering methods create a new
discrete attribute which contains the class labels. Dimension reduction methods generate one or more numerical attributes. In both
cases the attribute values are calculated by applying the selected
methods to the data items.
The third task computing or refining a discretization is actually a
preparatory step for the independence test and is only performed on
one attribute at a time. Numerical attributes and also discrete attributes with an excessively high number of different items must be
mapped to a discretization with a sufficient number of data samples
per category. Technically, the discretization is a proxy representation of the attribute, which can also be used in other methods if
required. We do not prescribe the discretization method to be used.
However, if an initial discretization must be computed for the first
independence test then we use a K-Medoids clustering method applied to the univariate distribution.
4 R ESULTS
Using the proposed strategy as a template, we built a framework including a pool of methods for the three tasks. Thus far, we have included visual and automated clustering, dimension reduction methods and classification methods. The methods have been applied
to the analysis of dependencies in the US-census data set, which
has 70 attributes. The independence test and the matrix visualization are highly scalable, providing an overview of the strongest and
weakest dependencies in the dataset. Results from previous iterations can be easily combined to describe more complex relationships.
The computational effort for the dependency testing increases linearily with the size of the data table. Because the intermediate results can be stored, the most expensive operation is the initialization.
The large number of options the user has to proceed after the independence test is one of the remaining issues. For the selection of
attributes and the selection of a suitable method for detailed analysis
the user needs to know the relationships which were found in previous iterations. A method for the automatic preselection of attributes
and methods could improve the effectiveness of the strategy. However, we believe that for the most generic data sets the possibility of
human intervention remains a crucial factor.
R EFERENCES

Figure 2: The overview visualization shows the results of the independence test (left). Blue cells indicate high dependencies with regard to either specific attributes (columns) or specific partitions in the
dataset. The user selects a number of attributes and/or data partitions based on her goals in the detailed analysis. Detailed analysis
(right) can be used to inspect and create new attributes which are
returned to the attribute pool.

3 D ETAILED ANALYSIS
Our strategy does not prescribe individual methods. Instead every
task can be associated with a class of methods:
The first task identifying redundant attributes applies if a strong dependency is detected between two or more attributes selected for
classification. An attribute is redundant if its dependence on other
attributes can be modeled as a function. Strong dependencies can be
analyzed with methods such as similarity testing, classification or
a regression model. If the redundant attribute is the target attribute

240

[1] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine Learning Resarch, 3:1157–1182, 2003.
[2] S. Johansson. Visual exploration of categorical and mixed data sets. In
VAKD ’09: Proceedings of the ACM SIGKDD Workshop on Visual Analytics and Knowledge Discovery, pages 21–29, New York, NY, USA,
2009. ACM.
[3] A. M. MacEachren, X. Dai, F. Hardisty, D. Guo, and E. Lengerich.
Exploring high-d spaces with multiform matrices and small multiples.
In Proceedings of the IEEE Symposium on Information Visualization
(InfoVis). IEEE Computer Society, 2003.
[4] L. Wilkinson and M. Friendly. The history of the cluster heat map. The
American Statistician, 63(2):179–184, 2009.
[5] J. Yang, M. O. Ward, E. A. Rundensteiner, and S. Huang. Visual
hierarchical dimension reduction for exploration of high dimensional
datasets. In Proceedings of the symposium on Data visualisation (VisSym), pages 19–28, Aire-la-Ville, Switzerland, Switzerland, 2003. Eurographics Association.

