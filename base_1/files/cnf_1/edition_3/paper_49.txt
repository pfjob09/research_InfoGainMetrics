VIDI Surveillance - Embassy Monitoring and Oversight System
VAST 2009 Traffic Mini Challenge Award: Good Clarity of Analysis Supported by Visuals
VAST 2009 Video Mini Challenge Award: Good Integration of Open Source Tools for Video Analysis

Chad Jones∗

Michael Ogawa†

James Shearer‡

Anna Tikhonova§

Kwan-Liu Ma¶

VIDI Group
University of California, Davis

1

T RAFFIC C HALLENGE

What follows is the analytic approach we submitted to the challenge, edited for space.
1.1 Finding Suspicious IP Activity
We hypothesized that potential spies would try to use other employees’ terminals in order to not draw attention to themselves. Of
course, this means that the actual owner of the compromised terminal could not be present while the spy uses it. While we do not have
exact information on when employees enter or leave the building
(the badge reader is not reliable), we do know when they are inside
the classified area. Therefore, we define one type of suspicious activity as IP use on a terminal when the owner is inside the classified
area.
We created a timeline visualization of IP usage, overlaid with
classified area entrances and exits. The vertical axis divides the
timelines into 31 rows, one for each day of the month. The horizontal axis represents the time of day from early morning to late
evening. A single employee’s entire month is viewed all at once using this visualization. The employee being viewed can be changed
using the arrow keys. Every IP event is represented by a vertical
bar positioned at the exact time of its appearance. We color the
IP events by port number, which is either intranet, http, tomcat, or
email, and size the bar based on the outgoing data size. Whenever
an employee enters the classified area, a semi-transparent yellow
region is drawn until that user exits the classified area. In rare cases
when the user double enters, the region is twice as opaque, and in
the other rare case where a user leaves the exits without entering, a
red region is drawn until the next time the employee enters. The legend key and office diagram showing the current selected employee,
highlighted in red, can be seen in the top left-hand corner.
Employee behavior begins to emerge as each person is viewed in
turn. Whenever an employee begins using his or her computer for
the day, there is a single authenication into the intranet server. It is
also possible to note each employee’s lunch habits on a daily basis
by observing periods of absent activity and occasional re-entries
into the building during the middle of the day.
Since there are 60 employees and 31 days of data, we visually
highlighted IP accesses when the employee is inside a classified
area by eliminating the time spans when they are not (i.e. in the
normal office area or outside the building). This showed us quite
clearly that there were several suspicious accesses on several employees’ terminals. Furthermore, by querying these accesses, we
determined that, in all cases, the destination IP is 100.59.151.133
∗ e-mail:
† e-mail:
‡ e-mail:
§ e-mail:
¶ e-mail:

cejjones@ucdavis.edu
msogawa@ucdavis.edu
jjshearer@ucdavis.edu
tikhonov@cs.ucdavis.edu
ma@cs.ucdavis.edu

IEEE Symposium on Visual Analytics Science and Technology
October 12 - 13, Atlantic City, New Jersey, USA
978-1-4244-5283-5/09/$25.00 ©2009 IEEE

using socket 8080. We refer to traffic with this IP/socket pattern as
package drops.
We then asked the question: Do the same type of suspicious accesses occur when victim employees are not in the classified area?
To test this hypothesis, we highlighted the package drops by increasing their visual size and darkening their color.
We can now see these package drops clearly, even with normal
traffic present in the visualization. Many of the package drops, if
not occurring when the victim is inside the classified area, appear
when the victim is plausibly outside the building. We are now confident that the package drops represent the espionage activity the
embassy suspects is happening.
1.2 Finding the Culprit
Using our tool, we can mark the times when package drops occur
and see them overlaid on all employee activities. This is done manually by freehand drawing onto the timeline itself. We are able to
circle areas of interest, make notes, and emphasis important times
as we please.
Starting with the pool of all employees, we eliminate employees
who are in the classified area at any time a package drop occured.
This leaves us with employees #27 and #30 who had plausible
opportunity to hijack a terminal. However, if #27 were the culprit,
they would hijack a terminal almost 2 hours before they show any
other activity on the 22nd day. This makes #30 the most likely
culprit. (Figure 1)
The likelihood of #30 being the culprit is supported by the fact
that the first two package drops, as well as the fourth, were made
from Employee 31’s terminal. Since 30 and 31 share an office,
gaining access to 31’s account would be easier for 30 than any other.
All of the culprit’s victoms can be seen as the green highlighted
spaces in the office diagram on the left. Employee #30’s central
location gives him an easy view to many of the terminals, and the
access pattern is concentrated around his office.
1.3 Unanswered Questions and Recommendations
• How did the culprit get access to so many terminals? We recommend an evaluation of computer security policies and practices.
• What do the badge anomalies mean physically, i.e. double entries and no-entry exits? We recommend camera and/or guard
placement at the door to the secure area to make sure piggybacking and unauthorized access do not occur.
2 V IDEO C HALLENGE
Our goal was to design a system for assisting an analyst in finding
meetings in a large, noisy video data set. In exploring this goal,
we employed a variety of open source and free software packages,
eventually arriving at a useful and robust system.
2.1 Initial Steps
We felt the best approach to the given video data was to first utilize the background/foreground segmentation routines provided by

257

denote object size. We reasoned that cases where two lines cross
in both plots simultaneously would correspond to meetings. This
yielded a useful but absolutely non-scalable visualization.
We decided to take a different approach and offer direct browsing of the video guided by the extracted objects. First, we needed
to pare down the raw data into something more manageable. We
reasoned that each viewpoint offered certain expected corridors of
movement where meetings could take place, and many regions,
such as the building front faces, where they could not. Furthermore,
we reasoned people had an expected size range in each viewpoint.
We wrote a few python-based filtering scripts that enabled us to remove objects that were too large, too small, or did not occur in the
expected pixel regions.

Figure 1: Visualization of Employee 30’s badge and network IP activity (cropped). Times when package drops occur are manually
marked in black. Employee 30 does not have alibis during these
times. Created with Processing [3].

OpenCV [2] to identify moving objects in the footage. We developed a small program that transformed a single-viewpoint video
into a text listing of the detected foreground objects, wherein each
object has a position and size for each frame it appears. In order to
obtain single-viewpoint videos, however, we first had to multiplex
the provided two videos into independent viewpoint streams.
At first, we assumed that the security camera changes viewpoints
after a fixed amount to time. Yet some initial, simple frame extraction using ffmpeg [1] demonstrated that we could not obtain
frame-accurate multiplexing using wall-clock timing. Instead, we
again turned to OpenCV, creating a simple program that leveraged
basic frame differencing and image thresholding to identify frameto-frame transitions in which a predefined percentage of the pixels
dramatically changed. This yielded excellent multiplexing results
for only a few hours of development and debugging. However, we
quickly found that OpenCV is not a good tool for extracting a large
number of individual frames. We used our program to output a
list of ranges of time steps corresponding to each viewpoint. Then,
we wrote a python script to quickly extract actual frames from the
video.
2.2 Exploration
After processing the eight resulting videos with our object detection
and tracking tool, we had several text files overflowing with objects,
positions, and sizes. Being students in a visualization lab, our first
instinct was to transform this text into something visual - something
that would reveal the meetings at a glance. We used Processing [3]
to develop a quick visual prototype which separately plotted object x and y positions as a function of time, using line thickness to

258

2.3 Tying Everything Together
We now had a filtered list of potentially interesting, meeting-worthy
objects for each viewpoint, but no way to examine them effectively.
We desired a way to separate the spurious detections that remained
after the filtering step, such as those induced by dramatic lighting changes, from real people walking down the street. Our idea
was to create a “super-thumbnail”, representing the entirety of a
detected object’s movement in the video, in a single image. Our
first idea was to use Matlab to simplify the object plots discussed
previously and to extract inflection points from those plots. The
inflection points correspond to “key” frames that show such important events as an object appearing on the screen, stopping to talk to
somebody, walking away from a meeting, and leaving the screen.
Then, for each detected object, we extracted pixels containing the
object from the key frames and over-composited the objects onto a
starting frame, where an object first appears. We quickly realized,
however, that that was too little information.
We then wrote a python script - using free bindings to
OpenCV and Apple’s CoreGraphics framework - that created superthumbnails. Each thumbnail is an alpha-composited sampling of all
the frames in which a given object exists in the video stream. The
end result is a single frame with ghosted-trails for each moving object. This allows the user to see at a glance all the objects which
appear in a given detection sequence, and the approximate trajectories of those objects.
We unified the per-viewpoint video streams and the corresponding thumbnails in a simple interface created using the Cocoa frameworks bundled with Mac OS X. In this interface, the analyst can
navigate the thumbnails with a mouse or keyboard. As he/she does
this, the video automatically scurbs to the correct location. The user
can then further scrub back and forth through the video using fingers on a trackpad. The composite thumbnails provide the spuriousverus-interesting separation we desired, and the scrubbable movie
easily confirms or rejects detected objects as suspicious.
Our video entry was supported by open source (OpenCV,
Python, Processing, ffmpeg), or free software (Cocoa) every step
of the way. Initial exploration, fruitful but false starts, and the final result would not have been possible in the short time we had to
spend on this entry if not for these valuable tools.
ACKNOWLEDGEMENTS
The authors wish to thank the VAST Challenge committee for creating a fun and well-prepared dataset.
R EFERENCES
[1] FFmpeg. http://ffmpeg.org.
[2] OpenCV. http://opencv.willowgarage.com/wiki/.
[3] Processing. http://processing.org.

