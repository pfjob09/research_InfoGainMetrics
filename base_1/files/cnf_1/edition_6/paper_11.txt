Interactive Visualization and Analysis of Network and Sensor Data on
Mobile Devices
Avin Pattath∗
Xuan Zhong†
∗ Purdue

Brian Bue∗
Yun Jang∗
David Ebert∗
Aaron Ault†
Edward Coyle†

University Regional Visualization and Analytics Center (PURVAC)
† Center for Wireless Systems and Applications (CWSA)
Purdue University, West Lafayette, IN

A BSTRACT
Mobile devices are rapidly gaining popularity due to their small
size and their wide range of functionality. With the constant improvement in wireless network access, they are an attractive option
not only for day to day use, but also for in-field analytics by first
responders in wide spread areas. However, their limited processing, display, graphics and power resources pose a major challenge
in developing effective applications. Nevertheless, they are vital for
rapid decision making in emergencies when combined with appropriate analysis tools.
In this paper, we present an efficient, interactive visual analytic system using a PDA to visualize network information from
Purdue’s Ross-Ade Stadium during football games as an example of in-field data analytics combined with text and video analysis. With our system, we can monitor the distribution of attendees
with mobile devices throughout the stadium through their access
of information and association/disassociation from wireless access
points, enabling the detection of crowd movement and event activity. Through correlative visualization and analysis of synchronized
video (instant replay video) and text information (play statistics)
with the network activity, we can provide insightful information to
network monitoring personnel, safety personnel and analysts. This
work provides a demonstration and testbed for mobile sensor analytics that will help to improve network performance and provide
safety personnel with information for better emergency planning
and guidance.
Keywords: mobile visualization, network visualization, visual analytics
Index Terms: I.3.2 [Computer Graphics]: Graphics Systems—
Network Graphics; I.3.8 [Computer Graphics]: Applications—
Visual Analytics
1

I NTRODUCTION

Mobile devices are becoming ubiquitous in our high-tech society.
As such, technological advances are occurring at an increasing rate,
enabling advanced information processing capabilities on such devices. Advances in processor clock rate, graphics capabilities, wireless networking, and low-power designs have increased the potential for detailed analysis using mobile devices.
∗ e-mail:
† e-mail:

{apattath|bbue|jangy|ebertd}@purdue.edu
{zhongx|aultac|coyle}@purdue.edu

IEEE Symposium on Visual Analytics Science and Technology 2006
October 31 - November 2, Baltimore, MD, USA
1-4244-0592-0/06/$20.00 © 2006 IEEE

Simultaneously, the quantity of data available for analysis is
steadily increasing, requiring improved processing tools in order
to quickly access and evaluate information. In some situations, it
is desirable or necessary to process this information on site and, as
a result, mobile devices provide a critical solution to in-field data
analysis.
Management of sensor and network data is one area that benefits from in-field analysis. Wireless networks pose a challenge in
extracting relevant connectivity information for issues such as network diagnostics and load balancing, while sensor networks pose
similar problems. Additionally, as sensor technology advances, the
quantity of data increases, and methods need to be developed to
allow in-field visual analysis of large data streams.
In-field analysis can also be beneficial to the analysis of social
behavior based on situational stimuli. Sporting events are an excellent test area for such analysis, as there are many issues that arise
in the effective production of such events. For example, the sheer
quantity of attendees may require additional considerations with
respect to emergency situations and general crowd control. Also,
event organizers can benefit from exploratory data analysis in order
to improve their services and to increase customer satisfaction.
It should be noted that data from seemingly unrelated sources
can often be used to better analyze some types of situational characteristics. One such method is the analysis of network data to determine crowd activity and interactions. By examining connectivity
information in a wireless network, we can extract cues not only on
network issues, but also on how users react in certain situations.
Thus, the same information that we use to improve network performance could potentially be used for crowd-guidance situations.
Our work is best described as “visual analytics.” Visual analytics is defined as “the science of analytical reasoning facilitated by
interactive visual interfaces” [21]. More specifically, the integration of mobile devices into the analytical process is referred to as
“mobile analytics.” Using state-of-the-art mobile devices, we can
enhance the analytical process through interactive, integrated data
analysis and visualization, enabling the user to extract important
features necessary for rapid, actionable decision making on site.
However, the capabilities of mobile devices are not without their
limits. Low screen resolution, such as 240 x 320, prevents large
amounts of data from being displayed on screen. Power considerations prevent large storage capacities on the device, as well as
severely limiting display and network capabilities. These limitations drive us to incorporate compact (but detailed) analysis, correlation and visualization techniques through which complex information can be clearly conveyed.
We must also be aware of issues in data uncertainty and confidence. In many cases, using a combination of data sources can
help to resolve ambiguities. Multi-modal fusion and synchronized
multiple information techniques can be of great use in the prompt,
accurate analysis of input data.
In this paper, we utilize information from Purdue’s Ross-Ade

83

(a)

(b)
Figure 1: (a) Aerial view of Purdue’s Ross-Ade Stadium. (b) Access
point layout in the stadium. This figure shows 18 of the 20 access
points in the visualization, that are typically accessed by users. Eight
are installed outside (four near the south score board and the other
four on the west side) and the remaining ten are inside the pavilion
on the third, fourth, fifth and sixth floors as shown above.

Stadium during football games as a testbed. Figure 1 shows an
aerial view of Ross-Ade Stadium at Purdue and the eStadium access point (AP) layout in the stadium. Figure 2 shows the wireless
coverage area of all the APs in the stadium, some of which overlap. Wireless network access information is collected through these
APs. We use these network logs, along with the related video and
text data, to display various visualizations synchronized with time.
Through the combined visualization and analysis of video and text
data, we can gain insight into network performance and congestion, crowd analysis, and emergent social behavior. This work provides a proof of concept for mobile sensor analytics that will help
to improve network performance and provide safety personnel with
information for better emergency planning and guidance.
Our paper is organized as follows: Section 2 discusses previous
related work. Section 3 gives a brief description of the eStadium
testbed. Section 4 discusses in detail, the visualization system we
implemented along with the characteristics of the input data. Section 5 discusses how the visualization capabilities in our system aid
in various analytic tasks. Section 6 presents some analysis and observations made by both visualization experts and system experts
using our system for game data obtained in November 2005. Finally, section 7 discusses some possible extensions for mobile sensor analytics by combining data from various other types of sensors.
2 R ELATED W ORK
Visual analytics research is a key focus at several research centers
nationwide [10], and we build upon techniques developed at these
centers in this paper. Most notably is the National Visualization
and Analytics Center (NVAC) at Pacific Northwest National Lab
(PNNL). Several projects developed at NVAC are related to our
work. Their InfoStar system [20] has provided some inspiration
for this project, while research by Wong et al. on large-scale graph
visualization [24] and temporal analysis methods [23] has been ap-

84

Figure 2: eStadium access point coverage. Presently access points
on press box and south score board side are installed. Installation of
access points on the north score board is in progress.

plied in the context of mobile visual analytics in this project.
The majority of visual analytics applications for mobile devices comes in the form of either mapping tools [13], sometimes
enhanced with GPS capabilities, or convenience utilities [5, 14].
There have been efforts to use mobile devices in emergency response situations (such as the Measured Response project headed
by the Purdue Synthetic Environment for Analysis and Simulation
(SEAS) [1]), and for location-aware services. However, using a
PDA for visual situational analysis is a fairly new research topic.
Of the visualization techniques employed in this paper, timevarying (temporal) graph visualization plays an important role.
Time varying graph visualization is a common technique for network and relational analysis, for communication network analysis
[11, 15], social network analysis [6, 22, 16], and relational [19] and
geospatial data [12] visualization. With respect to communication
networks, temporal visualization techniques are useful in detecting
traffic anomalies and network intrusion events. In the social network context, time-varying visualization is used to display interpersonal communication activities between individuals, and these
techniques lend themselves readily to the analysis of social trends,
or knowledge propagation between agents. In both areas, visualizing task-specific events and relationships strengthens existing analytical techniques; however, none of these techniques are targeted
for mobile information analytics.
Visualization techniques have previously been used in the context of sporting events to analyze human performance [9, 8] or for
selective content delivery [18, 17]. Television broadcasting companies, such as CBS, employ systems such as VizEngine by Vizrt
[2] in viewing sporting events in real-time, but these systems are
generally for creating customized viewer content, not for detailed
analysis of the respective events.
3

T HE eStadium T ESTBED

eStadium is a long-term, large-scale, collaborative project involving the Center for Wireless Systems & Applications (CWSA), Information Technology at Purdue (ITaP), and Purdue Intercollegiate
Athletics [3]. The goals of the project include:

• Making Purdue’s Ross Ade Stadium the most technologically
advanced in intercollegiate athletics;
• Creating a “Living Lab” for research and education in the design and use of wireless networks;
• Identifying and solving problems in the on-demand delivery
over a wireless network of multi-media applications to football fans’ PDAs, cell phones, or other portable devices.
eStadium is known as a “Living Lab” because its wireless APs
and content/load-balancing servers in the stadium support real users
- football fans on game days. Via their PDAs or Smartphones,
the fans can access a wide variety of football-related infotainment
applications that have been developed by the eStadium VerticallyIntegrated Project (VIP) team [4, 7]. These applications include:
game play-by-play (a list of plays that is updated after each play)
up-to-the-moment game statistics, player and coach bios, and a
search tool for food concessions, stadium facilities, and local hotels and restaurants.
The most challenging application currently supported by eStadium enables fans to view video clips of game highlights [25, 26].
This real-time video distribution system consists of a Location Discovery System (LODS) and an admission control module implemented as a custom plugin under Windows Media Service, which
limits the video traffic per AP basis. The client logging plugin was
enabled when setting up the eStadium streaming publishing point
so that the client-perceived video streaming performance can be
recorded. Both the LODS and admission control are running as
Windows services on the video server. These services store the
client networking activities in the syslog and video streaming details in the streaming video log and dump them into a SQL server
database for later analysis of video streaming and networking performance.
Cisco APs can be configured to send syslog messages to a remote
computer; in our case, they are sent to the video server. The logged
events can be chosen appropriately to help understand the status and
movement of wireless clients. An AP sends a time-stamped syslog
message whenever a client associates, disassociates, reassociates,
authenticates or deauthenticates. These terms/actions, which are
used later in the paper, are defined as follows:
Associated: This event occurs, and a corresponding message is
generated, when a wireless client associates itself with an AP.
Once the association process is complete, the wireless client
can send and receive data packets via that AP.
Disassociated: This event occurs, and a corresponding message is
generated, when a wireless client disconnects itself from the
wireless network and disassociates with the currently associated AP.
Reassociated: When a wireless client changes locations and detects another AP with a stronger signal, it may decide to drop
its association with one AP and start an association with the
new AP. These two steps are reported as one event, called a
reassociation, and only one message is generated.
Authenticated: Before the wireless client can associate with an
AP, it must authenticate itself with the network. Since ITaP
does not require authentication in the eStadium network, we
do not see messages related to these events.
Deauthenticated The AP sends a de-authentication message when
a wireless client has not sent or received any messages for a
certain period of time or when the AP decides that the wireless
device has left its coverage area.

Figure 3: System overview

4

N ETWORK

AND

DATA V ISUALIZATION

ON

M OBILE D E -

VICES

Mobile devices provide immediate and dynamic access to relevant
real-time information in the field. For example, network analysts
may need to make adjustments to load balancing factors, such as
wireless AP power settings and preemptive broadcasting of certain
types of data, as and when such issues present themselves on site.
With the eStadium testbed, we visualize network information
and video and text information from games on mobile devices. Figure 3 shows our visualization system. As shown in the figure, we
can access the database at selected time intervals and visualize the
corresponding data synchronously. We present our data structures,
layouts and primitives of our visualization according to data characteristics, and synchronization of all information in the following
sections.
4.1 Data Characteristics
Data for our system comes from several sources, including data acquired from APs (hereafter referred to as “syslog” messages), video
messages collected by streaming video servers (as mentioned in the
section 3), and a manually-logged “play-by-play” of the sporting
event in progress. Currently, all messages are parsed from historical data in the form of text logs, but the visualization system is designed to present both historical and real-time data. The parsed text
logs of all types are stored on the PDA locally for instant access.
Real-time data access will be provided during Fall 2006 games by
direct access to the live SQL play database.
For our application, we consider syslog messages as a tuple consisting of a time-stamp, a message type (either “association” or
“disassociation”) and the MAC address of the client (connecting)
device. Also, we consider any “Reassociation” message as an “Association” event for our application’s purposes. The video messages
have a similar format and contain additional information related to
streaming video data, including which clip was requested, the number of requests, and other diagnostic information. Play-by-play or
“game” messages consist of seven parts: the quarter in which the
message is from, the game clock time within that quarter, a text description of the event, an optional video clip of the event, the type
of play event (such as touchdown, interception, etc.), a label indicating which team holds the ball currently and the destination of the
ball for the particular event.
In our implementation, we store the syslog and video messages
in a hash table, hashed on device MAC addresses. We then generate a list of messages within a given time interval from this
hash table. All events occurring in the same time interval can be
queried/generated from the hash table using the same query. The
lists generated by such a range query for each different type of

85

multi-modal data can subsequently be traversed and translated to
a relevant visualization.
Because we are working with multi-modal data on different time
scales, we must organize our data carefully so our range queries
will return events in the applicable time range. Our main issue is
that while the syslog and video log data are recorded based on the
real-world clock, the play-by-play data is currently logged based
on the game clock. Therefore, it does not have a real-world timestamp for each event. The closest association we have to remedy
this situation is the first video access event for those game events for
which a video clip is available. Such a remedy allows us to make
an abstract association with real-time video events, but introduces
some level of uncertainty with respect to when the game events
occur.
4.2 Visualization on Mobile Devices
One of the main and obvious concerns for visualization on a PDA
or smartphone is the lack of screen space. This becomes an even
bigger challenge when we consider the limited computational resources on these devices. The main goal of visualization is to display information in a way that makes the user’s cognitive task easier. While all visualization should display relevant data, the limited
display resolution of mobile devices makes the appropriate visual
representation even more crucial. We have to avoid cluttering the
screen, but at the same time, we should also display all the essential
information in an aesthetically pleasing manner. Hence we need to
first extract relevant data at an appropriate level of abstraction, and
then match the data to an appropriate visual representation. In the
case of network data during a football game, we must also take into
account the time variation, and provide synchronized visual display.
The following sections describe how we solve these two problems
in our system.
4.2.1 Information Selection
Our system handles several different data modes including syslog,
video and text. In this case, filtered data not only must convey essential information on its own, but the various modes have to be
correlated in order to convey meaningful information. Here we describe how we filter information for the individual data modes.
Syslog Data: The syslog data contains association and disassociation events of each PDA device in the stadium. For visualization
purposes, we represent an “event” by a set of disassociation and
association activities. For a given time interval, there are numerous such events. Displaying all these events would clutter the PDA
screen and wouldn’t add any useful information. So, we combine
the events between a pair of APs in a particular direction and represent them with a single curve whose thickness is based on the
number of such events. We also have to display the devices associated with each AP. This can be computed by tracking the AP with
which a device has an association message without a corresponding
disassociation message during a time interval.
Video Data: Video logs from the eStadium test bed contain
information about how a client perceives videos during a game.
Again, displaying all the minute statistics might not be of any practical use since it can crowd the screen. An analyst might be interested in looking at the request patterns of videos from various
sections of the stadium. We visualize the number of video accesses
from each AP for each different video.
Game Data: The eStadium group keeps track of play-by-play
events with links to video replays for important events in the game.
The information covered in the game data describes significant happenings during the game (for example, touchdowns or field goals).
Events are logged by quarters, and events with particular significance are marked with a game-clock time-stamp. We display the
text statistics and color code actions (touchdowns, field goals, etc.),

86

from this game log and let a user play related video clips for events
occurring in the current time interval.
4.2.2 Information Display
The visualization system consists of several views, each representing a different type of data. These are the initial view, network
event view, video event view, hybrid view, video download statistics view, most popular videos list, play by play visualization, game
event view and a color legend for different types of plays. These are
linked views and are accessible from the initial view. By linking all
these views to the timeslider interface, a user can switch between
views for the currently selected time interval. Here we describe
each of these views in detail and how they are linked to each other.
Main Screen: The main screen shown in Figure 4(a) shows the
APs (represented as blue dots placed at their approximate physical
locations relative to the stadium). Four of the APs (the rightmost
column) represent the ones that are mounted outside the pavilion,
and the row of four APs at the bottom of the screen are the ones inside the south score board. The remaining 12 APs are inside the stadium pavilion on the third, fourth, fifth and sixth floors. To differentiate these 12 APs, we draw a semi-transparent rectangle around
them. The layout of these APs in the stadium can be seen in Figure
1. Two of the APs on the third and fifth floors aren’t shown in this
figure, because they aren’t typically used. We have a time slider at
the bottom of the screen which can be moved in either direction. It
enables a user to scroll through continuous time intervals and see
the visualization corresponding to the time interval between its current and previous positions. The time range for the slider is set to
the minimum and maximum time-stamps from the log files. There
are eight screen modes for displaying different information and all
of them are synchronized with the time slider. The synchronization
helps a user or an analyst have situational awareness with respect
to various aspects of the game such as network access, video access and important events in the game at the same time. Buttons at
the top of the screen help a user switch between the screen modes.
These screen modes of our system are as follows:
Network Event Screen: This screen shown in Figure 4(b) displays sets of association/disassociation events of devices in the stadium. For each selected event per device (event selected as mentioned in the previous section) we draw a curve with random curvature from the source (AP to which the device was previously connected) to the destination (AP to which the device reconnected).
The direction is indicated by the changing opacity, with the opacity
value increasing from the source to the destination. Colors for the
curves were chosen to distinguish them from other visualization
aspects, and will be used for attribute visualization in the future.
Events from the prior time interval are drawn as semitransparent to
represent past events while also retaining the context. To reduce
visual clutter, we draw curves with thickness proportional to the
amount of activity in a particular direction between a set of APs
Devices actively associated with an AP during a time interval are
represented by transparent squares in a circular distribution around
the corresponding AP.
Video Event Screen: The video event screen in Figure 4(c)
shows how many videos have been accessed from each AP. At each
AP location, circles are drawn with size based on the quantity of
videos accessed. A user can then examine an exploded view (in
the subscreen labeled as “Video Event Subscreen” in Figure 4(c)
on the left) of individual video accesses. It is divided into four
portions (one for each quarter, starting from the top), with each
portion representing videos uploaded for a particular quarter. The
quadrant corresponding to the current game quarter is highlighted
as the user moves across time periods. For example, we can see the
highlighted second quadrant in Figure 4(c). Each interior square indicates a unique video associated with that particular quarter. The
opacity value of each square is proportional to the number of down-

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

Figure 4: Different screen modes of our visualization showing (a) Main Screen: Main screen with APs, time slider and screen buttons. The
row of four APs at the bottom are the south score board APs, the rightmost column represents the APs mounted outside the pavilion and the
remaining are the ones mounted inside the pavilion. (b) Network Event Screen: Association/Disassociation events of the current (blue) and
previous time interval (grey). If there are multiple movements in the same direction between two APs, its exploded view is shown in a blue screen
inset into the main screen. (c) Video Event Screen: Video Access per AP and Individual video access details. Inset, is the text describing the
event corresponding to a selected video from the Video Event SubScreen. (d) Hybrid Screen: Hybrid mode showing both (b) and (c). (e) Video
Statistics Screen: Video download statistics in the form of a stacked bar graph, and text description corresponding to a selected video from the
graph. Color legend is displayed on the left, with abbreviated names for each play type. (f) Popular Video Screen: Three most popular videos
from the current time interval, along with their number of accesses are displayed. Video link plays the associated video and the text description
corresponding to the game event is displayed below the video link. (g) Game Text Screen: Game play-by-play events as text. (h) Play by Play
Visualization Screen: Visualization of play-by-play events in the current time interval, shown as drives for each team.

loads of that particular video. The highest opacity is mapped to the
video with the most downloads. The square color is based on the
class of game event to which the corresponding video is related.
The classes include touchdowns, interceptions, rushes, and passes,
to name a few. This will help an analyst effortlessly correlate video
download patterns to the type of game event.
Hybrid Network and Video Event Screen: This screen displays the combined information from both the views mentioned
above and can be seen in Figure 4(d)
Video Statistics Screen: In this screen (shown in Figure 4(e)),
we display the statistics of video downloads incrementally for each
time interval (as the user steps through the time axis), as a stacked
bar graph. The graph is drawn vertically for lack of screen space.
The graph axis is marked with two types of tick marks. Smaller
ticks on the axis represent individual time intervals and the larger
ticks represent game quarter marks. On this graph, in a given time
interval, there is space for four bars, one for each quarter. Each
bar by itself consists of multiple stacked rectangles one over the
other, with their individual sizes representing the number of videos

downloaded in that time interval and color coded based on the type
of game event. Clicking on any of these rectangles will display the
number of accesses to the video and the corresponding game event
as text within a small window. We also display a color legend, for
the colors used in the bar graph. The play type corresponding to
each color is displayed as text in abbreviated form.
Popular Video Screen: This screen (as shown in Figure 4(f))
displays the list of top three videos downloaded by fans in the current time interval. For each video, it displays a “video” link which
plays it in Windows Media Player. The number of downloads of the
video is displayed beside the video link. The text description of the
corresponding game event appears below the video link.
Game Text Screen: This is a text display showing the important
events in the game logged by the eStadium team in the selected time
interval. This view can be seen in Figure 4(g). It shows a subset of
events from the current time interval.
Play by Play Visualization Screen: This is a functionality that
we added to improve fans’ game experience. As shown in Figure 4(h), it displays important play by play events in the game

87

using various visualization entities. Each play is displayed based
on the type of action such as interception, field goal, pass, etc. A
“Rush” action is displayed using an arrow in the direction of the
rush. “Pass”, “Kick” and “Punt” actions are displayed using curves
and appropriate icons for each event. Similarly, we have icons to
represent events like “Penalty”, “Incomplete Pass”, etc. where the
ball isn’t moved by the player. We represent the play events in
the current time interval as continuous drives starting from the left
moving to the right for each drive. A user can click on any of the
icons or arrows to view the selected game event screen displaying
the text description of the corresponding play.
Exploded Network Event Screen: This is a subscreen inset
into the main screen. As mentioned before, each network association/disassociation event is drawn as a single curve with thickness
proportional to the amount of activity. So, each curve with number
of events greater than one, is drawn with a small square on it. A
user can click this square to view the detailed view. This screen
shows the exploded view on the right side of the screen as can be
seen in Figure 4(b). Within this screen, the specific APs are drawn
maintaining their relative orientation and position with respect to
each other so that it is easy for the user to correlate between this
screen and the main screen. The screen can be closed by clicking
on the top right corner.
Selected Game Event Screen: A user can see the game event
corresponding to a video by clicking on such an item and pop up a
screen at the bottom of the screen (just above the time slider) as can
be seen in Figure 4(c) and (e).
5

V ISUAL A NALYTICS I SSUES

Visual analytics typically deals with multi-modal data which require specific visualization techniques for each modality. This section explains some of these issues and their solutions in our visualization system.
Temporal Visualization: Data for visual analytics is usually
time varying. Specifically, in our case, network data is collected
over the entire duration of the football game, and can be streamed
during the game. Our system includes a time slider, which allows
a user to move back and forth in time and view the visualization
related to that time interval. All modes of data (syslog, video and
text data) are synchronized with the time slider. In order not to lose
continuity between subsequent time intervals, we also “ghost” the
events from the previous time interval in a faded color.
Additionally, as stated earlier, time synchronization and correlation issues must be considered with multi-modal datasets. With
respect to this work, only two of the three data sources have global
clock information, while the other dataset only has time-stamps relative to the game clock. Subsequently, efforts must be made to link
these events to the datasets ordered in global time. This introduces
some measure of uncertainty, which we must be aware of in the
analytical process.
Information Clustering: As mentioned before, a big challenge
for mobile visual analytics is the limited screen space. To overcome this limitation, information has to be visualized selectively
and related information has to be grouped together. A user should
be able to then view details at multiple resolutions when desired
(detail on demand). In our system, we employ information filtering
as mentioned in 4.2.1. Multiple events between a set of APs in a
particular direction are grouped together using a single curve with
proportional thickness. A user can then see its exploded or detailed
view in the Exploded Network Event screen (shown in Figure 4(b)).
Similarly, all video access events per AP are shown as a single circle. A user can then view the detailed video access information for
any selected AP. A user can view even more detailed information
about the game event corresponding to any particular video in the
Selected Game Event Screen (shown in Figure 4(c) and (e)).
Multi-modal Data Visualization: Analysts usually have to

88

work with data from multiple sources in various formats and correlate them. Using our system, the user can visualize syslog, video
and text data linked with each other while keeping the context of
the current time interval constant. Since all these modes of data are
interlinked, a user can switch between any of these from any screen.
Analytic Tasks: The eStadium team plans to use our prototype
system during Fall 2006 football games for various analytic tasks.
One of the main issues at hand is with respect to load balancing.
Our system will be used to identify network bottlenecks for the
APs, as well as for streaming video. The usage patterns of association and disassociation can help coverage/capacity planning, QoS
provisioning and support location-based services. Power level settings of the APs will also be adjusted based on these in-field observations. The power level settings for the APs in the stadium are important parameters that should be tuned to maximize the available
channel capacity. When many APs are operating in the same area,
there is a tradeoff between coverage and interference. Determining
the power settings that yield the best combination of coverage and
quality of service is a significant challenge. Moreover, though network load balancing is adjusted automatically, an analyst needs to
be able to determine if the load balancing algorithm is performing
correctly on site.
Also, from the video log visualization, one can immediately estimate the source location for the video requests and also the type of
events for these videos based on the color coding (touchdown, interception, etc.). Recognizing such patterns can help the network administrators decide to preemptively broadcast or multicast to these
locations when certain events happen. The popular video display
also helps a game fan go back and effortlessly look at the most popular video replays in case he missed a part of the game.
6

I MPLEMENTATION

AND

R ESULTS

We have implemented and tested the system on a Dell Axim x51v
PDA with a screen resolution of 240 x 320, but it will run on any
PDA or a smartphone with sufficient graphics and processing capabilities. Photos of our system running on a PDA are shown in
Figure 6. We use the OpenGL ES API for development. It is a
low-level, lightweight API for advanced embedded graphics and is
a subset of the traditional desktop version of OpenGL API. A significant change in OpenGL ES is that all the drawing calls are done
using vertex arrays and color arrays, since glBegin() and glEnd()
calls are absent. We also use GLES common lite profile which uses
fixed arithmetic instead of floating point computation, since floating
point arithmetic is emulated on the PDA.
The data for visualization is obtained from the eStadium, LODS
and streaming video databases. Currently, we do not have real time
data since it is off season for football. We query the previously mentioned database to obtain historical data from last year’s PurdueIllinois game on November 12, 2005.
The graphics environment is set up as a 2D orthographic projection since the visualization entities are all 2D. All of the buttons used to switch between various screen modes are drawn using graphics calls and the text is rendered using texture-mapped
bitmap fonts. Stylus input is handled by the mouse function of the
GLUTES library. Video playback functionality is handled simply
with a call to the external Windows media player.
Using the visualization, we are able to do a simultaneous observation of which were the APs with heavy video requests (from the
video event screen), which individual videos among them had the
highest hits (detailed video event sub screen), number of hits and
what was the related game event (from the Selected Game Event
screen).
The results from our system can be seen in Figure 4 for different timesteps and visualization screen modes. Based on an overall
analysis for this game data, we observe that there are many association/disassociation events, as well as video downloads from the four

(a)

(b)

(c)

(d)

Figure 5: Visualization at different time intervals for the 11/12/2005 Purdue-Illinois game. All of these show a high activity around the four APs
outside the pavilion and the third floor APs. (a) also displays the text description of the play-by-play event corresponding to the video shown as
a dark blue square. This interception video has been the most often downloaded video during this game. (b) shows that even though the time
slider is in third quarter, at that time, people are downloading video from the first quarter (shown as blue square). (c) shows another popular
downloaded video shown as dark purple square with its text description. (d) also shows how people are still downloading popular videos from
previous quarters.

APs outside the pavilion and none to or from the four APs near the
south score board (Figure 5(a), (b), (c) and (d)). The circle around
the AP which is currently selected to view the detailed video access
screen is highlighted in blue to differentiate it from the others. In
Figure 5(b) and (d), we can also observe that fans typically tend to
access important play videos from previous quarters as the game
progresses.
Based on a more specific analysis, we observe the following behavior:
The Network Event Screen (Figure 4(b)) shows that most switching between APs seems to be “tit-for-tat” in that it is likely the same
mobile device is associating back and forth between APs. This is
generally caused by a poorly designed driver on the client device,
indicating that we need to perform some further analysis to determine if the jumping is occuring from our set of PDA’s or from outside user devices.
The Video Event Screen (Figure 4(c)) shows that the vast majority of video accesses are coming from the northern, third-floor
region of the pavilion. This is an indication that the power level
settings of the APs should be reduced in that area while at the same
time restricting the client association rates in order to serve more
clients with the desired quality of service.
The Hybrid Network and Video Event screen (Figure 4(d))
shows that most of the AP hopping is occurring in the regions of
the network where the highest number of video downloads are occurring. It is unclear at this point how much of this phenomena is
related directly to the amount of traffic at an AP, and how much
is caused by the tautological fact that usually the only APs with
devices connected are the ones that are downloading video.
Also, the visual analytics of the syslog data made it clear that
simply matching each association with a disassociation is inadequate to estimate session count and length. This is because the associations and disassociations are out of order and not quantifiably
matched. This spurred research into the cause of these errors:
• the syslog messages are sent using UDP, meaning that messages will arrive at the syslog out-of-order, and some could be
lost
• reassociations after a timeout period seem to cause the majority of the discrepancies between the number of associations
and disassociations

We are currently investigating a more complex method of analysis
to better understand the syslog events.
The Video Statistics Screen (Figure 4(e)) makes it clear that the
vast majority of video download requests are for a very small subset of videos (in this case, interceptions and touchdowns), although
the time of the request is not always predictable. This indicates that
multi-casting these heavily-accessed videos should significantly increase the number of clients we are able to serve simultaneously.
7

C ONCLUSION

AND

F UTURE W ORK

We have shown in this paper how we can develop an interactive
visualization system for analysis of network data. Our system displays time-varying multi-modal data in a synchronized fashion. It
helps an analyst process data simultaneously without losing context while switching between visualization of different modes of
data. Moreover, since it’s developed for a mobile device, a user can
use it in real time on a football stadium or anywhere else in field for
immediate analysis and respond if necessary.
For example, we discovered that some APs were totally free of
any association/disassociation, while during certain time intervals
there was marked activity between a particular set of APs. Observations such as these will be of use to our network analysts in
determining usage patterns and parameter settings of APs.
User and network statistics are generally very useful in network
troubleshooting. Plans to integrate specific network characteristics,
such as link quality/packet loss, users per AP details, and throughput information, are in progress. Also, additional exploratory abilities within our current visualization of AP data would be of service in determining link outages and identifying wireless access
patterns.
As future work, the integration of temperature, acoustic, and
video sensor data in the Fall 2006 will allow us to detect several
characteristics that are currently unavailable. Using such data, we
can provide estimates on where crowds tend to populate in the stadium and provide location-specific services based on crowd density.
Also the integration of emergency response data based on sensor information will be added. By using crowd density measurements, we
could provide crowd guidance information to emergency response
personnel, such as optimal exit paths. Moreover, we can extend our
system to identify fire hot spots using temperature sensors within

89

(a)

(b)
Figure 6: Photos of our system running on PDA

the stadium and prepare fire-fighters appropriately based on that
data.
ACKNOWLEDGEMENTS
We wish to thank the reviewers for their suggestions and helpful
comments. We also thank the National Visualization and Analytics
Center (NVAC) for their support. This work has been supported
by the US National Science Foundation under grants NSF ACI0328984, NSF ACI-0081581, and NSF ACI-0121288, by the Air
Force Research Lab under grant FA8650-05-2-6648, by the Motorola Foundation, and by Cisco.
R EFERENCES
[1] [Synthetic Environment for Analysis and Simulation] http://
www.mgmt.purdue.edu/centers/perc/html/.
[2] [Visrt is the worlds leading provider of real-time 2D and 3D broadcast
graphics] http://www.vizrt.com.
[3] [e-Stadium - the Wireless Football Experience] http:
//estadium.purdue.edu/.
[4] [e-Stadium: A Vertically Integrated Project] http://dynamo.
ecn.purdue.edu/vip/∼estadium.
[5] Benjamin B. Bederson, Aaron Clamage, Mary P. Czerwinski, and
George G. Robertson. A fisheye calendar interface for pdas: providing
overviews for small displays. In CHI ’03: CHI ’03 extended abstracts
on Human factors in computing systems, pages 618–619. ACM Press,
2003.
[6] Danah Boyd. Social network fragments: An interactive tool for exploring digital social connections. In SIGGRAPH Sketches 2003,
2003.
[7] Edward J. Coyle, Jan P. Allebach, and J. Garton Krueger. The
vertically-integrated projects VIP program in ECE at Purdue: Fully
integrating undergraduate education and graduate research. In Proceedings of the 2006 ASEE Annual Conference and Exposition, 2006.

90

[8] Pingali G.S., Opalach A., Jean Y., and Carlbom I. Visualization of
sports using motion trajectories: Providing insights into performance,
style, and strategy. In IEEE Visualization 2001. IEEE Computer Society, 2001.
[9] Pingali G.S., Jean Y., and Carlbom I. Lucent vision: A system for
enhanced sports viewing. In Third International Conference in Visual
Information Systems, 1999.
[10] Thomas J. and Cook K., editors. Illuminating the Path: The Research
and Development Agenda for Visual Analytics. IEEE Press, 2005.
[11] Goodall J.R., Lutters W.G., Rheingans P., and Komlodi A. Preserving
the big picture: Visual network traffic analysis with tnv. In InfoVIS
’05: Proceedings of the 2005 IEEE Visualization for Computer Security. IEEE Computer Society, 2005.
[12] Thomas Kapler and William Wright. Geotime information visualization. In IEEE Symposium on Information Visualization, 2004. IEEE
Computer Society, 2004.
[13] Chittaro L. Visualizing information on mobile devices. IEEE Computer, 39(3).
[14] Masood Masoodian and Daryl Budd. Visualization of travel itinerary
information on pdas. In CRPIT ’04: Proceedings of the fifth conference on Australasian user interface, pages 65–71. Australian Computer Society, Inc., 2004.
[15] Chris Muelder, Kwan-Liu Ma, and Tony Bartoletti. A visualization
methodology for characterization of network scans. In VizSEC, page 4,
2005.
[16] Gloor P., Laubacher R., Zhao Y., and S. Dynes. Temporal visualization and analysis of social networks. In Proceedings of the North
American Association for Computational, Social and Organizational
Science Conference, 2004.
[17] Weng P., Cai R., and Yang S. Contextual browsing for highlights in
sports video. In 2004 IEEE International Conference on Multimedia
and Expo (ICME 2004), 2004.
[18] Arthur Pope, Rakesh Kumar, Harpreet Sawhney, and Charles Wan.
Video abstraction: Summarizing video content for retrieval and visualization. In Proc. Thirty-Second Asilomar Conference on Signals,
Systems and Computers, 1998.
[19] Havre S., Hetzler E., Whitney P., and Nowell L. Themeriver: Visualizing thematic changes in large document collections. IEEE Transactions on Visualization and Computer Graphics, 8(1), 2002.
[20] Antonio Sanfilippo, Richard May, Gary Danielson, Bob Baddeley,
Rick Riensche, Skip Kirby, Sharon Collins, Susan Thornton, Kenneth
Washington, Matt Schrager, Jamie Van Randwyk, Bob Borchers, and
Doug Gatchell. An adaptive visual analytics platform for mobile devices. In SC ’05: Proceedings of the 2005 ACM/IEEE conference on
Supercomputing, page 74. IEEE Computer Society, 2005.
[21] Jim Thomas. Visual Analytics Challenges and Future Research Areas.
In Workshop on Visual Analytics.
[22] Brandes U., Fleischer D., and Lerner J. Highlighting conflict dynamics in event data. In IEEE Symposium on Information Visualization,
2005. IEEE Computer Society, 2005.
[23] Pak Chung Wong, Wendy Cowley, Harlan Foote, Elizabeth Jurrus,
and Jim Thomas. Visualizing sequential patterns for text mining. In
IEEE Symposium on Information Visualization, 2000. IEEE Computer
Society, 2000.
[24] Pak Chung Wong, Patrick Mackey, Ken Perrine, James Eagan, Harlan Foote, and Jim Thomas. Dynamic visualization of graphs with
extended labels. In Proceedings of the 2005 IEEE Symposium on Information Visualization (INFOVIS05). IEEE Computer Society, 2005.
[25] Xuan Zhong, Hoi-Ho Chan, Timothy J. Rogers, Catherine P. Rosenberg, and Edward J. Coyle. The development and eStadium testbeds
for research and development of wireless services for large-scale
sports venues. In Proceedings of TridentCom 2006, 2006.
[26] Xuan Zhong, Hoi-Ho Chan, Timothy J. Rogers, Catherine P. Rosenberg, and Edward J. Coyle. eStadium - the ”Living Lab”. In Proceedings of Infocom 2006, 2006.

