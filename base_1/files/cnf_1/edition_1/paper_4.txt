Visual Analytic Roadblocks for Novice Investigators
Bum chul Kwon

Brian Fisher

Ji Soo Yi

Purdue University

Simon Fraser University

Purdue University

ABSTRACT
We have observed increasing interest in visual analytics tools and
their applications in investigative analysis. Despite the growing
interest and substantial studies regarding the topic, understanding
the major roadblocks of using such tools from novice users’ perspectives is still limited. Therefore, we attempted to identify such
“visual analytic roadblocks” for novice users in an investigative
analysis scenario. To achieve this goal, we reviewed the existing
models, theories, and frameworks that could explain the cognitive
processes of human-visualization interaction in investigative analysis. Then, we conducted a qualitative experiment with six novice
participants, using a slightly modified version of pair analytics,
and analyzed the results through the open-coding method. As a
result, we came up with four visual analytic roadblocks and explained these roadblocks using existing cognitive models and
theories. We also provided design suggestions to overcome these
roadblocks.
KEYWORDS: Visual analytics, investigative analysis, cognitive
model, framework, roadblock, qualitative experiment.
INDEX TERMS: H.1.2 [Models and Principles]: Human Information Processing J.4 [Social and Behavioral Sciences]: Psychology - Experimentation
1

INTRODUCTION

We have recently seen increasing interest in visual analytics tools
and their applications in investigative analysis. Recognizing the
effects of visual analytics tools and visualizations in various intellectual activities, researchers have attempted to understand the
potential benefits of such systems in investigative analysis [1].
Many visual analytics systems, such as Jigsaw [2] and INSPIRE
[3], have been developed and evaluated to provide analysts with
insight-gaining platforms. In addition, many scholars have proposed theories, frameworks, and models to understand the interaction between human and visualization systems. Such studies proposed interaction models [4], [5], analytic activity models [1], [6],
and cognitive models [7-10]. There was also a qualitative study
[11].
Despite a plethora of such studies, we still do not know what
kinds of roadblocks novice users face while interacting with visual analytics tools to solve difficult problems in investigative analysis. In particular, investigative analysis, such as terrorism prevention, requires developing hypotheses, performing sensemaking tasks, and building stories, which is beyond identifying
trends in graphical representations [1]. Thus, it is difficult to grasp
what kinds of problems novice analysts struggle with when using
visual analytics tools for their analytic activities, which we define
“visual analytic roadblocks” (henceforth roadblocks).
e-mail: kwonb@purdue.edu, bfisher@sfu.ca, yij@purdue.edu

IEEE Symposium on Visual Analytics Science and Technology
October 23 - 28, Providence, RI, USA
978-1-4673-0014-8/11/$26.00 ©2011 IEEE

Therefore, we were motivated to investigate this uncharted area.
We would like to answer the following research questions in this
study:
•
•
•

What is the cognitive model of interaction between human
users and visual analytics tools in the context of investigative
analysis?
What are the cognitive roadblocks when people conduct
investigative analysis using a visual analytics tool?
What are the proper solutions to reduce/remove the roadblocks?

To answer these questions, we reviewed the theories, frameworks, and models that explain the cognitive process of interaction between human users and visual analytic tools in the context
of investigative analysis (see Section 2). Then, we designed a
qualitative experiment to find out the roadblocks of using a visual
analytics tool, Jigsaw, in investigative analysis (see Section 3). In
particular, we studied how human subjects use Jigsaw in a fictitious investigative analysis scenario. To analyze the cognitive
process in the investigative analysis setting, we engaged in deep
conversation with the participants during the experiment. We
analyzed the recorded conversation along with screen activities,
created a list of roadblocks, and compared them against relevant
models (see Section 4). Based on the results, we attempted to
substantiate the human cognitive models of interaction between
human users and visual analytics tools in the context of investigative analysis (see Section 5). In this article, we review our procedures and results; then, we discuss the implications of our findings
and discuss the limitations and future works (see Sections 6).
2

RELATED WORKS

2.1
Cognitive Models of Visual Analytics
We found two models that describe cognitive processes of investigative analysts. One is the sense-making loop developed by Pirolli and Card [12]. Their model introduces two major loops: an
information-foraging loop and a sense-making loop. The final
outcome, a story, is derived from the iterative analysis across both
loops in this model. Green et al. [10] focused on higher-level tasks
of reasoning process, such as decision-making and problem solving, and discussed how interface designs of information visualization systems should be made to facilitate the process. Though
these two models helped us understand the cognitive process of
investigative analysts using visual analytics tools, they do not
cover roadblocks in the process. Thus, we expanded the scope of
literature review, not being limited to investigative analysis, but
including general cognitive processes in human-visualization
interaction.
2.2
Potential Visual Analytic Roadblocks
Many models, theories, and frameworks have been proposed to
explain the interaction between users and visualization techniques.
Through them, we could collect a list of potential roadblocks.
Roadblocks may be found in low-level interactions. Yi et al. [4]
investigated the role of interaction techniques in information visualization. One interesting aspect of their study is the emphasis on

3

users’ intents. They used users’ intents to categorize various interaction techniques, which shows that there is a common set of
intents that users would like to achieve. Thus, when users’ intents
fail to be realized by given interactions, users may be frustrated.
Lam [5] proposed a framework of interaction costs in information
visualization. Her framework includes seven categories of interaction costs that impact the user interface and information visualization use. The costs articulated in her study, such as decision costs
and visual-cluttering costs, could be potential roadblocks for users. However, these models tend to focus on low-level interactions
between users and visualization systems, but not on the users’
high-level intelligence activities.
The study of Amar and Stasko [6] may provide notion of highlevel cognition. They focused on the analytic gap, which refers to
troubles encountered by users while using visualization systems to
perform analytic activities such as decision-making and learning.
In this study, they suggest two different kinds of gaps: the Rationale Gap and the Worldview Gap. The Rationale Gap is “the
gap between perceiving a relationship and actually being able to
explain confidence in that relationship and the usefulness of that
relationship” (p. 114); the Worldview Gap is “the gap between
what is being shown and what actually needs to be shown to draw
a straightforward representational conclusion for making a decision” (p. 114). These gaps could prevent users from conducting
analytic tasks properly while using visual analytics tools and,
thus, can be identified as roadblocks. The study is useful to understand the users’ challenges though it was not fully dedicated to the
context of investigative analysis.
Additional roadblocks may exist in the interaction between internal visualization and external visualization. Liu et al. [7] proposed distributed cognitions as a theoretical framework for information visualization. They claim that cognition is not a property
of the human mind, but an emergent property of interaction. Using
their framework, they described how humans distribute cognitions
around visual representations and process them to achieve the
users’ goal. This framework indicates that we may be able to find
some roadblocks by observing how visual systems are adjusted or
modified, which actually reflects human cognition. Ziemkiewicz
and Kosara [9] modeled human-visualization interaction as the
shaping of information by visual metaphors. They claimed that
understanding visualization involves the interaction between the
external visual representations and the users’ internal knowledge
representations; when these two representations are in conflict,
user roadblocks may occur. In addition, visual metaphors can
affect the level of understanding visualizations. Therefore, users
consistently interact with visual metaphors using users’ internal
representations while conducting investigative analysis. Thus, a
user can struggle with visualizations in cases where the visual
metaphors used in the visualizations do not accord with the users’
internal knowledge representation. Liu and Stasko [8] suggested
the mental model-based reasoning in information visualization.
They examine the process of interplay between internal representations and external representations. Their results demonstrate that
interaction in information visualization systems could be interpreted as the following: external anchoring, information foraging,
and cognitive offloading. Therefore, investigative analysis may
proceed under the tight interplay between the user’s mental model
and external visualizations. In other words, users may meet roadblocks when the mental model conflicts with the visual analytics
tools.
In addition to the roadblocks identified in mental models, we
also can identify roadblocks in the process of selecting visualizations for given tasks. Grammel et al. [11] studied the challenges
novice users had while attempting to construct visualization while
exploring data sets. The major roadblocks—barriers in their
term—they found were “translating questions into data attributes,

4

designing visual mappings, and interpreting the visualizations” (p.
947). Although what they found is relevant and similar to the
purpose of our study, the study was not conducted in the context
of investigative analysis. In addition, they intentionally did not
allow their participants to directly interact with the tool, but we
believe that direction observation of user interaction would be
essential to understand roadblocks. Thus, to extend their findings
in the investigative analysis setting, we felt that additional empirical study in the context of investigative analysis is needed.
2.3
Qualitative Research Methods
We surveyed a number of research methods before settling on a
variation of pair analytics. Protocol analysis, often called “thinkaloud” method, is sometimes suggested in visual analytics studies
[13]. Protocol analysis reveals participants’ thinking process by
verbal reports given while performing specific tasks. However, a
major disadvantage of this method is that verbal reporting could
affect the performance of tasks and vice versa [13]. There is an
alternative protocol, called insight-based study [14], which addresses this issue. The process requires users to keep a journal that
records insights gained from using information visualizations to
their routine analysis works over a period of time. Although this
process is less intrusive than protocol analysis, it can miss some
issues that can emerge from the cognitive process of using information visualizations. As an alternative, Arias-Hernandez et al.
[15] proposed the pair analytics method to study the interaction
and cognition in visual analytics. The pair analytics method requires pairing a dyad of participants, one subject matter expert
(SME) and one visual analytics expert (VAE), and to generate
dialogue between two while performing a visual analytics task
together. This method can elicit the cognitive process of individuals in a more natural and less intrusive way than protocol analysis.
3

EXPERIMENTAL METHODS

3.1
Participants
Six university students (5 male) were recruited for this study
through fliers and a mailing list. All of them were engineering
students (two of them majoring in industrial engineering, one in
electrical computer engineering, one in biomedical engineering,
and two in undecided engineering). They were compensated $20
for 2-hours of participation. Although we could not recruit actual
investigative analysts, we explicitly described the study goals and
provided a simulated context as an investigative analyst to our
participants as Kang et al. did [1]. Thus, we believe that our participants were fully motivated and reasonably performed investigative analysis tasks. Since we are also particularly interested in
the hurdles encountered by novice users, we believe that university students are reasonable alternative population for professional
investigative analysts for our research goal. We will discuss the
areas that we want to investigate further with experienced investigative analysts in Section 6.
3.2
Equipment
A desktop computer with Microsoft Windows XP, a microphone,
and two 19" LCD displays with standard peripheral devices (a
keyboard and a mouse) were used for this study. Participants were
asked to use Jigsaw [2] (see Section 3.5 for more details) to conduct investigative analysis. A screen capturing software called
Camtasia [16] was also used to record user activities on both displays as well as the dialogue between participant and experimenter. After experiments were done, a qualitative data analysis software, called “ATLAS.ti” [17], was also used to code the recorded
screen activities and dialogues.

Figure 1: The ten views of Jigsaw used in this study. (Top row) Document, List, Document Cluster, Graph, Document Grid; (Bottom row) Calendar, Timeline, WordTree, Scatterplot, Circular Graph. Figures are captured from (http://www.cc.gatech.edu/gvu/ii/jigsaw/views.html).

3.3
Procedure
When a participant arrived to the usability lab, an experimenter
introduced the purpose, the procedure, and the benefits of this
experiment to participants. They were also informed the voluntary
nature of the experiment and signed the consent form.
Then, the experimenter informed the participant of how to use
the system. The features of each view of Jigsaw were thoroughly
explained and demonstrated based on the manual of Jigsaw 0.3.
To avoid the variation in instruction, we read a scripted instruction to participants in the order that was used in the manual as
appeared in Figure 1. Then, the participant was allowed to explore
each view for 5 minutes, but most of them ended up using much
less than the allotted time. After experiencing each view, the participant rated the perceived difficulty of each view in 5-point Likert scale (1: Easy to use, self-explanatory, 2: Clear, possible to
use, 3: Somewhat clear, hard to use, 4: Not clear, hard to use, and
5: Totally incomprehensible). We collected the difficulty ratings
to see the relationship between the perceived difficulty at the introduction session and the performance during the experiment. It
took about 30 minutes to finish this introduction session. At the
end of introduction, we asked participants about their first impressions of the views.
After the introduction session, each participant was asked to
accomplish three mini tasks and five investigative analytic tasks
listed in Section 3.4. While participants complete the tasks, the
participant’s interaction on the computer screen and the participant’s comments were recorded. We employed a slightly modified
version of pair analytics method [15]. In the original version of
pair analytics, a dyad of participants, one subject matter expert
(SME) and one visual analytics expert (VAE), performed a task
together while generating dialogue. However, through our pilot
studies, we found that roadblocks were more apparent when we
allowed a participant to directly interact with Jigsaw while an
experimenter passively guided the participant through how to use
the tool. Thus, we decided to let an experimenter provide guidance only when participants requested it. The leading author takes
the role of the experimenter for all participants to maintain consistency. During the experiment, the experimenter actively listened to the concerns and problems as well as findings from their
analysis and clarified the points made by them. The experimenter
also initiated dialogue in case that participants elicited frustrations
in words (e.g., participants murmured, “I think I am stuck.”) or in
actions (e.g., participants shook their head).
After the experiment session, the participant was asked to fill
out a simple demographic survey questionnaire. The whole experiment took approximately two hours. The recorded screen shots

and discourse were subsequently transcribed and coded by two
independent coders, which is detailed in Section 3.6.
3.4
Experimental Scenario and Tasks
We used the same scenario used by Kang et al. [1]. This scenario
includes 50 fictitious agency reports, each of which describes a
terrorist event with time, places, and people involved. With the 50
reports, the participant was asked to perform three mini-tasks and
five investigative tasks.
Mini-tasks were designed to provide simple and easy questions
that a participant can answer easily without in-depth understanding of the given scenario. We provided these mini-tasks to observe participants’ basic understanding of Jigsaw.
•
•
•

T1: Name the largest social network (Network: a group of
organizations or people are connected).
T2: Name the smallest social network.
T3: How many reports mention “Los Angeles?”

Upon completion of mini-tasks, participants were asked to perform relatively longer but guided investigative tasks. We did not
want to make these tasks too complicated, so we created five serial tasks so that participants could only focus on one major terrorist
plot by following these five related tasks. In particular, we provided the keyword “U-Haul” to make the investigative analysis even
easier.
•
•
•
•
•

T4: Please conduct a short analysis of the following names
and indicate if any of them are possible suspect(s). Provide
supporting evidence. Three names were given.
T5: Is there a way to tell if U-Haul were being used in the
suspicious activity? If so, how many U-Haul were rented?
What were the destinations for each U-Haul?
T6: Who (people and organizations) rented and drove UHaul?
T7: What was U-Haul being used for?
T8: Reveal the full story from the beginning to the end.
Name all the key personnel and organizations involved in
this suspicious activity and what you think is being planned.

3.5
Experimental Software
As previously mentioned, we used Jigsaw 0.3 [2] for our experiment. We chose Jigsaw because it provides 11 different kinds of
visualizations: Document, List, Document Cluster, Graph, Document Grid, Calendar, Timeline, WordTree, Scatterplot, Circular
Graph, and Shoebox (renamed as Tablet in Jigsaw 0.4) Views,
where we can examine different roadblocks caused by different

5

visualizations. We expected that some of visualizations were more
difficult to understand than others. Among the 11 views available
in Jigsaw, we did not use the Shoebox View because this view
would allow too much freedom to build participants’ own visualizations. It could take more than two hours for a participant to
grasp its value. However, we recognize its value in a long-term
usage, so we will consider studying its role when longitudinal
studies are conducted.
3.6
Coding Procedure and Analysis
Two independent coders, the leading author and an external expert, coded video clips together. A video clip is a unit that refers
to a meaningful, mutually exclusive segment of video excerpted
from one entire video file per participant. We focused on finding
evidence that reveal the moments that participants encountered
roadblocks or were troubled. We first searched for the moments
that participants struggled the most. However, those moments
were not easily detected because participants often built up frustration over time, so it was often unclear which point, exactly, was
a frustrated moment. Therefore, as an alternative, we searched for
more distinguishable moments that could result from such troubles. After extensive reviews of all video records, we identified
two distinguishable moments: view-switching moments and leaning moments. View-switching moments refer to the moments that
participants switched from one view to another. Leaning moments
are the moments that participants leaned upon the experimenter to
ask for direct or indirect aids on their tasks. Reviewing the entire
video records, we marked those moments as view-switching and
leaning clips, and coded what happened in these clips.
3.6.1
View-Switching Moments
View-switching moments refer to when a participant deactivates
the current view (e.g., minimizing the window, closing the window, and bringing the current window backward by prompting
another window front) and activates another view (e.g., opening a
new view, recovering the window size, and prompting the window front). There was one exception that we did not consider as
view-switching moments. When a participant organized two or
three views in the dual screens to control and read multiple visualizations at once, those views were considered as one view pool.
More specifically, we defined a view pool as a current set of activated, readily viewable views in screens without any substantial
occlusion from one another. Thus, we did not count the moments
that participants switched from one view to another in their view
pools as view-switching moments.
After collecting all view-switching moments into 56 separate
clips, the two coders reviewed the clips and generated codes from
the clips. In this process, we focused on why the participant left a
particular view or a view pool and selected another view or create
a view pool. We observed speech and activity before and after the
view-switching moments. We used a Grounded Theory-based
approach, so we let the codes emerge from the data. Following the
coding strategy in [15], the two coders tried to capture cognitive
and behavioral phenomena by constructing agreed codes. We also
did same for leaning moments in Section 3.6.2. The two coders
coded randomly selected 25% of all view-switching moments (14
clips) and finalized the definitions of the codes together. A coder,
the leading author, coded the rest of 42 clips afterward. Then, the
two coders reviewed the codes and made agreement together. The
process was repeated until codes are stabilized.
3.6.2
Leaning Moments
Leaning moments are moments that a participant directly or indirectly asks for help from the experimenter. We collected the video
clips where the participant asked for help in the experiment. We
collected 33 clips in total. In this process, we thoroughly exam-

6

ined the conversation and the screen activity in those video clips.
Based on the concerns and questions raised by the participant, we
coded what kinds of roadblocks participants went through in the
moment. During the process, we found similar patterns with the
view-switching moments. In addition to the existing codes, we
found some evidence that the preconceived mental model of the
participant impeded the process of adapting to existing views. We
added this code and other miscellaneous troubles and iteratively
coded all clips. After the leading author coded the entire clips, an
external coder thoroughly examined the codes, and the two
reached agreement together.
3.7
Limitations
We acknowledge that the design of this qualitative experiment is
not ideal. The number of participants might not be sufficient to
gain in-depth and comprehensive insights into all the potential
roadblocks. All of participants were university students, so they
are not only novice visual analytics tool users, but also novice
investigative analysts. The lack of expertise in the intelligence
activity might influence the results of this study. We only used
Jigsaw as a tool and let participants perform investigative analysis
with a fictitious scenario. To generalize our findings, we should
employ various tools, scenarios, and contexts. However, despite
these limitations, we believe that this study is an interesting first
step that may raise the awareness of novice investigative analysts,
and the methodological lessons from this study would be interesting to someone in the visual analytics community.
4

RESULTS

4.1
The Introduction Session
We collected and organized the difficulty ratings on the ten views
in Table 1. The average difficulty ratings indicate that participants
perceived all views between “Easy to use, self-explanatory (1)”
and “Somewhat clear, hard to use (3).” However, we also observed that there are individual differences in the ratings. For
example, P2 rated Scatterplot View as 4, but P4 and P5 rated it as
1. P5 rated all views as 1, but others rated different visualizations
in various levels.
Table 1: Difficulty ratings on the ten views of Jigsaw. The ratings
were collected from 5-level Likert scale in the introduction session
(1: Easy to use, self-explanatory, 2: Clear, possible to use, 3:
Somewhat clear, hard to use, 4: Not clear, hard to use, and 5: Totally incomprehensible). The rows are ordered by the average difficulty rating.
Views \ Participants

P1

P2

P3

P4

P5

P6

Avg.

Circular Graph View

1

2

1

1

1

1

1.17

Graph View

2

2

1

1

1

1

1.33

Document View

1

2

2

1

1

1

1.33

Timeline View

1

3

1

1

1

1

1.33

List View

1

3

2

1

1

2

1.67

Document Cluster View

1

1

3

3

1

1

1.67

WordTree View

2

2

1

1

1

3

1.67

Calendar View

1

3

3

3

1

2

2.17

Document Grid View

2

4

3

2

1

2

2.33

Scatterplot View

3

4

3

1

1

2

2.33

Even though the difficulty ratings were very low in general, we
observed quite a few problems encountered by participants during
the introduction session. Some participants reported that some

views have limited capabilities. Some other participants complained that certain views were difficult to read or understand
because of poor presentations and lack of visual cues. On the other hand, some other participants reported that the views were unfamiliar or unclear to them.
Some participants reported that some views were not useful
because those views can only show limited information. P1 said
that the Document Grid View seemed not useful because it only
showed the number of entities in different colors. P2 also said, in
regards to the Scatterplot View, “I mean it makes sense, but it
doesn’t do much. I think other ones you showed [the List View] a
lot better at correlating things.” P1 and P2 said that the WordTree
View could be useful only when they searched words properly. P4
said that the Document Grid and Cluster seemed easy-to-use but
not useful. P5 said the Calendar View shows “less information
than the Timeline View.”
Some other participants felt that visual representations were
difficult to read and interact with due to visual clutter, lack of
visual cues, or other problems in visual elements in certain views.
P2 found the Graph View was more difficult to understand because entities could be clustered. P2 found that it is difficult to
track what kinds of entities were expanded in the Calendar View
because there is no visual cue that shows the history of expansion
and the entities are visualized as a very small item. P4 could not
interpret the Timeline View easily because the stacks do not have
text labels. For the same reason, P4 did not like the Calendar
View. P4 thought the Scatterplot View was overwhelming when
too many entities were drawn simultaneously. P5 thought that the
List View was better than the Graph View because the Graph
View did not provide the immediate connection between entities,
but only through central entities, the documents. P5 also thought
that the Circular Graph is redundant with the Graph View, and
preferred the Graph View because it is more spacious. P5 said
about the Scatterplot View was not useful because of “large spatial separation” between entities in x-axis and y-axis. P6 commented about the List View, “I guess personally this isn’t my type
of tool. Especially, since it can get messy quickly.”
Some participants could not understand how or why they use
some views because the views are unfamiliar or unclear to the
participants. P1 said that the Scatterplot View was somewhat unclear. P3 stated, “I think I am less familiar…[with this view].” P3
also said “I don’t really understand what the Document Grid View
will be used for.” P3 also added, “Again, at this time not immediately clear to me why you would use it… Here, there’s even less
organization than the Document Grid View. So, you are just seeing a cluster of documents.” For similar reasons, P6 disliked the
Document Cluster View because of “how everything is just
thrown into a cluster like this… I think just a simple list view
would be easier to pick out which document you’re looking
for…” P6 also disliked the WordTree View because P6 was “not
really sure how it’s determining to show this string.”

small set of views was used, P5 stated, “Some views are redundant and some others are not relevant to tasks.” P2 and P4 reported that they were not familiar with Jigsaw, so they used what they
could understand and use easily. Especially, P2 used the List
View frequently and P2 stated, “I really like the searched connection because this is how my brain works.” This limited usage patterns are in line with what Grammel et al. [11] reported.

4.2
General Usage Patterns
We observed common usage patterns. The most salient pattern is
that participants used only a subset of the 10 possible views as
their view pools for investigative analysis questions. Participants
tried multiple views at the early stage. We observed many instances of view-switching in mini-tasks (T1 – T3). However, most
participants narrowed down to the List View and two or three
more views for their final view pools after starting the guided
investigative tasks (T4 – T8). Different participants tended to use
different sets of views as well. Besides the List View, P1, P2, and
P3 used the Graph View and the Circular Graph View more often;
P4 primarily used the Document View; P5 and P6 used the Timeline View plus all the above views. No participant opened the
Calendar View. In response to the question about why only a

4.3.2
R2: Failure to execute appropriate interactions
To display information in a view, a participant should execute an
appropriate set of interactions. Participants sometimes chose the
correct view, but could not execute the appropriate interactions to
display the information needed for given tasks. For example, P4
struggled to use the List View while working on T1. P4 said, “I
am trying to figure out a way that I can see all the connections to
everything or at least see them… put in order of connections.” In
order to do this, P4 needed was to press the “Add all” button.
However, P4 failed to find the appropriate action in menu immediately. P6 had the same problem. Additionally, P2 was looking
for entities with the most connections in the List View. P2 misused an interaction because P2 was confused by the term “sort of
frequency.” P2 misinterpreted “sort by frequency” as the amount

4.3

Visual Analytic Roadblocks

4.3.1
R1: Failure to choose appropriate views
It is essential for analysts to choose appropriate views that are
able to represent the information needed to accomplish the task
objectives. However, participants sometimes could not choose
appropriate views that can provide the information they were
looking for. This often led to unnecessary view-switches.
We observed a number of incidents where people struggled to
find the proper views in their verbal reports and screen activities.
When answering T2, P1 was looking at some entities in the Circular Graph View. While struggling in the view, P1 admitted, “I just
feel like having a hard time to choose the best display for this kind
of question.” P1 apparently understood the task objective by the
comment, “I just want to see which ones have the least connection
would be smallest network.” However, P1 could not effectively
use the List View. P2 also ran into a similar roadblock. When
answering T3, P2 did not use the Document View to see which
documents include the right answer; rather P2 stayed on views
like the List View or the Circular Graph View, which mainly
show the relationships between entities. Similarly, P4 also chose
the Document Grid View, which only provides the number of
entities in documents, while answering T1, which requires viewing the relationship of entities.
In some cases, some participants left a certain view even though
the current view was appropriate to answer a particular question.
While answering T1, P6 left the List View and wandered around
with the Circular Graph View and the Graph View. For the same
task, P3 also left the List View and chose the Graph View. While
answering T4, P5 was trying to show relationships between three
given names in the Graph View. P5 could have expanded entities
by double-clicking, but P5 closed this view and chose the List
View.
In other cases, some participants wishfully expected a certain
view could provide the information they want to retrieve. P5 was
trying to find evidence that supports a hypothetical story found
from relationships between entities. P5 opened the Word Tree
View and searched terms. However, P5 could not find sufficient
information from there because the Word Tree View can only
show excerpts that include searched terms. P5 also tried to find a
story from the Timeline View in T8, but the story cannot be revealed until P5 looked up the actual documents in the Document
View later.

7

of connections, but it refers to the frequency of entities in documents. P2, later, could not figure out how to search an entity and
highlight the documents that include the entity in yellow in the
Document Grid View. P3 also had similar problems with “sort by
connection strength.”
4.3.3
R3: Failure to interpret visualizations
Failure to read and understand visualizations often interrupted the
investigative analysis process. Participants sometimes faced difficulty with the perception of visual items by clutters. In some other
cases, they failed to interpret visualizations, so they could not
grasp information represented by visualizations.
Visual clutters in particular views impeded most of our participants. P5 was trying to see entities by expanding a document in
the Graph View. P5 found that there are many entities from one
document. P5 said, "Just from the interface respective, it might be
useful to check occlusions. I gave here... the information is colliding here on the graph that's hard to use.” P5 later made mistakes in
interpreting connections in the Graph View as shown in Figure 2
(top). P5 thought there was a line drawn between two entities, but
there was no line. Because of similar clutters in the List View, P6
miscounted the number of connections of an entity because P6
took some neighboring irrelevant entities into account as well (see
Figure 2 (bottom)). P6 said, “I don’t know why I thought this…
but these (irrelevant items) were connected to different groups…
and I kinda took these account … I should have only taken the
highlighted into account.”

!

ment (so, the answer is four), in the view. However, P2 did not
know that each bar represents a stack of entities in each document.
P3 also stated, “I was trying to get an idea of the Time Line View,
but it’s hard.” P3 interestingly chose the Scatter Plot View with xaxis as time and y-axis as entities. This indicates that what P3
wanted was, indeed, a temporal visualization, but the Timeline
View did not make sense to P3. P6 mistakenly read the number
next to entities in the List View as the number of connections, but
it was frequency.
4.3.4
R4: Failure to match expectations and functionality
Participants’ expectations sometimes do not match with the actual
functionalities of some views, which slowed down the investigative analysis process. We observed some incidents where participants’ expectations of views sometimes did not align with existing
views from Jigsaw. They kept trying to find the best matching
view for themselves, but they gradually adapted to existing views.
Many participants wanted to start the analysis by loading all
entities into views like the Graph View, the Circular Graph View,
the Scatterplot View, and the Timeline view. This is interesting to
us because attempting to see the overview is in line with visual
information seeking mantra, “Overview first, zoom and filter, then
details-on-demand” [18]. Even though it is not possible to do so
without loading each item one-by-one, participants scanned
through menu items to try to perform the overview action. Their
strategy seemed to start with full entities and to remove irrelevant
items in those views (“filter”). This also suggests that participants
wanted to load their memory off to visualizations by removing
irrelevant entities from the view [8].
Some participants had specific expectations about how views
should work. For example, P3 wanted to see the organization with
most connections in the List View. However, when P3 realized
that the List View is not automatically sorted in the order of the
connection frequency, P3 got confused and frustrated. However,
there are individual differences between these expectations. For
example, P6 expected that whenever a new view was opened up,
the new view should be automatically synchronized with other
already-opened views by deriving all the entities from those
views. On the other hand, P1 thought that an interaction with one
view should only affect the view.
One might argue that this roadblock is a superset or a root cause
of the other roadblocks (R1, R2, and R3). However, we decided to
keep this issue as a separate roadblock because we noticed that the
other roadblocks were easily overcome by subtle nudges, but R4
appears to be more persistent through the experiment. We experienced that R4 is not matter of simple misunderstanding (e.g., interpreting “sort by frequency” as “sort by the amount of connections”) but a relatively serious collision between the way in that
participants’ minds work and the way in that Jigsaw works.
4.3.5
Other issues
Other issues are reported and observed:
•

Figure 2: P5 mistakenly thought there was a line between two entities (top). P6 could not see clearly which entities are connected
with which entities because of clutter in the List View (bottom).

Some participants did not understand how to read and interpret
the displayed information in certain views. While answering T3,
P2 could have answered a question easily on the Time Line View
because there were four bars, each of which represents a docu-

8

!

•
•
•

Some participants had difficulties in understanding terms
used in tasks and the scope of a task. We believe that these
difficulties are caused by a lack of expertise in investigative
analysis though this is one of rather salient difficulties that
we observed.
It was interrupting to have to close all the views and reopen
to start a new analysis. P3 wanted to have the ‘clear’ button
to remove all entities in a view.
It is difficult to learn how to use Jigsaw in two hours. P2
could not familiarize with Jigsaw enough to use it for investigative analysis during the experiment.
Investigative analysis was difficult. P5 had difficulty in creating stories from information found in views.

•

The scenario had much information. P1 thought that there
was too much information to follow.

4.4
Special Observations From P5
We gathered interesting observations from P5, who rated all
views with 1 in difficulty ratings and used more diverse views
than others. P5 used special ways to learn the tool during the introduction session. P5 repeatedly commented about the view that
P5 was interacting with, what the displayed information meant,
and how the view generated information. P5 verbalized all the
actions and results in the introduction session. While learning how
to use the List View, P5 said, “Click on a person and obviously
clearly indicates that which organization they belong to in the list
and vice versa.” P5 also tried to understand what the experimenter
explained by repeating P5’s own words. When the experimenter
explained that there was a small pane on the right side, which
showed the entity names in the selected stack, P5 stated, “Yeah, as
you select the different stacks, it changes the view, got it.” P5
maintained this attitude during the introduction session. After
then, P5 did not have any notable issues with R4 from mini-tasks
to guided investigative analysis tasks. Thus, P5 used more views
for analysis than other participants.
5

DISCUSSION

5.1
Roadblocks Characteristics
Four different roadblocks emerged from our analysis: Failure to
choose appropriate view (R1); Failure to execute appropriate interactions (R2); failure to interpret visualizations (R3); and Failure
to match expectations and functionality (R4).
We initially hypothesized that R2 and R3 would be major roadblocks because we introduced the ten new visualizations to participants within half an hour. However, we found that other roadblocks, such as R1 and R4, are also substantial. We observed that
many problems encountered by participants were beyond understanding what visual elements were supposed to mean. Even after
they understood such basic elements, they had difficult time in
combining information on visualizations with stories in their
minds. Designers should consider methods to support users’ abilities to overcome such troubles in the investigative analysis process. One important lesson from this study is that we should not
forget what the visual analytics tool is intended to do: help users
solve problems in investigative analysis. This calls for much more
attention to the study of the actual cognitive tasks that investigative analysts deal with. We should shed light on the full spectrum
of investigative analysis tasks and confirm the results in this study
by revisiting our taxonomy and model in the context.
5.2
Roadblocks and Other Cognitive Models
The roadblocks we unearth here are interesting because various
cognitive studies done in information visualization and visual
analytics already describe the exact same or similar notions.
R1. Lam [5] also identified “Choosing amongst interface option” as an interaction cost. The examples in Lam’s paper are not
exactly in line with R1, but the general description aligns with this
roadblock. “Visual mapping barrier” described by Grammel et al.
[11] is almost in line with this roadblock. They also demonstrate
that participants switch back and forth between multiple views,
which is the exact behaviors we observed in our study.
R2. We found that these roadblocks are more usability issues
than others. Various heuristic evaluation principles, such as “visibility,” are in line with roadblocks [19]. However, we also noticed
that participants have certain intentions for what to do while they
interact with different views, and they got frustrated when views
do not serve their intentions. Yi et al. [4] investigated potential
user intentions, which can be applied directly to roadblocks.

R3. The closest description to this roadblock in literature is the
“World View Gap” in Amar and Stasko [6]. Again, there is a gap
between what people expect and what they see in the visualization, which demonstrates the problem experienced in this roadblock. Another explanation may be problems of “interpreting
visualizations” reported by Grammel et al. [11].
R4: The roles of mental models (internal representation or visual metaphor) have been discussed in Liu and Stasko [8] and empirically shown in Ziemkiewicz and Kosara [9]. Our observation
of mismatch between expectations and functionality is in line with
those findings. We believe that our study helps emphasize the
importance of mental model in analyzing visual analytic roadblocks.
One common theme that we observed is that users have a certain
expectation (or mental model or internal representation) for each
particular view. If the expectation is not matched with actual behaviors of a view, a participant runs into many problems. In some
sense, R1-R4 happened in different phases of investigative analysis in different levels of seriousness, but their root cause might be
common: ill-structured mental model. We observed that participants often had these kinds of mismatches (shown through R1R4) at the beginning stage, and they went through an adaptation
period that eventually made them work with certain sets of views.
We also found that providing a simple tutorial does not necessary
help this issue.
5.3
Individual Differences
Even though we minimized inconsistency among participants in
this study, we observed different behavior patterns among them as
Kang et al. did [1]. More specifically, we thoroughly covered the
features of Jigsaw in the introduction session, and we made sure
that six participants had no previous exposure to visual analytics
tools. However, participants still had different initial mental models and their preferences toward different views. Thus, we could
speculate that other factors, such as cognitive styles and prior
knowledge, may affect the establishment and adaption of a mental
model. In particular, we should examine 1) what kinds of cognitive styles influence the establishment of the mental models, 2)
how we can see the exact picture of one’s mental model, and 3)
how we can help one adapt to existing views or even how visual
analytics tools can adapt to the person.
First, cognitive styles may affect the mental model conflicts.
According to Richardson [20], people can be divided into two
groups: visualizers who rely on imagery for cognitive performance and verbalizers who rely on verbal analytical strategies.
We believe that visual analytics require both types of cognitive
styles to some extents. P5’s case supports this idea. In the introduction session, P5 could successfully learn all visualizations by
translating the visual information into verbal information and vice
versa. Visual analytics require users to derive the task objective,
to apply it into visualizations, and to read the displayed information iteratively. Therefore, the cognitive style of investigative
analysts may need to be more flexible so that they can constantly
integrate one type of information into another.
Second, internal visualization activities may impact the process
of understanding how to use external visualizations for investigative analysis. Internal visualization is the ability to mentally represent objects, events and abstract information [21]. In particular,
spatial visualization ability is known as the ability to comprehend,
encode and mentally manipulate spatial forms [22]. A skilled
viewer automatically forms a link between these visual chunks
and the interpretation of the data [23]. However, participants who
lacked such visualization ability might face difficulty in adapting
to existing visualizations. One can improve internal visualization
ability by experiencing many external visualization techniques

9

[21]. Therefore, the tutorial session like we had in the introduction
session is important.
Third, prior knowledge in the context may also be important to
effective interactions with visual analytics tools. A novice in a
certain field can have difficulty in understanding graphic data on
the subject [24]. Thus, the level of experience in investigative
analysis may play a role in visual analytics. This also suggests that
the tutorial session for visual analytics tools should be done in the
context of investigative analysis.
5.4
Methodological Lessons
We found that a simple survey cannot predict the level of user
understanding. The difficulty ratings for all views in Table 1 indicate that participants thought that they had no problem with understanding how to use the ten views. However, we have seen a
decrease in the number of views in the usage pattern and many
roadblocks in investigative analysis. That is because ‘using a visual analytics tool’ requires more than simply ‘understanding what
representations in visualizations mean.’ We could identify that
every single step in the entire cognitive process requires different
aspects of knowledge and skills of a visual analytics tool. Therefore, a user should know what views are able to provide the information required to complete the task, and the user should be
able to interact with the view.
Instead, we found that a variation of pair analytics and opencoding analysis revealed the deeper cognitive process of using
visualizations, so they can be used in many other visual analytics
studies. We used the modified version of pair analytics to run this
study. We found several advantages of this protocol over the
‘think-aloud’ protocol. Conversation between the experimenter
and participants reveal how users actually think more easily. Since
it is conversation, not like talking to oneself, participants tend to
organize their thoughts more carefully before speaking about
them. With some modifications, this method can also be used to
identify some design-specific roadblocks for visual analytics
tools. Thus, we recommend pairing a participant with an experimenter or another participant.
5.5
Potential Solutions
Based on the results, we established the following design suggestions to overcome roadblocks based on our findings. These suggestions are not meant to provide ultimate solutions but to suggest
initial ideas to promote more discussion.
• R1
•
•

• R2
•

•

10

Create a default view and let participants explore other
views gradually as the Martini Glass Structure suggested [25].
The next views can be recommended by an automated
selection mechanism like the “Show Me” function in
Tableau [26] until participants are able to select the appropriate views with full confidence.
Foremost, the labels of buttons and menu items should
be intuitively designed. One can simply ask a potential
user what would be the most appropriate label for an interaction after the user thoroughly gets through the particular interaction.
An interactive and in-situ help tip could prompt where
participants hover around. For example, when a user
hovers a mouse cursor over the “Add All” button, a
small help tip saying “Show all of the connections in the
People column” could popup around the mouse cursor.
Note that this help tip is tailored for the People column
that the user interacts with, which clarifies what the interaction does.

•

• R3
•
•

• R4
•

•

6

The idea of ScentedWidgets [27] could be utilized. It
provides visual navigation cues, which were created
from the data, so that users can easily explore the data
by following those cues. This similar approach can be
used to provide visual interaction cues instead, which
can result in the most probable and suitable interaction
lists for users.
Many existing techniques (see [28] for a taxonomy of
such techniques) could be implemented to avoid visual
clutter and confusion.
The interactive and in-situ help tip could be useful here
as well. For example, when a user hover a mouse cursor
over a number 20 next to the U-Haul entity, a help tip
shows up and clarifies “U-Haul appears 20 times in all
documents. 20 doesn’t mean that U-Haul has 20 connections with other entities.”
Design suggestion: An interface agent could play a role
of the experimenter in this study to guide and reinforce
the tool’s capability. The mental model can be adapted
to the existing views more easily if the agent can closely
observe, track, and capture the mental model by the history of interaction and task failures and employ treatment immediately.
Tutorial suggestion: Practicing on actual tasks while
speaking about how views work in participant’s own
words could overcome the conflict between the mental
model and the existing views earlier. We can adapt to
P5’s strategy.

CONCLUSIONS

In this study, we investigated the roadblocks that novice users
face while they are conducting investigative analysis using a visual analytics tool. We found several roadblocks that exist in understanding task objective, choosing, using, and reading views
properly. We also proposed a human cognitive model that describes the process of investigative analysis using a visual analytics tool. We also recommend using pair analytics to study more
in-depth cognitive process in using visual analytics tools.
However, to make the cognitive model fully useful for future
studies, we need confirmation from actual investigative analysts.
The main limitation of this study is that we used university students instead of investigative analysts as our experiment participants. As discussed by Pirolli and Card [12], expert analysts have
expertise schemas built from expertise and experience, so that
they can make sense of information and perform actions quickly.
They also collect more relevant information more quickly than
non-experts. University students hired for this experiment might
have faced more analytic challenges, so they could meet unnecessary roadblocks more often. Another limitation is that we only
used Jigsaw for this study, so some of roadblocks may look more
prominent due to the peculiar design of Jigsaw. Even though these
could be limitations in our study, it could also be an opportunity
to compare the results between novice users and investigative
analysts as well as between Jigsaw and other visual analytics systems in future studies.
ACKNOWLEDGEMENTS
This work is partially supported by a seed grant awarded by Visual Analytics for Command, Control, and Interoperability Environments (VACCINE) at Purdue University. We would also like
to acknowledge gratitude to Linda T. Kaastra, who guided and
supported the project throughout the entire process. We also thank
John Stasko, Sung-Hee Kim, and all the reviewers for their excellent reviews that help in shaping the final version of this paper.

REFERENCES
[1]

[2]

[3]
[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

Y. Kang, C. Görg, and J. T. Stasko, “Evaluating visual analytics
systems for investigative analysis: Deriving design principles from
a case study,” presented at the IEEE Symposium on Visual Analytics Science and Technology (VAST), 2009, pp. 139-146.
J. Stasko, C. Görg, Z. Liu, and K. Singhal, “Jigsaw: Supporting
Investigative Analysis through Interactive Visualization,” presented at the IEEE Symposium On Visual Analytics Science And
Technology (VAST), 2007, pp. 131-138.
PNNL, [Online]. Available: http://in-spire.pnl.gov/. [Accessed:
21-Mar-2011].
J. S. Yi, Y. Kang, J. T. Stasko, and J. A. Jacko, “Toward a Deeper
Understanding of the Role of Interaction in Information Visualization,” IEEE Transactions on Visualization and Computer
Graphics, vol. 13, no. 6, pp. 1224-1231, 2007.
H. Lam, “A Framework of Interaction Costs in Information Visualization,” IEEE Transactions on Visualization and Computer
Graphics, vol. 14, no. 6, pp. 1149-1156, 2008.
R. Amar and J. T. Stasko, “A Knowledge Task-Based Framework
for Design and Evaluation of Information Visualizations,” presented at the IEEE Symposium on Information Visualization (InfoVis), 2004, pp. 143-150.
Z. Liu, N. Nersessian, and J. T. Stasko, “Distributed Cognition as
a Theoretical Framework for Information Visualization,” IEEE
Transactions on Visualization and Computer Graphics, vol. 14,
no. 6, pp. 1173-1180, 2008.
Z. Liu and J. T. Stasko, “Mental Models, Visual Reasoning and
Interaction in Information Visualization: A Top-down Perspective,” IEEE Transactions on Visualization and Computer
Graphics, vol. 16, no. 6, pp. 999-1008, 2010.
C. Ziemkiewicz and R. Kosara, “The Shaping of Information by
Visual Metaphors,” IEEE Transactions on Visualization and
Computer Graphics, vol. 14, no. 6, pp. 1269-1276, 2008.
T. M. Green, W. Ribarsky, and B. Fisher, “Visual analytics for
complex concepts using a human cognition model,” presented at
the IEEE Symposium on Visual Analytics Science and Technology (VAST), 2008, pp. 91-98.
L. Grammel, M. Tory, and M. Storey, “How Information Visualization Novices Construct Visualizations,” Visualization and Computer Graphics, IEEE Transactions on, vol. 16, no. 6, pp. 943-952,
2010.
P. Pirolli and S. Card, “The sensemaking process and leverage
points for analyst technology as identified through cognitive task
analysis,” International Conference on Intelligence Analysis, pp.
1-6, 2005.
J. Scholtz, “Beyond Usability: Evaluation Aspects of Visual
Analytic Environments,” presented at the IEEE Symposium on

[14]

[15]

[16]
[17]
[18]

[19]

[20]

[21]

[22]
[23]

[24]

[25]

[26]

[27]

[28]

Visual Analytics Science And Technology (VAST), 2006, pp.
145-150.
P. Saraiya, C. North, and K. Duca, “An evaluation of microarray
visualization tools for biological insight,” in Proceedings of the
IEEE Symposium on Information Visualization, 2004, pp. 1-8.
R. Arias-Hernandez, L. T. Kaastra, T. M. Green, and B. Fisher,
“Pair Analytics: Capturing Reasoning Processes in Collaborative
Visual Analytics,” presented at the Hawaii International Conference on System Sciences (HICSS), 2011, pp. 1-10.
Camtasia, [Online]. Available: http://www.techsmith.com/ camtasia/. [Accessed: 21-Mar-2011].
ATLAS.ti, [Online]. Available: http://www.atlasti.com/. [Accessed: 23-Mar-2011].
B. Shneiderman, “The Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations,” presented at the IEEE
Symposium on Visual Languages, 1996, p. 336.
T. R. G. Green and M. Petre, “Usability Analysis of Visual Programming Environments: A ‘Cognitive Dimensions’ Framework,”
Journal of Visual Languages & Computing, vol. 7, no. 2, pp. 131174, Jun. 1996.
A. Richardson, “Verbalizer-visualizer: A cognitive style dimension.,” Journal of Mental Imagery, vol. 1, no. 1, pp. 109-125,
1977.
M. Hegarty, “Diagrams in the mind and in the world: Relations
between internal and external visualizations,” presented at the Diagrammatic Representation and Inference, 2004, pp. 1-13.
J. B. Carroll, Human cognitive abilities: a survey of factoranalytic studies. Cambridge University Press, 1993.
S. Pinker, “A Theory of Graph Comprehension,” in Artificial
intelligence and the future of testing, Psychology Press, 1990, pp.
73-126.
H. J. M. Tabachneck-Schijf, A. M. Leonardo, and H. A. Simon,
“CaMeRa: A computational model of multiple representations,”
Cognitive Science, vol. 21, no. 3, pp. 305-350.
E. Segel and J. Heer, “Narrative Visualization: Telling Stories
with Data,” Visualization and Computer Graphics, IEEE Transactions on, vol. 16, no. 6, pp. 1139-1148, 2010.
J. Mackinlay, P. Hanrahan, and C. Stolte, “Show Me: Automatic
Presentation for Visual Analysis,” IEEE Transactions on Visualization and Computer Graphics, vol. 13, no. 6, pp. 1137-1144,
2007.
W. Willett, J. Heer, and M. Agrawala, “Scented Widgets: Improving Navigation Cues with Embedded Visualizations,” IEEE
Transactions on Visualization and Computer Graphics, vol. 13,
no. 6, pp. 1129-1136, 2007.
G. Ellis and A. Dix, “A Taxonomy of Clutter Reduction for Information Visualisation,” IEEE Transactions on Visualization and
Computer Graphics, vol. 13, no. 6, pp. 1216-1223, 2007.

11

