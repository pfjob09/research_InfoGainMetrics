DOI: 10.1111/j.1467-8659.2010.01656.x

COMPUTER GRAPHICS

forum

Volume 29 (2010), number 6 pp. 1756–1769

Designing a Highly Immersive Interactive Environment:
The Virtual Mine
L. P. Soares1 , F. Pires2 , R. Varela2 , R. Bastos2 , N. Carvalho2 , F. Gaspar2 and M. S. Dias2,3
1 Tecgraf

- Computer Graphics Technology Group, Pontifical Catholic University of Rio de Janeiro, Brazil
2 ISCTE - Institute of Management, Social Sciences and Technology, Lisbon, Portugal
3 MLDC - Microsoft Language Development Center, Lisbon, Porto Salvo, Portugal
lpsoares@tecgraf.puc-rio.br

Abstract
To achieve a full-scale simulation of a pyrite mine, a highly immersive environment becomes necessary and this
research has led to a complex system enabling users to walk through a virtual mine in real time, presenting all the
behaviours present in such environment. Some of the problems encountered are the tunnels behaviours, including
highly contrasted images due to the presence of the head light, narrow paths, elevators, sound reverberation and
tunnels texture shades. The use of immersive virtual reality enables the generation of high-quality simulations,
because it is possible to control several feedback mechanisms such as the degree of luminance of produced imagery
and spatial sound. In this research, a projection infrastructure and tracking system were specified and developed,
aiming at producing the best results for this kind of simulation. To achieve our purposes, distributed algorithms were
developed to run in a cluster solution that drives a four-sided CAVE-like environment. The complete production
pipeline is presented, ranging from the developed authoring techniques, enabling fast production of new content
for the simulation, to the tracking techniques produced for the improvement of the interaction.
Keywords: immersive environments, multi-projection, 3D multimodal interaction
ACM CCS: Computer Graphics [I.3.7]: Three-Dimensional Graphics and Realism – virtual reality – Computer
Graphics [I.3.2]: Graphic System–distributed/network graphics

1. Introduction
The research project presented in this work was originally
aimed to enable people to virtually visit an old pyrite mine,
which is no longer in use. Several innovations were developed
and implemented to make this virtual visit possible, creating
a realistic scenario with rich human–computer interaction.
The main focus was to simulate the perception of being in a
real mine. After studying several possibilities, a CAVE-like
solution [CNSD93] was chosen for this project, but this was
just the first step. The development of the infrastructure and
all the involved software components was a big challenge
and some already known technologies were revamped and
recycled. Because PC clusters were used to drive the facility, data distribution and multi-projection visualization have
always been issues to address. In this work, we are going to
c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and
Blackwell Publishing Ltd. Published by Blackwell Publishing,
9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main
Street, Malden, MA 02148, USA.

cover topics such as driving of large projection systems, as
well as sound simulation and multi-modal interaction. We
will also discuss the content creation pipeline that feeds the
immersive system with virtual worlds as well as all the specially designed and developed tools for content authoring.
Some of the characteristics of the infrastructure described in
this paper can be highlighted, such as the support to high
stereoscopic resolution in three-dimensional (3D), enabling
the user to be aware of small details of around 1.5 mm and
the synchronization and geometry alignment among screen
images at a sustained rate of 60 Hz, to force an idea of a
continuous cinematic motion. We will address the problem
of tracking the user position to update the correct point of
view at high refresh rates (60 Hz), maintaining a correct
visual feedback and thus avoiding motion sickness effects.
We will also cover the synchronization of 3D sound and the

1756

L. P. Soares et al. / Designing a Highly Immersive Interactive Environment: The Virtual Mine

1757

Figure 2: Layout for the projection walls with a virtual set.
how technologies integrate. Section 6 shows and discusses
the achieved results. Section 7 presents conclusions.
2. Related work

Figure 1: User interacting with the virtual mine simulation.

support of accurate Newton physics rigid body dynamics in
real time, giving a realistic perception of immersion.
A deactivated mine at Lousal in the Alentejo region, in the
south of Portugal, was the place of the development of this
research project, as a part of a Public Life Science Centre.
Mining activities lasted from 1900 to 1988, when finally
the mine was closed [Var07]. The main objective now is
to teach visitors about how was the mining operation at that
time, using immersive virtual environment simulation. Figure
1 presents a user interacting in some scenes of the mine
simulation.
In the next section we will briefly present some related work. In Section 3 we will discuss all the architectural and technological issues dealing with the support of
highly immersive virtual reality system. Section 4 presents
an overview of the software solution developed to drive all
the resources available, pointing out the innovations created
for this project, explaining the advanced acceleration rendering techniques and special visual effects developed for
this project as well as the in-house tools created. Section 5
presents the process to author content for the application and

One well-known solution to drive these immersive solutions
is the VRJuggler [BJH∗ 08]. It offers several tools that simplify the development of application for distributed environment; however, we decided not to use this tool because we
would like to have control of all the system and it would be
difficult to modify VRJuggler because it is a quite complete
and complex tool. It is also possible to note that some other
groups are working on these highly immersive projections
to present some sites. An ancient Greek temple in Messene
is presented in a museum in Greece [CAL∗ 06], the projection system has a similar shape, but they focused on haptic
simulation. Other similar system developed is a hang-gliding
simulation over the city of Rio de Janeiro [SNC∗ 05], in its
many variants it has always being used with a simpler headtracking just with orientation.
3. Achieving Immersive Environments
Multi-projection environments [SJD∗ 08] are quite common
solutions for immersive virtual reality simulation, but there
are several issues that must be addressed to have these systems working properly with tracking mechanisms to guarantee correct point of view, spatial sound and any other sensory
feedback. The solution implemented is a four-sided CAVElike system with the frontal screen wider than the sides in a
parallelepiped volume, as shown in Figure 2. As seen from
the top it consists of six projection planes in a U-shaped layout: two back-projected planes in front with 44% of blending
area (5.6 m × 2.7 m), two front-projected planes on the floor
and one back-projected plane on each side (3.4 m × 2.7 m).
The space configuration allows up to 14 people to attend a
simulation at the same time, including a guide. A physical
platform was built, allowing up to two rows of spectators
where people can stand to get higher position.
The guide controls the simulation through a Wiimote
[WII09] and a microphone. The microphone controls the

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1758

L. P. Soares et al. / Designing a Highly Immersive Interactive Environment: The Virtual Mine

virtual environment by some pre-defined commands through
an automatic speech recognition system, supporting Portuguese and other languages [MLD09].

3.1. Projection
Active stereo has some drawbacks such as infrared light
to synchronize the goggles, problems to keep the batteries
charged for the complete set of glasses and the sickness
caused by the flickering of the shutters. Reviewing the most
modern installations, it is possible to confirm that Infitec
[JF03] is being adopted by the community as stereoscopy
technology with advantages: reduction of the crosstalk between eyes and the adoption of screens that do not need to
maintain light polarization. Infitec works by bandpass filtering the colour spectrum with two complementary filters: one
for the left and one for the right eyes. At our facility, internal
Infitec filters were installed inside the projectors. This is due
to the fact that Infitec filtering depends on the light wavelength and the light beam must reach the filter perpendicularly for optimal quality. The projection technology chosen
for this project was the DLP because it is quite bright and
presents good resolutions. One of the advantages is the fill
factor of about 88%, which avoid the aliasing effect found in
most of LCD systems. Also the technology available has very
low latency, which is very important for the tracking system.
To deal with the edge-blending issue in the frontal and floor
projection, an edge-blending solution developed by Barco
[BAR09], with optical attenuation filter, was chosen, which
presents some advantages as opposed to sharp edges, as the
optical filter gradually attenuates the blending region, and reduces any diffraction caused by the filters borders [CSB02],
thus avoiding colour artefacts (Figure 4).

3.2. User tracking setup and hardware
An infrared user tracking system was developed to enhance
the user experience inside the environment. It was based on
optical tracking techniques and algorithms such [KKR∗ 97]
and [PK07]. We have evaluated state-of-the-art mechanical, inertial, electromagnetic, acoustic, vision-based, optical and GPS-based tracking techniques and assessed their
advantages/disadvantages. The mechanical techniques have
constraints in what concerns the user movements [BOO09].
Current inertial tracking techniques [INE09] are unable to
produce robust and stable tracking estimations ‘per se’, without the aid of external devices such as a compass. The assessed electromagnetic techniques [FAS09] have shown to
suffer from interference in the presence of other magnetic
fields. The acoustic tracking setups [DBS∗ 04] are subject to
sensor errors (phase coherence based) and low update rates
(time of flight based). It is very common to combine methods in hybrid solutions to achieve results precise enough for
practical uses. An infrared optical tracking system presents
some restrictions in what concerns line of sight, ambient

Figure 3: Difference among camera lenses.

Figure 4: Hardware components of the Infrared Camera
system. (a) Colour camera, (b) infrared LED emitter, (c)
shutter controller and (d) artefact.

and infrared radiation, but is able to operate without motion
constraints, drift or error accumulation. Based on these facts
we have decided to invest in developing a robust, precise and
accurate optical tracking solution, which are able to work in
real time. To solve the line of sight problem we have chosen a
setup of four cameras in a controlled environment (total darkness and usually with a room temperature of 21◦ C). These
cameras were positioned in the four top corners of the projection walls. Four firewire cameras [AVT09] were used due to
their resources, with resolution of 640 × 480 pixels @ 25 fps
(the retained configuration) or 320 × 240 pixels @ 100 fps.
This camera array is controlled by an external shutter (trigger) [NI009], to assure frame synchronization. We have also
assessed several lens models to select the optimal overture
angle at best precision, because precision and operational
volume is inversely proportional. Some examples of lens assessment are depicted in Figure 3. The retained solution was
the one with 3.5 mm lens.
Most CCD-based cameras are sensitive to the infrared
spectrum. The attached infrared filter can be replaced by
an infrared bandpass filter. To enhance infrared detection by
light reflection, we have chosen to mount an infrared light
ring emitter in each camera.
Infrared targets or artefacts are a set of markers arranged
in space with known pairwise Euclidean distances in a
pre-defined structure (with known geometry and topology).

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

L. P. Soares et al. / Designing a Highly Immersive Interactive Environment: The Virtual Mine

1759

Figure 5: Audio configuration (subwoofer behind).

Each target is a unique object with a detectable pose within
a tracking volume (spheres, in our case). We have opted by
passive retro-reflective markers instead of active markers because the last ones require an electric source, which would
become more expensive, heavy and intrusive. Our lab-made
markers are built from plastic spheres with 20 mm of radius and covered with retro-reflective self-adhesive paper.
We have chosen spheres because it is a geometric form that
allows an approximation to an isotropic reflection system.

3.3. Spatial sound
Audio simulation has special challenges because the virtual
environment is quite closed. The reverberation of sound in
tunnels is not as common as open areas, and special procedures are needed to give the correct sound impression. In
our installation a 7.1 sound distribution system was installed,
with sound speakers and subwoofer positioned as shown in
Figure 5, in a way that people attending outside the parallelepiped can have a correct sensation of auralization. A 7.1
sound system has advantages, such as the possibility to use
standard audio content and, in addition, specific algorithms
can use the 7.1 geometry together with the available headtracking, creating a complete surround simulation. To keep
a high-quality sound in the environment, the building wall
were covered with an acoustic material to absorb the sound
and eliminate echos.

3.4. Distributed computation for virtual reality
A cluster was designed to drive the solution based on the MS
Windows operating system platform. The cluster includes
six graphic nodes to render the images, one audio/video
node taking care of auralization and camera grabbing, one
parallel computing node for high-performance computing
and storage and one last node for user and network access.
Each graphic computer node was designed to drive two video
channels using two G80 Nvidia GPUs, one of for each eye,
producing good balance between resolution and perfor-

Figure 6: Cluster configuration.
mance. Figure 6 shows a diagram of the cluster setup. A
configuration with 12 graphic nodes would also be possible,
having two graphic cards connect with SLI to make then
a single graphics processor and having the most powerful
GeForce-based cluster solution, but this would increase the
size and complexity of the system and so six nodes were
considered enough for the planed simulation.
The cluster nodes were housed in a 19 in. rack. This decision limited the available options for computer cases, because the ones with support for high-performance graphic
cards are usually only available in workstations, which are
not always rack-mountable. A Keyboard, Video and Mouse
switch (KVM) and a wireless keyboard and mouse were chosen because the setup of this kind of cluster solution requires
constant access. But the use of virtual network computing
(VNC) systems was the best solution to operate the cluster
after it was configured.
4. The Simulation
In order to run the simulation, at least 12 video signals were
needed to drive the 12 projector system of the infrastructure.
At the time of the development of this project, a single PC
was not able to drive more than eight display ports in a single
node. More recently, NVidia [NVI10] introduced the Quadro
Plex solution, which is able to drive up to 16 video outputs in
the D4 version of the product, which is in fact a valid alternative to our approach. In a future project, this solution might
be taken as an option because it simplifies the setup. However, some drawbacks do exist if we are planning to adopt this
other option. This new concentration of graphic resources in a
single node could create some bottlenecks in the simulations
and the hardware (CPU and graphic boards) is considerably
expensive. There are two typical architectural approaches for
distributed computer clusters to drive CAVEs: master/slave
(replicated) and client/server (centralized). In the first, the

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1760

L. P. Soares et al. / Designing a Highly Immersive Interactive Environment: The Virtual Mine

same application replica runs on all nodes. Determinism of
computers assures that—for the same input—the output of
each node is the same. In this topology, one node is defined as
being the master. It has the responsibility of handle the state
changes and broadcast the new state to the other nodes (the
slaves). This approach reduces the amount of information
exchanged between nodes.
In the client/server approach, the application runs in one
computer (client), which handles and processes all the input (which may come from a separate I/O client). The client
application client transmits graphical information to all rendering servers (which can be from pixels to scene graph
semantics). This approach is more flexible and has a lower
upgrade cost. However, low-level information (pixels, graphics primitives) requires high bandwidth and high-level information (highly featured scene graphs) requires complex
protocols. Because the models used in the simulations can
become very complex, the client/server approach can be a
problem because the network bandwidth introduces a limitation in the graphics data transfer rate, which is specially
observed with the increase of the polygon complexity of the
modelled scenes. This limitation causes a noticeable delay
in the simulation, which will lead to a latency perceived by
the user during the tracked operation. On the other hand, the
flexibility of the client/server approach simplifies some algorithms that require less transmission of data than graphic
primitives by the network.
The CAVE of Lousal is a hybrid, which mostly based on
a master/slave architecture but with some client–server features as well. All nodes run a replica of the application, and
the master collects the inputs and distributes them along with
a timestamp, and thus synchronizes the simulation. However, the Newton physics and sound simulation run on a
high-performance CPU node, in client/server mode, to boost
the overall system performance. The computer cluster environment was named CaveH Middleware [PD07]. The overall
features of this solution are: produce high complexity realistic real-time (60 Hz) images in complex 3D scenarios (with
millions of triangles); multi-projection visualization in mono
or stereoscopy; precise synchronization and 3D data consistency among cluster computers; latency and bandwidth control; global and local illumination; cinematic and dynamic
collision detection and response; rigid body dynamic simulation; sound auralization (spatial 3D sound); several 3D formats; character animation (keyframe and mechanical-based);
scripting and immersive virtual environments authoring. A
graphical frontend called CaveHSpawner (Figure 7) was created to ease usability of the CaveH environment. Its interface
includes features for easy cluster configuration, including its
operation in mono or stereoscopic modes, content optimization preprocessing, cluster data content synchronization and
distributed application launch/termination. The CaveH Middleware layer provides two critical services to distributed
virtual environment (VE) applications: (1) the synchronous
provision of data resources to all application replicas, or

Figure 7: CaveHSpawner.

Figure 8: CaveH middleware logical modules.

data-lock, and (2), the synchronous image generation and
frame swapping, or frame-lock. The CaveH Middleware provides also other types of services to distributed VE applications, as depicted in Figure 8.
• OpenSceneGraph [OSG09] is the 3D rendering backend used, it is a robust and optimized open source scene
graph API, with several features that help the development of VR applications, enabling a variety of 3D and
image formats.
• Cal3D [CAL09] is the character animation language.
• Lua [LUA09] is the scripting language for automation.
• Ageia PhysX [AGE09] is a Newton physics rigid and
deformable body simulation, enriching the environment
with basic simulation features such as collision detection
and response, hierarchical soft and rigid bodies simulation under gravity.
• MX-Toolkit [DSB03] is a multi-modal human–computer
interaction module, supporting user tracking, tangible
interfaces (Wii and Nunchuck [WII09]) and speech.
• ADE (Abstract Distributed Engine) [PD07] provides
primitives to ensure data- and frame-lock, supporting
simultaneously master/slave and client/server topologies
in the cluster.

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

L. P. Soares et al. / Designing a Highly Immersive Interactive Environment: The Virtual Mine

• World State manages the avatar pose, the time-stamp
of each consolidated frame of the cluster, the messages
from the input devices (Wii, Nunchuck, Speech, tracking
system), the distributed commands, etc.
• OpenAL [OAL09] is used to create 3D spatial sound
synthesis and distribution (sound auralization) for virtual environments. This library enables the positioning
in 3D of an arbitrary number of sound sources and the
listener, computing the sound signal sent to each speaker
according to their topology.
• MPI is used as a message communication and transport
layer, and also is used for providing distribution synchronization.

1761

• Swap: After being synchronized, all engine nodes propagate updated transformation matrix and polygon mesh
data, through the engine data, to the scene graph.
Frame-lock guarantees that the frames are swapped at the
same time in each node otherwise inconsistencies could appear among screens. Frame-lock occurs after the flushing of
all graphical information and just before the swapping of the
frame buffers. Gen-lock is not really necessary for this kind
of environment as we aim at passive stereo. Gen-lock in this
case could be used to set the black level of the projectors that
supports this control.

4.2. 3D visualization
4.1. Synchronization
The CaveH Middleware is responsible for the simulation
synchronization. This is achieved through the proposed
component-based architecture, the developer can create simulation engines that wrap a part of the simulation logic to
be distributed. Data consistency in the virtual environment
is achieved in two phases: prior to the simulation start (3D
content consistency) and during the simulation (data- and
frame-lock).
Prior to run time, the master monitors a template directory for any change and transfers to the slaves (a daemon
running at each node) whatever files need to be updated, before launching the simulation. It then signals the slaves for
readiness, after which each slave starts up their application
replica. This approach reduces network runtime bottlenecks
because the information is previously ready. The master’s
graphical interface allows the centralized configuration of
each node. The state of the simulation is reflected on the
engine components managed by the master node. Whenever
these components are changed, their state is serialized and
sent to the slave nodes for data consistency. This approach
minimizes the use of network bandwidth and allows the balance of different engines across the cluster. In run time, dataand frame-lock across the cluster are guaranteed by ADE,
which supports operations for initialization, termination and
stepping. The latter is defined, in the ADE library, as having
three phases: refresh, synchronize and swap.
• Refresh: For the master, it first requests all registered engine data to update their state. After this step, it queries
for state change. If changed, the engine data is serialized
and sent to all slaves. For a slave, this phase resumes to
waiting for update messages. The serialization mechanism allows the entities to send any kind of data, such as
transformation matrices polygon meshes, etc.
• Synchronize: At this stage, the master and slaves exchange control communication to ensure that data has
been received by all slaves. This acts as a data synchronization barrier (ensuring data-lock).

A rendering back end was developed based on the OpenSceneGraph [OSG09] library, a modular C++ API, which
can be extended through development of plug-ins. Although
Open-SceneGraph supports simultaneous rendering of the
same scene for multi-GPU systems, the core rendering
mechanism was adapted to support frame-lock after threads
synchronization and multi-projection support in distributed
environments. Our system is targeted for real-time rendering of complex scenes and models in the order of millions
of triangles and, to satisfy this requirement some acceleration techniques where implemented. Among the techniques
developed, Level Of Detail is one that uses several presimplified 3D object replicas, chosen to be rendered by a
simple technique that takes into the account the distance between the viewer and the object. The simplified nodes are
manually created through a tool that uses an edge-collapse
simplification algorithm [HG97]. The cave hollowspace infrastructure has 12 different view volumes (two times, since
we support stereo), when viewing a large model with each
view volume capturing a fraction of the model. To process a
complex model of several million polygons with this setup,
allowing real-time image rendering (60 Hz), we have realized that we need some sort of hierarchical structuring of
the scene graph and of its geometrical nodes. Therefore, we
have developed a visualization acceleration technique that
combines per-object level of detail (LOD), per-object octree
spatial organization, octree organization of the entire scene
and view frustum culling. By applying an octree to the static
parts of entire scene graph, we accelerate the rendering process of these large models by exploiting the fact that when
the object is near enough to be examined, it is most likely
to be spanning across multiple view frustums. The octree
partitions the 3D space by recursively subdividing it into
eight octants, until we reach to a specified side dimension
in world coordinates. Objects with large polygon count are
previously organised (off-line) with a per-object octree and
with level of detail. At culling time, while traversing the octree, the visible leaf objects are retained (using view frustum
culling algorithm), and passed to the draw stage. Some of
these objects may be organized with octree and/or LOD. The

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1762

L. P. Soares et al. / Designing a Highly Immersive Interactive Environment: The Virtual Mine

Figure 9: Motion blur, flashlight and magnifying glass.
Figure 10: Infrared tracking algorithm.
software quickly discards non-visible parts of octree/LOD
organized objects and chooses the right replica taking into
account the camera’s distance to the object. We have developed a method and a tool that produces simpler replicas by
using geometry decimation [Hop96] and another tool that
assembles multiple models (raw models, simplified models
and octree partitioned models) on LOD structures. At content
authoring time, the modeller uses these tools to assemble the
final 3D object content, trading off visual quality and speed.
Moving objects (which move either cinematically or dynamically) are organized in a separate scene graphs group with
per-object LOD.
Special real-time graphical effects (Figure 9) were also
developed to increase the immersive impact on audiences.
Per-Pixel Phong shading with multiple dynamic lights was
implemented on programmable shaders, calculating local
lighting information (such as the pixel normal vector) at the
fragment-processing stage and calculating the illumination
colour directly at each pixel, using the Phong lighting model.
Complex polygon meshes will not appear faceted with the
Mach-band effect, as in standard vertex-based Phong lighting with Gouraud shading. A virtual flashlight controlled by
a Wiimote tangible interface was developed. The Wiimote
pose is directly mapped to the flashlight orientation causing
dark scenes, such as those found in the inside of mines, to be
lighted realistically. The motion blur implemented is used to
highlight rapid moving scenes. Its implementation consists in
a post-processing render stage based on accumulation buffer
method [NDW93]. Lens distortion equation was applied as
a post-processing effect creating a magnifying glass effect,
which simulates a miner’s real magnifying glass used to inspect minerals. To achieved this effect, the complete scene
is rendered to a texture, and then drawn using a screen-sized
QUAD OpenGL primitive. Texture coordinates mapping for
the QUAD use the lens distortion equation to distort and magnify the scene. Pixels outside the magnifying-glass radius are
discarded, so magnification is not applied there. Some other
environmental visual effects such as fog, rain, puddles and
specular highlights were implemented to improve the mine
realism.

4.3. Tracking algorithm and multi-modal interaction
To enable a fully immersive experience, an infrared-based
user tracking system was developed, which is used for

Figure 11: Feature segmentation algorithm.

perspective and pose correction and also for user interaction. This technique recovers the pose [six degree of freedom
(DOF)] of 3D artefacts that have several markers arranged in
a well-defined structure with known geometry and topology.
Different topologies are used to distinguish each artefact.
These artefacts may be tangible user interfaces (such as a
wand) or even a reference for the user’s head pose, which
can be used to perform viewpoint and view frustum adaptation in real time. The tracking algorithm works as follows.
First, an off-line calibration stage to recover the camera’s
intrinsic and extrinsic parameters through well-known calibration techniques [Zha99] is performed. Next, still in offline mode, an artefact calibration step records the Euclidean
distances between each pair of artefact markers and computes a reference pose for comparison at runtime, analogous
to pattern description in texture-based tracking [DB06]. In
online mode, the algorithm starts its multi-thread oriented
pipeline (depicted in Figure 10) by collecting each input
video frame, acquired from the video camera array. We perform a straightforward non-destructive image threshold to
acquire potential blob markers, a technique highly enhanced
by the lighting controlled environment and the infrared filters. To identify the location of blobs and estimate the true
spheres centroid and radius projection, instead of using the
costly Hough Circle Transform (HCT) applied to the whole
image, we designed a feature segmentation algorithm that
comprises four different techniques (Figure 11). To isolate
regions of interest (ROIs) in images, a recursive ‘in-house’
developed algorithm is applied, to search for closed set of
pixels with non-zero value—the Pixel-Connected algorithm.
For each resulting ROI, a bounding box is created and labelled as ‘occluded’ and ‘normal’ by an Occlusion Metric

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

L. P. Soares et al. / Designing a Highly Immersive Interactive Environment: The Virtual Mine

technique developed. When the features are labelled as ‘normal’, we use the Minimum Enclosing Circle Fitting (MECF)
algorithm [FF95] which, given a set of pixel coordinates, the
algorithm retrieves the minimum circle (centre and radius)
that includes all pixels in ROI, minimized by an error function. Although, this technique is not applicable in the case of
occluded features, therefore we use the Hough Circle Transform [KBS75] to cover these cases. At this stage, our major
contribution is the Occlusion Metric developed. Although
several approaches attempt to solve the feature identification
problem in a straightforward way, compromising the speed
[GL99] or robustness [ZZRR97], we propose a feature labelling metric based on an OR logic operation of five rules
tested in 200 different scenarios. A feature is labelled as
occluded when: (1) its bounding box retrieved by pixel connected algorithm has less than 65% of white pixels, (2) the
bounding box has a width and height difference greater than
1.5 pixels, (3) the MECF and ROI radius difference is greater
than 0.9 pixels, (4) MEFC and ROI area difference is greater
than 60 pixels and (5) MEFC circle exceeds the ROI limits
in more than 2.5 pixels.
Although we use MECF to verify rules 4 and 5, this strategy of using HCT depending on the Occlusion Metric results,
allows us saving 10.06 ms per frame with a robustness of
97%, because MECF is faster and accurate but only HCT is
applicable to overlapped features. Having the markers well
identified across the various camera images, the feature correspondence is performed via epipolar geometry [HZ04] constraints. These constraints are easily defined using the known
extrinsic camera parameters from which we can derive the
fundamental matrices of the various stereoscopic pairs. To
perform 3D metric reconstruction of the corresponding markers, which must be visible in at least two different views, we
apply the algorithm proposed in [HZ04] using the Singular
Value Decomposition algorithm [GHG94]. At the stage of
the application of the algorithm, we have the corresponding
3D location of each marker. The next step is model fitting so
that we understand to which artefact the set of reconstructed
3D markers belong, labelling its 3D spatial model and estimating the artefact pose relatively to the calibrated one.
Because artefact markers pairwise Euclidean distance are
known from the calibration, we correlate the reconstructed
points Euclidean distances with the calibrated ones following the paradigm proposed in [PK07], which is finalized
by a graph search algorithm—the Maximum Clique Algorithm [KJ07]. Our model fitting correlation approach differs
from Pintaric’s in several aspects. First, each reconstructed
points has additional information about which point in the
concerned artefact it corresponds to and what are the Euclidean distances between this point and other points, which
show the highest probability of matching with artefact points.
Therefore, we could introduce our second improvement to
eliminate false positives in the 3D reconstructed point to artefact point matching and labelling process: the definition of a
threshold in the number of links with other points that show

1763

Figure 12: Authoring chain for the simulation.

a higher probability of matching. These improvements guarantee a faster model fitting algorithm, avoiding additional
processing time to establish correspondences between the
real-time solution and the model. The result is the correspondence between calibrated model 3D points and runtime 3D
points, which finally allows us to compute each artefact pose
through a 3D–3D least-square estimation method [HJL∗ 89]
with re-projection error minimization.
We are also able to control user actions with the Nintendo’s
Wiimote [WII09], which presents three DOF in acceleration,
two DOF in rotation (pitch, roll), IR sensor for Yaw and push
buttons. We also use the NunChuk, which similarly presents
three DOF in acceleration, two DOF in rotation, additional
selection buttons for applications and one analog joystick.
We can also use the embedded speaker and rumble unit as
an alternative feedback source for the user. We support up
to four devices at the same time, for interaction paradigms
such as controlling the pickaxe and the flashlights used in
the virtual environment. Finally, another possible command
and control interaction model is accomplished using the Microsoft Speech API, which uses the European and Brazilian
Portuguese recognition engines developed by the Microsoft
Language Development Centre (MLDC) [MLD09], as well
as other supported languages. The recognized words can then
be used as input commands to application actions, such as
starting and ending applications, changing user interaction
gadgets, controlling the virtual navigation, etc.

5. Content Authoring
To create the virtual visit to the outside and inside mine
scenarios, an authoring pipeline was devised, including the
integration of several computing resources and stakeholders,
some existing, other developed for our purposes. Figure 12
presents the complete authoring chain to produce this simulation and the relevant stakeholders [PD07]. The virtual mine
environment is fed with 3D content produced in real time by
using procedural methods (e.g. an .osf 3D model file generated by a C++/OpenSceneGraph program) or by a modelling
tool. In this project we have extensively used 3D modelling

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1764

L. P. Soares et al. / Designing a Highly Immersive Interactive Environment: The Virtual Mine

Figure 13: Positioning the user avatar.

tools, such as Autodesk 3ds Max9 [AUT09], which is used to
model the virtual mine and its elements and Maya [AUT09],
our preferred authoring tool for avatars and their animations.
To create the simulation scenery and all its elements (geometry, lights, particle systems, 3D sound sources, special visual effects, animated characters, sensors, behaviours, etc.),
the authoring tool GtkRadiant [GTK09], which allows for
developing and integrating programmable extensions, was
used. This tool enables us to set the initial position of the
avatar/player, 3D scenery (static) objects, moving/dynamic
objects, position of lights and sound sources, define and parameterize various entities and apply to them certain behaviours, which can be triggered by proximity, touch or time
sensors. For in-game behaviours we have developed a small,
yet powerful, trigger/action system. After being activated by
sensors, triggers will execute actions, which correspond to
game behaviours. A large collection of actions was produced
and can be easily extended to support custom ones. Figure 13
shows a user placing the start position of the avatar/player in
the interior of the mine geometry, using orthographic views.
After authoring time, the CaveHSpawner will compile all the
3D static scenery (maps) and dynamic 3D model information. At this compile stage, the system will create the octree
spatial organization data structure for the static parts of the
scenery, and combine it with the pre-generated LODs for the
more complex objects. At this stage, the system will also
handle the collision data associated with the complete virtual scene in its whole and will optimize texture data in an
existing binary model format specially designed for loading efficiency (OpenSceneGraph ive format), allowing high
performance at runtime.
As mentioned, the system also supports pre-recorded
avatar/player animation, by using the CaveHFlyEditor tool
(Figure 14). This tool records the cinematic movement of the
camera: it reads the 3D scene data (static and moving objects) and uses a keyframe-based animation scheme to mark
arbitrary key positions and orientations. It records the position and the orientation of the avatar/player. The animation
author has the ability to edit the animation keyframes later.
3D character animation is supported through the adoption of the Cal3D animation library and the osgCal3D
plug-in for openscenegraph. To animate the 3D avatars, we
have developed yet another tool: the CaveHCharacterEditor

Figure 14: The CaveFlyEditor tool.

Figure 15: The CaveCharacterEditor tool.

(Figure 15). The content author creates a set of animations
in a 3D modelling tool such as 3D Studio Max or Maya
[AUT09], exporting these animations along with skeleton,
mesh, materials and textures to the Cal3d format. The tool
reads this file and allows the game author to create sequences
of actions for the characters. Given these actions the game
author can quickly create ‘human like’ animations and expressions in the 3D game. Also this tools uses the same
engine as the application runtime, so the animation edition
environment is exactly the same as the running environment.
One core feature of the system is the ability of reproduce
realistic mouth animation based on an input speech stream

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

L. P. Soares et al. / Designing a Highly Immersive Interactive Environment: The Virtual Mine

Figure 16: Phone extraction tool.

to achieve high-quality lip sync for the animated characters
in the immersive environment. To reduce the time spent in
this type of facial animation problem, we have developed an
automatic method of viseme playback, or audio to 3D visual
speech transformation process.
At first, the input speech signal is converted to a phonetic
description, so that all phones can be extracted and time
stamped. Microsoft Speech API (SAPI) framework was used
for speech recognition (Figure 16), which allows us to recognize the set of spoken words and phrases of the text script
and the corresponding phonetic pronunciation. Because this
is a command and control speech recognition problem, we
handle it by just generating an XML grammar on demand,
with the text scripts that were recorded, and providing it to
the recognition engine via SAPI, along with the input speech
signal. Upon successful real-time recognition, SAPI returns
the identified list of words with a probabilistic confidence
score and their corresponding phonetic transcription. However, SAPI does not return the timestamp of each identified
phone in the stream. To tackle this, we have partnered with
MLDC to obtain an estimate of each phone average duration
(in ms) of the phone set, by analysing an available large phonetic lexicon and speech corpus of the language in question
(this study has been applied to the European Portuguese).
With this analysis, we are able to estimate the average duration of each phone of the language, and derive a timestamp for
all phones present in the phonetic transcription. We then apply a set of linguistic and computer animation rules, specific
of the selected language, to decrease the number of phones
considered for the synthetic facial animation. At the end of
this stage, we obtain a time sequence of phones from the input speech signal. Each phone corresponds to an animation
frame (a viseme, controlled by a 3D rigging system). From
the time-stamped phonetic transcription of the input speech
signal, we are now able to index, in time, the corresponding
visemes and interpolating the in-betweens, therefore creating
synthetic lipsync for the facial animation of our characters.
The results of ‘automatic lip-sync from speech’ technique,
are depicted in Figure 17.
Finally, the simulation will play with the run-time application developed by the CaveH Middleware. This API
can be easily expanded to new features as desired. One of

1765

Figure 17: Automatic lip-sync animation.
the main ideas during the development was to follow the
Model–View–Controller (MVC) design pattern, to have a
better control of the development of applications.
6. Results
Regarding the projection technology used for our system,
the DLP technology brought the advantage of showing a
small gap between the pixels, minimizing the perception of
screen door effect. The Infitec stereoscopy technique proved
to present a good stereo separation even in a high contrast
scenes. Due to the fact that wall and floor projection surfaces
materials are different, it is possible to notice some lack of
continuity in those corners, due to the difference in specular
reflectance of the projection. The algorithm used to calibrate
colour and brightness [SCAJ07] improved the overall quality
of the projected image. It has to be recalibrated from time to
time, mainly to correct chrominance variations because the
lamps and colour filters age differently. The CaveH Middleware handles scenes with high complexity at real-time rates:
8.3 MPixels at sustained 60 Hz, per eye, and 3D scenes with
3–10 million triangles (throughput of more than 180 million triangles per second and per eye). The observed network
impact is low and the real-time performance requirement is
achieved in this distributed system. Figure 18 presents some
scenes of the final application, simulating the natural landscape of Lousal mine (Figure 19).
The CaveHSpawner tool maps the computer nodes to each
projection plane (Figure 7). The plane layout can be easily
configured and each projection plane can be enabled or disabled: arbitrary projection topologies are therefore supported
from CAD walls to CAVEs. This tool also allows the operator to define which node handles the physics simulation;
the sound output; which 3D scene/map should be launched;
the stereo configuration of the cluster and the inter-ocular
distance of the current user. A content-rich authoring environment was achieved by supporting many industry ‘de
facto’ standard model formats. The system is able to import, both in authoring and in run-time modes, all the file
formats supported by the underlying OpenScenegraph platform. Therefore, it is also possible to mix popular 3D model
formats such as Collada, 3DS, OBJ, OSG and others. The
support middleware was planned so that new behaviours and

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1766

L. P. Soares et al. / Designing a Highly Immersive Interactive Environment: The Virtual Mine

Figure 19: Natural landscape of the old pirite mine of
Lousal, Portugal.

Figure 18: Simulation of the exterior landscape of the old
pirite mine at Lousal.

interactions can become quickly available. The 3D scene
author has a visual tool, where he can position models, enhanced with simulation behaviours, and sensors and triggers
associated to actions, these actions stays in a separated file
and can be easily managed to modify the behaviour of an
object. Regarding the optical tracking system, several tests
were performed to assess the system performance. To measure the precision of the artefact pose retrieval, an artefact
was built and placed in the working volume. Next, the artefact pose was calibrated and 10 000 estimations of the same
pose were performed. The histogram in Figure 20 shows
the deviation of the estimated translation (the Euclidian distance between the calibrated geometric centre of the artefact and the estimated centre). In Figure 21, we present the
overall rotation error (average rotation error over the three
axes X, Y and Z) between the calibrated and the estimated
pose.
To compute the absolute accuracy in pose retrieval over
the six DOF, a similar experiment was performed. This time,
the objective had to take into account the tracking algorithms
errors (feature segmentation error, re-projection error and

Figure 20: Precision deviation in overall translation.

Figure 21: Precision deviation in overall rotation.

reconstruction error and pose estimation error). The experiment worked as follows. First, a ground truth 3D transformation was computed and mathematically applied to the
calibrated points, keeping record of the resulting transformed
points. Next, all 3D transformed points were re-projected in
each camera image space, considering the radius of markers
(20 mm). For these synthetic generated grey-scale images

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

L. P. Soares et al. / Designing a Highly Immersive Interactive Environment: The Virtual Mine

1767

gaming, have experienced the simulation in desktop and in
CaveH environment [BD08]. As a result of these preliminary
experiments, the CaveH system revealed to be better than
desktop, except for colour perception.

7. Conclusions

Figure 22: Overall translation accuracy error in rotation
experiment over Y-axis.

Figure 23: Overall rotation accuracy error in rotation experiment over Y-axis.

with the projected points, the subsequent steps of this
experiment followed the regular system workflow: (1) feature segmentation; (2) 3D reconstruction and (3) model fitting, whose output is a pose, that is a 3D transformation. The
absolute difference between this 3D transformation and the
known ground truth transformation, defines the pose accuracy error. Experiments over each DOF have been performed,
resulting in six different trials: translation experiments across
X, Y and Z axes, and rotation experiments over X, Y and Z
axes. In Figures 22 and 23, we show the observed errors in
overall translation and overall rotation, in the rotation experiment over the Y-axis.
User interaction has been achieved at very low cost. The
chosen tracking system uses the developed optical tracking
algorithm that processes images from four cameras, allowing
us to track two independent six DOF artefacts, in real time
(25 Hz), with mean accuracy of 0.93 mm/0.51◦ and mean
precision of 0.19 mm/0.04◦ , respectively, in overall translation/rotation DOF. The user interacts with the system through
tangible interfaces, such as the Nintendo WiiMote, for scene
navigation and interaction, and by using speech recognition
(made available to the system through the Microsoft SAPI),
for command and control interaction. Five test subjects of
both genders and age range of 20, with prior experience of

Although several open-source tools are available in the
community [VRJ09;OSG09;OPE09], an out-of-the-box solution for this kind of multi-projection system based in
Microsoft operating systems, working correctly and fulfilling all the requirements, is still uncommon specially in the
academia. In this research we have overcome several challenges, and it is clear now that this area is still much open
for new developments to increase realism and performance.
Experimental evidence shows the robustness of our developed solution: the system is being used for about 2 years
and we can confirm that from the projection and computing
cluster hardware to the run-time software, everything works
as expected. The content creation pipeline process is still
a complex topic in the VR community. Due to the lack of
available solutions that match our requirements, we have had
to develop specific tools for some of the phases of content
creation, as described in this paper. We have used an industry
standard for game level creation (GtkRadiant), as a content
authoring platform, and we believe that we should further investigate the game industry processes and tools to find ideas
and concepts to further improve the game authoring and simulation and enhance the immersive VR experience. It is also
clear that with our proposed framework artists can produce
refined models and they will be correctly displayed even with
high complexity. Although we have achieved sufficient precision and accuracy (average values) in the ‘developed from
scratch’ optical tracking system, by observing the accuracy
results, is evident that improvements should be addressed
to minimize the maximum error in pose estimation. During
the experiments performed, we have discovered that severe
errors happen when the Hough Circle Transform algorithm
fails the detection of two overlapped features. Although such
cases only represent 0.80% of the tested samples, we aim
at improving our feature segmentation algorithm with alternative more accurate and robust techniques. Future research
ideas pass through circle fitting algorithms such as the Minimum Enclosing Circle Fitting properly improved to cope
with overlapped features, predicting algorithms (Kalman filtering) to apply in pose estimation or even alternative approaches to Hough Circle Transform.

Acknowledgments
The project was developed by many organizations with public and private funds, namely by ISCTE and its R&D centre
ADETTI, IST and its R&D centre INESC-ID, FCUL, both
at Lisbon-Portugal and PUC-Rio in Brazil, with the support
of Eurographics Portuguese Chapter. This solution is part of

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1768

L. P. Soares et al. / Designing a Highly Immersive Interactive Environment: The Virtual Mine

the Centro de Ciˆencia Viva do Lousal from the Portuguese
Ministry of Science, Technology and Higher Education,
sponsored by the Fundac¸a˜ o Frederic Velge, an association
of SAPEC, with the Grˆandola City Hall.

[DSB03] DIAS J. M. S., SANTOS P., BASTOS R.: Gesturing with
tangible interfaces for mixed reality. In Gesture Workshop
2003 (2003), pp. 399–408.
[FAS09] Fastrak. http://www.polhemus.com, 2009.
[FF95] FITZGIBBON A. W., FISCHER R.: A buyer’s guide to
conic fitting. In Proc. of the 5th British Machine Vision
Conference (Birmingham, 1995).

References
[AGE09] Physx. http://www.nvidia.com, 2009.
[AUT09] Autodesk. http://autodesk.com, 2009.
[AVT09] Allied vision technologies
http://www.alliedvisiontec.com, 2009.

pike

camera.

[BAR09] Barco. http://www.barco.com, 2009.
[BD08] BASTOS P., DIAS M. S.: Experiencia de realidade virtual imersiva no ambiente cavehollowspace do
´
lousal. In Proc. Interacao 2008 (Evora,
Portugal, 2008),
pp. 117–126.
[BJH∗ 08] BIERBAUM A., JUST C., HARTLING P., MEINERT K.,
BAKER A., CRUZ-NEIRA C.: Vr juggler: A virtual platform
for virtual reality application development. In SIGGRAPH
Asia ’08: ACM SIGGRAPH ASIA 2008 courses (New
York, NY, USA, 2008), ACM, pp. 1–8.
[BOO09] Fakespace boom. http://mechdyne.com, 2009.
∗

[CAL 06] CHRISTOU C., ANGUS C., LOSCOS C., DETTORI A.,
ROUSSOU M.: A versatile large-scale multimodal vr system for cultural heritage visualization. In VRST ’06: Proceedings of the ACM Symposium on Virtual Reality Software and Technology (New York, NY, USA, 2006), ACM,
pp. 133–140.
[CAL09] Character animation library. http://home.gna.org/
cal3d, 2009.
[CNSD93] CRUZ-NEIRA C., SANDIN D. J., DEFANTI T. A.:
Surround-screen projection-based virtual reality: The design and implementation of the cave. In SIGGRAPH
’93: Proceedings of the 20th annual conference on Computer graphics and interactive techniques (New York, NY,
USA, 1993), ACM, pp. 135–142.
[CSB02] CLODFELTER R., SADLER D., BLONDELLE J.: Large
High Resolution Display Systems via Tiling of Projectors.
Technical Report, Barco, 2002.

[GHG94] GOLUB G. H., VAN LOAN, C.: Matrix Computations, Second Edition. Johns Hopkins University Press,
1994.
[GL99] GOULERMAS J. Y., LIATSIS P.: Incorporating gradient
estimations in a circle-finding probabilistic hough transform. Pattern Anal. Appl., 2, 3 (1999), 239–250.
[GTK09] Gtkradiant. http://qeradiant.com, 2009.
[HG97] HECKBERT P. S., GARLAND M.: Survey of polygonal
surface simplification algorithms. In Proc. SIGGRAPH 97
(LA, USA, 1997).
[HJL∗ 89] HARALICK R. M., JOO H., LEE C. N., ZHUANG X.,
VAIDYA V. G., KIM M. B.: Pose estimation from corresponding point data. In IEEE Transactions on Systems
Man and Cybernetics (bla, 1989).
[Hop96] HOPPE H.: Progressive meshes. In SIGGRAPH ’96:
Proceedings of the 23rd annual conference on Computer graphics and interactive techniques (New York, NY,
USA, 1996), ACM, pp. 99–108.
[HZ04] HARTLEY R. I., ZISSERMAN A.: Multiple View Geometry in Computer Vision, Second Edition. Cambridge
University Press, 2004.
[INE09] Intersense inertia cube3. http://www.i-glassesstore.
com, 2009.
[JF03] JORKE H., FRITZ M.: Infitec—A new stereoscopic visualisation tool by wavelength multiplex imaging. In Proceedings Electronic Displays (Wiesbaden, 2003).
[KBS75] KIMME C., BALLARD D. H., SKLANSKY J.: Finding
circles by an array of accumulators. In Communications
of the Association for Computing Machinery 18 (New
York, 1975).

[DB06] DIAS J. M. S., BASTOS R.: X3m – an optimized
marker tracking system. In Proc. 12th Eurographics Symposium on Virtual Environments (Lisbon, 2006).

[KJ07] KONC J., JANEZˇ ICˇ D.: An improved branch and
bound algorithm for the maximum clique problem. In
MATCH Communications in Mathematical and in Computer Chemistry 58 (Serbia, 2007).

[DBS∗ 04] DIAS J. M. S., BASTOS R., SANTOS P., MONTEIRO L.,
CANHOTO J.: The arena: An indoor mixed reality space. In
Proc. Interacc¸a˜ o 2004 (Lisbon, 2004).

[KKR∗ 97] KOLLER D., KLINKER G., ROSE E., WHITAKER D. B.
R., TUCERYAN M.: Real-time vision-based camera tracking
for augmented reality applications. In Proceedings of the

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

L. P. Soares et al. / Designing a Highly Immersive Interactive Environment: The Virtual Mine

1769

ACM Symposium on Virtual Reality Software and Technology (Lausanne, Switzerland, 1997).

Virtual Environments Workshop at the IEEE VR 07 (Charlotte, NC, USA, 2007).

[LUA09] The programming language lua. http://www.
lua.org, 2009.

[SCAJ07] SOARES L. P., COSTA R. J., ARAUJO B. R., JORGE
J. A.: Automatic color calibration for commodity multiprojection display walls. In Proceedings of the IX Symposium of Virtual Reality (2007).

[MLD09] Microsoft language development
http://www.microsoft.com/portugal/mldc, 2009.

center.

[NDW93] NEIDER J., DAVIS T., WOO M.: OpenGL Programming Guide. Addison-Wesley, 1993.
[NI009] National instruments usb-6501 shutter. http://www.
ni.com, 2009.
[NVI10] Nvidia quadroplex. http://www.nvidia.com/page/
quadroplex.html, 2010.
[OAL09] Openal library. http://connect.creativelabs.com/
openal, 2009.
[OPE09] Opensg. http://www.opensg.org, 2009.
[OSG09] Openscenegraph.
org, 2009.

http://www.openscenegraph.

[PD07] PIRES F., DIAS M. S.: Abstract distributed engine
(ade): A library for data consistency in distributed virtual
environments. In Proceedings of 15◦ EPCG (Porto Salvo,
Portugal, October 2007).
[PK07] PINTARIC T., KAUFMANN H.: Affordable infraredoptical pose-tracking for virtual and augmented reality.
In Proceedings of the Trends and Issues in Tracking for

[SJD∗ 08] SOARES L., JORGE J., DIAS M., RAPOSO A., ARAUJO
B., BASTOS R.: Designing multi-projector VR systems:
From bits to bolts. Tutorial Eurographics 08, Greece, April
2008.
[SNC∗ 05] SOARES L. P., NOMURA L., CABRAL M. C.,
NAGAMURA M., LOPES R. D., ZUFFO M. K.: Virtual hanggliding over rio de janeiro. In SIGGRAPH ’05: ACM SIGGRAPH 2005 Emerging technologies (New York, NY,
USA, 2005), ACM, p. 29.
[Var07] VARELA T.: Modelos de Minas do S´eculo XIX Mostra
de engenhos de explorac¸a˜ o. Lousal, 2007.
[VRJ09] Vrjuggler. http://www.vrjuggler.org, 2009.
[WII09] Wii. http://www.nintendo.com/wii, 2009.
[Zha99] ZHANG Z.: Flexible camera calibration by viewing
a plane from unknown orientations. In Proceedings of the
International Conference on Computer Vision (ICCV’99)
(1999).
[ZZRR97] ZHANG Z., ZHANG Z., ROBOTIQUE P., ROBOTVIS P.:
Parameter estimation techniques: A tutorial with application to conic fitting. Image and Vision Computing, 15
(1997), 59–76.

c 2010 The Authors
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

