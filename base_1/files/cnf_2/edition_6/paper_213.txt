DOI: 10.1111/j.1467-8659.2010.01804.x
Pacific Graphics 2010
P. Alliez, K. Bala, and K. Zhou
(Guest Editors)

Volume 29 (2010), Number 7

Manifold-Based 3D Face Caricature Generation with
Individualized Facial Feature Extraction
S. F. Wang1 and S. H. Lai1
1 Department

of Computer Science, National Tsing Hua University, Taiwan

Abstract
Caricature is an interesting art to express exaggerated views of different persons and things through drawing.
The face caricature is popular and widely used for different applications. To do this, we have to properly extract
unique/specialized features of a person’s face. A person’s facial feature not only depends on his/her natural appearance, but also the associated expression style. Therefore, we would like to extract the neutural facial features
and personal expression style for different applicaions. In this paper, we represent the 3D neutral face models
in BU-3DFE database by sparse signal decomposition in the training phase. With this decomposition, the sparse
training data can be used for robust linear subspace modeling of public faces. For an input 3D face model, we fit
the model and decompose the 3D model geometry into a neutral face and the expression deformation separately.
The neutral geomertry can be further decomposed into public face and individualized facial feature. We exaggerate the facial features and the expressions by estimating the probability on the corresponding manifold. The public
face, the exaggerated facial features and the exaggerated expression are combined to synthesize a 3D caricature
for a 3D face model. The proposed algorithm is automatic and can effectively extract the individualized facial
features from an input 3D face model to create 3D face caricature.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism—Virtual reality

1. Introduction
3D human face modeling is a very popular topic for many
applications, such as facial animation, synthesis and modelbased facial video communication, etc. With proper face feature extraction, the creation of 3D caricature becomes easy
and feasible for practical use. Previous works on 3D face
caricature mainly rely on manual efforts or simple linearbased methods to exaggerate facial features. In addition, it
is difficult to automatically exaggerate a 3D face model with
expression since facial expression induces 3D face model
deformation in a complex manner. The main challenge is the
coupling of the personal facial features on neutral 3D face
model and the 3D deformation due to expression, thus making the question of 3D model caricature with facial expression very challenging. In this paper, we propose a 3D face
caricature generation system , which consists of the modc 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

eling of neutral facial features, the sparse representation of
public 3D face geometry and appearance, and expressional
deformation analysis. The proposed algorithm integrates the
robust linear and non-linear subspace representations for a
prior 3D face model, probabilistic manifold-based modeling
of personal facial features and 3D expressional deformation.
1.1. Related work
With the improvement of computer techniques and the evolution of intelligent devices, more and more researchers
work on the exploration of intelligent artworks. Generally
speaking, caricature can be considered as a pictorial description or drawing of something or someone by exaggerating
the main features and simplifying other parts. In this paper,
we focus on the personal facial features extraction and expression modeling for 3D human face caricature generation.

2162

S.F. Wang & S.H. Lai / Manifold-Based 3D Face Caricature Generation

Difference

[ WL08 ]

Figure 1: The framework of our 3D face caricature generation system. The training phase includes public face modeling,
neutral facial features and facial expression manifold analysis. The input can either be a single image or a 3D face model and
the output result is a 3D face caricature that exaggerates the facial features and expression.
For two-dimensional example-based approaches proposed in [LCXS02, MLN04], they learned an exaggeration
model from a pre-labeled examples. To further deform the
input face image, they used the active shape model to extract
the face shape and classify the shape into one of the exaggeration prototypes. Chen et al. [CqXyS∗ 01] proposed to learn
a sketch model from the examples created by an artist, and
generate the cartoon face. Liu et al [LCG06] proposed to
learn a model which maps the original face and the caricature to a PCA subspace, and transform the input face image
into a caricature with this model. However, the application
of these methods is limited to the two-dimensional image
representation of human faces.
Similar to the 2D based methods, researchers [AR04]
[XCL∗ 09] developed interactive tools to make 3D caricature by user defined facial features and replace the facial features with suitable caricature components. In their
works, the 3D modeling process is quite labor-intensive and
time-consuming. Therefore, there have been research efforts
on achieving automatic 3D caricature generation, e.g. PICASSO [KMK97, FNT∗ 99, FKF∗ 01, XCL∗ 09]. The series
of works exaggerate the differences between the input 3D
model and the average 3D model. The basic idea is under the
assumption that the average face stands for the face without
any feature that can be easily recognized. In the work of Li
et al. [LCLF08], non-linear learning technique is also used
to train the relationship between the face photographs and
the corresponding 3D caricatures.
For a caricature generation system, the more important issue would be the extraction of personal facial features and
the feature representation. The previous works show evidence that the machine learning techniques provide possible solutions for the modeling of non-linear facial deformation. Recently, non-linear embedding methods such
as ISOMAP [TdSL00], locally linear embedding [RS00],

Laplacian Eigenmaps [BN01] and global coordinate of local linear method [RSH01] were proposed to handle highdimensional non-linear data, which may be more appropriate to model the facial deformation. Generally, continuous
facial deformations in images represent a smooth manifold
in the high-dimensional space and the intrinsic dimension
may be much smaller. Based on this argument, researchers
have developed manifold-based learning methods for facial
expressions and applied them to 2D face expression recognition or synthesis [CHT03, YCT04, HCFT04]. Since the
manifold analysis can effectively represent the continuous
deformation due to facial expression, it can be used as a
prior to transfer expression to a 3D face model by using the
unified manifold space analysis [WHL∗ 04]. In Leyvand’s
work [LCODL06], they beautify the face images by utilizing manifold learning technique. The manifold-based techniques were also used in interactive 3D caricature generation [XCL∗ 09], 3D expressive face model reconstruction
and retargeting [WL08]. Since the non-linear modeling of
facial expression deformation can be represented by manifold techniques, the personal facial feature could also be
represented by manifold methods.
When seeing a face image, our perception observes and
compares the input face with all the faces saved in the
brain. The face components that are very different from
other people’s are considered as the personal facial features
used for later recognition. We also have the experience that
we cannot recognize a person’s face because the extraction
of the feature is difficult and we call these faces Public
Faces. Public Faces tend to have the property of featureless which can be represented as the average of all training
faces [KMK97, FNT∗ 99, FKF∗ 01, XCL∗ 09]. We observed
that not only the average face, but also the faces in the space
formed by faces with common features are lack of distrinct
features. We also observed that features of a face come from
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2163

S.F. Wang & S.H. Lai / Manifold-Based 3D Face Caricature Generation
MDS

LLE

Laplacian

Isomap

0.06
0.25
200

1000

0.04

0.2

800
0.02

0.15

100

600

0.1
0

0
400

0.05
−0.02

200

0
−100

−0.05
−0.1

−200

−0.15
−300

−0.04

0

−0.06

−200

−0.08

−400

−0.2
−200

0

200

400

−0.2

0

0.2

0.4

−0.1

−0.05

0

−600
−2000

0.05

−1000

0

1000

Figure 2: The manifold analysis for personal facial features by different methods, including (a) Multidimensional scaling,
(b) Locally Linear Embedding, (c) Laplacian Eigenmaps and (d) Isomap. The red circle denotes the embedded coordinate of
additional featureless zeros field eL0 which helps us to determine the exaggerating direction.
the nature features on the neutral face and the special expression style on the expressive face. Therefore, we decompose the geometry of a 3D neutral face model into featureless part (public face) and neutral feature part. The modeling
of the public face, neutral face features and expressional deformation are separated and used in the proposed caricature
generation system.
A global view of our training and reconstruction process is
shown in Fig.1. The modeling of 3D facial features will be
described in the following section, including sparse representation of public face and manifold analysis of facial features. In section 3, we briefly review the expression deformation modeling which was originally proposed in [WL08]
and present our modifications for the application to caricature generation in this paper. In section 4, we describe how
to generate the 3D face caricature based on the expression
manifold modeling. The input data to the 3D caricature generation system can be a single 2D face image or a 3D face
model. We demonstrate the effectiveness of the proposed algorithm through experiments on real images and scanned 3D
face models in section 5. The final section concludes this paper.
2. Training: The Modeling of 3D Facial Features
In order to establish the statistical model for neutral faces
and analyze the motion of facial expressions, correspondences between 3D models must be established. In this
work, we use the 3D face scans and 2D face images from
BU-3DFE database [YWS∗ 06] as the training faces. The 3D
scans and 2D images from the neutral faces in BU-3DFE
are registered to the generic model and re-sampled for the
following training. Model-based statistical techniques have
been widely used for robust 3D human face modeling. The
standard training phase of the traditional methods first collect a lot of training data and then apply statistical methods,
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

e.g. PCA, for feature extraction. Take PICASSO for example, they assume that the average 3D model is the featureless
one and any other face that is different from the average has
the features. While in our experience, individuals with different appearances can be featureless. Therefore, our definition
of featureless face is not the simple average face, instead it
means similar faces that appear frequently. Let us start from
the features analysis of the neutral face.

2.1. Sparse Representation of Public Face
After the registration mentioned in previous paragraph, the
registered 3D neutral face models are used as the training
data. In this subsection, we aim to decompose the neutral
faces into featureless part and the typical feature part. Since
the featureless part (Public Faces) may spread sparsely over
the human face space, low-rank matrix factorization techniques can be used to represent Public Face. First of all,
the geometry of a 3D face model is represented as S =
(x1 , y1 , z1 · · · , xN , yN , zN ) ∈ 3N and the appearance can be
represented as a vector T = (r1 , g1 , b1 · · · , rN , gN , bN ) ∈ 3N
, where N is the total number of vertices in a 3D model. We
stack the 3D neutral face models (S or T) as column of a matrix A. Since we want to decompose the facial geometry into
public face and perosnalized facial feature, the problem is to
find the correct sparse solution θ from the over-determined
system of linear equations for one of the training data Si :
Si = Aθi + eif , Si , eif ∈

3N

(1)

where Si denotes the i-th training data and eif means the personalized facial feature residual surface. Thus, we are looking for a sparse solution θi and the personalized facial feature vector eif by solving the constrained minimization problem given in equation 2.
min θi

0

+ eif

0

s.t. Si = Aθi + eif

(2)

2164

S.F. Wang & S.H. Lai / Manifold-Based 3D Face Caricature Generation

where · 0 denotes the 0 norm which counts the number
of nonzeros in a vector. We apply the low-rank matrix factorization technique proposed in [WYG∗ 09]
min θi

1

+ eif

s.t. Si = Aθi + eif

1

(3)

where θi 1 = Σ j | θi ( j) |. For each training data, we can
decompose Si into Pi = Aθi (public face) and eif (facial features) by applying the iterative algorithm [LGW∗ 09].
2.2. Robust Linear and Non-Linear Modeling
After the sparse representation of face geometry and texture, we apply principal component analysis on the low-rank
matrix Pall and obtain the linear subspace of public faces.
Therefore, the geometry S p and texture T p of a 3D public
face can be approximated by a mean and a linear combination of several eigenhead basis vectors and Fig.3(a) depicts
some examples of Public Face linear space.
S p = S + ∑ ai si
i

T p = T + ∑ bi ti

(4)

i

where S and T are the mean shape vector in public face
space, si and ti denotes the i-th eigen-shape basis of the statistical model, and a and b contain the shape and appearance
coefficients, respectively.
To well represent the non-linear inner structure of facial
features e f , we conduct a small experiment to determine
which manifold analysis is suitable for the representation
of e f . We include additional fully featureless data e0 = 0
into the training data set to better locate the relative position of features. We employ the MDS [CC00], LLE [RS00],
ISOMAP [TdSL00] and Laplacian Eigenmaps [BN01] to
achieve different low-dimensional non-linear embedding.
For our training data set, MDS and Laplacian Eigenmaps
outperform other methods and Laplacian Eigenmaps wellrepresent the magnitude of personal facial feautures. Please
see Fig.2 for comparison of these methods with 2D display,
the red circle is the embedding of fully featureless data eL0
and Laplacian Eigenmaps form a near triangle manifold with
eL0 in the center. For any embedded facial feature eLf , the direction away from eL0 , i.e. eLf − eL0 , denotes the suitable direction to amplify the facial features.
The training data sets are projected onto a 3D manifold,
including facial features of different gender, age and skin
color. In order to represent the distribution of features, Gaussian Mixture Model (GMM) is used to approximate the probability distribution of the 3D facial features in this lowdimensional expression manifold as follows:
p(eLf ) =

3. Training: The Modeling of Facial Expression
Deformation
As mentioned above, we observed that caricature is either
created from the personal face features on the neutral face or
the special expression style on the expressive face. To amplify the expression deformation, our system should be capable of decomposing the input human face 3D model into
the neutral part and the expression deformation. With this
consideration, our previous work [WL08] is suitable for this
application. In the work, the goal is to reconstruct a 3D expressive face model from a single face image. Their statistical model of neutral face and expression deformation is separated and it means we can treat the two parts in different
ways. In this paper, we use the similar idea to build the expression manifold and the Laplacian Eigenmaps is employed
for building the nonlinear manifold space.
3.1. Non-linear Facial Expression Manifold
Based on the work [WL08], the expression deformations ee
are obtained by the registered 3D face models with and without expression. In the BU-3DFE database [YWS∗ 06], six
categories of expression at four intensity levels are all used
in the training phase. Similar to the facial feature manifold
analysis, we include additional fully zero deformation vector e0 = 0 into ee . While applying the Laplacian Eigenmaps
to the expression data, we observed that the embedded 3D
data clouds eLe form a near-flat surface that is curved slightly
(Fig.4). This means the estimated manifold surface can well
represent the original expression deformation data.
The training data sets are embedded onto a manifold, including different intensities, types, and styles of expressions.
Each expression of a certain individual forms a trajectory in
the manifold space. Similarly, to represent the distribution of
expression deformations, Gaussian Mixture Model (GMM)
is used to approximate the probability distribution of the
3D expression deformation in this low-dimensional expression manifold. Fig. 4 shows the embedded three-dimensional
manifold space and the estimated probability distribution.

K

∑ ωk N(eLf ; µk , Σk )

(5)

k=1

eLf

∑K
k=1 ωk = 1 , µk and Σk are the mean and covariance matrix for the k-th Gaussian distribution. To facilitate us to
search the geometry deformation along the computed manifold space, we fit the embedded data eLf in the manifold space
by a smooth surface. Similarly, the personal facial texture
features can also be used to build on appearance manifold
space that model the deformation of texture color change.
Fig.3(b) and (c) depict the embedded data (blue points),
manifold surface and the estimated probability distribution
of geometry and texture features in the manifold space.

is the data coordinate after Laplacian embedding,
where
ωk is the probability of being in cluster k, 0 < ωk < 1 and

4. 3D Face Caricature Generation
While generating 3D face caricature, the input data can be a
2D face image or a scanned 3D face model. For the case with
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2165

S.F. Wang & S.H. Lai / Manifold-Based 3D Face Caricature Generation

Linear Subspace of
Public Face

(a)

(b)

(c)

Figure 3: The linear subspace of public face models can be modeled by principal component analysis as shown in (a). The
estimated Laplacian Eigenmaps and Gaussian Mixture Models of facial geometry features are shown in (b) and those of facial
texture features are shown in (c).
an image, we would like to reconstruct the 3D face model by
using the algorithm in [WL08]. When a 3D face is used as
input, we need to register and fit the model by the pre-trained
neutral linear model and non-linear expression manifold. To
make our paper self-contained, we briefly describe the reconstruction algorithm [WL08] in the following subsections.

4.1. 2D Input Image: 3D Neutral and Expressive Model
Reconstruction
Based on the work in [WL08], they applied an iterative
scheme to optimize the deformation of 3D human face models within and across subjects. Before the reconstruction procedure, a statistical weighting figure can be obtained by analyzing the magnitude of the expression deformation. From
the statistics on the deformation magnitudes for different expressions at all vertices in the 3D face model, the weighting
of all nodes in the morphable 3D face model (neutral face)
as well as those for the expression model can be determined.
With the pre-trained linear PCA model of neutral face
and non-linear manifold expression model, the initial geometry, appearance, illumination and pose can be determined
with several facial feature landmarks detected on the image.
Based on the initial parameters, we can iteratively perform
the following two steps with increasing model resolution and
the number of eigenhead bases until convergence.
1. optimize the texture coefficient vector and update the illumination coefficient vector.
2. update the pose parameters, neutral face geometry parameters, and expression parameters.
The reconstructed 3D face model includes the neutral part
Sn and expression ee and this facilitates us to apply the result
to generate a 3D face caricature if the input is a 2D image.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Figure 4: The non-linear manifold embedding of facial expression deformation and the corresponding GMM.
4.2. 3D Input Model: Registration and Statistical Model
Fitting
For a 3D input model Sin , we need to register and fit the
input data by using the pre-trained PCA model and manifold.
We first align the input face model to the mean model of
Public Face by the standard Iterative Closest Points (ICP)
technique [BM92] to obtain a coarse registration. Since the
motions of eyes and nose is relatively smaller than those of
other areas, we adopt the level-based alignment by applying
ICP twice. Subsequently, we register the whole source model
to the whole area and then to the eye-nose area of the target
public mean face.
With this rigid transformation, we have the initial pose
and coarse correspondences between 3D face models and
more parameters can be used for refined registration. Our
goal is to determine the neutral shape parameters a, neutral
texture parameters b and the manifold location of expression
deformation eLe .
(a, b, eLe ) = arg min Sin − S(a, b, eLe )

2

.

(6)

2166

S.F. Wang & S.H. Lai / Manifold-Based 3D Face Caricature Generation

S

in

S

p

S

f

Figure 5: The reconstruction of public face and the facial
feature distribution from the input neutral face model.

where S(·) is the function that combine the neutral shape and
expression deformation determined by (a, eLe ). If the input
3D model has texture information, the parameters in b are
also used to approximate the appearance. In order to obtain
the optimal solution, we can iteratively update the parameters until the error is converged.; i.e.
1. Apply Iterative Closest Points (ICP) technique to update
the rigid transformation and the correspondences of 3D
vertices.
2. Update the vertex coordinates by estimating new neutral face geometry parameters a, appearance parameters
b and expression deformation parameters eLe .
After the optimization process, the parameters a can be
used to reconstruct the neutral part Sn , and eLe can be used
to reconstruct the expression deformation ee of the input 3D
face model.
4.3. 3D Facial features and Expression Exaggeration
Whether the input is a 2D image or a 3D face model, we
can obtain the 3D neutral face and the corresponding 3D expression deformation by the methods described in the previous subsections. For the exaggeration of personal facial
features, we first approximate the 3D neutral part of input
model based on a robust PCA model pre-trained in Sec.2.2
as follows:
Sn = S p + S f

(7)

where S f is the difference of input neutral face Sn and reconstructed public face S p . Fig.5 shows examples of Public Face reconstruction and the extracted facial features S f
which is visualized with jet color map.
In order to exaggerate the estimated facial features S f ,
we project S f ∈ 3N to eˆ Lf in the Laplacian manifold space
3
based on the neighboring relation with the training examples. Recall that the manifold coordinate of fully featureless

Figure 6: The searching path of the exaggerated facial features on the manifold surface.
data 0 is represented as eL0 which is included during the training phase in Sec.2.2. Therefore, we can determine a searchL
ing direction in the manifold space v = eˆ L
f − e0 . However,
exaggerating the facial features along this direction may go
out of the manifold surface. To find the exaggerated features
in a feasible space, we project the direction v to the manifold surface and search the solution on the manifold surface
along the projected direction (see Fig.6). The blue dots in
Fig.6 denotes the searching path from eL0 on the manifold.
The more distant away from eL0 , the more exaggerative the
result is. Larger intensity of exaggeration effect usually corresponds to lower possibility of the features appearing on the
face. Assume we want to exaggerate the facial feature and its
probability is known as p∗ , we can start from the fully featureless coordinate eL0 and walking along the searching direction on the manifold, we move and recalculate the probability of current position with the pre-trained GMM until the
probability of the current coordinate equals the target prob∗
ability, p(eL∗
f )= p .
3
back to e∗f in the original space
We re-project eL∗
f ∈
3N
based on the neighboring relation with the training data.
Thus, the exaggerated features can be obtained and the exaggerated expression e∗e can also be determined by the same
way. As long as we have Public Face S p , the exaggerated
facial features e∗f and the facial expression e∗e , the facial caricature C can be generated by combining all these components C = S p + λ1 e∗f + λ2 e∗e with user-contrlled weighting
factors λ1 and λ2 .

5. Experimental Results
In this section, we conduct several experiments to validate
the effect of caricature generation. We define the mapping
between the probability of the personal facial features and
the exaggeration level as p∗ = exp(− ). As long as the exaggeration level is determined, the target probability of personal facial features p∗ can also be determined. Based on the
Public Face linear subspace and the feature non-linear manc 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2167

S.F. Wang & S.H. Lai / Manifold-Based 3D Face Caricature Generation

subjects

Input 3D model

Caricatures (level=3)

Caricatures (level=4)

Caricatures (level=5)

Linear Method

Figure 7: The 3D caricature generation from different subjects. The first column shows the original subjects used in the experiments and the second column gives the corresponding 3D face models. The third column to the right most column show the
results of 3D caricatures with exaggeration level = 3, 4, 5 respectively. The right-most column shows the results of traditional
linear method.
ifold surface, we use different exaggeration level ( = 3, 4, 5)
to generate the caricature with/without texture enhancement.
In the following experiments, we define the parameters λ1
and λ2 in the weighted combination for caricature generation, described in the previous section, to be adaptively√re√
lated to the exaggeration level, i.e. λ1 =
and λ2 = 2 .
Fig.7 shows examples of 3D caricature generation. The input 3D face models include geometry and texture which are
shown in the second column. The third column to the fifth
column are the exaggerated face geometries and appearances
generated by the proposed system with increasing exaggeration levels. We also provide the results generated by traditional linear method in the right-most column, i.e. linearly
exaggerate the differences of the input model and the average face. From the results, we can observe that the proposed
system focus the facial features and amplify them by using Public Face subspace and manifold modeling. The linear
method directly enhances the differences of input model and
the Average Face which may include non-feature area, and
the whole size of caricatures will become larger or smaller
when the exaggeration level increases.
For the experiments with input 2D images, we applied the
algorithm described in Sec.4.1 to reconstruct the 3D geometries and appearances of the input images and generated the
corresponding 3D caricature. The experiment is first conducted on CMU-PIE data set [SBB03] and some of the results are shown in Fig.8. The face images include expressions with different intensities and styles and we show different combination of processing steps for exaggerating the
facial characteristics, texture and the expression. From the
left column to the right-most column are the input 2D images
from CMU-PIE data set, the 3D face models reconstructed
from the images and the generated 3D face caricatures with
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Input Images

(a)

(b)

(c)

(d)

Figure 8: The exaggerated results with different combinations of processing steps. (a) Reconstructed 3D face
models. (b) Exaggerated 3D model with enhanced personal characteristics and texture (C+T). (c) Exaggerated
3D model with enhanced personal characteristics and
expression (C+E). (d) Exaggerated 3D model with enhanced
personal characteristics, texture and expression (C+T+E).

combinations of different processing steps, i.e. C+T, C+E
and C+T+E.
For real world applications, we also conduct several experiments on the face images of celebrities. We
show synthesized caricature images with the combination of recovered pose information and the 3D caricatures. More restuls and comparisons are available
at http://sites.google.com/site/savan520/home/3dcaricature.
The experiments were all conducted on AMD Phenom(tm)
II X4 965 Processor with 3.25GB RAM and the proposed
method was implemented in Matlab 2009b. In our unopti-

2168

S.F. Wang & S.H. Lai / Manifold-Based 3D Face Caricature Generation

mized implementation, exclude the step of 3D reconstruction from a single image, the memory consumption is about
68 MB and the average processing time for generating a 3D
caricature is about 4 seconds.
6. Conclusion
In this paper, we proposed a novel 3D caricature generation
algorithm. The sparse representation can well represent Public Face subspace. With the modeling of public faces, we can
identify the personal facial features by fitting the input 3D
face models with linear eigenfaces. The exaggerated features
can be estimated on the non-linear manifold surface which is
pre-learned with Laplacian eigenmaps. The manifold-based
method can also be used for expression exaggeration. The
combination of linear subspace, facial feature and expression manifold surfaces helps to capture the subtle features
of human face. The proposed method does not require an
artist to generate samples for training and or any user interaction. The experimental results show that the proposed algorithm can automatically generate realistic and artistic 3D caricatures for different applications. On the other hand, since
this work uses the statistical model proposed in the previous
work in [WL08], this work also inherits the limitation of expression modeling. The statistical learning approach for 3D
face reconstruction makes it difficult to recover the structure
of facial expression detail. Therefore, we plan to investigate
better learning techniques for 3D facial expression deformation and simultaneously improve the reconstruction of facial
expression details and the results of 3D face caricature.
References
[AR04] A KLEMAN E., R EISCH J.: Modeling expressive 3d caricatures. In ACM SIGGRAPH 2004 Sketches (2004), p. 61. 2
[BM92] B ESL P. J., M CKAY H. D.: A method for registration of
3-d shapes. IEEE Trans. on Pattern Anal. and Mach. Intell. 14, 2
(1992), 239–256. 5
[BN01] B ELKIN M., N IYOGI P.: Laplacian eigenmaps and spectral techniques for embedding and clustering. In Advances in
Neural Information Processing Systems 14 (2001), pp. 585–591.
2, 4
[CC00] C OX T. F., C OX M. A. A.: Multidimensional Scaling,
Second Edition. Chapman & Hall/CRC, September 2000. 4
[CHT03] C HANG Y., H U C., T URK M.: Manifold of facial expression. Workshop AMFG (2003). 2
[CqXyS∗ 01] C HEN H., QING X U Y., YEUNG S HUM H., CHUN
Z HU S., NING Z HENG N.: Example-based facial sketch generation with non-parametric sampling. In In ICCV01 (2001),
pp. 433–438. 2
[FKF∗ 01] F UJIWARA T., KOSHIMIZU H., F UJIMURA K., K I HARA H., N OGUCHI Y., I SHIKAWA N.: 3d modeling system
of human face and full 3d facial caricaturing. In 3DIM (2001),
pp. 385–392. 2
[FNT∗ 99] F UJIWARA T., N ISHIHARA T., T OMINAGA M.,
KOSHIMIZU H., K ATO K., M URAKAMI K.: On the detection
of feature points of 3d facial image and its application to 3d facial caricature. In 3DIM (1999), pp. 490–496. 2

[HCFT04] H U C., C HANG Y., F ERIS R., T URK M.: Manifold
based analysis of facial expression. IEEE Workshop on Face Processing in Video (2004). 2
[KMK97] KONDO T., M URAKAMI K., KOSHIMIZU H.: From
coarse to fine correspondence of 3-d facial images and its application to 3-d facial caricaturing. In Proceedings of the International Conference on Recent Advances in 3-D Digital Imaging
and Modeling (1997), IEEE Computer Society, p. 283. 2
[LCG06] L IU J., C HEN Y., G AO W.: Mapping learning in
eigenspace for harmonious caricature generation. In MULTIMEDIA ’06: Proceedings of the 14th annual ACM international conference on Multimedia (2006), pp. 683–686. 2
[LCLF08] L I P., C HEN Y., L IU J., F U G.: 3d caricature generation by manifold learning. In ICME (2008), pp. 941–944. 2
[LCODL06] L EYVAND T., C OHEN -O R D., D ROR G., L ISCHIN SKI D.: Digital face beautification. In ACM SIGGRAPH 2006
Sketches (New York, NY, USA, 2006), ACM, p. 169. 2
[LCXS02] L IANG L., C HEN H., X U Y.-Q., S HUM H.-Y.:
Example-based caricature generation with exaggeration. In Proceedings of the Pacific Conference on Computer Graphics and
Applications (2002), IEEE Computer Society, p. 386. 2
[LGW∗ 09] L IN Z., G ANESH A., W RIGHT J., W U L.,
Fast convex optimization algorithms
C HEN M., M A Y.:
for recovering a corrupted low-rank matrix.
preprint:
http://perception.csl.uiuc.edu/ jnwright/ (2009). 4
[MLN04] M O Z., L EWIS J. P., N EUMANN U.: Improved automatic caricature by feature normalization and exaggeration. In
ACM SIGGRAPH 2004 Sketches (2004), p. 57. 2
[RS00] ROWEIS S., S AUL L.: Nonlinear dimensionality reduction by locally linear embedding. Science (2000). 2, 4
[RSH01] ROWEIS S., S AUL L., H INTON G.: Global coordination
of local linear models. Neural Information Processing Systems
14 (2001), 889–896. 2
[SBB03] S IM T., BAKER S., B SAT M.: The cmu pose, illumination, and expression database. IEEE Trans. on Pattern Anal. and
Mach. Intell. (2003), 1615–1618. 7
[TdSL00] T ENENBAUM J., DE S ILVA V., L ANGFORD J.: A
global geometric framework for nonlinear dimensionality reduction. Science (2000). 2, 4
[WHL∗ 04] WANG Y., H UANG X., L EE C.-S., Z HANG S., L I
Z., S AMARAS D., M ETAXAS D., E LGAMMAL A., H UANG P.:
High resolution acquisition, learning and transfer of dynamic 3-d
facial expressions. EUROGRAPHICS (2004), 677–686. 2
[WL08] WANG S.-F., L AI S.-H.: Estimating 3d face model and
facial deformation from a single image based on expression manifold optimization. In ECCV (1) (2008), pp. 589–602. 2, 3, 4, 5,
8
[WYG∗ 09] W RIGHT J., YANG A. Y., G ANESH A., S ASTRY
S. S., M A Y.: Robust face recognition via sparse representation.
IEEE Trans. Pattern Anal. Mach. Intell. 31, 2 (2009). 4
[XCL∗ 09] X IE J., C HEN Y., L IU J., M IAO C., G AO X.: Interactive 3d caricature generation based on double sampling. In MM
’09: Proceedings of the seventeen ACM international conference
on Multimedia (2009), pp. 745–748. 2
[YCT04] Y. C HANG C. H., T URK M.: Probabilistic expression
analysis on manifolds. CVPR (2004). 2
[YWS∗ 06] Y IN L., W EI X., S UN Y., WANG J., ROSATO M. J.:
A 3d facial expression database for facial behavior research. International Conference on Automatic Face and Gesture Recognition (2006), 211–216. 3, 4

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

