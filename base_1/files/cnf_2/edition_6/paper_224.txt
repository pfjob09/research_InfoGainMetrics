DOI: 10.1111/j.1467-8659.2010.01815.x
Pacific Graphics 2010
P. Alliez, K. Bala, and K. Zhou
(Guest Editors)

Volume 29 (2010), Number 7

Towards Perceptual Simplification of Models with Arbitrary
Materials
Nicolas Menzel1 and Michael Guthe1
1 Graphics

and Multimedia Group, FB12, Philipps-Universität Marburg, Germany

Abstract
Real-time rendering of models with high polygon count is still an important issue in interactive computer graphics.
A common way to improve rendering performance is to generate different levels of detail of a model. These are
mostly computed using polygonal simplification techniques, which aim to reduce the number of polygons without
significant loss of visual fidelity. Most existing algorithms use geometric error bounds, which are well-suited for
silhouette preservation. They ignore the fact that a much more aggressive simplification is possible in low-contrast
areas inside the model. The main contribution of this paper is an efficient simplification algorithm based on the
human visual system. The key idea is to move the domain of error computation from image-space to vertex-space to
avoid a costly per-pixel comparison. This way the error estimation of a simplification operation can be accelerated
significantly. To account for the human vision, we introduce a perceptually based metric depending on the contrast
and spatial frequency of the model at a single vertex. Finally, we validate our approach with a user study.
Categories and Subject Descriptors (according to ACM CCS): I.3.5 [Computer Graphics]: Three-Dimensional
Graphics and Realism—Color, shading, shadowing, and texture

1. Introduction
Current graphics hardware is able to render scenes of striking realism in real-time: the ever growing processing power,
memory size and bandwidth allows for the rendering of
global illumination, realistic materials and smooth animations reserved to offline-renderers a few years ago. Three dimensional meshes have to keep pace with the render quality
in terms of accuracy and amount of detail.
Modern 3D acquisition techniques are able to provide digitized objects with very high accuracy in the sub-millimeter
range. This amount of detail often exceeds the ability of the
graphics hardware to render the object in real-time. Therefore, real-time applications rely on simplified versions of the
original mesh. The main goal of mesh simplification is to
generate a low-polygon-count approximation that maintains
the high fidelity of the original model. This involves preserving the model’s features and overall appearance, such as
surface position, curvature, color, and shading. Many simplification algorithms use error bounds on surface position
only. This guarantees an upper bound on the maximum deviation of the object’s silhouette, but other attributes are not
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

Figure 1: Stanford bunny with different materials simplified
up to an error threshold of 0.2%. Note how the number of
triangles (32902, 24198, 17669, and 27447) depends on frequency content.

preserved. The preservation of attributes – like texture coordinates or normals – is only possible when their deviations
are mapped to a geometric difference.

2262

N. Menzel & M. Guthe / Towards Perceptual Simplification of Models with Arbitrary Materials

The assumption that mesh quality improves with the number of primitives ignores the fact that visual fidelity is much
more difficult to quantify. The important question concerning mesh simplification should not be geometric but perceptual: does the simplified model look like the original? This is
motivated by the fact that certain regions of an object can be
simplified in a more aggressive manner, resulting in a lower
polygon count compared to geometric approaches.
We describe a polygonal simplification algorithm directly
based on the principles of the human visual system. In addition to the well known Hausdorff distance as geometric
difference, we use a per-vertex perceptual metric that guarantees an upper bound on the visual error and preserves the
visual appearance of the model. The basic idea is to measure
the changes in contrast, curvature, and lighting at each vertex after a simplification step. Then the step is only applied
if it is considered imperceptible. Deviations in image-space
are measured using the contrast sensitivity function (CSF).
In addition, we include the interaction of spatial frequencies
and orientations to account for visual masking. The result
is a simplification algorithm that adapts to the appearance
of the model to reduce the number of primitives as much
as possible without visual degradation. Figure 1 shows this
adaptation for various materials.
2. Related Work
As we integrate models for human visual perception into
geometric simplification, we give a short overview of both
fields and analyze first approaches of their combination.
2.1. Simplification
The principle of using multiple geometric representations
of objects to improve performance was first introduced by
Clark [Cla76]. Hoppe [Hop96] developed a geomorphing
algorithm that smoothly interpolates between different levels of detail (LODs). These LODs are precomputed using a specialized simplification algorithm. Later he optimized the data structures and algorithms to further improve
speed and memory usage [Hop98]. Hoppe also proposed an
appearance-based quadric error metrics [Hop99] by comparing geometric correspondences. Fundamental work by Garland and Heckbert [GH97] introduced quadric error metrics
for the geometric error created by edge removal. For the special case of multiresolution meshes with texture, Schilling
and Klein [SK98a] proposed a simplification algorithm that
does not strictly preserve texture coordinates unless the simplification affects the appearance. This is achieved by storing
the color variation for each simplified vertex and simplifying
more aggressively if it is not modified.
Cohen et al. [COM98] proposed to decouple geometry
from appearance by sampling surface normal, curvature, and
color attributes and storing them in texture and normal maps.
These maps can be precomputed and are a common tool

in interactive computer graphics. A similar approach based
on deviation in texture space was presented by Schilling
and Klein [SK98b]. They also developed an illuminationbased simplification algorithm [KSS98], where normal deviation is used for the accurate simplification of surfaces
with specular highlights that are rasterized with Gouraud
shading. They also proposed an algorithm for lightingdependent refinement of multiresolution meshes based on
normal cones [KS99]. Unfortunately, neither of these two
approaches can be used for textured surfaces. Their ideas
were later extended to textured models and arbitrary lighting
by Guthe et al. [GBBK04]. Based on the local variation of
an attribute in the unsimplified mesh, the attribute deviation
is mapped to a geometric error. Although the results have
a high visual quality, the number of required primitives is
drastically increased. Luebke and Hallen [LH01] used a perceptual metric for the simplification operations by considering contrast and spatial frequency. Unfortunately, this approach only works for Gouraud shaded surfaces. Williams et
al. [WLC∗ 03] improved this approach by accounting for textured and dynamically lit surfaces. The proposed algorithm
is view-dependent, sensitive to silhouettes, texture content
and illumination but cannot handle arbitrary materials. A
more aggressive perceptual-based simplification technique
was proposed by Luebke et al. [LHNW00]. They incorporate gaze-directed rendering to simplify even more aggressively in regions that are outside the center of the user’s
vision. Dumont et al. [DPF03] presented a decision framework based on efficient perceptual metrics for realistic rendering on commodity hardware. They address problems such
as diffuse texture caching, environment map prioritization,
and mesh simplification. Reddy was the first to attempt an
LOD selection system completely guided by a model of the
human visual system [Red97, Red01]. By analyzing the frequency content of objects and their LOD’s from multiple
viewpoints, the highest perceivable spatial frequency guides
the LOD selection. While all of these approaches modify
the LOD selection mechanism, much better results in terms
of the number of required primitives could be achieved by
integrating a visual model into the LOD creation, as proved
by Luebke et al. [LH01] before.
2.2. Perception
Kelly [Kel75] was the first to develop an analytical model
describing the spatial frequency characteristics of retinal receptive fields. He showed that the model can be used to describe the sensitivity of the visual system to sine-wave patterns. He also performed studies of the spatiotemporal sensitivity under conditions of stabilized vision [Kel79]. He found
that the shape of the CSF remains nearly constant for a wide
range of velocities. Later Burbeck and Kelly [BK80] formulated an analytic contrast sensitivity function based on
these experiments. Their insights are elemental features of
the metric that is proposed in this work. Based on this psychophysical model several measures for image and video
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2263

N. Menzel & M. Guthe / Towards Perceptual Simplification of Models with Arbitrary Materials

difference were proposed. In our context the most important one is Daly’s visible difference predictor (VDP) [Dal93]
that imitates the image processing in the human eye and
brain. A specialized model of visual masking was introduced by Ferweda et al. [FSPG97]. It describes how the
presence of a visual pattern affects the detectability of another by predicting changes in contrast, spatial frequency
and orientation of texture patterns. Based on Daly’s VDP,
an extension to high dynamic range images was introduced
by Mantiuk et al. [MMS04]. The HDR VDP could be used
for image-based simplification, similar to the approach of
Lindstrom [LT00]. Mantiuk et al. [MDMS05] later extended
their work by calibrating the HDR VDP using a HDR display. A first approach to efficiently integrate human perception into the simplification pipeline was proposed by Qu and
Meyer [QM08]. First, an importance map is generated based
on pre-computed visual masking and then it is used to locally increase the maximum simplification error. While the
number of primitives can be reduced compared to traditional
geometric simplification, there is neither a guarantee for the
non-perceivability of the simplification nor for its efficiency
with respect to the number of remaining primitives. Lavoue
et al. [Lav09] [Lav09] show how curvature, due to masking,
can play an important role in the simplification of geometric models. Pan et al [YP05] examine the factors that determine the quality of 3D images including geometry- and
texture resolution, shading complexity, frame rate, and other
psycho-visual factors. Corsini et al. [CDGEB07] address the
problem of assessing distortions produces by watermarking
3D meshes with a special visual metric. Yan et al. propose
an algorithm that decouples the simplification process into
shape analysis and edge contraction to extent collapses from
immediately local to the more global [YSZ04]. Lee et al. introduce a low-level human vision model by integrating mesh
saliency into the simplification process [LVJ05]. Their approach is based on a center-surround operator present in
many models of human vision.

3. Overview
Figure 2 shows the overall concept of our approach: First, the
original model is simplified using edge collapse operations.
These operations are sorted into a priority queue. As stated
before, the simplification can continue as long as the simplified model looks like the original. Unfortunately, threedimensional input data can not be used directly for a visual
comparison. For this reason, existing approaches rely on a
per-pixel comparison of several pairs of rendered images.
When rendering the two models, the color of each surface
point depends on the position as well as the incoming and
outgoing light direction. This results in a six dimensional
domain: two dimensions for the position on the surface, and
two pairs of spherical angles.
To reduce this high dimensionality, we introduce a special BRDF parametrization which is explained in section 4.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Original
Mesh

Dimension
Reduction

Visual Transformation

BRDF
Parameterization

Visual
Metric

Simplification
Algorithm

Simplified
Mesh

Dimension
Reduction

BRDF
Parameterization

Visual Transformation

Figure 2: Overview of the main concepts used in this paper.

The basic idea is to compare the models on a per vertex basis. This reduces the comparison to computating the visual
difference between two BRDFs. The advantages and disadvantages of this representation are discussed in section 7.
After the BRDF parametrization we can represent changes
in reflection for a fixed light direction by two-dimensional
maps. Using our visual metric a perception-based comparison can be performed on these maps. The result is a distance
at which the operation becomes perceivable.
The remainder of this paper is structured as follows: we
start with a brief summary of the simplification pipeline and
the modifications we have applied. After this, the BRDF
parametrization is explained in detail. Then, basic concepts
of our visual metric – such as contrast sensitivity and masking – are explained. Finally, we present our results and validate the algorithm with a user study.
3.1. Simplification Pipeline
The LOD is typically modulated based on an object’s distance from the viewpoint. Depending on the application’s
time and memory constraints, other criteria would be size,
velocity, eccentricity, fixed frame rate or orientation. One
common criterion is that geometric errors are only fully visible at grazing angles (due to the background). Ideally, the
reduced visual quality of the model is unnoticeable because
of the small difference of the object’s appearance when it
is far away or moving fast. Both the acceptable perceptual
quality and resource requirements depend on the particular
computing environment: the acceptable perceptual quality is
quite different for an automotive-selling application, or in
a game environment in which the player may be willing to
accept a level of artificiality.
As shown in Figure 3, our simplification pipeline is only

Process
priority queue

Visual
Difference

no

OK?
yes
Hausdorff
Distance

Collapse Edge

yes

OK?

no

Postpone Edge

Figure 3: Overall simplification pipeline.

2264

N. Menzel & M. Guthe / Towards Perceptual Simplification of Models with Arbitrary Materials

slightly different from the common geometric approach.
First, the edge collapses are sorted into a priority queue, depending on the error that this particular collapse induces: the
collapse operation causing the smallest geometric error is the
first element and the collapse operation causing the biggest
error is the last. The error is estimated by the quadric error
metric in [Hop99]. The classic approach works by removing the edge inducing the smallest error and recomputing
the error for all affected edges. This step is repeated until the
smallest error is above a user-defined threshold.
We modified this simple loop by computing the visual error for the first element in the queue. If the collapse operation induces a visual error higher than the one estimated
by the error quadric, the operation is postponed with the actual error. Otherwise, the Hausdorff distance is computed to
preserve the silhouettes. If the geometric error is above the
estimated one, the edge collapse is also postponed. If the operation passes both tests, the edge is collapsed and the priority queue is rebuilt for all affected edges. The collapse is
stored along with it’s perceptual and geometric error and the
algorithm repeats until no more collapses are possible.
Figure 4 shows a sphere (blue) approximated by an octahedron (red) consisting of 8 triangles. The distance d depicts
the geometric deviation (or error) induced by this approximation. Obviously, this error is most visible when the view
direction V is orthogonal to the surface normal N, because
of the strong change of the silhouette.

popping artifacts, the difference to the original model can become arbitrary. Therefore, we additionally compute the difference to the original model.
4. Perceptual Model
Each collapse operation inevitably results in a geometric deviation from the original surface. The geometric error allows
one to simplify planar regions in a very aggressive manner,
whereas silhouettes or curved areas can only be simplified to
a certain degree, depending on viewing distance and direction. Additionally, each simplification step induces a change
of curvature, lighting, and parametrization. The idea of our
perceptual simplification is, that the degree of reduction depends on the contrast of a surface area. For this purpose we
propose a perceptual model to determine whether a collapse
operation induces a perceivable change in contrast or not.
With the help of such a model, it can be estimated if this
particular operation is visible from a given distance.
A straightforward approach is to measure contrast in
image-space. If for example a surface has a strong, sharp
highlight, it is nearly impossible to simplify that region without significantly altering the original appearance. On the
other hand, if there is no contrast at all (e.g. in a shadowed
area), it would be possible to replace even the most complex
geometry by a simple surface patch. Finally, if an object’s
texture contains very high and random frequencies, it is also
possible to simplify more aggressively due to a phenomenon
called visual masking, which we address in Section 4.4.

α
d

4.1. Color Perception and Color Space Conversion

Figure 4: Projection of geometric simplification error between original (blue) and simplified model (red).

1
The induced frequency with maximum amplitude is 2α
d
cpd, where α = 2 arctan( 2D sin θr ) depends on the viewing
distance D and the angle θr between V and N. If this frequency is visible, i.e. it’s contrast sensitivity is not zero,
the collapse is perceived. As we require an upper bound of
the split/collapse distance for the priority queue, we use the
maximal projection of the geometric error (sin θr = 1). This
way we derive the maximal edge displacement frequency.
1

To prevent popping artifacts when switching between levels of detail, we always need to measure the difference between two subsequent levels. The most fine grained techniques are progressive meshes, where each simplification or
refinement operation represents a switch between two levels. In this case we need to compare the mesh before and after the current simplification operation. While this prevents

The first step to determine the visibility of a collapse operation is to mimic human color perception. Its main property is
that it operates on contrast rather than absolute luminances.
Another important issue is that we have a trichromatic perception of color signals: There are three different types of
cones on the retina with peak sensitivities of 570nm, 540nm,
and 440nm. They are referred to as long, middle and short
(LMS) cones. Sensations of red/green and blue/yellow are
encoded as color difference signals in separate visual pathways. This means that chromatic values are encoded in an
opponent color space with the color channels white-black
(W − K), red-green (R − G), and blue-yellow (B −Y ).
The first step of the perceptual model is the conversion
from the non-linear sRGB color space of the display into
the energy absorbed by each of the three cone types in the
human retina. This includes inverting the gamma-correction
of sRGB and transforming from the device color space to
the standard XYZ color space. From there, a transformation
to the long, middle and short (LMS) wavelength color space
can be performed as described by Hunt [Hun95]:
 

 
X
0.7328 0.4296 −0.1624
L
 M  =  −0.7036 1.6975 0.0061   Y 
Z
0.0030 0.0136 0.9834
S
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2265

N. Menzel & M. Guthe / Towards Perceptual Simplification of Models with Arbitrary Materials

Like Mantiuk et al. [MMS04] we do not model the photoreceptor response but transform the linear luminance values y (i.e. L/M/S) in the JND-scaled space. Their accurate
approach is unfortunately too costly for our purposes. We
thus approximate the transformation with

L

αN N

ϕi

αB B

∂ψ(l)
tvi (ψ(l)) =
∂l
The resulting constants are kl = 100 and ky = 1.02 cd/m2 .
Note that the controversial flattening of the receptor response
above 108 cd/m2 can be ignored since we can safely assume
that no display will produce such a high luminance.
Based on perceptual experiments one can derive different opponent color spaces for the standard human observer.
Since we are interested in pattern color separability, we
use the color transformation defined by Poirson and Wandell [PW96]:
 



L
0.990 −0.106 −0.094
W −K
 R − G  =  −0.669 0.742 −0.027   M ,
−0.212 −0.354 0.911
B −Y
S
where the non-linear cone responses L , M , and S are given
in JND-scaled space.
4.2. Local Visual Difference
As shown in Figure 2, the visual metric compares the original model to the simplified one. For each edge collapse, we
require the distance at which it becomes imperceivable for
every combination of view and light direction. One possibility to evaluate pairs of input images would be to directly
use the HDR VDP [MMS04]. Unfortunately, this is computationally too expensive, since it would result in a very high
number of input image pairs.
To avoid the per-pixel comparison, we only compare at the
vertices of the original and the simplified model. Since the
number of vertices is usually much smaller than the number of pixels, a significant speedup can be expected. As a
vertex defines an infinitesimal point on the surface, the frequencies can only originate from the curvature of the surface at this point. To capture all possible frequencies, each
vertex needs to be rendered from all light and view directions. These samples can be generated by rendering a hemisphere with its normal parallel to the view-direction. This
is performed with different light elevations by varying the
angle between light and view direction. The result is a fourdimensional reflectance field as shown in Figure 5 (right),
where it is mapped into a two-dimensional image.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

θi

T

αT
αB

l = ψ−1 (y) ≈ kl log (y + ky ) ,
where l is the response in JND-scaled space (i.e. L’/M’/S’),
kl and ky are determined by a least squares fit to the threshold versus intensity function tvi, and ψ is the photoreceptor
response. Under the assumption that the eye can adapt to a
single pixel of luminance [Dal93], this function is:

αT

α N φi

Figure 5: Hemispherical view and light sampling (left) and
resulting reflectance field (right).

When the curvatures of both the simplified and original
local patch are identical, there are only two sources of differences. Either there is a rotation of the surface, or other
shading parameters have changed. If we describe the rotation
relative to the local coordinate system of the surface, we can
obtain the angles αt , αb , and αn for the rotation around tangent, bitangent, and normal respectively (see Figure 5 left).
This is a similar parametrization to the well known rotation
quaternions with the only difference that it consists of explicit rotation angles. The three rotation angles are computed
from the quaternion q with
qx/y/z
αt/b/n =
· 2 arcsin qxyz .
qxyz
Note that the absolute values of the angles are independent
of the ordering of the two local coordinate systems as only
the signs of the quaternion coordinates change when rotating
in the opposite direction.
The rotations induce a phase shift φ:
φ =

φt2 + φ2b + φ2n =

αt2 α2b α2n
+
+
,
ωt2 ω2b ω2n

where ωt , ωb , and ωn are the angular frequencies of the signal. The first two correspond to spherical oscillations and
can be derived using a discrete spherical harmonics transformation [DH94]. ωn corresponds to a discrete Fourier transformation along αn . In total, we gain a signal amplitude for
all combinations of some discrete values of ωt , ωb , and ωn
from these transformations. In addition to the angular phase
shift, the difference in shading parameters can also lead to
an additional phase shift for each frequency combination.
If we were only interested in the visual difference at the
current vertex, we could simply compute the difference between the two shifted signals. Since we can assume that at
some nearby point on the mesh at least the phase shift becomes zero again (e.g. at a not yet simplified vertex), we
instead consider the maximum difference δmax of the two
signals with any phase shift up to the computed one. This
yields the following maximal signal difference:
δmax = min (a1 , a2 ) + |a1 − a2 | 2 − 2 cos φmax
φmax = max (φ + φs , π) ,
where ai is the amplitude (L’M’S’) of signal i and φs the

2266

N. Menzel & M. Guthe / Towards Perceptual Simplification of Models with Arbitrary Materials

phase shift between the two signals in their local coordinate
systems.
4.3. Distance Computation
To compute the perceived difference between the two signals, we also need to know their frequency in cycles per degree of visual angle. For this purpose, we first require the
spatial frequency fs which is then projected onto the retina
depending on the viewing conditions. The mapping of the
angular frequencies to local spatial frequencies depends on
the curvatures of the surface. The torsial frequency fn can be
calculated directly from ωn using the mean torsial curvature
κn (i.e. the mean absolute derivative of the torsion τ) with
f n = κ n ωn =

∂τ
∂u

2

+

∂τ
∂v

2

ωn .

Note that in physical terms, the torsion τ corresponds to
the angular momentum of an idealized top pointing along
the tangent of the curve. For surfaces the top direction corresponds to the normal and the curve moves in tangent or
bitangent direction. For the tangential frequencies the mapping is not that simple. The tangential frequencies ft and fb
can only be defined in relation to each other. This is due to
the fact that a combined rotation of light and view direction
around the surface normal has the same effect as a rotation
of the surface itself. Since ft and fb are orthogonal, they depend on the minimum and maximum tangential frequencies
fmin and fmax as follows:
ft =

( fmin sin β)2 + ( fmax cos β)2

fb =

( fmin cos β)2 + ( fmax sin β)2 ,

where β is the angle between the current light direction and
the direction of minimal curvature. The minimum and maximum frequencies are then:

Given the signals amplitude difference and frequency, we
can compute if the average human observer will perceive
a difference between them. For this purpose we use the
spatio-temporal CSFs Ca/c ( fs , ft ) defined by Burbeck and
Kelly [BK80]. To prevent visible popping when the level
of detail changes, we must use the maximum sensitivity
for each spatial frequency with respect to any temporal frequency:
Ca/c ( fs ) := max Ca/c ( fs , ft )
∀ ft

This is simple for the chromatic contrast sensitivity Cc since
the highest sensitivity is always at 0 Hz [Kel79]. For the
achromatic contrast sensitivity Ca finding the maximum is
too costly to be evaluated for each coefficient. We therefore
approximate Ca using a sum of three Gaussians. For the sake
of simplicity the chromatic sensitivity is also approximated
using a sum of two Gaussians:
Ca/c ( fs ) ≈ ∑ wi e

− fs2
σ2i

i

The spreads σi and weights wi for a least squares fit are listed
in Table 1 and Figure 6 shows the approximated CSFs.

Ca

Cc

i
1
2
3
1
2

wi
1.98488
1.06781
−1.18094
0.27519
0.24116

σi
9.45161◦
7.07609◦
3.14760◦
7.00112◦
3.78524◦

Table 1: Spread and weight of Gaussian functions used to
model the contrast sensitivity functions.

0.001
achromatic
chromatic
30 cpd

0.01

fmin = κmin min(ωt , ωb )
fmax = κmax max(ωt , ωb ),
where κmin and κmax are the minimum and maximum principal curvatures of the local surface patch. The total spatial
surface frequency f of the coefficient is now simply the L2 norm of the three frequencies:
f =

ft2 + fb2 + fn2 .

Exploiting the fact that cos2 β + sin2 β = 1 we can write:
f =

0.1

1

10
1

10

100
cycles per degree

Figure 6: Chromatic and achromatic contrast sensitivity:
the reduction of the fall-off towards zero is necessary to prevent popping.

2 + f 2 + f 2.
fmin
max
n

The mapping of spatial surface frequencies onto the retina
then depends on the view direction and distance. At distance
D and observed under the angle θr between view direction
and surface normal, the retinal frequency fs is:
D
f ≥ fs (D, θr ) ≥ D f .
cos θr

For both the contrast difference δmax and the detection
threshold td we need to know the mapping of the spatial
frequency into the field of view. For this the viewing distance D is required. To determine the split distance, we need
to find that Dmax above which the difference cannot be perceived. As this distance becomes smaller with increasing frec 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

N. Menzel & M. Guthe / Towards Perceptual Simplification of Models with Arbitrary Materials

quency, only the lower bound of the spatial frequency fs after projection needs to be considered. Dmax itself can then
be computed using Newton iteration to find the distance at
which δ = td holds. In the achromatic case there may be
two distances satisfying this condition, Dmin and Dmax , so
one can either store both or Dmax only. In the first case, the
edge could also be collapsed if the distance D is below Dmin .
Since for each operation three vertices and the split range
of the two previous vertices need to be considered, this can
however only marginally reduce the number of triangles.

2267

under the assumption that when the masking is maximal, the
signal difference is maximal as well. This assumption is reasonable for all real world objects and materials. To finally
compute td we use the masking function of Daly with masking amplitude am and frequency f , where the masking amplitude is a combination of local and global masking with:
am = max(ag ( f ), min(a1 , a2 ))
We again need to use the maximum of the signals as
the highest masker contrast determines the level of masking [Dal93].

4.4. Visual Masking
Visual masking is a perceptual phenomenon that was studied in detail by Campbell and Gubisch [CG66]. It occurs because the stimuli in the photoreceptors cells are not independent of each other but interact in various ways. As a result, a
stimulus that is visible by itself cannot be detected due to the
presence of another. The reason is that each ganglion cell on
the retina receives its input from a local region called receptive field. The receptive field is not restricted to cells in the
retina, but also occurs in many later stages of the visual pathways. Without masking the detection threshold td is always
one. Masking can elevate td for a given vertex, allowing for
a more aggressive simplification beyond the bounds of the
just noticeable difference. Values smaller than one are also
possible for td due to the so-called facilitation, which occurs
for very weak stimuli with similar characteristics.

5. Implementation
The first step when calculating the validity range of a collapse operation is to generate the hemispherical samples for
a given vertex and the closest interpolated point on the reference model. For this purpose we use the same shader that
is used to render the model in the interactive application. To
generate the samples, we use precomputed meshes required
with appropriate normal and tangent vectors. Note that only
meshes for αn and φi are required as for different θi the
meshes can be reused.
The following spherical harmonics transformation is underdetermined, since only the upper hemisphere is sampled.
If we set the lower half to zero, frequency ringing occurs at
the boundary. To prevent this, we solve the underdetermined
linear equation system set up by the inverse transformation
under the constraint that

We consider two types of masking in our approach: local masking, which occurs in the vicinity of a single vertex,
and global masking, that depends on the entire model. A cortex transformation [MMS04], which performs a decomposition into spatial and orientational channels, is applied for the
global masking. This is done by applying directional filters
to each color channel (orientation processing). The transformation cannot be transferred to the local case, because here
the signals are already localized. This limits the local masking to the frequency domain for which we have already performed a transformation using the spherical harmonics and
Fourier transformations.

where ci is the i-th coefficient and fi its frequency. We do not
need to solve this least squares problem for each transformation but simply calculate the pseudo-inverse of the constraint
matrix once. Another possibility would be to use the hemispherical harmonics proposed by Gautron et al. [GKPB04].
These have however the drawback that the phase shift cannot
be computed directly from the coefficients.

For the global masking the object is rendered from different view and lighting positions distributed over the unit
sphere. This step results in a set of input images Ik for incoming and outgoing directions (ωi,k , ωo,k ), which means that Ik
is rendered from view direction ωi,k with light direction ωo,k .
For each Ik the color space conversion to JND-scaled space is
applied. In contrast to local masking, where the cortex transform is calculated for the parametrization of a single vertex,
it is now applied to the entire image as in [MMS04]. Then
the visibility of each vertex is checked, since the difference
at a vertex can only be masked when it is visible. If a vertex
is visible, the maximum local amplitude of all receptive field
directions is calculated for each frequency band f . Finally,
the maximum amplitude over all images is computed and
stored as global masking ag ( f ) for the vertex. This is correct

The transformations that need to be applied to the generated samples are entirely implemented using CUDA. Such
a parallel implementation is possible since each transformation is essentially a sparse matrix vector multiplication.
In our implementation we used this general formulation
even for the Fourier transformation because the dimension is
rather small and thus the transformations are not the performance bottleneck. The most demanding part of the distance
computation is the comparison of the transformed samples
due to the iterative search for the distance at which the difference becomes just noticeable. In our approach, the distance
computation is performed in parallel on each transformed
coefficient. While this optimally exploits parallelism since
the code executed for each coefficient is identical and only
the data differs, an inter thread communication could be used

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

∑
i

|ci |2
→ min,
fi2

2268

N. Menzel & M. Guthe / Towards Perceptual Simplification of Models with Arbitrary Materials

to terminate those with smaller distance than the currently
computed lower bound.
5.1. Out-of-core Simplification
To process very large polygonal models we developed an
out-of-core implementation. To optimize the input data for
caching, we resort the vertices and triangles of the model
using spectral mesh sequencing. Other methods like cacheoblivious layouts [YLPM05] are possible as well.
The priority queue containing all collapse operations is
stored on disk. Each element in the priority queue consists
of a priority value and an index. The algorithm starts by loading the first n elements of the queue into memory. These elements are then sorted by the index of the target vertex v. Due
to the preprocessing step this results in a cache local access
pattern. The simplification error is then computed for this
sorted list. Note that to compute the error, the mesh datastructure has to be loaded as well to find the vertex neighbors. File accessing is done using memory map files, which
are provided by the operating system to optimize disk IO.
If a vertex passes all tests it is stored in another list. After
checking all vertices of the current block, the accepted operations are sorted by their perceivability distance. Then they
are applied in that order unless they have become invalid due
to the collapse of a neighbor vertex.
6. Results
Our test system is a PC with an Intel Core2 Duo 3.33 GHz
CPU, 2 GByte of main memory, and a GeForce GTX 295.
As maximum display luminance of the final interactive rendering application, we used 1000 cd/m2 , but this has little
influence on the simplification result and no effect on the
computation time. The hemispherical sampling has a resolution of 16 × 16 × 32 × 16 (αb , αt , αn /φi , θi ) which is sufficient for high frequency materials.
For each simplification operation three vertices are
checked. Each of these tests requires approximately 0.9 ms
which sums up to 2.7 ms per operation. With one third of
the quadric error as estimate for the Hausdorff distance, almost every second checked operation is postponed during
simplification. This results in a ratio of 2:1 for perception
tests versus performed operations. The Hausdorff test on the
other hand needs 4 ms on average, but is passed by nearly
every operation. In total, approximately 100 collapses were
computed per second which is a throughput of about 1 million faces in 90 minutes. This is still acceptable, although
the fastest simplification algorithms that do not guarantee a
maximum error are almost two orders of magnitude faster.
As simplification is a preprocessing step that is performed
once per model only, the number of primitives required for
a specific quality is far more important. In this context we
compare our algorithm to the attribute preserving simplification of Guthe et al. [GBBK04]. To map view distance to

simplification error, we use the same scaling as for the Haus1 ◦
dorff distance ( 30
fov). The resulting number of triangles
for different simplification errors are shown in Table 2 for
simplification errors of 0.1% and 1% of the bounding box
diagonal. Using our approach saves roughly half of the primitives compared to the attribute preserving simplification.

phlegmatic
dragon
Stanford
dragon
Happy
Buddha
XYZRGB
manuscript
XYZRGB
dragon

simpl. error
0.1%
1%
0.1%
1%
0.1%
1%
0.1%
1%
0.1%
1%

our algorithm
276, 742
61, 208
439, 446
14, 664
652, 808
53, 054
1, 203, 295
156, 437
1, 570, 734
266, 145

attrib. pres.
440, 646
133, 528
723, 732
36, 034
834, 450
61, 678
3, 357, 094
263, 793
3, 776, 114
424, 131

Table 2: Number of triangles for models simplified to a specified error.

To validate our perceptual measure, we produced several
simplifications of the three smaller models with varying perceptual error threshold. They were presented to a group of 32
(11 female, 21 male) subjects that had to decide if they can
perceive a difference to the non-simplified reference model
or not. The subject can interactively view the model that is lit
by a single directional light source that can also be rotated.
For rendering we created progressive meshes of the three
input models and used view-dependent refinement [Hop98]
to adapt them for the current viewing conditions. For each
model, we performed two tests, one with increasing and one
with decreasing error, until the subject noticed or stopped to
notice the visual difference. To remove any subject that was
only guessing, we also added a comparison of the original
model with itself. We used a calibrated display with a maximum luminance of 500cd/m2 and a viewing distance of 1 m
(1.9× the image width). The surrounding illuminance was
approximately 500 lux such that the ratio of display to surrounding luminance conforms with ITU-R BT.500-11. Figure 7 shows the experimental setup.

Figure 7: Setup for perceptual experiment. Here, the model
on the left is simplified up to four times the just noticeable
difference and the original model is shown on the right.

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

N. Menzel & M. Guthe / Towards Perceptual Simplification of Models with Arbitrary Materials

2269

Table 3 shows the results of this study that are as expected
as approximately half of the subjects started to notice the
difference at 1 JND for most models. Only the results for
the phlegmatic dragon differ. Here the estimated JND is only
half of the real one, leading to the assumption that a more aggressive simplification would have been possible. This might
be due to the rather sparse sampling when calculating the
global masking. For both of the models, the real just noticeable difference approximately lies between 0.8 and 1.2 of
the estimated JND. This validates our model as it complies
with the formal definition of the just noticeable difference.
phlegmatic dr.
1
4
1
2

JND
JND
1 JND
2 JND
4 JND

11% (23%/ 0%)
17% (28%/ 7%)
37% (49%/24%)
50% (65%/34%)
63% (72%/54%)

Stanford dr.
total (increasing/decreasing)
0% (0%/ 0%)
6% (8%/ 3%)
34% (38%/ 30%)
95% (96%/ 94%)
100% (100%/100%)

Happy Buddha
22% (33%/12%)
41% (52%/30%)
63% (63%/63%)
73% (63%/84%)
86% (88%/84%)

Table 3: Percentage of subjects that perceived a visual difference with respect to perceptual error threshold of our
model.

Figure 8 shows a comparison of the meshes generated by
the two simplification algorithms. Our approach especially
reduces the number of triangles in smooth regions that contain high frequency noise, e.g. at the belly of the Buddha and
scales of the XYZRGB dragon.
Finally, Figure 9 shows a comparison of the visual quality
using QSlim [Hop99] and attribute preservation [GBBK04]
with the same number of triangles as our approach. While
the quadric error metric does not preserve smooth variation
of attributes (especially texture coordinates), the attribute
preservation needs too many triangles resulting in a higher
geometric error.

Figure 8: Simplified models at 0.2% simplification error
using our algorithm (left) compared to attribute preservation(right).

7. Conclusion and Limitations
We presented a perceptual-based simplification algorithm
that can be applied to arbitrary materials. In contrast to existing approaches, where a perceptual metric is applied per
pixel, our metric is evaluated for each vertex. Since the number of visible vertices is usually much smaller than the number of pixels, we achieve a significant speedup. The simplification performance of 100 collapses per second is at least
an order of magnitude faster than any algorithm based on
per-pixel comparisons. To compare two vertices, we introduced a special BRDF parameterization that maps incoming
and outgoing lighting directions into a two-dimensional map
and accounts for aniostropic materials as well.
The main limitation of our method is that only the difference at vertices is checked which can lead to unnoticed deviations on textured surfaces. However, this did not occur for
the models and materials we have tested. This limitation is
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Figure 9: Simplified models at the same number of triangles
using QSlim (top left), attribute preservation (top right), and
our approach (bottom left) compared to the original model
(bottom right).

most critical for models with few vertices and textures containing very high frequencies. For such materials, a per pixel
metric has to be applied.

2270

N. Menzel & M. Guthe / Towards Perceptual Simplification of Models with Arbitrary Materials

References
[BK80] B URBECK C. A., K ELLY D. H.: Spatiotemporal characteristics of visual mechanisms: Excitatory-inhibitory model. J.
Opt. Soc. Am. 70, 9 (1980), 1121–1126.
[CDGEB07] C ORSINI M., D RELIE G ELASCA E., E BRAHIMI T.,
BARNI M.: Watermarked 3d mesh quality assessment. IEEE
Transaction on Multimedia 9, 2 (2007), 247–256.
[CG66] C AMPBELL F., G UBISCH R.: Optical quality of the hu˝
man eye. Journal of Physiology 186 (1966), 558–U578.
[Cla76] C LARK J. H.: Hierarchical geometric models for visible
surface algorithms. Commun. ACM 19, 10 (1976), 547–554.
[COM98] C OHEN J., O LANO M., M ANOCHA D.: Appearancepreserving simplification. In SIGGRAPH ’98: Proceedings of the
25th annual conference on Computer graphics and interactive
techniques (1998), pp. 115–122.

[KSS98] K LEIN R., S CHILLING A., S TRASSER W.: Illumination
dependent refinement of multiresolution meshes. In Computer
Graphics International (1998), pp. 680–687.
[Lav09] L AVOUÉ G.: A local roughness measure for 3d meshes
and its application to visual masking. ACM Trans. Appl. Percept.
5, 4 (2009), 1–23.
[LH01] L UEBKE D. P., H ALLEN B.: Perceptually-driven simplification for interactive rendering. In Proceedings of the 12th Eurographics Workshop on Rendering Techniques (2001), pp. 223–
234.
[LHNW00] L UEBKE D., H ALLEN B., N EWFIELD D., WATSON
B.: Perceptually Driven Simplification Using Gaze-Directed
Rendering. Tech. Rep. CS-2000-04, University of Virginia,
2000.
[LT00] L INDSTROM P., T URK G.: Image-driven simplification.
ACM Trans. Graph. 19, 3 (2000), 204–241.

[Dal93] DALY S.: The visible difference predictor: An algorithm
for the assessment of image fidelity. Digital Image and Human
Vision (1993), 179–206.

[LVJ05] L EE C., VARSHNEY A., JACOBS D.: Mesh saliency. In
ACM SIGGRAPH 2005 Papers (2005), ACM, p. 666.

[DH94] D RISCOLL J. R., H EALY J R . D. M.: Computing fourier
transforms and convolutions on the 2-sphere. Adv. Appl. Math.
15, 2 (1994), 202–250.

[MDMS05] M ANTIUK R., DALY S., M YSZKOWSKI K., S EIDEL
H.-P.: Predicting visible differences in high dynamic range images - model and its calibration. In Human Vision and Electronic
Imaging X (2005), vol. 5666, pp. 204–214.

[DPF03] D UMONT R., P ELLACINI F., F ERWERDA J. A.:
Perceptually-driven decision theory for interactive realistic rendering. ACM Trans. Graph. 22, 2 (2003), 152–181.
[FSPG97] F ERWERDA J. A., S HIRLEY P., PATTANAIK S. N.,
G REENBERG D. P.: A model of visual masking for computer
graphics. In SIGGRAPH ’97: Proceedings of the 24th annual
conference on Computer graphics and interactive techniques
(1997), pp. 143–152.
[GBBK04] G UTHE M., B ORODIN P., BALÁZS Á., K LEIN R.:
Real-time appearance preserving out-of-core rendering with
shadows. In Rendering Techniques 2004 (Proceedings of Eurographics Symposium on Rendering) (2004), Keller A., Jensen
H. W., (Eds.), pp. 69–79 + 409.
[GH97] G ARLAND M., H ECKBERT P. S.: Surface simplification
using quadric error metrics. In SIGGRAPH ’97: Proceedings of
the 24th annual conference on Computer graphics and interactive techniques (1997), pp. 209–216.
G AUTRON P., K RIVÁNEK J., PATTANAIK S., B OUA K.: A novel hemispherical basis for accurate and efficient rendering. In Rendering Techniques 2004 (Proceedings of
Eurographics Symposium on Rendering) (2004), pp. 321–330.

[GKPB04]
TOUCH

[Hop96] H OPPE H.: Progressive meshes. In SIGGRAPH ’96:
Proceedings of the 23rd annual conference on Computer graphics and interactive techniques (1996), pp. 99–108.
[Hop98] H OPPE H.: Efficient implementation of progressive
meshes. Computers & Graphics 22, 1 (1998), 27–36.
[Hop99] H OPPE H.: New quadric metric for simplifying meshes
with appearance attributes. In VISUALIZATION ’99: Proceedings of the 10th IEEE Visualization 1999 Conference (VIS ’99)
(1999).
[Hun95] H UNT R. W. G.: The Reproduction of Colour. Wiley
InterScience, 1995.
[Kel75] K ELLY D. H.: Perceptually optimized 3d graphics. Vision Research 15 (1975), 665–672.
[Kel79] K ELLY D. H.: Motion and vision. ii. stabilized spatiotemporal threshold surface. J. Opt. Soc. Am. 69, 10 (1979), 1340–
1349.
[KS99] K LEIN R., S CHILLING A.: Efficient rendering of multiresolution meshes with guaranteed image quality. The Visual
Computer 15, 9 (1999), 443–452.

[MMS04] M ANTIUK R., M YSZKOWSKI K., S EIDEL H.-P.: Visible difference predicator for high dynamic range images. In Proceedings of IEEE International Conference on Systems, Man and
Cybernetics (2004), pp. 2763–2769.
[PW96] P OIRSON A. B., WANDELL B. A.: Pattern-color separable pathways predict sensitivity to simple colored patterns. Vision
Research 36, 4 (1996), 515–526.
[QM08] Q U L., M EYER G. W.: Perceptually guided polygon
reduction. IEEE Transactions on Visualization and Computer
Graphics 14, 5 (2008), 1015–1029.
[Red97] R EDDY M.: Perceptually Modulated Level of Detail
for Virtual Environments. PhD thesis, University of Edinburgh,
1997.
[Red01] R EDDY M.: Perceptually optimized 3d graphics. IEEE
Comput. Graph. Appl. 21, 5 (2001), 68–75.
[SK98a] S CHILLING A., K LEIN R.: Rendering of multiresolution
models with texture. Computer and Graphics 22, 6 (1998), 667–
674.
[SK98b] S CHILLING A., K LEIN R.: Texture dependent refinement of multiresolution meshes. Tech. Rep. WSI–98–2, WilhelmSchickard-Institut für Informatik, Graphisch-Interaktive Systeme
(WSI/GRIS), Universität Tübingen, 1998.
[WLC∗ 03] W ILLIAMS N., L UEBKE D., C OHEN J. D., K ELLEY
M., S CHUBERT B.: Perceptually guided simplification of lit,
textured meshes. In I3D ’03: Proceedings of the 2003 symposium
on Interactive 3D graphics (2003), pp. 113–121.
[YLPM05] YOON S.-E., L INDSTROM P., PASCUCCI V.,
M ANOCHA D.: Cache-oblivious mesh layouts. ACM Trans.
Graph 24 (2005), 886–893.
[YP05] Y IXIN PAN L. I RENE C HENG A. B.: Quality metric for
approximating subjective evaluation of 3-d objects. IEEE Transactions on Multimedia. 7, 2 (2005), 269–279.
[YSZ04] YAN J., S HI P., Z HANG D.: Mesh simplification with
hierarchical shape analysis and iterative edge contraction. IEEE
Transactions on visualization and computer graphics (2004),
142–151.

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

