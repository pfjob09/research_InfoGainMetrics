DOI: 10.1111/j.1467-8659.2010.01741.x
Eurographics Symposium on Rendering 2010
Jason Lawrence and Marc Stamminger
(Guest Editors)

Volume 29 (2010), Number 4

Visibility Editing For All-Frequency Shadow Design
Juraj Obert1†

Fabio Pellacini2
1 University

Sumanta Pattanaik1

of Central Florida
College

2 Dartmouth

Abstract
We present an approach for editing shadows in all-frequency lighting environments. To support artistic control,
we propose to decouple shadowing from lighting and focus on providing intuitive controls to edit the former. To
accomplish this task, we precompute and store scene visibility information separately from lighting and BRDFs
and allow artists to edit visibility directly, by providing operations to select shadows and edit their shape. To
facilitate a wider range of editing operations, we generalize visibility from binary to three-channel floating point
quantities and introduce a novel shadow representation based on computation of visibility ratios between the
original render and the edited one. We demonstrate our results for diffuse and glossy surfaces, still scenes and
animations.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism—Color, shading, shadowing, and texture

1. Introduction

editing, we render the final image by multiplying the visibility and the transport operator.

The appearance of surfaces comes from the interaction of
materials, illumination and visibility. Manipulation of these
three components allows artists to control the look of synthetic images. To enhance this control, designers often perform non-physical edits to materials and illumination. To
simplify these edits, recent research efforts have proposed
appearance representation and intuitive interfaces. Rather
than focusing on illumination or materials, this paper explores the complementary aspect of editing visibility. We
focus on editing shadows in all-frequency lighting environment by non-physically editing the scene visibility.

Simple edits to the visibility function allows artists to
control shadow position, softness, color and gradient. To
achieve more complex shadow edits, such as arbitrary projective transforms or applying arbitrary filters, we introduce
a novel representation for shadows. We compute visibility
ratios (VRs) by comparing two different renderings — the
original rendering and a secondary rendering obtained by using modified visibility information.
All edits proposed in this work support animated environments and can be keyframed. When multiple shadows are
visible, either from multiple bright spots in the environment
map or due to multiple objects, designers might want to edit
only a subset of all shadows. We facilitate this by introducing a sketch-based selection algorithm such that edits can be
localized in the angular (lighting) and spatial domain.

To allow independent control of illumination and shadowing, we decouple visibility from lighting and subsequently
interactively edit the visibility alone. We achieve real-time
rendering by precomputing a transport operator as the product of lighting and BRDFs, similarly to PRT techniques,
and compress it in the wavelet domain. Scene visibility is
also precomputed and compressed in the wavelet domain.
We generalize visibility from binary to colored (fractional
three channel) to allow for more control. Visibility edits are
performed by altering the precomputed visibility data. After

To sum up, our work contains these major contributions:
• Decoupling of visibility and lighting, which allows artists
to interactively design all-frequency shadows independently of lighting.
• A shadow selection algorithm to localize shadow edits in
the angular (lighting) and spatial domain.
• A shadow representation in terms of visibility ratios and

† e-mail: jobert@cs.ucf.edu
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

1441

1442

Juraj Obert, Fabio Pellacini and Sumanta Pattanaik / All-Frequency Shadow Design

Figure 1: (Left) Original rendering. (Middle) Gradient and recoloring applied to the shadow of the railing, artistic filter
applied to the left shadow of the pirate, right shadow removed. (Right) Railing shadow removed, gradient removal applied to
self-shadowing on the stairs, shadow pattern applied to the shadow of the pirate.
fractional three-channel visibility representation for more
artistic control.
• A wide range of shadow editing operations along with numerical parametrization for animation.
2. Related Work
Shadow Editing and Design The work on user interfaces
for shadow manipulation [PTG02] allows users to design
sharp shadows, but only works by modifying parameters of
discrete light sources. The closest to our work is an imagespace algorithm for controlling four different parameters
of shadows cast by point light sources [DCFR07]. While
their work focuses on rendering, we put emphasis on editing
of visibility, shadow selection, shadow extraction and allfrequency environments. Furthermore, by editing visibility,
we do not limit ourselves to image-space operations, but also
support a variety of advanced edits (see Section 5).
Poulin and Fournier [PF92] present a simple user interface for shadow manipulation, but address neither shadow
selection nor advanced edits such as recoloring. Poulin et
al. [PRJ97] also demonstrate how light positions can be
computed from shadows, an approach which only allows
for global edits. [CGC∗ 03] describe a method for extracting shadows from one natural scene and inserting them into
another, but do not handle any editing operations. Similarly, [FHDT02] propose a technique for shadow removal
from photographs, but again, do not handle any editing.
Relighting and BRDF editing A whole body of work
focuses on interactive relighting [RKKS∗ 07], [SZC∗ 07],
[WTL06], [SZS∗ 08], [HPB06], lighting design [KPC93],
[PBMF07],
[TL04],
[OKP∗ 08] and BRDF editing [BAOR06], [CPK06], [BAEDR08]. These and our work
share the common aspect of interactive/real-time editing, but
to the best of our knowledge, ours is the first one to focus on
visibility editing with all-frequency lighting.
Wavelet-based rendering methods A very common and
efficient way of compressing precomputed datasets uses projection to wavelet space. Haar wavelets have been used

to render scenes with time-varying all-frequency illumination [NRH03]. Efficient evaluation of triple product wavelet
integrals has enabled rendering of glossy objects with varying viewpoint [NRH04]. Further extensions have enabled
rendering of dynamic glossy objects [SM06] and near-field
illumination [SR09]. Our work utilizes wavelets to compress
precomputed datasets too, but differs in the way we factor
the components of the rendering equation.
Artistic editing In order to allow artists to achieve predetermined visual goals, researchers have devised techniques
that do not rely on physically-correct rendering, but rather
provide a wide variety of editing operations. Besides techniques for lighting design mentioned at the beginning of
this section, algorithms exist to facilitate editing of reflections [ROTS09], all-frequency lighting [OMSI07] or subsurface scattering [STPP09]. Our shadow editing operations
complement these techniques and further increase the expressive power of digital artists.
3. Visibility Decoupling
One of the main goals of our work is to allow editing of
shadows independently of lighting. Currently, two common
approaches exist to solve this problem. The first one requires that artists render the scene into multiple layers, edit
them separately using image processing techniques and use
a compositing software for final blending. The second approach requires using of different light sources for rendering
of shadows and different light sources for lighting [Bar97].
Our work proposes a technique, which allows artists to extract and edit shadows without using different light sources
and makes shadow editing an inherent part of the design process (as opposed to being just a part of post-processing).
In order to decouple visibility from lighting, we proceed
in a fashion similar to other PRT approaches. We start with
the following formulation of the rendering equation:
L(ωi ) fr (ωi , ωo )V (x, ωi )(nx · ωi )dωi

L(x, ωo ) =
Ω

where L(x, ωo ) is computed outgoing radiance at point x in
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Juraj Obert, Fabio Pellacini and Sumanta Pattanaik / All-Frequency Shadow Design

1443

Figure 2: Horse scene. (Left) Original rendering. (Middle) Removal of two shadows and gradient edit on the dark shadow.
(Right) Translation of the dark shadow. Notice that only one shadow is affected, everything else stays intact.
direction ωo , L(ωi ) is incident lighting from an environment
map in direction ωi , fr is the BRDF, V (x, ωi ) is binary visibility between point x and direction ωi , nx is the surface
normal and (nx · ωi ) is the cosine term at point x.
Since our goal is to edit shadows by manipulating visibility, we group the non-visibility terms together and keep
visibility separate:
L(x, ωo ) =

V (x, ωi )T (x, ωi , ωo )dωi

sampling points. We considered two options — per-vertex
and per-texel sampling — and finally opted to use the latter one. While per-vertex sampling is very intuitive, scenes
with high-frequency geometry and high-frequency shadows
become undersampled and require an overwhelmingly large
number of vertices. Per-texel sampling alleviates these problems by sampling the geometry according to surface UV coordinates, thus allowing for finer control and a simple levelof-detail mechanism based on texture resolution.

Ω

where T (x, ωi , ωo ) = L(ωi ) fr (ωi , ωo )(nx · ωi ).
4. Framework
In order to maintain high-quality output and low storage requirements, we propose to use a wavelet-based framework
for rendering. After projecting visibility to the wavelet basis, we obtain:
L(x, ωo ) =
Ω

T (x, ωi , ωo ) ∑ v j Ψ j (x, ωi )dωi
j

= ∑ v j T j (x, ωo )
j

where v j are visibility coefficients in the wavelet basis and
T j (x, ωo ) are precomputed transport coefficients containing
incident lighting and BRDF. Depending on further application requirements, the transport coefficients can be precomputed for two configurations.
We allow the designer to work in a fixed viewpoint setting
with arbitrary BRDFs. Direction ωo then becomes only a
function of location x (image pixels in this configuration):

Choosing which configuration to use largely depends on
the application of interest. While movie productions will
benefit from the fixed viewpoint configuration (since lighting and shadow design is performed after the animation and
camera paths have been fixed), real-time applications such
as video games might prefer the free viewpoint scenario.
We represent both the transport operator and visibility information as coefficient matrices, with matrix columns corresponding to sampling directions ωi and matrix rows corresponding to surface locations x. Since we use cube maps
to represent environment lighting, we compute one visibility
and one transport matrix for each cube map face visible from
the positive side of the surface plane.
Visibility matrix V f (x) (cube map face f , location x) is
precomputed by ray tracing. For each sampling location (an
image pixel or a surface texel), we shoot rays through cube
map texels corresponding to the visible hemisphere around
the normal and record visibility information. Transport matrix T f (x) is precomputed by sampling the environment map
and multiplying with the BRDF. Section 8 discusses a more
efficient method, which utilizes the GPU.

L(ωi ) fr (ωi , ωo (x))(nx · ωi )Ψ j (x, ωi )dωi

T j (x) =
Ω

For free viewpoint scenarios, we limit the BRDF to be
diffuse-only, hence eliminating outgoing direction ωo from
the equation completely:
L(ωi ) fr (ωi )(nx · ωi )Ψ j (x, ωi )dωi

T j (x) =
Ω

Locations x in this configuration correspond to surface
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Upon completing precomputation for each matrix row, we
reduce the data size by employing non-linear wavelet compression. We compute 2D Haar wavelet coefficients, weigh
them with areas of corresponding basis functions and discard the coefficients of lowest magnitudes. The compression
yields wavelet matrices V f (x) and T f (x).
Given the transport and visibility matrices in wavelet
space, we render the final image by computing dot products

1444

Juraj Obert, Fabio Pellacini and Sumanta Pattanaik / All-Frequency Shadow Design

Figure 3: (a) Original. (b) Shadow of the car was translated away from the viewer. (c) Original. (d) Shadows of the chain rings
cast on the curved object were translated. Self-shadowing on the rings was removed. Notice that lighting stays intact.
of corresponding matrix rows:
co (x) = ∑ V f (x) · T f (x)
f

where co (x) is the color at location x and V f (x) · T f (x) is the
dot product of matrix rows at location x for cube map face f .
While it is possible to perform the computation on the GPU,
we render using the CPU mainly due to the simplicity of
implementation. For per-pixel sampling, we directly display
the rendered image on the screen and for per-texel sampling,
we store the result as per-object textures and rasterize the
scene using the GPU.
5. Direct Editing of the Visibility Function
The most straightforward way of editing shadows without
affecting lighting is by editing of the visibility matrix. In
general, we edit the whole visibility function, without performing selection in the angular domain. Refined selection
mechanism will be described in a separate section.
Our approach generalizes the visibility function and stores
three-channel fractional quantities instead of one binary
quantity. This allows us to implement per-channel edits, such
as recoloring. Out of the many other possibilities, we implemented four main visibility edits (see Figure 4).

Figure 4: Visibility edits. (a) Sampling locations in yellow.
(b) Original visibility. (c) Blur. (d) Translation.
intensity:
G(V f (x)) = lerp(V f (x), L(V f (x)), α)
where L refers to the shadow removal operator defined in
Section 6.3 and α is the gradient value.
Shadow Blur Shadow blur is an operation, which softens
the edges of a shadow, giving the illusion of a larger area
light source. We implement it by applying a low-pass filter
to the visibility function. Since the visibility function is in
fact a 2D image of the environment, it can be easily seen that
blurring it will blur the resulting shadow as well. Similarly to
translation/rotation, we implement this operation in the cube
map domain.
6. Editing of Visibility Ratios

Translation/Rotation Rotating and/or translating the visibility function is the main operation we use to change
the shape of the shadow. Translating visibility by vector
(vx , vy , vz ) results in a shadow transformation we would
get if we moved the light in the opposite direction
(−vx , −vy , −vz ). However, translation by modifying visibility affects only the shadow, whereas translation by modifying lighting would affect the whole image. Preferably, this
operation would be implemented by transformation in world
coordinates, but we found it sufficient and faster to perform
it by operating just on texels of the cube map.
Shadow Removal/Shadow Gradient Shadow removal allows us to remove a shadow from an image. We implement
this operation by removing occluders so that all major contributing lights become visible. The shadow removal algorithm is described in detail in Section 6.3. Shadow gradient
is a generalization, which allows the user to draw a gradient arrow over the shadow and remove it by interpolating its

While direct editing of visibility already provides novel
shadow editing operations, it doesn’t allow users to perform
advanced edits such as freeform transformations. To address
this problem, we introduce a novel shadow representation
based on visibility ratios.
6.1. Visibility Ratios (VRs)
Visibility ratios vr (x) are quantities computed for each sampling location by calculating the ratio of two colors — color
of the original rendering co (x) and color after shadow removal cr (x) (the secondary rendering created by the shadow
removal operation). Visualized as an image, visibility ratios
represent the shadow extracted from the original rendering.
vr (x) =

∑ f V f (x) · T f (x)
co (x)
=
cr (x)
∑ f L(V f (x)) · T f (x)

(1)

where L(V f (x)) is modified visibility after shadow removal.
VRs will be equal to 1.0 for sampling locations where no
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Juraj Obert, Fabio Pellacini and Sumanta Pattanaik / All-Frequency Shadow Design

Figure 5: Two shadows are caused by two major lights in
the environment map. The user paints over both shadows,
effectively selecting both lights. The removal algorithm then
extracts one shadow at a time.
shadow is present and less than 1.0 for sampling locations
in shadow. As can be seen from the equation, a singularity
occurs when the color after shadow removal equals zero. In
practice, however, such situations do not pose any problems,
because they only happen when the sampling location is either completely occluded (therefore invisible to the viewer)
or when the environment map is completely black.
Computation of visibility ratios requires two steps —
(1) localization of major contributing lights responsible for
shadows in the image and (2) shadow removal. Before
dwelling into details of VR editing, we describe algorithms
for both steps in the following sections.
6.2. Locating Major Contributing Lights
The goal of the light localization algorithm is to find lights,
which are responsible for shadows in the image. We make
the assumption that appearance of each shadow can be attributed to occluders blocking incoming light.
Our algorithm is based on accumulating visibility from
shadowed areas and overlaying it with incoming light information. The process starts with the user painting over a shadowed area and identifying shadowed surface sampling locations (set P). Since the sampling locations are in shadow, we
know that corresponding visibility functions must be blocking the light (see Figure 5).
For each selected sampling location, we read its corresponding row from the visibility matrix (one sampling location corresponds to one matrix row), unproject the visibility
back from wavelet space and accumulate its inverse into a
buffer. Finally, we normalize the buffer, multiply it with the
environment map and perform thresholding.
M f = threshold(

Ef
∑ (1.0 −V f (x)))
|P| x∈P

where M f is the located major light and E f is the environc 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1445

Figure 6: Shadow removal. (Top left) Original with main
and residual shadows. (Top right) Main shadow removed.
(Bottom left) Residual shadow removed and main shadow
edited. (Bottom right) Main and residual shadows removed.
ment map. Values above the threshold then belong to the
light responsible for the shadow, values below the threshold
become 0.
6.3. Shadow Removal
We remove shadows by rendering the image with modified
visibility information so that all occluded lights become visible. In our implementation, we first ask the user to paint
a boundary around the shadow she wishes to edit/remove.
Similarly to the light localization algorithm, the painting effectively selects shadowed surface sampling locations. Note
that while the painted area for the light localization algorithm is very small, shadow removal requires that the user
paint over the whole shadow.
Next, we read rows corresponding to painted locations
from the visibility matrix, unproject them from wavelet
space and modify. The modification removes occluding geometry by setting visibility values to 1 for incoming directions corresponding to the occluded light found by the light
localization algorithm.
L(V f (x))[i, j] =

1
V f (x)[i, j]

if
otherwise

M f [i, j] > 0

where [i, j] indexes rows and columns of one cube map face.
Finally, we render a secondary image cr (x) by multiplying
the modified visibility matrix with the transport matrix.
It can be easily seen that this algorithm would only work
for simple cases, for example scenes with one major contributing light and one object. However, it would fail for
more complex scenarios, such as multiple overlapping shadows. To handle all other situations, we generalize the algorithm for three scenarios.
Multiple lights cause multiple shadows of an object to ap-

1446

Juraj Obert, Fabio Pellacini and Sumanta Pattanaik / All-Frequency Shadow Design

the original (primary) image:
cr (x) =

co (x)
vr (x)

(3)

Or in other words, we can use visibility ratios to synthesize
a shadow in the secondary rendering:
co (x) = cr (x)vr (x)
Figure 7: (Left) Overlapping shadows are caused by multiple objects and one major light. We precompute multiple
visibility matrices, one for each visibility group, i.e. each
olympic ring in this case. (Right) The algorithm splits the
shadows and allows the user to edit them individually.
pear. Potentially, the shadows can overlap. In order to handle this situation, we allow the user to perform the selection process one light at a time. In each iteration, the algorithm locates one occluded light and extracts the shadows it
causes. This process is repeated for all occluded lights. Figure 5 demonstrates the algorithm for a scene with two major
contributing lights.
If silhouettes of multiple objects overlap when viewed
from a light’s point of view, their shadows overlap too. For
such situations, we allow the user to partition the scene into
separate visibility groups. We maintain a separate visibility
matrix for each visibility group and have the shadow removal
algorithm take an additional input — visibility group whose
shadow we are interested in removing/editing. Multiple visibility matrices increases storage requirements, but pay off
with increased robustness of the selection. Figure 7 demonstrates how we can split one shadow of one object into five
separate shadows.
The third scenario deals with residual shadows, i.e. shadows caused by lights we don’t consider to be occluded
(termed residual lights). These lights are not responsible for
the main shadow(s) in the image, but cause an additional, often barely visible, secondary shadow. In most cases, we are
interested in editing/removing the main shadow, but we can
use the same algorithms to remove/edit the residual shadow.
After locating the major occluded light responsible for the
main shadow, we invert the result to locate the residual light.
Residual shadows are shown in Figure 6.
6.4. Shadow Synthesis
Recall from Section 6 that visibility ratios are computed as:
vr (x) =

co (x)
cr (x)

(2)

where co (x) is the primary rendering and cr (x) is the secondary rendering, i.e. rendering obtained with the shadow
removal operation described in the previous section.
Thus we can use visibility ratios to remove a shadow from

(4)

Imagine now that the visibility ratios undergo a transformation (edit). This transformation yields new target visibility ratios wr (x) which we use to synthesize a new shadow:
cm (x) = cr (x)wr (x) =

co (x)
wr (x)
vr (x)

(5)

where cm (x) is a new rendering with a modified shadow.
Equation 5 is called the shadow synthesis equation and is the
main equation we use for editing based on visibility ratios. It
explains our synthesis algorithm in three steps: (1) removal
of the old shadow and computation of visibility ratios (2)
editing of visibility ratios (3) rendering of a new image.
6.5. VR editing
Editing of visibility ratios requires editing of a 2D array of
3-component values and is therefore considerably more efficient than editing of the entire visibility function. However,
since VRs effectively discard the directional aspect of visibility, their expressive power is constrained. Therefore, the
best use of VR edits is in combination with visibility editing
operations from Section 5.
VR edits not only allow for very accurate approximation
of existing edits, but also add new ones (see Figure 8). For
example, blurring the visibility ratios results in blurring of
the shadow (same as Shadow Blur). Similarly, Shadow Gradient can be implemented as a recoloring operation applied
to visibility ratios. It can be easily seen that in both cases,
editing of visibility ratios is more efficient and allows the
user to design both edits interactively.
Shadow Translation/Rotation can be approximated by applying a 2D image transformation to visibility ratios. For example, 2D projective transformation is very well suited for
per-pixel sampling if the shadow is cast on a planar object.
On the other hand, UV shifting is suitable for non-planar
surfaces and per-texel sampling.
As an example, Figure 8 shows a lattice controller
for manipulating shadow shape. The controller splits the
shadow into quadrilateral regions and uses mean-value coordinates [HT04] to resample the shadow as the user manipulates the control points. Target VRs are computed using a
filtered lookup:
wr (x) = filter(vr (mvc(x)))
where mvc(x) returns mean-value coordinates for location x.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Juraj Obert, Fabio Pellacini and Sumanta Pattanaik / All-Frequency Shadow Design

1447

Figure 8: Examples of VR edits and the lattice controller.
7. Rendering and Animation

7.1. Animation

Edits based on modifications of the visibility function do not
require any changes to the rendering method described at
the end of Section 4. For each sampling location, we simply
compute a dot product of the transport row and visibility row.
The remainder of this section therefore describes rendering
changes for shadow editing with visibility ratios.

We support animation for both classes of edits by freezing
edit parameters for a set of keyframes and interpolating them
for all other frames. Each of our edits can be described by a
set of numerical parameters — Shadow Blur is parameterized by the kernel size, Shadow Rotation is parameterized
by the angle of rotation, etc.

We implemented VR edits as GPU shaders, which take
as input a 2D texture of visiblity ratios and perform edits
according to user parameters (such as HSV parameters for a
recoloring edit). The output is a modified texture containing
new, target visibility ratios. Target visibility ratios are then
used to synthesize a new shadow, as per Equation 5.

For example, let P(i) be the translation vector for the
Shadow Translation edit for keyframe i and P(k) be the
translation vector for the same edit for keyframe k. For all
other frames j, i < j < k, we compute the translation vector
for Shadow Translation P( j) as follows:
j−i
k− j
P( j) = P(i)
+ P(k)
k−i
k−i

For per-texel sampling, we evaluate Equation 5 in texel
space and the simply render the final image by projecting
scene geometry on the screen. Since per-texel sampling uses
only diffuse BRDFs, the user is allowed to freely navigate
around the scene. Furthermore, per-texel sampling allows us
to remove aliasing from the image by utilizing multisampling directly on the GPU.
With per-pixel sampling, each sampling location corresponds to one screen pixel and the camera is locked.
While this restriction simplifies some parts of the rendering pipeline, it effectively prevents us from using GPU multisampling to remove edge aliasing. Therefore, we remove
aliasing by applying a bilateral filter as follows:
1. Precompute visibility and transport matrices for desired
image resolution W × H.
2. Upsample c(x) to resolution 2W × 2H using a bilateral
filter, where c(x) refers to an image obtained by multiplication of the transport and visibility matrices.
3. Downsample the result back to resolution W × H using
bilinear filtering.
For implementation details of bilateral upsampling, we refer the reader to [SGNS07].
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Computed translation vector P( j) is then applied to either
visibility function or VRs corresponding to frame j. Parameters for all other edits are computed in the same fashion.
This interpolation scheme corresponds to linear interpolation, but can be easily generalized to curve-driven interpolation as seen in popular modeling applications.
8. Results and Performance
We present results for three main scenes with varying complexity and different BRDFs (see Figures 1, 2 and 3). All
scenes were rendered using either one or two cube map faces
of the environment map, sampling each face with 128x128
rays. We used commodity hardware, an Intel Core Duo
E6850 processor, with 4GB of RAM and GeForce8800 Ultra. We used 200 wavelet coefficients in each case. The timings and required storage space are summarized in Figure 9.
We implemented the precomputation on both the CPU and
the GPU. The CPU precomputation uses ray tracing to compute the visibility matrix (column CPU V in Figure 9) and
CPU-side evaluation of the BRDF and the cosine term for
each direction in the cube map (column CPU T). Wavelet
encoding is also performed on the CPU. Scene complexity

1448
Scene
Rings
Horse
Pirate
Car

Juraj Obert, Fabio Pellacini and Sumanta Pattanaik / All-Frequency Shadow Design

Samples
per-texel 589,824
per-pixel 307,200
per-pixel 307,200
per-pixel 307,200

Triangles
8,192
16,416
214,726
184,560

CPU V
140 min.
64 min.
195 min.
146 min.

CPU T
45 min.
23 min.
23 min.
23 min.

GPU V
27 min.
16 min.
16 min.
16 min.

GPU T
27 min.
16 min.
16 min.
16 min.

Raw Data
5.34 GB
2.74 GB
2.68 GB
2.36 GB

Compressed
1.1 GB
172 MB
220 MB
617 MB

Figure 9: Timings and data sizes for presented scenes. The key to making the precomputation of the visibility matrix efficient is
in using the GPU to rasterize visibility (instead of ray tracing) and to perform the wavelet encoding. The transport matrix can
be also precomputed on the GPU — by evaluating the BRDF and the cosine term in a shader.
and the number of samples are the most important factors
affecting the performance of visibility precomputation. The
precomputation time for the transport matrix depends only
on the number of samples and BRDF complexity.
To precompute the visibility matrix on the GPU, we rasterize scene geometry into a cube map. To precompute the
transport matrix, we render quads aligned with cube map
faces and evaluate the BRDF and the cosine term in a pixel
shader. We also perform the wavelet encoding on the GPU.
The timings for GPU precomputation are summarized in
columns GPU V and GPU T in Figure 9. While the precomputation on the GPU is significantly faster than on the CPU,
equal timings for both matrices (GPU T and GPU V) imply
that the GPU-CPU data transfer is the main bottleneck of
our current implementation. Overall, GPU precomputation
of the visibility matrix delivers a speedup of factor 4-5 for
simple scenes and 9-12 for complex scenes. GPU precomputation of the transport matrix delivers a speedup of 1.5.
In all presented scenes, removal of all shadows took between 1 and 3 minutes and editing was interactive. We would
like to point out that further reduction of the precomputation time can be achieved by parallelizing. Since precomputations for each texel/pixel are independent, we can use an
arbitrary number of available GPUs/CPUs to speed up the
process. We performed our tests on commodity hardware
(manufactured in 2006) with only 2 cores. Current multicore high-end hardware will increase the performance significantly. To conserve storage space, the user might opt to
apply lossless compression to both matrices. For our data,
the RAR algorithm performed the best (column Compressed
in Figure 9) and achieved average compression ratio of 1:9.
9. Discussion
Even though we believe that our work presents a robust and
simple approach to all-frequency shadow design, several important aspects need to be considered before deploying it in
client applications.
The main limitation of our work is the upper frequency
bound of shadows. Our examples illustrate shadows due to
all-frequency lighting, for which our algorithms behave very
well. However, in an extreme example of an environment
map with a single non-zero texel, undersampling problems
arise. We believe that such situations are better handled with
methods tailored specifically for high-frequency shadows, as
opposed to an all-frequency method such as ours.

It is important to realize that thresholding used in the light
localization algorithm might introduce artifacts if the threshold value is not set correctly. We found that the threshold
values mainly depend on the environment map, but are in
practice very easy to set. Other factors affecting the accuracy
of the light localization algorithm include the resolution of
the environment map and the number of wavelet coefficients
used to compress the visibility matrix.
Another important aspect is the selection of a rendering
technique. Our focus was on high-quality rendering and our
implementation therefore relies on a precomputed approach.
However, it can be easily seen that a real-time method such
as [CK07] can be used as well. The only requirement is
the ability to access explicit visibility information. Finally,
our techniques are not limited to environment maps. Other
types of all-frequency lights, such as area lights or near-field
lights [SR09] can be used as well.
10. Conclusion and Future Work
We have presented a set of techniques for all-frequency
shadow design. Our work is based on editing of visibility information and its main advantage is the decoupling of
shadow design from other stages of the production pipeline.
The ability to access explicit visibility information allowed
us to design an intuitive user interface and develop robust
algorithms for shadow selection and extraction.
Our work generalizes visibility representation from binary to three-channel fractional quantities and thus accommodates a large set of all-frequency shadow editing operations. We support both direct editing of visibility information and editing of a novel shadow representation based on
visibility ratios.
In the future, we would like to investigate all-frequency
shadow design in more complex lighting environments, e.g.
secondary-bounce shadows caused by global illumination.
Also, we are interested in designing an automatic shadow
design tool and a perceptual evaluation of shadow effects.
Acknowledgments
This work has been supported by University of Central
Florida I2Lab Fellowship and Florida High-Tech Corridor
Research Funding. Fabio Pellacini was supported by the
NSF (CNS-070820, CCF-0746117), Intel and the Sloan
Foundation. Many thanks to Vlasta Havran for the Golem
ray tracer and to Alex Zelenin for models.
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

Juraj Obert, Fabio Pellacini and Sumanta Pattanaik / All-Frequency Shadow Design

References
[BAEDR08]

B EN -A RTZI A., E GAN K., D URAND F., R A R.: A precomputed polynomial representation for
interactive brdf editing with global illumination. ACM Trans.
Graph. 27, 2 (2008), 1–13. 2
MAMOORTHI

[BAOR06] B EN -A RTZI A., OVERBECK R., R AMAMOORTHI R.:
Real-time brdf editing in complex lighting. In SIGGRAPH ’06:
ACM SIGGRAPH 2006 Papers (New York, NY, USA, 2006),
ACM, pp. 945–954. 2
[Bar97] BARZEL R.: Lighting controls for computer cinematography. J. Graph. Tools 2, 1 (1997), 1–20. 2
[CGC∗ 03] C HUANG Y.-Y., G OLDMAN D. B., C URLESS B.,
S ALESIN D. H., S ZELISKI R.: Shadow matting and compositing. In SIGGRAPH ’03: ACM SIGGRAPH 2003 Papers (New
York, NY, USA, 2003), ACM, pp. 494–500. 2
ˇ
J.: Real-time shading with
[CK07] C OLBERT M., K RIVÁNEK
filtered importance sampling. In SIGGRAPH ’07: ACM SIGGRAPH 2007 sketches (New York, NY, USA, 2007), ACM, p. 71.
8

[CPK06] C OLBERT M., PATTANAIK S., K RIVANEK J.: Brdfshop: Creating physically correct bidirectional reflectance distribution functions. IEEE Comput. Graph. Appl. 26, 1 (2006), 30–
36. 2
[DCFR07] D E C ORO C., C OLE F., F INKELSTEIN A.,
RUSINKIEWICZ S.: Stylized shadows. In NPAR ’07: Proceedings of the 5th international symposium on Non-photorealistic
animation and rendering (New York, NY, USA, 2007), ACM,
pp. 77–83. 2
[FHDT02] F INLAYSON G. D., H ORDLEY S. D., D REW M. S.,
T J E. N.: Removing shadows from images. In ECCV 2002:
European Conference on Computer Vision (2002), pp. 823–836.
2
[HPB06] H AŠAN M., P ELLACINI F., BALA K.: Direct-toindirect transfer for cinematic relighting. In SIGGRAPH ’06:
ACM SIGGRAPH 2006 Papers (New York, NY, USA, 2006),
ACM, pp. 1089–1097. 2
[HT04] H ORMANN K., TARINI M.: A quadrilateral rendering primitive. In HWWS ’04: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware
(New York, NY, USA, 2004), ACM, pp. 7–14. 6
[KPC93] K AWAI J. K., PAINTER J. S., C OHEN M. F.: Radioptimization: goal based rendering. In SIGGRAPH ’93: Proceedings
of the 20th annual conference on Computer graphics and interactive techniques (New York, NY, USA, 1993), ACM, pp. 147–154.
2
[NRH03] N G R., R AMAMOORTHI R., H ANRAHAN P.: Allfrequency shadows using non-linear wavelet lighting approximation. In SIGGRAPH ’03: ACM SIGGRAPH 2003 Papers (New
York, NY, USA, 2003), ACM, pp. 376–381. 2
[NRH04] N G R., R AMAMOORTHI R., H ANRAHAN P.: Triple
product wavelet integrals for all-frequency relighting. In SIGGRAPH ’04: ACM SIGGRAPH 2004 Papers (New York, NY,
USA, 2004), ACM, pp. 477–487. 2
[OKP∗ 08] O BERT J., K RIVÁNEK J., P ELLACINI F., S ÝKORA
D., PATTANAIK S. N.: icheat: A representation for artistic control of indirect cinematic lighting. Comput. Graph. Forum 27, 4
(2008), 1217–1223. 2
[OMSI07] O KABE M., M ATSUSHITA Y., S HEN L., I GARASHI
T.: Illumination brush: Interactive design of all-frequency lighting. In Proc. Pacific Conference on Computer Graphics and Applications (2007), IEEE Computer Society (2007), pp. 171–180.
2
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

1449

[PBMF07] P ELLACINI F., BATTAGLIA F., M ORLEY R. K.,
F INKELSTEIN A.: Lighting with paint. ACM Trans. Graph. 26,
2 (2007), 9. 2
[PF92] P OULIN P., F OURNIER A.: Lights from highlights and
shadows. In I3D ’92: Proceedings of the 1992 symposium on
Interactive 3D graphics (New York, NY, USA, 1992), ACM,
pp. 31–38. 2
[PRJ97] P OULIN P., R ATIB K., JACQUES M.: Sketching shadows
and highlights to position lights. In CGI ’97: Proceedings of the
1997 Conference on Computer Graphics International (Washington, DC, USA, 1997), IEEE Computer Society, p. 56. 2
[PTG02] P ELLACINI F., T OLE P., G REENBERG D. P.: A user interface for interactive cinematic shadow design. In SIGGRAPH
’02: Proceedings of the 29th annual conference on Computer
graphics and interactive techniques (New York, NY, USA, 2002),
ACM, pp. 563–566. 2
[RKKS∗ 07] R AGAN -K ELLEY J., K ILPATRICK C., S MITH
B. W., E PPS D., G REEN P., H ERY C., D URAND F.: The lightspeed automatic interactive lighting preview system. In SIGGRAPH ’07: ACM SIGGRAPH 2007 papers (New York, NY,
USA, 2007), ACM, p. 25. 2
[ROTS09] R ITSCHEL T., O KABE M., T HORMÄHLEN T., S EI DEL H.-P.: Interactive reflection editing. In SIGGRAPH Asia
’09: ACM SIGGRAPH Asia 2009 papers (New York, NY, USA,
2009), ACM, pp. 1–7. 2
P.-P.,
G OVINDARAJU
N.
K.,
[SGNS07] S LOAN
N OWROUZEZAHRAI D., S NYDER J.:
Image-based proxy
accumulation for real-time soft global illumination. In PG
’07: Proceedings of the 15th Pacific Conference on Computer
Graphics and Applications (Washington, DC, USA, 2007), IEEE
Computer Society, pp. 97–105. 7
[SM06] S UN W., M UKHERJEE A.: Generalized wavelet product
integral for rendering dynamic glossy objects. In SIGGRAPH
’06: ACM SIGGRAPH 2006 Papers (New York, NY, USA, 2006),
ACM, pp. 955–966. 2
[SR09] S UN B., R AMAMOORTHI R.: Affine double- and tripleproduct wavelet integrals for rendering. ACM Trans. Graph. 28,
2 (2009), 1–17. 2, 8
[STPP09] S ONG Y., T ONG X., P ELLACINI F., P EERS P.: Subedit: a representation for editing measured heterogeneous subsurface scattering. In SIGGRAPH ’09: ACM SIGGRAPH 2009 papers (New York, NY, USA, 2009), ACM, pp. 1–10. 2
[SZC∗ 07] S UN X., Z HOU K., C HEN Y., L IN S., S HI J., G UO
B.: Interactive relighting with dynamic brdfs. In SIGGRAPH
’07: ACM SIGGRAPH 2007 papers (New York, NY, USA, 2007),
ACM, p. 27. 2
[SZS∗ 08] S UN X., Z HOU K., S TOLLNITZ E., S HI J., G UO B.:
Interactive relighting of dynamic refractive objects. In SIGGRAPH ’08: ACM SIGGRAPH 2008 papers (New York, NY,
USA, 2008), ACM, pp. 1–9. 2
[TL04] TABELLION E., L AMORLETTE A.: An approximate
global illumination system for computer generated films. In SIGGRAPH ’04: ACM SIGGRAPH 2004 Papers (New York, NY,
USA, 2004), ACM, pp. 469–476. 2
[WTL06] WANG R., T RAN J., L UEBKE D.: All-frequency relighting of glossy objects. ACM Trans. Graph. 25, 2 (2006),
293–318. 2

