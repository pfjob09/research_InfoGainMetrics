DOI: 10.1111/j.1467-8659.2010.01754.x

COMPUTER GRAPHICS

forum

Volume 29 (2010), number 8 pp. 2427–2437

An Eyeglass Simulator Using Conoid Tracing
M. Kakimoto1 , T. Tatsukawa1,2 and T. Nishita2
1 SGI Japan, Ltd., Japan
kaki@sgi.co.jp, tatsukawa@gmail.com
2 The University of Tokyo, Japan
nis@is.s.u-tokyo.ac.jp

Abstract
This paper proposes a method for displaying images at the fovea of the retina taking visual acuity into account.
Previous research has shown that a point light source projected onto the retina forms an ellipse, which can be
computed with wavefront tracing from each point in space. We propose a novel concept using conoid tracing,
with which we can acquire defocusing information several times faster than that acquired by previous methods.
We also show that conoid tracing is more robust and produces higher quality results. In conoid tracing the ray is
regarded as a conoid, a thin cone-like shape with varying elliptical cross-section. The viewing ray from the retina
is traced as a conoid and evaluated at each sample location. Using the sampled and pre-computed data for the
spatial distribution of blurring, we implemented an interactive eyeglass simulator. This paper demonstrates some
visualization results utilizing the interactivity of the simulator, which an eyeglass lens design company uses to
evaluate the design of complex progressive lenses.
Keywords: progressive lens, wavefront tracing, conoid tracing, defocus, depth of field
ACM CCS: I.3.3 [Computer Graphics]: Picture/Image Generation—Display algorithms

1. Introduction
Modelling the optical system of the human eye and simulating defocusing phenomena are important factors in eyeglass
lens design. In recent years, with aging populations in many
advanced countries and activities such as the use of computers becoming widespread, the acuity required for near vision
is becoming more important. This has led to an increased demand for progressive lenses, which are made with complex
and smoothly curved surfaces such that the upper half is for
far vision and the lower half for near vision.
The design of progressive lenses requires a number of
input conditions such as myopia (near sight), pre-sbyopia
(aged eyes), and astigmatism (axial differences in refraction).
More recently people have been tending to emphasize the
quality of vision, which requires spectacle lens designers to
fine tune the priorities of near vision, far vision, and middle
distance vision in a single lens. In addition, there is a strong
requirement for the lens to have the minimum amount of
image distortion for the spectacle user.
c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics
Association and Blackwell Publishing Ltd. Published by
Blackwell Publishing, 9600 Garsington Road, Oxford OX4
2DQ, UK and 350 Main Street, Malden, MA 02148, USA.

Thus, spectacle lens designers are forced to go through
a long process of trial and error. A verification tool which
quickly recreates what the viewer would see with the eyeglass
under design would effectively help designers evaluate their
products. We propose an eyeglass simulator, which generates
views interactively, with appropriate refraction and defocusing, through spectacle lenses.
A unique and difficult aspect of rendering the view through
an eyeglass is the reproduction of the accommodation made
by the human eye. Image generation for evaluating human visual acuity is different from regular computer graphics imaging. The eyeball rotates toward the direction of each pixel,
chooses the best accommodated optical parameters, and renders the colour intensity projected at the central fovea of the
retina. Loos et al. [LSS98] refers to this as ‘an image that
would be perceived by a viewer as he scans the field of view’
and Barsky [Bar04] calls it the ‘scanned foveal image’.
In general, it takes time to render a scanned foveal image
because both the accommodation and distribution of the ray

2427

2428

M. Kakimoto et al. / An Eyeglass Simulator Using Conoid Tracing

need to be done for each pixel. Kakimoto et al. [KTMN07]
pre-computed blurring information for each sample point in
the field of view, and used it in the vertex shader. Our rendering method is the same as theirs but the pre-computation
process is different.

blurring each vertex in the scene using pre-computed spatial
distributions of the ellipse of confusion (EOC). EOC is an
expanded concept of the circle of confusion (COC), which is
an optical spot caused by light rays from a lens. They called
the spatial distribution of EOC a ‘blur field’.

For spectacle lens design verification, certain visualization techniques often play important roles. Traditionally,
pseudo-colour expression over the lens surface is used to
comprehensibly illustrate lens characteristics such as the refractive power. We show in this paper the visualization of
more straightforward and dynamic metrics used to measure
lens performance.

A blur field is a set of three-dimensional (3D) voxels
which fills the field of view, or the view frustum. In the
pre-computation, each voxel sampling point becomes a light
source for wavefront tracing. When tracing of a wavefront
from the voxel to the retina is completed, an ellipsoidal circle of confusion is obtained. This process is done for each
voxel and the resulting set of ellipses is stored as an intermediate data set. The iteration for every 3D voxel is timeconsuming and the process cannot be included in the interactive mainloop. The method proposed in this paper makes it
significantly faster by inverting the tracing order, making the
computational complexity of the major processes less than
O(n3 ).

This paper is organized as follows. Section 2 introduces
previous work. Section 3 gives an overview of the simulation
process. Section 4 describes the new pre-computing process
for producing blurring information. We give some results
in Section 5, discussions in Section 6 and conclusions and
future work in Section 7.

2. Related Work
A common scheme to simulate defocusing is distribution
ray tracing [CPC84]. Since the processing time required is
several times longer than standard ray tracing, real-time or
interactive rendering is hard to achieve. In addition, computation of extra refraction and accommodation in the human
eye model makes the processing time even longer [MKL97,
LSS98, Bar04].
The depth of field (DOF) effect using graphics hardware
was first introduced by Haeberli et al. [HA90]. This is a multipass rendering and blending technique, widely applied in
the field of real-time rendering. More recently, pixel shading
has been used to access arbitrary pixels in a textured image.
This feature can be utilized for post-processing a DOF effect.
Although it suffers from a couple of types of visual artefact
[Dem04], it is much faster than blending.
These graphics hardware techniques are based on the pinhole camera model rather than a human eye model, and therefore cannot be directly applied to our purpose. Lee et al.
[LKC08] proposed a per-pixel post-processing DOF method
which employs a thin camera model, and works well in most
cases with high performance. However, it has difficulty in
handling complicated defocus distributions caused by the
accommodation made by the human eye and a complex progressive lens.
These real-time schemes also lack accurate refraction information and are unsatisfactory for eyeglass simulators.
Kakimoto et al. [KTMN07] proposed a fast DOF rendering
method taking human visual acuity and spectacle lenses into
account. They used ray tracing on a per-vertex basis for
refraction, which we have employed in the rendering process.
For the DOF effect, they used a blending technique while

The wavefront tracing operation in our method is accompanied by a novel technique which we call conoid tracing. A
similar idea was introduced to synthesize caustics caused by
reflection or refraction from curved objects [Elb94, MH92].
In that method, the spread of rays are traced using their actual
size because caustics can spread light very rapidly depending
on the curvature of the reflective or refractive object.
In contrast, in our application defocusing phenomena are
simulated and the spread of the light ray is much less rapid.
We employ the assumption that the width of the cone-shaped
ray is small enough that its intersection region with the
refractive surface can be regarded as being planar. This
linear approximation works for the human eye defocusing
problem and its correction by moderately curved eyeglass
lenses.
3. Overview of Pre-Computation and Rendering
Our eyeglass simulator is divided into two processes, precomputation and rendering. The pre-computed simulation
process produces an intermediate data set which contains
spatial distributions of blurring information over the field
of view. The rendering process generates a blurred image by blending iterated rendering results while displacing
each vertex in the view according to the intermediate data
set.
Sections 3.1 and 3.2 introduce wavefront tracing, a major
tool for the simulation. Section 3.3 describes the blurring
(defocusing) simulation sequence used in previous work, and
Section 3.4 explains the rendering algorithm.
3.1. Variables of wavefront tracing
Wavefront tracing [Kne64, Sta72] is a common method used
to mathematically analyse a travelling light ray. The method

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2429

M. Kakimoto et al. / An Eyeglass Simulator Using Conoid Tracing

Refraction
In refraction, the transformation is given by Snell’s Law,
denoted using these five variables. The physical boundary
between the different media, which is usually the lens surface, is also described by the five variables at the refraction
point.
The wavefront normal vector (ray direction) N is transformed as
Figure 1: Five variables in wavefront tracing.

N = μN + γ N(s) ,

traces the following five variables along a given ray from the
light source all the way to its destination:
• wavefront normal vector N
• principal curvatures κ 1 , κ 2
• principal directions e1 , e2
Figure 1 illustrates the variables. Here, N, e1 and e2 are
unit vectors always perpendicular to one another. The curvatures κ 1 and κ 2 are defined as the reciprocals of the corresponding radii of curvature at the intersection of the ray and
the wavefront. They are positive when the centre of curvature is in front of the wave (when the wave is converging),
and negative when the centre is behind the wave (diverging).
The wavefront is planar when κ 1 = κ 2 = 0, spherical when
κ1 = κ2 = 0, inwardly cylindrical when κ 1 > 0 and κ 2 = 0,
outwardly cylindrical when κ 1 = 0 and κ 2 < 0 and at a saddle
point when κ 2 < 0 < κ 1 .

(2)

where N(s) is the unit normal vector of the refractive surface and μ = nn12 , γ = −μ cos ϕ + cos ϕ . n1 and n2 are the
indices of refraction of the media before and behind the surface, respectively, ϕ is the angle of incidence, and ϕ is the
angle of refraction.
For curvature transformations, we use another coordinate
system formed by (u, v, N) instead of (e1 , e2 , N), where u
is tangent both to the incident wavefront and to the surface,
that is u ∝ N × N(s) , and v is another unit vector defined
by v = N × u. These two unit vectors can also be denoted
as
u = cos ωe1 − sin ωe2 ,

v = sin ωe1 + cos ωe2 , (3)

where ω is the angle between u and the first principal direction e1 . We obtain directional curvatures κ u along u and κ v
along v by the following formula:
κu = κ1 cos2 ω + κ2 sin2 ω,
κv = κ1 sin2 ω + κ2 cos2 ω,

(4)

κuv = (κ1 − κ2 ) cos ω sin ω,
where κ uv is the torsion in direction u.

3.2. Wavefront operations

By refraction, these variables are transformed as

Wavefront tracing operations transform the five variables in
the situations shown later.

κu = μκu + γ κu(s) ,
cos2 ϕ
1
+ γ κv(s)
,
cos2 ϕ
cos2 ϕ
cos ϕ
1
(s)
= μκuv
+ γ κuv
,
cos ϕ
cos ϕ

κv = μκv

• Transfer
• Refraction

κuv

• Vergence conversion
• Reflection

Transfer
By a wave transfer with distance d, curvatures are converted
as follows:
κ1
,
κ1 =
1 − dκ1

κ2
κ2 =
.
1 − dκ2

Here, the vectors N, e1 and e2 do not change.

(1)

(5)

(s)
where κu(s) , κv(s) and κuv
are, respectively, the directional curvatures and the torsion of the refractive surface in direction
u at the incident point. They can be computed using Equation (4) similarly to the wavefront. To represent the wavefront
after refraction, we need to find the principal curvatures and
directions by inverting Equation (4).

κ1 = κu cos2 ω + 2κuv cos ω sin ω + κv sin2 ω,
κ2 = κu sin2 ω − 2κuv cos ω sin ω + κv cos2 ω,

(6)

2κuv
.
tan 2ω =
κu − κv

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2430

M. Kakimoto et al. / An Eyeglass Simulator Using Conoid Tracing
Table 1: The sequence of conventional defocus simulation for each
object point.

Figure 2: Vergence formula operation for a non-spherical
wave and the eye as a thin toric lens with directional refractive powers.

Step

Place or media

1

Eye to sample point

Ray generation

2

Sample point

Wave initialization

3

Air

Transfer

4

Eye-

Front face

Refraction

5

glass

Glass

Transfer

6

lens

Back face

Refraction

7

Air

8
9

The refraction transformation equations are also outlined
in [MH92, LSS98], and are fully described in references
[Kne64, Sta72].
Vergence conversion
The basic vergence formula is a simple, fundamental lens
equation assuming a thin lens (single refraction) and is formulated as
V +P =V ,

Operation

Equations

(1)
(2)(4)(5)(6)
(1)
(2)(4)(5)(6)

Transfer
Thin lens

Eye

(1)

Accommodation
Vergence

(4)(8)(6)

Pupil and
10
11

retina
Sample point

Spread evaluation
Scaling

Note: Steps 2–11 are executed for each voxel, that is the iteration
complexity for voxel size n (in one dimension) is O(n3 ). Step 1 is
O(n2 ) because a ray is shared by voxels with the same x, y indices
and different depths.

x-axis and y-axis directions. Then Equation (4) is extended
as follows:

(7)
κx = μκx +

Px
,
n2

κy = μκy +

Py
,
n2

κxy = μκxy +

Pxy
.
n2

where V is the vergence of the incident light wave and V ≡
κn1 by definition, where κ is the wave curvature and n1 is
the refractive index of the media in the incident direction. P
is the refractive power of the thin lens. The unit of vergence
and refractive power is the dioptre (D), which is equivalent
to m−1 . V is the vergence of the outgoing light and V ≡
κ n2 . Because the optical system of the human eye is usually
characterized using its refractive power in ophthalmology,
the vergence formula is the most appropriate tool for the eye
simulation model.

This is a special case of Equation (4) with ϕ = ϕ = 0
and with refractive powers replacing surface curvatures. Finally, the principal curvatures of the outgoing wave shown in
Figure 2(c) are obtained using Equation (4).

For our eyeglass simulation, we extend the vergence formula and apply it to the refraction at the human eye. We
assume the light waves may be non-spherical and the lens
may be toric (an astigmatic eye) with arbitrary principal
directions. In addition, the light ray is assumed to be perpendicular to the lens because our purpose is to generate a
scanned foveal image and the eyeball always rotates toward
a target object point.

3.3. Wavefront tracing sequence in previous work

Figure 2 illustrates the incident light wave, a thin toric lens,
and the output wave, all of which are at the incident point.
The x-axis and y-axis lie on the lens plane and the z-axis
is coincident with the ray. Here, the coordinate conversion
described in Equation (4) is applied both to the incident
wave [Figure 2(a)] and to the toric lens refractive powers
[Figure 2(b)]. Consequently we obtain the wave curvatures
(κ x , κ y , κ xy ) and the refractive powers (Px , Py , Pxy ) in the

(8)

Reflection
Reflection can also be formulated by the wavefront variables [Sta72] but our method does not use wavefront
reflection.

Wavefront tracing can be used to simulate human eye defocusing phenomena. Loos et al. [LSS98] and Kakimoto
et al. [KTMN07] traced waves issued from each sampled
object point through an eyeglass lens all the way to the pupil,
and evaluated the circle or ellipse of confusion at the retina.
Table 1 indicates the sequence for each object point, showing
corresponding equations carried out for some of the steps.
In [LSS98], the sampling object points are the nearest
intersections corresponding to ray-traced pixels, whereas in
[KTMN07], each object point is sampled in each voxel of
subdivided field of view.

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2431

M. Kakimoto et al. / An Eyeglass Simulator Using Conoid Tracing
Table 2: The sequence of our wavefront and conoid tracing.

Step

Place or media

1

Eye to sample point

Ray generation

2

Retina
Vitreum

Transfer

(1)

Thin lens

Vergence

(4)(8)(6)

Pupil

Conoid initialization

Eye

5

In the latter framework, the subdivided field of view is
called Blur Field, which is an intermediate data set to be
referred to in rendering. A blur field is pre-computed using a
wavefront tracing sequence and, as a result, contains spatial
distributions of each voxel’s ellipse of confusion represented
by a 2 × 2 matrix. Our proposed method employs this framework but the wavefront tracing sequence is different from
that of Table 1. Section 4 describes the sequence.

Equations

3
4

Figure 3: The vertex shader operation in a rendering pass.
A different pass uses a different displacement vector.

Operation

Wave initialization

6

Air

7

Eye-

Transfer

8

glass

Glass

9

lens

Front face Refraction

Back face Refraction
Transfer

10

Air

Transfer

11

Sample point

Accommodation

(9)
(1)(10)
(2)(4)(5)(6)(11)
(1)(10)
(2)(4)(5)(6)(11)
(1)(10)
(12)

Note: The complexity is O(n2 ) for step 1, O(n0 ) for steps 2–5, O(n2 )
for steps 6–9, and O(n3 ) for steps 10 and 11.

4.1. Motivation of conoid tracing
3.4. Blurred image generation using a blur field
In using graphics hardware, a blurred image is commonly
generated by blending multiple rendering pass results while
displacing the camera position within some specific region.
On the other hand, our rendering process displaces every
vertex position within the corresponding ellipse of confusion
looked up in the blur field [KTMN07].
As many displacement vectors as the blending images are
pre-defined to be evenly distributed in a unit circle. Each rendering pass uses a corresponding displacement vector while
the vertex shader transforms the vector according to the vertex position using the blur field and applies the transformed
displacement to the vertex. Figure 3 is a diagram of the
vertex shader operation in a rendering pass.
By blending rendered images with displaced vertices, we
get each object point to be blurred within the ellipse of confusion associated with its distance and direction from the eye.
Thus, we obtain a blurred image which takes the human eye
defocusing simulation result into account.
4. Reverse Wavefront Tracing and Conoid Tracing
Our eyeglass simulator employs the rendering algorithm described in Section 3.4. The blur field data structure is identical
to the previous method but the pre-computation sequence is
different and a new concept, conoid tracing, is introduced.
The basic idea is to put a point light source at the central
fovea of the retina and to trace the wave towards a sampled
object point. The tracing direction is opposite to the conventional wavefront tracing sequence. This section describes the
detailed process of the sequences in our proposed method.

The previous method computed the output ellipses of confusion at the retina (screen space) but they need to be applied
to the vertices in the world space. The scaling operation (step
11 in Table 1) between the two spaces appears to be simple.
However, since the eyeglass lens refracts the viewing ray, the
scaling is not a trivial problem. Kakimoto et al. computed
the scaling ratio using the incident and refracted angle of the
ray in both the horizontal and vertical directions [KTMN07].
This approximation occasionally causes a noticeable amount
of errors when the incident angle is large. In addition, the
scaling operation approximates the thick lens refraction with
a single refraction, which may make the error greater. Our
conoid tracing is a set of more elaborately traced scaling
operations and computes the spread of light at each voxel
without such an error.
Another advantage of our method is better computational
complexity. To compute the blur field values, one needs to
evaluate the ellipse of confusion as many times as the total
number of voxels, which is typically 32 × 32 × 256. In the
previous method, almost all steps (presented in Table 1) are
carried out for each voxel, which means the computational
complexity is O(n3 ). In our new method, only the last few
steps are O(n3 ), the majority of the time-consuming steps
are O(n2 ), and some others are executed only once. Even
if we have additional conoid tracing operations, the total
performance is several times better.
4.2. Reverse wavefront tracing sequence
Table 2 shows our proposed sequence of wavefront and
conoid tracing operations. The basic idea is to save the

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2432

M. Kakimoto et al. / An Eyeglass Simulator Using Conoid Tracing

Figure 4: A schematic of the eye and a wavefront tracing
ray. Operations from the wave source (retina) to the pupil
are carried out only once and the intermediate result at the
pupil is shared by the rest of the sequence.

Figure 6: Conoid tracing sequence.
glass lens surface at a specific refraction point is calculated
on the fly using barycentric interpolation from the three vertices forming the smallest triangle that contains the refraction
point. The source of the eyeglass lens mesh is a set of freeform surfaces from CAD software and we obtain accurate
per-vertex curvatures.

Figure 5: A wavefront tracing ray shared by voxels connected in the depth direction.

operations by tracing the wave from the retina toward the
sampled object point.
We need to trace the ray from the retina to the pupil only
once (Figure 4), and the intermediate result can be shared by
the rest of the tracing operations for each voxel. The reason
why only one operation is sufficient for the eyeball is the
fact that the eyeball rotates to each target voxel when the eye
accommodates or tries to focus on the target. Because what
we require is a scanned foveal image, it is convenient for the
new method that the eyeball rotates.
Similarly, we need to trace the wavefront from the pupil
to the front surface of the eyeglass lens O(n2 ) times, not
O(n3 ) times, and each result is shared by each line of O(n)
voxels connected in the depth direction. Figure 5 illustrates
a ray shared by in-line voxels.

4.3. Input data
The input data for computing the blur field are an eye model
(Figure 4) including ophthalmological parameters, an eyeglass lens model (Figure 5) consisting of a mesh of vertices
including curvature information, and the positions of these
two models.
The eye model is optically a single thin lens, whereas the
spectacle lens model is a thick lens. The curvature of the eye-

A number of opthalmologic parameters [GvH09] are associated with the eye model. By changing appropriate parameters, one can simulate ophthalmic symptoms such as
myopia (near-sighted), hyperopia (far-sighted), presbyopia
(smaller adjustment range of accommodation due to aging)
and astigmatism (oval cornea lens).
The human eye tries to focus on target objects at the end of
the line of sight by changing its refractive power according
to the distance to the target. To take this accommodation
mechanism into account, we trace the wavefront twice with
a pair of minimum (relaxed) and maximum (tense) refractive
powers. Section 4.5 describes how to use the dual result to
estimate the best accommodation.
4.4. Conoid tracing
The major cause of defocusing is the pupil aperture. We
assume that the shape of the aperture is a circle. When the ray
being traced from the retina reaches the centre of the pupil, a
circular light spread is created. The spread is traced together
with the wavefront in the rest of the tracing sequence.
We call this series of additional operations accompanying
the wavefront tracing conoid tracing. A conoid is a coniclike shape with elliptical, linear or circular cross-sections.
Figure 6 is an illustration of a conoid tracing sequence for
an optical system consisting of an eye and an eyeglass lens.
Tracing variables and their initialization
The variables of conoid tracing are the semi-major and semiminor axes of the elliptic cross-section that is perpendicular
to the ray. The directions of the axes are coincident with the
principal directions of the wavefront, e1 and e2 . Initially, at the

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

M. Kakimoto et al. / An Eyeglass Simulator Using Conoid Tracing

2433

To compute Equation (4), we define a coordinate system
for e1 , e2 , e1 and e2 with the origin at the refraction point and
the z-direction normal to the boundary between the media.
Figure 7 right illustrates how the ellipsoidal spread changes
in refraction at the media boundary.
We regard the lengths of the axis to be sufficiently small
so that we can assume the media boundary is planar near the
refraction point. In other words, the pupil size is sufficiently
small that the curved eyeglass surface can be regarded as
planar in the region it crosses the conoid. Higher order shape
information in refraction is tracked as the curvature of the
outgoing wavefront, which will be used to compute ε1 and
ε2 when the wavefront proceeds after refraction.
Figure 7: Conoid tracing operations. Left: Transfer. Right:
Refraction occurring at a media boundary.
pupil, the light spread is a circle but just after the wavefront
moves forward, the spread becomes an ellipse of confusion if
the given eye is astigmatic. In conoid tracing, we regard the
two axis lengths as sufficiently small, and denote these semimajor and semi-minor axes using ε1 and ε2 (signed values).
When the wavefront goes out from the pupil, we initialize
these variables as
ε1 = ε2 = rp ,

Vergence and reflection operations
In contrast to the wavefront curvature, which changes in the
case of vergence [Equation (4)] or reflection [Sta72], conoid
tracing variables do not change. Because the incident angle
and the outgoing angle are identical in these situations, the
incident and outgoing shapes of the elliptic cross-sections
are congruent. In any case, our eyeglass simulation application does not use conoid tracing operations of vergence or
reflection.

(9)

where r p is the radius of the pupil.

4.5. Conoid evaluation taking accommodation
into account

Transfer operation

As a result of conoid tracing, we obtain two final elliptical
shapes at each sampling point, one with no accommodation
(relaxed eye lens) and the other with maximum accommodation (tense eye lens). These have two pairs of semi-major
and semi-minor axes (εR1 , εR2 ) and (εT1 , εT2 ).

In transferring the wavefront, ε1 and ε2 vary according to the
following equations, assuming we already know the principal curvatures before and after the transfer operation of
wavefront tracing.
ε1 =

κ1
ε1 ,
κ1

ε2 =

κ2
ε2 .
κ2

(10)

Figure 7 left shows how the spread varies in the wave transfer
operation.
Refraction operation
The transformation of the axes in ray refraction is a little more
complicated. Using the variable transformations E1 ≡ 1/ε12
and E2 ≡ 1/ε22 , the variation in refraction is denoted by the
following equation:
E1
E2

=

e 21x

e 22x

2
1y

2
2y

e

e

−1

2
e1x

2
e2x

E1

2
e1y

2
e2y

E2

, (11)

where e1x and e1y are, respectively, the x and y components
of the principal vector e1 , and e2x and e2y are those of e2 .
We already know the values of e1 and e2 from the wavefront
refraction operation at the same point. The signs of ε1 and ε2
are identical to those of ε1 and ε2 , respectively.

If we introduce a conceptual plane formed by the two
scalar values ε1 and ε2 , the best accommodated result is on
the segment that connects two points εR = (εR1 , εR2 ), εT =
(εT 1 , εT 2 ) on the plane.
Let us first think about the naked eye (no spectacles lens)
with no astigmatism (spherical lens), in which case the result always satisfies ε1 = ε2 . Figure 8 illustrates the circular
spread shapes changing according to the distance from the
cornea for the two cases, relaxed and tense. The best accommodated (smallest) spread is selected from between the
relaxed and tense circles. Figure 9 shows geometrical representations of three cases of how the best spread is chosen.
Astigmatic eyes or toric (non-spherical) eyeglass lenses
have different refractive powers according to the direction
of the axis lying on the surface. In general, the outgoing
rays or beams from such lenses do not converge to a point.
Instead, they produce two focal lines; anterior and posterior
focal lines. In this situation, it is a known fact that the human
eye adjusts its refractive power so that the point of gaze lies
where the beam forms a circular disc called the ‘circle of

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2434

M. Kakimoto et al. / An Eyeglass Simulator Using Conoid Tracing

Figure 11: Choices of the best accommodated ellipsoidal
spread (εBest1 , εBest2 ). The point that is closest to the condition
of the circle of least confusion (line ε 1 = −ε2 ) is chosen
between εR and εT .
Figure 8: Evaluation of circular conoids (i.e. cones) produced by non-astigmatic eyes or lenses. The thin ellipses are
comprehensible depictions of the side views of sample crosssection circles. The best accommodated result is selected
from between the tense and the relaxed conoids.

that is closest to line ε1 = −ε2 (the condition of COLC) is
chosen from between the two points εR and εT . As a result,
we obtain the semi-major and semi-minor axes of the best
accommodated ellipse of confusion as
εBest1 = (1 − t)εR1 + tεT 1
εBest2 = (1 − t)εR2 + tεT 2

(12)
εR1 + εR2
,1 .
t = min max 0,
εR1 + εR2 − (εT 1 + εT 2 )

Figure 9: Geometrical representations of three cases of how
the best accommodated spread is chosen for a circular spread
case. The point that is closest to the origin is chosen from
between εR and εT .

The above axis lengths and the principal directions computed by wavefront tracing define a final ellipse of confusion
in the world space. The ellipse is transformed into the normalized device coordinate (NDC) system and stored as an
equivalent 2 × 2 matrix for each voxel subdividing the view
volume in NDC. The matrices are stored as four-component
floating point 3D texture and one of them is retrieved to transform displacement vectors in the vertex shader, as shown in
Figure 3.

5. Simulation Results and Performance
5.1. Visualization of ellipses of confusion

Figure 10: The circle of least confusion found in a conoid.
The whole shape is known as the Sturm’s Conoid.

least confusion’ (COLC), which exists at a place between
the two focal lines. Figure 10 illustrates how the circle of
least confusion is formed.
The condition for the circle of least confusion is ε 1 =
−ε2 as shown in Figure 10. The geometric representations
in Figure 11 are the three cases of choosing the best accommodated spread from a conoid. In the ε 1 −ε2 plane, the point

Figure 12 presents a conoid tracing result. We visualized
the ellipses of confusion at three 32 × 32 layers out of a
32 × 32 × 256 blur field data set. Each layer is parallel
to the projection plane and their distances from the eye are
27, 78 and 340 cm. The eye model was set to have myopia of mild degree (+1.0 dioptre or +1.0D) and strong
presbyopia with 0.5D accommodation ability, which corresponds to age 70. The progressive lens model is basically
concave with −1.0D refractive power and has +3.0D power
of accommodation (made convex) added in the lower central
part of the surface. At the near distance (27 cm) the view
is well focused (very small ellipses of confusion) around a
small region in the lower centre of the lens, and at the far
distance (340 cm) the view is focused in the upper central
region. The scale of the visualized ellipses of confusion in
the figure is exaggerated to be three times as large as the
true ellipse scale. Figure 13 demonstrates rendering results

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

M. Kakimoto et al. / An Eyeglass Simulator Using Conoid Tracing

2435

Figure 12: Layers of a conoid tracing result. The layer distances from the eye are 27, 78 and 340 cm from left to right.

Figure 13: Interactive simulation scenes for which an eyeglass wearer is trying to focus on the slide screen using different parts
of a progressive lens by rotating the head and counter-rotating the eyeball. For each sample place in the field of view associated
with the lens, an ellipse of confusion (EOC) has been pre-computed using conoid tracing. While in rendering, every vertex in
the scene is blurred within its nearest EOC.

results. We selected a layer with greater ellipses of confusion
to reveal the difference.

5.3. Pseudo-colour visualization of the amount
of defocusing on the lens surface

Figure 14: Quality comparison in case of a wide field of
view. The upper left quarter of each images is shown.

using the pre-computed conoid tracing result with a similar
condition.

5.2. Quality
In eyeglass simulation, the field of view angle is relatively
large (around 100◦ ). The scaling operation in the previous
sequence shown in Table 1 (step 11) causes noticeable errors
typically near the edge of the eyeglass lens, where the incident
angles of the rays are large. Figure 14 shows a comparison of

In evaluating spectacle lenses in the design process, it is
crucial to check the optical performance from a quantitative
standpoint. A widely used means for lens evaluation is overlaying a pseudo-colour map to visualize the distribution of a
specific static metric, such as the refractive power.
To take advantage of our defocus rendering method, we
realized a dynamic defocus map on the surface of the lens
model. We used a metric which indicates the diagonal length
2|εBest | of the ellipse of confusion retrieved at the ray–scene
intersection for each pixel. Figure 15 presents defocus maps
of four different types of progressive lens, (a) wider nearportion, (b) occupational (mid-near), (c) regular progressive
and (d) wider far-portion. The whiter translucent regions have
stronger defocusing, and the transparent regions have distinct
vision.
According to a professional spectacle lens design team,
judging from the visualized ellipses and the defocus map,

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2436

M. Kakimoto et al. / An Eyeglass Simulator Using Conoid Tracing

Figure 15: Distributions of defocus strengths over the surfaces for four different types of spectacle lens.
Table 3: Comparison of pre-computation time between our method (Table 2) and our implementation of the previous method (Table 1).

Model-1

Model-2

Voxel size

Ours

Previous

Ratio

Ours

Previous

Ratio

32 × 32 × 64
32 × 32 × 128
32 × 32 × 256
64 × 64 × 128
64 × 64 × 256

0.24
0.27
0.32
1.03
1.30

1.81
2.00
2.39
5.10
6.55

7.5 (5.1)
7.4 (5.2)
7.5 (5.4)
5.0 (4.5)
5.0 (4.6)

0.24
0.26
0.32
1.00
1.29

1.59
1.83
2.23
4.76
6.28

6.6 (4.6)
7.0 (4.8)
7.0 (5.0)
4.8 (4.3)
4.9 (4.5)

Note: The common ray generation step (step 1 in Tables 1 and 2) is not included in the timings. Ratios in parentheses are the results including
the common step. The units are seconds except for the ratios.

the quality and accuracy of the conoid tracing results are
good enough to verify the optical performance of the lens
under design.

in Section 1), it is for now impossible to acquire ‘ground
truth’ images with which we can compare the rendered image using MTF. Camera capturing of scanned foveal image
is impossible because there exists no optical apparatus which
can mimic accommodation of the human eye.

5.4. Performance
In Table 3, the wavefront and conoid tracing pre-computation
sequence that this paper proposes is compared to our implementation of a previous method (Table 1). Our method is four
to more than seven times faster than the previous method for
all the scenes and conditions we tested.
The rendering time with blur field defocus information
and per-vertex accurate refraction was 30–50 fps using a
2.66 GHz CPU and GeForce 8800 Ultra for a 12K polygon
scene with a 1280 × 1024 screen size.
The simulator process consumed 0.8 GB memory when
using 64 × 64 × 128 voxels and 1.4 GB for 64 × 64 × 256
voxels.

6. Discussions
Although we have shown some metric-based visualization results of conoid tracing, it is desirable for a simulation method
in general to have quantitative measures for its verification.
One of the common quantitative measures for defocused images is modulation transfer function (MTF). However, because our target is to render scanned foveal images (explained

As for rendering, we employed a multipass and blending
method and its performance directly depends on the number
of blended images. For higher performance, we may need to
consider a GPU-based per-pixel algorithm for scattering or
gathering instead of multipass rendering. However, per-pixel
depth of field methods suffer from artefacts such as pixel
bleeding and depth discontinuities [Dem04]. Our multipass
and blending method uses NDC space, per-vertex jittering
of geometric models in the scene. This method fully maintains the 3D relationship and the subsequent z-buffer hidden
surface removal works perfectly, and thus is free from the
common DOF artefacts.
On the other hand, there are some drawbacks in per-vertex
jittering. One of them is that the models in the scene need
to be tessellated with granularity similar to the voxel size, or
the blur field sampling interval, which is a painful restriction
for modelling designers.

7. Conclusion and Future Work
We have presented a method to quickly pre-compute spatial
distributions of the amount of defocusing, taking into account

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

M. Kakimoto et al. / An Eyeglass Simulator Using Conoid Tracing

the optical conditions of the human eye and the eyeglass lens
which corrects the visual acuity. We developed a novel algorithm called conoid tracing. It re-creates the behaviour of
a ray emanating from the central fovea of the retina, going
through the eye lens, the circular pupil, the eyeglass lens,
and reaching sampling points in the view volume. Using the
pre-computed defocus distribution, our eyeglass simulator
can render in near real-time the image that the eyeglass user
would see using progressive lenses consisting of complex,
subtle curved surfaces. The system also provides lens designers with a means to verify the optical characteristics of
the lens under design using conoid tracing results. The simulator is actively being utilized in an eyeglass lens design
company. They compared the simulator with their existing
evaluation tool and judged that the approximations of our
method have no problem for lens design verification.
Future work includes pursuing more applications of conoid
tracing (e.g. caustics), multi-threading or GPU implementation of wavefront and/or conoid tracing, accuracy analysis of
conoid tracing on refraction and per-pixel defocus rendering
without any depth related artefacts. As for rendering with pervertex jittering, it is preferable to get around the limitation
that a large polygon size affects the image quality.
References
[Bar04] BARSKY B. A.: Vision-realistic rendering: simulation of the scanned foveal image from wavefront data of
human subjects. In Proceedings of the APGV ’04 (Los
Angeles, CA, USA, 2004), pp. 73–81.
[CPC84] COOK R. L., PORTER T., CARPENTER L.: Distributed
ray tracing. In Proceedings of the SIGGRAPH ’84 (Minneapolis, MN, USA, 1984), pp. 137–146.
[Dem04] DEMERS J.: Depth of field: a survey of techniques.
In GPU Gems. R. Fernando (Ed.). Addison-Wesley, Upper
Saddle River (2004), pp. 321–334.
[Elb94] ELBER G.: Low cost illumination computation using an approximation of light wavefronts. In Proceed-

2437

ings of the SIGGRAPH ’94 (Orlando, FL, USA, 1994),
pp. 335–342.
[GvH09] GULLSTRAND A., VON HELMHOLTZ H.: Handbuch der
Physiologischen Optik. 1909, p. 335.
[HA90] HAEBERLI P., AKELEY K.: The accumulation buffer:
hardware support for high-quality rendering. In Proceedings of the SIGGRAPH ’90 (Dallas, TX, USA, 1990),
pp. 309–318.
[Kne64] KNEISLY J. A.: Local curvature of wavefronts in an
optical system. Journal of the Optical Society of America
54, 2 (1964), 229–235.
[KTMN07] KAKIMOTO M., TATSUKAWA T., MUKAI Y., NISHITA
T.: Interactive simulation of the human eye depth of field
and its correction by spectacle lenses. Computer Graphics Forum (Proc. Eurographics 2007) 26, 3 (2007), 627–
636.
[LKC08] LEE S., KIM G. J., CHOI S.: Real-time depth-offield rendering using point splatting on per-pixel layers.
Computer Graphics Forum (Proc. Pacific Graphics 2008)
27, 7 (2008), 1955–1962.
[LSS98] LOOS J., SLUSALLEK P., SEIDEL H.-P.: Using wavefront tracing for the visualization and optimization of progressive lenses. Computer Graphics Forum (Proc. Eurographics 1998) 17, 3 (1998), 255–263.
[MH92] MITCHELL D., HANRAHAN P.: Illumination from
curved reflectors. In Proceedings of the SIGGRAPH ’92
(Chicago, IL, USA, 1992), pp. 283–291.
[MKL97] MOSTAFAWY S., KERMANI O., LUBATSCHOWSKI H.:
Virtual eye: retinal image visualization of the human
eye. IEEE CG&A 17, 1 (January–February 1997), 8–
12.
[Sta72] STAVROUDIS O. N.: The Optics of Rays, Wavefronts,
and Caustics. Academic Press, New York and London,
1972.

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

