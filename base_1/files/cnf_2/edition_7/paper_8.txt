DOI: 10.1111/j.1467-8659.2008.01299.x

COMPUTER GRAPHICS

forum

Volume 28 (2009), number 1 pp. 101–113

BTF-CIELab: A Perceptual Difference Measure for Quality
Assessment and Compression of BTFs
Michael Guthe1 , Gero M¨uller2 , Martin Schneider2 and Reinhard Klein2
1 Graphics

and Multimedia Programming, Universit¨at Marburg, FB 12, Marburg, Germany
Bonn, Institute for Computer Science II, Bonn, Germany
guthe@informatik.uni-marburg.de

2 Universit¨
at

Abstract
Driven by the advances in lossy compression of bidirectional texture functions (BTFs), there is a growing need for
reliable methods to numerically measure the visual quality of the various compressed representations. Based on the
CIE E 00 colour difference equation and concepts of its spatio-temporal extension ST-CIELab for video quality
assessment, this paper presents a numerical quality measure for compressed BTF representations. By analysing
the BTF in its full six-dimensional (6D) space, light and view transition effects are integrated into the measure.
In addition to the compressed representation, the method only requires the source BTF images as input and thus
aids the objective evaluation of different compression techniques by means of a simple numerical comparison. By
separating the spatial and angular components of the difference measure and linearizing each of them, the measure
can be incorporated into any linear or multi-linear compression technique. Using a per-colour-channel principal
component analysis (PCA), compression rates of about 500:1 can be achieved at excellent visual quality.
Keywords: perceptual graphics, image based rendering
ACM CCS: G.1.0 [Numerical Analysis]: Error Analysis; I.3.7; [Computer Graphics]: Three-Dimensional Graphics and Realism; I.4.9 [Image Processing and Computer Vision]: Applications I.4.10; [Image Processing and
Computer Vision]: Image Representations

well-known facts from the field of image and video quality
assessment which show that this error metric does not even
loosely correspond to the visual quality as perceived by a
human observer [Win05]. This fact can clearly be seen in the
upper three images of Figure 1. While very accurate methods for image and video quality assessment exist, that are
based on properties of the human visual system (HVS), they
cannot be utilized because BTFs are not a set or a temporal series of images, but light- and view-dependent textures.
As the change of compression artifacts between contiguous
light and view directions needs to be taken into account, a
standard image-wise metric cannot deliver accurate results,
as shown in the lower three images of Figure 1.

1. Introduction
Realistic visualization of materials is a very active field of
computer graphics. Due to the increasing amount of memory
on consumer graphics cards, image-based material rendering
has become feasible. The idea is to replace the traditional analytic reflection function with photographs. Instead of modelling bidirectional reflectance distribution function (BRDF)
parameters, a set of pictures is taken to sample the apparent BRDF for each point on the surface [DvGNK99]. The
resulting bidirectional texture function (BTF) can be used
like a texture and is a six-dimensional (6D) function of texture coordinate x, incident light direction ω i and reflected
light direction ω r . Although several BTF compression techniques were developed, the visual quality is still assessed by
calculating the root-mean-square (RMS) of the pixel-wise
red/green/blue (RGB) L2 difference between the original and
reconstructed BTF. This common practice however ignores
c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and
Blackwell Publishing Ltd. Published by Blackwell Publishing,
9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main
Street, Malden, MA 02148, USA.

To derive a quality measure for BTFs, their final usage as
texture maps in interactive computer graphics needs to be
considered. This implicates mapping onto curved surfaces
where changes in light and view direction map to spatial

101

Submitted November 2007
Revised March 2008
Accepted September 2008

102

M. Guthe et al. / BTF-CIELab

2. Related Work

Figure 1: The upper images demonstrate the unsuitability of
the RGB L2 difference for image quality assessment: the left
and right image have equal RMS difference to the original
one in the middle. The lower images exhibit the importance
of light and view transition effects: left and right have equal
light/view sample-wise difference to the middle one.
distance, as well as movements of the viewer and the light
source relative to the local coordinate system of the BTF.
Therefore, both the spatial and the temporal effect of differences between contiguous view and light directions need to
be accounted for in the quality measure. Despite arbitrary
mapping, curvature and movement speed, reasonable viewing condition ranges are also required for a numerical comparison of different BTF representations. Finally, it should
also be possible to use the derived metric to optimize the visual quality during compression of a BTF. The contributions
of this paper directly follow from these requirements:
1.

2.
3.
4.

A general visual model interpreting the angular transition effect such that both spatial and temporal interactions are taken into account which makes the model
applicable to still images and animations generated from
the BTF.
An accurate approximation of the general model that
allows for an efficient calculation of the visual quality.
A linearization of the model to make it applicable to any
linear or multi-linear compression technique.
The definition of a range of viewing conditions that
covers all reasonable texture mapping and viewing configurations.

First we derive the general visual model for BTF perception. In Section 4, we then build an algorithm for quality
assessment of compressed BTF on top of that model. In addition, we show how the model can be adapted to improve
the visual quality of state-of-the-art BTF compression techniques (Section 5) while increasing the compression rate.
Finally, in Section 6 a user study is performed to validate
the proposed quality metric and the modified compression
technique is compared to other recent approaches.

Many experiments have confirmed that simple pixel-wise
metrics in the RGB colour space, such as RMS difference,
do not correlate well with human perception and thus are
not suitable for image quality assessment. As a consequence,
much effort has gone into the development of quality assessment methods that incorporate the properties of the HVS.
These properties fall into three categories: the HVS has a very
characteristic contrast perception between colours, this contrast perception depends on the spatial frequency, and it also
changes with the temporal frequency. Current vision models can be subdivided into single- and multi-channel models.
The first ones regard the HVS as a single spatio-temporal
filter whose characteristics are defined by a contrast sensitivity function (CSF). In contrast to that, the latter ones assume
that each band of spatio-temporal frequencies is processed
in a separate channel. The focus of this work is on singlechannel models as the complexity of multi-channel models
would result in enormous computation times for BTFs which
are not justified by the slightly better results. An overview
of multi-channel models and an introduction to video quality
assessment can be found in [Win05].
As the basic requirement for visual quality evaluations is
a colour difference measure that coincides with human perception, the CIELab (strictly CIE 1976 L∗ a∗ b∗ ) colour space
together with its associated colour difference equation CIE
E 76 was developed. As the CIELab colour space is not completely perceptually uniform, the colour difference formulae
CIE E 94 and CIE E 00 where published. However, solely
pixel-wise application of the CIELab colour difference as an
image quality metric does not provide satisfactory results.
Zhang and Wandell [ZW96] proposed a spatial extension to
CIELab, called S-CIELab, to account for the influence of
spatial patterns. An input image is converted into an opponent colour space with one luminance and two chrominance
components and each component image is passed through
a spatial filter that resembles the spatial sensitivity of the
human eye for that component. Finally, the filtered images
are transformed back into the CIELab colour space so that
the standard CIELab E colour difference formula can be
applied. As S-CIELab accounts only for the spatial sensitivity of the HVS, it is only suited for the comparison of still
images but not for the comparison of image sequences.
An extension to S-CIELab, called ST-CIELab, that
additionally accounts for human sensitivity to different
spatio-temporal frequencies was proposed by Tong et al.
[THvdBL99]. Again, a spatio-temporal filtering is applied
to simulate human contrast perception. Following Burbeck
and Kelly [BK80], the achromatic CSF is approximated
by the linear difference of two space–time separable filters. These excitatory and inhibitory filters are approximated
by two and three truncated Gaussians, respectively. For the
chromatic CSF, the same filters are used but the responses
are added instead of subtracted and the result is scaled to

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

M. Guthe et al. / BTF-CIELab

account for poor acuity of colour vision. Finally, the colour
differences are spatially and temporarily pooled to produce
the final quality rating in units of the just noticeable difference (JND). Although ST-CIELab does not account for all
aspects of human pattern discrimination, it provides a conceptually simple and computationally efficient method for
evaluating video quality. Both methods, S-CIELab as well as
ST-CIELab, are backwards compatible to the CIELab standard, i.e. they reduce to CIELab for uniform colour fields.
The first approach to assess the quality of BTFs was made
by Meseth et al. [MMK∗ 06] who generated a set of images
from measured BTFs and compared them to photographs
showing the same scene. The comparison was performed
using both, numerical quality assessment methods, as well
as a user study. While it would be possible to assess the visual
quality of compressed BTFs in a similar way, it is hard to
construct a scene that captures all effects of any BTF.

2.1. BTF compression
A detailed overview of the state-of-the art in acquisition
and rendering of BTFs is provided by a recent SIGGRAPH
course [LGC∗ 05] and a Eurographics state-of-the-art report
[MMS∗ 05]. Early approaches interpreted the BTF as a set of
spatially varying BRDFs and fitted simple analytic functions
to the discrete BRDFs achieving impressive compression
rates, e.g. Lafortune lobe fitting [MLH02]. However, this
group of methods suffers from lack of rendering quality due
to the simplicity of the fitted analytic functions.
A group of methods with better quality was developed
in the context of pattern-recognition where the goal was to
model the statistical properties of BTFs. Each texel of a
BTF is considered as so-called apparent BRDF (ABRDF)
since it is in many respects similar to a BRDF, but e.g. not
necessarily reciprocal due to occlusion and parallax. These
data-driven approaches differ mainly within two respects:
how they align the elements of the BTF (full, per-texel, perview, per-cluster) and how they approximate them (linear,
multi-linear). The chained matrix factorization [SvBLD03]
is a per-texel method that is computed using a chained sequence of simple SVD factorizations each time with a different parametrization. The per-texel factorization methods
have the disadvantage that they do not exploit correlations
between ABRDFs. This can be accomplished by approximating the complete BTF-data arranged in a matrix. As full
BTF-matrix factorization requires an out-of-core principal
component analysis (PCA) algorithm, Sattler et al. [SSK03]
proposed to apply the PCA to slices of the BTF with fixed
view-direction. A more general, data-driven approach to
choose subsets for a PCA was proposed by M¨uller et al.
[MMK03], where the ABRDFs are clustered and each cluster is approximated separately. To exploit all correlations between the different dimensions, tensor approximations were
proposed. As there is no straightforward generalization of

103

Figure 2: Schematic overview of BTF perception. The output
is a representation of the BTF as perceived by a human
observer.
SVD or PCA to higher dimensions, several ways to decompose tensors were developed. Vasilescu and Terzopoulos
[VT04] proposed a method that approximates the BTF by
an N-mode SVD. An advantage of this approach is that the
number of components in each dimension can be adjusted
separately which allows a strategic dimensionality reduction.
2.2. Visual models and compression
For lossy image compression, properties of the human visual
system were exploited to only remove information that is
hardly perceived. The JPEG image format [Wal92], based
on a colour space conversion into YCbCr and a discrete
cosine transformation, is used very often to reduce the data
size on disk. Recently, the JPEG2000 standard [CSE00] was
introduced, that is build on a wavelet transformation followed
by quantization and arithmetic coding of the coefficients.
Wavelets were also used for compressed textures on programmable graphics hardware. As arithmetic coding cannot
be used, either vector quantization [SW03] or zero tree removal after arranging the wavelet coefficients in a quad tree
[DCH05] have been used. Both directly translate to higher
dimensions, but the compression rate at good visual quality
is too low for BTFs. A first approach trying to apply ideas
from image compression on BTFs by Ma et al. [MCC∗ 05] is
based on a Laplace transformation of the sample textures and
using eight luminance and four chrominance components for
each level. Although the compression rate is higher than for
previous methods, the visual quality is poor.
3. Perceptual Model
The main idea of the proposed perceptual BTF difference
measure is to simulate the contrast perception of the HVS
with respect to interactive applications and still images generated from a BTF. Figure 2 shows a schematic overview
of the model for BTF perception. The sample images of a
BTF are first transformed into the according cone response
levels in the human eye. Then, a contrast sensitivity function is applied to approximate the signal processing in the
retina. Afterwards, the processed BTF images represent the
perceived BTF that can be either used for quality assessment
or as a guidance during compression.
Although this is similar to real-time video stream quality assessment with ST-CIELab, a BTF has two major
differences: first, it contains four, typically irregularly

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

104

M. Guthe et al. / BTF-CIELab

sampled, angular dimensions instead of a single regularly
sampled temporal dimension and thus angular interactions
between the discrete sample textures have to be integrated
into the quality metric. Second, the mapping of the spatial dimensions onto the retina is not identical for all images since a
BTF is first mapped onto a surface and then onto the screen.

3.1. Angular interaction
In interactive applications, a main effect of the angular interaction is the change of the reflected light when either the light
source or the viewer are moving relative to the surface. In
contrast to a video stream, the light and view angles impose
four additional degrees of freedom, whereas the time is only
one-dimensional (1D). During interaction, light source and
viewer movement are time dependent and map onto a curve in
this four-dimensional (4D) space. When the BTF is mapped
onto a planar surface, this induces treating the sample textures as images of an animation and applying a video quality
metric to calculate the visual difference between compressed
and original BTF for any given movement curve. Although
a single temporal contrast frequency analysis in this space
cannot directly calculate the frequencies along any possible
curve, performing the analysis symmetrically in all four angular dimensions using a radially symmetric CSF yields a
very good approximation of the angular frequencies along
any movement curve. The rotation speed ω ang is the only
free parameter and thus the analysis has to be performed for
a set of distinct velocities. Another possible solution would
be a sampling of distinct lines in this 4D space. However,
even a very coarse sampling would lead to inhibitorily high
computation times.
In addition to the temporal effect, the mapping of the BTF
onto a curved surface needs to be considered. In this case,
a two-dimensional (2D) manifold is extracted from the 6D
BTF, which in addition to the spatial variation also leads
to angular variations of light and viewing directions. As for
the temporal analogy, the result of the difference calculation using radially symmetric filtering in all four angular
dimensions provides an estimate for the visual difference on
any 2D manifold. Despite the fact that the human temporal
CSF Kt (ft ), where ft is the temporal frequency, is slightly
lower than its spatial counterpart Ks (fs ) of the spatial frequency fs , they can be approximated by each other with
) [BK80] (see also Figure 3). The
Kt (ft ) ≈ Ks (ft · 1.5 deg
s
slight underestimation of the angular effect for still images is
no deficiency as it approximates the masking of the angular
effect by the higher frequencies of the spatial texture.
3.2. Projectional mapping
The projection of a parametrized surface onto the screen
scales its parametrization by a factor of cos θ r in the direction
of φ r , where φ r , θ r is the view direction (see Figure 4). When
a texture or BTF is isometrically mapped onto the surface, the

Figure 3: Relative spatio-temporal CSF according to
Burbeck and Kelly.

Figure 4: Directional scaling of a BTF sample image due
to projection.

same deformation is applied to the texture image. Directly
applying the CSF would not only require a resampling of
the reference BTF images, but also ignore the ability of the
HVS to track features and establish correspondences. These
are kept only if the original pixel-wise matching between the
sample images is preserved.
Instead of scaling and resampling the images, it is therefore
better to scale the frequencies by cos θ r , or the corresponding
filter function by cos1θr , to account for the projection. As this
scales the projected area, the contribution of each texel has
to be weighted with cos θ r .

3.3. Contrast sensitivity function
The new perceptual model can now be derived from any lower
dimensional spatio-temporal model. Due to the rather large
data size of BTFs, a simple and efficient model is required
and thus a single-channel model is used. More complex ones
incorporating directional frequency perception are not reasonable for textures or BTFs anyway since in contrast to a
video, the orientation is not known a priori. As basis for the
CSFs for quality assessment and compression, we use the
analytic function introduced by Burbeck and Kelly [BK80]
depicted in Figure 3. For the achromatic sensitivity they use
Ka (fs , ft ) = 4π 2 fs ft 6.1 + 7.3 log10

ft
3fs

e

−4π(ft +2fs )
45.9

.

This function can be separated into a spatial and temporal
filter by splitting it in an excitatory part E and an inhibitory

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

M. Guthe et al. / BTF-CIELab

105

one I with
Ka (fs , ft ) ≈ E(fs , ft ) − I (fs , ft )
SE (fs )
SI (fs )
− TI (ft )
= TE (ft )
SE (fs2 )
SI (fs2 )
TE (ft ) =
SE (ft ) =

c1 Ka (fs1 , ft )

:

ft ≤ ft 1

Ka (fs2 , ft )

:

ft > ft1

c2 Ka (fs , ft1 )

: fs ≤ fs1

Ka (fs , ft2 )

: fs > fs1

TI (ft ) = TE (ft ) − Ka (fs2 , ft )
SI (ft ) = SE (ft ) − Ka (fs , ft2 )
Ka (fs2 , ft1 )
Ka (fs1 , ft2 )
c2 =
.
c1 =
Ka (fs1 , ft1 )
Ka (fs1 , ft1 )
The chromatic sensitivity can be derived from the excitatory and inhibitory part by adding them together (as opposed
to the subtraction for the achromatic sensitivity) and then
scaling them down by a factor of 30 to account for the relatively poor colour acuity:
Kc (fs , ft ) ≈

1
(E(fs , ft ) + I (fs , ft )) .
30

The drawback of these functions is that they only describe
the sensitivity with respect to opponent colours but not the
perceived difference between large uniform areas of specific
colours. This is best described by colour difference metrics
like CIE- E 00 . Therefore, the CSF should reduce to CIEE 00 for large uniform areas. For the chromatic sensitivity,
this is a simple scaling; whereas for the achromatic one, the
sensitivity is capped to 100:1 below one cycle per degree.

Figure 5: Processing pipeline for visual difference calculation between compressed BTF and uncompressed reference.

second. Combined with the maximum speed, the interesting
interval is approximately [101 , 103 ] degrees per second.
For the spatial resolution rs , the minimum can also be de1
and the fact that
rived from the Nyquist frequency of 0.5 pixel
contrast perception is almost constant below one cycle per
degree. The minimum resolution is then two pixel per degree
or approximately 10◦ . The maximum spatial frequency depends on the size of the BTF samples in pixels. As usually the
resolution of any texture is chosen such that it does not comprise excessive oversampling, a maximum spatial resolution
of about 102 pixel per degree can be assumed, corresponding
to 4 × 4 texel per screen pixel. At higher resolutions, the total
contrast will be decreasing anyway since the high frequency
contrast is not perceived any more.

4. Quality Assessment
The quality assessment consists of three stages: first, the generation of images from the compressed BTF corresponding
to the original sample textures of the uncompressed BTF and
transformation into cone response levels, then the spatioangular filtering at different resolutions, and finally, the difference calculation between the filtered BTFs. Figure 5 shows
an overview of the overall processing pipeline.

3.4. Viewing conditions
Since BTFs are first mapped onto objects and then projected
on the screen, standard viewing parameters cannot be defined
for them. However, it is possible to derive a reasonable parameter range so that the computed quality is valid for any
normal viewing condition. The search for this range is aided
by the fact that one of the primary goals of a good compression algorithm is to reduce the maximum distortion, implying
that the range has to be chosen to contain the configuration
with the maximum total visible contrast.
As the BTF is sampled over the hemisphere, the minimum frequency is one oscillation over the circle with θ =
90◦ and φ = [0◦ . . . 360◦ ]. As the highest sensitivity is between 5 and 8 Hz, the perceived contrast is monotonously
decreasing for ω ang above 2880 degrees per second where
8 Hz is the minimum frequency. The maximum frequency
is the Nyquist frequency of the sampling rate, which is
7.5 degree for recent acquisition systems [LGC∗ 05]. As the
contrast perception does not change significantly below 1 Hz,
it will be almost constant for any ω ang below 15 degrees per

The view condition range can either be uniformly sampled
or the spatio-angular resolution pair with maximum difference can be calculated starting in the centre of that region and
using Levenberg-Marquardt optimization [PTVF02] to find
the parameters (ω∗ang , r ∗s ) where both derivatives of the root
mean square difference rms(ω ang , rs ) between original and
compressed BTF are zero. In this case, each iteration requires
the calculation of five resolution pairs. While the sample generation and cone response transformation are only required
once, filtering and difference calculation are required for each
spatio-angular resolution pair.

4.1. Cone response
The reconstructed sample textures can either be given as
a set of images corresponding to the uncompressed ones,
or as a high-level GPU-shader. In the latter case, the samples are generated using the shader and copied from the
frame buffer. The original and reconstructed sample textures
are transformed into cone response levels to simulate the

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

106

M. Guthe et al. / BTF-CIELab

signal processing inside the human eye. Before the CSF can
be applied, the cone response is transformed into the lαβ
opponent channels [RCC98] in which the retinal signal processing operates. Then, the transformed images are stored
on disk, as they are the input for all subsequent steps with
different spatial and angular resolutions.

Table 1: Weight and standard deviations (σ i in degrees and δ i in
seconds) of Gaussian convolution functions.

4.2. Spatio-angular CSF

Chromatic

As second step, the filtering to account for the frequency dependence of contrast perception is performed in the spatial
and angular dimensions with a set of a few resolutions for
each of them. For the angular filtering, a direct convolution
on the original light and view directions from the BTF acquisition is performed. This has the advantage that no error is
introduced by an interpolation between sample textures, that
would be necessary when using a frequency transformation,
i.e. spherical harmonics. Because a complete BTF typically
exceeds several gigabytes, this convolution needs to be separable into at least the two orthogonal sub-spaces ω r and
ωi .
One possibility to perform the frequency analysis is using
the four separated inhibitory and excitatory filters from the
ST-CIELab algorithm [THvdBL99] that are further separable
since they are sums of Gaussian functions and each of these
can again be separated. Applying this approach in the context
of BTFs has the drawback that the individual approximations
of all four filters require 15 temporary channels during the
angular filtering. Due to the huge storage requirements of
BTFs, a more compact approximation with a lower number
of separable components is required. As in previous work, a
Gaussian mixture model is used, but instead of approximating
each of the four filter functions separately, their combination
is approximated using a sum of six-dimensional multivariate
Gaussians with diagonal covariance matrices:
k(x, t) =

wi ei (x, t)

Stimulus

i

Achromatic

1
2
3
1
2

wi
2.05962
1.29338
−2.35300
0.67965
0.32035

σi
0.058406
0.038015
0.140856
0.096652
0.054198

δi
0.025135
0.054688
0.090229
0.069195
0.031830

human CSF with a mean relative difference to the original
CSF of approximately 9% for the achromatic and 11% for
the chromatic channels. The resulting parameters are listed
in Table 1.
First, the spatial filtering is performed as the projection
scales the spatial frequencies and thus first performing the
angular filtering would mix different spatial frequencies. After the spatial filtering, the angular filtering is performed in
two steps: first the convolution with respect to the view direction and then with respect to the light direction. Further
separation is neither necessary nor possible due to the nonradial kernels for the spatial dimensions and the irregular
sampling in the angular dimensions. Since only the Gaussian
functions, but not the complete filtering kernels can be separated into spatial and angular convolutions, each of the seven
Gaussian functions needs to be processed separately. In a
final step, all seven filtered channels are recombined into the
sample textures of the perceived BTF. While the convolution
for angular filtering needs to be performed explicitly due to
the irregular sampling, the convolution in image space can
efficiently be performed as multiplication in frequency space
after a Fourier transformation.

4.3. Psychometric difference

i
6

ei (x, t) =

√

2

πσi,j e

s
− j 2
σi,j

with s =

j =1

x
t

where x is the texture coordinate and t = (ω i , ω r ) the vector
of light and view directions in polar coordinates. As the
filtering is symmetric in the two spatial and the four angular
dimensions, the Gaussian function ei (x, t) becomes
ei (x, t) = π 3 σi2 δi4 e

−

x 2
σi2

2
+ t2
δ
i

To calculate the visual difference, the spatially and angularly
filtered sample textures are transformed from the lαβ into the
CIELab colour space and then compared using the CIE E 00
difference equation. This yields a per-texel visible difference
between compressed and original BTF. As final output three
statistical values are generated, namely mean, standard deviation, and RMS difference. This allows to compare different
BTF compression techniques by means of simple numerical
values.

.

The parameters wi , σ i and δ i were determined from the
measurement data of Burbeck and Kelly [BK80] and the
uniform contrast sensitivity determined by the CIE using
Levenberg-Marquardt optimization. Using three Gaussian
functions for the achromatic filter and two for the chromatic
provides a reasonable approximation of the spatio-temporal

4.4. Out-of-core processing
Due to the huge amount of data required during the spatioangular filtering, the results of each of the three filtering
stages need to be cached on disk. For a 2562 × 81 ×
81 BTF about 11.2 GB are required for the seven channels in
single precision floating point. To prevent stalling for disk i/o

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

107

M. Guthe et al. / BTF-CIELab

during the filtering operations, a background thread is used
to load the possibly already partially filtered BTF sample
images just before they are actually needed.

Table 2: Weight and standard deviations of the separated Gaussian
convolution functions.

To reduce disk access and storage requirements, each channel – both the three lαβ colour channels of the input images,
as well as the seven channels during spatio-angular filtering – is uniformly quantized. Using an eight bit quantization
instead of storing floating point values on disk reduces the
storage cost and thus increases the performance by a factor
of four. The relative quantization error of a uniform quantization to eight bit is bound by 2−(N+1) = 2−9 ≈ 0.2%, where
N = 8 is the number of bits [Poh05].

Stimulus

i

Achromatic

1
2
3

1.68025
1.31962
−1.99987

0.044888
0.116833
0.159155

1.43125
1.46968
−1.90093

0.028047
0.137456
0.159155

Chromatic

1
2

0.56684
0.43316

0.059308
0.113673

0.42810
0.57190

0.032472
0.074796

5. Quality Guided Compression
Due to the huge amount of data of measured BTFs, they cannot be directly used for real-time rendering. Therefore, different compression techniques capable of real-time decompression were proposed of which statistical methods like PCA or
tensor approximations. A conceptionally rather simple but
nevertheless effective compression technique for BTFs is a
full matrix factorization with a PCA. To apply it to a BTF,
ρ(x, ω i , ω r ) is written as a 3T × L · V matrix A ρ , where T
is the number of texels in each sample texture and L and V
the number of light and view directions. The PCA of A ρ is
then defined as
n

Aρ =

ui σi vTi = U VT ,
i=1

where n is the rank of A, σ i are the scalar eigenvalues, u i
vectors of size T, and v i vectors of size L · V . Commonly
the components of the PCA are written as matrices, where
is an n × n diagonal matrix. The PCA is well suited for
compression since a termination after k terms results in the
optimal rank-k approximation of A ρ . The approximation is
however only optimal with respect to the Frobenius norm of
the matrix. In the context of colour images, texture or BTFs,
this is equivalent to the root mean square RGB difference.
To optimize the visual quality, the BTF has to be transformed such that the Frobenius norm is approximately the
visual difference as expressed by the measure introduced in
Section 4. As finally RGB values are required for display, this
transformation needs to be invertible. Improving the visual
quality of the approximation with the proposed visual model
thus requires to derive an efficiently invertible transformation into an approximation of the perceived BTF as defined
by the model. While the colour space transformation can be
inverted after decoding the current pixel, the spatio-angular
frequency weighting would require access to the data of the
complete BTF which implies that a complete reconstruction
is required for every pixel. To solve this problem, the spatioangular transformation needs to be invertible on the PCA
components. This also implies, that the filtering operates on
the CIELab colour channels and not in the lαβ colour space.

w s,i

σ s,i

w a,i

σ a,i

Fortunately, the cubic root of CIELab approximates the cone
response function for intensities on standard displays. Let T
be the transformation applied to the BTF. Then, it must be
possible to apply it on the basis textures U and the ABRDFs
V independently, which implies that the transformed BTF ρ˜
can be written as
˜ ρ = T Aρ
A

= T U Vt

= Tu (U)

˜ V
˜t
(Tv (V))t = U

where T u is the linear transformation applied to the basis
textures and T v the one applied to the ABRDFs. If both are
invertible, the inverse transformation T−1 can be performed
independently on each component before reconstruction:
˜ V
˜t
T−1 Aρ˜ = T−1 U

˜
= T−1
U
u

˜t
T−1
V
v

−1
where T−1
u and Tv are the inverse of T u and T v . This way,
the optimal approximation will minimize the difference with
respect to ρ.
˜ Unfortunately, the proposed filters cannot be
split into a spatial and angular transformation without modifications. If the seven temporary channels would be used,
the transformation could be separated, but this would require
storing seven instead of three colour channels per eigenimage
and per eigen-ABRDFs. By reducing the number of Gaussian
functions to two for the achromatic and one for each chromatic channel, it would be possible to store the eigenimages
and eigen-ABRDFs as RGBA-textures. A further reduction
with even better accuracy is possible by using the product of
two univariate Gaussian mixture models:

˜ t) =
k(x,

ws,i es,i (x)
i

2
e
es,i (x) = π σs,i

wa,i ea,i (t)
i

2
− x2
σs,i

4
ea,i (t) = π 2 σa,i
e

2
− t2
σa,i

.

The separation is straightforward and requires three channels for the eigenimages and two for the eigen-ABRDFs
only. After the transformed components are calculated,
the inverse transformations have to be applied on them.
Table 2 shows the parameters obtained as described in
section 3.3. The mean relative difference to the original
CSF is 24% for the achromatic and 9% for the chromatic
channels.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

108

M. Guthe et al. / BTF-CIELab

Figure 6: First three eigentextures of the wallpaper BTF.
Even for BTFs with differently coloured materials, the components quickly turn grey.

3D textures is the maximum possible, the most efficient approach for quad-linear interpolation is to pack three angular
dimensions into a 3D texture and create a set of textures
for the fourth one. At runtime two texture lookups and a
linear interpolation are necessary. Mapping of the light and
view vectors onto 4D texture coordinates could be performed
using parabolic maps, but these have the disadvantage that
only a circular region of the quadratic domain is used for
each hemisphere. While this might not seem problematic at
first sight, the wasted space is almost 40%. This problem can
be solved by first mapping ω i and ω r onto a disc and then
the texture coordinate p into the square with:
pi/r =

5.1. Colour decorrelation
Due to the physical properties of almost any surface, most of
the variance is contained in the luminance as colour changes
are mostly due to significant three-dimensional (3D) structure. This can be observed for any PCA-based compression
technique, as the components very quickly turn into grey
images after the first few, even for BTFs composed of differently coloured materials, as the example in Figure 6 shows.
Mathematically this means, that the most of the variation is
contained in the L colour channel and only few in a and b.
To exploit this fact for compression, the BTF stored
in the CIELab colour space is written as three matrices
AL , Aa , Ab ∈ IRT ×L·V . In the worst case, the number of scalar
components is thrice the number of Lab-components which
requires the same amount of memory for the eigenimages and
50% more for the eigen-ABRDFs. Due to the typically low
correlation between the CIELab colour channels this does
however hardly occur for real-world BTFs.
An optimal approximation with independent colour channels and k total components can be computed by slightly
modifying the PCA algorithm: for each colour channel, the
next principal component is calculated and then the one with
maximal energy – i.e. the one with maximal σ i – is chosen.
5.2. Representation and rendering
For real-time rendering of the encoded BTF, an efficient decoding on programmable graphics hardware is crucial. As
texture lookup are relatively expensive operations, minimizing their number without significantly increasing the number
of other instructions is the key to fast rendering. On the other
hand, filtering should be possible in all six dimensions of the
BTF and mip-mapping and anisotropic filtering in the spatial
domain are desirable to further improve the visual quality.
For mip-mapping and anisotropic filtering, the only requirements are the generation of mip-map levels and that
the eigenimages are stored in separate textures. As the additional storage cost is only one third and mip-maps are only
required for the eigenimage textures u i , they are stored with
the compressed BTF on disk. As tri-linear interpolation of

pi/r
pi/r

2
∞

pi/r .

Another problem of the parabolic maps is by a factor of 4
lower resolution at the centre compared to the boundary of
the hemisphere. An area preserving mapping can be obtained
by writing p i/r as polar coordinates (d, φ), where
√
d = 1 − cos θi/r = ωi/r z .
φ = φi/r
Together with the square mapping, we have
pi/r =

1 − ωi/r z

ωi/r xy
.
ωi/r xy ∞

The third and fourth texture coordinate from the reflected
light vector follow analogously. The complete mapping requires only six GPU instructions which is the same as for
parabolic maps with mapping onto the square.
To reduce the number of texture lookups, every four successive scalar eigenimages u i and eigen-ABRDFs v i can
be stored in the RGBA channels of a common texture. The
multiplication with each other is then component-wise. The
diagonal matrices L , a , b can be directly stored in a
set of constants instead of a floating point texture. This way
the total number of texture lookups is three (one for u i and
two for v i ) for every four components. Finally, u i and v i are
scaled to [− 0.5, 0.5], shifted to [0, 1] and uniformly quantized to eight bit. The inverse is simply stored by scaling
σ i.
5.3. High frequency omission
Due to the frequency weighting with the CSF, high frequencies will have a lower contribution – especially for the a and
b colour channels – so additional compression is possible
by simply omitting them, i.e. using a lower resolution for
u i . Instead of reducing u i , the transformed basis textures ˜ic
are down sampled which allows to find the optimal u i with
reduced resolution. As the up sampling is a simple bilinear
interpolation that can be performed on the graphics card, the
down sampling operator is the pseudo-inverse.
The same technique can also be used for light and
view directions, because the first components typically have

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

109

M. Guthe et al. / BTF-CIELab

dominant low frequencies as well. As the spatial sub sampling, the angular sub-sampling is also applied to the Krylov
sub-space before calculating σ i , u i and v i . This means
that the irregular sampling has to be remapped onto the
parametrization described in Section 5.2 before the approximation. To prevent a bias, each texel in the resampled
ABRDFs is weighted by its contribution to the original
light/view samples.
As the high frequency omission reduces the storage requirements at the cost of an additional error, the down sampling factor with optimal size to quality ratio has to be determined. To find the optimal resolution, a scoring function
s(ru , r v ) is used, where ru is the resolution of u i and r v the
resolution of v i . As the score should not only depend on the
frequencies contained in the BTF but also on the desired visual quality, an additional base quality bq is introduced. This
parameter defines a quality scale from zero (maximum compression) to one (maximum quality, no sub-sampling). Since
both visual quality q and memory size m are exponential
quantities, s(ru , r v ) should depend on their logarithms:
s(ru , rv ) =

bq
log q(ρiru ,rv ) − qi−1 − log (m(ru , rv )) ,
1 − bq

where ρiru ,rv is the BTF with a resolution of ru , r v for the
component i and q i−1 the quality of the already encoded
BTF. The quality of the rank k approximated BTF can be
described by its signal to noise energy ratio
q(ρk ) =

1 − ρ˜ − ρ˜k
ρ˜ − ρ˜k 2F

2
F

the complete BTF data, we thus use
4k

i=1

where I is the matrix of the sub-space basis textures and
W the matrix of sub-space ABRDFs. The first basis vector
i 0 has to be guessed and normally a random unit vector is
chosen. In the context of BTFs, however, choosing a white
image as first basis textures leads to a more stable sub-space
construction, as the second component becomes the average
ABRDF.
Reducing the dimensionality of the BTF also allows for
a faster transformation into the perceptually uniform space.
Of course, only the linear part of this transformation can be
applied to the sub-space and thus the colour space conversion
into CIELab has to be performed on each sample texture before sub-space construction. In contrast to standard Lanczos
˜ i are not orthogonal
iteration, the transformed vectors ˜ii and w
and thus the constructed Hessenberg matrix cannot be used to
calculate the eigenvectors directly. A simple solution to this
problem would be the orthogonalization of the components
but a more efficient approach is to directly calculate the next
PCA components from the non-orthogonal vectors using a
slightly modified power iteration:
bn =

5.4. Efficient rank-k approximation
For such large matrices as BTFs, calculating the optimal
rank-k approximation efficiently is crucial. If only the first
few terms are required, the PCA can be computed by successive rank-1 approximations of the residual, so the problem
reduces to efficiently calculating the rank-1 approximation.
Furthermore, the rank of a BTF can be assumed to be significantly lower than the number of sample textures. This can
be exploited by first projecting the BTF into a Krylov subspace [Kry31] using Lanczos iteration [Lan50]. Calculating
the first k PCA components from a sub-space with a dimension of d = 4k yields a very good approximation. Instead of

bˆ n
bˆ n

˜ tc A
˜c
bˆ n = A

= qk

while m is simply the number of bytes required for σ i , u i , and
v i . After calculating the score of every reasonable resolution
combination, the one with the highest quality to size rating is
chosen. When the optimal resolutions for all k components
of the rank-k approximation are determined, they need to be
grouped by resolution because of the packing of every four
components into a single texture. This grouping is performed
by first sorting the components with increasing storage requirement and then using the maximum spatial and angular
resolution for every four successive components. Finally, the
optimal σ i , u i and v i are determined.

ii wti = IWt

A≈

=

n

b0 ≈

˜ cW
˜ tc I˜c
I˜tc W

n

b0

˜ cW
˜ tc I˜c I˜tc W
˜ cW
˜ tc n−1 I˜c b0
I˜tc W

where c is one of the CIELab colour channels and b 0 a
random unit vector. The iteration stops as soon as b n ≈ b n−1
or a maximum number of iterations is reached. Substituting
I˜c b0 with a random unit vector x 0 leads to the following:
u˜ i =

˜ tc xn
˜ cW
˜ tc xn
I˜tc W
W
v˜ i =
˜Itc W
˜
˜ cW
˜ tc xn
Wtc xn
n
˜ cW
˜ tc x0 .
xn = I˜c I˜tc W

σi =

xn
xn−1

For the successive PCA components, the residual BTF has
to be calculated by subtracting each encoded and quantized
component from the original BTF and then updating I˜c I˜tc and
˜ cW
˜ tc . The total runtime is O(k 3 + k 2 (T + L · V )) since
W
˜ cW
˜Ic I˜tc and W
˜ tc can be updated in O(k(T + L · V )) and each
eigenvector pair is calculated in O(k 2 ).
After calculating the eigenvectors of the transformed BTF,
the inverse transformation has to be applied to them to reconstruct the original BTF for rendering. Instead of explicitly calculating the inverse transformation matrices, we exploit the
linearity of the transformations and simply accumulate the
untransformed basis textures and ABRDFs. Thus we have:
ui =

˜W
˜ t xn
Itc W
˜Itc W
˜W
˜ t xn

vi =

Wt xn
.
˜ t xn
W

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

110

M. Guthe et al. / BTF-CIELab

Figure 7: Experimental setup to obtain subjective quality
ratings for different BTF compression techniques. On the left
the compressed BTF is presented with the reference on the
right. Below the images, the rating dialog is shown.

was not included because it was not available for the majority of the evaluated compression techniques. This does not
significantly influence the results since this BTF does not
have special characteristics not covered by the other five.
The BTFs were compressed with the following algorithms:
Lafortune fitting [MLH02] with two lobes, per view factorization [SSK03] with 16 components, chained matrix factorization [SvBLD03] without clustering and quantization,
local PCA [MMK03] with 32 cluster and 8 components, tensortextures [VT04] with 8 light and 16 view components,
LoD BTFs [MCC∗ 05] with 4 × 4 sub sampling, and the proposed method with 32 scalar components and 164 maximum
ABRDF resolution at 100%, 90% and 80% quality. As reference BTF, we also use the proposed compression but with
128 components and 324 maximum ABRDF resolution to
minimize angular resampling artifacts. The difference of the
reference to the uncompressed BTF is less than one third of
any other compression methods for every tested difference
measure such that the introduced bias is rather low.

6.1. Quality assessment
Due to the linearity of the transformations, σ i remains as
in the untransformed case. Note, however, that u i and v i are
neither normalized nor orthogonal which is not problematic
for compression and rendering purposes only.

6. Experimental Validation and Results
In order to demonstrate the expressiveness of the proposed
numerical distance metric for BTFs, we applied it to measure
the visual quality of different BTF compression techniques
and compare the result to other possible metrics. As reference, we conducted a Double Stimulus Impairment Scale
(DSIS) experiment [ITU02] with 5-point scale under controlled light conditions on a calibrated display. The experiment setup and user interface is shown in Figure 7. The
left view shows the model rendered with the compressed
BTF and the right one is mapped with the reference BTF.
The model onto which the BTFs are mapped is chosen such
that it contains different combinations of angular frequencies
while the user can additionally move both the model as well
as the light source. After a period of at most 60 seconds, the
subject had to give a rating for the quality loss of the compressed BTF. According to the standard 5-point scale, these
rating are: ‘imperceptible’, ‘perceptible, but not annoying’,
‘slightly annoying’, ‘annoying’ and ‘very annoying’. To prevent the contextual bias from the previous stimuli present
in any perceptual experiment, the ordering of methods and
BTFs was randomized for each subject. The total number of
subjects was 20, of which 15 were male and 5 female.
As input data, we used five of the 2562 × 81 × 81 datasets
from the BTFDBB [BTF], namely Corduroy, Impalla, Proposte, Wallpaper and Wool. The last one (i.e. the Pulli BTF)

The main goal of accurate quality measures is to maximize
the correlation between numerically calculated difference
and mean subjective opinion score. The relationship between
these two is typically expressed using a linear mapping with
the constraint that for zero difference a mean opinion score
of at least five (i.e. imperceptible difference) is predicted.
Note that a predicted score of more than five simply means
that the difference is simply below the JND. Figure 8 shows
the correlation between mean opinion score and numerical
difference for RGB L2 , CIE E 00 without spatial or angular
filtering, S-CIELab (spatial filtering only), and the proposed
BTF-CIELab.
The root mean square and maximum difference for the
fitted regression models are shown in Table 3. While the CIE
E 00 outperforms the simple RGB L2 difference as expected,
the small number of subjects does not allow to clearly judge
between S-CIELab and the proposed BTF-CIELab difference measure. For both, the correlation suggests that there is
still room for improvements. One of the main assumptions
of linear regression is that the residual is a normal distribution with zero mean. Only the residuals of the regression
fits for S-CIELab and the proposed BTF-CIELab pass the
Anderson-Darling test [AD52] for normal distributions using
a significance of 95%. To test the linearity of the relationship between mean opinion score and the results of the error
measure, we use the f -test [Ros87] to determine whether
a linear regression is sufficient or a polynomial of higher
degree is required. Here, only the proposed BTF-CIELab
difference passes the test, while CIE E 00 and S-CIELab
would require a quadratic function and the RGB L2 even a
cubic polynomial. So despite the marginally worse residual
of the regression fit of the BTF-CIELab difference compared

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

111

M. Guthe et al. / BTF-CIELab

Table 3: Difference between predicted and actual mean opinion
score.
RGB L2
RMS
Max.

0.569
2.246

CIE

E 00

0.553
1.944

S-CIELab

BTF-CIELab

0.467
1.332

0.469
1.339

Figure 9: BTF-CIELab RMS difference of the corduroy BTF
with local PCA as function of spatial and angular resolution
(top) and comparison of rendered images (bottom left: compressed, bottom right: original).

Figure 9 shows the BTF-CIELab difference of the corduroy
BTF compressed with [MMK03]. The angular difference is
significantly higher than in the spatial, which causes a visual
flattening of the material.

6.2. BTF compression
Figure 8: Correlation of measured difference and mean
opinion score with least-squares fitted linear regression.

to S-CIELab, the proposed method is the only one to clearly
exhibit a linear relationship.
The quality measure cannot only serve as a tool for comparison of different compression techniques, but a detailed
analysis of the visual difference with respect to spatial and angular resolution can also detect possible sources of artifacts.

To demonstrate the effectiveness of the visual model for compression, the compressed BTF is compared to the result when
using the same algorithm, but minimizing the RGB L2 difference using the orthonormal grey, red–green, blue–yellow
colour space. Both were compressed without additional sub
sampling of the components (bq = 100%). Figure 10 shows
the improved convergence and better approximation for the
impalla BTF. Notice the overall colour change when using
the RGB L2 difference and the better structure preservation
with the proposed difference measure.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

112

M. Guthe et al. / BTF-CIELab

by most techniques. Finding the resolution pair with the maximum difference requires between 3 and 5 hours depending
on the number of iterations. For the S-CIELab difference, the
runtime is approximately 2–3 hours. The compression time
is dominated by the sub-space construction that takes about
10 hours for 128 components and a 2562 × 81 × 81 BTF
on the above-mentioned machine. The remaining compression time is only about 1 hour for 32 scalar components.
Note that the time for both sub-space construction as well as
compression is quadratic with the number of components.
Figure 10: Comparison of impalla BTF compressed with the
proposed difference metric (top) and with RGB L2 difference
(bottom) using 4, 8, 16 and 32 components.

7. Conclusion and Future Work
The presented method to assess the visual quality of images
and animations generated from compressed BTF representations has, despite its simplicity, proven to provide reasonably
accurate predictions for the visual quality in contrast to the
RBG L2 difference or other existing techniques. In addition,
a detailed analysis of the difference with respect to spatial
and angular resolution can even help to improve existing
compression techniques by revealing their weak points. Furthermore, a linearized variant of the measure can be used to
improve the quality of almost any compression technique and
yields compression rates of about 500:1 at excellent quality
using a per-channel PCA and with 32 components.
Although not explicitly addressed, an extension to temporally varying or animated BTFs is straightforward, because it
would simply mean adding a seventh dimension in which the
frequency filtering needs to be performed. The radial filters
used for the spatial and angular frequency are a significant
simplification that introduce inaccuracies. The incorporation
of directional filters, that have become quite common in the
field of image and video quality assessment, could possibly
lead to a higher accuracy at the cost of longer computation
times. Similarly, extending the method to HDR BTFs by
adapting the difference metric from [MMS04] would require
three times the number of channels.

Figure 11: Comparison of different BTF compression techniques with respect to visual quality against required texture
memory (top) and fill rate (bottom).
Although visual quality is an important factor, others need
to be taken into account as well. Figure 11 shows the predicted opinion score of the different compression techniques
against the required texture memory and the fill rate. With
respect to storage requirements, the proposed compression
technique clearly dominates all others, while [MMK03] is
the only competitive one with respect to performance.
The total runtime to compute the visual quality of a
2562 × 81 × 81 BTF
√ with 25 spatio-angular resolution
pairs, i.e. a factor of 10 between successive resolutions,
is approximately 3 hours on an Intel Core 2 Duo PC with
2.4 GHz, which is less than the compression time required

References
[AD52] ANDERSON T. W., DARLING D. A.: Asymptotic theory of certain “goodness-of-fit” criteria based on stochastic processes. Annals of Mathematical Statistics 23 (1952),
193–212.
[BK80] BURBECK C. A., KELLY D. H.: Spatiotemporal characteristics of visual mechanisms: Excitatory-inhibitory
model. Journal of the Optical Society of America 70, 9
(1980), 1121–1126.
[BTF] BTF Database Bonn: Available at http://btf.cs.unibonn.de.
[CSE00] CHRISTOPOULOS C., SKODRAS A., EBRAHIMI T.: The
JPEG2000 still image coding system: An overview. IEEE

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

M. Guthe et al. / BTF-CIELab

113

Transactions on Consumer Electronics 46, 4 (2000),
1103–1127.

In Proceedings of IEEE International Conference on Systems, Man and Cybernetics (2004), pp. 2763–2769.

[DCH05] DIVERDI S., CANDUSSI N., H¨OLLERER T.: Real-time
Rendering with Wavelet-Compressed Multi-Dimensional
Datasets on the GPU. Technical Report UCSB//CSD-0505, University of California at Santa Barbara, 2005.

[MMS∗ 05] M¨ULLER G., MESETH J., SATTLER M., SARLETTE
R., KLEIN R.: Acquisition, synthesis and rendering of bidirectional texture functions. Computer Graphics Forum 24,
1 (2005), 83–109.

[DvGNK99] DANA K. J., VAN GINNEKEN B., NAYAR S. K.,
KOENDERINK J. J.: Reflectance and texture of real-world
surfaces. ACM Transactions on Graphics 18, 1 (1999),
1–34.

[Poh05] POHLMAN K. C.: Principles of Digital Audio, Fifth
Edition. Mcgraw-Hill, New York, USA, 2005.

[ITU02] ITU Radiocomunication Assembly: Methodology for thh subjective assessment of the quality of television pictures. Rec. ITU-R BT.500-11, 2002.
[Kry31] KRYLOV A. N.: On the numerical solution of the
equation by which, in technical matters, frequencies of
small oscillations of material systems are determined.
Izvestiˆa Akademii Nauk SSSR VII, 4 (1931), 491–539.
[Lan50] LANCZOS C.: An iteration method for the solution
of the eigenvalue problem of linear differential and integral
operators. Journal of Research of the National Bureau of
Standards 45, 4 (1950), 255–282.
∗

[LGC 05] LENSCH H., G¨OSELE M., CHUANG Y.-Y., HAWKINS
T., MARSCHNER S., MATUSIK W., M¨ULLER G.: Realistic materials in computer graphics. In SIGGRAPH 2005 Tutorials,
2005.
∗

[MCC 05] MA W.-C., CHAO S.-H., CHUANG Y.-Y., CHANG
C.-F., CHEN B.-Y., OUHYOUNG M.: Level-of-detail representation of bidirectional texture functions for real-time
rendering. In Proceedings of ACM 2005 Symposium on
Interactive 3D Graphics and Games (I3D 2005) (2005),
pp. 187–194.
[MLH02] MCALLISTER D. K., LASTRA A., HEIDRICH W.:
Efficient rendering of spatial bi-directional reflectance
distribution functions. In Proceedings of the ACM
SIGGRAPH/EUROGRAPHICS Conference on Graphics
Hardware (2002), pp. 79–88.
[MMK03] M¨ULLER G., MESETH J., KLEIN R.: Compression
and real-time rendering of measured btfs using local PCA.
In Vision, Modeling and Visualisation 2003 (2003), pp.
271–280.
[MMK∗ 06] MESETH J., M¨ULLER G., KLEIN R., R¨ODER F.,
ARNOLD M.: Verification of rendering quality from measured BTFS. In APGV ’06: Proceedings of the 3rd symposium on Applied Perception in Graphics and Visualization
(2006), pp. 127–134.
[MMS04] MANTIUK R., MYSZKOWSKI K., SEIDEL H.-P.: Visible difference predicator for high dynamic range images.

[PTVF02] PRESS W., TEUKOLSKY S., VETTERLING W.,
FLANNERY B.: Numerical Recipes in C – The Art of Scientific Computation. Cambridge University Press, Cambridge, UK, 2002.
[RCC98] RUDERMAN D. L., CRONIN T. W., CHIAO C.-C.:
Statistics of cone responses to natural images: Implications for visual coding. Journal of the Optical Society
America A: Optics, Image Science, and Vision, 15 (1998),
2036–2045.
[Ros87] ROSS S. M.: Introduction to Probability and Statistics for Engineers and Scientists. Wiley, Hoboken, USA,
1987.
[SSK03] SATTLER M., SARLETTE R., KLEIN R.: Efficient and
realistic visualization of cloth. In Proceedings of the 14th
Eurographics Workshop on Rendering (2003), pp. 167–
177.
[SvBLD03] SUYKENS F., VOM BERGE K., LAGAE A., DUTRE P.:
Interactive rendering with bidirectional texture functions.
Computer Graphics Forum 22, 3 (2003), 463–472.
[SW03] SCHNEIDER J., WESTERMANN R.: Compression domain volume rendering. In VIS ’03: Proceedings of the
14th IEEE Visualization 2003 (VIS’03) (Washington, DC,
USA, 2003), IEEE Computer Society, pp. 39–48.
[THvdBL99] TONG X., HEEGER D., VAN DEN BRANDEN
Lambrecht C.: Video quality evaluation using st-cielab.
SPIEHuman Vision and Electronic Imaging 3644 (1999),
185–196.
[VT04] VASILESCU, M. A. O., TERZOPOULOS, D.: Tensortextures: Multilinear image-based rendering. ACM Transactions On Graphics 23, 3 (2004), 336–342.
[Wal92] WALLACE G. K.: The JPEG still picture compression standard. IEEE Transactions on Consumer Electronics 38, 1 (1992).
[Win05] WINKLER S.,: Digital Video Quality – Vision Models and Metrics. Wiley & Sons, Hoboken, USA, 2005.
[ZW96] ZHANG X., WANDELL B. A.: A spatial extension of
cielab to predict the discriminality of colored patterns. SID
Symposium Digest 27 (1996), 731–735.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

