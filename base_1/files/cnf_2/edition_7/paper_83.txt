Eurographics/ IEEE-VGTC Symposium on Visualization 2009
H.-C. Hege, I. Hotz, and T. Munzner
(Guest Editors)

Volume 28 (2009), Number 3

MultiClusterTree: Interactive Visual Exploration of
Hierarchical Clusters in Multidimensional Multivariate Data
Tran Van Long and Lars Linsen
School of Engineering and Science
Jacobs University
Bremen, Germany
{v.tran,l.linsen}@jacobs-university.de

Abstract
Visual analytics of multidimensional multivariate data is a challenging task because of the difficulty in understanding metrics in attribute spaces with more than three dimensions. Frequently, the analysis goal is not to look
into individual records but to understand the distribution of the records at large and to find clusters of records with
similar attribute values. A large number of (typically hierarchical) clustering algorithms have been developed to
group individual records to clusters of statistical significance. However, only few visualization techniques exist for
further exploring and understanding the clustering results. We propose visualization and interaction methods for
analyzing individual clusters as well as cluster distribution within and across levels in the cluster hierarchy. We
also provide a clustering method that operates on density rather than individual records. To not restrict our search
for clusters, we compute density in the given multidimensional multivariate space. Clusters are formed by areas
of high density. We present an approach that automatically computes a hierarchical tree of high density clusters.
To visually represent the cluster hierarchy, we present a 2D radial layout that supports an intuitive understanding
of the distribution structure of the multidimensional multivariate data set. Individual clusters can be explored interactively using parallel coordinates when being selected in the cluster tree. Furthermore, we integrate circular
parallel coordinates into the radial hierarchical cluster tree layout, which allows for the analysis of the overall
cluster distribution. This visual representation supports the comprehension of the relations between clusters and
the original attributes. The combination of the 2D radial layout and the circular parallel coordinates is used to
overcome the overplotting problem of parallel coordinates when looking into data sets with many records. We
apply an automatic coloring scheme based on the 2D radial layout of the hierarchical cluster tree encoding hue,
saturation, and value of the HSV color space. The colors support linking the 2D radial layout to other views
such as the standard parallel coordinates or, in case data is obtained from multidimensional spatial data, the
distribution in object space.
Categories and Subject Descriptors (according to ACM CCS):
Generationâ€”Display Algorithms.

1. Introduction
As a consequence of the digital revolution, the amount of
digital data that analytical systems are facing nowadays goes
beyond what humans can fully understand by investigating
individual data records. Automatic approaches like clustering need to be coupled with interactive visualization methods to extract certain features and display them in an intuitive manner. This is particularly challenging for multic 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

Computer Graphics [I.3.3]: Picture/Image

dimensional multivariate data, as visual spaces are limited
to (at most) three dimensions. Multidimensional multivariate data visualizations need to map the data into a lowerdimensional layout that still conveys the important information.
When dealing with large data sets with many records,
clustering has proven to be extremely useful. Clustering is
a partition method of a data set into subsets of similar obser-

824

V. Tran & L. Linsen / MultiClusterTree

vations. Each subset is called a cluster, which consists of observations that are similar within themselves and dissimilar
to observations of other clusters. Cluster analysis tasks for
multidimensional data have the goal of finding areas where
individual records group together to form a cluster. Typical
questions include:
1. What clusters exist within the multidimensional domain?
2. What is the relationship or correlation between clusters?
3. What are the properties of the clusters (homogeneous
vs. heterogeneous)?
4. What are the ranges of the multidimensional variables of
each cluster?
5. How do clusters compare to each other?
Already several decades ago, Hartigan [Har75] made the observation that clusters can be identified as areas of high density surrounded by lower-density regions. Many approaches
have been using density since to detect clusters [CFF00]. In
this paper, we also analyze the data by looking into the density of the distribution, as it allows for a top-down analysis
of the data by successively refining clusters.
Density computations have to be performed in the original
high-dimensional space, as any kind of projections to lowerdimensional spaces would impose certain assumptions and
would distort the actual data. Our density computations are
based on subdividing the domain into hypercubes and evaluating density functions over the spatial subdivision. The
clustering is done hierarchically by iteratively splitting each
density cluster into subclusters (if any), see Section 2.
To display the hierarchical structure of the density clusters, we generate a hierarchical density cluster tree and show
it using a 2D radial layout. The root of the tree is placed in
the origin of a unit circle, while the leaves are evenly distributed on the circle. The internal nodes are placed on concentric circles corresponding to its depth. The clusters are
visually represented as colored disks, whose radius encodes
the size of the clusters and whose color encodes the position
within the radial layout using the HSV color space. Details
are given in Section 3.
The cluster tree visualization supports the comprehension
of the structure of clusters within the multidimensional multivariate data. To also support the visualization of the value
ranges for the individual dimensions, we employ parallel coordinates. Linked views are used to select clusters in the
cluster tree representation and display them in a parallel coordinates representation.
In case of multivariate (volumetric) spatial data, we use
our approach to explore the multidimensional feature (or attribute) space. In addition, we support another linked view
that renders the spatial distribution of clusters in the volumetric object space. Colors are used to establish a correlation
between the linked views when selecting multiple clusters.
The linked views are described in Section 4.
The linked views support the simultaneous exploration

and comparison of a few clusters using parallel coordinates.
However, when displaying a large data set in its entity with
all clusters in one parallel coordinates representation, the
display suffers from severe cluttering effects. We address
this issue by integrating circular parallel coordinates into the
cluster tree visualization. Each cluster is no longer visually
represented by a disk, but by a circular parallel coordinates
visualization of the cluster rendered at the respective position. This integrated representation is described in Section
5, where a focus+context technique is supported for interactive analysis. We apply our methods to real and synthetic
data sets and show how our visualization methods help to
answer the analysis tasks listed above.
1.1. Visualizing Multidimensional Multivariate Data
Information visualization is the use of computer-supported,
interactive, visual representations of abstract data to amplify cognition [CMS99]. Many methods exist to visualize
multidimensional multivariate data that help understanding
the data. One of the most popular multivariate visualization
technique in statistics is the scatterplot matrix, which displays multiple adjacent scatterplots, one for each pair of attributes [War94, FGW02]. Scatterplots support finding clusters, outliers, and correlations. However, when dealing with
multidimensional multivariate data, many scatterplots have
to be observed by the user, which is tedious. Moreover, it becomes difficult to comprehend multi-dimensional structures.
To avoid looking at many different plots displayed next to
each other, parallel coordinates have been introduced. Parallel vertical lines represent the different dimensions or attributes, and individual records are represented by a polygonal line connecting points on the parallel coordinate axes.
A large community is using parallel coordinates on a regular basis [Ins85, Weg90, FGW02]. Several extentions have
been proposed to alleviate the inherent clutter problem for
large number of records. Fua et al. [FWR99] presented a
multiresolution view based on hierarchical clustering, where
an opacity band displays clusters instead of drawing individual records. Artero et al. [AdOL04] proposed a density plot
using frequency-matrix filters to uncover clusters. Johansson
et al. [JLJC05, JLJC06] reveal clusters in the parallel coordinates using high-precision textures, where each cluster is
displayed as a uniform band of its true size and the colors
of the clusters are calculated in HSV color space using fixed
saturation and value, while the hue component is equally distributed on a circular unit. McDonnell and Mueller [MM08]
introduced an edge bundling technique for parallel coordinates, replacing polylines with curves.
1.2. Clustering
Cluster analysis divides data into meaningful or useful
groups (clusters). Clustering algorithms can be classified
into four main approaches: partitioning methods, hierarchic 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

825

V. Tran & L. Linsen / MultiClusterTree

cal methods, density-based methods, and grid-based methods [JD88, HK06]. In partitioning methods, data sets are divided into k disjoint clusters. In hierarchical methods, data
sets are represented using similarity trees and clusters are extracted from this hierarchical tree. In density-based methods,
clusters are a dense region of points separated by low-density
regions. In grid-based methods, the data space is divided into
a finite number of cells that form a grid structure and all of
the clustering operations are performed on the cells.
Hartigan [Har75, Har85] first proposed to identify clusters as high density clusters in data space. Wong and
Lane [WL83] define neighbors for each data point in data
space and use the k nearest neighbors to estimate density.
After defining dissimilarity between neighboring patterns, a
hierarchical cluster tree is generated by applying a singlelinkage algorithm. In their paper, they show that the highdensity clusters are strongly consistent. However, they do
not examine modes of the density function. Stuetzle and Nugent [SN07] propose to construct a graph whose vertices
are patterns and whose edges are weighted by the minimum
value of the density estimates along the line segment connecting the two vertices. The disadvantage of their approach
is that it depends on a threshold parameter (level-set value),
which is difficult to determine. Ankerst et al. [ABKS99] introduced the OPTICS algorithm, which computes a complex
hierarchical cluster structure and arranges it in a linear order
that is visualized in the reachability plot.
1.3. Visual Cluster Analysis
Visual cluster analysis denotes the integration of cluster
analysis and information visualization techniques. Seo and
Shneiderman [SS02] introduced the hierarchical clustering
explorer (HCE), which couples clustering algorithms (hierarchical algorithm, k-means) with information visualization
techniques (dendrograms, scatterplots, parallel coordinates).
It supports an interactive dendrogram representation to explore data sets at different levels of similarity. When a cluster
is selected in the dendrogram, all data points of this cluster
are displayed in a parallel coordinates layout. The number
of clusters vary depending on the level of similarity, such
that it becomes difficult for the user to understand the qualitative structure of the clusters. For large data sets, the dendrogram likely exhibits clutter. Johansson et al. [JTJ04] integrate the self-organizing map (SOM) unsupervised clustering algorithm with parallel coordinates. The clusters are
visualized by their average values with a band proportional
to the number of data points in the clusters. They used color
to distinguish the clusters, but they did not discuss how colors are assigned.
1.4. Contribution
In this paper, we describe MultiClusterTree, an interactive
exploration system for multidimensional multivariate data
analysis. In particular, we propose:
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

â€¢ Coupling automatic multidimensional clustering based on
efficient grid computations with the concept of cluster
trees. This leads to an automatic computation of multidimensional density cluster trees without manually adjusting level-set thresholds.
â€¢ Visualizing hierarchical density cluster trees based on a
2D radial layout with automatic color coding. This visualization allows for an understanding of the cluster distribution and their properties, e.g., leaf nodes are homogeneous, non-leaf nodes heterogeneous.
â€¢ Linking the cluster tree visualization to other views including parallel coordinates and object space renderings,
where interaction operates on cluster nodes and colors are
used for correspondences. The simultaneous display of selected clusters in parallel coordinates allows for a comparison of clusters with respect to the given dimensions.
â€¢ Integrating circular parallel coordinates into the cluster
tree visualization, also supporting a focus+context technique. This integrated view allows for an understanding of
the entire data set without overplotting, such that both the
individual clusters with respect to the given dimensions
and the overall cluster distribution are easily comprehensible.
2. Hierarchical Grid-based Density Cluster Trees
Given a multivariate density function f (x) in d dimensions,
modes of f (x) are positions, where f (x) has local maxima. Thus, a mode of a given distribution is more dense
than its surrounding area. We want to find the attraction regions of modes. They can be defined with respect to a value
Î» (0 < Î» < supx f (x)), considering regions of the sample
space where values of f (x) are greater than or equal to Î».
The Î»-level set of the density function f (x) denotes a set
S( f , Î») = {x âˆˆ Rd : f (x) â‰¥ Î»} .
The set S( f , Î») consists of a number q of connected components Si ( f , Î») that are pairwise disjoint, i. e.,
S( f , Î») =

q
[

Si ( f , Î»)

i=1

with Si ( f , Î») âˆ© S j ( f , Î») = âˆ… for i = j . The subsets Si ( f , Î»)
are called Î»-density clusters (Î»-clusters for short). A cluster can contain one or more modes of the respective density
function.
Given a multidimensional data set, we need to estimate
a density function and determine the number of clusters as
well as the high density region of each cluster. The density
kn
estimate at a point x in data space is fn (x) = nV
, where n
n
denotes the size of the data set, Vn is a small region or volume
around x, and kn is the number of points falling in this region
[JD88, DHS01]. The necessary and sufficient criteria for an
approximation of a density estimate are given by lim kn =
nâ†’âˆž

kn
nâ†’âˆž n

âˆž and lim

= 0.

826

V. Tran & L. Linsen / MultiClusterTree

Let the data set D={xi = (xi1 , . . ., xid ) : 1 â‰¤ i â‰¤ n} have n
data points in d dimensions. First, we transform the data into
a unit hypercube [0, 1]d such that the scaled values xÂ¯i j of xi j
all lie in the interval [0, 1]:
xÂ¯i j =

xi j âˆ’ min j
, j = 1, . . ., d; i = 1, . . ., n,
max j âˆ’ min j

where min j = min{xi j : 1 â‰¤ i â‰¤ n}, max j = max{xi j : 1 â‰¤
i â‰¤ n}, j = 1, . . ., d. Second, we divide each dimension into
N equally-sized portions. Hence, the size of the portions is
1
h = , which is the edge length of the grid cells.
N
For each cell we count the number of data points lying
inside. The multivariate density function is estimated by the
formula
number of points inside cell
f (x) =
n Ã— (volume of cell)
for any x within the domain. As the area is equal for all cells,
the density of each cell is proportional to the number of data
points lying inside the cell. Without loss of generality, we
can describe the density by the number of data points within
each d-dimensional cell.

and the edges represent neighborhood information. Therefore, clusters are connected components of the graph. We
apply the depth first search method to find connected components of the graph. Each connected component represents
a cluster [CLRS01].
In order to find higher-density clusters within the detected clusters, we can apply a divisive hierarchical clustering method [HK06], where each cluster is split according to
the density values. Given a detected cluster C, we remove all
those cells from cluster C that contain the minimum number
of data points. Then, we apply the graph-based cluster search
algorithm from above again to find connected components
in the reduced graph. The connected components represent
the higher-density clusters within cluster C. If the number of
higher-density clusters within C is q â‰¥ 2, we create q new
clusters (subclusters of C) and proceed with these (higherdensity) clusters in the same manner. If q = 1, we iteratively
remove further cells with minimum number of data points.
If we cannot find multiple higher-density clusters within a
cluster, we call this cluster a mode cluster.

Because of the curse of dimensionality, the data space
is sparse and there are many empty cells. We do not store
empty cells such that the amount of cells we are storing and
dealing with is less than the size of data set. Hence, operating on the grid instead of using the points does not increase
the amount of storage but typically decreases it.
To create the nonempty cells, we use a partitioning algorithm that iterates through all dimensions. Assuming that we
partition the kth dimension of hypercuboid R = I1 Ã— . . . Ã— Id
(where in the first k âˆ’ 1 dimensions the edges have length h
and the others length 1) into some smaller hypercuboids. Let
x1 , . . ., xm denote values on the kth dimension of the m points
inside hypercuboid R. These values fall into p (p â‰¤ N) nonj
empty intervals Ik , j = 1, . . ., p, whose length is equal to h.
Then, the hypercuboid R is partitioned into p small hyperj
cuboids R j = I1 Ã— . . . Ã— Ik Ã— . . . Ã— Id , j = 1, . . ., p, of which
we only store the non-empty ones. We apply this algorithm
for each dimension to partition the data set into a set of nonempty cells of size hd . Optionally, this step of the algorithm
can be adopted to deal with data sets containing noise. Storing only those cells containing a number of data points larger
than a noise threshold removes the noise. The time complexity for partitioning the data space into non-empty cells is
O(dn).
We define that two cells cube(i1 , . . ., id ) and
cube( j1 , . . ., jd ) are neighbored, if |ik âˆ’ jk | â‰¤ 1 for all
k = 1, . . ., d. Figure 1 (left) shows all the neighbors of the
red cell for the two-dimensional space. Analogously, we
define the neighborhood properties between a cluster and a
cell and the neighborhood properties of two clusters.
We define a graph where the vertices represent the cells

Figure 1: (Left) Neighborhood of the red cell in a twodimensional space, (Right) Cluster tree with 4 modes shown
as leaves of the tree.

This procedure automatically generates a collection of
high density clusters Ï„ that exhibit a hierarchical structure:
for any two high density clusters A and B in Ï„, we have A âŠ‚ B
or B âŠ‚ A or A âˆ© B = âˆ…. This hierarchical structure is summarized by the high density cluster tree (short: cluster tree). The
root of the cluster tree represents all sample points. Figure 1
(right) shows a cluster tree with 4 mode clusters represented
by the treeâ€™s leaves.
The cluster tree visualization provides a method to understand the distribution of data by displaying attraction regions
of modes of the multivariate density function. Each cluster
contains at least one mode. The leaf nodes of the cluster tree
are the mode clusters.
3. Radial Layout of Cluster Tree
Based on the hierarchical grid-based density clustering, we
present a layout for visualizing the resulting cluster tree. Of
course, the visualization techniques that are described in this
section and the subsequent ones apply to any hierarchical
clustering result of multidimensional multivariate data.
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

V. Tran & L. Linsen / MultiClusterTree

Our visualization is based on drawing the hierarchical tree
structure in a radial layout. A radial drawing is a variation
of a layered drawing, where the root of the tree is placed
at the origin and layers are represented as concentric circles
centered at the origin [TBET99].
Let n be the number of leaves and m + 1 be the depth of
the hierarchical tree structure. The fundamental idea for our
tree drawing is as follows: Considering a unit circle, the leaf
nodes are placed evenly distributed on that unit circle, the
root node is placed at the origin of the circle, and the internal nodes are placed on circular layers (with respect to the
same origin) whose radii are proportional to the depth of the
internal nodes. Hence, all mode clusters are represented by
nodes placed on the unit circle. These clusters are homogeneous. All other clusters are represented by nodes placed
on layers within the unit circle. These clusters are heterogeneous.

Figure 2: Radial layout of a cluster tree. (Left) An annulus
wedge domain W = (r, Î±, Î²). (Middle) Splitting the annual
wedge for placing three subtrees. (Right) Placing internal
nodes of cluster tree.

For the placement of internal nodes of the cluster tree, we
use the notation of an annulus wedge. Given, a polar coordinate representation, an annulus wedge W = (r, Î±, Î²) denotes
an unbounded region that lies outside a circle with center
at the origin and radius r and is restricted by the two lines
corresponding to angles Î± and Î². Figure 2 (left) shows an
annulus wedge W = (r, Î±, Î²) (restricted to the unit circle).
Let tree T be the subtree of our cluster tree that is to be
placed in the annulus wedge W = (r, Î±, Î²). The radius r denotes the distance of the root of T to the origin. If the root
of T has depth d in the entire cluster tree, then r = md . Moreover, we use the notation â„“(T ) for the number of leaves of a
tree T . Now, let T1 , . . ., Tk be those subtrees of tree T , whose
root is a child node of T . For each subtree Ti , we compute
the annulus wedge Wi = (ri , Î±i , Î²i ), where ri = d+1
m is the
radius for placing the root node Ti ,
Î±i = Î± + âˆ‘ â„“(T j )
j<i

2Ï€
n

and

Î²i = Î±i + â„“(Ti )

2Ï€
.
n

Figure 2 (middle) shows how an annulus wedge is split for a
tree T with three subtrees T1 , . . ., T3 .
This iterative splitting of the annulus wedge is started with
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

827

the root node of the cluster tree, which is represented by the
annulus wedge (0, 0, 2Ï€).
Finally, we can position all internal nodes of the cluster
tree within the respective annual wedge. Considering subtree T with the corresponding annulus wedge W = (r, Î±, Î²),
Î±+Î²
we place the node at position (r cos( Î±+Î²
2 ), r sin( 2 )) with
respect to the polar coordinate system. Figure 2 (right) shows
the placement of nodes for the annulus wedges shown in Figure 2 (middle).
4. Linked Views
Based on the hierarchical clustering algorithm in Section 2,
we construct a cluster tree, which is drawn with respect to the
layout described in Section 3. For drawing the nodes of the
tree, we use circular disks with an automatic size and color
encoding. The size of the nodes is determined with respect
to the size of the respective cluster that is represented by the
node. We use a logarithmic scaling to encode the size. The
color of the nodes is determined with respect to the position
in the radial layout. Color encoding is done using the HSV
color space. Hue H encodes the angle in our radial layout
and saturation S encodes the radius (distance to the origin),
while value V is constant (set to 1). Hence, the applied coloring scheme can be regarded as a slice V = 1 through the
HSV color space. Figure 3 (left) shows a visualization of the
cluster tree.
The cluster tree visualization also serves as a user interface for interaction with linked views. We support a linked
view with a parallel coordinates layout. In parallel coordinates, we have d parallel axes and each data point is displayed as a polyline that intersects the parallel axes at the respective value of the represented attribute. We display clusters by drawing a band that contains all polylines of the data
points belonging to the respective cluster. Colors are induced
by the cluster tree visualization and indicate, to which cluster
the drawn polyline belongs. The color for each data point is
defined by the color that was assigned to the node of the cluster of highest depth containing that data point. When drawing the band of polylines for a cluster, this band uses the colors of the contained data points and gets assigned an opacity
value that is proportional to the density of the polylines in the
parallel coordinates display. Figure 3 (right) shows a parallel
coordinate display being linked to the clusters selected Figure 3 (left). User interaction is performed by clicking at the
clusters of interest in the cluster tree visualization.
We demonstrate the functionality of our approach by applying it to two well-known data sets. The first data set called
"out5d" â€  . It contains 16, 384 data points with 5 attributes
(spot, magnetics, potassium, thorium, and uranium). We divide each dimension of the data set in to N = 10 equally

â€  http://davis.wpi.edu/ xmdv/datasets.html

828

V. Tran & L. Linsen / MultiClusterTree

sized intervals. We obtain 3, 661 non-empty cells and compute a hierarchical density cluster tree. The depth of the tree
is 10, and it contains 21 leave nodes (mode clusters) and 13
internal nodes.
The hierarchical density cluster tree is displayed in Figure 3 (left) and its linked parallel coordinates view in
Figure 3 (right). The cluster tree visualization exhibits an
overview of the distribution of multidimensional multivariate data, whereas parallel coordinates show clearly the values of data point attributes and their domains. The parallel
coordinates allows for the exploration of individual clusters
as well as the correlation between some selected clusters.
Homogeneous clusters appear in a unique color in the parallel coordinate layout, while heterogeneous clusters exhibit
multiple colors. Figure 3 (right) shows a heterogeneous cluster chosen by selecting an internal node in the cluster tree.
The clusterâ€™s attributes are high in magnetics, low in potassium, and medium in other attributes. Moreover, the cluster
partitions into three subclusters based on the attributes magnetics and uranium.

Figure 3: Linking cluster tree visualization with parallel coordinates. (Left) Radial layout of hierarchical density cluster tree. (Right) Interactively selected cluster is visualized in
parallel coordinates.

Our approach can also be applied to the visual analysis of
multi-field spatial data sets. The feature space is a multidimensional multivariate data space, to which we apply our
methods. In addition to the parallel coordinates view, we
provide another linked view that shows the distribution of
the data points (belonging to the selected clusters) in volumetric object space (physical space).
The second data set we used is such a multi-field spatial
data set and comes from the 2008 IEEE Visualization Design Contest [WN08]. We uniformly sample the object space
to obtain 1, 240, 000 data points with 11 feature attributes,
namely total particle density, gas temperature, abundances
of H mass, H+ mass, He mass, He+ mass, He + + mass,
Hâˆ’ mass, H2 mass, and H2 + mass, and the magnitude of
turbulence.
Figure 4 shows the selection of three clusters using the
cluster tree interface and displaying their attributes in parallel coordinates with different colors as well as their location

Figure 4: Linking cluster tree visualization with parallel coordinates and object space rendering. (Upper) Selected clusters in parallel coordinates (feature space). (Left) Radial
layout of hierarchical density cluster tree. (Right) Selected
clusters in volumetric object space (physical space).

in physical space. The physical space rendering displays all
data points that belong to the selected cluster in the respective color at its respective spatial coordinates. All three selected clusters show high magnitude of turbulence. In addition, the red cluster shows high H+ and He + + mass and
low H and He mass, while the blue cluster shows low H+
and He + + mass and high H and He mass, and the green
cluster shows medium values for H+, He + +, H, and He
mass. Interestingly, in physical space the red cluster lies between the blue and green cluster, which is not true when observing the attribute values in feature space.
5. Integrating Circular Parallel Coordinates
Parallel coordinates display successfully multidimensional
multivariate data, but for large data sets they suffer from
clutter due to overplotting polylines and clusters. The linked
view presented in the previous section avoids overplotting by
selecting individual clusters. If one is interested in observing
the entire data set with all (hierarchical) clusters simultaneously, one has to choose a different visualization. We propose to integrate circular parallel coordinates into our cluster tree visualization approach. The main idea is to display
the attributes of each cluster in a local circular parallel coordinates system that is placed at the node positions in the
radial layout. Hence, our system integrates multiple circular
parallel coordinates views in one layout. This layout supports both the comprehension of the cluster distribution and
the similarity/dissimilarity comparison of all clusters with
respect to their attribute values.
In circular parallel coordinates, the dimensions of data
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

V. Tran & L. Linsen / MultiClusterTree

829

sets are displayed as axes that emanate from the origin. The
range of the axes is scaled and starts with its minimum values next to the origin. As for standard parallel coordinates,
each data point is represented as a polyline that intersects the
axes at its values for the attributes.

Figure 6: Integrated circular parallel coordinates in clusters
tree visualization for data set with hierarchical clusters.

Figure 5: Integrated circular parallel coordinates in clusters
tree visualization for data set with 14 mode clusters.
To demonstrate the functionality of our visual analysis
system, we apply the integrated view to a synthetic data set
containing 480 data points with 10 attributes and the "out5d"
data set described in Section 4.
Figure 5 shows the integrated view applied to the synthetic data set. The data set exhibits 14 clusters, which are all
mode clusters. The circular parallel coordinates view in the
center displays the entire data set. Because of overplotting,
we cannot see how many clusters are contained and what
their distribution is. Using our cluster tree visualization with
integrated circular parallel coordinates, the user can easily
observe the value distributions of the individual clusters and
how the clusters relate to / differ from each other.
Figure 6 shows the integrated view applied to the "out5d"
data set. This data set contains a hierarchical structure, which
can be easily observed due to the cluster tree layout. Moreover, the different attribute ranges of all clusters can be investigated simultaneously.
In case of a data set with a large hierarchical structure. We
address this issue by providing a focus+context technique.
When the user drags the cursor over the display, the current
cursor position is the center of a circular focus. The focusâ€™
size is the size of one circular parallel coordinates layout.
The focus region is blown up linearly by a magnification
factor, which can also be adjusted interactively. The context
regions are linearly downscaled. For the linear scaling, the
current cursor position is chosen as a center and the linear
scaling is applied to all rays emerging from that center.
Figure 7 shows our integrated view with cluster tree
c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

Figure 7: Focus+context technique for integrated visualization with cluster tree and circular parallel coordinates.

and circular parallel coordinates when applying the focus+context technique. The cluster in the focus can be easily
investigated. It exhibits small ranges and high values for the
attributes potassium and uranium, a small range and medium
values for the attribute thorium, a small range and low values
for the attribute magnetics, and a large range in the attribute
spot. The cluster also exhibits multiple colors (red band and
yellow band) which indicates that the cluster is heterogeneous.
6. Conclusions and Future Work
We have presented a system for visual analysis of multidimensional multivariate data based on hierarchical cluster
visualization. Our system incorporates an automatic computation of hierarchical density clusters using an efficient
grid-based algorithm, visualization of the hierarchical den-

830

V. Tran & L. Linsen / MultiClusterTree

sity cluster tree using a 2D radial layout, linked views to
parallel coordinates and object space renderings, and the integration of circular parallel coordinates into the radial cluster tree layout.
The 2D radial layout of the hierarchical density cluster
tree supports an intuitive visualization to summarize the distribution structure of data set (clusters with different density
levels). The colors are assigned automatically by mapping
the HSV color space to the radial layout and allows for intuitive linking. The combination of color and opacity supports an intuitive visualization of selected clusters in a linked
parallel coordinates view. The integration of circular parallel
coordinates can solve the overplotting problem for large data
by displaying clusters in multiple views embedded into the
cluster tree layout. The linked object space view is important
in the context of spatial multi-channel data.
Our approach works well for a large number of data sets
containing up to 20 attributes, which was the category of the
data sets we wanted to tackle. For data sets with an extremely
large number of attributes, e.g., in the range of 1, 000, it is
infeasible to look at individual attributes using parallel coordinates. One would have to project such data into a lowerdimensional space first. An alternative would be to replace
the parallel coordinates with pixel matrices, which scale well
but are more difficult to explore.
The 2D radial layout of the hierarchical density cluster
tree provides more compact views and more flexible navigation and interaction techniques than a stardard dendrogram
tree layout. The more compact representation allows us to
assigned the available screen space more efficiently when
incorporating the circular parallel coordinates glyphs. One
aspect our approach does not incorporate is the display of
similarity between clusters. Such similarity visualization is
typically supported by standard dendrograms. However, to
obtain similarity measures the cluster tree must be generated in a bottom-up fashion by successively merging similar
clusters to larger one. Our clustering operates in a top-down
fashion using density rather than similarity. For future work,
it would be interesting to also investigate bottom-up strategies.

S TEIN C.: Introduction to Algorithms, Second Edition. MIT
Press and McGraw-Hill, 2001.
[CMS99] C ARD S. K., M ACKINLAY J., S HNEIDERMAN B.:
Readings in Information Visualization: Using Vision to Think.
Morgan Kaufmann Publishers, 1999.
[DHS01] D UDA R. O., H ART P. E., S TORK D. G.: Pattern Classification. Willey, 2001.
[FGW02] FAYYAD U., G RINSTEIN G., W IERSE A.: Information
Visualization in Data Mining and Knowledge Discovery. Morgan
Kaufmann Publishers, 2002.
[FWR99] F UA Y.-H., WARD M. O., RUNDENSTEINER E. A.:
Hierarchical parallel coordinates for exploration of large datasets.
Proceedings of IEEE Symposium on Information Visualization
(1999), 43â€“50.
[Har75]

H ARTIGAN J. A.: Clustering Algorithms. Wiley, 1975.

[Har85] H ARTIGAN J. A.: Statistical theory in clustering. Journal
of Classification 2 (1985), 62â€“76.
[HK06] H AN J., K AMBER M.: Data Mining: Concepts and Techniques. Morgan Kaufmann Publishers, 2006.
[Ins85] I NSELBERG A.: The plane with parallel coordinates. Visual Computer 1 (1985), 69â€“97.
[JD88] JAIN A. K., D UBES R. C.: Algorithms for Clustering
Data. Prentice Hall, 1988.
[JLJC05] J OHANSSON J., L JUNG P., J ERN M., C OOPER M.: Revealing structure within clustered parallel coordinates displays.
Proceedings of the Proceedings of the 2005 IEEE Symposium on
Information Visualization (2005), 125â€“132.
[JLJC06] J OHANSSON J., L JUNG P., J ERN M., C OOPER M.: Revealing structure in visualizations of dense 2d and 3d parallel coordinates. Information Visualization 5, 2 (2006), 125â€“136.
[JTJ04] J OHANSSON J., T RELOAR R., J ERN M.: Integration of
unsupervised clustering, interaction and parallel coordinates for
the exploration of large multivariate data. Proceedings of the
Information Visualisation (2004), 52â€“57.
[MM08] M C D ONNELL K. T., M UELLER K.: Illustrative parallel coordinates. Computer Graphics Forum 27, 3 (2008), 1031â€“
1038.
[SN07] S TUETZLE W., N UGENT R.: A generalized single linkage method for estimating the cluster tree of a density. Technical
Report (2007).
[SS02] S EO J., S HNEIDERMAN B.: Interactively exploring hierarchical clustering results. IEEE Computer 35, 7 (2002), 80â€“86.
[SS04] S COTT D. W., S AIN S. R.: Multidimensional Density
Estimation, in Handbook of Statistics, Vol 23: Data Mining and
Computational Statistics, Edited by C.R. Rao and E.J. Wegman.
Elsevier, Amsterdam, 2004.

References

[TBET99] T OLLIS I. G., BATTISTA G. D., E ADES P., TAMAS SIA R.: Graph Drawing: Algorithms for the Visualization of
Graphs. Prentice Hall, 1999.

[ABKS99] A NKERST M., B REUNIG M. M., K RIEGEL H.-P.,
S ANDER J.: Optics: ordering points to identify the clustering
structure. In Proceedings of the 1999 ACM SIGMOD international conference on Management of data (1999), pp. 49 â€“ 60.

[War94] WARD M. O.: Xmdvtool: Integrating multiple methods
for visualizing multivariatedata. Proceedings of the IEEE Symposium on Information Visualization (1994), 326â€“333.

[AdOL04] A RTERO A. O., DE O LIVEIRA M. C. F., L EVKOWITZ
H.: Uncovering clusters in crowded parallel coordinates visualizations. In Proceedings of the IEEE Symposium on Information
Visualization (2004), pp. 81â€“88.
[CFF00] C UEVAS A., F EBRERO M., F RAIMAN R.: Estimating
the number of clusters. The Canadian Journal of Statistics 28
(2000), pp. 367â€“382.
[CLRS01]

C ORMEN T. H., L EISERSON C. E., R IVEST R. L.,

[Weg90] W EGMAN E. J.: Hyper-dimensional data analysis using
parallel coordinates. Journal of the American Statistical Association 21 (1990), 664â€“675.
[WL83] W ONG A., L ANE T.: A kth nearest neighbor clustering
procedure. Journal of the Royal Statistical Society, Series B 45
(1983), 362â€“368.
[WN08] W HALEN D., N ORMAN M. L.: Competition data
set and description.
IEEE Visualization Design Contest,
http://vis.computer.org/VisWeek2008/vis/contests.html (2008).

c 2009 The Author(s)
Journal compilation c 2009 The Eurographics Association and Blackwell Publishing Ltd.

