DOI: 10.1111/j.1467-8659.2011.01852.x

COMPUTER GRAPHICS

forum

Volume 30 (2011), number 6 pp. 1655–1666

Exploring Non-Linear Relationship of Blendshape
Facial Animation
Xuecheng Liu,1,2 Shihong Xia1 , Yiwen Fan1,2 and Zhaoqi Wang1

1 Institute

of Computing Technology, Chinese Academy of Sciences, China
{liuxuecheng, xsh, fanyiwen, zqwang}@ict.ac.cn
2 Graduate School of the Chinese Academy of Sciences, China

Abstract
Human face is a complex biomechanical system and non-linearity is a remarkable feature of facial expressions.
However, in blendshape animation, facial expression space is linearized by regarding linear relationship between
blending weights and deformed face geometry. This results in the loss of reality in facial animation. To synthesize
more realistic facial animation, aforementioned relationship should be non-linear to allow the greatest generality
and fidelity of facial expressions. Unfortunately, few existing works pay attention to the topic about how to measure
the non-linear relationship. In this paper, we propose an optimization scheme that automatically explores the
non-linear relationship of blendshape facial animation from captured facial expressions. Experiments show that
the explored non-linear relationship is consistent with the non-linearity of facial expressions soundly and is able
to synthesize more realistic facial animation than the linear one.
Keywords: blendshape facial animation, nonlinear relationship, motion capture
ACM CCS: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Animation

1. Introduction
Creating realistic facial animation is one of the greatest
challenges in computer graphics. Human face is a complex
biomechanical system, so it is very hard to simulate facial
motion accurately. Furthermore, human are so sensitive to
facial expression that even a tiny flaw of facial animation
could hardly escape from our attention. Despite realistic facial animation synthesis has gone a long way and many useful
methods and tools have been developed, there is still space
for improvement.
In both research and industry domains, blendshape method
is widely used to synthesize facial animation due to its efficiency and intuition. In blendshape animation, the space
of potential faces is a linear subspace defined by a set of
key shapes, which are linearly blended to synthesize facial
expressions. It is illustrated as follows:

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics
Association and Blackwell Publishing Ltd. Published by
Blackwell Publishing, 9600 Garsington Road, Oxford OX4
2DQ, UK and 350 Main Street, Malden, MA 02148, USA.

Nbs

wi ei , 0 ≤ wi ≤ 1.

E=

(1)

i=1

In above equation, facial expression E is expressed as a vector
of vertexes coordinates increments relative to the ones of
neutral expression model. It is similar to key shape ei . Nbs
represents the number of key shapes. wi is blending weight.
To avoid meaningless facial expressions, blending weights
are generally restricted in the closed interval [0, 1].
There is a drawback in synthesizing realistic facial animation through linear blending of key shapes. Human face
is a complex biomechanical system composed by skeletons,
muscles, flesh, skin and so on. Therefore, non-linearity is
a remarkable feature of facial expressions. However, this
feature is ignored in blendshape method, and it inevitably
results in the loss of reality in facial animation. Taking jaw

1655

1656

X. Liu et al. / Exploring Non-Linear Relationship of Blendshape Facial Animation

(a)

(b)

(c)

Figure 1: The non-linearity of jaw motion. (a) The arc orbit (red curve) of vertex on chin during ‘Mouth Opening’. (b) In
blendshape facial animation, the orbit is linearly simplified (blue line). (c) Through adopting appropriate non-linear relationship
functions (green curves), the orbit can be well reconstructed.
motion as example, while adjusting the blending weight of
key shape ‘Mouth Opening’ gradually, the vertex on chin
should move along an approximate arc centred at temporomandibular joint [Figure 1(a)] in reality. However in blendshape facial animation, the orbit is linearized and poorly
simplified [Figure 1(b)]. Another example is ‘Eyes Closing’:
the trail of eyelid motion which should correspond with the
contour of cornea [Figure 2(a)] is simplified as a section of
line [Figure 2(b)] in blendshape facial animation.
As proposed by Pighin and Lewis, the relationship between blending weights and deformed face geometry should
be non-linear to allow the greatest generality and fidelity
of facial expressions [PL06]. However, the relationship is
linearized as wi ei in blendshape facial animation. Through
adopting non-linear relationship functions fi (wi ) that consist with the non-linearity of facial expressions, it is able
to synthesize more realistic facial animation [Equation (2)].
Taking jaw motion for example, the orbit of vertex on chin
can be well reconstructed through setting fi (wi ) as appropriate trigonometric functions [Figure 1(c)]. Non-linear relationship functions also apply to the case of ‘Eyes Closing’
[Figure 2(c)].
Nbs

fi (wi ), 0 ≤ wi ≤ 1.

E=

(2)

i=1

Unfortunately, to our knowledge, few researchers have discussed the topic of exploring the non-linear relationship of

blendshape facial animation. Generally, given an arbitrary
facial model, it is very difficult to construct non-linear relationship functions fi (wi ) due to the complex biomechanics
of human face. A potential solution is physical simulation of
anatomical face model such as techniques of [KHS01] and
[TSIF05]. However, it is not easy to construct the accurate
anatomy of the given face. Moreover, it is unexplored how to
transform blending weights to facial muscles contract. Therefore, physics-based simulation is not suitable for exploring
non-linear relationship functions.
In this paper, we propose an optimization scheme that
automatically explores the non-linear relationship between
blending weights and deformed face geometry of blendshape
facial animation. The essence of proposed optimization is
searching the non-linear relationship functions that match
captured facial expressions.
Through our scheme, the explored non-linear relationship
consists with the non-linearity of facial expressions and is
able to synthesize more realistic facial animation than the
linear one. In the experiments, we first show the non-linearity
of facial expressions brought by the explored non-linear relationship functions. Then, we contrast the facial animations
which are respectively synthesized by linear relationship
functions and the explored non-linear ones. The results are
assessed through both user study and quantitative evaluation. Finally, we discuss the computational efficiency of the
algorithms presented in this paper.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

X. Liu et al. / Exploring Non-Linear Relationship of Blendshape Facial Animation

(a)

1657

(b)

(c)

Figure 2: The non-linearity of upper eyelid motion. (a) The orbit (red curve) of vertex on upper eyelid during ‘closing eyes’. (b)
In blendshape facial animation, the orbit is linearly simplified (blue line). (c) Through setting appropriate non-linear relationship
functions (green curves), the orbit is well approximated.
The remainder of this paper is organized as follows:
Section 2 reviews related works of blendshape facial animation. Section 3 shows facial motion capture. Section 4
illustrates the optimization scheme in detail. Section 5 shows
the results of experiments. Finally, we conclude and discuss
our work in Section 6.
2. Related Work
Since blendshape facial animation was first proposed by
Parke’s pioneer work in [Par72] and [Par74], it has been
widely used in both research and industry domains of facial animation due to its intuition and convenience. In this
section, we will specifically review recent work related to
blendshape facial animation.
Principle components analysis (PCA) based methods construct basic shapes of facial animation through dimension
reduction of facial expression samples. Its primary advantage is the rigid orthogonality of the constructed space. The
disadvantage is that the principle components don’t possess
visual intuition and are not suitable for manual manipulation.
Aiming at this problem, Chuang has designed three schemes
that select key shapes from facial expression samples based
on PCA results [CDB02]; Li has used region-based PCA to
automatically construct local orthogonal space from motion
capture data [LD08].

Since facial action coding system (FACS) was first proposed by Ekman in [EF77], it has been popular in facial
animation. According to FACS, facial expressions can be
presented as combinations of distinct Action Units. Each
Action Unit intuitively corresponds with a basic shape of
facial animation. In specific applications, reduced or modified versions of FACS also animate face well with improved
usability [LMX*08, CK01, Hav06, Sag06].
In light of the importance of facial animation, MPEG4 has specified criterion of facial animation synthesization
for network transmission [PF03]. In essence, MPEG-4 defines a piece linear blendshape facial animation. MPEG-4
has defined 68 parameters (FAP) to animate face. In these
parameters, FAP1 and FAP2 depict fourteen static visemes
and six basic facial expressions respectively, and the other
FAPs defined 66 blending weights that enable synthesizing
arbitrary facial expressions. In MPEG-4 facial animation, it
is a vital issue that how to construct the facial animation
table (FAT) that defines rules of facial motion. Kshirsagar,
Fratarcangeli and Jiang have made their efforts to explore
FAT [KGMT01, FSF07, DZZW02].
Besides that, other works also have strived to construct linear space of facial expressions. For example, Joshi
has proposed an automatic physically-motivated scheme
that segments blendshapes into smaller regions [JTDP03].

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1658

X. Liu et al. / Exploring Non-Linear Relationship of Blendshape Facial Animation

In above vector, Mk is composed of markers coordinates
increments, k is captured expression index, Nm is the number
of markers, For ease of illustration, we rewrite above vector
as follows:
Mk = (Mk,1 , Mk,2 , . . . , Mk,j , . . . , Mk,Nm ×3 ).
In this vector, j indicates the jth component of the vector,
and Nm × 3 is the vector dimension.
Figure 3: Illustration of facial motion capture. The left
image shows the infrared cameras arrangement. The right
one shows optical markers distribution on actor’s face, the
smaller markers are used to reveal facial expressions and the
four larger ones are used as reference of computing transformation matrix of head.

Cao and Shin have used independent components analysis (ICA) to extract a set of meaningful parameters
that were independent to each other as much as possible
[CFP03, SL09].

3. Facial Motion Capture
A passive optical motion capture system is employed to capture facial motion. In this system, 12 infrared cameras are
used to record coordinates of the optical markers attached on
actor’s face, as shown in Figure 3.
We process the captured facial expressions as follows. At
first, we transfer the marker coordinates from world coordinate system to the facial local system. We take the larger
optical markers which are stationary to actor’s head as reference and compute the transformation matrix of head using
absolute orientation algorithm [Ume91]. After that, we retarget the captured expressions to the facial model using
[LMX*08, NN01], due to that the facial model is different from actor’s face. Through above re-targeting, the captured expressions from the actor can be used to explore
the non-linearity of different facial models. It makes the
proposed method more applicable. At last, the neutral expression model is subtracted from each captured facial expression to obtain the marker coordinates increments. For
ease of representation, in the latter section of paper, we
refer the captured facial expressions as the data processed
earlier.
In essence, the proposed scheme in this paper is independent of the specific data form of captured facial expressions.
Coordinates of sparse markers, points cloud, re-targeted or
copied expressions, are all applicable.
Each captured facial expression is presented as following
transversal vector:
Mk = (Mk,1,x , Mk,1,y , Mk,1,z , . . . , Mk,Nm ,x , Mk,Nm ,y , Mk,Nm ,z ).

We further present the captured facial expressions as the
following matrix:
⎡

M1,1

⎢
⎢ M2,1
⎢
M=⎢
⎢ .
⎢ ..
⎣
MNf ,1

M1,2

···

M2,2

···

..
.

..

MNf ,2

.

M1,Nm ×3

⎤

⎥
M2,Nm ×3 ⎥
⎥
⎥.
⎥
..
⎥
.
⎦

(3)

· · · MNf ,Nm ×3

In the matrix of Equation (3), each row indicates a captured
facial expression, and Nf is the expressions number.
We have captured two groups of facial expressions. The
first group is used to analyse and parameterize the nonlinear relationship functions fi (wi ). To do that, the actor acts
several basic facial actions such as ‘Open mouth’, ‘Close
eyes’, ‘Raise lip corners’ and so on. The motion capture
data records the facial deformation during performing above
basic actions. The second group is used to optimize the nonlinear relationship functions fi (wi ). We will optimize the
non-linear relationship functions from this group of captured facial expressions. For this purpose, the actor tries
to contract all his facial muscles to perform enough facial
expressions which are able to span the facial expression
space.

4. Explore the Non-Linear Relationship Functions
We explore the non-linear relationship functions of blendshape facial animation from captured facial expressions
through optimization technique. The scheme is illustrated
in Figure 4.
In above scheme, we first analyse and parameterize the
non-linear relationship functions in Section 4.1. It is done
through analysis of the first group expressions. Then, in
Section 4.2, we optimize the non-linear relationship functions from the second group expressions. In the optimization, the parameters of non-linear relationship functions are
regarded as optimization variables. The captured facial expressions only record the motion of sparse markers, and
above explored non-linear relationship functions are only
the ones of sparse markers. Therefore, at last, we expand
the non-linear relationship functions from sparse markers to

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

X. Liu et al. / Exploring Non-Linear Relationship of Blendshape Facial Animation

1659

(a)

(b)

Figure 4: Diagram of exploring non-linear relationship
functions.

Figure 5: (a) ( wi,k , M k,j ) samples (blue dots). (b) The fitting
results of (wi,k , M k,j ) samples through cubic polynomials (red
curves).

facial model through Radial Basis Function interpolation in
Section 4.3.

tion 4.2. Base on that, cubic polynomials are further selected because they can represent the non-linearity of facial expressions with fewer polynomial coefficients. They
fit the (wi,k , M k,j ) samples well, the fitting results are
shown in Figure 5(b).

4.1. Analyse and parameterize the non-linear
relationship functions

The cubic polynomial used in this paper is expressed as
follows:

We analyse and parameterize the non-linear relationship
functions from the first group of captured facial expressions which record the facial geometry deformation during performing basic facial actions. For each facial action,
we first selected out the expression Mmax−norm which has
maximum norm. It is regarded as the extreme expression of
this facial action. Then, the others expressions are projected
onto the extreme expression as illustrated in Equation (4),
and it forms samples of (w i,k , Mk ). In Equation (4), Mk
is a transversal vector projected onto Mmax−norm , and wi,k
is the projection weights. Some representative samples of
(wi,k , Mk,j ) are shown in Figure 5 (a), where Mk,j is the jth
component of expression Mk . They prove that the relationship between blending weights and deformed facial geometry
is definitely non-linear.

f (w) = aw 3 + bw2 + cw.
The constant item of above cubic polynomial is left out
because zero blending weights correspond with neutral facial
expression. The cubic polynomial is parameterized by the
polynomial coefficients a, b and c.

wi,k = arg min Mk − wi,k Mmax−norm , 0 ≤ wi,k ≤ 1. (4)

We set a cubic polynomial for each f i,j (wi ) which
is the jth component of relationship vector function
fi (wi ), that is, polynomial coefficients a i,j , bi,j , ci,j are
used to parameterize fi,j (wi ). We do not get above parameters from the fitting results of Figure 5(b), because it is generally impossible for actor to perform all
the expected facial actions precisely. We explore them
from the second group of captured expressions through
optimization.

We choose cubic polynomials as the form of nonlinear relationship functions fi (wi ). It is based on the
following considerations. First, polynomials contribute
to the computational efficiency of proposed optimization scheme in this paper, it will be illustrated in Sec-

In Shin’s work [SL09], cubic polynomials were used to approximate the vertexes movements due to actuation signals.
The polynomial coefficients were learnt to match motion capture data in the case of known actuation signals. Our work
is different to their one in two aspects. On the one hand,
in Shin’s work, key shapes ei are extracted from motion

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1660

X. Liu et al. / Exploring Non-Linear Relationship of Blendshape Facial Animation

capture data automatically. However in our work, key shapes
can be constructed according to specific application, it is
more neatly and usable in application. On the other hand, in
[SL09], blending weights wi are computed from motion capture data through independent components analysis (ICA).
In our work, blending weights together with polynomial coefficients are adjusted in the optimization. It guarantees the
best matching to motion capture data.

4.2. Optimize the non-linear relationship functions
In this section, we optimize the non-linear relationship functions of blendshape facial animation from the second group
captured facial expressions through optimization technique.
The purpose of the proposed optimization is to search the
non-linear relationship functions that are able to reproduce
the captured expressions. For that, we define the fitting energy as the target function of optimization.
Nf

Efitting =

Nbs

Mk −
k=1

2

fi (wi,k ) .
i=1

In above fitting energy, Mk is a captured expression; wi,k
bs
is the blending weight of this facial expression. N
i=1 fi (wi,k )
is the synthesized expression generated by non-linear relationship functions. E fitting represents the difference between the captured facial expressions and the synthesized
ones.
In this optimization, variables are composed of parameters of non-linear relationship functions a i,j , bi,j , ci,j and
blending weights of synthesized facial expressions w i,k . Because of the large amount of optimization variables (more
than 30,000 in our experiments), minimizing E fitting in global
variables space results to over-fitting. It means that the numerical optimal non-linear relationship functions are able to
synthesize captured facial expressions most accurately, but
their visual meanings miss. The over-fitted non-linear relationship functions are not the desired ones, and the over-fitted
results will be illustrated in experiments.
To avoid over-fitting, we restrict the optimization variables in an appropriate subspace rather than global space.
In the application of blendshape facial animation, users usually design a set of key shapes ei to animate face. These
shapes have specific visual meanings, such as facial action
units, basic expressions or visemes. In the optimization, other
than only being able to synthesize captured facial expressions accurately, the desired non-linear relationship functions fi (wi ) should also preserve the visual meanings of their
corresponding key shapes ei . It is vital in blendshape facial
animation.
In this paper, the subspace of optimization variables is
established as follows. First, construct a set of key shapes

Figure 6: The iterative optimization algorithm.

ei of blendshape facial animation according to the specific
application. In our experiments, given the facial model, we
have constructed 29 key shapes of basic facial action units
[EF77] and visemes. Then, we rewrite the linear relationship functions wi ei as non-linear forms by setting a i,j =
0, bi,j = 0 and ci,j = ei,j . That is, the non-linear form of
wi ei is f i,j (wi ) = ei,j wi . In above formulation, ei,j and f i,j (wi )
are the jth components of key shape ei and non-linear relationship function fi (wi ) respectively, and a i,j , bi,j and ci,j are
parameters of f i,j (wi ). Finally, we restrict parameters a i,j , bi,j
and ci,j in the intervals Ai,j , B i,j and C i,j which are neighbourhoods of 0, 0 and ei,j , respectively. Also, we restrict blending
weights wi in the closed interval [0, 1] to avoid meaningless
facial expressions.
From above, the complete optimization is expressed as
follows:
Nf

Mk −

min

wi,k ,ai,j ,bi,j ,ci,j

subj ect to :

k=1

2

Nbs

fi (wi,k )
i=1

(5)

0 ≤ wi,k ≤ 1,
ai,j ∈ Ai,j , bi,j ∈ Bi,j , ci,j ∈ Ci,j .

Above optimization is a bound-constraint non-linear least
square one. It is difficult to directly solve due to large
amount of variables. Aiming at efficient solving, we propose
a two steps iteration optimization algorithm, as shown in
Figure 6.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

X. Liu et al. / Exploring Non-Linear Relationship of Blendshape Facial Animation

The flow of this algorithm can be illustrated as follows:
1. Initialize optimization variables by setting w i,k = 0, a i,j
= 0, bi,j = 0 and ci,j = ei,j .
2. Minimize E fitting through blending weights wi,k of synthesized expressions. That is, to each captured facial expression Mk , we solve following optimization:

wi,k

fi (wi,k )

(6)

i=1

This optimization is a non-linear least square one but with
a small amount (Nbs , 29 in our experiments) of variables.
Therefore, it can be solved efficiently.
3. Minimize E fitting through parameters of non-linear relationship functions a i,j , bi,j and ci,j . It can be expressed as
the following optimization:
Nf
ai,j ,bi,j ,ci,j

2

Nbs

Mk −
k=1

Nm

F (v) =

pj m hm (v),

(8)

m=1

2 −1/2
hm (v) = (dist 2 (v, vm ) + cm
)
,

cm = mint=m (dist(vt , vm )).

subj ect to : 0 ≤ wi,k ≤ 1.

min

function hm (v) [Equation (9)]. The shortest path is set as the
distance dist(vt , vm ) between two vertexes vt and vm with
regarding facial model as undigraph [LMX*08]. The RBF
training algorithm is elaborated in [Hay94].

2

Nbs

min Mk −

1661

fi (wi,k )
i=1

(7)

subj ect to : ai,j ∈ Ai,j , bi,j ∈ Bi,j , ci,j ∈ Ci,j .
Because of that polynomial function is linear to their
polynomial coefficients, above optimization is a linear
least square one. Although with a large amount of variables, it can be solved efficiently.
4. Judgment of convergence. If E fitting converges, the algorithm terminates, else jump back to step 2 for a further
loop.
In above algorithm, It is crucial to efficiently solving of the
optimization in step 3 that non-linear relationship functions
are linear to their parameters. Otherwise, the optimization
will become a non-linear one with large amount variables
which are difficult to optimize. Polynomials are linear to
their coefficients, it is an important reason of choosing cubic polynomials as the non-linear relationship functions, as
illustrated in Section 4.1.

4.3. Expand the non-linear relationship to facial model
The explored non-linear relationship functions are expanded
from sparse markers to the whole facial model through
Radius Basis Function (RBF) interpolation. We train an
RBF [Equation (8)] for each non-linear relationship function fi (wi ). In this RBF, the input is vertex coordinates v of
facial model; the output is the difference between parameters of non-linear relationship functions and the linear ones
corresponding to this vertex, it can be expressed as a i,j , bi,j
and ci,j − ei,j . Coordinates and non-linear relationship functions of markers are regarded as the training samples. In this
RBF, we choose inverse multiquadrics [Har71] as the basis

(9)

5. Experiments
We show the experiment results of proposed scheme in this
section. We first reveal the non-linearity of facial expressions brought by the explored non-linear relationship functions. Then, we contrast the facial animations respectively
synthesized by linear relationship functions and the explored
non-linear ones. The results are assessed both through user
study and quantitative evaluation. After that, we introduce the
computational efficiency of the proposed algorithms in this
paper. Finally, we illustrated the over-fitting results due to the
optimization in global variable space. In the experiments, the
facial model is composed of more than 6000 vertexes, and
the main hardware of computer is 2.67 GHz CPU and 2.0GB
RAM. We have captured about 500 expressions to optimize
the non-linear relationship functions. To the intervals of optimization variables, Ai,j and B i,j are set as [− |ei,j |, |ei,j |],
and C i,j is set as [0.8ei,j , 1.2ei,j ].
By taking ‘Mouth Opening’ as example, we show the nonlinearity of jaw motion brought by the explored non-linear
relationship functions. As shown in Figure 7, the top face geometries are generated by linear relationship functions wi ei ;
the bottom images are synthesized by the explored non-linear
ones fi (wi ); from left to right, the blending weight wi increase progressively from 0.0 to 1.0 with 0.2 intervals; the
green spherules indicate the chin vertex positions of different blending weights; the purple ones reveal the orbit of chin
vertex during blending weight changing gradually. Furthermore, the example of ‘Eyes Closing’ is shown in Figure 8.
From Figures 7 and 8 we can see that, through optimization, the explored non-linear relationship between blending
weights and deformed geometry is well consistent with the
non-linearity of facial expressions. More examples are shown
in the accompanying video.
We contrast the facial animations which are respectively
synthesized by linear relationship functions [Equation (1)]
and the explored non-linear ones [Equation (2)]. The results are assessed through user study. To do that, we have
captured another 13 clips of facial motion whose durations

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1662

X. Liu et al. / Exploring Non-Linear Relationship of Blendshape Facial Animation

spread from 10 s to 1 min. These clips of facial motion
are independent from the captured expressions which are
used to explore the non-linear relationship functions. For
each clip, we synthesize facial animations through linear
relationship functions wi ei and non-linear ones fi (wi ), respectively. Given a frame of captured facial motion Mk , to
the linear one, blending weights are calculated through optimizations of Equation (10), and facial expression is synthesized through Equation (1); to the non-linear one, blending weights are optimized through Equation (6), and facial
expression is generated according to Equation (2). After
that, the two animations and the corresponding video clip
of facial motion are displayed synchronously in the screen
abreast. The two animations are randomly placed on left or
right side, which is out of observers’ awareness. Referring
to video clips of facial motion capture placed in the middle
of screen, observers vote the more realistic facial animation,

as shown in Figure 9. Fifteen observers have participated in
the user study, and the results of vote percents are list in
Table 1.
Nbs

wi,k ei , subject to : 0 ≤ wi,k ≤ 1. (10)

min Mk −
wi,k

i=1

From this table, we can see that, the explored non-linear
relationship functions are able to synthesize much more realistic facial animation than the linear ones (89.7% vs. 10.3%).
For intuition, we list several synthesized facial expressions of user study in Figure 10. In this figure, the green
spherules denote the marker positions of synthesized facial
expressions, the red ones represent the marker coordinates
of the captured facial motion. The second row and third
row facial expressions are respectively synthesized by linear

Figure 7: The non-linearity of ‘Mouth Opening’ brought by the explored non-linear relationship functions. The top images
show that non-linearity of facial expressions is linearized by the linear relationship function wi ei . Through optimization, the
explored non-linear relationship functionfi (wi ) corresponds to the non-linearity well (bottom images). From left to right, the
blending weight of ‘Mouth Opening’ increase progressively from 0.0 to 1.0 with 0.2 intervals.

Figure 8: The non-linearity of facial expression ‘Eyes Closing’ brought by the explored non-linear relationship functions.
c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1663

X. Liu et al. / Exploring Non-Linear Relationship of Blendshape Facial Animation

visual meanings. It leads to unusability, as shown in Figure 11(a). Through setting the optimization intervals Ai,j ,
B i,j and C i,j as the neighbourhoods of 0, 0 and ei,j respectively, the over-fitting is avoided. As results, the explored
non-linear relationship functions possess their visual meanings and are available for synthesizing facial expressions, as
shown in Figure 11(b).
Figure 9: The facial animations and video arrangement of
user study. The facial animations are respectively synthesized
by linear and non-linear relationship functions, and they are
randomly placed on the left and right side of the screen. In the
middle of them, it is the video clips of facial motion capture.

relationship functions and non-linear ones from the clips of
captured facial motion. Then, we remove markers of the
third row expressions to generate the bottom ones. The top
row images are corresponding stills of facial motion capture.
We also quantitatively evaluate the reality promotion
brought by the explored non-linear relationship functions
relative to linear ones. We quantify the synthesis error as
the markers average Euclidean distance between synthesized
facial expressions and captured ones. From the 13 clips of
captured facial motion, the synthesis error brought by linear
relationship functions is 3.52 mm, and the one brought by
the non-linear ones is 0.71 mm.
The algorithms proposed in this paper can be efficiently
solved. At first, in the iterative algorithm of exploring nonlinear relationship functions, the optimization of Equation (6)
takes about 0.03 s; the one of Equation (7) takes about 1.47
s; the iterative optimization algorithm totally spend about 10
min to converge. Further, it takes about another 3 min to
expand the explored non-linear relationship functions from
sparse markers to facial model. Finally, when synthesizing facial expressions through non-linear relationship functions according to Equation (2), it takes only about 0.0006 s frame−1 .
Although the synthesization takes about three times as long
as the linear blending method of Equation (1), the high efficiency does not cause any trouble of real-time application.
In the optimization, it results in over-fitting to optimize
non-linear relationship functions in global variables space.
The over-fitted non-linear relationship functions are able to
synthesize the captured facial expressions which are used
to explore them most accurately. However, they miss their

6. Conclusion and Discussion
In this paper, we work on the issue of exploring the non-linear
relationship between blending weights and deformed face
geometry of blendshape facial animation. We explore aforementioned non-linear relationship through optimization. The
optimization searches the non-linear relationship that can
synthesize realistic facial expressions in the appropriate subspace of variables. Experiments show that the explored nonlinear relationship functions consist with the non-linearity of
facial expressions well. Also, more realistic facial animation
can be synthesized by the explored non-linear relationship
functions than the linear ones.
The proposed scheme in this paper can be used in realistic
facial animation synthesization as conveniently as the linear
blending method. In blendshape animation, to animate a facial model, a set of key shapes is first constructed. It is an initialization procedure of blendshape facial animation. Then,
blending weights are adjusted to synthesize desired facial animation. Slightly different from above flow, after construction
of key shapes, a further step is needed to explore the nonlinear relationship functions fi (wi ). It is done efficiently with
little manual operation. As another initialization step, it does
not cause trouble of usability. After exploration of fi (wi ),
users can synthesize facial animation through non-linear relationship functions as if linear ones. When adjusting blending weights, facial expressions are synthesized through nonlinear relationship functions as illustrated in Equation (2).
We have applied the proposed scheme in the applications
of both performance-driven and key frame facial animation. In performance-driven application, given one frame of
captured facial motion, the blending weights are computed
through optimization of Equation (6). Then the facial expression is synthesized from blending weights through Equation (2). We have applied it in experiments of user study, as
shown in Figure 10.

Table 1: The vote percents of user study.

Clip index

1

2

3

4

5

6

7

8

9

10

11

12

13

Total

Non-linear
Linear

100
0

93.3
6.7

100
0

86.7
13.3

93.3
6.7

66.7
33.3

100
0

100
0

80
20

86.7
13.3

86.7
13.3

80
20

93.3
6.7

89.7
10.3

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1664

X. Liu et al. / Exploring Non-Linear Relationship of Blendshape Facial Animation

Figure 10: The contrast of facial expressions respectively synthesized by linear and non-linear relationship functions in user
study. The second row expressions are synthesized by linear relationship functions, and the third and bottom row images are
synthesized by the non-linear ones. The green spherules indicate the synthesized facial motion, the red ones mark the captured
ones.

(a)

(b)

Figure 11: Illustration of the over-fitting of non-linear relationship functions by taking ‘Mouth Opening’ as example.
(a) The over-fitted result of ‘Mouth Opening’. It is done
through optimization in the global variable space. (b) By restricting the optimization variables in appropriate intervals,
over-fitting is avoided.
The scheme can also be used in key frame facial animation.
First, users may configure key frames expressions through
manually adjusting blending weights, as shown in Figure 12.
Based on that, the other frames blending weights can be obtained by interpolating the ones between key frames. Finally,
the expression of each frame is synthesized through Equation (2). We have demonstrated several expressions results
of key frame facial animation in Figure 13.
In our work, the geometric constraints are ignored, which
may result in some artefacts. For example, when closing
eyes, the upper and lower eyelids may penetrate into each

Figure 12: Configuration of key frames expressions through
manually adjusting blending weights.

other in extreme cases. In blendshape facial animation, user
may achieve the geometric constraints through designing key
shapes carefully. However, the geometric constraints may
be broken during optimization. So in future work, we will
take the geometric constraints into consideration and try to
add equality constraints into the optimization to improve the
scheme.

Acknowledgments
This paper was supported in part by the National Natural Science Foundation of China, No. U0935003 and No. 60970086.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

X. Liu et al. / Exploring Non-Linear Relationship of Blendshape Facial Animation

1665

Figure 13: Results of key frame facial animation. Given blending weights of key frames (red bars), the ones of other frames
(slider block) are obtained through interpolation between key frames. The facial expressions are then synthesized from blending
weights through non-linear relationship functions.

References
[CDB02] CHUANG E. S., DESHPANDE H., BREGLER C.: Facial
expression space learning. In PG ’02: Proceedings of the
10th Pacific Conference on Computer Graphics and Applications (Washington, DC, USA, 2002), IEEE Computer
Society, p. 68.
[CFP03] CAO Y., FALOUTSOS P., PIGHIN F.: Unsupervised
learning for speech motion editing. In SCA ’03: Proceedings of the 2003 ACM SIGGRAPH/Eurographics Symposium on Computer Animation (Aire-la Ville, Switzerland,
2003), Eurographics Association, pp. 225–231.
[CK01] CHOE B., KO H.-S.: Analysis and synthesis of facial
expressions with hand-generated muscle actuation basis.
In Proceedings of the IEEE Computer Animation Conference (2001), IEEE Computer Society, pp. 12–19.
[DZZW02] DALONG J., ZHIGUO L., ZHAOQI W., WEN G.: Animating 3d facial models with mpeg-4 facedeftables. In
SS ’02: Proceedings of the 35th Annual Simulation Symposium (Washington, DC, USA, 2002), IEEE Computer
Society, p. 395.
[EF77] EKMAN P., FRIESEN W. (Eds.): Manual for the Facial
Action Coding System. Consulting Psychologists Press,
Palo Alto, 1977.
[FSF07] FRATARCANGELI M., SCHAERF M., FORCHHEIMER R.:
Facial motion cloning with radial basis functions in

mpeg-4 fba. Graphical Models 69, 2 (2007), 106–
118.
[Har71] HARDY R. L.: Multiquadric equations of topography
and other irregular surfaces. Journal of Geophysics Research 76 (1971), 1905–1915.
[Hav06] HAVALDAR P.: Sony pictures imageworks. In Proceedings of the SIGGRAPH ’06: ACM SIGGRAPH 2006
Courses (New York, NY, USA, 2006), ACM Press, p. 5.
[Hay94] HAYKIN S.: Neural Networks: A Comprehensive
Foundation. Prentice-Hall PTR, Upper Saddle River, NJ,
USA, 1994.
[JTDP03] JOSHI P., TIEN W. C., DESBRUN M., PIGHIN F.:
Learning controls for blend shape based realistic facial animation. In Proceedings of the 2003 ACM
SIGGRAPH/Eurographics Symposium on Computer Animation (SCA ’03) (Aire-la-Ville, Switzerland, 2003), Eurographics Association, pp. 187–192.
[KGMT01] KSHIRSAGAR S., GARCHERY S., MAGNENATTHALMANN N.: Feature point based mesh deformation applied to mpeg-4 facial animation. In
DEFORM ’00/AVATARS ’00: Proceedings of the
IFIP TC5/WG5.10 DEFORM’2000 Workshop and
AVATARS’2000 Workshop on Deformable Avatars (Deventer, The Netherlands, 2001), Kluwer, B.V., pp. 24–
34.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1666

X. Liu et al. / Exploring Non-Linear Relationship of Blendshape Facial Animation

[KHS01] K¨AHLER K., HABER J., SEIDEL H.-P.: Geometrybased muscle modeling for facial animation. In Proceedings of the GRIN’01: No description on Graphics Interface
2001 (Toronto, Ont., Canada, Canada, 2001), Canadian
Information Processing Society, pp. 37–46.
[LD08] LI Q., DENG Z.: Facial motion capture editing
by automated orthogonal blendshape construction and
weight propagation. In Proceedings of the IEEE Computer
Graphics and Applications 2008 (2008), IEEE Computer
Society, pp. 76–82.
[LMX*08] LIU X., MAO T., XIA S., YU Y., WANG Z., : Facial
animation by optimized blendshapes from motion capture
data. Computer Animated Virtual Worlds 19, 3–4 (2008),
235–245.
[NN01] NOH J.-Y., NEUMANN U.: Expression cloning. In SIGGRAPH ’01: Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques
(New York, NY, USA, 2001), ACM Press, pp. 277–
288.
[Par72] PARKE F. I.: Computer generated animation of faces.
In ACM ’72: Proceedings of the ACM Annual Conference
(New York, NY, USA, 1972), ACM Press, pp. 451–457.
[Par74] PARKE F. I.: A Parametric Model for Human Faces.
PhD thesis, The University of Utah, 1974.

[PF03] PANDZIC I. S., FORCHHEIMER R. (Eds.): MPEG-4 Facial Animation: The Standard, Implementation and Applications. John Wiley & Sons, Inc., New York, NY, USA,
2003.
[PL06] PIGHIN F., LEWIS J. P.: Facial motion retargeting. In
Proceedings of SIGGRAPH ’06: ACM SIGGRAPH 2006
Courses (2006), ACM Press, p. 2.
[Sag06] SAGAR M.: Facial performance capture and expressive translation for king kong. In Proceedings of SIGGRAPH ’06: ACM SIGGRAPH 2006 Sketches (New York,
NY, USA, 2006), ACM Press, p. 26.
[SL09] SHIN H., LEE Y.: Expression synthesis and transfer in
parameter spaces. Computer Graphics Forum 28 (2009),
1829–1835.
[TSIF05] TERAN J., SIFAKIS E., IRVING G., FEDKIW R.: Robust quasistatic finite elements and flesh simulation.
In SCA ’05: Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation (New York, NY, USA, 2005), ACM Press,
pp. 181–190.
[Ume91] UMEYAMA S.: Least-squares estimation of transformation parameters between two point patterns. IEEE
Transactions on Pattern Analysis and Machine Intelligence 13, 4 (1991), 376–380.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

