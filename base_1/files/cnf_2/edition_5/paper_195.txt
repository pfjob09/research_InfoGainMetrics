DOI: 10.1111/j.1467-8659.2011.02051.x
Pacific Graphics 2011
Bing-Yu Chen, Jan Kautz, Tong-Yee Lee, and Ming C. Lin
(Guest Editors)

Volume 30 (2011), Number 7

Flexible Splicing of Upper-Body Motion Spaces on
Locomotion
B. J. H. van Basten 1 and A. Egges1
1 Games

and Virtual Worlds Group, Utrecht University, The Netherlands

Abstract
This paper presents an efficient technique for synthesizing motions by stitching, or splicing, an upper-body motion
retrieved from a motion space on top of an existing lower-body locomotion of another motion. Compared to the
standard motion splicing problem, motion space splicing imposes new challenges as both the upper and lower
body motions might not be known in advance.
Our technique is the first motion (space) splicing technique that propagates temporal and spatial properties of the
lower-body locomotion to the newly generated upper-body motion and vice versa. Whereas existing techniques
only adapt the upper-body motion to fit the lower-body motion, our technique also adapts the lower-body locomotion based on the upper body task for a more coherent full-body motion. In this paper, we will show that our
decoupled approach is able to generate high-fidelity full-body motion for interactive applications such as games.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism—Animation

1. Introduction
In modern games, a virtual character often needs to perform several tasks that involve a combination of locomotion
and upper body actions. For example, the character needs to
pick up a gun while walking past a table. Even very mundane tasks, such as opening a door, often involve such a
combination. Therefore animation systems should ideally be
able to generate motions for every possible combination of
lower body locomotion and upper body actions. Unfortunately, these combined tasks are often poorly animated in
games. The naive, but very common, approach is to animate
locomotion and manipulation sequentially. This is very unnatural, as human motion is composite and locomotion and
upper body manipulation often happens simultaneously.
One of the most common techniques nowadays for synthesizing human motion is by recording it using motion capture systems. However, recording all combinations of various upper and lower body actions yields a combinatorial
explosion and the number of required recordings grows exponentially. In order to reduce the need for new motions,
motion splicing techniques have been investigated. Motion
splicing is the technique of transferring an existing upper
c 2011 The Author(s)
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd. Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ,
UK and 350 Main Street, Malden, MA 02148, USA.

body motion MU to a lower body motion ML . Ideally, instead of recording nu · nl motions, where nu is the number
of possible upper body actions and nl is the number of motions of locomotions, splicing would reduce the number of
required motions to nu + nl .
An extension of this problem is motion space splicing.
Here, the problem is to splice the resulting motions coming
from motion spaces [KG04, WH97]. A motion space M
uses a database of motions parameterized on a high-level
parameter such as the position of the apex of a reach or
the curvature of walking. In case the user desires a motion
corresponding to a parameter p not present in the database,
nearby motions in parameter space are then interpolated (or:
blended) with weights based on the distance to p to generate
a new motion M(p). All possible interpolations of motions
then comprise the motion space. We will elaborate more
on motion blending in Section 3. Motion space splicing
imposes a new challenge, as both the upper and lower body
motion are not known in advance and can be any motion out
of the corresponding parameter space.

1964

B.J.H van Basten & A. Egges / Splicing Motion Spaces

The problem on which we focus in this paper can be informally stated as follows: given an upper-body motion space
MU consisting of n example motions, parameterized on a
positional end-effector parameter p and all recorded during
locomotion, a base motion of locomotion ML and a desired
parameter pq , generate a coherent full body motion by splicing the resulting upper body motion MU (pq ) on ML .
Splicing upper-body motion spaces on locomotion reduces the need for additional recordings, as there is no need
for recording separate upper-body motion spaces for each
desired lower-body locomotion. In this paper, we use Cartesian Motion Blending (CMB) [vBSE11] for blending the
new upper body motion. The upper body motion space can
consist of upper body actions that are parameterized on a positional end-effector parameter, such as reaching, stabbing or
punching, of which the example motions are recorded during
a form of locomotion. The former assumption (CMB) is required for fast adaption of the upper body motion space, the
latter assumption (recorded during locomotion) is required
for effective temporal matching, as we will see in Section 5.
This paper is organized as follows: in Section 2 we provide an overview of existing splicing techniques. In Section
3 we will elaborate on Cartesian motion blending and explain the usefulness of this technique in the splicing process.
In Sections 4 and 5 we will explain our motion space splicing technique and show its results in Section 6. Section 7
will conclude the paper and provides possibilities for future
work.
Our contribution
Our technique is, to our knowledge, the first motion (space)
splicing technique that propagates temporal and spatial properties of the upper body to the lower body and vice versa.
Whereas existing splicing techniques only adapt the upper
body part to fit the lower body part, our technique considers both parts to be mutually dependent and have them both
influence each other, both temporally and spatially. For example, our technique allows for small changes to the pelvis
trajectory of the lower-body base motion, based on the newly
generated upper-body motion, for a more coherent spliced
motion. We provide the user with 3 intuitive parameters to
determine the influence both body parts should have on each
other.
Second, as Cartesian Motion Blending (see Section 3) always yields exact results, there is no need for pseudosampling. This is an advantage over existing techniques, as the
point cloud of pseudosamples does not need to be adapted
[HH08] when adjusting the upper body motion space. We
will elaborate on this in Section 3.
2. Related Work
Reusing existing motions has received a lot of attention
over the last decade. Basically, there are two main classes

of these example-based approaches. Motion concatenation
techniques focus on stitching existing clips of motion together into new motion [KGP02]. Motion parameterization
techniques interpolate between two or more existing motions
to generate new motions corresponding to a specific highlevel parameter such as end-effector position or locomotion
curvature [WH97, KG04]. Motion concatenation generally
yields more natural motion, while motion parameterization
offers a higher level of control.
Motion splicing techniques can be considered a third class
of example-based techniques. Here, a motion A, possibly defined for a subset of the DoF (such as only the upper body), is
transplanted to a motion B. For example, an upper body waving motion can be spliced on top of a walking motion. We
need to distinguish offline and online splicing techniques.
Offline techniques aim at enriching the database in a preprocessing step to improve the expressiveness of the database.
Online techniques aim at splicing auxiliary motions on base
motions in real-time, where both the auxiliary or base motion might not be known beforehand. We will start by elaborating on offline techniques. We will define MU as a wholebody motion of an upper body action (such as waving) and
ML as a whole-body base motion (such as walking).
Ikemoto and Forsyth [IF04] randomly transplant body
part motions on other motions to enrich the database. After transplantation, a support vector machine is used to classify resulting motions as natural or not. This technique has
been extended by Jang et al. [JLLL08] who only swap upper body motions if their lower-body motion is similar. Instead of a brute-force approach, some techniques focus on
splicing two selected MU and ML as good as possible, Heck
et al. [HKG06] present a splicing technique that explicitly
applies time, spatial and posture alignment. To find corresponding frames in MU and ML , the dynamic time warping
(DTW) technique based on a posture similarity metric from
Kovar and Gleicher [KG03] is used. The spatial alignments,
applied solely on the upper body, enforces the upper body of
MU to face the same general direction as the upper body of
the lower body motion ML and aligns the shoulders. DTW
has recently been improved by Chen et al. [CMC09], where
input motions are registered using a parameterized motion
model.
Whereas the previous techniques consider the lower-body
motion of ML as fixed, Al-Ghreimil and Hahn [AGH03] also
adapt the lower-body of ML . They subtract the upper-body
auxiliary motion MU from ML (both of them should have a
similar base motion, such as walking) to determine joint trajectory differences. These differences can then be applied on
other base motions. Oshita [Osh08] presents a similar technique. Here, the base motion is automatically determined by
filtering MU (the assumption is that auxiliary arm motions
are comprised by high-frequency motion). Then, the same
technique as Al-Ghreimil and Hahn is used by subtracting
the approximated base motion by MU . Unfortunately, both
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1965

B.J.H van Basten & A. Egges / Splicing Motion Spaces

techniques might result in foot skating. Several other techniques have been developed that take into account body part
dependencies, such as Ma et al. [MXH∗ 10], who applied it
on modeling motion variation. Tamada et al. [TKK10] splice
the results coming from a separate upper and a lower body
motion graph. The motion graphs are annotated with potential splicing points where the upper body motions are similar. For the actual splicing, DTW [KG03] and the spatial
alignment from Heck et al. [HKG06] are used. Finally, Majkowska et al. [MZF06] present a technique for transplanting
hand gestures onto full body motion.
In interactive applications such as games, there is a need
for real-time coupling. Real-time coupling is different from
offline techniques as the upper or lower body motion is not
known beforehand and there is less time for computation.
Therefore, DTW is not directly applicable. Ashraf and Wong
[AW00] use a standard motion blending technique for generating separate upper and lower body motions that are then
spliced together. This technique primarily focuses on temporal alignment. Ha and Hahn [HH08] present a technique to
splice upper and lower body motion spaces. Because DTW
is too time consuming, Ha and Hahn determine a time alignment from all example motions to reference motions, which
are then used to temporally align the blended upper and
lower body motion. For the temporal alignment, they also
use the DTW algorithm from Kovar and Gleicher [KG03]
but note that this technique will fail when the lower body
motions are fairly dissimilar. In that case they propose manual key event annotation. After the upper and lower body
are temporally aligned, they use the spatial alignment technique from Heck et al. [HKG06] to align upper body motions. In order not to resample the entire upper body parameter space [KG04] when a new ML has been generated, the
change of the upper body parameter space due to a new ML is
approximated. As our technique does not need pseudosamples, there is no need to adapt the pseudosample point cloud.

pe
p1
pq

(a)

p2

p1
pq

p2

(b)

Figure 1: In general, motion blending does not yield a linear
parameterization.
of the motions that corresponds to pq . Many variations exist,
but in general, the blend weights are determined based on the
distance of the blend candidates to pq in parameter space, either linear [WH97] or using radial basis functions [RSC01].
Some techniques consider all example motions as blend candidates [RSC01], some only the k-nearest [KG04]. Once the
blend candidates and weights are determined, the resulting
motion is constructed by interpolating all joint orientations
of every corresponding set of frames.
Unfortunately, because of the non-linearity of the orientation domain and the constraints defined by the rigid bones
of a skeleton, motion blending will not yield a linear parameterization of the space. An example is shown in Figure 1
where a 1D parameter space is spanned by the dashed line.
Here, two postures of a 1 DoF segment with end-effector
position p1 and p2 are interpolated to generate a posture at
end-effector position pq . Clearly, the resulting end-effector
position pe will not coincide with pq . In case of a longer
chain of joints, such as a humanoid skeleton, this local error will accumulate along the joints of the skeleton and, in
case of reaching, the character will miss the target position.
Therefore techniques have been developed to reduce the error, such as filling the parameter space with artificially generated pseudo samples [KG04], iterative adjustment of the
query position [RSC01] or inverse kinematics.

3. Cartesian Motion Blending
In our framework, the upper body motion blending technique is based on Cartesian Motion Blending (CMB)
[vBSE11]. In this section, we will explain the concept of
motion parameterization by blending and elaborate how this
is done using CMB.
Motion parameterization by blending can be formally defined as: given a set of n example motions M, each corresponding to a parameter value p ∈ P where P is a ddimensional parameter space, and a query parameter value
pq , interpolate a set of blend candidates B ⊆ M such that
the resulting motion corresponds to pq . Clearly the resulting
motion depends on the interpolation scheme. We define a
parameter mapping f : M → P as the function that retrieves
the parameter value p from a motion M. In case of motion
parameterization, we are looking for the inverse function
f −1 : P → M. Given a query parameter value pq , find one
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

Cartesian Motion Blending (CMB) [vBSE11] is a motion
blending technique that does not only allow interpolation
of joint orientations, but also joint positions. When applied
properly, interpolation of motions expressed in the representation depicted in Figure 2, will result in (reaching) motions
exactly corresponding to the desired parameter. In the skeleton in Figure 2, the color of the joints represents the interpolation technique used. The trajectories of red joints (socalled Cartesian joints) are determined by positional interpolation of the example motion. The green joints are determined by standard orientation interpolation. We also store
the swivel angles of the legs and arms, these are depicted
as circular arrows. The blue joints are, in our applications,
calculated by either simple, highly-efficient inverse kinematics or orientation blending. The transparent joints can be retrieved using closed-form solutions based on the swivel angles and do not need to be stored. The main advantage of

1966

B.J.H van Basten & A. Egges / Splicing Motion Spaces

4.1. Stance profile

Figure 2: A representation suitable for CMB.
Cartesian Motion Blending is that the resulting motion will
exactly correspond to the desired parameter value, in contrast to motion blending techniques based on interpolation
of joint orientations. Due to the positional interpolation, the
resulting reach position becomes linear in the blend weights.
Therefore there is no need for pseudosamples. For a more
thorough explanation of CMB, we refer the reader to Van
Basten et al. [vBSE11]. Although it is possible to generate
whole-body motions using CMB, in this paper, we will use
CMB to only generate upper body motions, of which examples are shown in Figure 3. We will elaborate on the details
of the Cartesian Motion Blending process in Section 5.2.

We are going find corresponding frames from ML and the
result coming from MU based on their support phases (recall that the examples in MU are recorded during locomotion). For each reaching motion in MU we will derive the
footdowns using a height and velocity-based footstep detector. In Figure 4, the footdowns are depicted as red bars. The
upper row are left foot downs, the lower row are right footdowns, with time going to the right. The moment of reach is
depicted as a green line. As can be seen in Figure 4, we store
2 stances before and after the stance at which the reach takes
place. This can be more in case the upper body action takes
longer. All reach motions are temporally normalized using
a piecewise linear time curve T (t), where t is the absolute
time in the animation. Key events, which are the footdowns
and the moment of reach, are mapped on a normalized time
T . The normalized time of the reach apex is T (t4 ) = 2.5.
Note that we also use the duration between key events. The
duration between key event ti and ti+1 is denoted as ∆ti .
L
R
t1

t2

t3

t4 t5

t6

t7

∆t6
∆t1
∆t2
∆t3 ∆t4 ∆t5
T (t1) = 0 T (t2) = 1
T (t3) = 2 T (t5) = 3 T (t6) = 4 T (t7) = 5
T (t4) = 2.5

Figure 4: A canonical representation of an upper body manipulation.

4.2. Reach space

Figure 3: Cartesian Motion Blending yields exact reaching motions, by using only a small set of example motions
(shown as brown spheres).

4. Preprocessing phase
In order to efficiently splice (or: couple) the result coming
from the upper-body motion space MU and the existing motion with lower-body locomotion ML , we need to compute
some properties of the upper-body motions in MU in a preprocessing stage. Although we are primarily focused on the
upper-body action, each motion Mr ∈ MU is a whole-body
motion and is recorded during locomotion. For the remainder of this paper, we will consider the upper-body actions
to be reach motions, but, as mentioned in the introduction,
it can be any upper-body motion that is parameterizable on
a positional end-effector parameter, such as punching (of
which we will show an example) or stabbing.

In order to generate upper-body reaching motions from MU ,
we parameterize MU on the 3D reach position f (Mr ) of
the reach examples Mr ∈ MU . Recall that f (M) retrieves
the parameter of a motion. Every example motion corresponds to a point in this 3D parameter space. In order to
efficiently retrieve nearby motions during the blending process, we construct a Delaunay tetrahedralization. We create
separate tetrahedralizations for left and right reaches. In Section 5.2 we will elaborate on how we will generate a reaching
motion MU from MU using CMB.
4.3. Coupling space
In this section we will elaborate on the datastructure (the
coupling space) that we will use to adapt the pelvis trajectory of ML given a desired reaching position. First, for each
reaching motion Mr ∈ MU , we extract the local pelvis trajectory. This is the original pelvis position porig (T ) and orientation qorig (T ) (where T is normalized time) from Mr expressed in a coordinate system aligned with a filtering of the
original pelvis trajectory. This filtered trajectory is called the
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

B.J.H van Basten & A. Egges / Splicing Motion Spaces

path abstraction. We use a Gaussian filter with shrinkage
correction [vBGE11] for filtering porig (T ) and an orientation filter [LS02] for filtering the pelvis orientation qorig (T ).
The abstraction coordinate frame at the moment of reach
is shown in Figure 5. Here, the blue curve is the original pelvis trajectory and the green curve its path abstraction. Note that such a path abstraction has both a positional and a rotational component. We denote the positional
component of the path abstraction at normalized time T as
pabstract (T ) and the rotational component as qabstract (T ).
After the path abstraction is determined, we express the original pelvis orientation qorig (T ) and position porig (T ) (the
green sphere in Figure 5) in this coordinate system to get
the local displacement of the pelvis. So, the local orientation
qlocal (T ) of the pelvis at normalized time T can be determined by q−1
abstract (T )qorig (T ). The local position plocal (T )
can then be determined by rotating (porig (T ) − pabstract (T ))
3
by q−1
abstract (T ). The local pelvis trajectory f local : T → (S ×
3
R ) is then comprised by (qlocal (T ), plocal (T )).

Figure 5: We express the reach position (red sphere) in the
path abstraction coordinate frame.
The local pelvis trajectory is then considered as six additional degrees of freedom and stored within the reaching
animation Mr . Furthermore, each reaching motion Mr is annotated with its reach apex position (the red sphere in Figure
5) expressed in the abstraction coordinate frame and denoted
r
as pM
abstract . We will define a reach position expressed in the
abstraction coordinate frame as an abstract reach position.
Now, we set up a second 3D parameter space that we will call
the coupling space Mcoupling and let each reaching motion
r
Mr correspond to a point pM
abstract . Just like MU , we also triangulate Mcoupling for fast retrieval of nearby motions. But,
in contrast to MU , Mcoupling is parameterized on the abstract
reach position and per example motion we only store the 6
DoF for the local pelvis trajectory. Basically, multiscattered
interpolation in the coupling space defines a mapping between an abstract reach position and a local pelvis trajectory
flocal (t). We will use this coupling space in Section 5.1.1 to
determine the adaption of the pelvis trajectory of ML based
on the desired reach position.
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1967

5. Interactive Splicing
In this section we will explain how we will apply motion
space splicing in an interactive setting. Given a motion with
lower-body locomotion ML , possibly generated by another
motion space such as [KG04, vBSE11], and a desired reach
position pq , we would like to blend a reaching motion from
MU and splice this on ML .
We first determine at which frame of ML we want the apex
of the reach to occur. When looking at the canonical representation described in the previous section and shown in Figure 4, we have observed that, in general, the apex of the reach
in the example motions is often between the stances starting
at T = 2 and T = 3. In case of a right-handed reach, the last
supporting stance at the apex of the reach is on the right foot
too. We determine the average relative time of the reach in
the window T = [2, 3] (of all the reaches in MU ). This is generally around 0.5 meaning that most reaches have their apex
(in absolute time) roughly in-between the stances starting at
T = 2 and T = 3. Now, in case of a right reach, we determine
the right stance in ML whose pelvis position is closest to pq .
This stance is shown in green in Figure 7. The moment of
reach is then (time wise) halfway the window starting at that
stance and the next. Let t1L , . . . ,t7L be the absolute timekeys
of the corresponding key events of ML where t4L is the desired moment of reach. These timekeys are also depicted in
Figure 7. In the next sections we will elaborate on the spatial
transfer. We adapt both ML as well as the example motions
in MU . Then, we can generate the upper body motion MU .
Based on the result of this blend, we can transfer temporal
properties of MU to ML .
5.1. Spatial Transfer
Ideally, when the target reach position is relatively far away
from the character, the pelvis should be able to move and
rotate towards the target. So, basically, the upper body task
should influence the lower body motion. Our technique allows adaption of the pelvis trajectory in ML for a more coherent full body motion. Second, in case ML is a sneaking
motion with a bended spine, we prefer the (bended) upper
body posture of ML to be transfered to MU . Our technique
allows both body parts to influence each other. First, we will
elaborate on transferring spatial properties from the upper
body task to the lower body in Section 5.1.1. In Section 5.1.2
we will elaborate on the transfer of spatial properties of the
lower body motion ML to the upper body motion MU .
5.1.1. Upper to Lower Body
In order to adapt ML based on the desired reach position
pq , we use the coupling space from Section 4.3 as approximation of the local displacement of the pelvis. We express
pq in the path abstraction coordinate frame of ML , denoted
q
by pabstract . We then apply multiscattered data interpolation by interpolating nearby examples in the coupling space

1968

B.J.H van Basten & A. Egges / Splicing Motion Spaces

Mcoupling with weights linearly based on their distance to
q
pabstract . The 4 example motions from Mcoupling that we
blend are the 4 vertices of the tetrahedron that contains
q
pabstract .
Basically, we are motion blending the 4 local pelvis trajectories flocal (T ) of nearby motions from Mcoupling . The positional component is linearly interpolated, the rotational component is interpolated in the exponential map. This yields a
blended local pelvis trajectory fblend . We also linearly interpolate the durations ∆t1 , . . . , ∆t6 between key events of
the example motions in Mcoupling (see Figure 4), which
we denote as ∆t1blend , . . . , ∆t6blend . The absolute duration of
fblend is ∑6i=1 ∆tiblend . To find the corresponding timekeys
between fblend (t) and ML (with t ∈ [0, . . . , ∑6i=1 ∆tiblend ]), we
can now construct a piecewise linear timewarp curve λ(t)
L
with key event pairs {(∑ic=0 ∆tcblend ,ti+1
)} , with i ∈ [0, 6]
blend
and ∆t0
= 0, that maps timekeys from fblend (t) to corresponding timekeys of ML . So, the reach apex occurs in fblend
at ∆t1blend + ∆t2blend + ∆t3blend and in ML at t4L .
The blended local pelvis trajectory fblend (t) can then easily be transfered to the abstract pelvis motion of ML . Again,
we denote the positional component of the path abstraction of ML at time λ(t) as pLabstract (λ(t)) and the rotational
component as qLabstract (λ(t)). Recall that λ is the timewarp
curve that maps timekeys from the blended local pelvis trajectory fblend (t) to ML . Let qblend (t) and pblend (t) be the
rotational and positional component of fblend (t). We only
adapt ML in the absolute time range of t1L to t7L (See Figure
7). The adapted pelvis orientation qadapt (λ(t)) of ML will
be qLabstract (λ(t)) · qblend (t) and the adapted pelvis position
padapt (λ(t)) will be pblend (t) rotated by qLabstract (λ(t)) and
then translated by pLabstract (λ(t)).
We would like to provide the user with a parameter
wU→L
spatial ∈ [0, 1] to indicate the influence that the upper body
reaching should have on the lower-body pelvis trajectory.
Let qLorig (t) and pLorig (t) with t ∈ [t1L , . . . ,t7L ] be the original rotational and positional component of the pelvis trajectory of ML , the final pelvis position p f inal (t) at time t
of ML will then be p f inal (t) = wU→L
spatial · padapt (t) + (1 −
L
wU→L
)
·
p
(t)).
The
final
pelvis
orientation q f inal (t) will
orig
spatial
be q f inal (t) = slerp(qLorig (t), qadapt (t), wU→L
spatial ). Basically,
we blend the original pelvis trajectory of ML with its adapted
version, where the blend weight depends on the parameter
value set by the user. So, when wU→L
spatial = 0.0, the pelvis trajectory of ML will be unaltered, and when wU→L
spatial is set to
1 the original local pelvis trajectory of ML will be fully replaced by fblend (t). Finally, q f inal (t) and p f inal (t) will then
be eased in and out on ML around t1L and t7L .
Clearly, adapting the pelvis trajectory of ML will lead to
footskating. However, using the Cartesian representation as
described in Section 3, the supporting foot can easily be
fixed and the swing foot trajectory can be kept as close as

possible to the swing foot trajectory, using simple closedform solutions and the stored swivel angles. Examples of
lower body adjustments can be seen in Figure 6. The coupling space is shown in green, the yellow ball is the desired position. The further the desired position is from the
character, the larger the pelvis displacement. Also note that
for lower desired positions, such as the one depicted in the
right figure, the pelvis is actually lowered and the knees
slightly bended. It is tempting to think that ML as a whole
can be blended with the newly generated motion from MU
(on which we will elaborate in Section 5.2). However, the
newly generated lower-body part can be very dissimilar to
ML and blending these two motions can lead to awkward or
incorrect motion.

Figure 6: The coupling space is used to estimate the adaption of the pelvis trajectory.

5.1.2. Lower to Upper Body
In case the original lower body is sneaking (with a bend
spine) or its upper body is bent in order to a avoid a collision with the head, we would like the generated upper body
motion to also reflect this. We do this by blending all the example reaching motions in the upper body motion space MU
with the upper body of ML . By adapting the example motions, we change the parameter space, and thus the resulting
motion, as shown in Figure 6. This figure shows the apex positions of the original reaching examples as brown spheres.
The end positions of the corrected examples are shown in the
lower figure as blue spheres. Note that, although the query
position pq is identical for both figures, the resulting posture
in the lower figure is closer to the upper body of the original avoiding motion and avoids collision. As the reaching
examples are all normalized in time (with T = [0, 5]) we can
easily construct a piecewise linear timewarp curve to match
the normalized timekeys of the reach examples with the key
events t1L , . . . ,t7L of ML (See Figure 7). This timewarp curve
λU→L (T ) has key event pairs {(0,t1L ), (1,t2L ), . . . , (5,t7L )}.
The user can set to what extent the example spines
are adjusted with parameter wL→U
spatial . If the spine consist of n joints, then the new orientation qnew
j (i) of spine
joint j (ordered from root to neck) of a reach example motion at timekey T ∈ [0, 5] will be: qnew
j (T ) =
orig

slerp(q j

L→U
(T ), qlow
j (λU→L (T )), w j ) where w j = wspatial −

c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

1969

B.J.H van Basten & A. Egges / Splicing Motion Spaces
wL→U
spatial

orig

j · n . q j (T ) and qlow
j (t) are the orientation of the ith spine joint of the reaching motion and the lower body
motion ML . So, we blend the spine joint orientations of the
examples with the corresponding spine joint orientations of
ML , where the blend weight gradually decreases when going up the spine, depending on the setting of wL→U
spatial . The
higher wL→U
,
the
more
the
spine
motion
of
the
examples
spatial
will be adjusted. Ha and Han [HH08] also adapt the parameter space. However, their motion space makes use of pseudosamples [KG04]. Not only does motion blending using
pseudosamples still require a small IK adjustment for exact end-effector positioning, but in case one changes one of
the original example reaches, the whole pseudosample point
cloud needs to be recalculated. Ha and Han solve this by approximating the change of the point cloud. Because we make
use of Cartesian Motion Blending, this is not necessary and
therefore more accurate and more efficient. Furthermore, it
is tempting to consider correcting the reach motions with a
similar coupling technique as Heck et al. [HKG06] (like Ha
and Han also did) that explicitly aligns the shoulders. However,this technique can be overrestricting the shoulder and
is not so suitable for motions where the shoulder motion is
important, like reaching [HKG06].
5.2. Generating the reach motion MU
After the lower body is adapted (depending on wU→L
spatial ) and
the reaching examples in the database MU are adapted (depending on wL→U
spatial ), we can generate the upper body reaching motion MU . Recall that MU is a motion space parameterized on the position of the apex of the reach in the pelvis
coordinate frame. As mentioned earlier, we will only generate the upper body part of the reaching. These are the joints
from (but not including) the pelvis upwards. Now, given
the target position expressed in the pelvis coordinate frame
q
p pelvis , we perform a point location query in the tetrahedralization of MU to determine the tetrahedron that contains
q
p pelvis . The 4 vertices of the tetrahedron are the example
blend motions, which we will denote as {M1r , . . . , M4r }. We
linearly derive the blend weights w = {w1 , . . . , w4 } such that
q
∑4c=1 wc f (Mcr ) = p pelvis with 0 ≤ wc ≤ 1 and ∑4c=1 wc = 1.
Recall that f (M) maps a motion to the wrist position at the
apex of its reach. Now that we have determined the blend
weights, we can blend the example motions to generate the
new upper body reaching motion. Note that we interpolate
all frames of the 4 blend candidates, resulting in a new reaching motion. Now, for each frame we first interpolate the positions of the upper-body red (Cartesian) joints in Figure 2.
These are the neck, shoulder and wrist joints. This results in
a wrist joint trajectory that exactly ends at the desired position. Now, the green joints are determined by interpolation
of the orientations of the joints of the blend candidates. We
interpolate the orientations in the exponential map representation [vBSE11].
As for the blue spine joints, we cannot perform Cartec 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

tL1 tL2 tL3 tL4 tL5 tL6

tL7

Figure 7: We temporally align upper body motions on ML
based on support phases.
sian interpolation because this will introduce bone stretching. In order to avoid using inverse kinematics for the spine,
we apply the following heuristic. We first estimate the spine
chain by orientation blending (and basically treat its joints as
green joints. If the wrist is still not reachable from the current shoulder position, we apply a simple cyclic coordinate
descent IK algorithm. The need for IK correction is generally small: none of the motions generated during our benchmarks (on which we will elaborate in Section 6) required
spine IK correction. After all joint trajectories have been determined, we can trivially retarget the joints to the skeleton
of the character. Figure 3 shows the apex of the motions of
two different desired positions. Note that, in contrast to standard motion blending techniques, the wrist exactly coincides
with the desired position. The blue volume in this figure is
the convex hull of the examples in MU .
Just as in Section 5.1.1, we also interpolate the durations ∆t1 . . . ∆t6 between key events (see Figure 4) of the
4 blend candidates. So, given 4 corresponding durations of
the blend candidates, where ∆t1c is the first duration between
key events for blend candidate c, then ∆tiblend = ∑4c=1 wc ∆t1c .
To find the corresponding timekeys between MU and ML ,
we can now construct a piecewise linear timewarp curve
L
λblend→L (t) with event pairs {(∑ic=0 ∆tcblend ,ti+1
)} , with
blend
i ∈ [0, 6] and ∆t0
= 0. The temporal alignment can be
seen in Figure 7. Now, using λblend→L (t) we can replace the
upper body motion of ML with the upper body motion of
MU , and we will denote the resulting motion as Mcoupled .
5.3. Temporal transfer
The temporal alignment from MU to ML is based on foot
stances. We have observed that the durations between key
events (∆ti ) can differ between ML and MU (and the motions in MU ). In case of a reaching MU (recorded during
walking) on a walking ML , we have observed that the footdowns happen faster after each other in ML than the MU .
This is logical, as there is no reaching motion in ML and
there is no reason for slowing down. When MU is temporally aligned with ML , this causes a speed up in the upper
body as the reaching motion is aligned to faster steps than the
original examples. An opposite example is splicing a punching MU (recorded during running) on a walking ML . This
causes a slowdown for a similar reason. Therefore, we want
the temporal properties of MU to influence ML based on the
parameter wU→L
temporal ∈ [0, 1] set by the user. We do this by in-

1970

B.J.H van Basten & A. Egges / Splicing Motion Spaces

terpolating the durations between key events from Mcoupled
(which are effectively those from ML ) with the durations
between key events from MU . So, ∆tiL will be replaced by
L
U→L
blend
(1 − wU→L
for i = [1, 6]. We
temporal ) · ∆ti + wtemporal · ∆ti
apply a timewarping λblend→L (t) on Mcoupled to enforce
this. The event pairs will be: {t1L + (∑0i ∆tiL ),t1L + ∑0i ((1 −
L
U→L
blend
wU→L
)}. In case wU→L
temporal ) · ∆ti + wtemporal · ∆ti
temporal is
set to 0, Mcoupled will remain the same, as the key events will
still happen at tiL for i ∈ [1, 6]. In case wU→L
temporal is set to 1, all
∆tiL will be replaced by ∆tiblend for all i, making the durations
between key events identical to MU . This techniques slows
down Mcoupled in case the duration between footdowns in
MU was larger than in ML .

6. Results
In this section we will present some results of our motion
space splicing technique. For the reaching results, we used a
database MU with 9 right reaching motions (all around 4 seconds) recorded during walking, sampled at 30 Hz. All experiments were executed on an Intel Pentium 3 GHz with 3 GB
RAM. The first example is shown in the upper figure of FigL→U
U→L
ure 8 with parameters wU→L
spatial , wspatial , wtemporal set to 0.8.
Here, ML was walking and the target position is shown as a
yellow sphere. The target position would be unreachable if
the pelvis trajectory would not have been adapted (or: when
wU→L
spatial would be set to 0.0). In the example in the lower
figure, we spliced MU on a side stepping motion with the
same parameter values as the previous example. The third
example, shown in Figure 6, shows MU spliced on a motion bending forward to avoid the red beam. In case wL→U
spatial
would be set to 0, and thus, MU would not be adapted, the
upper body posture would completely be determined from
MU , resulting in a collision with the beam, shown in the upper figure. In case we set wL→U
spatial to 1, the upper body posture
of ML is blended with the motions in MU . The resulting upper body posture of the spliced motion is more resembling to
ML , avoiding the obstacle as shown in the upper figure. So,
although direct head and spine control is not possible, there
is implicit control by setting wL→U
spatial . We also constructed a
punching motion space (recorded during running) consisting of 10 punches. In Figure 9 we show a splice of punching
(during running) on walking up a stairs. More examples (that
also illustrate temporal coupling and punching and reaching
on various forms of locomotion such as sneaking, dancing,
running, jumping and backwards walking) can be seen in the
accompanying videos. In order to analyze the performance
of our technique, we generated 200 reaching motions. We
generated and spliced 40 reaching motions (based on random desired positions) on 5 different locomotions. In our
lower-body motion test set was one straight normal locomotion, one curved locomotion, one sneaking motion, a dancing
motion and one motion where the upper body was bend over
in order to avoid a beam. Over all motions, the average computation time for 1 second of spliced locomotion was 0.02

Figure 8: Reaching (recorded during walking) spliced on
ordinary walking and side stepping.

Figure 9: Punching (recorded during running) spliced on
top of walking up a stairs.
seconds (σ = 0.004 second). On average, lower body adaption (Section 5.1.1) takes 21% of this computation time, Upper body motion space adaption (Section 5.1.2) takes 49%.
The generation of MU (Section 5.2) takes 21% and the final temporal coupling (Section 5.3) takes 9%. Although in
this paper we assume ML as given, retrieving the stances and
path abstraction from ML generally takes less than 0.01 second for 1 second of ML . Therefore ML can also be generated
online from a lower-body motion space.
7. Conclusions and Discussion
In this paper, we presented a real-time motion space splicing
technique that is able to generate coherent whole-body motion by splicing a generated upper-body motion on a motion
with lower-body locomotion. Motion space splicing reduces
the need for recording every possible combination of upperbody actions and lower-body locomotion. Our technique is
the only splicing technique that have both body parts influence each other, both temporally and spatially. Furthermore,
to our knowledge, all existing splicing techniques only adapt
the upper body orientation to couple both parts. Our technique also adapts the local pelvis translation, using a novel
coupling space and Cartesian Motion Blending.
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

B.J.H van Basten & A. Egges / Splicing Motion Spaces

1971

Acknowledgements This research has been supported by
the Dutch GATE project, funded by NWO and ICT Regie.
References
[AGH03] A L -G HREIMIL N., H AHN J.: Combined partial motion
clips. In WSCG (2003). 2
[AW00] A SHRAF G., W ONG K.: Generating consistent motion
transition via decoupled framespace interpolation. In Comp.
Graph. Forum (2000), vol. 19, John Wiley, pp. 447–456. 3
[CMC09] C HEN Y.-L., M IN J., C HAI J.: Flexible registration
of human motion data with parameterized motion models. In
Symp. on Interactive 3D graph. and games (New York, NY, USA,
2009), ACM, pp. 183–190. 2
[HH08] H A D., H AN J.: Motion synthesis with decoupled parameterization. Vis. Comput. 24, 7 (2008), 587–594. 2, 3, 7
[HKG06] H ECK R., KOVAR L., G LEICHER M.: Splicing upperbody actions with locomotion. Comput. Graph. Forum 25, 3
(2006), 459–466. 2, 3, 7

Figure 10: Setting wL→U
spatial = 1.0 maintains the upper body
posture of ML , avoiding collision.
Like any existing motion (space) splicing technique, we
require the example upper-body motions to be recorded during locomotion. In case the base motion does not exhibit
a gait cycle, efficient temporal alignment based on support
phase is not possible (see the example of splicing punching
on sideways shuffling in the accompanying videos). Nevertheless, our splicing technique allows splicing of upper-body
motion spaces (recorded during locomotion) on all motions
that exhibit a locomotion cycle. This means that there is no
need to record a separate reaching or boxing motion space
for various forms of locomotion. It would be interesting to
see whether it is possible to adapt a stationary upper body
motion space such that the resulting motion can be spliced
on a locomotion. Furthermore, we expect our technique to
work with any upper-body motion space that is parameterized on a spatial end-effector parameter, such as stabbing or
slapping.
Although it has not occurred in our benchmark tests, it
could be possible that, during the adaption of the pelvis of
ML , it is not possible to keep the supporting foot during the
reach fixed on the ground. This could happen in case MU
contains reaching motions that take extremely large sidewards steps during the reach, showing a very large pelvis
displacement. In our implementation, we then set wU→L
spatial to
0, meaning that the pelvis trajectory of ML is unaltered. In
that sense, we would like to stress that high-level composite
actions often involve more than the spatial (pelvis) adjustment applied in this paper. When the character picks up a
very heavy object, weight shifting occurs and the feet might
be placed differently. It could be interesting to consider regenerating ML with different footstep constraints [vBSE11]
such that the character remains in balance. In that case there
would be even a stronger coupling between the upper and
lower body.
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

[IF04] I KEMOTO L., F ORSYTH D. A.: Enriching a motion collection by transplanting limbs. In Symposium on Computer Animation (2004), Eurographics Association, pp. 99–108. 2
[JLLL08] JANG W.-S., L EE W.-K., L EE I.-K., L EE J.: Enriching a motion database by analogous combination of partial human motions. Vis. Comput. 24, 4 (2008), 271–280. 2
[KG03] KOVAR L., G LEICHER M.: Flexible automatic motion
blending with registration curves. In Symposium on Computer
Animation (2003), Eurographics Association, pp. 214–224. 2, 3
[KG04] KOVAR L., G LEICHER M.: Automated extraction and
parameterization of motions in large data sets. ACM Trans.
Graph. 23, 3 (2004), 559–568. 1, 2, 3, 5, 7
[KGP02] KOVAR L., G LEICHER M., P IGHIN F.: Motion graphs.
In SIGGRAPH (2002), ACM, pp. 473–482. 2
[LS02] L EE J., S HIN S.: General construction of time-domain filters for orientation data. IEEE Trans. on Vis. and Comp. Graphics
(2002), 119–128. 5
[MXH∗ 10] M A W., X IA S., H ODGINS J. K., YANG X., L I C.,
WANG Z.: Modeling style and variation in human motion. In
SCA 2010 (2010), Eurographics Association, pp. 21–30. 3
[MZF06] M AJKOWSKA A., Z ORDAN V., FALOUTSOS P.: Automatic splicing for hand and body animations. In Symposium on
Computer animation (2006), pp. 309–316. 3
[Osh08] O SHITA M.: Smart motion synthesis. Comput. Graph.
Forum 27, 7 (2008), 1909–1918. 2
[RSC01] ROSE III C. F., S LOAN P.-P. J., C OHEN M. F.: Artistdirected inverse-kinematics using radial basis function interpolation. Comput. Graph. Forum 20, 3 (2001), 239–250. 3
[TKK10] TAMADA K., K ITAOKA S., K ITAMURA Y.: Splicing
Motion Graphs: Interactive Generation of Character Animation.
In Short papers of Computer Graphics International (2010). 3
[vBGE11] VAN BASTEN B. J. H., G ERAERTS R., E GGES A.:
Combining path planners and motion graphs. Comp. Anim. and
Virt. Worlds 22, 1 (2011), 59–78. 5
[vBSE11] VAN BASTEN B. J. H., S TÜVEL S., E GGES A.: A hybrid interpolation scheme for footprint-driven walking synthesis.
In Graph. Int. 2011 (2011), ACM, pp. 9–16. 2, 3, 4, 5, 7, 9
[WH97] W ILEY D. J., H AHN J. K.: Interpolation synthesis of
articulated figure motion. Comput. Graph. Appl. 17, 6 (1997),
39–45. 1, 2, 3

