DOI: 10.1111/j.1467-8659.2011.01870.x
EUROGRAPHICS 2011 / M. Chen and O. Deussen
(Guest Editors)

Volume 30 (2011), Number 2

Freehand HDR Imaging of Moving Scenes
with Simultaneous Resolution Enhancement
Henning Zimmer1 and Andrés Bruhn2 and Joachim Weickert1
1 Mathematical

Image Analysis Group,
Faculty of Mathematics and Computer Science Saarland University, Saarbrücken, Germany
{zimmer,weickert}@mia.uni-saarland.de
2 Vision

and Image Processing Group,
Cluster of Excellence Multimodal Computing and Interaction, Saarland University, Saarbrücken, Germany
bruhn@mmci.uni-saarland.de

Abstract
Despite their high popularity, common high dynamic range (HDR) methods are still limited in their practical
applicability: They assume that the input images are perfectly aligned, which is often violated in practise. Our
paper does not only free the user from this unrealistic limitation, but even turns the missing alignment into an
advantage: By exploiting the multiple exposures, we can create a super-resolution image. The alignment step is
performed by a modern energy-based optic flow approach that takes into account the varying exposure conditions. Moreover, it produces dense displacement fields with subpixel precision. As a consequence, our approach
can handle arbitrary complex motion patterns, caused by severe camera shake and moving objects. Additionally,
it benefits from several advantages over existing strategies: (i) It is robust under outliers (noise, occlusions, saturation problems) and allows for sharp discontinuities in the displacement field. (ii) The alignment step neither
requires camera calibration nor knowledge of the exposure times. (iii) It can be efficiently implemented on CPU
and GPU architectures. After the alignment is performed, we use the obtained subpixel accurate displacement
fields as input for an energy-based, joint super-resolution and HDR (SR-HDR) approach. It introduces robust data
terms and anisotropic smoothness terms in the SR-HDR literature. Our experiments with challenging real world
data demonstrate that these novelties are pivotal for the favourable performance of our approach.
Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Picture/Image
Generation—Display algorithms I.4.3 [Image Processing and Computer Vision]: Enhancement—Registration
I.4.3 [Image Processing and Computer Vision]: Enhancement—Sharpening and deblurring

1. Introduction
One of the major obstacles for the practical application of
high dynamic range (HDR) imaging is the alignment problem. As common HDR techniques compute a weighted average over irradiances obtained from a given exposure series [MP95, DM97, RBS99, RWPD05], they rely on perfectly aligned images without displacements in between
them. However, this assumption is hardly met under real
world conditions: Displacements are caused by moving objects (persons, clouds, etc.) and by camera shake, unless the
effort is taken to use a tripod. Consequently, there is a strong
interest in methods that align the exposure series by compenc 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

sating the images for the displacements. However, a reliable
estimation of the displacements is challenging since the images are taken with varying exposure times that yield severe
brightness changes.
In this paper, we show how to adapt a modern energybased optic flow approach to cope with these brightness
changes. This allows for a robust and accurate estimation
of dense displacement fields between the input images and
yields an alignment method that outperforms existing strategies in challenging real world scenarios. Additionally, our
approach neither requires a preceding camera calibration nor

406

H. Zimmer et al. / Freehand HDR Imaging of Moving Scenes with Simultaneous Resolution Enhancement

=◮

Figure 1: Left: Freehand exposure series. Right: Result after alignment plus joint dynamic range and resolution enhancement

knowledge of the exposure times and can be efficiently implemented on CPU and GPU architectures.
As our alignment approach yields displacement fields
with subpixel precision, they not only allow for an accurate alignment, but can also be used for increasing the spatial resolution of the result. To this end, one can adapt concepts from super-resolution (SR) approaches [TH84,PPK03,
MPSC09]. Here, an image with increased spatial resolution
is obtained by combining the information from several images with some degree of subpixel displacement between
them. This basically allows to fuse different discrete samplings of the same continuous scene.
In this paper, we propose the first energy-based joint
super-resolution and high dynamic range (SR-HDR) approach that uses a robust data term in combination with an
anisotropic smoothness term. Our experiments show that this
model gives more appealing results than existing techniques
such as [GG06, CPK09]. A tone mapped result obtained by
our approach is shown in Fig. 1. In this context, we also wish
to note that SR and HDR imaging nicely complement each
other: While the SR methods increase the resolution of the
image domain, HDR techniques increase the resolution of
the image co-domain, i.e. the dynamic range.

1.1. Related Work
HDR Alignment. A simple and fast approach for aligning exposure series is to estimate one global transformation per image pair. Ward [War03] describes this transformation by a pure translation, whereas later extensions
use a translation plus a rotation [Gro06, JLW08]. To cope
with the brightness changes due to the varying exposures,
the aforementioned approaches consider mean threshold
bitmaps (MTB) obtained by a thresholding at the median
of all pixel values. Using a pyramid of these images, the
global displacement can then be computed by simple shift
and difference operations at each pyramid level. Altough
global strategies are thus very efficient, they fail in the
presence of independently moving objects or for camera
motions such as zooming and tiling. Similar restrictions

apply to the method in [TM07] where a homography is
computed from SIFT feature matches [Low04] that are robust under brightness changes. Such a strategy is implemented in the align_image_stack algorithm of the Hugin
toolkit (http://hugin.sourceforge.net). Homographybased approaches are known to fail for moving objects or
camera motions that are more complex than a pure rotation.
To describe arbitrary camera motions and to handle moving objects in the scene, dense methods are needed that allow
to estimate a different displacement vector for each pixel.
This can be achieved by multi-step methods such as the
global-local alignment strategies [KUWS03, JO08] that first
perform some global alignment and then refine it using a
classical, local optic flow approach [LK81]. As this optic
flow approach assumes a similar intensity at corresponding
pixels, the measured intensities first need to be transferred
to the irradiance domain. This, however, requires a preceding calibration step to estimate the camera response function.
Another problem is that local optic flow approaches cannot
estimate a displacement in flat image regions (aperture problem) and give blocky artefacts as they assume a constant (or
parametric) motion model within a local neighbourhood. A
more advanced multi-step method was proposed in [ST04].
Here, sparse correspondences obtained by feature matching
are used to compute a dense displacement field via weighted
linear regression. This result is then refined by a local optic
flow method [LK81]. To deal with the brightness changes,
a normalisation is proposed that gives a partial invariance to
the exposure changes without using the response function.
There also exist dense methods that do not need to apply several processing steps. Menzel and Guthe [MG07]
propose a hierarchical matching of patches based on
cross-correlation to ensure robustness under the brightness
changes. As no smoothness assumption on the displacements is imposed, this method is prone to give noisy displacement fields, leading to artefacts in the alignment. The
only approach that imposes an explicit smoothness assumption on the displacements can be found in [KP04]. Here,
a stereo method based on zero-mean normalised crosscorrelation is used. This, however, is only possible for static
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

H. Zimmer et al. / Freehand HDR Imaging of Moving Scenes with Simultaneous Resolution Enhancement

scenes without moving objects and additionally requires a
pair of cameras with known epipolar geometry.
SR-HDR Reconstruction. One major challenge for joint
SR-HDR approaches is an accurate displacement estimation
with subpixel precision. Thus, some methods rely on special
camera hardware to facilitate the displacement estimation:
While the methods in [NN05,HTO07] use multisampled images where the pixels on the image sensor are differently exposed, Nakai et al. [NYUS08] influence the displacements
by a controlled shift of the image sensor. Even simpler, Choi
et al. [CPK09] assume that the displacements are given.
Evidently, it is more convenient to use standard cameras
and to take an exposure series with varying viewpoints. This
strategy is applied in [RMVS07] where the images are first
aligned using a frequency domain approach that gives a
global translation and rotation, as in [Gro06, JLW08]. The
SR-HDR result is then computed by simply interpolating irradiances obtained from the aligned images. More powerful
are approaches that find the SR-HDR image by minimising
an energy formulation [GG06, CPK09]. These works also
prove that a joint SR-HDR reconstruction gives better results
than a sequential approach. The main problem of existing
energy-based methods is that they use a prior that enforces
the result to be close to a mean image obtained by averaging
all measurements. Although this prior stabilises the minimisation, it does not allow to fill in missing information and to
smooth the resulting image. However, our experiments show
that an appropriate filling in of information and smoothing of
the result is required to obtain favourable reconstructions.

1.2. Our Contributions
This paper presents two main contributions:
(i) We propose the first HDR alignment strategy that is
based on a specifically tailored, modern energy-based optic flow approach. The latter gives accurate and dense displacement fields that can describe arbitrary motion patterns
caused by complex camera motions and moving objects in
the scene. This allows to outperform existing strategies in
challenging real world scenarios. To cope with the brightness changes due to the varying exposures, we rely on a robust version of the gradient constancy assumption in the data
term. This uses edge information which is hardly perturbed
by brightness changes. We additionally perform a normalisation in the data term to prevent artefacts at image edges.
Our smoothness term uses a discontinuity-preserving strategy that gives sharp edges between areas of different motion. A further appealing aspect is that we do not require
information on the camera response curve or the exposure
times. Thus, our method can also be used to align images
for exposure fusion algorithms [MKR09] that do not use the
response curve or the exposure times. Concerning run times,
efficient implementations allow to align exposure series in
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

407

about one minute on a CPU and in a few seconds on parallel
GPU architectures.
(ii) As we estimate displacements with subpixel precision,
they can serve as input for an energy-based, joint SR-HDR
method. As novelties, we propose a robust data term and
the first smoothness term that allows to smooth the resulting image. Specifically, we design an anisotropic smoothness term that does not only allow to preserve, but even
pronounce edges in the image. Experiments show that our
proposed model achieves notably better results than existing
approaches.
Paper Organisation. We present the energy functions and
the minimisation strategies for the HDR alignment and the
SR-HDR approach in Sec. 2 and 3, respectively. Experiments that demonstrate the favourable performance of both
contributions are shown in Sec. 4. We conclude in Sec. 5 by
a brief summary and an outlook on future work.
2. HDR Alignment with Energy-Based Optic Flow
Assume we are given an exposure series gk (i, j) : Ω →
{0, . . . , 255} for k =1, . . . , m. Here, (i, j) ∈ Ω denotes a pixel
location in the image domain Ω = {1, . . . , nx }×{1, . . . , ny }.
The alignment of such an exposure series comes down to estimating displacement vector fields (uk , vk ) between images
gk and a reference image gr , where r:=⌈m/2⌉ for simplicity.
To ease presentation, we describe our model for greyscale
images, but also sketch the extension to colour images.
2.1. Energy Formulation.
We compute the dense displacement fields (uk , vk ) by minimising an energy function of the general form
E1 (uk , vk ) =

∑

D1 (uk , vk ) + α S1 (∇uk ,∇vk ) . (1)

(i, j)∈Ω

The data term D1 (uk , vk ) models how well the displacements match to the given images, and the smoothness term
S1 (∇uk ,∇vk ) reflects the assumption of a smooth displacement field. Its influence is steered by a smoothness parameter α > 0. The symbol ∇ := (Dx , Dy )⊤ denotes a discrete
version of the gradient operator, with Dx and Dy implementing discrete approximations of the x- and y-derivatives, respectively. Note that here and in the remainder of this paper,
we intentionally misuse concepts borrowed from continuous
models to facilitate notation. Let us now discuss the modelling of the data and the smoothness term.
Data Term. An appropriate design of the data term is
mandatory for obtaining reasonable results given images
with different exposures. We base our data term on the
gradient constancy assumption [BBPW04], stating that image gradients remain constant under their displacement, i.e.

408

H. Zimmer et al. / Freehand HDR Imaging of Moving Scenes with Simultaneous Resolution Enhancement

(a) Image 1 (t = 1/30 s)

(b) Image 3 (t = 1/80 s)

(c) x-derivative of (a)

(d) x-derivative of (b)

Figure 2: (a) – (b) Two images of an exposure series. (c) – (d) The x-derivatives for the red channel of the images in (a) and (b)

∇gr (i+uk , j+vk ) = ∇gk (i, j). This results in using edge information for the alignment, see Fig. 2. As noted in [War03],
edges are not completely invariant under exposure changes,
but our experiments will show that they give a sufficient cue
when used within a robust energy-based framework.
In our data term, we perform a robust penalisation of the
gradient constancy assumption to render our approach robust
against outliers caused by saturation problems, noise or occlusions. Furthermore, we follow [ZBW∗ 09] and normalise
the data term to prevent an undesirable overweighting that
leads to artefacts in the displacement fields. Incorporating
these concepts leads to our data term
(2)

D1 (uk , vk ) =
ΨD1

θx |Dx gr (i+uk , j+vk ) − Dx gk (i, j)|2
+ θy |Dy gr (i+uk , j+vk ) − Dy gk (i, j)|2

.

For the robust penaliser
√function ΨD1 we use the regularised
L1 -norm ΨD1 s2 = s2 + 0.0012 as in [BBPW04]. The
normalisation factors θx and θy are defined as
θx :=

1
|∇(Dx gk )|2 + ζ2

,

θy :=

1
|∇(Dy gk )|2 + ζ2

, (3)

where the parameter ζ = 0.1 avoids division by zero.
If colour images are to be processed, we sum up all channels in the argument of ΨD1 in (2). This reduces the influence
of a pixel as soon as any channel produces an outlier.
In preliminary experiments, we investigated data terms
that impose constancy of the irradiances to cope with the
exposure changes. This, however, requires a preceding camera calibration and knowledge of the exposure times as the
irradiances are computed via applying the inverse camera response function and dividing by the exposure time. Since the
irradiances make no sense in saturated regions, and due to
possible errors in the calibration, working in the irradiance
domain alone produced unsatisfactory results. By additionally imposing constancy of the irradiance gradients we could
improve the results. However, we could not obtain better results than with our proposed approach, which is simpler and
more efficient as it omits calibration and irradiance computations. Note that due to the latter, our approach also allows

to align image sets for exposure fusion [MKR09] where no
camera response curve or exposure times are given.
Smoothness Term. Unfortunately, the data term alone does
not allow to obtain a dense displacement field. For example
in flat image regions (that frequently occur in HDR imaging
due to saturation problems) the data term gives no information at all. In such regions, a smoothness term is needed to
fill in the displacement field. As in [BBPW04], we use a Total Variation (TV) regulariser given by
S1 (∇uk , ∇vk ) = ΨS1 |∇uk |2 + |∇vk |2

,

(4)

with the same penaliser function as in the data term, i.e.
ΨS1 = ΨD1 . This regulariser has been chosen since it is
known to give sharp discontinuities in the displacement field.
2.2. Energy Minimisation
A necessary condition for a minimiser (uk , vk ) of the energy (1) is given by the equations
∂uk(i, j) E1 = 0 , and ∂vk(i, j) E1 = 0 ,

∀(i, j) ∈ Ω .

(5)

Using the abbreviations
g∗∗ := D∗∗ gr (i+uk , j+vk ) ,

(6)

g∗z := D∗ gr (i+uk , j+vk ) − D∗ gk (i, j) ,

(7)

where ∗∗ ∈ {xx, xy, yy} and ∗ ∈ {x, y}, the equations in (5)
can be written as
Ψ′D1 θx g2xz + θy g2yz · θx gxx gxz + θy gxy gyz
− α div Ψ′S1 |∇uk |2 +|∇vk |2 ∇uk

=0 ,

Ψ′D1 θx g2xz + θy g2yz · θx gxy gxz + θy gyy gyz
− α div Ψ′S1 |∇uk |2 +|∇vk |2 ∇vk

(8)

(9)

=0 ,

where Ψ′ denotes the derivative of Ψ w.r.t. its argument and
div := ∇⊤ is a discrete variant of the divergence operator.
We solve equations (8)–(9) in a coarse-to-fine warping
framework that computes small displacement increments at
different levels of a multiscale pyramid [BBPW04]. This
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

409

H. Zimmer et al. / Freehand HDR Imaging of Moving Scenes with Simultaneous Resolution Enhancement

reduces the chance of getting trapped in a local minimum
and allows to handle large displacements. For solving the
problem at each pyramid level, we propose two strategies:
On sequential CPU architectures, we use a nonlinear multigrid scheme with a Gauß-Seidel type solver [BWKS06].
For parallel GPU architectures, we adapt the recent work
in [GZG∗ 10] that is based on a cascadic, fast explicit gradient descent scheme implemented in CUDA.
3. Energy-based SR-HDR Reconstruction

light arriving at the image sensor. From a set of aligned images, the response function can be estimated by a calibration
procedure as in [DM97, RBS99]. Thus, we assume the response function to be given for our SR-HDR approach.
Above constraints are combined in our SR-HDR data term
m

D2 (F) =

∑ ck · ΨD

RBWk F −

2

k=1

I(gk )
tk

2

,

(13)

where ck is a HDR weighting function that reduces the influence of too dark or bright pixels from the reconstruction,
as those give less reliable irradiance information. We use a
Gaussian-like weighting function [RBS99] given by

As the displacement fields resulting from our HDR alignment method are of subpixel precision, we can use them
as input for a joint super-resolution and HDR (SR-HDR)
method. In addition to the input images gk , the exposure
times tk and the estimated displacements (uk , vk ), such a
method also requires to specify a zoom factor z > 1. From
this, we aim at reconstructing a SR-HDR image F : ΩH →
R+ , where ΩH = {1, . . . , z · nx }×{1, . . . , z · ny } denotes the
SR image domain. The codomain of F are the positive real
numbers, representing the irradiances.

with a scale parameter s = 8. We further set ck = 0 if the
weight becomes smaller than 0.001 to ignore the influence of
less reliable pixels. For the subquadratic penaliser function
ΨD2 , we use as before the regularised L1 -norm, i.e. ΨD2 =
ΨD1 . This renders our approach robust against outliers due
to noise or incorrect displacement estimates.

3.1. Energy Formulation

If colour images are given, we again sum up the contributions of each channel in the argument of ΨD2 in (13).

As before, we find the SR-HDR image F by minimising a
suitable energy. This time it takes the form
E2 (F) =

∑

D2 (F) + λ S2 (∇F) ,

(10)

(i, j)∈ΩH

with a smoothness weight λ > 0.
Data Term. The data term D2 (F) combines common SR
and HDR data constraints. For the SR part we adopt a popular observation model, see e.g. [MPSC09], which can be
written as
RBWk G = gk .

(11)

It models the assumption that the low-resolution images
gk are obtained from the unknown SR image G : ΩH →
{0, . . . , 255} by a backward warping (Wk ), blurring (B) and a
downsampling/restriction (R). The warping Wk uses the estimated displacements (uk , vk ) after upsampling to the SR
grid. Note that in the SR-HDR case, the displacements are
computed from the images gk to a reference image gr . For
the HDR alignment in Section 2, they were computed in the
opposite direction. The blurring operator B performs a Gaussian convolution with standard deviation
√ σ. In accordance to
the sampling theorem, we set σ = z · 2/4.
For the HDR part, the standard observation model
[DM97,RBS99] states that the unknown irradiances f : Ω →
R+ are obtained as
I(gk )
,
(12)
f=
tk
where I is the inverse of the camera response function that
describes how image intensities are related to the amount of
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

2

ck = exp −s · gk (i, j) − 127.5 / (127.5)2

,

(14)

Smoothness Term. The smoothness term plays a major role
in the SR-HDR context as it has to fill in the SR-HDR image
in regions where no information is available. This can be due
to saturation problems (where ck ≈ 0), or in pixels where the
warping by the displacements does not provide information.
For our SR-HDR approach, we propose a novel
anisotropic smoothness term that adapts its smoothing direction to local image structures and is inspired from
[ZBW∗ 09]. To obtain the required directional information,
we consider the structure tensor [FG87] computed from an
image F 0 that is obtained by upsampling a pure HDR reconstruction computed from the aligned input images. This
leads to the following structure tensor:
J = Kρ1 ∗ ∇Fρ02 ∇Fρ02

⊤

, with Fρ02 = Kρ2 ∗ F 0 , (15)

where ∗ is the convolution operator. The parameters ρ1 = 0.4
and ρ2 = 0.2 serve as neighbourhood and smoothing scale,
respectively. By construction, the tensor J possesses two orthonormal eigenvectors v1 and v2 , which give the desired direction of local image structures: Whereas v1 points across
image structures, the vector v2 points along them.
Our smoothness term then penalises the projections of the
image gradients onto v1 and v2 differently: Along image
edges, we perform a quadratic penalisation to obtain a strong
smoothing that pronounces the edges. In the orthogonal direction across edges, we use a robust penaliser function that
reduces the smoothing and helps to preserve edges. This results in the smoothness term
S2 (∇F) = ΨS2

v⊤
1 ∇F

2

+ v⊤
2 ∇F

2

.

(16)

410

H. Zimmer et al. / Freehand HDR Imaging of Moving Scenes with Simultaneous Resolution Enhancement

For the robust penaliser ΨS2 , we use the Charbonnier function defined as ΨS2 s
parameter µ = 0.1.

2

=2µ

2

1+

s2 /µ2

, with a contrast

For colour images, we sum up the structure tensors of each
channel to obtain a joint edge direction. In the smoothness
term, we sum up the projections for each channel in F. For
the first summand, this is done within the argument of ΨS2
to reduce the smoothing across joint edges.
3.2. Energy Minimisation
We compute the SR-HDR image F by minimising the energy
(10) using a semi-implicit gradient descent scheme given by
F (l+1) − F (l)
= − ∂F E2
τ
m

=

∑

(17)
(l)

Wk⊤BR⊤ ck Ψ′D2 (·)

k=1

I(gk )
− RBWk F (l)
tk

+ λ div T v1 ,v2 ,∇F (l) ∇F (l+1)

,

where we used the abbreviation
(l)

Ψ′D2 (·) := Ψ′D2

RBWk F (l) −

I(gk )
tk

2

,

(18)

and where F (l) denotes the result at iteration l. The parameter τ serves as a numerical time step size. Concerning
the transposed operators, Wk⊤ describes a forward warping,
B = B⊤ for Gaussian blurring [MPSC09], and R⊤ results in
upsampling. The occurring diffusion tensor T is given by
T v1 ,v2 ,∇F (l) = Ψ′S2

(l)
v⊤
1 ∇F

2

⊤
v1 v⊤
1 + v2 v2 .

(19)
For solving the scheme (17) at iteration l, we perform a single Jacobi iteration. Experimentally, we found that setting
τ = 0.5 is feasible, which is more than one order of magnitude larger than the step size used in [MPSC09]. Further note
that due to the complicated structure of the data term (13),
efficient solution strategies as in the optic flow case are more
difficult here. As initialisation we set set F (0) = F 0 , see (15).
4. Experiments
All image data and results presented in this paper are
available for download at our supplementary web page
http://www.mia.uni-saarland.de/Research/SR-HDR.
There, we also give additional results for different test
scenarios and show limitations of our method.
4.1. Alignment for HDR Imaging
The first experiment compares our proposed HDR alignment method from Sec. 2 to several competing methods. To
this end, a real world exposure series (5 images, 512 × 340

pixels, exposure times between 1/250 to 1/30 seconds) was
taken freehand using a standard DSLR camera. Some images of the series are depicted in the first row of Fig. 3. As
we can see, the exposure series suffers from severe displacements due to camera shake and moving clouds, which are
common problems in HDR imaging.
After alignment, we compute HDR reconstructions following the approach in [RBS99]. For visualising the
float-valued HDR data in this and the upcoming experiments, we then apply the tone mapping operator
from [FLW02]. Implementations of the before mentioned algorithms can be found in the pfstools package
(http://pfstools.sourceforge.net).
Let us now compare our method to some other alignment
strategies. These are (i) the global method of Ward [War03],
(ii) the align_image_stack algorithm from the Hugin
toolkit which implements a homography-based approach as
in [TM07], (iii) a variant of our method without data term
normalisation, and (iv) the hierarchical block matching technique of Menzel and Guthe [MG07].
In Fig. 3 (d) we show the result obtained with the global
alignment strategy from [War03], where the black border
marks pixels warped outside the image domain. It becomes
clear that a global translation fails to describe the complex
displacements between the exposures. We also tried to apply
a local optic flow approach [LK81] to the globally aligned
images after transferring them to the irradiance domain. We
found that such a global-local strategy as in [KUWS03]
leads to unusable results due to the poor global alignment.
Also the align_image_stack algorithm, see Fig. 3 (e),
fails to correctly align the images as the homography computed from feature matches is not expressive enough to describe the present displacements. In Fig. 3 (f) we show a
zoom in the result of our approach without data term normalisation. Comparing the latter to the zoom in our final result
(Fig. 3 (i)), we realise that the proposed normalisation allows
to resolve problems with unpleasant artefacts. Of course, a
larger smoothness weight α would also resolve this problem,
but creates a too smooth displacement field that cannot capture the discontinuity between the buildings and the clouds
anymore. A corresponding zoom in the result obtained with
the approach from [MG07] is shown in Fig. 3 (g). It turns out
that the hierarchical block matching technique yields distracting artefacts at the roof of the house since it does not
impose smoothness assumptions on the displacement field.
In contrast, our final result in Fig. 3 (h) and (i) shows a
favourable HDR reconstruction without disturbing artefacts.
This becomes possible due to the robust and accurate displacement estimation of our approach. As an example consider the displacement field in Fig. 3 (j). For visualising the
flow vectors, we use the colour code illustrated in the lower
left corner of the image. As we can see, the motion of the
clouds and the shift of the buildings due to camera shake
are nicely discriminated. Aligning the whole exposure series
c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

H. Zimmer et al. / Freehand HDR Imaging of Moving Scenes with Simultaneous Resolution Enhancement

(a) Image 1

(b) Image 3 (reference)

(d) Ward [War03]

(f) No normalisation

411

(c) Image 5

(e) align_image_stack

(g) Menzel / Guthe [MG07]

(i) Our result

(h) Our result (zoom)

(j) Displacement field

Figure 3: HDR imaging under severe displacements due to moving clouds and camera shake. First row: (a) – (c) Some images
of the exposure series. Second row: (d) Tone mapped HDR image after alignment with the method from [War03]. (e) Same
using align_image_stack. Third row: (f) Zoom in our result without data term normalisation. (g) Same for the method
from [MG07]. (h) Same for our final result from (i) Fourth row: (i) Our result. (j) One displacement field.

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

412

H. Zimmer et al. / Freehand HDR Imaging of Moving Scenes with Simultaneous Resolution Enhancement

took 75 seconds using our CPU version with unoptimised C
code on a 3.2 GHz Intel Pentium 4. An optimised parallel
implementation on a GeForce GTX 480 reduced the runtime
to less than 3 seconds for the whole series. For computing
this and all further results (also the ones on our supplementary web page) we set α = 2.0 and presmoothed the images
by a Gaussian convolution with standard deviation 0.5. The
downsampling factor for the multiscale pyramid was set to
0.95. However, smaller values that lead to a further speed up
are often possible.
4.2. SR-HDR Reconstruction
The second experiment concerns our joint super-resolution
and HDR (SR-HDR) method described in Sec. 3. As before,
a real world exposure series (9 images, 268 × 178 pixels, exposure times from 1/800 to 1/8 seconds) was taken freehand.
As zoom factor we choose z = 2. Some of the input images
are shown in the first row of Fig. 4. In the same figure we also
compare our result to (i) an upsampled HDR result (based on
bicubic interpolation) computed from the aligned images,
(ii) a SR-HDR method similar to [GG06, CPK09] using a
quadratic data term and a mean prior, and (iii) a variant of
our method where we use an isotropic TV smoothness term
ΨS2 (∇F) = |∇F|2 + 0.0012 , as in our optic flow alignment approach.
Comparing the results in Fig. 4 (d)–(g) and focusing on
the zooms in (h)–(k), the following drawbacks of the competing methods become obvious: An upsampled HDR result
(Fig. 4 (d) and (h)) looks rather dull and pixelated. Also existing SR-HDR approaches [GG06, CPK09] (Fig. 4 (e) and
(i)) hardly improve the results, which we attribute to the
mean prior that does not smooth the resulting image. Considering our result with a TV smoothness term (Fig. 4 (f) and
(j)), we see that a prior that allows to fill in information and
that smoothes the result allows to tangibly improve the quality. However, the edges in the image are rather jagged, as
can be seen at the planks in the zoom in Fig. 4 (j). Using our
proposed anisotropic smoothness term (Fig. 4 (g) and (k)) allows to obtain a better reconstruction of edges due to the pronounced smoothing in edge direction. The reduced smoothing in the orthogonal direction allows to preserve image details, as can be seen at the plant in the zoom in Fig. 4 (k).
Concerning parameter settings, we propose to only tune the
value of the smoothness weight λ to get a visually appealing
result. All other parameters can be kept fixed as given in this
paper.
5. Conclusions and Outlook
This paper presented two contributions:
(i) We adapted a modern energy-based optic flow approach to reliably align exposure series for HDR imaging.
This shows how the HDR community can profit from the intensive research on optic flow within the last three decades.

The main advantage of our proposed strategy is that the
resulting dense displacement fields can describe arbitrary
complex motion patterns, which is indispensable when dealing with complex camera motions or moving objects in the
scene. Another attractive aspect is that we do not require
knowledge of the camera response curve or the exposure
times, which makes our method also applicable to align input data for exposure fusion algorithms [MKR09]. Concerning efficiency, reasonable runtimes can be achieved on sequential CPU architectures, whereas parallel GPU implementations reduce the computation times to a few seconds.
We thus hope that our method can be used for an online
alignment in portable HDR capturing devices such as the
Frankencamera [AJD∗ 10].
(ii) As our proposed alignment method yields displacements with subpixel precision, they can serve as input for a
joint super-resolution and HDR (SR-HDR) approach, which
allows to turn the problem of displacements between the input images into an advantage. In this paper, we presented
the first energy-based SR-HDR framework that uses a robust data term in combination with an anisotropic smoothness term. Our experiments showed that especially an appropriate strategy for filling in of missing information and
for smoothing the result is a key for obtaining visually appealing results.
Concerning future work, one remaining issue with our optic flow-based alignment strategy is that large displacements
of small objects cannot be estimated within the used coarseto-fine warping framework. An example for this problem is
shown on our supplementary web page. Possible solutions
could be using a feature-matching prior [BBM09] or applying an anti-ghosting technique [KUWS03]. Furthermore, a
joint estimation of displacements and SR-HDR reconstruction seems interesting, but it is questionable if the more difficult minimisation will pay off in terms of quality.
Acknowledgements. This work has been partially funded
by the International Max-Planck Research School and the
Cluster of Excellence “Multimodal Computing and Interaction” within the Excellence Initiative of the German Federal Government. We thank Christian Theobalt and Miguel
Granados (MPII Saarbrücken) for fruitful discussions.
References
[AJD∗ 10] A DAMS A., JACOBS D., D OLSON J., T ICO M., P ULLI
K., TALVALA E.-V., A JDIN B., VAQUERO D., L ENSCH H.,
H OROWITZ M., PARK S., G ELFAND N., BAEK J., M ATUSIK
W., L EVOY M.: The Frankencamera: an experimental platform for computational photography. In Proc. ACM SIGGRAPH
(2010). 8
[BBM09] B ROX T., B REGLER C., M ALIK J.: Large displacement optical flow. In Proc. 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Miami, FL,
June 2009), IEEE Computer Society Press, pp. 41–48. 8
[BBPW04]

B ROX T., B RUHN A., PAPENBERG N., W EICKERT

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

H. Zimmer et al. / Freehand HDR Imaging of Moving Scenes with Simultaneous Resolution Enhancement

(a) Image 1

(b) Image 5 (reference)

(d) Upsampled HDR result

(f) TV smoothness term

(h) Zoom in (d)

(i) Zoom in (e)

413

(c) Image 9

(e) Mean prior [GG06, CPK09]

(g) Our result

(j) Zoom in (f)

(k) Zoom in (g)

Figure 4: SR-HDR reconstruction on a real world exposure series. First row: (a) – (c) Some images of the exposure series.
Second row: (d) Upsampled HDR result (bicubic interpolation), tone mapped. (e) Tone mapped SR-HDR result of a method
similar to [GG06, CPK09] (quadratic data term, mean prior, λ = 5.0, τ = 0.1, 25 iterations). Third row: (f) Same with our
method, but using a TV smoothness term (λ = 0.4, τ = 0.5, 1000 iterations). (g) Same using our proposed method with an
anisotropic smoothness term (λ = 0.3, other parameters as in (f)). Fourth row: (h) – (k) Zooms in above results

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

414

H. Zimmer et al. / Freehand HDR Imaging of Moving Scenes with Simultaneous Resolution Enhancement

J.: High accuracy optical flow estimation based on a theory for
warping. In Computer Vision – ECCV 2004, Part IV, Pajdla T.,
Matas J., (Eds.), vol. 3024 of Lecture Notes in Computer Science.
Springer, Berlin, 2004, pp. 25–36. 3, 4
[BWKS06] B RUHN A., W EICKERT J., KOHLBERGER T.,
S CHNÖRR C.: A multigrid platform for real-time motion computation with discontinuity-preserving variational methods. International Journal of Computer Vision 70, 3 (Dec. 2006), 257–277.
5
[CPK09] C HOI J., PARK M., K ANG M.: High dynamic range
image reconstruction with spatial resolution enhancement. The
Computer Journal 52, 1 (2009), 114–125. 2, 3, 8, 9
[DM97] D EBEVEC P., M ALIK J.: Recovering high dynamic
range radiance maps from photographs. In Proc. ACM SIGGRAPH (1997), pp. 369–378. 1, 5
[FG87] F ÖRSTNER W., G ÜLCH E.: A fast operator for detection
and precise location of distinct points, corners and centres of circular features. In Proc. ISPRS Intercommission Conference on
Fast Processing of Photogrammetric Data (Interlaken, Switzerland, June 1987), pp. 281–305. 5
[FLW02] FATTAL R., L ISCHINSKI D., W ERMAN M.: Gradient
domain high dynamic range compression. In Proc. ACM SIGGRAPH (2002), pp. 249–256. 6
[GG06] G UNTURK B., G EVREKCI M.: High-resolution image
reconstruction from multiple differently exposed images. IEEE
Signal Processing Letters 13, 4 (2006), 197–200. 2, 3, 8, 9
[Gro06] G ROSCH T.: Fast and robust high dynamic range image
generation with camera and object movement. In Proceedings of
Vision, Modeling, and Visualization (VMV) 2006 (2006), Kobbelt
L., T.Kuhlen, Aach T., Westermann R., (Eds.), AKA Heidelberg,
pp. 277–284. 2, 3
[GZG∗ 10] G WOSDEK P., Z IMMER H., G REWENIG S., B RUHN
A., W EICKERT J.: A highly efficient GPU implementation for
variational optic flow based on the Euler-Lagrange framework.
In Proc. 2010 ECCV Workshop on Computer Vision with GPUs
(Heraklion, Greece, Sept. 2010). 5
[HTO07] H ARALDSSON H., TANAKA M., O KUTOMI M.: Reconstruction of a high dynamic range and high resolution image
from a multisampled image sequence. In Proc. 14th International
Conference on Image Analysis and Processing (ICIAP) (Modena,
Italy, Sept. 2007), pp. 303–310. 3
[JLW08] JACOBS K., L OSCOS C., WARD G.: Automatic highdynamic range image generation for dynamic scenes. IEEE Computer Graphics and Applications 28 (2008), 84–93. 2, 3
[JO08] J INNO T., O KUDA M.: Motion blur free HDR image acquisition using multiple exposures. In Proc. 2008 IEEE International Conference on Image Processing (San Diego, CA, USA,
Oct. 2008), pp. 1304–1307. 2
[KP04] K IM S., P OLLEFEYS M.: Radiometric alignment of image sequences. In Proc. 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Washington,
DC, June 2004), vol. 1, IEEE Computer Society Press, pp. 645–
651. 2
[KUWS03] K ANG S., U YTTENDAELE M., W INDER S.,
S ZELISKI R.: High dynamic range video. In Proc. ACM
SIGGRAPH (2003), pp. 319–325. 2, 6, 8
[LK81] L UCAS B., K ANADE T.: An iterative image registration
technique with an application to stereo vision. In Proc. Seventh
International Joint Conference on Artificial Intelligence (Vancouver, Canada, Aug. 1981), pp. 674–679. 2, 6
[Low04] L OWE D.: Distinctive image features from scaleinvariant keypoints. International Journal of Computer Vision
60, 2 (2004), 91–110. 2

[MG07] M ENZEL N., G UTHE M.: Freehand HDR photography
with motion compensation. In Proceedings of Vision, Modeling, and Visualization (VMV) (2007), Lensch H., Rosenhahn B.,
Seidel H.-P., Slusallek P., Weickert J., (Eds.), AKA Heidelberg,
pp. 127–134. 2, 6, 7
[MKR09] M ERTENS T., K AUTZ J., R EETH F. V.: Exposure fusion: A simple and practical alternative to high dynamic range
photography. Computer Graphics Forum 28, 1 (2009), 161–171.
3, 4, 8
[MP95] M ANN S., P ICARD R.: On being ‘undigital’ with digital cameras: Extending dynamic range by combining differently
exposed pictures. In Proc. 48th IS&T Annual Conference (Washington DC, USA, May 1995), pp. 442–448. 1
[MPSC09] M ITZEL D., P OCK T., S CHOENEMANN T., C RE MERS D.: Video super resolution using duality based TV-L1 optical flow. In Pattern Recognition, Denzler J., Notni G., Süße H.,
(Eds.), vol. 5748 of Lecture Notes in Computer Science. Springer,
Berlin, 2009, pp. 432–441. 2, 5, 6
[NN05] NARASIMHAN S., NAYAR S.: Enhancing resolution
along multiple imaging dimensions using assorted pixels. IEEE
Transactions on Pattern Analysis and Machine Intelligence 27, 4
(2005), 518–530. 3
[NYUS08] NAKAI H., YAMAMOTO S., U EDA Y., S HIGEYAMA
Y.: High resolution and high dynamic range image reconstruction
from differently exposed images. In Advances in Visual Computing, Bebis G., et al., (Eds.), vol. 5359 of Lecture Notes in Computer Science. Springer, Berlin, 2008, pp. 713–722. 3
[PPK03] PARK S., PARK M., K ANG M.: Super-resolution image
reconstruction: a technical overview. IEEE Signal Processing
Magazine 20 (2003), 21–36. 2
[RBS99] ROBERTSON M., B ORMAN S., S TEVENSON R.: Dynamic range improvement through multiple exposures. In
Proc. Sixth International Conference on Image Processing
(Kobe, Japan, Oct. 1999), vol. III, pp. 159–163. 1, 5, 6
[RMVS07] R AD A., M EYLAN L., VANDEWALLE P.,
Multidimensional image enhancement
S ÜSSTRUNK S.:
from a set of unregistered differently exposed images. In
Proc. IS&T/SPIE Electronic Imaging: Computational Imaging V
(2007), vol. 6498. 3
[RWPD05] R EINHARD E., WARD G., PATTANAIK S., D EBEVEC
P.: High Dynamic Range Imaging: Acquisition, Display, and
Image-Based Lighting. Morgan Kaufmann Publishers, 2005. 1
[ST04] S AND P., T ELLER S.: Video matching. In Proc. ACM
SIGGRAPH (2004), pp. 592–599. 2
[TH84] T SAI R., H UANG T.: Multiframe image restoration and
registration. In Advances in Computer Vision and Image Processing (1984), vol. 1. 2
[TM07] T OMASZEWSKA A., M ANTIUK R.: Image registration
for multi-exposure high dynamic range image acquisition. In
Proc. International Conference in Central Europe on Computer
Graphics, Visualization and Computer Vision (WSCG) (PlzenBory, CZ, Jan. 2007), pp. 49–56. 2, 6
[War03] WARD G.: Fast, robust image registration for compositing high dynamic range photographs from hand-held exposures.
Journal of Graphics, GPU, and Game Tools 8, 2 (2003), 17–30.
2, 4, 6, 7
[ZBW∗ 09]

Z IMMER H., B RUHN A., W EICKERT J., VAL L., S ALGADO A., ROSENHAHN B., S EIDEL H.-P.:
Complementary optic flow. In Energy Minimization Methods
in Computer Vision and Pattern Recognition (EMMCVPR), Cremers D., Boykov Y., Blake A., Schmidt F. R., (Eds.), vol. 5681
of Lecture Notes in Computer Science. Springer, Berlin, 2009,
pp. 207–220. 4, 5
GAERTS

c 2010 The Author(s)
Journal compilation c 2010 The Eurographics Association and Blackwell Publishing Ltd.

