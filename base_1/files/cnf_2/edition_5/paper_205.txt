DOI: 10.1111/j.1467-8659.2011.02044.x
Pacific Graphics 2011
Bing-Yu Chen, Jan Kautz, Tong-Yee Lee, and Ming C. Lin
(Guest Editors)

Volume 30 (2011), Number 7

RepSnapping: Efficient Image Cutout for Repeated
Scene Elements
Hua Huang†

Lei Zhang

Hong-Chao Zhang

School of Electronic and Information Engineering, Xi’an Jiaotong University, China

Abstract
Repeated scene elements are copious and ubiquitous in natural images. Cutout of those repeated elements usually
involves tedious and laborious user interaction by previous image segmentation methods. In this paper, we present
RepSnapping, a novel method oriented to cutout of repeated scene elements with much less user interaction.
By exploring inherent similarity between repeated elements, a new optimization model is introduced to thread
correlated elements in the segmentation procedure. The model proposed here enables efficient solution using maxflow/min cut on an extended graph. Experiments indicate that RepSnapping facilitates cutout of repeated elements
better than the state-of-the-art interactive image segmentation and repetition detection methods.
Categories and Subject Descriptors (according to ACM CCS): I.4.6 [Computer Graphics]: Image processing and
computer vision—Segmentation

1. Introduction
Image cutout is the task of segmenting foreground of interest in an input image from its background. For over a
decade now, image cutout has been extensively studied due
to its fundamental applications in image processing and
pattern recognition. Although many elegant segmentation
methods have been dedicated to image cutout, it still remains a severe challenge to automatically cut out meaningful
objects from arbitrary images. Appropriate user interaction
is proved of great help to conduct fast and accurate image
cutout [LSTS04, RKB04]. However, existing automatic and
interactive image cutout methods mainly process segmentation of individual objects, which have less investigation on
their validness and efficiency when applied to images with
many repeated scene elements like Figure 1.
Repetition is prevalent in forms of biology, engineering, architecture, art, and so on. One of the most important functions of human recognition system is to detect differences as well as similarities among objects in the visual
field. Thus, detection of the repeated elements is important
to recognize and understand the world around us [Tho92].
However, state-of-the-art image segmentation methods, like
† Corresponding author: huanghua@mail.xjtu.edu.cn
c 2011 The Author(s)
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd. Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ,
UK and 350 Main Street, Malden, MA 02148, USA.

LazySnapping [LSTS04] and Grabcut [RKB04], are not
suitable for interactive cutout of repeated elements from
their surroundings. When applying those methods to such
images, much more user interactions are needed to segment the repeated elements, but unfortunately might still
result in notable segmentation artifacts (see Figure 1). Besides image segmentation, some methods strive for analysis
of repeated elements by considering their geometry regularity [CZM∗ 10]. Although aiming at repetition detection in the
images, those methods mainly rely on the geometric similarity between the repeated elements, which would fail to generate accurate cutout for repeated elements with large shape
variance (see Figure 1 (d)).
The main contribution of our paper is to present a novel
image cutout method, namely RepSnapping, specialized on
segmentation of repeated scene elements from input images.
A new optimization model is designed by considering the
appearance similarity between the repeated elements. Our
RepSnapping is able to simultaneously cut out the repeated
elements with much less user interaction.
2. Related Work
Image cutout or segmentation is a classical issue in computer vision and computer graphics, and touches on a large
literature to address it in an automatic or interactive manner.

2060

H. Huang & L. Zhang & H.-C. Zhang / RepSnapping

(a) RepSnapping

(b) LazySnapping

(c) Grabcut

(d) RepFinder

 

Figure 1: Image cutout of repeated scene elements using (a) our RepSnapping, (b) LazySnapping [LSTS04], (c) Grabcut [RKB04], and (d) RepFinder [CZM∗ 10]. The red/blue scribbles in (a-c) and green rectangle in (c) indicate user interaction
for foreground and background, while the blue contour in (d) marks the template element.

Automatic segmentation methods try to classify regions of
an image according to some feature measurements without
any intervention, using mean-shift clustering [CM02], graph
partition [FH04], or random walk [Gra06]. However, due to
the variety of input images, it is almost impossible to satisfy
arbitrary segmentation in the automatic manner.
Interaction can alleviate the segmentation problems inherent to automatic segmentation that seems to never be perfect. Intelligent Scissors [MB95] traces foreground boundaries via dynamic programming on the minimum cost path,
while it still needs labor-intensive effort to place live-wire
near object boundaries for good segmentation results. Magic
Wand [Ado10] provides a more flexible tool to cut out regions by freely drag mouse over foreground, and areas with
consistent colors can be automatically selected. Image segmentation can cast as a Markov Random Fields (MRF) and
efficiently be solved by graph cuts optimization [KZ04]. In
the graph cuts framework, seed regions can be loosely specified by drawing some strokes on the foreground/background,
then pixels are classified according to their probabilities belonging to foreground/background [BJ01]. LazySnapping [LSTS04] combines graph cuts with pre-computed
over-segmentation to define graph structure, which achieves
instant segmentation feedback to guide user interaction.
Noting one-pass graph cuts is insufficient to depict foreground/background color distribution of the whole image,
Grabcut [RKB04] iteratively employs Gaussian Mixture
Model (GMM) to train foreground/background colors and
then uses graph cuts to obtain the cutout, which generates
more accurate segmentation results. User interaction greatly
helps image segmenation in efficiency and quality. However,
those interactive methods are designed for cutout of individ-

ual objects, but still need quite a few of user interactions for
segmentation of repeated elements (see Figure 1).
Foreground/background segmentation and image matting
are two closely related problems. Image matting is the process of extracting foreground object from an image along
with an opacity estimate for each pixel covered by the object.
Due to the ill-posed problem, image matting usually relies
on a trimap as prior knowledge on foreground, uncertain and
background. Then, opacity values of pixels in uncertain regions are estimated using methods like Gaussian probability
estimation [RT00], Bayesian reasoning [CCSS01], Poisson
equation [SJTS04], matting Laplacian [LLW06], or spectral
analysis [LRAD07]. The trimap can be automatically obtained from dilation and erosion on the foreground mask in
image segmentation. With the matte value, foreground objects can be well composited over a new background.
In recent years, another sort of image cutout, cosegmentation, has drawn much research attention, which
refers to simultaneously segmenting two images including the same or similar objects [RMBK06]. Observing the
common objects share consistent color distribution, cosegmentation can be achieved by extending graph cuts segmentation with histogram matching to select foreground
with minimal histogram difference. The histogram difference can be measured by L1-norm [RMBK06] or L2norm [MSD09], which yet leads to intractable optimization problem with intensive computation. Instead of penalizing histogram disparity, the matching cost can be defined
by rewarding histogram consistency, which enables efficient
computation of polynomial time [HS09]. When applying
co-segmentation to dozens of images, Joulin et al. [JBP10]
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2061

H. Huang & L. Zhang & H.-C. Zhang / RepSnapping

s

pixel node

s

300

250

terminal node Dp

Dp

blue

200

150

Blue

100

Vp ,q

Vp ,q

50

0
300
250
200

Green
150

t
(a)

(b)

100
50

green

0

0

50

red

(c)

200

Red

150

100

250

t

300

(d)

Figure 2: Classical MRF image segmentation on input image (a) casts as max-flow/min-cut optimization on the constructed s-t
graph (b). For pixels far way in the image, they are near to each other in RGB color space (c). Considering the repetition, new
graph is constructed by adding edges (purple) between pixels with prescribed similarity (d).

adapt discriminative clustering to bottom-up image segmentation, which extracts common information from the same
object class to improve the segmentation of all the input images. Batra et al. [BPK∗ 10] introduce an interactive
iCoseg system, which intelligently recommends where the
user should scribe next to complete the segmentation. Our
work is inspired by co-segmentation, but instead of segmentation of similar objects from multiple images, we are interested in cutout of repeated elements from single image.
Repetition is common and exploring repetitive similarity evokes increasing interest in many fields. Pauly et
al. [PMW∗ 08] analyze the pairwise similarity transformations and detect the regular or repeated structures in 3D
shapes. Cheng et al. [CZM∗ 10] present a repeated elements
detection method RepFinder, based on an improved boundary band method to locate similar object contours with small
changes. However, those methods impose much restriction
on shape cues. The pairwise similarity can also be used to
propagate interactive adjustment to all regions of similar appearance in an image [AP08, XLJ∗ 09, FFL10], which enables fast and smooth alteration on all the repeated elements.
In this paper, we would provide an efficient repetition cutout
tool to quickly segment the repeated elements.
3. Problem Statement
Given an input image I, image cutout is to assign a binary
label f p ∈ {0, 1} to each pixel p ∈ I. Then, the foreground
F = {p : f p = 0} and background B = {p : f p = 1} are
classified by assembling pixels with the same label. The resulting label set { f p } is typically obtained by optimization
on some well-defined energy functions, which usually take
regional homogeneity and meaningful segmentation into account. In this paper, we focus on cutout of repeated elements
possibly with notable shape variance in the images, which
demands exploration on their inherent repetition property.
3.1. MRF segmentation
Like some classical image segmentation methods [BJ01,
LSTS04, RKB04], we formulate the task of image cutout as
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

binary labeling of Markov Random Fields (MRF). In this
setting, input image I is represented by a graph G = (P, N ),
with each pixel as one node p ∈ P and pairwise adjacent
pixels as edge p, q ∈ N (see Figure 2 (b)). Then, the MRF
formulation for image cutout is defined as the following energy function:
E( f ) =

∑
p∈P

Dp( f p) +

∑

Vp,q ( f p , fq )

(1)

p<q∈N

where p < q indicates a mono-combination on the adjacent
nodes based on neighborhood system from graph edge links
in N . Then, the label assignment f p is such that the total
data item D p and smooth item Vp,q penalties are minimized.
The data item D p ( f p ) measures the conformity of node p assigned label f p , usually derived from the prior information.
The smooth item Vp,q ( f p , fq ) is charged for adjacent nodes
with different labels, which is critical to the labeling continuity and computational efficiency of solving Equation (1).
Generally, finding global optimization of Equation (1) is
difficult, but can be efficiently solved to local minimum
via graph cuts optimization [KZ04], which has been successfully applied in image segmentation [BJ01, LSTS04]. In
those graph cuts segmentation, two terminal nodes s and t
are added as well as edges from s-t to each pixel node, which
forms the s-t graph G = (P ∪ {s,t}, E). The weights on the
edges are defined as data item D p or smooth item Vp,q (see
Figure 2 (b)). Then, segmentation boundaries are obtained
by max-flow/min-cut procedure, which generates the final
image cuout according to labels on the pixel nodes.
3.2. RepSnapping for repeated elements
Our model attempts to simultaneously cut out all the repeated scene elements in graph cuts image segmentation.
However, previous MRF segmentation inhibits segmentation
of the repeated elements in one shot. The reason lies in that
the smooth item Vp,q involves only locally positional similarity based on the neighborhood N , which besieges labeling to
the similar repeated regions at far locations.
The distinction of an image with repeated elements is that

2062

H. Huang & L. Zhang & H.-C. Zhang / RepSnapping


p

(a)



(b)

(c)

(d)

Figure 3: (a) Input image and interactive red/blue scribbles to indicate foreground F and background B seed regions. (b) For
each pixel-node, the labeling priority is inferred from colors of seed regions. (c) Cutout results by classical MRF segmentation.
(d) Cutout results by RepSnapping. In (c) and (d), the cutout foreground is marked by dash lines

similar regions appear concurrently at different locations in
the image. Although the spatial relation between elements
might be distant, they share some repetitive similarity in
color, texture or shape, which makes them adjacent in this
sense. For example in Figure 2, two strawberries are far
away, but in RGB color space, they are quite near to each
other in the neighbor. Hence, the key of our method is to
simultaneously snap the repetition based on the similarity
between the repeated elements, which is denoted as γ(i, j)
for two pixels i and j.
Inspired by image co-segmentation [HS09], we adapt the
traditional MRF image segmentation (Equation (1)) by combining a repetition energy item based on the repetitive similarity measurement γ to suit cutout of the repeated elements.
Our new method, namely RepSnapping, is to optimize the
following energy function:
E( f ) =

∑
p∈P

D p ( f p )+

∑
p<q∈N

Vp,q ( f p , fq )+

∑

4.1. Graph with extended neighborhood
The key modification of MRF graph structure is the addition of edges i, j determined by the similarity of repeated elements. We still use G = {P ∪ {s,t}, E} to denote s-t graph proposed here. Consequently, the constructed
graph G consists of pixel-nodes P = {p ∈ I} and the set
of edges E, containing edges between adjacent pixels in 4neighbor (or 8-neighbor), edges between terminal nodes and
pixels, and edges linking two pixels with acceptable similarity. Then, appropriate weights are designated to the corresponding edges.
We use previous formulation of data item D p and smooth
item Vp,q in Equation (1). The data item D p in Equation (2)
can be defined as:
D p ( f p = 0) =

dB
p
B
dF
p + dp

(3)

D p ( f p = 1) =

dF
p
B
dF
p + dp

(4)

Ui, j ( fi , f j )

i, j∈H

(2)
where the data item D p and smooth item Vp,q are defined
as in Equation (1), Ui, j measures the labeling smoothness
on the similar pixels, and H is an extended neighborhood
system based on the repetitive similarity.
Actually, it can be seen that the main difference between
Equation (1) and (2) is the addition of energy item Ui, j ,
which accommodates the repetitive similarity on the extended neighborhood system H. If the value of γ(i, j) is
smaller than a threshold ε, there is an edge linking the two
pixels (see Figure 2 (d)). We will show that H can greatly
help the cutout of repeated elements but with little extra
computational effort on an extened graph.
4. The Graph Construction
We now discuss the graph construction of RepSnapping,
which embodies the repetitive similarity in cutout of repeated elements. In our implementation, user specifies some
seed regions F and B to indicate foreground and background
by drawing red/blue scribbles (see Figure 3 (a)), which then
provide prior evidence for repetition cutout.

B
where d F
p and d p denote the minimum color distance from
p to seed regions of F and B. As in [LSTS04], we first partition F and B into a set of clusters {KnF } and {KnB } usB
ing k-mean algorithm. Then, d F
p and d p are computed as
F
F
B
d p = min C(p) − Kn and d p = min C(p) − KmB , where
C(·) denotes pixel color.

The smooth item Vp,q based on 4-connected neighborhood
system N represents labeling energy due to gradient along
the object boundaries, which is defined as
Vp,q = λ · | f p − fq | · exp(−β · C(p) −C(q) 2 ),

p, q ∈ E
(5)
where λ is a trade-off weight, and β is a constant controlling variance tolerance when assigning labels to pixels with
different colors. As mentioned above, the spatial neighborhood system N enables compact segmentation for individual
object (see Figure 3 (c)). To cut out repeated elements, we
extend the neighborhood system by exploring adjacency according to the similarity measurement γ. Then, for pairwise
pixels i and j with γ(i, j) < ε, the edge i, j is designated
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

H. Huang & L. Zhang & H.-C. Zhang / RepSnapping

5. Experimental Results

weight as smooth item with the following function:
2

Ui, j = µ · | fi − f j | · exp(−β · γ(i, j) ),

i, j ∈ H

2063

(6)

With the smooth energy Ui, j on enhancement of
the repetitive similarity, Equation (2) is still graphrepsentable [KZ04], because it suggests Ui, j (0, 0) +
Ui, j (1, 1) ≤ Ui, j (0, 1) + Ui, j (1, 0). Hence, Equation (2) can
be efficiently solved via max-flow/min-cut optimization on
the constructed graph with extended neighborhood system.
Actually, the smooth energy item Ui, j add more effective
constraints on the pairs of similar pixels, which can thread
the pixels of repeated elements across the image. Thus, optimization of Equation (2) enforces the similar pixels with the
same labeling. Then based on the computed pixel-node labels, we are able to simultaneously cutout the repeated scene
elements using minor user interaction (see Figure 3 (d)).
4.2. Graph Cuts with Pre-segmentation
Based on the pixel nodes, the constructed graph G typically
contains a large number of edges, which consumes considerable time on graph cuts optimization. To reduce the computational cost, we perform pre-segmentation on the input
image to get over-segmentation as in [LSTS04]. Here, we
use mean-shift segmentation algorithm [CM02] to partition
the input image into small patches at first, which produces
a small amount of patches but can well preserve local structures of objects. Then, we replace the pixel nodes in G with
segmented patches, and the edges in neighborhood systems
N and H are re-built according to the adjacency of segmented patches, i.e., if two patches have common boundaries or acceptable repetitive similarity, there is an edge linking them (see Figure 4).

Figure 4: From segmentation by mean-shift clustering algorithm (left), the graph is constructed with segmented patches
as nodes and edges between the patches (right).

To verify the effectiveness of our RepSnapping, We have
tested it on a variety of input images with repeated scene
elements. Next, we discuss our experiments for evaluating
the performance of RepSnapping qualitatively and comparison to the state-of-the-art image segmentation and repetition detection methods. The key ingredient of RepSnapping is the similarity measurement γ and the corresponding
neighborhood system H. In the experiments described here,
we just use color information like other segmentation methods [LSTS04, RKB04], i.e., γ(i, j) = C(i) − C( j) . More
feature descriptors, like Gabor texture, SIFT descriptor, can
also be incorporated into our method. Figure 1, Figure 3 and
Figure 5 show some cutout results. From the cutout mask of
repeated elements, we can compute the alpha matte based
on the trimap by dilating and eroding the foreground region [LLW06]. Those repeated elements can be further applied in other image processings, like repetition editing and
image composition (see Figure 6).

5.1. Parameters setting
There is four dominant parameters related to our RepSnapping energy function, i.e., λ, µ, β and ε. The weights λ and µ
control tradeoff between segmentation compactness and repetition detection. In the examples of this paper, we set λ = 2,
µ = 10, which works well in cutout of repeated elements.
The parameter β determines variance for labeling smoothness, and is set to 0.1. The parameter ε is much important
in our algorithm, which affects repetition detection accuracy
and the computational cost. Ideal ε should be able to classify
the repetion based on the repetitive similarity. In our implementation, we set ε = 4 for all the tested images.
To obtain instant image cutout, we used mean-shift presegmentation to build the graph. In our experiment, the parameters of mean-shift segmentation is set as follows: the
spatial bandwidth is 3, the range bandwidth is 3, the minimum size in pixels of the final segmented regions allowed
is 30, and the weighting kernel is uniform. Experimental results show that such configuration has good balance in segmentation quality and computational efficiency.

5.2. Comparison
For the data item and smooth item, we use the average
color of each patch to represent the regional color, which
can well define the similarity between patches. With meanshift segmentation as pre-processing, the nodes and edges in
graph G can be significantly reduced. For example in Figure 4, the number of pixel nodes is reduced from 512 × 384
(image size) to 1, 612, and the number of edges is reduced
from 11, 251, 740 to 19, 210. Consequently, RepSnapping
on pre-segmented patches enables instant feedback when
adding new user interaction to adjust the cutout results.
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

We compared RepSnapping with the stat-of-the-art interactive image segmentation methods LazySnapping [LSTS04]
and Grabcut [RKB04], and repetition detection method
RepFinder [CZM∗ 10]. For fair comparison, no boundary
editing was further applied to locally refine the cut-out
boundary. We use mean-shift pre-segmentation (Section 4.2)
to build the graph in RepSnapping, LazySnapping and Grabcut. Figure 5 shows the comparison results. LazySnapping
and Grabcut are interactive image segmentation methods,
which provide instant cutout of interested regions based on

2064

H. Huang & L. Zhang & H.-C. Zhang / RepSnapping

Figure 5: Comparison of different repetition cutout methods. From left to right: input images, results by RepSnapping, results
by LazySnapping [LSTS04], results by Grabcut [RKB04] and results by RepFinder [CZM∗ 10]. For the first three methods,
the amount of user interaction is marked by the red/blue scribbles for the foreground/background. For Grabcut, the initial
background is indicated by the green rectangle. For RepFinder, the template elements are indicated by the blue contours.
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

2065

H. Huang & L. Zhang & H.-C. Zhang / RepSnapping

graph cuts segmentation. These two methods have good performance on cutout of individual objects. Broadly speaking, Grabcut can achieve better quality due to its iterative optimization. However, both LazySnapping and Grabcut are incompetent for cutout of repeated elements, even
with pretty much interaction involved in the procedure (see
Figure 5). RepFinder is to detect approximately repeated elements by shape matching, which is sensitive to moderate
shape changes. In contrast, our RepSnapping is able to simultaneously cut out almost all the repeated elements with
quite few user interactions.

equal or better cutout results with much less interactive effort. Hence, a user study was conducted to compare the performance of RepSnapping, LazySnapping and Grabcut. We
selected 20 novices and asked them to complete two repitition cutout tasks with the input images in Figure 1, Figure 3
and Figure 5. The first task is to employ the three methods
to cut out repeated elements in each image as accurate as
possible but with as less user interaction as possible. Then,
we recorded the average cutout timing for each image. In
the second task, the participants were instructed to get as
accurate cutout as possible on the repeated elements, but
only given 60 seconds for each image. Then, we evaluate
the quality of cutout by counting the number of error pixels
with respect to the “ground truth” cutout produced by experts. Figure 7 shows the statistics of user study results. It
can be seen that our RepSnapping shows significant advantages over LazySnapping and Grabcut in both execution time
and cutout quality.
RS

250

LS

GC

RS

90

LS

GC

80

Figure 6: With the alpha mattes (left), the cutout of repeated
elements can be used in many applications like repetition
editing (middle) and image composition (right).

200

70
60

150

50
40

100

30
20

50

10

Timing Although using extended neighborhood system
with more edges in the constructed graph, our RepSnapping
can still achieve instant segmentation feedback comparable
to LazySnapping and Grabcut. Most running time is consumed in mean-shift pre-segmentation. While RepFinder exhausts shape matching over the whole image, it needs much
more time on repetition detection. We run the methods on a
2.67GHz CPU with 4GB RAM. Table 1 shows the amount of
user interaction and timing statistics for different methods.
Table 1: Comparison of RepSnapping (RS), LazySnapping
(LS), Grabcut (GC) and RepFinder (RF). For RS, LS and
GC, the running time is graph cuts segmentation when
adding a new scribble, while for RF, it is the total running
time.(Ps: pre-segmentation time; #UI: number of interactive
scribbles.) Timing unit is second.
Image

Size

Ps

Goose (Fig. 1)
Cake (Fig. 3)
Duck (Fig.5)
Starfish (Fig.5)
Zebra (Fig.5)
Cream (Fig.5)
Lotus (Fig.5)
Lantern (Fig.5)

684x511
512x384
520x392
700x525
800x600
500x476
550x673
385x579

1.5
0.7
0.9
1.6
2.2
1.3
1.7
0.9

RS
2/0.018
2/0.008
2/0.004
2/0.013
4/0.023
3/0.007
2/0.012
3/0.012

#UI/Running time
LS
GC
17/0.015 25/0.243
16/0.007 18/0.154
10/0.004 5/0.122
4/0.012 5/0.195
19/0.021 8/0.285
7/0.007 4/0.120
10/0.011 6/0.152
55/0.011 47/0.161

RF
-/0.568
-/0.487
-/0.455
-/0.595
-/0.618
-/0.554
-/0.620
-/0.542

User study
We believe that RepSnapping outperforms existing state-of-the-art interactive image segmentation methods in ease of use, and capability of producing
c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

0

0
Goose

Cake

Duck

Starfish Zebra

Task 1

Cream

Lotus Lantern

Goose

Cake

Duck Starfish Zebra Cream Lotus Lantern

Task 2

Figure 7: Statistics of user study.

5.3. Limitations
RepSnapping provides an efficient computational framework
to cut out repeated elements in the image. However, only
color based similarity might result in insufficient repetition
cutout when the repeated elements happen to be similar in
color with background. Figure 8 shows a failure example,
where the bird’s body resembles regions on the roof. In this
case, we should combine more descriptive feature in repsimilarity and need more user scribbles to manually classify
the birds from the background.

Figure 8: Due to the repetitive similarity based on colors
between birds and roof, RepSnapping fails to cut out the repeated elements in the image.

6. Conclusion
We have developed a particular image cutout method, RepSnapping, oriented to images with repeated scene elements.

2066

H. Huang & L. Zhang & H.-C. Zhang / RepSnapping

A new optimization model is proposed using similarity between repeated elements for this special purpose, and an extended graph is constructed to solve the model via graph
cuts optimization. Our RepSnapping can simultaneously cut
out the repeated elements with much less user interaction,
which outperforms the state-of-the-art image segmentation
and repetition detection methods.
RepSnapping is an image cutout framework for simultaneous segmentation of multiple similar objects, which exploits the self-organization of repetition based on the inherent correlation. In our implementation, we mainly use
the color information to describe similarity between the
repeated elements, which is sufficient to achieve efficient
cutout for most examples. In the future, we plan to employ more sophisticated feature descriptors, like texture feature [AP08] or edge orientation [WYM09], and unsupervised clustering approach, like [YW08] to improve the accuracy of repetition snapping. Besides, it is promising to apply this method to videos having similar objects in the frame
sequences [HZF10], to faciliate a series of video processing
applications.
Acknowledgements
We thank the anonymous reviews for their helpful comments. We are also grateful to Ming-Ming Cheng and FangLue Zhang for producing the results of RepFinder. This
work was partly supported by the Program for New Century Excellent Talents in University (No. NCET-09-0635)
and the National Basic Research Project of China (No.
2010CB327900).
References
[Ado10]

A DOBE: Adobe Photoshop User Guide. 2010. 2

[FH04] F ELZENSZWALB P. F., H UTTENLOCHER D. P.: Efficient
graph-based image segmentation. Int. J. Comput. Vision 59, 2
(2004), 167–181. 2
[Gra06] G RADY L.: Random walks for image segmentation.
IEEE Trans. Pattern Anal. Mach. Intell. 28, 11 (2006), 1768–
1783. 2
[HS09] H OCHBAUM D. S., S INGH V.: An efficient algorithm for
co-segmentation. In Computer Vision and Pattern Recognition
(2009). 2, 4
[HZF10] H UANG H., Z HANG L., F U T.-N.: Video painting via
motion layer manipulation. Comput. Graph. Forum 29, 7 (2010),
2055–2064. 8
[JBP10] J OULIN A., BACH F., P ONCE J.: Discriminative clustering for image co-segmentation. In Computer Vision and Pattern
Recognition (2010). 2
[KZ04] KOLMOGOROV V., Z ABIH R.: What energy functions
can be minimized via graph cuts. IEEE Trans. Pattern Anal.
Mach. Intell. 26, 2 (2004), 147–159. 2, 3, 5
[LLW06] L EVIN A., L ISCHINSKI D., W EISS Y.: A closed form
solution to natural image matting. In Computer Vision and Pattern Recognition (2006), pp. 61–68. 2, 5
[LRAD07] L EVIN A., R AV-ACHA A., DANI L.: Spectral matting. In Computer Vision and Pattern Recognition (2007). 2
[LSTS04] L I Y., S UN J., TANG C.-K., S HUM H.-Y.: Lazy snapping. ACM Trans. Graph. 23, 3 (2004), 303–308. 1, 2, 3, 4, 5,
6
[MB95] M ORTENSEN E., BARRETT A.: Intelligent scissors for
image composition. In Siggraph (1995). 2
[MSD09] M UKHERJEE L., S INGH V., DYER C. R.: Halfintegrality based algorithms for cosegmentation of images. In
Computer Vision and Pattern Recognition (2009). 2
[PMW∗ 08] PAULY M., M ITRA N. J., WALLNER J., P OTTMANN
H., G UIBAS L. J.: Discovering structural regularity in 3D geometry. ACM Trans. Graph 27, 3 (2008), 43:1–43:11. 3
[RKB04] ROTHER C., KOLMOGOROV V., B LAKE A.: “Grabcut”: interactive foreground extraction using iterated graph cuts.
ACM Trans. Graph 23 (2004), 309–314. 1, 2, 3, 5, 6

[AP08] A N X.-B., P ELLACINI F.: Appprop: all-pairs appearance
space edit propagation. ACM Trans. Graph. 27, 3 (2008), 40:1–
40:9. 3, 8

[RMBK06] ROTHER C., M INKA T., B LAKE A., KOLMOGOROV
V.: Cosegmentation of image pairs by histogram matching - incorporating a global constraint into mrfs. In Computer Vision and
Pattern Recognition (2006), pp. 993–1000. 2

[BJ01] B OYKOV Y., J OLLY M.: Interactive graph cuts for optimal
boundary & region segmentation of objects in n-d images. In
International Conference on Computer Vision (2001). 2, 3

[RT00] RUZON M. A., T OMASI C.: Alpha estimation in natural
images. In Computer Vision and Pattern Recognition (2000). 2

[BPK∗ 10] BATRA D., PARIKH D., KOWDLE A., C HEN T., L UO
J.: iCoseg: interactive cosegmentation with intelligent scribble
guidance. In Computer Vision and Pattern Recognition (2010). 3
[CCSS01] C HUANG Y. Y., C URLESS B., S ALESIN D., S ZELISKI
R.: A Bayesian approach to digital matting. In Computer Vision
and Pattern Recognition (2001). 2
[CM02] C OMANICIU D., M EER P.: Mean shift: a robust approach toward feature space analysis. IEEE Trans. Pattern Anal.
Mach. Intell. 24, 5 (2002), 603–619. 2, 5
[CZM∗ 10] C HENG M. M., Z HANG F. L., M ITRA N. J., H UANG
X., H U S. M.: RepFinder: finding approximately repeated scene
elements for image editing. ACM Trans. Graph. 29, 4 (2010),
83:1–83:8. 1, 2, 3, 5, 6

[SJTS04] S UN J., J IA J., TANG C.-K., S HUM H.-Y.: Poisson
matting. ACM Trans. Graph 23, 3 (2004), 315–321. 2
[Tho92]
1

T HOMPSON D. W.: On Growth and Form. Dover, 1992.

[WYM09] WAN C. K., Y UAN B. Z., M IAO Z. J.: A moving object segmentation algorithm for static camera via active contours
and GMM. Science in China 52, 2 (2009), 322–328. 8
[XLJ∗ 09] X U K., L I Y., J U T., H U S.-M., L IU T.-Q.: Efficient
affinity-based edit propagation using k-d tree. ACM Trans. Graph
28, 5 (2009), 118:1–118:6. 3
[YW08] Y UAN J.-S., W U Y.: Context-aware clustering. In Computer Vision and Pattern Recognition (2008). 8

[FFL10] FARBMAN Z., FATTAL R., L ISCHINSKI D.: Diffusion
maps for edge-aware image editing. ACM Trans. Graph. 29, 6
(2010), 145:1–145:10. 3

c 2011 The Author(s)
c 2011 The Eurographics Association and Blackwell Publishing Ltd.

