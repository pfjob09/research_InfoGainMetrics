DOI: 10.1111/j.1467-8659.2010.01838.x

COMPUTER GRAPHICS

forum

Volume 30 (2011), number 1 pp. 139–151

Taylor Prediction for Mesh Geometry Compression
Cl´ement Courbet and C´eline Hudelot
Ecole Centrale Paris, France
{clement.courbet, celine.hudelot}@ecp.fr

Abstract
In this paper, we introduce a new formalism for mesh geometry prediction. We derive a class of smooth linear
predictors from a simple approach based on the Taylor expansion of the mesh geometry function. We use this
method as a generic way to compute weights for various linear predictors used for mesh compression and compare
them with those of existing methods. We show that our scheme is actually equivalent to the Modified Butterfly
subdivision scheme used for wavelet mesh compression. We also build new efficient predictors that can be used for
connectivity-driven compression in place of other schemes like Average/Dual Parallelogram Prediction and High
Degree Polygon Prediction. The new predictors use the same neighbourhood, but do not make any assumption on
mesh anisotropy. In the case of Average Parallelogram Prediction, our new weights improve compression rates
from 3% to 18% on our test meshes. For Dual Parallelogram Prediction, our weights are equivalent to those of the
previous Freelence approach, that outperforms traditional schemes by 16% on average. Our method effectively
shows that these weights are optimal for the class of smooth meshes. Modifying existing schemes to make use of
our method is free because only the prediction weights have to be modified in the code.
Keywords: geometry compression, mesh compression, subdivision, parallelogram prediction
Categories and Subject Descriptors (according to ACM CCS): I.3.5 [Computer Graphics]: Computational
Geometry and Object Modeling—Curve, surface, solid, and object representations - Computational Geometry and
Object Modeling

the same algorithms typically use around 16 bits per vertex
to represent it losslessly.

1. Introduction
Over the last decades, the size of three-dimensional (3D)
meshes has continuously raised with the needs for more
accurate representations of models coming from numerical
simulations or 3D scans of real-world objects. Because the
available storage and network bandwidth are limited, compression is needed to reduce the size of these data sets. Triangle mesh compression has been a very active area of research,
and several very efficient algorithms providing single-rate as
well as progressive compression of triangular and polygonal meshes have been described in the literature [PKK05,
AG03]. Of the two components of the mesh information,
connectivity and geometry, the former has been the target of
most of the effort. The best algorithms offer a compression
performance of 1.5–3 bits per vertex for connectivity compression [AD01]. Geometry remains the bottleneck, because
c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics
Association and Blackwell Publishing Ltd. Published by
Blackwell Publishing, 9600 Garsington Road, Oxford OX4
2DQ, UK and 350 Main Street, Malden, MA 02148, USA.

We propose an algorithm which predicts the position of a
vertex based on a linear combination of neighbouring vertices. Although this is a common method [TG98, SKL03,
CoCI02, KSS00], we derive the weights from a local approach based on the Taylor expansion of the function representing the geometry of the mesh. This enables the computation of weights for different methods in a generic way,
whereas each existing method usually uses its own way for
determining weights.
After a review of existing techniques, we explain how
to compute prediction weights using our approach in the
general case. Then, we apply our method to lossless singlerate mesh compression as well as mesh subdivision, which

139

140

C. Courbet & C. Hudelot / Taylor Prediction for Mesh Geometry Compression

Figure 1: Linear predictors, from top left to bottom right: PP, DPP [SKL03], Freelence [KPRW05], APP [CoCI02], and CPM
[PR00]. The vertices of the triangles in blue are known to the decoder.
is strongly related to lossy progressive mesh compression
[KSS00]. In the lossless case, we use our paradigm to show
that the weights of the Freelence scheme [KPRW05] are optimal for smooth meshes. We also improve the high-degree
polygon predictor of Isenburg et al. [IIGS05]. For lossy compression, we show that our method is another generic way of
determining wavelet transform coefficients already used in
existing compression schemes [KG02].

2. Related Work
2.1. Geometry compression
Various approaches have been proposed to compress the
geometry of triangle meshes. The most efficient methods
are lossy. The spectral approach of Karni and Gotsman
[KG00a] projects the geometry of the mesh on a global,
mesh-dependent spectral basis derived from the mesh Laplacian. This method gives the best results, but is computationally inefficient as the eigenvectors have to be computed for
the very large mesh Laplacian. To alleviate this problem, they
later proposed to use fixed spectral bases [KG01], but the results are less satisfying than [KG00a]. Other approaches use
progressive representations of a mesh and predict the geometry of a finer mesh from its coarser representation. The methods based on wavelets [KSS00, KG02] are very efficient, but
usually involve a semi-regular remeshing step which changes
the mesh connectivity.
In some cases, where a degradation of the mesh data cannot be tolerated, lossless compression must be provided.
Valette et al. [VP04] have proposed a wavelet method, called
wavemesh, which does not need remeshing, but the authors
do not give results concerning the geometry compression
ratios of their method. Gandoin and Devillers use a progres-

sive geometry-driven Kd-Tree approach to compress triangle
meshes and achieve 15.7 bpv on average [GD02]. A more recent approach uses octrees, and achieve approximately 90%
of the above figure for most meshes, but down to 40% for
some meshes [PK05].
However, for single-rate compression, the Parallelogram
Prediction (PP) of Touma and Gotsman [TG98] remains very
popular. The position of a vertex is predicted as completing
the parallelogram containing the three vertices of an already
decoded adjacent triangle. A residual vector is stored, which
is usually small and can be compressed using an entropy
coder. This method typically results in compression ratios of
about 16–18 bits per vertex at 12 bits quantification. It has
the advantages of simplicity and speed, which make it and
its derivatives the most widely used methods for single-rate
geometry compression.
2.2. Linear prediction
2.2.1. Parallelogram rule and extensions
The parallelogram rule can be seen as a linear predictor which
assigns the weights { − 1, 1, 1} to three already visited vertices of the neighbourhood which form a triangle (Figure 1,
top left). Isenburg and Alliez [IA02] have successfully extended the use of the parallelogram rule to compress quaddominant polygonal meshes, and improve the compression
ratios (by about 10–40%) by applying the parallelogram rule
within rather than across polygons. The parallelogram rule
has also been extended to predict the geometry of polygons
of higher degree by using weights derived from a spectral
representation of the geometry of a polygon [IIGS05].
The prediction error of the parallelogram rule is highly dependent on the order in which the mesh is traversed. Various

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

C. Courbet & C. Hudelot / Taylor Prediction for Mesh Geometry Compression

methods [KG00b, CCMW05] have applied the parallelogram
rule along a geometry-driven traversal of the mesh to reduce
this error. They obtain 32% improvement on average.
Some linear predictors use different neighbourhoods. Sim
et al. [SKL03] introduce the dual parallelogram prediction
(DPP). When possible, they predict the position of a vertex
as the average of two parallelogram predictions (Figure 1,
top centre). Approximately 75% of the vertices of their test
meshes can be predicted this way. This improves compression ratios by 3% over simple PP.
In a later work, K¨alberer et al. use the same neighbourhood but average three parallelogram rules, the first two
being the previous parallelogram rule, and the third being
applied across a virtual edge joining the two outer vertices
(dashed line on Figure 1, top right). We call this rule Freelence Dual Parallelogram Prediction (fDPP). They report
improved compression rates compared to applying a simple parallelogram rule for prediction, but they only compare
with the parallelogram rule (we will see later in this paper
that their scheme also performs better than the DPP). They
also discuss another stencil which would take into account
the last vertex of the one ring of the centre vertex of the
DPP neighbourhood if that vertex is of degree 6. They experimentally determine weights for this neighbourhood, and
conclude that these weights depend on the type of model
(irregular, subdivision, CAD). They do not give any method
to derive these weights in a theoretical, consistent manner.
Cohen-Or et al. [CoCI02] take the parallelogram averaging idea one step further by using all possible parallelogram
predictions around a vertex. They proceed in two steps. The
first one starts with the connectivity of the mesh and the position of a small number of vertices. The mesh is processed
using several passes. Each pass predicts each vertex using the
average of all the possible parallelogram predictions around
this vertex (Average Parallelogram Prediction or APP), assuming that the displacement is zero. This predictor is shown
on the bottom left of Figure 1. A smooth approximation to
the mesh is thus obtained. In a second step, they apply a
scheme similar to [SKL03] that uses a dual parallelogram
rule to correct the position of the vertices. This scheme gives
the best result for lossless single-rate geometry compression.

141

They use a variation of it (that we call CPM) where they
instead of simply − K1 and K2
use the weights − Kα and 1+α
K
(where K is the number of parallelogram predictions). They
experimentally determined that α = 0.15 performed well on
average (Figure 1, bottom right).

2.2.2. Regular grids
Mesh and image compression traditionally have a lot of ideas
in common. Prediction is widely used for image compression,
and the parallelogram rule finds its regular counterpart in the
JPEG-LS predictor [WSS96]. It is thus very tempting to adapt
other predictors used for image compression.
For prediction on regular grids (e.g. images), a wealth
of predictors exist, because the neighbourhood of a specific sample always has the same structure. Recently, Ibarria et al. [ILR07] have introduced a spectral interpolation
method which enables prediction when an irregular set of
the 3 × 3 stencil of neighbouring samples is known. This is
the case, for example, during scanline or progressive transmission. Similarly to [IIGS05], they decompose the stencil
on the 9 vectors of the Discrete Cosine Transform basis. If
there are k unknown samples, they set the k highest frequencies to 0, and solve the resulting system analytically. Thus,
they determine the weights to predict one unknown sample
from the known samples.
Their approach finds the weights that give the smoothest
prediction (in terms of frequency) that fits the known samples. They have computed a lookup table that gives the
weights for a given stencil configuration. However, their approach is not suitable in the case of mesh compression, because the connectivity itself of the neighbourhood changes
from vertex to vertex, and not just the number of known and
unknown samples.

2.2.3. Wavelets and subdivision

Sorkine et al. use a global optimization approach to reduce the visibility of coordinates quantization by applying
quantization in the prediction residual space (delta coordinates) instead of the original coordinates space [CoCI02].
This results in quantization errors that are mostly in the low
frequencies. They predict the position of a vertex as the average of the positions of the vertices in the one-ring. However,
they also study the reduction in prediction error when using
the average of all parallelogram predictions and reach the
same conclusions as [CoCI02].

Wavelets are a very powerful tool for the analysis of tensor
product spaces (e.g. regular grids), with numerous applications in image compression. This has led to a lot of research
effort to adapt wavelet compression to meshes, with very
good results. Because the construction of wavelets on irregular meshes is a hard problem, most authors choose to change
the connectivity information by first remeshing the surface
[KSS00, KG02] to obtain a subdivision mesh. To the best
of our knowledge, only one approach [VP04] is completely
lossless, in the sense that it keeps the initial connectivity of
the mesh. Remeshing is particularly acceptable in cases when
the connectivity is of lesser importance, for example in the
case of scanned objects, where only the underlying surface
is of interest.

For progressive compression, Pajarola and Rossignac remarked that the APP did not give the best results [PR00].

Wavelet-based approaches intrinsically use linear prediction to define their filters. To reconstruct the final mesh,

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

142

C. Courbet & C. Hudelot / Taylor Prediction for Mesh Geometry Compression

a base (usually irregular) coarse mesh M 0 is progressively
subdivided. At each subdivision level, a finer mesh M i+1 is
constructed from M i by adding new vertices at the middle of
each edge. The position of each new vertex is predicted linearly from the vertices of a given neighbourhood in M i . This
step constitutes the predict step of the wavelet scheme, and
the wavelet coefficients are the difference between the actual
and predicted positions. Some schemes (e.g. [KSS00]) then
use lifting and update the positions of the vertices of M i using
the vertices of M i+1 . Another scheme [KG02] uses non-lifted
wavelets and only stores the normal component of the difference between two levels of subdivision. In both cases, the
efficiency of linear prediction schemes directly influences the
bitrates of wavelet-based compression methods.

2.3. Determining prediction weights
The linear predictors described above mainly use two different approaches to determine weights. On one hand, the spatial approach considers prediction as a geometric problem.
For example, the parallelogram rule predicts that adjacent
triangles are geometrically the same. Average Parallelogram
Prediction uses the point closest to all different predictions.
On the other hand, spectral methods work in the frequency
domain. This is the idea of [IIGS05] and [ILR07], but is
also true in the case of wavelets. The wavelet coefficients
used by Khodakovsky and Guskov [KG02] are determined
using a spectral approach very similar to [ILR07] ([ZSS96]).
Although the spatial and spectral methods use different arguments, we will see in Sections 4 and 5 that they can sometimes
be bridged.

Around these points, f can be expanded using the secondorder Taylor approximation:
∀p ∈ D − {q1 , . . . qm },
T

f (p + dp) =

f (p) + dp .(∇ f )(p)
+

1 T
T
dp .(∇∇ f )(p).dp + o(dp .dp),
2

(1)

where ∇f and ∇∇f are, respectively, the Gradient and Hessian of f .
Let us suppose that the value of the coordinates of a point
of M at parameter value (u, v) is unknown. This happens
when the description of M is incomplete, for example if
M is a subdivision surface specified using a finite number
of control points, or if M is being decompressed. In the
following, we will show how to derive a linear predictor to
compute f (u, v) from k vertices of the neighbourhood by
making the assumption that f is 2-smooth. We will call α i
the coefficients of the prediction:

Definition 2. Let (p1 , . . . , pk ) ∈ D k . Then a linear prediction of f (p) is
k

f (p) =

αi f (pi ).

(2)

i=1

3.2. Prediction
If f is 2-smooth, then we can approximate the value of f
at p1 , . . . , pk in the prediction definition (3.1) using Equation (1):
k

3. Prediction Using Taylor Expansion

0=

αi

− 1 .f

i=1

The prediction method that we propose belongs to the spatial category of approaches. Before detailing the prediction
scheme, we introduce the definitions and notations that we
use throughout the paper.

k

i=1

+
3.1. Background

+

Definition 1. Let M be a differentiable two-manifold
defined by the vector-valued function:
f (u, v) = (x(u, v), y(u, v), z(u, v)) ,

αi (ui − u)

+

∂f
+
∂u

k

1
2

∂ f
∂u2

αi (vi − v)2

∂ 2f
∂v 2

k

1
2

i=1
k

+

αi (ui − u)(vi − v)
i=1

αi (vi − v)
i=1

∂f
∂v

2

αi (ui − u)2
i=1

k

∂ 2f
∂u∂v

+ o (ui − u)2 + (vi − v)2 + (ui − u)(vi − v) .
(3)

where (u, v) are parameter values and span a certain subdomain D of R2 . In the following, we note p = (u, v). x, y
and z are the coordinates of the points of M.
M is 2-smooth if f is C 2 -differentiable on all but a finite
number of points {q1 , . . . , qm }.

We can see that if the predictor is to generate constant
surfaces (f is not zero, but ∇ f and ∇∇ f are), then the
condition ( ki=1 αi ) − 1 = 0 must be verified. This property
makes the scheme affine invariant.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

143

C. Courbet & C. Hudelot / Taylor Prediction for Mesh Geometry Compression

If surfaces with local zero curvature (∇∇ f = 0) are to be
predicted exactly, then the coefficients of ∂∂uf and ∂∂vf must
be zero. By repeating this process, the predictor weights can
be determined by solving the linear system arising from the
reproduction of polynomials of increasing degree.
It may happen that for some stencils, the system is overconstrained. In that case, one may decrease the order of expansion of the Taylor series (Section 5.1), or make some
additional assumptions on f (Sections 4 and 5.2). If the
system is underconstrained, it is possible to increase the order of the expansion to find additional constraints, or use
other assumptions (Section 4, for valences ≥7).
To illustrate the process, the following sections show how
to use this formalism to compute the weights of several prediction schemes. We have seen in Section 2 that linear prediction is widely used for mesh compression. Several waveletbased algorithms for meshes [KG02, SS95, LDW97] rely on
the butterfly scheme of Dyn et al. [DLG90], later generalized
to irregular meshes by Zorin et al. [ZSS96]. Therefore, we
will take this scheme as example to show how our method can
be used to derive weights for subdivision. Then, we will apply our approach to various typical settings found in lossless
single-rate compression algorithms.

4. Subdivision: Modified Butterfly Scheme
The modified butterfly scheme [ZSS96] uses two different
stencils depending on the regularity of the mesh around each
newly added vertex (Figure 2 ), the regular case being the
same as in the original butterfly scheme [DLG90]. The authors note that polynomial reproduction up to degree n is a
necessary condition for n-smoothness of the limit surface.
They use the fact that the neighbourhood is symmetric to use
the Discrete Fourier Transform (DFT). They translate the
polynomial reproduction condition to the spectral domain,
which, for polynomials of degree 2, constrains five of the

Figure 2: Prediction stencils of the modified butterfly
scheme, around ordinary vertices (left panel), and extraordinary vertices (right right). The vertex to predict is drawn
in black, and identical colours indicate similar weights. K is
the degree of the extraordinary vertex.

eigenvalues of the subdivision matrix. The weights are determined by taking the inverse DFT of the vector of eigenvalues
of the subdivision matrix, where these constraints have been
met. They are given in Figure 2.
It is to be noted that the constraints are sometimes not
sufficient to completely determine the weights, and sometimes too strong. In the case of an extraordinary vertex of
degree K (Figure 2, right panel), the polynomial reproduction condition fixes the eigenvalues 1 and K − 1 to 1/2
and the eigenvalues 0, 2, and K − 2 to 1/4. In the case when
K = 3, the constraints for the first and second eigenvalues are
incompatible. When K > 5, there are eigenvalues that are not
fixed. Both of these situations need additional assumptions to
uniquely determine the weights. We will discuss them later
in this section.
Because both approaches rely on polynomial reproduction
to determine the weights, our approach is equivalent to the
modified butterfly scheme. However we do not transfer the
problem from the spatial to the spectral domain. Although
we do not derive new weights for this subdivision scheme,
the spatial domain approach can help to justify in a more
intuitive manner some of the choices made when all constraints cannot be met (e.g. at extraordinary vertices when
K = 3) or when there remain some degrees of freedom (K ≥
7). Therefore, we show below how to compute the modified
butterfly weights using the spatial approach, in the ordinary
and extraordinary case.
For ordinary vertices, the modified butterfly scheme uses
the first stencil of Figure 2. Note that because of symmetry, α 1 = α 2 , α 3 = α 4 and α 5 = α 6 = α 7 = α 8 . Affine
invariance enables us to choose p = (0, 0) and {p1 , . . . , pk }
equal to:

p1 =

u+

1
2

,

v

p2 =

u−

1
2

,

v

p3 =

u

,

v+

p4 =

u

,

v−

p5 =

u+1

,

v+

p6 =

u−1

,

v+

p7 =

u+1

,

v−

p8 =

u−1

,

v−

√

3
2
√
3
2
√
3
2
√
3
2
√
3
2
√
3
2

.

(4)

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

144

C. Courbet & C. Hudelot / Taylor Prediction for Mesh Geometry Compression

These conditions are met by the weights of the modified
butterfly scheme, except for K = 3. In this particular case, the
resulting system is underconstrained for a first-order Taylor
expansion, but overconstrained at order 2:

Then by applying Equation (3), we have
0 = [2α1 + 2α3 + 4α5 − 1]f
+0 ×

∂f
∂f
+0×
∂u
∂v

0 = [α1 + α2 + 2α3 − 1]f

(5)

∂ 2f
∂ 2f
1
+0 ×
+ [α1 + 8α5 ] ×
∂u∂v
4
∂u2

+ −

3
∂ 2f
+ [α3 + 2α5 ] ×
.
4
∂v 2

The two approaches being equivalent, these weights are
the same as those of the butterfly scheme.
Around extraordinary points, the modified butterfly
scheme uses the vertices of the 1-ring of the irregular vertex
of degree K. The corresponding stencil is shown in Figure 2.
This time, the neighbourhood is defined by
⎧
1
⎪
p = u − ,v ,
⎪
⎪
⎨ 1
2
2π i
K

(9)

∂ 2f
3 ∂ 2f
α1
α2
+
.
+
+
+ α3
α3
8
8
4 ∂v 2
∂u2

From this equation, we can immediately see that to reproduce all polynomials of degree 2, the following conditions
must be met
⎧
1
⎪
⎪
⎧
α1 = +
⎪
⎪
2
2α + 2α3 + 4α5 = 1
⎪
⎪
⎪
⎪
⎨
⎨ 1
1
α1 + 8α5 = 0
⇒ α3 = +
(6)
⎪
⎪
8
⎪
⎪
⎪
⎩
⎪
α3 + 2α5 = 0
⎪
⎪
⎩ α5 = − 1
16

1
⎪
⎪
⎪
⎩ pi+2 = u + − 2 + cos

∂f
α2
α1
+
− 2α3
2
2
∂u

, v + sin

2π i
K

.
(7)

Expanding to the second order gives the following conditions:
⎧
⎪
α1 +
αk = 1,
⎪
⎪
⎪
⎪
k≥2
⎪
⎪
⎪
⎪
⎪
1
2kπ
⎪
⎪
= ,
αk+2 . cos
⎪
⎪
⎪
K
2
⎪
k≥0
⎪
⎪
⎪
⎪
⎪
2kπ
⎪
⎪
⎪
= 0,
αk+2 . sin
⎪
⎪
K
⎨ k≥0
(8)
⎪
2kπ
⎪
2
⎪
=
0,
α
.
sin
k+2
⎪
⎪
K
⎪
⎪
⎪ k≥0
⎪
⎪
⎪
⎪
3
⎪
⎪
α1 = ,
⎪
⎪
4
⎪
⎪
⎪
⎪
⎪
4kπ
⎪
⎪
= 0.
αk+2 . sin
⎪
⎪
K
⎪ k≥0
⎩

The first-order expansion imposes that
⎧
1
⎪
⎪
⎨ α1 = 2 − 3α3 ,
1
⎪
⎪
⎩ α2 = + α3 .
2

(10)

To determine α 3 in the system above, an additional constraint is needed, that will be derived from the second-order
reproduction constraint. In their scheme, Zorin et al. manually pick the constraints that will be ignored. In the spatial
domain, it is possible to better justify the choice that we
make. Although it is not possible to zero the second-order
term in the expansion, it can be minimized. If we model the
2
2
value of the second order derivatives ∂∂vf2 and ∂∂uf2 by two
random variables Cu and Cv , then we may try to minimize
the variance of the second order Taylor expansion, that is
find α 3 such that V ar([ α81 + α82 + α3 ]Cu + 34 α3 Cv ) is minimal. Because of the symmetry of the problem, Cu and Cv
have the same variance σC2u Cu = σC2v Cv = σ 2 . We call σC2u Cv
the covariance of Cu and Cv . Then, using Equation (10), the
variance of the second-order term of the Taylor expansion
can be written:
Var
=

α2
3
α1
+
+ α3 Cu + α3 Cv
8
8
4
1 3
+ α3
8 4

2

σ2 +

9 2 2
3
α3 σ + σC2u Cv α3 (1 + 12α3 ) .
16
32
(11)

Minimizing the variance is a simple linear problem ( 163 (1 +
σ2

12α3 )(1 + Cσu2Cv ) = 0). The optimal weights do not depend
on σ 2 nor σC2u Cv , and the weights are α3 = − 121 , α 1 = 3/4
and α 2 = 5/12, which are the same as in the scheme of Zorin
et al. In the cases K = 4 and K = 5, the system obtained via
the spatial approach is consistent, and yields once again the
same weights as the modified butterfly scheme.
For K >= 7, Zorin et al. choose to take the nonconstrained eigenvalues equal to zero. They justify the choice
by the simplicity of the approach. In the spatial domain, however, it is easier to find a rationale for this choice. As remarked

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

C. Courbet & C. Hudelot / Taylor Prediction for Mesh Geometry Compression

145

Figure 3: Prediction stencils traditionally used for compression: parallelogram (left panel), dual parallelogram (centre panel)
and K-star (right panel, K = 6).

elsewhere [IIGS05], in the absence of other assumptions to
help determine the remaining degrees of freedom, a good
heuristic is to choose the weights of minimum amplitude (i.e.
for which the norm of the weights vector α = ki=1 αi 2 is
smallest). This spreads the dependency of the prediction on
all the points by avoiding giving too much importance to a
single one of them. This argument, that is initially valid in
the spatial domain, can be used retroactively to justify the
choice made by Zorin et al.. Because the (inverse) DFT is a
unitary transformation, the norm of the weights vector is the
same as the norm of the vector of eigenvalues. Making the
remaining eigenvalues zero actually minimizes the norm of
the vector of eigenvalues, thus also minimizing the norm of
the weights.

prediction error and residual entropy. However, it is usually
possible to further reduce the error by using other prediction
weights derived from our approach.
As noted by Cohen-Or et al. [CoCI02], conquest-based
compression methods will predict vertices using two PP rules
on average. This gives a special importance to the DPP stencil
(Figure 3, centre panel). Applying (3) to this stencil yields
the following equation:
0 = [α1 + 2α2 + 2α4 − 1] f
− [α1 + α2 + 3α4 ]
+

5. Lossless Compression

∂f
∂u

α1
∂ 2f
∂ 2f
3
α2
9
+ [α2 + α4 ] 2 .
+
+ α4
2
2
4
4
4
∂u
∂v
(13)

The first-order expansion imposes that

5.1. Parallelogram rule
The parallelogram rule, used by nearly all predictors, is a
special case of our approach. For symmetry reasons, it has
only two different weights shown in Figure 3. Because the
number of weights is small, expanding Equation (3) to the
first order is sufficient to constrain them:
√
∂f
0 = [2α1 + α3 − 1]f − 3[α1 + α3 ]
∂u
⎧
⎨ α1 = 1
⇒
⎩ α3 = −1.

α1 = −1 − 4α4 ,
α2 = 1 + α 4 .

We can easily verify that the DPP weights α 1 = 1, α2 = 12 ,
α4 = − 21 fulfill these requirements. However, substituting
these weights into (13) yields
0=−

(12)

5.2. Dual parallelogram prediction
Because the parallelogram predictor works so well, it is natural to try to extend it to cases where several neighbours can
be used to predict a vertex. We have seen in Section 2.2.1 that
other approaches [CoCI02, SKL03] have used an average of
several parallelogram predictions. This results in a predictor
that is also linear, and that led to the expected decrease in

(14)

1 ∂ 2f
.
2 ∂u2

(15)

This means that the DPP weights force a zero curvature
along the u direction. This makes sense if the mesh is aligned
with the features of the surface it represents. This is the
case for some CAD models when the modeller has taken
into account the anisotropy of the model. However, for most
triangle meshes, there is no reason why the mesh should have
such properties, so it is more natural to avoid privileging a
direction.
For this reason, we use the same symmetric approach as in
Section 4, and we minimize the variance of the second-order

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

146

C. Courbet & C. Hudelot / Taylor Prediction for Mesh Geometry Compression

Figure 4. Some of the meshes used in our experiments, with close-up views to show their regularity. From left to right: armadillo
[173 kv], Chinese guardian lion [656 kv], guardian (uniform) [153 kv], gipshand [137 kv], Max Planck [199 kv], Neptune
(uniform) [1.6 Mv].

term in the expansion. This time, α 4 is the solution to
2 + α4 5 + 3

σC2u Cv
σ2

= 0.

And the corresponding weights are
⎞
⎛
⎧
σC2u Cv
⎪
⎪
3(1
−
)
⎪
⎜
⎪
⎪
σ2 ⎟
⎟,
⎪
α1 = ⎜
⎪
⎝
⎪
⎪
σC2u Cv ⎠
⎪
⎪
5+3 2
⎪
⎪
⎪
σ
⎪
⎪
⎞
⎛
⎪
2
⎪
σ
⎪
Cu Cv
⎪
⎨
⎜ 3(1 + σ 2 ) ⎟
⎟,
α2 = ⎜
⎝
⎪
σC2u Cv ⎠
⎪
⎪
5+3 2
⎪
⎪
⎪
σ
⎪
⎪
⎞
⎛
⎪
⎪
⎪
⎪
⎪
⎪
⎟
⎜
2
⎪
⎪
⎟.
⎪
α4 = ⎜
⎪
2
⎠
⎝−
⎪
σ
⎪
Cu Cv
⎩
5+3 2
σ

(16)

Model

(17)

To avoid breaking the line of thoughts, we postpone to the
σ2

annex the calculus of Cσu2Cv . To determine the weights here
we only need its value, which is 13 and enables us to write
⎧
1
⎪
⎪
α1 = ,
⎪
⎪
3
⎪
⎪
⎨
2
α2 = ,
⎪
3
⎪
⎪
⎪
⎪
⎪
⎩ α4 = − 1 .
3

Table 1: Comparison of the original DPP prediction scheme of
[CoCI02, SKL03] and the optimal Freelence approach. The two
first columns are the geometry compression bit rates (entropy of
residuals), in bits per vertex, for an original quantization of 12 bpv
(14 bpv for the finer Neptune mesh). The last column shows the
improvement using the optimal weights.

(18)

This shows that the Freelence weights are optimal for
smooth meshes, in the sense that they provide the smallest
prediction error.

armadillo
guardian
gipshand
Max Planck
Neptune (uniform)
guardian (uniform)

Classical

Optimal

7.40
6.85
5.54
4.54
4.43
6.39

6.67
5.95
4.66
3.46
3.55
5.65

R/R
10%
13%
16%
24%
20%
12%

To show the difference between the DPP and optimal
weights in terms of compression rate, we implemented a Dual
Parallelogram Compressor similar to Freelence [KPRW05].
In our experiments, we used both irregular and uniform
meshes. They are illustrated in Figure 4. The Table 1 compares the geometry compression bit rates for the DPP and
Freelence predictors. The experiments show a constant improvement of around 16% using the optimal weights.

5.3. Average parallelogram prediction
We also computed the weights for the case when a vertex of
degree K can be reached via K applications of the parallelogram rule, that we call K-star (the Figure 3 shows a 6-star).
This stencil will be used for example in the first step of
[CoCI02] or to replace the experimental α = 0.15 of [PR00].
Because of symmetry, there are only two different weights,

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

C. Courbet & C. Hudelot / Taylor Prediction for Mesh Geometry Compression
Table 2: Comparison of the APP, CPM and Taylor prediction
schemes, when applied to the geometry compression method described in [PR00], modified to use local quantization. The three first
columns are the entropy of residuals, in bits per vertex, using a quantization of 14 bpv. The last column shows the improvement using the
Taylor weights instead of CPM.

Model

APP

CPM

Taylor

armadillo
guardian
gipshand
Max Planck
Neptune (uniform)
guardian (uniform)

15.83
13.28
11.16
10.59
8.06
14.27

13.10
12.31
8.86
8.32
6.32
12.87

10.67
11.94
7.61
7.76
5.74
12.25

R/R
18%
3%
14%
7%
9%
5%

one for the creases of the star (Figure 3, red) and the other
for the tips (Figure 3, blue):
⎧
1
1
⎪
αtip =
⎪
⎪
√
π
π 2
⎪
K
⎪
⎪
+ 3sin
1 − cos
⎪
⎨
K
K
⎛
⎞
⎪
⎪
1
1 ⎜
⎪
⎟
⎪
⎪
αcrease =
⎝1 −
⎪
√
π
π 2⎠
⎪
K
⎩
+ 3sin
1 − cos
K
K
(19)
To compare the efficiency of the three different schemes
(APP, CPM and ours), we implemented a geometry compression scheme similar to that of [PR00], where the original
CPM predictor is presented. We only slightly modified the
quantization scheme: Instead of a global quantization, we apply quantization in a local frame, as in Freelence [KPRW05].
The results are given in Table 2. We can see that our Taylor predictor consistently improves compression compared
to CPM, which is in turn superior to APP.

cases, however, the weights are different (polygons in green
and red). For odd number of known points and for cases
where only one vertex is missing, the system is simply constrained and there is no need for variance or weights norm
minimization.
We have repeated the experiment conducted in [IIGS05],
and compared the results with our method. For a variety of
models, we have computed the prediction error given by the
weights from [IIGS05] and the weights using our method.
As these weights are the same as those of Isenburg et al.
for pentagons, Table 3 only report results for hexa-, heptaand octagons. We can see that in nearly all cases, the prediction error was reduced. In the case where there are four
known points of the octagon, results are mitigated. Although
the error reduction is usually biggest for higher degree polygons, the improvements in weights will be most useful for
the hexagon. For the meshes we used, approximately 70%
of the vertices are predicted using hexagon rules, and 25%
of the total number of vertices are predicted using the rule
hexagon/v4. The problem noted by Isenburg et al. remains:
It is sometimes better to use the octagon/v3 rule instead of
octagon/v5 and octagon/v6 rules.

5.5. Spectral prediction
Although developed for meshes in the irregular setting, our
method can also be used to determine prediction weights on
regular grids. In the following, we will show that our method
is actually equivalent to that of Ibarria et al. [ILR07] on 3 ×
3 stencils.
The spectral method of Ibarria et al. consists in finding
the prediction that zeroes the high frequency content of the
discrete cosine transform of f . This is actually equivalent
to finding the prediction weights that reproduce the basis
vectors Bi of the DCT, that is the prediction weights α i , i ∈
[1, 8] are such that
B¯i .α = Bi,u ,

5.4. High degree polygon prediction
Our method can also be used to derive weights to predict
polygon geometry as in [IIGS05]. To determine their prediction weights, Isenburg et al. also have to deal with an overconstrained system. Their heuristic is to use the weights that
minimize the norm of the vector of weights α = ki=1 αi 2 .
According to this heuristic, the weights derived using our
variance minimization approach are deemed less efficient.
However, experiments show that this is generally not the
case.
Figure 5 gives the prediction weights for the spectral
method of Isenburg et al. and our spatial approach. In most
situations, the weights are the same for the two methods.
Such polygons are shown in blue on the figure. In some

147

(20)

where
• u is the unknown component to be predicted;
• A¯ is the matrix A without its uth row.
In the case of the 3 × 3 stencil, the basis vectors of the
DCT are the same as the ‘basis vectors’ of our method,
which are the polynomials in du and dv. The Figure 6
shows the correspondence between the DCT basis and the
values of f that generate the same vectors. Therefore, finding
the weights that enable polynomial reproduction—as in our
method—is the same as determining the weights that guarantee the reproduction of the basis vectors of the DCT—as
in the method of Ibarria et al. This shows that for the 3 × 3
neighbourhood, spatial smoothness (small high-order terms

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

148

C. Courbet & C. Hudelot / Taylor Prediction for Mesh Geometry Compression

Figure 5: Polygon prediction weights. The black vertex is being predicted using the geometry of the red vertices. Blue polygons
have the same prediction weights as [IIGS05]. Other colours denote cases where the new weights perform always better (green)
or sometimes better and sometimes worse (red) than [IIGS05].
Table 3: Average prediction error when predicting the missing vertices of polygons. For each polygon of degree K, we computed the error
when 3 to K − 1 vertices were known. The figures have been scaled per model, 1 corresponding to the smallest error. For every model, each
row respectively gives the results of [IIGS05] (I), our method (VM). As the weights differ only for hexa-, hepta- and octagons, we only give the
results for these polygons. (Note: The max Planck model we used is finer than that of [IIGS05], so the numerical results differ.)

hexa
Model
armadillo
guardian
fertility
gipshand
max
Planck

I
VM
I
VM
I
VM
I
VM
I
VM

hepta

octa

v3

v4

v5

v3

v4

v5

v6

v3

v4

v5

v6

v7

630

577
545
8
8
690
652
340
321
116
109

1

1249

810

351

3335

66

43

19

151

1

1522

1025

445

2827

1

1294

694

301

2608

1

1081

789

342

3187

3054
3335
158
151
2677
2827
2994
2608
3156
3187

4923
3939
177
146
3621
3212
6537
1895
4326
3400

2879
2795
107
104
2347
2279
1385
1344
2485
2412

3

1

1266
1173
67
62
1528
1425
1359
1230
1060
1004

9
753
371
126

6
3
9
8

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

C. Courbet & C. Hudelot / Taylor Prediction for Mesh Geometry Compression

149

[AG03] ALLIEZ P., GOTSMAN C.: Recent advances in compression of 3d meshes. In Proceedings of the Symposium
on Multiresolution in Geometric Modeling (2003).
[BCG05] BEN-CHEN M., GOTSMAN C.: On the optimality of
spectral compression of mesh data. ACM Transactions on
Graphics 24, 1 (2005), 60–80.

Figure 6: The nine basis vectors of the DCT used in [ILR07]
and the corresponding values of f .
in taylor expansion) and spectral smoothness (small highfrequency content) are equivalent.
6. Conclusion
In this paper, we have presented an algorithm to determine
prediction weights in a generic way. While remaining very
simple, it can be used to determine the weights of other
more complicated methods based on spectral approaches
([ILR07, ZSS96, IIGS05]). If different assumptions are used,
it can even outperform some of them ([IIGS05]). Because
our scheme does not use the Discrete Fourier Transform like
some other schemes ([IIGS05, ILR07, ZSS96]), it can handle both the symmetric stencils typically used for subdivision
and the asymmetric ones used for mesh compression and thus
be applied to a broader set of problems, from single-rate and
progressive lossless compression to connectivity-oblivious
wavelet schemes. We have applied our method to the Dual
Parallelogram Predictor used by various prediction methods
([CoCI02, SKL03, KPRW05]), and shown that the weights
used by the Freelence algorithm are optimal for meshes that
approximate 2-smooth surfaces. Confronted to experiments,
these weights perform very well, showing that the 2-smooth
assumption can be used in practice. In addition, we have
derived a new set of weights to be used in place of Average
Parallelogram or CPM [PR00] prediction. These new weights
consistently improve compression rates on our test meshes
(9% on average).
Acknowledgements
The authors would like to thank the reviewers for their insightful comments that largely contributed to the improvement of this paper. This work has been supported by French
National Research Agency (ANR) through COSINUS program (project COLLAVIZ #ANR-08-COSI-003) and by the
collaboration SACO with the CEA DAM/DIF.

[CCMW05] CHEN D., CHIANG Y.-J., MEMON N., WU X.: Optimized prediction for geometry compression of triangle
meshes. In DCC ’05: Proceedings of the Data Compression Conference (Snowbird, UT, USA, 2005), IEEE, pp.
83–92.
[CoCI02] COHEN-OR D., COHEN D., IRONY D.: Multi-way geometry encoding. Tech. Rep., Tel Aviv University, 2002.
[DLG90] DYN N., LEVIN D., GREGORY J.: A butterfly subdivision scheme for surface interpolation with tension control.
ACM Transaction on Graphics 9, 2 (1990), 160–169.
[GD02] GANDOIN P.-M., DEVILLERS O.: Progressive lossless compression of arbitrary simplicial complexes. ACM
Transaction on Graphics 21 (2002), 372–379.
[IA02] ISENBURG M., ALLIEZ P.: Compressing polygon mesh
geometry with parallelogram prediction. In VIS ’02: Proceedings of the Conference on Visualization ’02 (Boston,
MA, USA, 2002), IEEE, pp. 141–146.
[IIGS05] ISENBURG M., IVRISSIMTZIS I., GUMHOLD S., SEIDEL
H.-P.: Geometry prediction for high degree polygons.
In Proceedings of the Spring Conference on Computer Graphics (Budmerice, Slovakia, 2005), ACM, pp.
147–152.
[ILR07] IBARRIA L., LINDSTROM P., ROSSIGNAC J.: Spectral
interpolation on 3×3 stencils for prediction and compression. Journal of Computers 2, 8 (2007), Academy Publisher, 53–63.
[KG00a] KARNI Z., GOTSMAN C.: Spectral compression of
mesh geometry. In SIGGRAPH ’00: Proceedings of the
27th Annual Conference on Computer Graphics and Interactive Techniques (New Orleans, LA, USA, 2000).
[KG00b] KRONROD B., GOTSMAN C.: Optimized triangle
mesh compression using prediction trees. In Proceedings
of 8th Pacific Graphics 2000 Conference (Hong Kong,
China, 2000), IEEE, pp. 406–408.

References

[KG01] KARNI Z., GOTSMAN C.: 3d mesh compression using
fixed spectral bases. In Proceedings of Graphics Interface
2001 (Ottawa, ON, Canada, 2001), Canadian Information
Processing Society, pp. 1–8.

[AD01] ALLIEZ P., DESBRUN M.: Valence-driven connectivity
encoding for 3D meshes. Computer Graphics Forum 20,
3 (2001), 480–489.

[KG02] KHODAKOVSKY A., GUSKOV I.: Compression of Normal Meshes. Springer-Verlag, Berlin, 2002. ISBN 978-3540-40116-2.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

150

C. Courbet & C. Hudelot / Taylor Prediction for Mesh Geometry Compression

[KPRW05] K¨ALBERER F., POLTHIER K., REITEBUCH U.,
WARDETZKY M.: Freelence: Coding with free valences.
In Proceedings of Eurographics (Dublin, Ireland, 2005),
Eurographics, pp. 469–478.

ence on Computer Graphics and Interactive Techniques
(New Orleans, LA, USA, 1996), ACM, pp. 189–192.

[KSS00] KHODAKOVSKY A., SCHRO¨ DER P., SWELDENS W.: Progressive geometry compression. In SIGGRAPH ’00: Proceedings of the 27th Annual Conference on Computer
Graphics and Interactive Techniques (New Orleans, LA,
USA, 2000), ACM, pp. 271–278.

Annex: Computing the covariance of partial derivatives
of F

[LDW97] LOUNSBERY M., DEROSE T.-D., WARREN J.: Multiresolution analysis for surfaces of arbitrary topological
type. ACM Transactions on Graphics 18, 1 (1997), 17–32.
[PK05] PENG J., KUO C.-C.: Geometry-guided progressive
lossless 3d mesh coding with octree (ot) decomposition.
ACM Transaction on Graphics 24, 3 (2005), 609–616.
[PKK05] PENG J., KIM C.-S., KUO C.-C.: Technologies for
3D mesh compression: A survey. Journal of Visual Communication and Image Representation 16, 6 (2005), 688–
733.
[PR00] PAJAROLA R., ROSSIGNAC J.: Compressed progressive
meshes. IEEE Transactions on Visualization and Computer Graphics (2000).
[SKL03] SIM J.-Y., KIM C.-S., LEE S.-U.: An efficient 3d
mesh compression technique based on triangle fan structure. Signal Processing. Image Communication 16, 1
(2003), 34–73.
[SS95] SCHRO¨ DER P., SWELDENS W.: Spherical wavelets: Efficiently representing functions on the sphere. In Proceedings of International Conference on Computer Graphics and Interactive Techniques (Los Angeles, CA, USA,
1995), ACM, pp. 161–172.
[TG98] TOUMA C., GOTSMAN C.: Triangle mesh compression.
In Proceedings of Graphics Interface (Vancouver, BC,
Canada, 1998), Canadian Information Processing Society,
pp. 26–34.
[VP04] VALETTE S., PROST R.: Wavelet-based multiresolution analysis of irregular surface meshes. IEEE Transactions on Visualization and Computer Graphics 10, 2
(2004), 123–129.

Determining Taylor prediction weights in the general case
(i.e. when Variance Minimization is used) requires knowing
the value of the variance/covariance matrix of the partial
derivatives of F . In fact, the value of the matrix need only
be determined up to a multiplicative constant, because only
the ratio of its elements appear in the Variance Minimization
process. We choose to scale the matrix so that the top-left
element is 1, by setting:
2
i,j

= Cov

∂ 2F
∂ 2F
, (j −1) (3−j )
(3−i)
∂u
∂v
∂u
∂v
(i−1)

Var

∂ 2F
.
∂u2
(A1)

To compute this value, we consider the geometry of
a mesh as an outcome of a random variable F (u, v) =
(X(u, v), Y (u, v), Z(u, v)). To simplify the demonstration,
we work in the Frenet frame of the mesh (eu , ev , en =
eu × ev ), and we consider only the normal component of
the geometry: N (u, v) = F (u, v).en .
Locally, the geometry can be approximated by a secondorder polynomial N¯ in (u, v):
N¯ a (u, v) = a1 u2 + a2 uv + a3 v 2 + a4 u + a5 v + a6
(A2)
∂2N

∂2N

where (a 1 , a 2 , . . .) will be approximations of ( ∂u2 , ∂u∂v , . . .).
If the mesh is sufficiently densely sampled, then a good approximation of the parameters (a 1 , . . ., a 6 ) can be obtained by
using only the one-ring neighbourhood of a vertex. In the following, we consider a vertex of degree 6 and its neighbours,
numbered as shown on Figure A1 . The best approximation
of N by N¯ in the least squares sense is found by minimizing:
6

¯ k , vk ) − N (uk , vk ) 2 .
N(u

H (a) =

(A3)

k=0

[WSS96] WEINBERGER M. J., SEROUSSI G., SAPIRO G.: Loco-i:
A low complexity, context-based, lossless image compression algorithm. In Proceedings of IEEE Data Compression Conference (Snowbird, UT, USA, 1996), IEEE, pp.
140–149.
[ZSS96] ZORIN D., SCHRO¨ DER P., SWELDENS W.: Interpolating subdivision for meshes with arbitrary topology. In
SIGGRAPH ’96: Proceedings of the 23th Annual Confer-

Figure A1: The numbering of the one-ring used in the proof.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

151

C. Courbet & C. Hudelot / Taylor Prediction for Mesh Geometry Compression

Minimizing H (a) is a linear problem M.a = b with:
⎧
6
⎪
∂ N¯ a
∂ N¯ a
⎪
⎪ Mi,j =
(uk , vk ).
(uk , vk ),
⎪
⎪
⎨
∂ai
∂aj
k=0
⎪
⎪
⎪
⎪
⎪
⎩ bi

6

=
k=0

∂ N¯ a
(uk , vk ).N (uk , vk ).
∂ai

Using the neighbourhood of Figure A1 yields
⎤
⎡
9
3
0
0
0
3
⎥
⎢4
4
⎥
⎢
⎥
⎢
3
⎥
⎢0
0
0
0
0
⎥
⎢
4
⎥
⎢
⎥
⎢3
9
⎢
M=⎢
0
0 0 3⎥
⎥
4
4
⎥
⎢
⎥
⎢
0
0
3 0 0⎥
⎢0
⎥
⎢
⎢0
0
0
0 3 0⎥
⎦
⎣
3
0
3
0 0 7
and therefore
a = M −1 b
⎡

N1 + N4
− N0
2

Table A1: Covariances of partial derivatives measured on our test
meshes.

Model

(A4)

(A5)

⎤

(A6)
The covariance of the partial derivatives of N can be derived
from this expression
⎧
∂ 2N
N1 + N4
⎪
⎪
⎪
− N0
= Var (a1 ) = Var
⎪ Var
2
⎪
2
∂u
⎪
⎪
⎨
= Var (a3 )
(A7)

Thus, the covariance of partial derivatives can be expressed
as linear combinations of terms Var(Ni ) and Cov(Ni , Nj ).
To simplify the problem, we make an additional assumption
on the mesh, that has been introduced and discussed by BenChen and Gotsman [BCG05]: We suppose that the covariance
of two vertices that are not adjacent is 0, or more formally:
/ {i, j }, (i, j ) ∈
/ E = 0.
Cov Ni , Nj |Nk = nk , k ∈

2
1,3

2
1,1

0.00
−0.66
0.00
0.00
−0.01
0.00
0

0.38
−0.02
0.42
0.12
0.36
0.34
0.33

1.27
1.68
1.15
0.57
1.30
1.35
1.33

ances of the positions of adjacent vertices are equal, and we
note σn2 = Cov(Ni , Nj ) for (i, j ) ∈ E.

⎢
⎥
⎢
⎥
√
⎢
⎥
3
⎢
⎥
(N6 − N5 + N3 − N2 )
⎢
⎥
⎢
⎥
3
⎢
⎥
1
⎢ N1 + N4
⎥
⎢−
+ (N6 + N5 + N3 + N2 ) − N0 ⎥
⎢
⎥.
6
3
=⎢
⎥
⎢
⎥
1
N
−
N
1
4
⎢
⎥
+ (N6 − N5 − N3 + N2 )
⎢
⎥
3
6
⎢
⎥
√
⎢
⎥
3
⎢
⎥
⎢
⎥
(−N6 − N5 + N3 + N2 )
⎣
⎦
6
N0

∂ 2N
⎪
Var
⎪
⎪
⎪
∂v 2
⎪
⎪
⎪
⎩
...

armadillo
guardian
gipshand
Max Planck
Neptune (uniform)
guardian (uniform)
our theoretical model

2
1,2

(A8)

Furthermore, because of symmetry, all variances are equal,
and we note σs2 = Var(Ni ). For the same reason, the covari-

This yields
⎧
3 2
2
⎪
⎪
⎪ Var(a1 , a1 ) = Var(a3 , a3 ) = 2 σs − 2σn
⎪
⎪
⎪
⎪
⎪
⎨ Var(a , a ) = 4 σ 2 − σ 2
2
2
n
3 s
⎪
⎪
5
4
⎪
⎪
Cov(a1 , a3 ) = σs2 − σn2
⎪
⎪
⎪
6
3
⎪
⎩
Cov(a1 , a2 ) = Cov(a2 , a3 ) = 0

(A9)

On the other hand, because the value of the derivatives
of N should not be dependent on a global translation of
all the vertices (uk , vk ), we can choose a 6 = 0, that is
Cov(N 0 , Ni ) = 0 ∀i ∈ [0, 6]. This yields
⎧
1
⎪
Var(a1 , a1 ) = Var(a3 , a3 ) = σs2
⎪
⎪
⎪
2
⎪
⎪
⎪
⎪
⎨ Var(a , a ) = 4 σ 2 − σ 2
2
2
n
3 s
(A10)
⎪
⎪
1 2 2 2
⎪
⎪
σ
σ
,
a
)
=
−
+
Cov(a
⎪
1
3
⎪
⎪
6 s
3 n
⎪
⎩
Cov(a1 , a2 ) = Cov(a2 , a3 ) = 0
Combining Equations (A9) and (A10), we can deduce that
σs2 = 2σn2 . Putting back this value in (A9) or (A10) shows
that the normalized variance/covariance matrix of the partial
derivatives is
⎤
⎡
1
1
0
⎢
3⎥
⎥
⎢
⎥
⎢
4
2
(A11)
=⎢
0 ⎥
⎥.
⎢0
3
⎥
⎢
⎦
⎣1
0
1
3
We confronted these theoretical values with experiments.
Table A1 gives the measured covariances on our set of test
meshes. We can see that the results agree very well with the
theoretical results, except for the guardian and Max Planck
Models. We cannot currently explain why our model fails
on these meshes, however we suspect that this may come
from the fact that they have a high number of very distorted
triangles.

c 2011 The Authors
Computer Graphics Forum c 2011 The Eurographics Association and Blackwell Publishing Ltd.

