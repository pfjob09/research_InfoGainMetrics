DOI: 10.1111/j.1467-8659.2010.01785.x

COMPUTER GRAPHICS

forum

Volume 30 (2011), number 1 pp. 27–42

Hermite Radial Basis Functions Implicits
I. Macˆedo1 , J. P. Gois2 and L. Velho1
1 Vision

and Graphics Laboratory, Instituto Nacional de Matem´atica Pura e Aplicada, Brazil
{ijamj, lvelho}@impa.br
2 Centro de Matem´
atica, Computac¸a˜ o e Cognic¸a˜ o, Universidade Federal do ABC, Brazil
joao.gois@ufabc.edu.br

Abstract
The Hermite radial basis functions (HRBF) implicits reconstruct an implicit function which interpolates or
approximates scattered multivariate Hermite data (i.e. unstructured points and their corresponding normals).
Experiments suggest that HRBF implicits allow the reconstruction of surfaces rich in details and behave better
than previous related methods under coarse and/or non-uniform samplings, even in the presence of close sheets.
HRBF implicits theory unifies a recently introduced class of surface reconstruction methods based on radial basis
functions (RBF), which incorporate normals directly in their problem formulation. Such class has the advantage of
not depending on manufactured offset-points to ensure existence of a non-trivial implicit surface RBF interpolant.
In fact, we show that HRBF implicits constitute a particular case of Hermite–Birkhoff interpolation with radial
basis functions, whose main results we present here. This framework not only allows us to show connections
between the present method and others but also enable us to enhance the flexibility of our method by ensuring
well-posedness of an interesting combined interpolation/regularization approach.
Keywords: implicit surfaces, Hermite data, radial basis functions, Hermite-Birkhoff interpolation, scattered data
approximation, geometric modelling, surface reconstruction
ACM CCS: G.1.2 [Numerical Analysis]: Approximation—Approximation of surfaces and contours; I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling—Curve, surface, solid, and object representations

independently points and normals and also (v) reproduces
polynomial-based surfaces.

1. Introduction
Surface reconstruction methods based on radial basis functions (RBFs) [CBC∗ 01, PMW09, WSC06] can
be successfully used in several applications involving
interpolation or approximation of scattered data, for example in geometric modelling [TO02], tracking of timedependent surfaces [GNNB08, GB10], interpolation of
polygon soups [SOS04] and hole-filling of incomplete
meshes [CBC∗ 01].

On the theoretical side, HRBF implicits framework unifies
recently introduced surface reconstruction methods based on
RBF which incorporate normals directly in their problem
formulation (see [PMW09, WSC06]).
1.1. Related work
Although there is a vast literature on surface reconstruction
from point clouds, those works based on projection operators [ABCO∗ 03, FCOS05] and implicit surfaces [GPJE∗ 08,
CBC∗ 01, GG07, TO02, AA09] have gained especial attention. Part of this attention is to implicit methods based on
Radial Basis Function [CBC∗ 01, TO02], as long as they are
capable to satisfactorily handle sparse point clouds. However,

Here, we present Hermite radial basis functions implicits
(HRBF implicits): an interpolant to first-order Hermite data
based on radial basis functions and polynomials that (i) is
robust with respect to coarse and non-uniformly sampled
data, (ii) deals effectively with close surface sheets (iii) is able
to produce detailed surface reconstructions, (iv) regularize

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics
Association and Blackwell Publishing Ltd. Published by
Blackwell Publishing, 9600 Garsington Road, Oxford OX4
2DQ, UK and 350 Main Street, Malden, MA 02148, USA.

27

28

I. Macˆedo et al. / HRBF Implicits

to achieve a non-trivial interpolant, the first RBF methods require the definition of hand-tuned offset-points. For instance,
Carr et al. [CBC∗ 01] prescribed the offset-points as the
following procedure: for each input point xi with normal
ni , create two offset points at xi ± εni , with associated offset
scalars ±ε.
In fact, a single optimal choice for ε does not exist in
general, and it is not hard to see that such approaches typically do not interpolate the given normals. Furthermore, the
requirement of those offset points is not natural and errorprone, especially when there are close sheets and the samples
distribution is irregular.
The offset requirement was avoided in posterior works
deduced from a statistical-learning perspective [PMW09,
WSC06], where normals were directly used in the variational
problem. Pan et al. [PMW09] incorporate normals directly in
their regularized variational problem, where the reasoning is
to consider the alignment of the gradient of the surface with
the normals at the sample points. This amounts to solving an
N × N linear system for N point/normal pairs. However, we
observed that this approach is quite sensitive to non-uniform
point distributions and does not ensure interpolated normals.
Another important property desirable for a surface reconstruction method is its capability to handle close sheets.
Many approaches have been proposed which employ combinations of spatial data structures, geometric heuristics
(e.g. normal clustering) or even statistical inference to
select points belonging to the same sheet before evaluating the implicit function [GPJE∗ 08, FCOS05]. Other
approaches (similarly to HRBF Implicits) build into the interpolation/approximation scheme a capability of identifying
and handling close sheets without the need of additional information [GNNB08, CBC∗ 01, GG07, MYC∗ 01].
Finally, interpolation from Hermite data is a frequent requirement in geometric modelling. Beyond the RBF-based
methods (e.g. [WSC06] and the present approach) Alexa
and Adamson [AA09] proposed a method entirely based on
moving-least-squares (MLS) approximation [Wen05].

1.2. Properties and contributions of HRBF implicits
HRBF implicits are a special case of a generalized interpolation theory—Hermite–Birkhoff interpolation with RBFs
[Wen05]—so that new variants of surface reconstruction
methods can be designed for additional flexibility. Further, exploiting the theoretical results, we show that previous methods [PMW09, WSC06], originally deduced from
a statistical-learning perspective, can also be studied and improved by the theoretical framework we present.
Considering those approaches under this unifying framework, we gain many theoretical and computational insights
to improve the methods. For instance, considering the work

of Walder et al. [WSC06], we were able to enhance the flexibility of both schemes by ensuring well posedness of an
interesting combined interpolation/regularization approach.
Similar insights come up by comparing our derivations and
results with those from a recent variationally deduced approach [PMW09], allowing a better understanding of the
method in that work.
Given the advantages of the theoretical framework of
HRBF implicits, we conclude this section presenting its practical properties and contributions.
1.2.1. Ability to handle non-uniform samplings
HRBF implicits compute desirable reconstructions even in
the presence of very non-uniform data distributions. Systematic comparisons with previous methods [PMW09, CBC∗ 01]
suggest its effectiveness.
1.2.2. Ability to handle close sheets
Our results suggest that HRBF implicits perform better than
previous work [PMW09, CBC∗ 01, GNNB08] when reconstructing surfaces with close sheets even in the presence of
very irregular and coarse data sets.
1.2.3. Approximability
When interpolation is not desirable, approximation is feasible by choosing two regularization parameters for HRBF
implicits—one for points and another for normals. This allows the user to decide the relative importance of points and
normals in the fitting process.
1.2.4. Space augmentation and polynomial reproduction
Compactly supported implicit functions can be problematic
for many basic operations in graphics, for example computing projections [ABCO∗ 03], ray-tracing [PR10, SN09] and
isosurface extraction [SSS06, DSS∗ 09]. To reduce that issue
for these purposes, achieving a way to enforce reproduction
of polynomial functions as a byproduct, we present a way to
enrich the HRBF Implicit interpolation space with predefined
(finite-dimensional) linear function spaces.
1.2.5. Simple implementation
Our formulation and subsequent treatment is built upon theoretical results from scattered data approximation theory
[Wen05] and concepts from functional analysis [BN00], yet
it leads to a simple matrix-based algorithm that is a direct
translation of the mathematical results. By exploiting existing
linear algebra packages, this allows a simple computational
implementation, general enough to be independent of the
ambient space dimension.

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

29

I. Macˆedo et al. / HRBF Implicits

ment functionals, that is f ∗ =
αi v i , where each αi ∈ R
and the v i are such that λi (f ) = v i , f H for each f ∈ H.
Together with the interpolation conditions λi (f ∗ ) = ci , this
characterization allows determining the coefficients in the
sum by computing a solution of a system of linear equations Aα = c, where (A)ij = λi (v j ) = v i , v j H . Since A is
an inner-product (Gramm) matrix, it is symmetric and positive semi-definite for any set of measurement functionals in
H∗ . Moreover, under the assumption of linear independence
of the functionals λi , the interpolation matrix is positive definite, ensuring uniqueness of α ∈ RL for any arbitrary data
vector c ∈ RL . This is an important conclusion, because it
means that all we need to have a minimum-norm generalized
interpolant for a data set whose measurement functionals are
linearly independent is to be able to manipulate the representers v i and to take inner-products among them.
L
i=1

2. Theoretical Framework
For the purpose of fitting an implicitly defined hypersurn
n−1
, we
face to given Hermite data {(xi , ni )}N
i=1 ⊂ R × S
n
are going to look for a function f : R → R satisfying
both f (xi ) = 0 and ∇f (xi ) = ni for each i = 1, . . . , N . Our
search will take place in a suitable subspace H of the linear space C 1 (Rn ) of continuously differentiable scalar-valued
functions in Rn . This problem is classified in approximation
theory as an instance of (multivariate) first-order Hermite interpolation, which is a particular case of Hermite–Birkhoff
interpolation.
In this section, we present some results from a general theoretical framework of Hermite–Birkhoff interpolation with
radial basis functions. We specialize those results to the firstorder Hermite interpolation problem and show how to augment the derived interpolant with polynomial terms in order
to achieve reproduction of polynomials and suitable globally supported interpolants. Following that development, we
show how to incorporate regularization in the process and
discuss connections with two related approaches to implicit
surface reconstruction from points and normals.
Our presentation is strongly influenced by Wendland’s
wonderful monograph [Wen05] and, at least for this theoretical reasoning, we believe the reader would benefit from
some basic knowledge of functional analysis [BN00].

In Hermite–Birkhoff interpolation, other measurement
functionals λi are linear combinations of simple functionals of the form δx ◦ D γ , where δx (f ) := f (x) is the evaluation functional at the point x ∈ Rn and D γ is a differentiation operator, in which γ ∈ (N ∪ {0})n indicates how
many times (γj ) it differentiates the function with respect to
the jth-variable xj . For simplicity of exposition, we will asi
i
sume that λi = δxi ◦ D γ , therefore λi (f ) = (D γ f )(xi ). In
words, applying the measurement functional λi to a sufficiently smooth function f : Rn → R corresponds to differentiating f according to γ i and evaluating the result at the
point xi ∈ Rn .

2.1. Hermite–Birkhoff interpolation with RBFs
Hermite–Birkhoff interpolation is a generalized interpolation
problem in which differentials of the function sought have
values prescribed at given points, for example f (x) = c0x ,
∂f
∂2f
z
(y) = czy and ∂x∂y
(z) = cx,y
. Unlike in Hermite interpola∂z
tion, in the Hermite–Birkhoff problem, one does not need
to prescribe values for all the function’s derivatives up to
a certain order, for example one might only prescribe firstorder derivatives without providing actual function values
[KEHK08]. This provides enormous flexibility for approaching a variety of important problems, for example designing
collocation methods for the numerical solution of partial differential equations [Fas97].
Let us begin with a real Hilbert space H endowed with the
inner-product ·, · : H × H → R and its dual space H∗ , that
is the space of continuous linear functionals λ : H → R. Assume we are given a data set {(λi , ci )}Li=1 ⊂ H∗ × R of measurement functionals λi ∈ H∗ and data values ci ∈ R. We call
f ∈ H a generalized interpolant of that data set whenever
λi (f ) = ci for each data pair. It is not unusual for the set of
generalized interpolants of a given data set to be a non-trivial
closed affine subspace of H, for that reason, in this Hilbert
space setting, it is natural to choose the unique minimum
H-norm generalized interpolant f ∗ ∈ H. An important theorem characterizes this norm-optimal interpolant as a linear
combination of the Riesz representers v i ∈ H of the measure-

From the general result we presented for abstract real
Hilbert spaces, all we need now is a construction of H suitable for Hermite–Birkhoff interpolation, that is one in which
the Hermite–Birkhoff functionals are continuous and linearly
independent under reasonable conditions and in which their
Riesz representers are easily computable as well as innerproducts among them. As a matter of fact, it can be shown
that some spaces with such properties are naturally induced
by certain classes of functions. Formally, let k ∈ N ∪ {0}, associated to every positive definite radial basis function φ :
R+ → R such that ψ := φ( · ) ∈ C 2k (Rn ) ∩ L1 (Rn ) there
is a native Hilbert space Hφ ⊂ C k (Rn ) in which δx ◦ D γ is
continuous for every x ∈ Rn and |γ | := ni=1 γi ≤ k. Moreover, its Riesz representer v ∈ Hφ has the simple form
v = (−1)|γ | (D γ ψ)(· − x) and, as long as the measurement
functionals λi are pairwise distinct, they are linearly independent and the inner-products among their representers are
j
i
j
given by v i , v j Hφ = (−1)|γ | (D γ +γ ψ)(xi − xj ). Notable
examples of such PD-RBFs are the Gaussians and the compactly supported Wendland’s functions [Wen95].

2.2. First-order Hermite interpolation with RBFs
As a particular case of Hermite–Birkhoff interpolation with
RBFs, we now specialize the results presented earlier to
the problem of first-order Hermite interpolation of scattered

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

30

I. Macˆedo et al. / HRBF Implicits

multivariate data. In this problem, the data consists of prescribed function fi and gradient values gi at each point
xi , that is for each sample point, the data comes in 1 + n
flavours: (δxi , fi ), (δxi ◦ ∂x∂ 1 , (gi )1 ), . . . , (δxi ◦ ∂x∂ n , (gi )n ).
Therefore, from our previous theoretical considerations, the
minimum Hφ -norm interpolant to this data in a Hilbert space
Hφ ⊂ C 1 (Rn ) induced by a PD-RBF φ has the concrete form,

L

m

f∗ =

αi v i +
i=1

N

f ∗ (x) =

with a suitable finite-dimensional function space [Wen05],
for example d (Rn ) the space of polynomials in Rn whose
degree is at most d. Formally, let P ⊂ Li=1 dom(λi ) be
finite-dimensional and {p1 , . . . , pm } be a basis for P, our
augmented generalized interpolant will have the form

{αi ψ(x − xi ) − β i , ∇ψ(x − xi )

Rn },

(1)

i=1

where both scalar-coefficients αi ∈ R and vector-coefficients
β i ∈ Rn are uniquely determined by the interpolation
constraints f ∗ (xi ) = fi and ∇f ∗ (xi ) = gi , for each i =
1, . . . , N , that is
N

{αj ψ(xi − xj ) − β j , ∇ψ(xi − xj )

Rn }

= fi ,

(2)

j =1

γk pk ,

(4)

k=1

where as long as p ∈ P and λi (p) = 0 for each 1 ≤ i ≤ L
imply that p = 0, the coefficients αi and γk of the augmented
generalized interpolant can be uniquelly determined by the
interpolation conditions and the condition Li=1 αi λi (pk ) =
0 for each 1 ≤ k ≤ m. In words, if the augmenting linear
space P is finite-dimensional and all measurement functionals λi are defined on all of its elements and the only function
p in P which is annihilated by every measurement is the constant zero, then the augmented generalized interpolant can be
recovered by solving a (symmetric indefinite) linear system.

N

{αj ∇ψ(xi − xj ) − Hψ(xi − xj )β j } = gi ,

(3)

j =1
2

where H is the Hessian operator defined by (H)ij = ∂x∂i ∂xj . In
these equations, the requirement that ψ be at least twice as
much continuously differentiable as the maximum order of
the operators in the measurement functions is more visible.
Therefore, for first-order Hermite interpolation, we need to
choose a basis function φ inducing a ψ which is at least C 2 ,
resulting in an optimal interpolant at least C 1 .

It is noteworthy that, if the measurement functionals and
the augmenting space obey the hypothesis above and the
data values are actually generated from probing an element
p ∈ P, it will hold that f ∗ = p, that is the augmented generalized interpolation process reproduces elements from P.
Although we motivated augmentation by another argument,
this reproduction property is important per se because it provides a way to enforce invariants in the interpolation process.

2.4. On two regularization-based approaches
In the context of surface reconstruction from points and
normals as we posed the problem, we call the optimal interpolant in (1), a hermite radial basis function implicit or an
HRBF implicit for short. More generally, this denomination
will also be used to denote the version augmented by polynomials introduced in the following. From that expression and
our previous considerations, it is clear that an HRBF implicit
is a function at least continuously differentiable representing
an implicit hypersurface at least C 1 , naturally, as long as zero
is a regular value of that function, property we do not know
yet under which conditions can be guaranteed.

2.3. Space augmentation and polynomial reproduction
For certain tasks, it is interesting that the interpolant be globally supported without requiring the use of a globally supported basis function φ. A concrete example is the projection
of points onto a certain level-set of the implicit function, an
operation which is often implemented with iterative methods that only use local information. When the interpolant
has compact support, away from that support, local methods
‘perceive’ the function as being zero everywhere.
A simple way to encourage globally supported interpolants
while still relying on compactly supported basis functions is
by augmenting the space in which the interpolant is sought

The interpolation theoretic framework we presented so far
allows us to recover an implicit surface from exact multivariate scattered Hermite data. However, in some important
applications, the data is corrupted by noise motivating an approximative fitting scheme. In such a setting, the traditional
approach consists of minimizing the sum of a loss and a
regularization functional, where the loss functional penalises
deviation from the input data while the regularizer encourages low complexity solutions. In a Hilbert space setting, the
squared H-norm provides a natural regularizer (as we have
already seen when we used it to solve an uniqueness issue)
and is most often used along with the least squares loss.
In the following, we discuss some connections with two
regularization-based approaches to the implicit surface reconstruction problem from Hermite data. They were derived
from other, albeit very related, theoretical frameworks and
share the same operational form for the implicit function as
that from our derivations prior to augmentation (1).

2.4.1. Connections to a regularized regression scheme
Simply stated, our interpolatory approach can be posed as
min

λi (f )=ci

f

H

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

31

I. Macˆedo et al. / HRBF Implicits

while building upon results from statistical learning theory,
Walder et al. presented in [WSC06] a regularized regression
approach which can be, a bit more generally here, stated as
L

min
f ∈H

f

2
H

+

ρi · (λi (f ) − ci )2 ,

(5)

i=1

where the ρi > 0 are penalization parameters controlling fit.

2.4.2. On a regularized variational method
Another regularization-based approach for Hermite surface reconstruction was recently proposed by Pan et al. in
[PMW09] derived from a variational construction in which
the loss functional is a combination of least squares for function values and a linear term for gradients

min f
f ∈H

It turns out that (5) also admits a unique solution as a
linear combination of the Riesz representers of the measurement functionals. By restricting that optimization problem
to this finite-dimensional subspace, it can be shown that the
coefficients for the regularized solution can be computed by
solving a system of the form (A + D−1 )α = c, where A and c
are as in our previous derivations and D = diag(ρ1 , . . . , ρL ).
Our previous theoretical considerations shed some insight
on this method by ensuring well posedness of this regularization method even in the limiting interpolatory case, where
each ρi → +∞ and D−1 → 0. Moreover, the results we presented even allow for just a subset of the ρi to be taken in the
interpolation limit, since the corresponding diagonal terms
in the regularization matrix are zeroed while the others remain positive resulting in a positive semi-definite regularizer,
which still ensures positive definitess of the linear system.
This insight opens up the possibility to a combined interpolation/regularization method in which only a subset of the
data is modelled as hard-constraints while the remaining are
treated as soft-constraints. In the context of Hermite surface
reconstruction, this is interesting for example when the normals come from an inexact estimation procedure while the
points are considered to be exactly on the surface.
Another interesting insight derived from the effort of
Walder et al. regards the thin-plate energy. Because the
minimizer of the thin-plate energy derived by Duchon in
[Duc77] has global support (thus not easily adapted to many
medium- to large-scale problems), Walder et al. proposed in
their work [WSC06] to regularize a least squares problem
with the thin-plate energy and restrict the optimization to a
certain finite-dimensional subspace. This amounts to the solution of a system as sparse as the original and thus amenable
to large-scale problems. Although we have not yet pursued
this idea further, we believe it might be interesting to further
regularize the functional in (5) with the thin-plate energy
and restrict the minimization to V := span{v 1 , . . . , v L }, this
results in a system of the form (A + D−1 + T)α = c, where
T is a Gramm matrix induced by the representers and the
thin-plane semi-inner-product (thus symmetric and positive
semi-definite) and everything else as before. Even more interesting might be to consider completely forgetting about
the matrix D−1 and just regularize the interpolation system
with the thin-plate matrix, because A is already positive definite, the resulting linear system would still be symmetric
positive definite and as sparse as the interpolation system.

2
H

+

C1
N

N

(f (xi ) − fi )2 −
i=1

C2
N

N

ni , ∇f (xi )

Rn

i=1

where C1 , C2 > 0 are penalization parameters. In our framework, this variational problem can be put as
⎧
⎨
min
f ∈H

⎩

L

f

2
H

+

M

ρi · (λi (f ) − ci )2 − 2
j =1

i=1

⎫
⎬
μj (f ) ,
⎭
(6)

where the μj ∈ H∗ have representers uj ∈ H. It can be
shown that the solution f ∗ ∈ H for this problem has the
form
L

f∗ =

M

αi v i +
i=1

uj ,

(7)

j =1

where the coefficients are the solution of (A + D−1 )α =
i
j
c − d and (d)i = M
j =1 v , u H . Thus, the same considerations we made before regarding the interpolation limit for
the work of Walder et al. also hold for the work of Pan et al.
So, it is possible to interpolate points on the surface while
the normals can only be approximated. This may introduce
complications regarding the magnitude of gradients in the
recovered implicit function, especially when the sampling is
relatively dense (resulting in very small timesteps in rootfinding algorithms, used for ray-tracing implicits) and distortions
on the fitted surface when the sampling is far from uniform.

3. Computational Aspects
In the past section, our theoretical considerations led us to
a concrete form for an augmented Hermite–Birkhoff interpolant to given linear measurements. In the following, we
discuss some of the computational aspects related to numerically assembling and solving the first-order Hermite interpolation/approximation system in our treatment of the problem
of reconstructing hypersurfaces from Hermite data.

3.1. Assembling the interpolation/approximation system
Prior to the direct solution of the augmented interpolation
system, we need to assemble the corresponding matrix. To
this end, notice that the augmented interpolation conditions

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

32

I. Macˆedo et al. / HRBF Implicits

can be written as, for each 1 ≤ i ≤ N and 1 ≤ k ≤ m,
⎤
⎡
N
ψ(xi − xj )
−∇ψ(xi − xj )
αj
⎦
⎣
βj
∇ψ(xi − xj ) −Hψ(xi − xj )
j =1
m

+

γl

pl (xi )

=

hood covered by the basis functions. This issue is well known
in methods which use compactly supported kernels (e.g. RBF
and moving least squares). Although we do not elaborate on
this problem in this work, in Section 5, we discuss how we intend to build upon previous methods developed for classical
interpolation problems with RBFs [Wen05].

0

ni
∇pl (x )
⎡ ⎤
αj
∇pk (xj )T ⎣ j ⎦ = 0,
β
i

l=1

N

pk (xj )
j =1

3.2. Solving the interpolation/approximation system

with each block in the first sum as a (1 + n) × (1 + n) matrix.
By suitable grouping, we can define the partitioned system
A
P

T

P

α

0

γ

=

c
0

,

(8)

where A ∈ RN(1+n)×N(1+n) , P ∈ RN(1+n)×m and α and c are
α
0
N (1 + n) vectors whose ith blocks are [ ii ] and [ i ]. Thereβ
n
fore, the (n + 1)N × (n + 1)N RBF-submatrix can be assembled one block at a time, corresponding to a pair (i, j )
of samples. All we need to compute such a block is to know
how to evaluate ψ, its gradient and its Hessian on a point.
We incorporate regularization by setting two parameters
ηv , ηg ≥ 0 corresponding to penalizations of 1/ηv on the
function’s values and 1/ηg on its gradients. Clearly, setting
either one to zero corresponds to an interpolation constraint.
Incorporating regularization in the above system amounts to
adding, to each diagonal block of A, the diagonal matrix
diag(ηv , ηg e), where e ∈ Rn is the vector of ones.
For fixed n and m, we notice that the cost O(N 2 ) of assembling such a system with globally supported RBFs is
prohibitive for some applications. Indeed, our first implementation used dense matrix packages and this limited our
experiments to about 8K points in R2 and 6K points in R3
on a commodity laptop with 2 GB of RAM.
We have addressed this problem by using compactly
supported RBFs and employing sparse matrix packages.
In our implementation, we used Wendland’s φ3,1 function
which defines a family ψr (x) := ψ( xr ) indexed by a radius
r > 0, where each ψr is a C 2 positive definite function, for
n ≤ 3, ψ(x) := φ3,1 ( x ) and φ3,1 (t) = (1 − t)4 (4t + 1), for
t ∈ [0, 1], and φ3,1 (t) = 0, otherwise [Wen95]. That is all one
needs to compute formulae for the gradient and Hessian of
the ψr and, consequently, assemble the interpolation matrix.
Nonetheless, even with compactly supported basis and efficient numerical methods, the assembling procedure can become the bottleneck when a very large number of samples are
used. These cases demand a data structure for accelerating
the range queries and retrieving only those pairs for which the
blocks were non-zero, effectively building a neighbourhoodgraph. However, this introduces another problem, the tradeoff between system sparsity and the width of the neighbour-

It is well known and reported elsewhere that RBF interpolation systems suffer from numerical conditioning problems
when the sampling is large and too dense. Many methods for
improving the condition numbers have been developed to circumvent this issue and allow faster convergence of iterative
methods for the sparse systems [BCM99, Wen05].
With this in mind, we chose a simpler implementation for
our first prototypes and decided to employ direct methods
tailored for sparse systems. Because our analysis ensures
that, in the interpolation system, the RBF-submatrix A is
a large sparse symmetric and positive definite matrix, we
employ a sparse Cholesky factorization as implemented in the
CHOLMOD solver [CDHR08] and the efficient codes in the
BLAS and LAPACK linear algebra packages as implemented
by the ATLAS project [ATL10] in our core mathematical
routines for evaluating gradients and Hessians.
Although the partitioned augmented interpolation system
is symmetric indefinite, because it is equivalent to a linearly
constrained quadratic programming problem, we can exploit
the fact that almost always m
N and solve that one indefinite system in a couple steps involving only the symmetric
positive definite matrices A and (PT A−1 P) ∈ Rm×m ,
γ = (PT A−1 P)−1 PA−1 c
α = A−1 c − (A−1 P)γ
thus we solve m + 1 systems involving A and one system involving PT A−1 P. Because solving for a couple of right-hand
sides is far less expensive than the actual factorization of A,
the added computational cost of augmentation is negligible.
Using these numerical packages along with the C++ language, we were able to develop a code completely independent of dimension, including the range-query acceleration
data structure, which we chose to be a kd-tree. This prototype allowed us to solve Hermite interpolation problems
with up to 500K samples in R3 with a suitably chosen sparsity structure, limited by the memory requirements of the
direct solver.
4. Results
Because normals are also interpolated, in Figure 1 we show
examples of how the normals affect the results. In Figure 1(a)
we observe a zero-contour which does not interpolate any
point due to the normal orientations, the appearance of such

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

I. Macˆedo et al. / HRBF Implicits

33

Figure 1: Examples of how normals can affect results (computed without polynomial augmentation): (a) inconsistent orientation
causes an additional zero-level. This can be avoided by flipping the outer normals (b). In (c), arbitrarily chosen normals at
non-differentiable regions (corner of the mouth) can cause oscillatory results.
It is worth mentioning that we also experienced spurious zero levels, when augmenting the space by 2 (R3 ) and
adopting support radii covering a small number of points (the
neighbours to a given sample). In that case, it may be more
useful to opt for lower-degree terms or larger radii.
Figure 2: Varying the radius in ψr with no augmentation
(r = 0.5, 1, 1.5, 2 and 4 ): six antipodal points on the sphere.
a zero-contour can be explained by the Intermediate Value
Theorem. That issue is avoided when normals are accordingly oriented [Figure 1(b)]. Notice that oscillations occur in
the corner of the mouth in Figure 1(c). In that case, because
we set arbitrary normals in at non-differentiable corners, it is
expected such oscillations as HRBF implicits are at least C 1
and need to satisfy all constraints of normals and points.
In Figure 2(a), as we consider a radius smaller than the
distance between two points, HRBF implicits consists of six
disjoint discs. It can be noticed in the following images that,
the larger the radius, the smoother (or closer to a sphere) the
HRBF implicits turn out.
Figure 3 illustrates space augmentation by polynomial
terms. From (b) to (d) augmentations are with d (R3 ), d =
0, 1 and 2, respectively. In (a) space is not augmented. For
each example, the images depict a fitted HRBF Implicit being cut by a slice, the zero-level contour on the slice and the
gradient magnitude variation on that slice. In (a), one notices
the presence of isosurfacing artefacts (in light blue regions),
because outside the interior of the RBFs the function is zero,
and which is straightforwardly avoided by augmenting the
interpolant with a constant term, as in (b). In that case, all
the domain, except for the zero-level interpolant, is free of
zero values. However, when verifying the gradient magnitude, one can notice that far from the reconstruction, there
exists zero-valued gradients. For several applications which
depend on a gradient estimate, constant polynomials do not
suffice. Our experience indicates that a quadratic term is a
good enough choice for many common operations in graphics which depend on evaluating gradients.

In practice, for the Kitten data set, where the average number of points inside the support was about 14, we opted for
using 0 (R3 ). Tackling this trade-off between the size of the
support radius, the augmentation basis and the computational
costs is a topic we discuss later for the future.
We also illustrate the behaviour of the (Hermite) Regularized variational scheme proposed by Pan et al. in [PMW09]
(Figure 4). In that example, we considered a radius large
enough to enclose all samples. One can notice the flatness
of the reconstructed function near the zero set, where the
modulus of the function is at most 0.005. Figure 5 presents in
higher resolution the head of the Homer from that reconstruction. One notice that the regularized variational reconstruction is slightly smoother than HRBF implicits. We believe
that the reasons for such smoothness in the reconstruction
are the flatness of the function and the absence of normal
interpolation.
One important property is the capability of dealing with irregular point distributions and close sheets. In particular, this
quality is especially important for tracking time-dependent
surfaces where samples are evolved freely along a vectorfield [GNNB08]. To assess the ability of HRBF implicits
when dealing with these situations, we elaborated an experiment which considers points distributed along two orthogonally interlaced tori, with inner radius 1 and outer radius
2.25, distanced 0.25 from each other. Each one of those tori
has one of the following four numbers of (uniformly spaced)
points: 32, 128, 512 and 2048. We perform two distinct experiments: one considering both tori with the same sampling
density (Figure 6), and another considering each torus with
one of the four distinct densities (Figures 7–9).
For comparison, we ran tests for the regularized variational
method in [PMW09] and FastRBF, described in [CBC∗ 01].

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

34

I. Macˆedo et al. / HRBF Implicits

Figure 3: HRBF implicits augmentation: (a) no augmentation; (b–d) augmenting with

d (R

3

), d = 0, 1 and 2, respectively.

Figure 4: Behaviour of the regularized variational scheme
(support radii enclosing all points) [PMW09].

For both HRBF implicits and regularized variational method
we used the same support radius, chosen under the following
constraint: the smallest possible support radius which produces a reasonable reconstruction for the regularized variational scheme. The support radii thus chosen were 8, 4, 2
and 1.5 whenever a torus with 32, 128, 512 and 2048 samples, respectively, was present in the test. For FastRBF, offset
points were created as described in Introduction, assuming
two possible values for ε: 0.1 and 0.01. We emphasize that
those choices were hand-picked after several trial-and-error
brute-force tests in order to take the best results from the
approaches with which we compare ours.

Figure 5: Homer (head) fitted using the largest radius for
each method: HRBF implicits’ interpolant enhances details.

When both tori are the same, all three techniques produce
similar results (Figure 6), especially on denser cases. However, the difference among the techniques is salient when the
tori have distinct sampling densities (Figures 7–9). In those
cases, one can observe the importance of normal interpolation. In comparison with the regularized variational method,
for instance, in Figure 8, whenever there is a 32-samples

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

I. Macˆedo et al. / HRBF Implicits

35

Figure 6: Close sheets test (regularly distributed points): all methods present similar behaviours when points are regularly
distributed. The more refined the set of samples, the more accurate the reconstructions become.
torus, there are normals far from being orthogonal to the
surface, which does not happen with HRBF implicits.
We assess trade-offs between memory-footprint/support
radius and processing time of HRBF implicits and regularized variational method (Tables 1 and 2). To that end,
we first assume that the larger support radius is, the better the reconstruction becomes. From this premise, we seek,
for each tested dataset, the largest support radius each tech-

nique tolerates while requiring around 2.5 GB RAM memory (running on a commodity laptop with 4 GB of RAM).
It must be observed that, for the regularized variational
scheme, it is possible to choose larger radii than HRBF implicits, because it requires solving a linear system whose
order is (1 + n) times smaller than HRBF implicits’. For
enriching our comparison, we also ran the Hermite variational method with the same radii employed by our HRBF
implicits.

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

36

I. Macˆedo et al. / HRBF Implicits

Figure 7: Close sheets test (non-uniformly distributed points). HRBF implicits: even for coarse non-uniformly sampled datasets,
we are able to achieve geometrically stable results while interpolating both points and normals.

Figure 8: Close sheets test (non-uniformly distributed points). Regularized variational approach: good results are only attained
under almost uniform sampling. Moreover, we observe that, in the coarsest torus, normals are not well approximated.
Table 1 shows along its columns the data set names, the
number of points of each data set, the order n of matrix A, the
number of stored non-zeros elements of A (nnz(A)), a sparsity ratio (from 0, which represents a null matrix, to about
0.5, which represents a full symmetric matrix, since A is
symmetric, only its lower-triangle is actually allocated), an
estimate of the peak-memory used and the average number
of samples inside the support for each radial function (neighbours). One can observe that the number of neighbours is
proportional to the density of the system. Notice that, for
the Homer dataset, the Hermite variational scheme requires

a low memory footprint even when using a global radius
(approximately 750 MB).
Table 2 presents the most time-consuming steps: the construction of the neighbourhood-graph, the construction, symbolic analysis and numerical factorization of the matrix A
and the forward/backward substitutions. As shown in that table, in most cases, the Hermite variational method is slower
than HRBF implicits for the largest radius picked, a consequence of the matrix density. In fact, we can observe that the
system density dictates more the processing time than the

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

I. Macˆedo et al. / HRBF Implicits

37

Figure 9: Close sheets test (non-uniformly distributed points). FastRBF has difficulties on all datasets in which one of the tori
is the coarsest. One also can notice how FastRBF is sensitive to the choice of the offset ε (the distance between the tori is 0.25).

Figure 10: HRBF implicits and regularized variational present similar results.
system size, when sparsity is exploited in the linear system
solver.
That experiment poses a natural question regarding the
surface reconstruction quality of methods with respect to
their support radii. We present reconstructed surfaces of the
Octopus data set for both HRBF implicits and the regular-

ized variational scheme (Figure 10) with the same support
radii used in the memory tests of Table 1. For the largest
radius case, both HRBF Implicits and regularized variational
present satisfactory results [Figures 10(a) and (b) for HRBF
implicits and (c) for regularized variational]. When picking
the support radius from HRBF implicits, usually the smaller,
the regularized variational solution oscillates more at the

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

38

I. Macˆedo et al. / HRBF Implicits

Table 1: Memory footprint assessments: (from left to right) the dataset names, the number of points of each data set, the order n of matrix A,
the number of stored non-zero elements of A (nnz(A)), a sparsity ratio (from 0, which represents a null matrix, to about 0.5, which represents
a full symmetric matrix), an estimate of the peak-memory used and the average number of samples inside the support for each radial function
(neighbours).

n

nnz(A)

nnz(A)/(n2 )

Data set

No. of points

Memory (GB)

Average No. of neighbours

Homer
Octopus
Raptor
Kitten

5103
16944
65536
137098

Homer
Octopus
Raptor
Kitten

5103
16944
65536
137098

Regularized variational method with largest radius under a memory limit
5103
13022856
0.50
0.74
16944
26623753
0.09
2.58
65536
38919427
9e-3
2.72
137098
12024097
6e-4
2.49

5102
3140
1185
173

Homer
Octopus
Raptor
Kitten

5103
16944
65536
137098

Regularized variational method borrowing the (largest) radius of HRBF Implicits tests
5103
3943177
0.15
0.36
16944
3079404
0.01
0.33
65536
1632067
3e-4
0.28
137098
1098562
5e-5
0.25

1543
361
47
14

HRBF implicits with largest radius under a memory limit
20412
63060214
0.15
2.47
67776
49168800
0.01
2.50
262144
25719856
3e-4
2.62
548392
16754404
5e-5
2.50

1543
361
47
14

Table 2: Processing time assessments (in seconds).

Data set

Graph built

Homer
Octopus
Raptor
Kitten

3.11
4.32
3.36
2.23

Homer
Octopus
Raptor
Kitten

3.98
31.29
58.68
23.06

Homer
Octopus
Raptor
Kitten

3.11
4.48
3.36
2.21

System built

System analysed

System factorized

Fwd/Bwd substitutions

HRBF implicits with largest radius under a memory limit
10.20
45.12
89.39
2.33
7.458
42.33
74.82
2.80
4.013
26.11
74.54
5.78
2.898
19.57
80.89
2.75
Regularized variational method with largest radius under a memory limit
8.62
7.11
9.75
0.23
17.77
16.03
185.73
2.07
26.10
35.94
75.96
3.36
8.17
5.28
128.7
5.76
Regularized variational method borrowing the (largest) radius of HRBF implicits tests
2.69
5.56
2.07
0.13
2.10
8.27
1.43
0.13
1.11
9.80
2.24
0.40
0.77
10.04
2.10
0.41

bottom of the head of the octopus than the previous results
[Figure 10(d)].
We believe the spurious surfaces in the results of the regularized variational scheme (Figure 10) arise as rendering
artefacts (from our customized version of POV-Ray [PR10]
with our function evaluators), due to the rootfinder’s sensitivity to the high gradients present in the reconstructed function.
Such steep gradients pose difficulties for identifying the isosurface of interest. Trying to avoid such undesirable artefacts,
we augmented the interpolantion space with a polynomial basis. Results of augmenting the regularized variational method
with 0 (R3 ) are presented in Figure 11(a), using largest

Total time

150.4
132.01
114.21
108.68
29.78
253.26
200.371
171.41
13.59
16.46
16.98
15.64

radius for the variational method, and Figure 11(b), borrowing the largest radius from HRBF implicits. For the latter case, the addition of the constant term, despite eliminating those artefact surfaces, produces a more oscillatory
result than previously [Figure 10(c)]. By increasing the polynomial space, reconstructions becomes slightly better, but
the renderer still has difficulties to find only the isosurface
[Figures 11(c) and (d)].
Some results of HRBF implicits concerning regularization
of points and normals are presented in Figure 12. When regularization is not employed (a and b), details are enhanced,
but a hole appears. In (c) both regularization of points and

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

I. Macˆedo et al. / HRBF Implicits

39

Figure 11: Regularized variational method with polynomial augmentation: spurious surfaces (probably caused by the difficulty
of the renderer rootfinder to identify only the component of interest) are eliminated when 0 (R3 ) is used, but with HRBF
implicits’ radius (b), reconstruction oscillates more than without augmentation. In case of using d (R3 ), d = 1, 2 (c) and (d),
reconstructions become less oscillatory, but the renderer again has difficulties to find only the connected component of interest.
normals are employed. In (d) we apply the same regularization parameter of (c) for points, but we increase the parameter
for normals, whereas in (e) we invert the situation, we use
the same of (c) for normals, but increase the parameter for
points. It can be observed in (d) that the surface becomes
smoother and fatter, allowing to fill totally a hole. On the
other hand, there are fewer details than in (e). In (f), choosing larger values for both regularization parameters, a very
smooth reconstruction arises. For comparison, in (g) we apply the same regularization parameter of (f) for normals and
no regularization for points while, in (h), inverting again, we
use the same regularization parameter of (f) for points and
zero for normals.
Finally, we observe in Figure 13 the ability of HRBF to
enhance details due to its proper enforcement of normal constraints. In (a), for the sake of comparison, we present a flatshaded triangle mesh of the head of the raptor, whereas on
(b) we show its HRBF implicitization, interpolating both the
mesh vertices and their normals (approximated by renormalized area-weighted averaging of incident faces’ normals).
5. Concluding Remarks
In this paper, we presented a framework for dealing with
surface reconstruction problems in which the data represents
linear measurements of an unknown implicit function. Our
theoretical considerations naturally induce a computationally
tractable form for the recovered function and allow us to both
interpolate or approximate the data by solving a few sparse
symmetric positive definite linear systems.
As a concrete realization of that framework, we introduced HRBF implicits, which are a direct result of
Hermite–Birkhoff interpolation theory with radial basis functions under the assumption of (first-order) Hermite-data,
that is measurements of the implicit function’s values and
gradients.
Our prototype system has already allowed us to solve surface interpolation problems of up to 500 K point/normal

samples in R3 within just a couple minutes. Its implementation is almost a direct translation of the mathematical results
and is general enough to be independent of the space dimension, except for the visualization module, yet including the
main acceleration data structure, an unbalanced kd-tree.
Also, as a byproduct of our theoretical considerations,
we gained some insights on a related regularization-based
scheme, which was deduced from a statistical learning perspective, and this allowed us to enhance the flexibility of
both methods by ensuring well-posedness of an interesting
combined interpolation/regularization approach. Similar insights came up by comparing our derivations and results with
those from a recent variationally deduced approach allowing
a better understanding of the method in that work.
The framework we presented opens many avenues for further research. It is well known in RBF theory and practice the
trade-off between stability of the interpolation system and the
error associated with the recovered function. Intuitively, the
larger the radius of the compactly supported basis functions,
smaller the error in the recovered function, but more difficult
it can be to solve the interpolation system. This difficulty can
be observed as denser matrices and more nonzero terms when
evaluating the recovered function, also the condition number
of the interpolation system becomes larger. The RBF community came up with many approaches to tackle this issue,
some of them previously adopted by computer graphics researchers, presenting us interesting opportunities for further
investigation.

5.1.1. Domain decomposition
To allow larger data sets and larger supports, the memory
requirements of a sparse Cholesky decomposition can be
alleviated by employing an iterative process in which each
step consists of interpolating a residual on many smaller
subsets of the input data. The results presented by Beatson
et al. in [BLB01] could be extended to our framework and
even coupled with a Krylov-subspace accelerator such as the

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

40

I. Macˆedo et al. / HRBF Implicits

Figure 13: HRBF implicits: reconstruction with enhanced
details due to the use of first-order (normals) information.

preconditioned conjugate gradients method. Thus, allowing
us to deal with larger data sets without the large memory
consumption of a full sparse factorization.
5.1.2. Partition of unity
A decomposition of the data set in subdomains can also be
used to build an interpolant without an iterative process by
employing partition of unity functions (e.g. [OBA∗ 03]). It can
be shown that, by interpolating the data inside the support of
each PU, an implicit function can be constructed by blending
those interpolants with the PU functions such that its zero-set
passes through the input points and whose gradient matches
the normals provided.
5.1.3. Mutiple scales
By creating a hierarchy of subsets of the input samples (e.g.
[DIW07]), one is able to employ larger radii on coarser levels
of the hierarchy while interpolating residuals using smaller
radii on the finer levels. This way, a global interpolant can
be computed with large support even for very large data sets.
And actually, by stopping the procedure before interpolating
the whole data set, an approximating implicit function with
very good practical reproduction may be recovered [OBS05].
5.1.4. Basis selection

Figure 12: HRBF implicits with regularization.

Methods which reduce the complexity of the interpolant comprise a very promising subject for investigation because they
allow for larger supports even when dealing with large redundant data sets. Some interesting greedy approaches rely only
on the general Hilbert space structure [SW00], by choosing the most correlated basis with respect to a residual, or

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

I. Macˆedo et al. / HRBF Implicits

41

end, it still remains the issue of suitably resampling the implicit function resulting from a displacement of the original
samples’ locations and gradients. This application represents
an important problem in fluid mechanics and dynamic implicit surfaces in general.
Acknowledgements

Figure 14: Hermite–Birkhoff reconstruction: we assume
that the gradient of the implicit function is just parallel to the
provided normals and enforce gradient interpolation at only
one of the sample points, visible mid-left on the mouth in (b).

specifically with the surface reconstruction problem where
the centres are chosen by local curvature error estimates
[WSC06].

The authors thank the reviewers of both SIBGRAPI’09 and
Computer Graphics Forum for their comments and suggestions for improving the presentation. The authors are also
grateful to Emilio Vital Brazil, Thiago Pereira, Julio Daniel
Silva and Mario Costa Sousa for participating in fruitful
and insightful discussions and for kindly providing computational resources. This work was partially supported by grants
from CNPq, CAPES, FAPESP, FINEP, FAPERJ, University
of British Columbia and University of Calgary.
References
[AA09] ALEXA M., ADAMSON A.: Interpolatory point set
surfaces—convexity and Hermite data. ACM Transactions
on Graphics 28, 2 (2009), 1–10.

5.1.5. Hermite thin-plate splines
In Duchon’s seminal paper [Duc77], it has been shown how to
design kernels by minimizing certain semi-norms in suitable
spaces of smooth functions under generalized interpolation
constraints. By requiring C 1 -continuity and first-order Hermite data, he derived a globally supported interpolant which
generalizes the thin-plate splines of classical interpolation
by minimizing a curvature-related semi-norm similar to that
which originates the kernel used by Carr et al. in [CBC∗ 01].
An interesting direction of research concerns deriving fast
multipole methods and basis selection algorithms analogous
to those used by Carr et al. for this interpolant.

5.1.6. Hermite–Birkhoff interpolation
We believe that many problems in computer graphics can
benefit from the general framework of Hermite–Birkhoff interpolation with RBFs. Very promising results which might
be applicable to photometric stereo and shape-from-shading
have been presented by Kaminski et al. [KEHK08]. Also,
encouraging preliminary results indicate that it is possible
to achieve surface interpolation even with inconsistently oriented normals by assuming that the gradient of the implicit
function is just parallel to the provided normals and enforcing gradient interpolation at only one of the sample points,
cf. Figure 14.

5.1.7. Dynamic implicit surfaces
This research was motivated by the problem of tracking surfaces evolving along a vector field [GNNB08, GB10]. To this

[ABCO*03] ALEXA M., BEHR J., COHEN-OR D., FLEISHMAN
S., LEVIN D., SILVA C. T.: Computing and rendering point
set surfaces. IEEE Transactions on Visualization and
Computer Graphics 9, 1 (2003), 3–15.
[ATL10] ATLAS: Automatically tuned linear algebra software, June 2010. http://math-atlas.sourceforge.net/.
[BCM99] BEATSON R., CHERRIE J., MOUAT C.: Fast fitting of
radial basis functions: Methods based on preconditioned
GMRES iteration. Advances in Computational Mathematics 11, 2 (November 1999), 253–270.
[BLB01] BEATSON R. K., LIGHT W. A., BILLINGS S.: Fast solution of the radial basis function interpolation equations:
Domain decomposition methods. SIAM Journal on Scientific Computing 22, 5 (2001), 1717–1740.
[BN00] BACHMAN G., NARICI L.: Functional Analysis. Dover
Publications Inc., Mineola, New York, 2000.
[CBC*01] CARR J. C., BEATSON R. K., CHERRIE J. B.,
MITCHELL T. J., FRIGHT W. R., MCCALLUM B. C., EVANS
T. R.: Reconstruction and representation of 3D objects
with radial basis functions. In Proceedings of the International Conference on Computer Graphics and Interactive
Techniques (Los Angeles, CA, USA, 2001).
[CDHR08] CHEN Y., DAVIS T. A., HAGER W. W.,
RAJAMANICKAM S.: Algorithm 887: CHOLMOD, supernodal sparse Cholesky factorization and update/downdate.
ACM Transactions on Mathematical Software (TOMS) 35,
3 (2008), 1–14.

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

42

I. Macˆedo et al. / HRBF Implicits

[DIW07] DYN N., ISKE A., WENDLAND H.: Meshfree thinning of 3D point clouds. Foundations of Computational
Mathematics 8, 4 (2007), 409–425.

[OBA*03] OHTAKE Y., BELYAEV A., ALEXA M., TURK G.,
SEIDEL H.-P.: Multi-level partition of unity implicits. ACM
Transactions on Graphics (TOG) 22, 3 (2003), 463–470.

[DSS*09] DIETRICH C. A., SCHEIDEGGER C. E., SCHREINER J.,
COMBA J. L. D., NEDEL L. P., SILVA C. T.: Edge transformations for improving mesh quality of marching cubes. IEEE
Transactions on Visualization and Computer Graphics 15,
1 (2009), 150–159.

[OBS05] OHTAKE Y., BELYAEV A., SEIDEL H.-P.: 3D scattered
data interpolation and approximation with multilevel compactly supported RBFs. Graphical Models 67, 3 (2005),
150–165.

[Duc77] DUCHON J.: Splines Minimizing Rotation-Invariant
Semi-Norms in Sobolev Spaces, vol. 571 of Lecture
Notes in Mathematics. Springer, Berlin, Heidelberg, 1977,
pp. 85–100.
[Fas97] FASSHAUER G.: Solving partial differential equations
by collocation with radial basis functions. In Surface Fitting and Multiresolution Methods. Vanderbilt University
Press, Nashville, TN, 1997, pp. 131–138.
[FCOS05] FLEISHMAN S., COHEN-OR D., SILVA C. T.: Robust moving least-squares fitting with sharp features.
ACM Transactions on Graphics 24, 3 (2005), 544–
552.
[GB10] GOIS J. P., BUSCAGLIA G. C.: Resampling strategies
for deforming mls surfaces. Computer Graphics Forum
(2010). DOI:10.1111/j.1467-8659.2010.01663.x.
[GG07] GUENNEBAUD G., GROSS M.: Algebraic point set surfaces. In SIGGRAPH ’07: ACM SIGGRAPH 2007 papers
(New York, NY, USA, 2007), ACM, p. 23.
[GNNB08] GOIS J. A. P., NAKANO A., NONATO L. G.,
BUSCAGLIA G. C.: Front tracking with moving-leastsquares surfaces. Journal of Computational Physics 227,
22 (2008), 9643–9669.
[GPJE*08] GOIS J. P., Polizelli-Junior V., ETIENE T., TEJADA
E., CASTELO A., NONATO L. G., ERTL T.: Twofold adaptive
partition of unity implicits. The Visual Computer 24, 12
(2008), 1013–1023.
[KEHK08] KAMINSKI J., ETTL S. H. G., KNAUER M. C.: Shape
reconstruction from gradient data. Applied Optics 47, 12
(2008), 2091–2097.
[MYC*01] MORSE B. S., YOO T. S., CHEN D. T.,
RHEINGANS P., SUBRAMANIAN K.: Interpolating implicit surfaces from scattered surface data using compactly supported radial basis functions. In Proceedings of SMA’01
(Genova, Italy, 2001).

[PMW09] PAN R., MENG X., WHANGBO T.: Hermite variational implicit surface reconstruction. Science in China
Series F: Information Sciences 52, 2 (2009), 308–315.
[PR10] POV-Ray: Persistence of vision ray tracer, June
2010. http://www.povray.org/.
[SN09] SINGH J. M., NARAYANAN P.: Real-time ray tracing of
implicit surfaces on the gpu. IEEE Transactions on Visualization and Computer Graphics 99, RapidPosts (2009),
261–272.
[SOS04] SHEN C., O’Brien J. F., SHEWCHUK J. R.: Interpolating and approximating implicit surfaces from polygon
soup. In Proceedings of the SIGGRAPH ’04: ACM SIGGRAPH 2004 Papers (New York, NY, USA, 2004), ACM,
pp. 896–904.
[SSS06] SCHREINER J., SCHEIDEGGER C., SILVA C.: Highquality extraction of isosurfaces from regular and irregular
grids. IEEE Transactions on Visualization and Computer
Graphics 12, 5 (2006), 1205–1212.
[SW00] SCHABACK R., WENDLAND H.: Adaptive greedy techniques for approximate solution of large RBF systems.
Numerical Algorithms 24, 3 (2000), 239–254.
[TO02] TURK G., O’BRIEN J. F.: Modelling with implicit
surfaces that interpolate. ACM Transactions on Graphics
21, 4 (2002), 855–873.
[Wen95] WENDLAND H.: Piecewise polynomial, positive definite and compactly supported radial functions of minimal degree. Advances in Computational Mathematics 4,
1 (1995), 389–396.
[Wen05] WENDLAND H.: Scattered Data Approximation.
Cambridge University Press, Cambridge, 2005.
[WSC06] WALDER C., SCHO¨ LKOPF B., CHAPELLE O.: Implicit
surface modelling with a globally regularised basis of
compact support. Computer Graphics Forum 25, 3 (2006),
635–644.

c 2010 The Authors
Computer Graphics Forum c 2010 The Eurographics Association and Blackwell Publishing Ltd.

