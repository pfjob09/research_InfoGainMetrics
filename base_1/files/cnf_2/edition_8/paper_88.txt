Eurographics/ IEEE-VGTC Symposium on Visualization 2008
A. Vilanova, A. Telea, G. Scheuermann, and T. Möller
(Guest Editors)

Volume 27 (2008), Number 3

Illustrative Hybrid Visualization and Exploration
of Anatomical and Functional Brain Data
W. M. Jainek1 , S. Born2 , D. Bartz2 , W. Straßer1 , J. Fischer3
1 WSI/GRIS,

2 ICCAS/VCM, Universität Leipzig, Germany
Eberhard-Karls-Universität Tübingen, Germany
3 Graphics Group, University of Victoria, Canada

Abstract
Common practice in brain research and brain surgery involves the multi-modal acquisition of brain anatomy and
brain activation data. These highly complex three-dimensional data have to be displayed simultaneously in order
to convey spatial relationships. Unique challenges in information and interaction design have to be solved in
order to keep the visualization sufficiently complete and uncluttered at the same time. The visualization method
presented in this paper addresses these issues by using a hybrid combination of polygonal rendering of brain
structures and direct volume rendering of activation data. Advanced rendering techniques including illustrative
display styles and ambient occlusion calculations enhance the clarity of the visual output. The presented rendering
pipeline produces real-time frame rates and offers a high degree of configurability. Newly designed interaction and
measurement tools are provided, which enable the user to explore the data at large, but also to inspect specific
features closely. We demonstrate the system in the context of a cognitive neurosciences dataset. An initial informal
evaluation shows that our visualization method is deemed useful for clinical research.
Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Display algorithms

1. Introduction
The field of human brain mapping has evolved significantly
over the past decades through many technological advancements, such as electroencephalography (EEG), magnetoencephalography (MEG), and positron emission tomography
(PET). While these methods can all be used to record brain
activity, a tradeoff between temporal and spatial resolution
has to be made. The advent of magnetic resonance imaging
(MRI) and its specialization, functional magnetic resonance
imaging (fMRI), however, made it possible to record brain
activity with a better temporal resolution than PET and at a
better spatial resolution than EEG/MEG at the same time.
Ever since, the utilization of fMRI has grown quadratically
[Ban07] and is now used in such diverse areas as cognitive
neuroscience, psychiatry, the study of memory, of language
systems, of visual pathways, as well as in clinical routine for
preoperative planning of neurosurgical interventions.
A fMRI session usually consists of several periods during
which the person under examination either receives passive
stimuli (visual, acoustic, or haptic) or performs a task (e.g.,
finger tapping). These periods of stimulation are alternated
with periods of rest, while the fMRI response is continuously observed. After acquisition, a brain activation map is
produced from the data. For that purpose, a statistical value
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

is assigned to every voxel reflecting the correlation of the
brain response at that position with the stimulation. All voxels with a value above a certain threshold are considered active. This implies that the brain activity cannot be definitely
localized, but only with a certain probability [JMS01].
It is common practice to record a high-resolution (anatomical) MRI scan of the brain in addition to the fMRI data and
to register both datasets. This helps in determining the spatial relationships between the activation areas and various
brain structures. In particular, the location and the extent of
the activation areas with respect to subcortical structures, the
cortex surface, and the different cortex areas are of interest.
The visualization presented in this paper aims to convey as
much of this information as possible through the use of advanced rendering techniques. The rendering pipeline is implemented on the GPU in order to achieve interactive frame
rates and to facilitate user interaction. Concerning the visualization of fMRI results, two criteria are important. First,
the exact location of the activation areas in the brain has to
be displayed. Second, instead of displaying only the surface
of an activation region, it needs to be possible to visualize
its volumetric depth to give a better insight into its internal
structures. The main challenge is to combine these two requirements and still provide a clearly-arranged visualization
that can be grasped quickly.

856

Jainek, et.al. / Illustrative Hybrid Visualization and Exploration of Anatomical and Functional Brain Data

2. Related Work
For the statistical analysis of fMRI data, many different software packages exist, which are either freely (e.g., the SPM
Matlab package, [SPM]) or commercially (e.g., BrainVoyager, [Bra]) available. The most common way of visualizing
the activation areas is the highlighting of the corresponding
voxels in the 2D slices of the fMRI data set. The main problem of this kind of visualization is the limited perception of
the three-dimensional structure and location of the activated
areas in the brain.
Three-dimensional visualization is typically carried out by
either direct volume rendering (DVR) or surface rendering. DVR of both brain anatomy and active brain areas is
rather straightforward. Hidden activation areas can be exposed by setting parts of the brain anatomy to be transparent [BHWB07] or by applying clipping planes [RTF∗ 06].
While this allows a better localization of activation areas, it
is still not easy to grasp their three-dimensional structure.
Early work on using surfaces for the three-dimensional
visualization of functional neuroimaging data includes
[RLF∗ 98]. Surface rendering of activation regions as geometrical models, superimposed on 2D anatomical images is
another possible visualization method [PBF∗ 07]. The advantage is the increased perception of the shape of the activation
areas and their relative position to other brain structures that
are rendered as surfaces as well. However, in order to be able
to derive the exact location of the brain activity, interaction
such as scrolling through the 2D slices and rotation in order
to compensate for occlusion, is needed. While this interaction increases three-dimensional perception, it is not always
practical, e.g., in the operating room.
There have been previous approaches to combine direct volume rendering and surface rendering in order to convey both
the structure and the relative location of activation areas. One
example is the work by [SRWE07], where DVR is used to
render activation areas, and surface rendering is used to display the structure of the cortex surface. Instead of rendering
a fully shaded surface, a line representation is used that is
derived from the surface curvature, allowing the activation
areas to be visible beneath. The line representation itself,
however, has a high spatial frequency even on parts of the
cortex that have a slowly varying curvature, and is thus introducing visual clutter.
An earlier approach by [TIP05] avoids this clutter by using silhouette lines when depicting various organs. It has
been shown that the combination of silhouette, surface and
volume rendering is indeed appropriate to convey complex
spatial relations. One important drawback of the technique,
however, is that it is not possible to draw semi-transparent
shading with the volume rendering shining through – an aspect that is important when rendering activation regions.
In both approaches, a light diffuse shading is used to better
convey the structure of the surface. This shading, however,
does not incorporate local occlusion effects which are important for an increased depth perception. [Ste03] adressed this

issue through a technique called “vicinity shading”, which is
similar to the ambient occlusion shading used in our system.
3. Preprocessing
The MRI data used in this paper is gathered using a T1weighted scan that is resampled to an isotropic volume of
2563 voxels. This data is processed with the software package FreeSurfer [Fre] in order to extract the cortex surface
as well as the surface describing the white matter/gray matter boundary. These complex surfaces are created based
on tissue segmentation and are refined using iso-gradients
[DFS99, FSD99]. Additionally, FreeSurfer performs a segmentation and labeling of the volume and the extracted surfaces into different functional regions. The preprocessing
typically takes several hours on a state-of-the-art PC.
The fMRI data is gathered through a T2*-weighted BOLD
scan during either block- or event-related test paradigms.
This data is processed with the SPM software package to
create the fMRI activation volume.
4. Visualization
The combined visualization of MRI and fMRI data has to
convey many different aspects about the data at the same
time. Among them are the shape of the cortex surface, the
extent of different cortex areas, the cortex folding structure
and thickness, as well as the location and shape of the fMRI
activation areas. In order to achieve a clear and concise output, our visualization extends on previous approaches by utilizing render styles such as silhouette rendering and ambient occlusion shading of the cortex surface as well as direct
volume rendering of the brain activation areas. In contrast
to previous approaches, however, our hybrid rendering algorithm is capable of seamlessly integrating all of these techniques into the final render output, while at the same time
lifting some of the constraints imposed by earlier algorithms.

Figure 1: Example of a combined MRI-fMRI visualization
rendered with our system.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Jainek, et.al. / Illustrative Hybrid Visualization and Exploration of Anatomical and Functional Brain Data
Clipping Plane

Camera

857

Upper Surface
Upper Volume

Volume

Lower Volume
Lower Surface

Cortex Surface
Far Clipping Plane

Middle Surface
Middle Volume

Camera Viewing Volume

Figure 2: Hybrid Mesh and Volume Rendering. Left: A schematic view of the scene. The camera is set to orthographic projection
for simplicity, but the presented concepts are equally applicable to perspective projection. The near clipping plane is left out
for visual clarity. Right: The scene is split up into six layers. The thick, colored lines on the cortex surface denote the parts of
the mesh that are topmost in the framebuffer after the respective layer has been rendered.
An important aspect is that it allows for semi-transparent
surface shading to be combined with DVR. Additionally,
a user adjustable clipping plane is provided for explorative
purposes. A sample output of the visualization method can
be seen in Figure 1.
In this section, we will present our hybrid illustrative mesh
and volume rendering method and the render styles we used
for different aspects of the brain data. In the next section, the
various user interaction methods are explained.
Illustrative Mesh and Volume Rendering. Using traditional interleaving methods for mesh and volume rendering
such as the one presented in [KK99] is not feasible when
using illustrative mesh rendering. The main reason is that illustrative rendering methods are usually divided into scene
and screen space render passes, where the latter require the
mesh rendering results from previous passes as inputs. These
results are not available, however, if the mesh and volume
slices are interleaved directly in a single render pass.
Our approach to interleaving mesh and volume rendering
is to split up the entire scene into different render layers.
These are rendered individually, stored in intermediate textures, and are later combined to produce the final output. The
camera position, the clipping plane, and the surface mesh are
all taken into account when splitting up the scene. Figure 2
shows a schematic scene setup and the resulting render layers. The clipping plane (middle surface) divides the cortex
surface into a part that is farther away and one that is closer
to the camera: the lower and upper surface, respectively. The
volume is split up into three parts: the part that lies below the
lower surface (lower volume), the part that lies between the
lower surface and the clipping plane (middle volume) and
the part that lies above the clipping plane (upper volume).
All layers are composited back to front in the following order to produce the final output: lower volume, lower surface,
middle volume, middle surface (clipping plane), upper volume, upper surface. Note that for a general pipeline, the upper volume would have to be split up into two parts as well,
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

one that lies below the upper surface and one that lies above.
In the special case of a volume that holds activation areas,
however, all voxels containing activations lie below the upper surface, and the entire upper volume can therefore be
rendered using only one layer.
The composition of the different render layers can be seen
as a typical emission-absorption volume rendering approach,
where emission and absorption values for different segments
of viewing rays have already been pre-computed, e.g., the
middle volume render layer holds the emission and absorption values for the ray segments that start at the lower surface
and end at the middle surface. Surfaces are treated the same
way as volumes, i.e., they also store emission and absorption
values. This allows the easy integration of semi-transparent
surfaces into the final render output.
Since the surface layers are rendered into different textures
before they are composed to the final output, it is easy to
perform additional screen space render passes in order to
achieve non-photorealistic render styles such as silhouette
drawing.
Silhouettes. The cortex surface provides a context, or frame
of reference, for all other parts of the visualization and
should therefore be visible at all times. On the other hand,
the surface should not occlude the parts that lie behind it.
This is why we have chosen to convey the general shape
of the surface through contours and silhouettes. We compute these silhouettes by rendering the surface mesh into the
framebuffer and using a special fragment shader that stores
the normal vector for each fragment in the RGB channels
of the output, and the depth of the fragment relative to the
camera in the alpha channel. In a second screen space render
pass, another fragment shader accesses the previously rendered texture to compute the magnitude of the depth gradient and the normal similarity at each pixel. These values are
then thresholded to determine if a silhouette is to be drawn
for the current pixel. The mathematical details can be found
in [FBS05]. Non-thresholded results can be seen in Figure 3.

858

Jainek, et.al. / Illustrative Hybrid Visualization and Exploration of Anatomical and Functional Brain Data

for each vertex. The ambient occlusion for each vertex is
pre-computed using the following formula [Jai07]:
Z

A(x) =
Ω

Figure 3: Results for the normal similarity (left) and depth
gradient magnitude (right) computations.
When taking the clipping plane into account, the above steps
are performed individually for both the lower and the upper
surface meshes. We have found, however, that the silhouette
detection method described above fails at robustly identifying the outlines of the white matter (WM) / gray matter (GM)
boundary as well as the cortex boundary on the clipping
plane. The reason is that for many boundary pixels, both the
magnitude of the depth gradient is very small and the normal
similarity is very high. In order to correctly compute the outlines at those pixels, we encode the category for each pixel
by scaling the stored normal during the first render pass, and
use a simple edge detection on the stored categories in the
screen space pass. The three pixel categories and a sample
result are shown in Figures 4 and 5, respectively.

1
[1 −V (x, ω)] · |n · ω| dω,
π

(1)

where A(x) denotes the ambient occlusion at the vertex position x, n denotes the surface normal at the given vertex and
Ω denotes the upper hemisphere of direction vectors for the
given normal. V (x, ω) is the visibility function that is 1 if
the background is visible at position x in direction ω, and 0
if it is occluded by scene geometry. We use an importance
weighted Monte Carlo integration method for the evaluation
of the integral, where the hemisphere above each vertex is
sampled according to a cosine-distribution: p(ω) = π1 |n · ω|.
For N samples, the ambient occlusion is approximated by:
A(x) ≈

1
N

N

∑ [1 −V (x, ωi )] .

(2)

i=1

To speed up the occlusion computation, a kd-tree is constructed for the surface mesh. Figure 6 shows results for the
ambient occlusion shading and the surface coloring.

inner WM/GM boundary
outer WM/GM boundary / inner cortex surface
outer cortex surface

Figure 4: We classify pixels into three different categories
depending on their position on the cortex surface and the
white matter / gray matter boundary.

Figure 6: Top: Coloring of the different surface areas. Bottom: Combined diffuse light and ambient occlusion shading.
1000 samples per vertex have been used to compute the ambient occlusion values.
Figure 5: Left: Pixel categories stored when rendering the
lower surface. Right: The result of the edge detection on the
inner two categories.
Cortex Area Coloring and Ambient Occlusion. The different cortex areas determined by FreeSurfer are included
into the output image by filling the area between the silhouettes with different semi-transparent colors. This impairs
the perceived three-dimensional shape of the cortex surface.
Hence, we decided to include a small amount of surface
shading into the final output. For this, we combine a simple diffuse light source with ambient occlusion values stored

Volume Rendering. To render the fMRI activation areas,
we chose a volume rendering approach to preserve the information on the spatial extent and falloff of the different areas.
We use standard slice based volume rendering with transfer functions for both the emission and absorption values of
each voxel. The transfer functions have been manually designed so that an understandable output with high color contrasts is generated for typical datasets.
When the activation volume is clipped at the clipping plane,
artifacts like the ones seen in Figure 7a appear. To reduce
these artifacts, we have implemented a soft clipping mechanism, where fragments that lie within a small range below
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Jainek, et.al. / Illustrative Hybrid Visualization and Exploration of Anatomical and Functional Brain Data

or above the clipping plane are not discarded abruptly, but
rather gradually faded away. This is achieved by linearly
scaling down their absorption and emission until both values reach zero. The result of this soft clipping can be seen in
Figure 7b.

(a)

859

(b)

(c)

(a)

(b)

Figure 7: (a) The clipping plane introduces banding artifacts in the display of activation areas. (b) When using the
soft clipping approach, the artifacts disappear.
The Clipping Plane. The clipping plane plays a very important role in our visualization approach. As observed before, the cortex folding structure as well as the cortex thickness are important pieces of information that have to be conveyed to the user. If they were to be displayed for the entire cortex at once, however, the visual output would be very
cluttered. Hence, we made this information “explorable” by
the user, i.e., it is only shown on the clipping plane, and the
user can tilt and adjust the clipping plane to explore the specific structures that are of interest to him/her.
Visually, the clipping plane is also responsible for filling the
gap between the cortex surface and the white matter / gray
matter boundary. To this end, we render a rectangle with the
same orientation as the clipping plane and with a volume texture applied to it that holds the computed FreeSurfer cortex
area segmentation. Figure 8a shows the output when directly
accessing the FreeSurfer segmentation results, which suffers
from the relatively low spatial resolution of the underlying
volume texture. To enhance the visual quality, we first perform a step similar to a morphological dilation, where the
segmentation labels are spread to a larger area. This is done
by iteratively visiting the 6 direct neighbors of each voxel
and assigning them the same segmentation label if they don’t
have one yet. The resulting segmentation is shown in Figure 8b. Finally, in the clipping plane (middle surface) rendering pass, the fragment shader accesses the pixel categories
texture shown in Figure 5 to restrict the rendering of the labels to the actual gray matter cortex area (see Fig 8c). The
entire intersection area of the clipping plane with the brain
volume is also determined by querying the pixel categories
texture and keeping only those pixels that belong to the inner
two categories . This area is then drawn semi-transparently,
so that activation areas lying behind the clipping plane can
shine through (e.g., see Fig 7).
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Figure 8: (a) When using the FreeSurfer cortex segmentation directly, the output suffers from the low spatial resolution of the volume texture. (b) Expanded cortex segmentation. (c) Same as (b), except the output is confined to the
actual gray matter cortex area using the stored pixel categories from Figure 5.
Combining the Render Layers. We have chosen a gray
color for the background of the visualization output. This
allows for both the subtraction and addition of brightness
through the following render layers. Combined with the fact
that all but the fMRI activation volume render layers mostly
darken the final output, the choice of a gray background has
proven to be very practical: the bright activation areas are
easy to distinguish in the final render output.
Since the different render layers are all computed individually, the presented pipeline makes it very easy to fine tune
different parameters to produce the desired output, e.g., the
opacity of the cortex surface, the saturation of the surface
area colors, the amount of ambient occlusion shading, as
well as others.
5. User Interaction
The user is given various tools to interact with the presented
data, making it possible to focus on specific parts that are
of particular interest. The camera used to render the scene
can be rotated and moved, and the projection type can be
gradually changed from a strong perspective projection to an
orthogonal one. This is achieved by gradually reducing the
field of view, while at the same time increasing the camera
distance to the scene, or vice versa.
Clipping Plane Control. Since the clipping plane plays an
important role in the exploration of the data, an intuitive manipulation tool for the orientation and height of the clipping
plane is necessary. The clipping plane, although infinite in
nature, has a perceived spatial extent that is defined by the
parts of the scene that the plane intersects. We found that

860

Jainek, et.al. / Illustrative Hybrid Visualization and Exploration of Anatomical and Functional Brain Data

the center of this perceived spatial extent is an intuitive rotation center (as opposed to, e.g., the world origin). We approximate this center r by projecting a pre-computed scene
center s onto the clipping plane (see Fig. 9a). After the rotation of the clipping plane is completed, the rotation center
is recomputed (r ) to reflect the new clipping plane orientation (see Fig. 9b). Figure 10 shows how different clipping
plane orientations can be used to explore the surface around
an activation area.
(a)

r

s

(b)

(c)

(d)

Start

(b)
r

(a)

Figure 11: fMRI volume segmentation results for different
1 1
pre-flood heights. From left to right, top to bottom: 0, 50
, 25
1
and 5 of the maximal voxel intensity is used.

r

o
End

Figure 9: Intuitive Clipping Plane Rotation. (a) o denotes
the world origin, s is the scene center and r is the clipping
plane rotation center that is computed by projecting s onto
the clipping plane. (b) Once a rotation is initiated, the rotation center r is held constant. When the rotation stops, the
rotation center is recomputed (r ).

Figure 12: Only the currently selected area is rendered with
full saturation. This allows the user to focus on a specific
part of the data.

Figure 10: Different clipping plane orientations reveal different parts of the cortex folding structure.
Activation Area Selection. When examining the fMRI activation data, the user can choose to focus on only one specific activation area at a time. Technically, this means that
the activation data has to be segmented into different activation areas in a pre-processing step. For this task, we use a
1
watershed algorithm [RM00] with a pre-flood height of 25
of the maximal voxel intensity, but it can be noted that good
results are achieved over a wide range of pre-flood heights
(see Fig. 11). When the user clicks on the screen, a “picking”
volume rendering pass determines the chosen area. In order
to highlight the selected activation area, all other areas are
rendered in a desaturated manner (see Fig. 12).

Surface Point Selection and Distance Measurements.
When the user has selected a specific activation area, the
possibility to select an arbitrary point on the cortex surface
is provided in order to perform distance measurements. During the surface selection process, the user is given real-time
feedback of the selected surface point as the mouse pointer
moves over the cortex surface. The surface point is computed
by intersecting a ray starting at the camera in the direction of
the mouse pointer with the kd-tree that has been constructed
for the cortex surface. As visual feedback, the cortex surface
in the surrounding of the selected point is illuminated using a very quick falloff (Fig. 13a). Even a small change of
the mouse pointer coordinates in screen space can result in
a drastic change of the surface point position in world coordinates. This is very well captured by the chosen visual
feedback, as can be seen in Figure 13b.
In order to perform the actual distance measurement, a ruler
is rendered into the scene. Its endpoints lie at the point of
highest activity inside the selected activation area and at the
chosen surface point, respectively. A small text label shows
the current distance in millimeters (see Fig. 14).
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Jainek, et.al. / Illustrative Hybrid Visualization and Exploration of Anatomical and Functional Brain Data

(a)

(b)

Figure 13: Surface point selection. The surface point moves
1 pixel to the right from (a) to (b). In (a), it lies on top of the
gyrus, in (b), it has “descended” into the sulcus.

861

Cortex Labels. For orientation purposes, the user can
choose to display labels for the different cortex areas (see
Fig. 15). We devised an algorithm that automatically computes both the label positions on the cortex surface as
well as their direction vectors. The first step is to compute
an envelope for the cortex surface by using alpha-shapes
[BB97, EM92]. For a given cortex area, all its surface vertices are projected onto the computed envelope, and their
distance to the envelope is recorded (the previously constructed kd-tree is used again to speed up the projection).
Only the projected points of the closest 30% vertices are
then used to compute an average point. By choosing only
the closest 30% of the vertices, we have found that the resulting average point is close to the center of the cortex area
as seen from the outside, since its position is not affected
by the invisible vertices that belong to deep foldings of the
cortex. The average point is again projected onto the cortex
envelope to evaluate the surface normal at that position. It is
also projected onto the cortex surface itself to determine the
label position. This process is repeated for every cortex area.
The cortex labels are rendered in a fixed distance from the
cortex surface and oriented to always face the camera. A
dark semi-transparent label background is used to improve
the readability on arbitrary backgrounds (see Fig. 15).
6. Results

Figure 14: Both a surface point and an activation area are
selected. A ruler is used to show the distance (in mm).

Figure 15: Cortex Area Labels are displayed for orientation
purposes. Only one brain hemisphere is rendered to allow
for the inspection of otherwise occluded cortex parts.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Rendering Speed. All images presented in this paper were
rendered using a cognitive neurosciences dataset. Runtime
measurements were performed on a Windows XP machine
with an Athlon XP 2600+ precessor, 1.5 GB RAM and an
ATI Radeon X1950 Pro (8x AGP) graphics card. At program
startup, the watershed algorithm is performed to compute the
fMRI activation volume segmentation (approx. 5 seconds)
and a kd-tree is constructed for both brain hemispheres (approx. 22 seconds for 600.000 triangles). Each frame, a total
of 4.8 million triangles and 180 volume slices are rendered.
At an output resolution of 1024x1024 pixels, a frame rate of
3.5 frames per second (fps) is achieved for the scene shown
in Figure 14. At an output resolution of 512x512 pixels, a
frame rate of 7.9 fps is achieved. When displaying only one
cortex hemisphere as is shown in Figure 15, the frame rates
are 4.0 fps and 10.9 fps, respectively.
Evaluation. As an informal initial clinical evaluation of our
approach, the visualization was demonstrated to a neurosurgeon at the University Hospital Leipzig. The initial feedback
suggests that the possibility to explicitly select activation areas in the fMRI volume as well as brain surface locations
is considered to be a particularly useful aspect of our system. The associated highlighting of selected structures, and
especially the use of illustrative rendering styles, result in a
focus-and-context representation, reducing unneccesary visual clutter. Moreover, the functionality to accurately measure distances was deemed especially helpful, as was the
Freesurfer-based functional region segmentation.
The main suggestion for improvement is that the system

862

Jainek, et.al. / Illustrative Hybrid Visualization and Exploration of Anatomical and Functional Brain Data

should be capable of loading and displaying additional
anatomical information, for instance blood vessels and tumor tissue. This would facilitate the localization of the respective activation areas, in particular in the context of clinical diagnostics and therapy. In summary, in its current state
our system was evaluated as being an interesting tool for
neurosurgical research and a good basis for a future version,
which could be helpful for the clinical practice by presenting
additional anatomical information.
7. Conclusion
We have presented a novel approach to interleaving illustrative mesh and volume rendering at interactive frame rates.
This is achieved by splitting up the scene into different layers that are rendered individually and are later composited to
generate the final image. We apply this method to interleave
anatomical and functional brain data.
We have combined a number of techniques to keep the visualization output clear and uncluttered. These include illustrative visualization techniques, ambient occlusion, cortex surface coloring, as well as volume rendering of the activation
areas. Additionally, various interaction tools have been implemented to facilitate the exploration of the data. These include an intuitive tool to control the clipping plane, methods
to select a particular fMRI activation area and cortex surface
points, as well as a distance measurement tool. Finally, we
have implemented an algorithm for the automatic placement
of cortex surface labels.
We believe that the visualization and user interaction methods presented in this paper constitute a valuable addition
to the toolbox of neuroscientists and brain surgeons alike.
An initial informal evaluation shows that our visualization
method is deemed useful for clinical research. A feature that
we have not yet implemented is the integration of additional
anatomical information, such as blood vessels, tumor tissue
and other subcortical structures. We consider such an extension to be one interesting potential direction of future work.
Acknowledgements
The authors wish to thank Dirk Lindner and Christos Trantakis of the University Hospital Leipzig for their support
regarding the initial clinical evaluation of our system. We
would also like to thank Hanspeter Mallot for our discussions, his insights, and his advice.
References
[Ban07] BANDETTINI P.: Functional MRI today. International
Journal of Psychophysiology 63 (2007), 138–145.
[BB97] B ERNARDINI F., BAJAJ C. L.: Sampling and Reconstructing Manifolds using Alpha-Shapes. In Proc. of Canadian
Conference on Computational Geometry (1997), pp. 193–198.
[BHWB07] B EYER J., H ADWIGER M., W OLFSBERGER S.,
B ÜHLER K.: High-Quality Multimodal Volume Rendering for
Preoperative Planning of Neurosurgical Interventions. In Proc.
of IEEE Visualization (2007), pp. 1696–1703.

[Bra] B RAIN VOYAGER: BrainVoyager. Information on http:
//www.brainvoyager.com last visited Dec.6, 2007.
[DFS99] DALE A. M., F ISCHL B., S ERENO M. I.: Cortical
Surface-Based Analysis I: Segmentation and Surface Reconstruction. NeuroImage 9, 2 (1999), 179–194.
[EM92] E DELSBRUNNER H., M ÜCKE E. P.: Three-Dimensional
Alpha Shapes. In Proc. of Workshop on Volume Visualization
(1992), pp. 75–82.
[FBS05] F ISCHER J., BARTZ D., S TRASSER W.: Illustrative Display of Hidden Iso-Surface Structures. In Proc. of IEEE Visualization (2005), pp. 663–670.
[Fre] F REE S URFER: FreeSurfer. Information on http://
surfer.nmr.mgh.harvard.edu last visited Dec.6, 2007.
[FSD99] F ISCHL B., S ERENO M. I., DALE A. M.: Cortical
Surface-Based Analysis II: Inflation, Flattening, and SurfaceBased Coordinate System. NeuroImage 9, 2 (1999), 195–207.
[Jai07] JAINEK W. M.: Illustrative Visualization of Brain Structure and Functional MRI Data. Master’s thesis, Eberhard-KarlsUniversität Tübingen, 2007.
[JMS01] J EZZARD P., M ATTHEWS P., S MITH S.: Functional
MRI - An Introduction to Methods. Oxford University Press,
2001.
[KK99] K REEGER K., K AUFMAN A.: Mixing Translucent Polygons with Volumes. In Proc. of IEEE Visualization (1999),
pp. 191–198.
[PBF∗ 07] P FEIFLE M., B ORN S., F ISCHER J., D UFFNER F.,
H OFFMANN J., BARTZ D.: VolV - Eine OpenSource-Plattform
für die medizinische Visualisierung.
In Jahrestagung der
Deutschen Gesellschaft für Computer-und Roboterassistierte
Chirurgie e.V. (CURAC) (2007).
[RLF∗ 98] R EHM K., L AKSHMINARYAN K., F RUTIGER S.,
S CHAPER K. A., S UMNERS D. W., S TROTHER S. C., A NDER SON J. R., ROTTENBERG D. A.: A Symbolic Environment for
Visualizing Activated Foci in Functional Neuroimaging Datasets.
Medical Image Analysis 2, 3 (1998), 215–226.
[RM00] ROERDINK J. B. T. M., M EIJSTER A.: The Watershed
Transform: Definitions, Algorithms and Parallelization Strategies. Fundamenta Informaticae 41, 1-2 (2000), 187–228.
[RTF∗ 06] RÖSSLER F., T EJADA E., FANGMEIER T., E RTL T.,
K NAUFF M.: GPU-based Multi-Volume Rendering for the Visualization of Functional Brain Images. In Proc. of SimVis (2006),
pp. 305–318.
[SPM] SPM: SPM. Available on http://www.fil.ion.
ucl.ac.uk/spm/ last visited Dec.6, 2007.
[SRWE07] S CHAFHITZEL T., RÖSSLER F., W EISKOPF D.,
E RTL T.: Simultaneous Visualization of Anatomical and Functional 3D Data by Combining Volume Rendering and Flow Visualization. In Proc. of SPIE Medical Imaging: Visualization and
Image-Guided Procedures (2007), p. 650902.
[Ste03] S TEWART A. J.: Vicinity Shading for Enhanced Perception of Volumetric Data. In Proc. of IEEE Visualization (2003),
p. 47.
[TIP05] T IETJEN C., I SENBERG T., P REIM B.: Combining Silhouettes, Surface, and Volume Rendering for Surgery Education
and Planning. In Proc. of EuroVis (2005), pp. 303–310.

c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

