DOI: 10.1111/j.1467-8659.2008.01180.x

COMPUTER GRAPHICS

forum

Volume 27 (2008), number 8 pp. 2013–2027

Compression and Importance Sampling of Near-Field
Light Sources
Albert Mas, Ignacio Mart´ın and Gustavo Patow
Institut d’Informatica i Aplicacions, Girona, Spain

Abstract
This paper presents a method for compressing measured datasets of the near-field emission of physical light sources
(represented by raysets). We create a mesh on the bounding surface of the light source that stores illumination
information. The mesh is augmented with information about directional distribution and energy density. We have
developed a new approach to smoothly generate random samples on the illumination distribution represented by the
mesh, and to efficiently handle importance sampling of points and directions. We will show that our representation
can compress a 10 million particle rayset into a mesh of a few hundred triangles. We also show that the error of
this representation is low, even for very close objects.
Keywords: Near-field, complex light sources, light sources compression, importance sampling
ACM CCS: I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism

Traditionally, a far-field approximation has been used in
industry to model real light sources. A far-field representation
models a luminaire as an anisotropic point light source, and
assumes that the objects to be illuminated are located far
away. There are some established standards to represent farfield light sources, the most important being IESNA and
EULUMDAT [ANS02, bCL99].

In recent years there has been an important effort to improve real light source capture and representation methods
using near-field models. In this case, luminaries are modelled as extended light sources, and there is no assumption on
the distance of objects to be illuminated [Ash93, GGHS03a,
SS96]. However, the raw data produced by these capture
methods can produce several gigabytes of images. Captured
data can be compressed to a light field or interpolated to generate a rayset [AR98]. A rayset is a set of particles (point, direction) with equal energy and without spectral distribution,
emitted at the light source and captured in a virtual bounding surface (Figure 2). The capture process uses a goniophotometer that could be mounted on two rotating arms that
allow the device to capture the light coming from a source
from all possible directions (Figure 1), but other setups are
also possible.

However, far-field representations do not produce accurate results when objects are close to the light source. As an
example, a far-field representation of a bulb cannot be used
to compute the light distribution produced by a reflector, because the distance between the bulb and the reflector surface
is usually very small. This is crucial for inverse reflector
design applications, like in [EN91, PPV04, PP05].

Our method focuses compressing measured data of the
near-field emission of physical light sources in a way
that allows incorporating these sources within global illumination algorithms, such as photon mapping [Jen96].
Raysets are a convenient representation for these types
of algorithms because they provide a set of particles
that can be used directly for shooting. However, raysets

1. Introduction
One of the most important factors for accuracy and realism
in global illumination is lighting complexity. However, most
of the times non-measurement-based light sources are used.
This is reasonable in applications where physical accuracy
is not important, but it is critical in situations where we want
the lighting simulations to be as close as possible to the real
illumination.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and
Blackwell Publishing Ltd. Published by Blackwell Publishing,
9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main
Street, Malden, MA 02148, USA.

2013

Submitted February 2007
Revised May 2007
Accepted March 2008

2014

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources

lighting environment, so spatial information in the emission
of the light can be neglected. The light source is considered as a point light source and its emittance is modelled as
a non-uniform directional distribution. These measurements
are obtained using a gonio-photometer, and it is assumed that
the target distances are several times the maximum dimension of the source. Empirically, this distance is no closer than
five times the maximum width of the luminaire [Ash93].
The second category is the near-field representation, which
stores the complete directional and positional emittance information for the source. In recent years, there has been
a development in devices that are able to capture the nearfield light distribution from a light source [AR98, GGHS03a,
GGHS03b]. These devices use a digital camera and a robot
arm to take a set of photographs from the light source at
different angles (Figure 1).

Figure 1: Gonio-photometer RiGO [Rad] used to capture
the rayset from light source.

are usually too big (10-M particles) to be efficiently
sampled.
The main contribution of this work is a novel approach
for compressing dense raysets of near-field light source measurements into a set of point light sources, with no significant
loss of information. We also present an importance sampling
method to efficiently sample the emittance of the light source
from the compressed data. A rayset is limited to its original
elements, and therefore we are not able to generate new ray
positions or directions. The compression technique has the
benefit that such new ray positions and directions can be
generated.
The rest of the paper is organized as follows. We discuss previous work in Section 2. We present an overview of
our method in Section 3, present the compression method
in Section 4, and in Section 5 we explain how to perform
importance sampling on the light source. Then, we show the
results in Section 6. Conclusions are given in Section 7.

Near-field measurements can be represented as light fields
[LH96, GGSC96]. The first attempt to capture the illumination of a light source as a light field was using canned
light sources [HKSS98]. Using a representation similar to
[Ash95], the idea was to compute a light field for a synthetic
light source and then use it in a ray tracing environment.
The main drawback of the method is that they do not show
a way to importance sample the light field to use it in a particle shooting algorithm. Also, the method presented here
outperforms those regularly sampled representations at very
short distances, as the new method is specifically designed
to preserve spatial and directional information following the
distribution of the original rayset, something those representations cannot do.
More recently, in [GGHS03a] a method is presented for
light source acquisition and rendering. The method allows
for much faster rendering, and allows efficient sampling for
photon emission. However, their representation needs around
100 MB per light using an efficient memory representation.
Near-field measurements can also be represented as raysets. This representation is just a set of photons that represent the emittance of a light source. The rayset is computed
by interpolation from the set of images acquired during the
capture process. This representation is the industry standard
for optical illumination analysis software [Bre, Lam, OPT,
Rad]

3. Overview

2. Previous Work

Our method deals with rayset models. The goal is to highly
compress the dataset with a small error even for closely
illuminated objects.

Light source modelling from measured data can be classified
into two categories. The first one is the far-field representation
that is established as an industry standard (IESNA, EULUMDAT), and assumes large distances from the sources to the

A rayset consists of a list of pairs of a point and a direction
(see Figure 2). Thus, each particle has a location and an outgoing direction. Each pair in the list can be considered as an
exitant particle that comes from the measured light source,

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources

2015

Figure 2: A rayset is a set of particles (point + direction)
stored on a bounding surface. These particles represent the
light emission from a light source.

Figure 4: The process of transforming a rayset into a set of
anisotropic point light sources.
Figure 3: A regular subdivision of the sphere using spherical
triangles. From left to right, the images correspond to levels
1,2,3 and 4 of the subdivision.

all of them carrying the same energy. The particles are located on a virtual bounding surface that usually corresponds
to a convex surface like sphere or a cylinder, but different
providers use different supporting shapes. To take into account this variety, our algorithm is able to handle any sort of
surface as long as it is convex.
In a first step, we compute a partition of the initial rayset
into clusters using the particle locations and their directions.
For each cluster we compute a representative point, and an
average particle density, obtained from the particles included
in this cluster. In order to accurately capture the particle
density changes, the clustering produces more density in
areas with a rapid variation of particle density. Areas with
constant particle density will have less clusters. Once the
clustering is finished, we create an anisotropic point light
source for each cluster. The position is the representative
point, and the directional distribution is computed using the
directions of the particles of the corresponding cluster. We
use a simple constant basis function over a subdivision of the
sphere of directions into spherical triangles (see Figure 3) in
an hierarchical way.

Figure 5: Mesh produced from a 10-M rayset corresponding
to a OSRAM PowerBall bulb.
At the end of this process, we have a set of directiondependent point light sources (see Figure 4) located on the
virtual bounding surface of the light source. Then, we triangulate this obtaining a mesh representation of the bounding
surface (see Figure 5). This triangulation is used for importance sampling. Each time we sample the light source, we
first select a triangle and then we select a point inside the
triangle. Then, we sample one of the three anisotropic point
light sources corresponding to the vertices of the triangle for
sampling an outgoing direction. This way, we ensure that we
sample all the domain of possible outgoing directions.

4. Compressing the Near-Field
Rayset compression has two steps. The first one groups the
particles using a clustering technique. The second one creates
directional distributions over each cluster from the original
particle directional data. The result is a set of point light

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

2016

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources

After each iteration, the new cluster set is retriangularizated before a new iteration starts. Following, we explain in
more detail the first two steps.

4.1.1. Triangulation

Figure 6: Set of clusters (below) that represents the original
particle spatial distribution (above).

In first step, we need to triangulate the cluster representatives
that lie on the virtual bounding surface of the light source.
A cluster representative is the particle location belonging to
the given cluster Ci that is closer to the average location of
the subset of particles associated to the cluster. This average
location of a cluster is computed as
Ci =

sources that represents, in a simpler way, the original light
source.

1
Ni

Ni

Pj ,
j

where P j is the position of particle j, and Ni is the number of particles associated to cluster i. Then, we choose the
representative R i as:
Ri = {Pn : ∀m ∈ 1..Ni , m = n, Pm ∈ Ci
Pm − Ci > Pn − Ci }.

4.1. Clustering creation
The first step of our method is to group the original set of
particles into a set of clusters. At the end, each cluster will
have associated a subset of particles, and a representative
point. The goal is to have a higher cluster density where the
emittance is more variable, so we can correctly capture the
high frequencies on the emission distribution, both positional
and directional.
The algorithm starts with a very dense initial clustering
that is computed by creating an octree (see Figure 6). The
criterion for voxel subdivision in this first step is to have
a maximum number of particles in each voxel. This helps
avoiding a too fine initial discretization, which would lead to
an unacceptable computational cost. Our experiments show
that octrees with a number of leafs between 20 000 and 30 000
are enough. Once the octree has been created, each leaf voxel
of the octree corresponds to a cluster. Then, an iterative process removes unnecessary clusters until no cluster has to be
removed. The particles of the removed clusters are redistributed to the remaining clusters. Each of these iterations
has the following steps:
1. 3D triangulation of cluster representatives. This produces a mesh approximation of the luminaire virtual
bounding surface.
2. All clusters are traversed and marked for removal in
case that they are unnecessary. A cluster can be removed
if it does not help to capture detail. We will show the
specific criteria used below.
3. Cluster removal and particle redistribution. Particles of
the removed clusters are redistributed to the nearest
neighbouring cluster that has not been removed.

There are a lot of methods that calculate triangulated surfaces from point clouds, such as [SR01] or [AB99]. Unfortunately, most of these methods do not guarantee that all input
points are used as mesh vertices, considering some of them
irrelevant for mesh construction. On the one hand, as we are
dealing with clusters of carrying-energy photons, we cannot
neglect any point. So, we need a method that considers all
input points, because each point represents a cluster. On the
other hand, those methods have a high computational cost for
a simple surface like the ones used by most rayset providers.
Our method starts from the tetrahedralization of the point
cluster representation and the posterior elimination of tetrahedra sides that do not belong to the virtual bounding surface.
The tetrahedralization is done by the Delaunay algorithm
[She97]. The correct triangles are chosen in three steps. First,
all input points are duplicated and projected over two concentric bounding spheres with different radii, the original ones
over the inner sphere, and the duplicated ones on the exterior
one. Then, all points are tetrahedralized, and the tetrahedra
faces that have three vertices formed by points in the initial
set are chosen. Next, the selected triangles are reprojected
over the original volume surface, so the triangle edges are
projected over the surface. In Figure 7, a simplified 2D sectional illustration of this method can be observed.
This algorithm is only valid for star-shaped surfaces, but
the nature of the input models used in industry already guarantees that. Note that the centre of the star must be also
the centre of the bounding spheres. This algorithm works
for simple convex shapes, such as spheres or cylinders. For
more complex convex shapes, we could use the ball pivoting
algorithm [BMR∗ 99] with a large sphere.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

2017

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources

Figure 7: Simplified 2D sectional illustration of light source
bounding surface triangulation.

Figure 8: Top: neighbouring cluster representatives R j are
projected onto plane Si . Bottom: Each projected cluster representative Q j is augmented with its density values. A regression plane is computed with all dj values.

4.1.2. Cluster removal
This is the second step of the iterative process mentioned
before. Once the triangulation is finished, all clusters are traversed and tested for removal. The idea is that, if the density
of a cluster can be approximated by linear interpolation of
the neighbouring vertices, then the cluster can be removed
because the particle density is correctly represented without
it. Then, the particles are redistributed among its neighbours,
and the set is retriangulated (which is much more efficient
in this case than keeping information for an incremental update).
The algorithm consists of the following steps. For each
cluster Ci , a normal vector N i and a plane Si is approximated.
This approximation is computed performing a nearest neighbours search centred on the cluster representative R i . This
search of nearest clusters is accelerated using a kd-tree built
with the original particle data. Next, all adjacent cluster representatives R j in the triangulated mesh are projected onto
Si (see top of Figure 8). For each projected cluster representative Q j and for R i , and considering only the 2D projected
coordinates, the respective density values dj and di are added
as a third coordinate. Finally, a new regression plane Sr is
calculated for the new points (see bottom of Figure 8). If
the projection distance t between di and Sr is larger than a
user-defined threshold (called density threshold td ), the cluster cannot be removed from the mesh. The threshold is a
percentage of the maximum projection distance of neighbouring clusters. Otherwise, if this threshold test is passed,
the distance from the cluster to its neighbours is verified, in
a way such that the cluster will not be removed if the distance is larger than a given threshold, called edge filtering
threshold te . The edge filtering threshold is a percentage of

Table 1: Number of clusters, number of loops in iterative clustering
process, precomputation time and result memory usage for different
thresholds (angle difference, density and edge filtering threshold),
for the Osram PowerBall

ta
(deg)

td
(%)

te
(%)

Clusters

Loops

Time
(sec)

Size
(MB)

90
70
60
30
25

50
45
40
35
25

20
20
20
15
15

347
841
1680
4696
8776

29
38
48
57
51

476
428
496
531
642

1.2
2.6
5.8
16.6
31.6

the length of the longest edge of the bounding box, which
was observed to be a good distance measure. Finally, the
cluster’s mean emittance direction is compared with the ones
from its neighbours, and the cluster is removed if the angle
they form is smaller than a third threshold, the angle threshold ta . Observe that this last verification avoids collapsing
clusters on edges with sharp angles. Note also that this is
only a first approximation that works sufficiently well on our
experiments.
In Table 1, there are some results of clustering creation
method.

4.2. Creation of point light sources
Once the clustering is finished, we create a point light
source for each cluster Ci at its representative point. The

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

2018

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources

Figure 10: Uniform sampling over triangles produces artifacts (left). A continuous sampling over the edges of the mesh
avoids them (right).
As one of the main advantages of the rayset representation
is that it is very easy to use in light tracing algorithms (photon maps, radiosity, etc.), we want to maintain as much as
possible the accuracy of our compressed model (within userprovided thresholds), but at a fraction of the original storage
cost.
Figure 9: Adaptive spherical triangle subdivision for directional data of a cluster. Each point over triangles is a ray
direction in directional space.

approximation used is to accumulate all the particles to R i .
This will result in a set of directions centred at the point.
As the number of directions can be quite high, we have to
choose a more compact representation that can be efficiently
sampled with importance sampling.
We have used a piece-wise constant set of basis functions
defined over the sphere of directions in an hierarchical way.
The support of each basis function is a spherical triangle. The
sphere of directions is initially subdivided into eight spherical triangles, and then each triangle is recursively subdivided
using a quad-tree [Arv95]. We force this subdivision for a
fixed initial number of levels, so we get a uniform subdivision
of the sphere of directions (see Figure 3). Then, a new subdivision is done adaptively to perform a tightest directional
representation. For the raysets we have used to test our technique, we have found that from 3 to 5 levels of subdivision
are enough to get an accurate representation (see Figure 9).
Once the subdivision is created, we store at each spherical
triangle the number of original particles in the cluster with
directions that belong to the corresponding solid angle. This
number also represents the exitant energy through the corresponding solid angle. To improve compression, we only
store the triangles that have non-zero energy. Typically, less
than half of the triangles need to be stored as only half of the
sphere is pointing towards the source of the light.

5. Importance Sampling
One of the main goals of the rayset compression technique is
to be able to perform importance sampling on the light source.

In order to be able to sample the complete domain, we
create a triangulation of the bounding surface using the point
light sources as the vertices for such triangulation (see Section 4.1.1). Once we have created the triangulation, every
time we want to generate the 3D position of a particle, we
first have to choose a triangle. We construct a probability
density function (pdf ) for this, and each triangle is assigned
a given value proportional to its energy. We set this energy
to the amount of original particles that exit the light surface
through the given triangle, without taking into account the
densities computed for the respective vertices.
With this pdf , we can choose a triangle. Then, we have
to select a random point on the triangle. The straightforward
approach would be to choose a point following a uniform
distribution. But this poses another problem: the pdf that
results is not continuous over the edges of the mesh, resulting
in illumination artifacts. These artifacts are caused because
the spatial distribution generated by this approach changes
strongly from triangle to triangle (see Figure 10, left).
To solve this problem, we propose a non-uniform pdf that
is C 0 continuous over the edges of the mesh (see Figure 10,
right). The idea is to compute the density of particles at each
vertex of the mesh and perform a linear interpolation across
each polygon. For a triangle n, we define the pdf at a point x
in parametric space as (see Section 5.1)
pn =

2An

1
0

d0 + u(d1 − d0 ) + v(d2 − d0 )
,
1−v
(d0 + u(d1 − d0 ) + v(d2 − d0 )) du dv
0

where u, v are the coordinates of point p within the triangle,
d 0 , d 1 and d 2 are the densities of vertices V 0 , V 1 and V 2
(see Figure 11), and An is the area of the triangle.
Unfortunately, sampling values from pn is a complex task
due to the integration needed over the triangular domain,
which would require a slow numerical integration. In order to simplify computations, we created an instrumental

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources

2019

Figure 11: Plot of the pdf used for sampling a point inside a
triangle. We consider the function over the whole quadrilateral, but samples outside the triangle V 0 ,V 1 ,V 2 are rejected.
distribution p n , which is the extension of the previous pdf
to the whole unit square, and get our samples by the rejection sampling method [Bek99], see below. To guarantee the
positiveness of the new pdf , we choose the origin of the u,
v parameterization (vertex V 0 ) as the vertex with a lower
density, allowing us to be sure that the pdf is positive all over
the domain. The pdf also holds the condition that the integral
over the domain is 1:
pn (u, v) =

d0 + u(d1 − d0 ) + v(d2 − d0 )
2An

1
0

1
(d0
0

+ u(d1 − d0 ) + v(d2 − d0 )) du dv
(1)

If we want to generate random points proportionally distributed to this pdf , we have to apply the principles described
in [Scr66, Shi90]. Given two uniformly sampled random
numbers ru ∈ [0, 1] and r v ∈ [0, 1], the sample point is
−1
ui = F −1
0 (ru ), vi = F 1 (r v ), where functions F 0 and F 1 are
defined using a function F:
v

u

F (u, v) =

pn (u , v ) du dv
0

Figure 12: Not all of the random sample pairs ru , r v such
that ru + r v ≤ 1 produce u, v pairs such that u + v ≤ 1.
However, all the random pairs in the green area produce u,
v pairs that are not rejected.

because at least 50% of the samples are rejected. Even worse,
in case that d 0 is much lower than d 1 and d 2 , the number of
rejected samples can be much higher.
However, as vertex V 0 corresponds to the lower density,
it is clear that random numbers such that ru + r v > 1 will
always produce invalid samples (see Figure 12). Taking this
into account, we always generate uniform random points on
the triangle defined by equation ru + r v ≤ 1. Results show
that, by using this sampling strategy, the number of rejected
samples is very small: in our experiments, less than 10% of
the samples were rejected.
Figure 13 shows the original rayset point distribution (no
directions shown) and the point set produced by our sampling
technique.

0

and, from here:

5.1. PDF for position sampling
F0 (u) = F (u, 1),
F1 (v) =

F (ui , v)
.
F (ui , 1)

Solving these integrals gives a quadratic polynomial that
can be easily evaluated. Unfortunately, this mechanism produces samples all over the domain ui ∈ [0, 1] , vi ∈ [0, 1]. That
means that we have to apply rejection sampling and reject
samples that verify ui + vi > 1. This can be very inefficient

In order to have a continuous pdf defined all over the light
surface domain, it is necessary to propose a well-defined,
continuous global function, and normalize it. We decided to
use a piece-wise linear function defined over each triangle of
our triangulation: at each vertex P i , we require the evaluation
of the global function to be f (P i ) = di , where di is the
density associated with the corresponding vertex. Continuity
is granted over different triangles as triangles with common
vertices will use the same di values. This way, continuity is
C ∞ within the interior of the triangles and C 0 at the edges.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

2020

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources

where the integration domain Trin is the lower half triangle in
is the Jacobian of the transformation
the unit square, and ∂(x,y)
∂(u,v)
[Wu99].
It is important to notice that computing the absolute value
of the determinant of the Jacobian gives the same expression
as 2An , twice the area of the triangle An = 1/2 |(r j −r i ) ×
(r k −r i )|, where r i , r j and r k are the three vertices of the
triangle.
Now, we can proceed with our integration
N

I =2

1

N

1−v

fn (u, v)dudv = 2

An
0

n=0

0

An In .
n=0

Then, we can write our final pdf as
N
n=0

pdf =

fn (r)δ(
I

n , r)

,

where δ ( n , r) is 1 over the triangular domain n and zero
everywhere else. In order to have a clearer sampling strategy,
we can multiply and divide each term by An In , what will give
N

pdf =
n=0

An In fn (r)δ( n , r)
I
An In

N

=
n=0

Figure 13: Comparison of original rayset with importance
sampled set. Top image shows the point distribution of the
original rayset, and bottom image shows the point distribution generated by our sampling technique.

fn (u, v) = (d1 − d0 )u + (d2 − d0 )v + d0 .
Now that we have our function defined over all the triangles i, we can compute the global normalization, defined as
the integral I = f (r)dr where the domain is the triangulated surface we got. This expression is nothing else than
the sum over all N triangles, each with domain n :

An In fn (r)δ( n , r)
,
An In
An In

which clearly is a linear combination of pdf s [Bek99]. Actually, we can say that it is a linear combination of N primary
pdf ’s pn :
pn =

In the u, v parameter space, we can formulate an expression
for the n-th triangle:

2

fn (r)δ( n , r)
f (r(u, v))
= n
.
2An In
2An In

To be able to sample the pdf , a primary pn is selected first
with probability

and next a sample is drawn using pn . So, the final expression
for the pdf becomes

N−1

I=

N

f (r)dr =

fn (r)dr
n=0

n

N

fn (r(u, v))
n=0

T rin

pdf =

p(n)pn .
n=0

by changing variables to the unit square, we get that this is
equal to
I=

An In
An In

p(n) =

∂(x, y)
dudv,
∂(u, v)

Although only a single primary pdf is sampled, the result
of that sampling is obtained following the combined pdf .
Obviously, when sampling pn the δ( n , x, y) factor can be
omitted, as it was built to be 1 all over the domain.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources

2021

5.2. Sampling directions
Sampling a direction involves using importance sampling on
the three-directional distributions stored at the vertices of the
triangle.
Once we have selected a point x, we compute its barycentric coordinates u, v and 1 − u − v. As their sum is 1,
we can use these values to construct a small CDF for the
three vertices. This allows us to select one vertex and its
corresponding point light source. Then, we can perform importance sampling on the point light source by creating a
CDF from all the spherical triangles. With the CDF, we can
select a spherical triangle and then sample it uniformly with
respect to the solid angle.

Figure 14: Real-measured raysets. At top, the OSRAM
PowerBall model. At bottom, the Tungsten Halogen model.

5.3. Direct illumination
On the one as explained above, the light source is represented
by a bounding geometry, which can be sampled to find the
illumination at a given point. But, for lights with a high
directional variation, this approach can be quite inefficient.
Also, the highly directional distributions that can be generated with our method preclude the use of this technique in
conjunction with Lightcuts [WFA∗ 05], which is a technique
valid for isotropic point lights or directional lights with a
cosine distribution which cannot model those distributions.
So, we decided to separate the Global Map of the
traditional Photon Map algorithm into two, as done in
[GGHS03a]. On the one side, we create a Direct Map that
stores direct impacts from the light sources, and on the other,
a new Global Map that stores impacts after one diffuse reflection. So, two different possibilities arise to compute the
direct illumination: to reconstruct the illumination directly
from the Direct Map, or use the Direct Map for generating importance information to guide a standard Monte Carlo
lighting integration. The option of directly using the Direct
Map promises to be efficient for highly directional sources,
which can be made even better by storing a large quantity
of photons in the Direct Map, while keeping a low quantity
for the Global Map. Of course, this involves the limitation
of the integration to the areas where there are direct photons,
or the construction of an importance sampling table from the
nearest particles in the Direct Map [GGHS03a].
6. Results and Discussion
We have tested our method with two raysets corresponding
to real measurements. Both of them have 10 million particles. The first one corresponds to a OSRAM PowerBall
bulb (courtesy of Lambda Research), and the second one
is a Tungsten Halogen bulb (Radiant Imaging demo) (see
Figure 14).
Also, we have tested four synthetic raysets (see
Figure 15), sampling 10 million particles for each one in

Figure 15: Synthetic tested raysets: Phong (top left corner,
Phong exponent = 500), Phong directional pattern (top right
corner, Phong exponent = 25), radial with pattern (bottom
left corner) and cosine with pattern (bottom right corner,
exponent = 1) distributions.
a uniform way. The first one has a Phong distribution over a
sphere. We use the same sampling method than [LW94] to
construct the Phong distribution. The second one has Phong
distribution, but with a directional pattern distribution. The
other two are radial and a cosine ray direction distribution
over the sphere, but with a positional pattern onto the sphere
of origins. These synthetic raysets are used to check the
performance of our method in different conditions, such as
high frequencies in ray positions or directions. It is specially
interesting that the case of the Phong directional pattern distribution, which leads to a triangularization which is shown
in Figure 16, showing that the angle threshold criteria for
the triangularization effectively preserves the discontinuity
in the distribution.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

2022

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources

Figure 16: The resulting triangulation of the Phong directional distribution with a directional patterns shown on the
right top part of Figure 15.
Figures 17 and 18 show false color images for particle
emission experiments. The images represent the energy arriving at a plane that is located 1 mm from the bounding
surface of the rayset. There is a side by side comparison between the 10e6 original particles and a 10e6 particle emission
using importance sampling. Also, difference images are displayed for each one. Observe that, with the above-explained
method, no photons are generated inside the bounding surface or pointing inwards from it, so any surface intersecting
its interior will not receive any hit. Actually, this cannot be a
problem because this is the space physically occupied by the
light bulb itself.
We have tested different compression levels for each one of
our raysets. The memory sizes have been reduced drastically,
as it can be seen in Figure 19, because the rayset representation of all of these models have a memory consumption of
about 270 MB. In Figures 20 and 21 there are some results of
the OSRAM PowerBall rayset using different compression
levels and measuring the error at different distances. Two
error metrics have been used: l 2
N

Dl 2 (a, b) =

In Figure 20, it can be observed how the error decreases
as the number of clusters increases, in the same way for each
tested distance.

(ai − bi )2
i

and Hellinger [RFS03],
2

DHellinger (a, b) =
with similar behaviour on results.

N
i

−

ai
N

2

Figure 17: Images of 10 million particles gathered on a
plane situated at 1 mm of the bounding surface. First row corresponds to the OSRAM PowerBall bulb with a compressed
data of 1680 clusters (see Table 2). Second row corresponds
to the Tungsten Halogen bulb with a compressed data of 452
clusters. In columns, from left to right, the images correspond
to original rayset, sampled compressed data, difference image (error), and scaled difference image, respectively (both
at x5). Under the false color images you can find the scale
used, normalized over the entire set of positions/directions.

bi
N

In Figure 21, three zones of interest are shown. The first
one is the error obtained at near distances, as 1 mm. In this
case, the importance sampling positional error is the main
contributor to the overall error. The second one is the error
obtained at large distances. Here, the directional sampling
error was found to be the main source of error. The third

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources

2023

Figure 19: Relationship between number of clusters and
memory usage for OSRAM PowerBall.

case is the peak observed at distance 300 mm. To explain
it, we have traced the particles of original rayset on a set of
bounding spheres of different radii. The results (Figure 22)
show that, at distance 300 mm, it can be observed a pattern
over sphere. This is because the pattern of the acquisition
mechanism that has been used to obtain the rayset can be
found in this region. If the gonio-photometer used in acquisition system uses photosensors placed over a virtual bounding
sphere, then the gathering distance (bounding sphere radii) is
300 mm. So, each accumulation point in the pattern corresponds to each photosensor position in the acquisition process.

different numbers of clusters, and at different distances from
the light sources. All models show similar behaviours to the
previously explained one. As a reference, we have included
for each rayset the errors for a far-field distribution created
with the original raysets, but using 2048 spherical triangles,
as a distribution with less spherical triangles fails to keep the
different pattern details. As we can see, the new compression
method outperforms the far-field representation at very short
distances, also demonstrating that a far-field representation
is unsuited for real light bulbs at short distances, as they cannot be approximated by an anisotropic point light. At large
distances compared with the size of the light bulb, both converge to the same values, showing that for those distances
it is better to use a far field representation, because of the
easy evaluation. However, many applications (e.g. reflector
design) require evaluations at short distances, where a farfield is clearly not good enough. One further point should be
noted: all measurements in Table 2 have been evaluated by the
procedure described in Section 5, so they have a variance associated. The variance depends on the emitting distribution,
the more diffuse, the more variance, as shown in [PPV04].
So, the measurements have a variance, which we have measured to range from ±31 and ±64 for the Tungsten Halogen
and the OSRAM Powerball, respectively, to values of ±125
for the cosine pattern (which is like a Phong lobe with exponent k = 1), of ±64 for the radial pattern, of ±41 for
the Phong and Phong pattern distributions (with k = 500).
This variance is enough to explain some strange behaviours
at large distances for some distributions, as the values plus
their respective variances overlap, as happens for the cosine
pattern distribution at 100 and 1200 mm. Also, in Table 2,
we have included the resulting sizes of each compressed set,
clearly showing the much lower memory usage required by
our compression method.

All the other models have also been tested. In Table 2, there
is a summary of l 2 error values obtained for each rayset, with

To prove that the representation is accurate enough for
cases such as reflector design, we show in Figure 23 a set of

Figure 18: Images of 10 million particles gathered in a plane
situated at 1 mm of the bounding surface. First row corresponds to the Phong synthetic rayset, using a compressed
data of 1597 clusters (see Table 2). Second row corresponds
to the Phong Pattern synthetic rayset, using a compressed
data with 1146 clusters. Third row corresponds to the radial
pattern synthetic rayset, using a compressed data of 4454
clusters. And the fourth row corresponds to the Cosinus Pattern synthetic rayset, using a compressed data of 2244 clusters. In columns, from left to right, the images correspond to
original rayset, sampled compressed data, difference image
(error), and scaled difference image, respectively, (phong
model at x8, and the others at x3). Under the false color images you can find the scale used, normalized over the entire
set of positions/directions.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

2024

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources

Figure 20: OSRAM PowerBall Hellinger errors for different measurement distances in function of number of clusters .

Figure 21: OSRAM PowerBall l 2 errors for different number of clusters in function of measurement distances.

Figure 22: Left: Acquisition system scheme. Right: Ray gathering over bounding spheres at different distances. The observed
pattern at distance of 300 mm corresponds to photosensor distance, and each shot accumulation is each photosensor placement.
c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

2025

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources
Table 2: Summary table of memory storage needs and

Ray set

Clust.

Tungsten
Halogen

Phong
Pattern
k = 25
Phong
k = 500

Radial
Pattern

Cosine
Pattern

347
1680
8776
FF
452
1160
2702
FF
432
658
1146
FF
430
1597
5349
FF
350
1431
4454
FF
2244
3782
8177
FF

errors for the tested ray sets

Size
(MB)

Osram
PowerBall

l2

1.2
5.8
31.6
0.017
0.66
2
2.5
0.017
0.94
1.4
3.2
0.017
0.76
2.2
6
0.017
0.35
0.7
1.3
0.017
16.9
29.9
71.5
0.017

Distances (mm)
1
6682.60
5705.55
4455.03
12 567.00
6222.04
4664.81
4460.94
6215.19
10 672.50
9085.03
5844.87
50 999.89
4609.50
4419.81
4281.86
4439.08
27 099.80
20 322.60
10 765.80
18 449.10
6687.94
5300.57
4760.15
22 778.30

5
5726.10
5258.51
4525.02
9957.12
6559.96
4923.44
4713.48
5764.81
12 590.80
10 265.20
6301.60
48431.90
4647.55
4469.87
4367.94
4413.14
27 008.30
20 253.20
10 794.50
18 436.50
4544.10
4381.88
4248.68
15 483.10

10
5566.62
5306.23
4702.06
8288.00
6866.29
5159.80
4948.81
5595.39
11 269.40
8963.22
5876.75
45 218.50
4706.65
4550.48
4474.97
4408.29
26 889.60
20 166.90
10 828.90
18 420.40
4433.10
4391.92
4287.43
11 775.30

50

100

6188.14
5731.51
5224.36
5356.63
7911.24
5672.74
5068.34
5456.35
7594.12
6897.97
6179.90
27 912.10
5426.99
5207.57
5121.33
4362.60
26 619.90
19 812.40
10 675.10
18 376.50
4430.48
4384.90
4375.61
5671.10

6488.13
5884.42
5328.28
4964.11
8383.64
5875.72
5159.71
5576.94
7424.74
7224.52
6744.38
18 765.60
5977.92
5553.68
5207.98
4328.09
26 634.10
19 765.60
10 432.10
18 364.00
4461.37
4423.53
4483.85
4850.43

300
10 057.50
9463.65
8960.63
8850.31
8888.70
6125.81
5265.77
5575.54
7288.99
7191.77
6669.15
9441.57
6555.81
5872.38
5032.52
4316.66
26 762.00
19 892.40
10 148.10
18 359.10
4711.09
4703.22
4625.79
4584.34

1200
6858.35
5969.38
5011.70
4741.15
9167.13
6293.39
5369.85
5640.79
7232.32
7023.90
6430.00
6165.64
6811.54
6033.69
4975.92
4291.46
26 858.60
20 041.90
10 080.50
18 354.90
4749.27
4883.37
4998.64
4435.97

3000
6754.26
5807.02
4890.81
4586.17
9227.08
6281.38
5329.95
5638.35
7268.94
7056.02
6442.95
5894.46
6874.29
6070.29
5007.21
4288.56
26 888.90
20 083.60
10 078.10
18 354.10
4471.47
4550.63
4542.61
4340.02

10000
6714.48
5768.99
4831.09
4530.26
9251.94
6274.02
5345.32
5600.29
7277.87
7054.05
6478.13
5863.09
6905.22
6091.30
5028.44
4294.95
26 904.40
20 105.70
10 069.90
18 352.00
4429.14
4449.72
4391.55
4308.85

Three representative cluster solutions and a far-field (FF) representation have been tested for each ray set at different distances from the light
source. The far-field spherical triangle subdivision is similar for each case, so the memory usages differ in a few bytes.

Figure 23: Lighting from a reflector with the OSRAM Powerball mounted in. At left, the reflector and bulb setup, and the plane
used to gather the lighting. Next, from left to right, the lighting using the original rayset, using the compressed rayset (1680
clusters, see Table 2) and using only the bulb farfield.
renderings of the OSRAM PowerBall bulb model mounted
in a reflector, illuminating a plane. We have used three representations of this bulb: the original rayset, the compressed
rayset and the farfield. The compressed model has 1680 clusters (see Table 2).
Finally, we have rendered some examples using the Mental Ray Renderer on Maya. To do it, we have developed a
plugin that works as interface between our compressed rayset

and the Maya rendering system. In Figure 24, you can see
a comparison between two Photon Mapping results (without gathering), one using the original rayset, and the other
using our compressed rayset, both placed in a near (1 mm)
bounding box around the light source bounding volume. The
figures are rendered using only the Direct Map mentioned in
Section 5.3. The figure uses the OSRAM Powerball example,
which has a bounding cylindrical shape (70 mm length and
20 mm diameter dimensions). There are some gaps on the

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

2026

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources

distributions over the mesh with almost no artifacts, even for
very close objects.
It is important to notice that our importance sampling
method for smoothly generated random points on the mesh
can be used for other applications. Any point cloud distributed over a surface can be represented using our approach.
This is one of our future research lines.
The mesh representation is also very efficient in storage
terms. High precision raysets can contain up to 10 million
particles. The storage needed for this representation is about
270 MB, while our equivalent mesh representation uses up
to a few megabytes.
We consider, as future work, the use of non-constant
piece-wise linear functions to represent the directional distributions, such as wavelets or spherical harmonics. Also, a
more discriminant comparisions in clustering process, such
as mean directions, can be improved.
Finally, a GPU hardware implementation of the sampling
algorithm is another of our future research lines.
Figure 24: Photon Map results (without gathering). At top
there are the original rayset result. At bottom there is our
compressed rayset result.
illumination of the sides of the box for original rayset, because very few photons are emitted at cylinder caps. These
gaps disappear on compressed rayset results, due to an insufficient sampling, creating a smooth filtering effect.
6.1. Discussion
The most time and storage consuming part of our method
is the directional distribution management. Obtaining the
spherical triangle from a given direction means descending
through the levels of the subdivision, and this is a costly
operation. It must be taken into account that this kind of directional non-analytical representations always have a higher
cost than analytic distributions [LRR04, MPBM03].
On the other hand, directional distributions are also
very storage consuming. In our technique, we have the
advantage that, with a low number of point light sources,
we can characterize the illumination distribution of the bulb
and, as we only store information of the spherical triangles
with non-zero energy, the total memory used is not too high
in comparison.
7. Conclusions and Future Work
We have presented a novel approach for compressing nearfield light source measurements. From a dense rayset, we
create a mesh with a relatively low number of triangles that
contains illumination information. One of the main contributions is the method to perform importance sampling in this
mesh representation. Our method allows for smooth point

Acknowledgments
We would like to thank Radiant Imaging for the rayset models, and the anonymous reviewers for their useful comments.
This work was done under grant TIN2004-07672-C03 and
the Ram´on y Cajal program, from the Spanish Government.
References
[AB99] AMENTA N., BERN M.: Surface reconstruction by
voronoi filtering. Discrete Computational Geometry 22, 4
(1999), 481–504.
[ANS02] ANSI/IESNA: Lm-63-02. ANSI approved standard file format for electronic transfer of photometric data
and related information, 2002.
[AR98] ASHDOWN I., RYKOWSKI R.: Making near-field photometry practical. Journal of the Illuminating Engineering
Society 27 (1998), 67–79.
[Arv95] ARVO J.: Stratified sampling of spherical triangles. In SIGGRAPH 1995: Proceedings of the 22nd Annual Conference on Computer Graphics and Interactive
Techniques (New York, NY, USA, 1995), ACM Press,
pp. 437–438.
[Ash93] ASHDOWN I.: Near-field photometry: A new approach. Journal of the Illuminating Engineering Society
22, 1 (1993), 163–180.
[Ash95] ASHDOWN I.: Near-field photometry: Measuring
and modeling complex 3-D light sources. In ACM SIGGRAPH 1995 Course Notes – Realistic Input for Realistic
Images (1995), pp. 1–15.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

A. Mas et al./Compression and Importance Sampling of Near-Field Light Sources

[bCL99]
byHeart Consultants Limited: Eulumdat
file format specification, 1999. http://www.helios32.
com/Eulumdat.htm. Accessed on August 2008.
[Bek99] BEKAERT P.: Hierarchical and Stochastic Algorithms for Radiosity. PhD thesis, Department of Computer Science, Katholieke Universiteit Leuven, Leuven,
Belgium, 1999.
[BMR∗ 99] BERNARDINI F., MITTLEMAN J., RUSHMEIER H.,
SILVA C., TAUBIN G.: The ball-pivoting algorithm for surface reconstruction. IEEE Transactions on Visualization
and Computer Graphics 5, 4 (1999), 349–359.
[Bre]
Breault Research Organization:
breault.com/. Accessed on August 2008.

http://www.

[EN91] ENGL H. W., NEUBAUER A.: Reflector design as an
inverse problem. In Proceedings of the Fifth European
Conference on Mathematics in Industry (Teubner, Stutgart, 1991), M. H., (Ed.), pp. 13–24.
[GGHS03a] GOESELE M., GRANIER X., HEIDRICH W., SEIDEL
H.-P.: Accurate light source acquisition and rendering. In
ACM SIGGRAPH 2003 (2003).
[GGHS03b] GRANIER X., GOESELE M., HEIDRICH W., SEIDEL
H.-P.: Interactive visualization of complex real-world light
sources. In Proceedings of Pacific Graphics 2003 (Canmore, Alberta, October 2003).
[GGSC96] GORTLER S. J., GRZESZCZUK R., SZELISKI R.,
COHEN M. F.: The Lumigraph. In Computer Graphics
Proceedings, Annual Conference Series, 1996 (ACM SIGGRAPH 1996 Proceedings) (1996), pp. 43–54.
[HKSS98] HEIDRICH W., KAUTZ J., SLUSALLEK P., SEIDEL H.P.: Canned light sources. In Rendering Techniques 1998
(Proceedings of Eurographics Rendering Workshop ’98)
(1998), Drettakis G., Max N., (Eds.), Springer, Wien,
pp. 293–300.
[Jen96] JENSEN H. W.: Global Illumination Using Photon
Maps. In Rendering Techniques ’96 (Proceedings of the
Seventh Eurographics Workshop on Rendering) (1996),
Springer-Verlag/Wien, pp. 21–30.
[Lam]
Lambda Research, Inc.: http://www.
lambdares.com.. Accessed on August 2008.
[LH96] LEVOY M., HANRAHAN P.: Light Field Rendering. In
Computer Graphics Proceedings, Annual Conference Series, 1996 (ACM SIGGRAPH 1996 Proceedings) (1996),
pp. 31–42.
[LRR04] LAWRENCE J., RUSINKIEWICZ S., RAMAMOORTHI R.:
Efficient BRDF importance sampling using a factored representation. In ACM SIGGRAPH 2004 (August 2004),
pp. 496–505.

2027

[LW94] LAFORTUNE E., WILLEMS Y.: Using the Modied
Phong Reflectance Model for Physically Based Rendering.
Technical Report cw197, Department of Computer Science, Katholieke Universiteit Leuven, Leuven, Belgium,
1994.
[MPBM03] MATUSIK W., PFISTER H., BRAND M., MCMILLAN
L.: A data-driven reflectance model. In ACM SIGGRAPH
2003 (August 2003), pp. 759–769.
[OPT] OPTIS, Inc.: http://www.optis-world.com/.
Accessed on August 2008.
[PP05] PATOW G., PUEYO X.: A survey of inverse surface design from light transport behavior specification. Computer
Graphics Forum 24, 4 (2005), 773–789.
[PPV04] PATOW G., PUEYO X., VINACUA A.: Reflector design from radiance distributions. International Journal of
Shape Modelling 10, 2 (2004), 211–235.
[Rad]
Radiant
Imaging,
Inc.:
http://www.
radiantimaging.com. Accessed on August 2008.
[RFS03] RIGAU J., FEIXAS M., SBERT M.: Refinement criteria based on f-divergences. In EGRW ’03: Proceedings
of the 14th Eurographics Workshop on Rendering (Airela-Ville, Switzerland, Switzerland, 2003), Eurographics
Association, pp. 260–269.
[Scr66] SCREIDER Y.: The Monte Carlo Method. Pergamon
Press, New York, N.Y, 1966.
[She97] SHEWCHUCK J.-R.: Delaunay Refinement Mesh
Generation. PhD thesis, School of Computer Science,
Carnegie Mellon University, 1997.
[Shi90] SHIRLEY P.: Physically Based Lighting Calculations for Computer Graphics. Ph.D. thesis,
November 1990.
[SR01] FLOATER S. M., REIMERS M.: Meshless parameterization and surface reconstruction. Computed Aided Geometric Design 18 (2001), 77–92.
[SS96] SIEGEL M. W., STOCK R.: A general near-zone light
source model and its application to computer automated
reflector design. SPIE Optical Engineering 35, 9 (1996),
2661–2679.
[WFA∗ 05] WALTER B., FERNANDEZ S., ARBREE A., BALA K.,
DONIKIAN M., GREENBERG D. P.: Lightcuts: A scalable approach to illumination. In SIGGRAPH 2005: ACM SIGGRAPH 2005 Papers (New York, NY, USA, 2005), ACM,
pp. 1098–1107.
[Wu99] WU K.: The Transformation Between Positional
Space and Texture Space. Hpl-1999-103 technical Report,
HP Labs, 1999.

c 2008 The Authors
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

