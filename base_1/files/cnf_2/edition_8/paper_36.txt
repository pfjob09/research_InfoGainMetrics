Volume 27 (2008), Number 2

EUROGRAPHICS 2008 / G. Drettakis and R. Scopigno
(Guest Editors)

Expressive Facial Gestures From Motion Capture Data
Eunjung Ju and Jehee Lee
Seoul National University†

Abstract
Human facial gestures often exhibit such natural stochastic variations as how often the eyes blink, how often
the eyebrows and the nose twitch, and how the head moves while speaking. The stochastic movements of facial
features are key ingredients for generating convincing facial expressions. Although such small variations have
been simulated using noise functions in many graphics applications, modulating noise functions to match natural
variations induced from the affective states and the personality of characters is difficult and not intuitive. We
present a technique for generating subtle expressive facial gestures (facial expressions and head motion) semiautomatically from motion capture data. Our approach is based on Markov random fields that are simulated in
two levels. In the lower level, the coordinated movements of facial features are captured, parameterized, and
transferred to synthetic faces using basis shapes. The upper level represents independent stochastic behavior of
facial features. The experimental results show that our system generates expressive facial gestures synchronized
with input speech.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Three-Dimensional Graphics and Realism]:
Animation

1. Introduction
Human facial expression is dynamic in nature, often exhibiting natural stochastic variations. The emotional state and individual personality are closely related to such natural variations as how often the eyes blink, how often the eyebrows
and the nose twitch, and how the head moves while speaking. The stochastic movements of facial features are key ingredients for creating expressive conversational agents.
Virtual characters in many video games and virtual environments often employ noise functions [Per95, Per07] to incorporate noise-like subtle movements into facial expression.
Adding smooth temporal noise patterns makes an animated
character look less like a mannequin. However, modulating
noise functions to simulate natural variations induced from
the emotional state and the personality of characters is neither easy nor intuitive.
An alternative approach we are pursuing is to capture the
stochastic variational patterns from live human actors and
use the recorded patterns to add life-like subtle movements
† e-mail: {ejjoo,jehee}@mrl.snu.ac.kr
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.
Published by Blackwell Publishing, 9600 Garsington Road, Oxford OX4 2DQ, UK and
350 Main Street, Malden, MA 02148, USA.

Figure 1: Expressive facial expressions and head motions
are captured, parameterized, and transferred to the synthetic
face.

into a synthetic face and its head motion. Such stochastic
patterns are typically modelled as Markov random fields,
which pose several challenges for addressing issues specific to facial expression. Facial features sometimes move
in a strongly coordinated manner in order to convey specific expressions, whereas some facial features look rather

382

Eunjung Ju & Jehee Lee / Facial Gestures From Motion Capture

independent and stochastic. For example, a smiling face is a
combination of smiling eyes, brows, and a mouth that pose
and move in harmony. On the other hand, small expressive
variations of eye brows, eye blinking, and the head motion
are stochastic and not strongly coordinated with the other
facial features. These coordinated facial gestures and independent facial gestures should be considered simultaneously.
We found that the subtle change of facial expressions in conversation is often synchronous with the change of acoustic
features such as the intensity of speech sounds rather than
more detailed prosodic features such as phonemes.
In this work, we present an intuitive, easy-to-use facial
animation system that generates expressive facial gestures
semi-automatically. Even though face motion capture data
are used for synthesizing the stylistic variations and mood
of facial expression, the animated face is not necessarily human-like, but can be extremely exaggerated or even
cartoon-like. Recorded speech data can also be employed to
generate synchronized lip movements and drive the stochastic process of facial feature movements. Given any speech
data, our system allows the user to create lively animated
talking faces in a few minutes.

2. Related Work
Since Perlin [Per85] introduced his well-known noise function in 1985, the noise function has been employed in a wide
variety of graphics applications. In particular, Perlin and his
colleagues used the noise function to make animated characters and their faces appear more natural by adding small
variations in their movements [Per95, PG96]. Similar ideas
have been exercised in a wide variety of applications such as
personalized idle motion synthesis [EMMT04].
A great deal of previous research on facial animation involve lip-synching, which refers to synthesizing facial motion that is synchronized with input speech [Bra99, BCS97,
CTFP05,CB05,CMRH04,DN06,EGP02,KMT03]. Most approaches used phonemes as elementary speech units and intended to generate realistic lip movements driven by a sequence of phonemes. Our synthetic faces are also driven by
input speech, but the generation of realistic lip movements
is not a major focus of this work. The goal of this work
is to reproduce small variations in facial gestures (including facial expressions and head motion) that convey the affective states, mood, and personality of the character. The
strong interrelation between facial gestures and prosodic
features has been reported in the speech processing literature [BN07, BDG∗ 07, YRVB98]. However, the interrelation
between facial gestures and individual phonemes is not obvious. Our main focus is to synthesize facial gestures possibly
driven by acoustic features of input speech. For the fidelity of
the resulting facial animation, supplementary lip movements
are generated separately and blended in later by analyzing
phonemes from input speech.

Figure 2: Facial motion capture.

Facial expression synthesis has been extensively explored in
the computer graphics community. Facial expressions can be
procedurally defined [BB02, CPB∗ 94, ZLGS03], physically
simulated [LTW95, SSRMF06], or defined using a basis of
shapes [CB05, JTDP03, PKC∗ 03]. Our approach makes use
of basis shapes for capturing and synthesizing facial expressions because of their versatility and flexibility. The basis
shapes can be manually defined [PKC∗ 03,ZLGS03] or automatically identified from a stream of training data via component analysis [CTFP05, CXH03, MKPG05] or factorization [CB05, VBPP05]. Although existing methods can generate convincing static facial expressions and simulate the
short-term dynamics of facial action units successfully, only
a few of them addressed the problem of capturing and simulating the temporal stochastic movements of facial features
in extended expressive motions.
A popular way of modeling temporal stochastic behavior is
based on Markov processes, which were employed for animating fullbody motions using motion databases [LCR∗ 02].
The Markov process with motion data entails data structures, called motion graphs, which can also be used for synthesizing facial animation [CTFP05, ZSCS04]. We present
a novel two-level representation based on Markov random
fields. In the lower level, the coordinated movements of facial features are captured, parameterized, and transferred to
synthetic faces using basis shapes. In the upper level, independent stochastic behavior of facial features are simulated.
3. Data Collection and Processing
To acquire realistic human facial motion data, we used a Vicon optical motion capture system. 12 cameras tracked 68
retro-reflection markers on the face and 7 markers on the
head at the rate of 120 frames/second (see Figure 2). Facial motion data thus captured are then down-sampled to 30
frames/second for realtime display. We recorded 12 subjects
reading fairy tales and conversing freely at 6 different emotional states (neutral, depressed, delighted, annoyed, exaggerated, exasperated) for about a minute for each. We also
captured static facial expressions (neutral, sad, happy, afraid,
angry, surprise) for each subject. The rigid transform of the
head motion is computed using 7 head markers and the coordinates of 68 facial markers are represented with respective to the head reference system. Three of our subjects are
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

383

Eunjung Ju & Jehee Lee / Facial Gestures From Motion Capture
A Basis of Expressions

Extract Blend Weights

Select Target Sound Data

Phoneme Analysis
/a/

Sad

1118 1 0.000000
1119 1 0.000000
1120 1 0.000000
1121 1 0.000000
1122 1 0.000000
1123 1 0.000000
1124 2 0.000000
1125 1 0.000000
1126 1 0.000000
1127 1 0.072770
1128 1 0.212257
1129 1 0.138812

Surprise
Neutral

Angry

Happy

Afraid

Surprise

Sad

Happy
Afraid

Extract Head Motion

Angry

/e/

/i/

/o/

0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000

0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000

0.154844
0.212327
0.268810
0.356541
0.346298
0.319142
0.294191
0.197678
0.130844
0.071065
0.034969
0.019434

/u/
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.022826
0.078593
0.033156
0.002686
0.000000

/eu/ /eo/ final consonant neutral
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000

0.188355
0.090817
0.030935
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000528
0.028451
0.163543

0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000

0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000

0.656802
0.696856
0.700255
0.643459
0.653702
0.680858
0.705809
0.779496
0.790563
0.822482
0.721637
0.678211

Weights of Reference Phonemes

Neutral

Facial Capture
Acoustic Analysis

Create Speech Animation

Extract Detail Features
Visemes

Intensity Information of Speech
Facial Motion

Expressive Speech Animation

Speech
Neutral

Angry

Happy

Afraid

Surprise

Sad

Acoustic Analysis

Intensity Information of Speech

Markov
Random
Fields

Expressive Speech Animation using Emotion Blend Shapes

Figure 3: The overview of our facial gesture synthesis system.

professional (two actors and a narrator) and all the others
are non-professional volunteers. The speech sound data were
recorded simultaneously in the motion capture session at the
rate of 32KHz. We used Praat software [BW07] to analyze
the pitch and loudness of speech data.

4. Facial Gestures From Motion Capture
We would like to identify expressive facial gestures independently of the content (utterance of sentences and the corresponding lip movements) of facial motion capture data. The
facial gestures thus obtained are used to animate synthetic
faces uttering different contents (see Figure 3). Simply ignoring markers on the lips is not an ideal solution, because
the shape and movement of lips retain a lot of information
for capturing and transferring expressive gestures.
The facial expression F(t) captured at frame t is represented
as a long vector that concatenates the coordinates of marker
points.
F(t) = R(t) N + ∑ ci (t)Bi + P(t) + ∑ D j (t) + T (t),
i

j

where R(t) and T (t) represent the rigid transform of the head
motion, N is a neutral expression, Bi ’s are a basis of facial
expressions, P(t) is lip movements, and D j (t) is the detailed
movements of facial features. Each element of Bi is actually
a displacement from the neutral expression, so N + Bi for
each i represents an individual facial expression in a certain
emotional state and ci are the weight values of basis expresc 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

sions. In our experiments, six basis expressions are used (see
Figure 4).
Among the terms in the above equation, R(t), T (t), N and
Bi are determined trivially in the motion capture session. We
would like to determine two terms, ci (t) and D j (t), independently of P(t). Blend weight ci (t) for each i affects the
entire facial region and thus encodes a coordinated facial expression that requires a harmony of synchronized facial features. On the other hand, independent feature D j (t) for each
j acts on a small portion of the facial region and thus encodes the stochastic movements of an individual facial feature. P(t) is considered as a sort of independent features that
generates lip-synching. We identified five non-overlapping
regions (one for P(t) and the others for D j (t)) and sorted
corresponding facial markers into groups (see Figure 5).

4.1. Determining blend weights
Given the configuration of marker points F for frame t,
weight values ci ’s are determined such that R−1 (F − T ) −
N − ∑i ci Bi is minimized. The weight values thus obtained
are quite noisy because of small variations induced from
subtle facial gestures and uttering sentences. In order to cancel out small variations and lip movements, we consider the
average configuration F¯ = F(t − k) + · · · + F(t + k) /(2k +
1) of neighboring frames. In our experiments, the window
size is 2k + 1 = 41 (1.36 seconds). We use F¯ instead of F to
determine the weight values. The difference between them
is considered as stochastic movements that will be encoded
in P and D j .

384

Eunjung Ju & Jehee Lee / Facial Gestures From Motion Capture

Error

Number of Neighbors
20
40
60
80
100

1000

3000

2000

Number of
Parameter Samples

Figure 6: The plot of approximation errors with respect to
the number of samples and the size of nearest neighborhood
selection.
Neutral

Sad

Happy

Afraid

Angry

Surprise

Figure 4: The basis of facial expressions in motion capture
data.

Right Eyebrow
Right Eye
Left Eyebrow

bors from the set of random samples are used to estimate its
weight values. Without loss of generality, we assume that the
¯ Then, F¯ can be estimated
first nk samples are nearest to F.
as a combination of its neighboring samples.
nk

∑ ak Fk

R−1 (F¯ − T ) − N

k=1

Left Eye
Lips

= ∑ ak (∑ cki Bi ) = ∑(∑ ak cki )Bi = ∑ ci Bi
k

Figure 5: Groupings of facial markers. Each group corresponds to either P or D j .

This parameter estimation problem allows a least squares
solution, which often provides us with extremely extrapolated results. Such extrapolated blend weights are undesirable for transferring facial expressions from motion capture
data to synthetic faces. In order to avoid extreme extrapolation, the range of each weight should be constrained such
that −ε ≤ ci ≤ 1 + ε . If ε = 0 and ∑i ci ≤ 1, facial expressions are represented as a convex combination of basis expressions. In practice, the convexity condition is so restrictive that some exaggerated expressions cannot be reproduced
faithfully. If ε is too large, the weight values tend towards
noisy. In our experiments, we set 0 ≤ ε ≤ 1.
With this constraint, the least squares problem is not straightforward any more. We determine the weight values approximately by sampling the parameter space. We generate random weight values {cki |i = 1, · · · , nb , k = 1, · · · , ns } satisfying the constraint and produce a set of facial expressions
{Fk |k = 1, · · · , ns }, where ns is the number of random samples and nb is the number of basis expressions.
Fk =

nb

∑

i=1

cki Bi .

Given any novel facial expression F, its nk -nearest neigh-

i

i

i

k

where ak is inversely proportional to the distance between
R−1 (F¯ − T ) − N and Fk and normalized such that ∑k ak = 1.
Hence, F¯ can be represented approximately as a combination of basis expressions with weights ci = ∑k ak cki . In order
to locate nearest neighbors efficiently, we store data in a kdtree and search nk -nearest neighbors approximately within a
small error bound using ANN library [MA06].
Our experimental results in Figure 6 show that the approximation error decreases gracefully with respect to the number
of random samples. The number of nearest neighbors participating in the estimation process also affects the convergence
of approximation errors. In our experiments, we generated
1000 random samples and selected 60 nearest neighbors to
estimate weight values.
4.2. Determining independent stochastic features
Once the blend weights of basis expressions are determined,
independent features D j ’s and lip movements P can be determined simply as the residual of blend shape approximation.
D j and P are long vectors that concatenates the coordinates
of marker points. Most of their elements are zero and nonzero values are only for the markers in the corresponding
marker group (see Figure 5). Since the marker groups of P
and Di ’s are non-overlapping,
R−1 F − T

− N − ∑ ci Bi = P + ∑ D j + ε
i

j

determines their values. The error ε (t) is the movement of
markers that are not included in any of marker groups. ε (t)
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

385

Eunjung Ju & Jehee Lee / Facial Gestures From Motion Capture

˜ B˜ i are P˜l are
Among the terms in the above equation, N,
static geometric models manually designed by artists (see
Figure 7). dl ’s are determined from the phoneme analysis of
the driving speech signal. All the other terms are determined
in the process of Markov random fields.
5.1. Markov Random Fields
In this section, we discuss how to determine a sequence of
blend weight c˜i (t) using Markov random fields. Other terms
˜
T˜ (t), D˜ j (t) can be computed similarly.
R(t),

Neutral

Sad

Happy

Afraid

Angry

Surprise

Assume that the first t − 1 frames c˜i (1), · · · , c˜i (t − 1) has
already been computed and we want to determine the
next frame c˜i (t). The Markov random fields method determines c˜i (t) by matching its h-preceding frames {c˜i (t −
1), · · · , c˜i (t − h)} to reference data. The probability of
selecting ci (tˆ) as the value of c˜i (t) is proportional to
exp(−dist(t, tˆ)/σ ), where
dist(t, tˆ) =

h

∑

j=1

˜ − j) − S(tˆ − j)|2 +
α |S(t
nb

/a/

/e/

/i/

/o/

/u/

/eu/

/eo/

consonant
with open lips

consonant
with closed lips

Figure 7: (Top) The basis of synthetic facial expressions and
(Bottom) The basis of visemes.

is, in practice, not clearly noticeable in facial animation and
thus we simply ignore the error term.
5. Facial Gesture Synthesis
In this section, we discuss how to transfer the natural variations of facial gestures obtained from human motion to
synthetic faces. The synthetic face can be driven by input
speech. The driving speech signal is, in general, different
from the content we recorded in the motion capture session.
We intend to synthesize facial gestures independently of language, syllables, and phonemes except for lip movements.
Lip synching is handled separately.
˜
of our character at
The synthetic facial expression F(t)
frame t is represented as a vector that concatenates the coordinates of mesh points in the face geometry. We use a tilde
to denote symbols related to synthetic faces.
˜
˜ = R(t)
N˜ + ∑ c˜i (t)B˜ i + ∑ d˜l (t)P˜l + ∑ D˜ j (t) + T˜ (t)
F(t)
i

l

˜ and T˜ (t) represent the rigid transform of the head
where R(t)
motion. N˜ is a static, neutral expression of our face mesh.
B˜ i ’s are a basis of facial expressions and c˜i ’s are their weight
values. P˜l ’s are a basis of visemes and d˜l ’s are their weight
values.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

∑ |c˜i (t − j) − ci (tˆ − j)|2

i=1

.

σ controls the mapping between the distance measure and
the probability of selection. S(·) is the intensity of reference
speech data recorded synchronized with facial motion cap˜ is the intensity of speech data that the synture data and S(·)
thetic face utters. The first term favors the match of speech
and the second term favors the match of facial expressions at
previous frames. α weighs the importance of two terms. If
α is high, facial gestures synchronize well with the speech
data. The low value of α tends to favor the smooth transitioning of facial gestures over lip synching.
At every frame of the synthesis process, the probability of
transitioning from the current frame to every other frames
in the reference data can be computed to select the next
frame probabilistically. This approach allows bad transitions
to happen, though their probabilities are pretty low. In practice, such bad transitions are sometimes disturbing. Instead,
we search a small number (2 to 4 in our experiments) of good
candidates tˆ for transitioning that minimize dist(t, tˆ) and select one of them based on their probabilities.
The window size h is selected empirically. A small value for
h permits the flexibility in transitioning, while a larger value
weighs better context matching over the variety of output
animation. In our experiments, h = 10 was a nice trade-off
between the variety and quality of the resulting animation.
5.2. Transferring Detailed Features
The detailed facial features are dependent on the face geometry. Some of our face models are exaggerated or even

386

Eunjung Ju & Jehee Lee / Facial Gestures From Motion Capture

ample, the eyebrows can imitate the gestures of subject A,
while the head motion is simulating the style of subject B.
Facial expression transfer. Captured facial expressions can
be directly transferred to a synthetic face, even though the
synthetic face does not resemble the motion capture subject
at all (see Figure 9). It is possible because basis expressions are used on both sides [PKC∗ 03]. Captured expressions and transferred animated expressions share the same
blend weights, while their bases are totally different. With
this idea, we can manipulate synthetic faces for computer
puppetry via real time motion capture.

Figure 8: The user interface of our facial animation system

cartoon-like. Therefore, transferring D j to D˜ j requires extra efforts for establishing correspondences between facial
markers and synthetic face geometry. We manually picked
locations on the face geometry corresponding to facial markers. The movements of facial markers in D j are then scaled
appropriately to match the geometry of synthetic faces.
For example, we found an affine transform between actual
marker locations on the eye lids and their corresponding
points on the synthetic face such that they match at wide
opening and closing of eye lids.
Once the correspondences are established, RBF (radial basis function) interpolation [TO99] generates deformed face
geometry D˜ j corresponding to D j . In order to localize the
influence region of the deformation, bell-shaped cubic Bspline basis functions are employed. The linear polynomial
approximation term is not used because its support region is
infinitely wide.
6. Experimental Results
We recorded a variety of facial gestures from 12 subjects.
In the motion capture session, each subject was instructed to
read fairy tales and talk freely in several different emotional
states. We also recorded each subject exhibiting his/her own
distinctive personality in their facial expressions as much as
possible. Each motion clip is about one minute long and contains an individual “style” of facial gestures. A set of such
motion clips is used as a palette of facial gesture styles.
User interface. Our system provides the user with an easyto-use user interface for creating expressive facial animations in a few minutes (see Figure 8). In the workflow, the
user first imports input speech data and a palette of facial
gesture styles. Then, the user can “paint” the gesture styles
on the timeline synchronized with the input speech. It is also
possible to use different styles for different features. For ex-

Comparison to noise functions. Our animated result easily
outperforms the result using Perlin’s noise function in terms
of how natural it looks. One might be able to modulate the
noise function by adjusting parameters such that it generates
an animated face similar to ours. However, we found that
modulating the noise function took significant time and efforts.
Talking baby. Our gesture synthesis method is independent
of the language and sentences that the synthetic face talks.
The talking baby reads a French story and his gestures are
transferred from motion capture data uttering in a different
language. It took us only one minute to generate the 20 second talking baby animation.
7. Discussion
The primary advantage of our approach is its capability of
reproducing natural variations of human facial gestures in
animated faces. In our experiments, we observed the complexity and subtle details of facial gestures that cannot easily
be accomplished by using noise functions.
Our system learned expressive facial gestures from a small
amount of training data, primarily because we decoupled facial gestures from phonemes and lip movements. Although
we have not yet attempted to verify the interrelation between
prosodic features and facial gestures, the intensity and pitch
of the speech seem to be two major factors strongly synchronized with facial gestures. We made use of the intensity
information in our experiments. It would be interesting future work to incorporate the pitch information as well in our
system.
All facial models used in our experiments were designed
manually by artists and thus the resulting animation looks
cartoon-like rather than realistic. There are several guidelines for selecting basis expressions and designing their corresponding face geometry. We used only six basis models
(neutral, sad, happy, afraid, angry, surprise) in our experiments. More basis models can be employed for representing
a wider variety of facial expressions. We found that it is important to choose a basis expression that exhibits a coordination of facial features across the entire face. Otherwise, it
will interfere with stochastic movements of individual facial
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

Eunjung Ju & Jehee Lee / Facial Gestures From Motion Capture

387

Figure 9: Facial expression transfer from motion capture data to synthetic faces.

features and thus the blend weight estimation can be noisy.
We prefer to have our face geometry models with a closed
mouth. An open-mouth expression would cause mouth opening even for plosives in lip-synching. In order to make more
realistic facial animation, our method can still be used with
the basis models acquired from 3D face scanning.
We have visually compared the original motion capture data
and animated faces to see if natural human gestures are successfully captured and reproduced in the animated faces.
Though this visual comparison is an effective way of evaluating the quality of facial gestures in a subjective point of
view, we also need a quantitative method for evaluating and
characterzing facial gestures.

Acknowledgements

[BN07] B USSO C., NARAYANAN S.: Interrelation between speech and facial gestures in emotional utterances:
A single subject study. IEEE Transactions on Audio,
Speech and Language Processing (2007), In Press.
[Bra99] B RAND M.: Voice puppetry. In Proceedings of
SIGGRAPH 99 (August 1999), pp. 21–28.
[BW07] B OERSMA P., W EENINK D.: Praat: doing phonetics by computer. http://www.praat.org/, 2007.
[CB05] C HUANG E., B REGLER C.: Mood swings: expressive speech animation. ACM Transactions Graphics
24, 2 (2005), 331–347.
[CMRH04] C OSKER D., M ARSHALL D., ROSIN P. L.,
H ICKS Y.: Speech driven facial animation using a hidden markov coarticulation model. In Proceedings of ICPR
2004 (2004), pp. 128–131.

We would like to thank all the members of the SNU Movement Research Laboratory for their help in collecting motion
data. This work was partly supported by the Korea Science
and Engineering Foundation (R01-2007-000-11560-0).

[CPB∗ 94] C ASSELL J., P ELACHAUD C., BADLER N.,
S TEEDMAN M., ACHORN B., B ECHET T., D OUVILLE
B., P REVOST S., S TONE M.: Animated conversation:
Rule-based generation of facial expression gesture and
spoken intonation for multiple conversational agents. In
Proceedings of SIGGRAPH 94 (July 1994), pp. 413–420.

References

[CTFP05] C AO Y., T IEN W. C., FALOUTSOS P., P IGHIN
F.: Expressive speech-driven facial animation. ACM
Transactions Graphics 24, 4 (2005), 1283–1302.

[BB02] B YUN M., BADLER N. I.: FacEMOTE: qualitative parametric modifiers for facial animations. In
Proceedings of the 2002 ACM SIGGRAPH/Eurographics
symposium on Computer animation (2002), pp. 65–71.
[BCS97] B REGLER C., C OVELL M., S LANEY M.: Video
rewrite: driving visual speech with audio. In Proceedings
of SIGGRAPH 97 (1997), pp. 353–360.
[BDG∗ 07]

B USSO C., D ENG Z., G RIMM M., N EUMANN
U., NARAYANAN S.: Rigid head motion in expressive
speech animation: Analysis and synthesis. IEEE Transactions on Audio, Speech and Language Processing 15, 3
(2007), 1075 – 1086.

c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

[CXH03] C HAI J., X IAO J., H ODGINS J.: Vision-based
control of 3d facial animation. In Proceedings of the 2003
ACM SIGGRAPH/Eurographics symposium on Computer
animation (2003), pp. 193–206.
[DN06] D ENG Z., N EUMANN U.: efase: expressive
facial animation synthesis and editing with phonemeisomap controls. In Proceedings of the 2006 ACM SIGGRAPH/Eurographics symposium on Computer animation (2006), pp. 251–260.
[EGP02]

E ZZAT T., G EIGER G., P OGGIO T.: Trainable

388

Eunjung Ju & Jehee Lee / Facial Gestures From Motion Capture

videorealistic speech animation. ACM Transactions on
Graphics (SIGGRAPH 2002) (2002), 388–398.
[EMMT04] E GGES A., M OLET T., M AGNENATT HALMANN N.: Personalised real-time idle motion
synthesis. In Proceedings of Pacific Graphics 2004
(2004), pp. 121–130.
[JTDP03] J OSHI P., T IEN W. C., D ESBRUN M., P IGHIN
F.: Learning controls for blend shape based realistic facial animation. In Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation (2003), pp. 187–192.
[KMT03] K SHIRSAGAR S., M AGNENAT-T HALMANN
N.: Visyllable based speech animation. Computer Graphics Forum 22, 3 (2003), 632–640.
[LCR∗ 02] L EE J., C HAI J., R EITSMA P. S. A., H ODGINS
J. K., P OLLARD N. S.: Interactive control of avatars animated with human motion data. ACM Transactions on
Graphics (SIGGRAPH 2002) 21, 3 (2002), 491–500.

[VBPP05] V LASIC D., B RAND M., P FISTER H.,
P OPOVI C´ J.: Face transfer with multilinear models.
ACM Transactions on Graphics (SIGGRAPH 2005) 24,
3 (2005), 426–433.
[YRVB98] Y EHIA H., RUBIN P., VATIKIOTIS -BATESON
E.: Quantitative association of vocal-tract and facial behavior. Speech Communication 26, 1-2 (1998), 23 – 43.
[ZLGS03] Z HANG Q., L IU Z., G UO B., S HUM H.:
Geometry-driven photorealistic facial expression synthesis.
In Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation (2003), pp. 177–186.
[ZSCS04] Z HANG L., S NAVELY N., C URLESS B., S EITZ
S. M.: Spacetime faces: High-resolution capture for modeling and animation. ACM Transactions on Graphics
(SIGGRAPH 2004) 24, 3 (2004), 548–558.

[LTW95] L EE Y., T ERZOPOULOS D., WALTERS K.: Realistic modeling for facial animation. In Proceedings of
SIGGRAPH 95 (1995), pp. 55–62.
Ann: Li[MA06] M OUNT D., A RYA S.:
brary
for
approximate
nearest
neighbor
searching,
http://www.cs.sunysb.edu/
algorith/implement/ann/distrib/index1.html, 2006.
[MKPG05] M UELLER P., K ALBERER G. A., P ROES MANS M., G OOL L. V.: Realistic speech animation based
on observed 3d face dynamics. IEE Proc. Vision, Image
& Signal Processing 152 (August 2005), 491–500.
[Per85] P ERLIN K.: An image synthesizer. In Proceedings
of SIGGRAPH 85 (1985), pp. 287–296.
[Per95] P ERLIN K.: Real time responsive animation with
personality. IEEE Transactions on Visualization and
Computer Graphics) 1, 1 (1995), 5 – 15.
[Per07] P ERLIN K.: Improving noise. ACM Transactions
on Graphics (SIGGRAPH 2002) 26, 3 (2007), 681 – 682.
[PG96] P ERLIN K., G OLDBERG A.: Improv: A system
for scripting interactive actors in virtual worlds. In Proceedings of SIGGRAPH 96 (1996), pp. 205–216.
[PKC∗ 03] P YUN H., K IM Y., C HAE W., K ANG H. W.,
S HIN S. Y.: An example-based approach for facial expression cloning. In Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation (2003), pp. 167–176.
[SSRMF06] S IFAKIS E., S ELLE A., ROBINSON M OSHER A., F EDKIW R.: Simulating speech with a
physics-based facial muscle model. In Proceedings of
the 2006 ACM SIGGRAPH/Eurographics symposium on
Computer animation (2006), pp. 261–270.
[TO99] T URK G., O’B RIEN J. F.: Shape transformation
using variational implicit functions. In Proceedings of
SIGGRAPH 99 (1999), pp. 335–342.
c 2008 The Author(s)
Journal compilation c 2008 The Eurographics Association and Blackwell Publishing Ltd.

